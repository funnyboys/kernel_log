commit 201c1db90cd643282185a00770f12f95da330eca
Author: Joerg Roedel <jroedel@suse.de>
Date:   Tue Jul 23 09:51:00 2019 +0200

    iommu/iova: Fix compilation error with !CONFIG_IOMMU_IOVA
    
    The stub function for !CONFIG_IOMMU_IOVA needs to be
    'static inline'.
    
    Fixes: effa467870c76 ('iommu/vt-d: Don't queue_iova() if there is no flush queue')
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index cd0f1de901a8..a0637abffee8 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -236,7 +236,7 @@ static inline void init_iova_domain(struct iova_domain *iovad,
 {
 }
 
-bool has_iova_flush_queue(struct iova_domain *iovad)
+static inline bool has_iova_flush_queue(struct iova_domain *iovad)
 {
 	return false;
 }

commit effa467870c7612012885df4e246bdb8ffd8e44c
Author: Dmitry Safonov <dima@arista.com>
Date:   Tue Jul 16 22:38:05 2019 +0100

    iommu/vt-d: Don't queue_iova() if there is no flush queue
    
    Intel VT-d driver was reworked to use common deferred flushing
    implementation. Previously there was one global per-cpu flush queue,
    afterwards - one per domain.
    
    Before deferring a flush, the queue should be allocated and initialized.
    
    Currently only domains with IOMMU_DOMAIN_DMA type initialize their flush
    queue. It's probably worth to init it for static or unmanaged domains
    too, but it may be arguable - I'm leaving it to iommu folks.
    
    Prevent queuing an iova flush if the domain doesn't have a queue.
    The defensive check seems to be worth to keep even if queue would be
    initialized for all kinds of domains. And is easy backportable.
    
    On 4.19.43 stable kernel it has a user-visible effect: previously for
    devices in si domain there were crashes, on sata devices:
    
     BUG: spinlock bad magic on CPU#6, swapper/0/1
      lock: 0xffff88844f582008, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
     CPU: 6 PID: 1 Comm: swapper/0 Not tainted 4.19.43 #1
     Call Trace:
      <IRQ>
      dump_stack+0x61/0x7e
      spin_bug+0x9d/0xa3
      do_raw_spin_lock+0x22/0x8e
      _raw_spin_lock_irqsave+0x32/0x3a
      queue_iova+0x45/0x115
      intel_unmap+0x107/0x113
      intel_unmap_sg+0x6b/0x76
      __ata_qc_complete+0x7f/0x103
      ata_qc_complete+0x9b/0x26a
      ata_qc_complete_multiple+0xd0/0xe3
      ahci_handle_port_interrupt+0x3ee/0x48a
      ahci_handle_port_intr+0x73/0xa9
      ahci_single_level_irq_intr+0x40/0x60
      __handle_irq_event_percpu+0x7f/0x19a
      handle_irq_event_percpu+0x32/0x72
      handle_irq_event+0x38/0x56
      handle_edge_irq+0x102/0x121
      handle_irq+0x147/0x15c
      do_IRQ+0x66/0xf2
      common_interrupt+0xf/0xf
     RIP: 0010:__do_softirq+0x8c/0x2df
    
    The same for usb devices that use ehci-pci:
     BUG: spinlock bad magic on CPU#0, swapper/0/1
      lock: 0xffff88844f402008, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 4.19.43 #4
     Call Trace:
      <IRQ>
      dump_stack+0x61/0x7e
      spin_bug+0x9d/0xa3
      do_raw_spin_lock+0x22/0x8e
      _raw_spin_lock_irqsave+0x32/0x3a
      queue_iova+0x77/0x145
      intel_unmap+0x107/0x113
      intel_unmap_page+0xe/0x10
      usb_hcd_unmap_urb_setup_for_dma+0x53/0x9d
      usb_hcd_unmap_urb_for_dma+0x17/0x100
      unmap_urb_for_dma+0x22/0x24
      __usb_hcd_giveback_urb+0x51/0xc3
      usb_giveback_urb_bh+0x97/0xde
      tasklet_action_common.isra.4+0x5f/0xa1
      tasklet_action+0x2d/0x30
      __do_softirq+0x138/0x2df
      irq_exit+0x7d/0x8b
      smp_apic_timer_interrupt+0x10f/0x151
      apic_timer_interrupt+0xf/0x20
      </IRQ>
     RIP: 0010:_raw_spin_unlock_irqrestore+0x17/0x39
    
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Lu Baolu <baolu.lu@linux.intel.com>
    Cc: iommu@lists.linux-foundation.org
    Cc: <stable@vger.kernel.org> # 4.14+
    Fixes: 13cf01744608 ("iommu/vt-d: Make use of iova deferred flushing")
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Reviewed-by: Lu Baolu <baolu.lu@linux.intel.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 781b96ac706f..cd0f1de901a8 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -155,6 +155,7 @@ struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
 void init_iova_domain(struct iova_domain *iovad, unsigned long granule,
 	unsigned long start_pfn);
+bool has_iova_flush_queue(struct iova_domain *iovad);
 int init_iova_flush_queue(struct iova_domain *iovad,
 			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
@@ -235,6 +236,11 @@ static inline void init_iova_domain(struct iova_domain *iovad,
 {
 }
 
+bool has_iova_flush_queue(struct iova_domain *iovad)
+{
+	return false;
+}
+
 static inline int init_iova_flush_queue(struct iova_domain *iovad,
 					iova_flush_cb flush_cb,
 					iova_entry_dtor entry_dtor)

commit 55716d26439f5c4008b0bcb7f17d1f7c0d8fbcfc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:42 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 428
    
    Based on 1 normalized pattern(s):
    
      this file is released under the gplv2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 68 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190114.292346262@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 28a5128405f8..781b96ac706f 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -1,11 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (c) 2006, Intel Corporation.
  *
- * This file is released under the GPLv2.
- *
  * Copyright (C) 2006-2008 Intel Corporation
  * Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
- *
  */
 
 #ifndef _IOVA_H_

commit 14bd9a607f9082e7b5690c27e69072f2aeae0de4
Author: Jinyu Qi <jinyuqi@huawei.com>
Date:   Wed Apr 3 16:35:21 2019 +0800

    iommu/iova: Separate atomic variables to improve performance
    
    In struct iova_domain, there are three atomic variables, the former two
    are about TLB flush counters which use atomic_add operation, anoter is
    used to flush timer that use cmpxhg operation.
    These variables are in the same cache line, so it will cause some
    performance loss under the condition that many cores call queue_iova
    function, Let's isolate the two type atomic variables to different
    cache line to reduce cache line conflict.
    
    Cc: Joerg Roedel <joro@8bytes.org>
    Signed-off-by: Jinyu Qi <jinyuqi@huawei.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 0b93bf96693e..28a5128405f8 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -76,6 +76,14 @@ struct iova_domain {
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
 	unsigned long	max32_alloc_size; /* Size of last failed allocation */
+	struct iova_fq __percpu *fq;	/* Flush Queue */
+
+	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
+						   have been started */
+
+	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
+						   have been finished */
+
 	struct iova	anchor;		/* rbtree lookup anchor */
 	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
 
@@ -85,14 +93,6 @@ struct iova_domain {
 	iova_entry_dtor entry_dtor;	/* IOMMU driver specific destructor for
 					   iova entry */
 
-	struct iova_fq __percpu *fq;	/* Flush Queue */
-
-	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
-						   have been started */
-
-	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
-						   have been finished */
-
 	struct timer_list fq_timer;		/* Timer to regularily empty the
 						   flush-queues */
 	atomic_t fq_timer_on;			/* 1 when timer is active, 0

commit bee60e94a1e20ec0b8ffdafae270731d8fda4551
Author: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
Date:   Wed Sep 5 09:57:36 2018 +0530

    iommu/iova: Optimise attempts to allocate iova from 32bit address range
    
    As an optimisation for PCI devices, there is always first attempt
    been made to allocate iova from SAC address range. This will lead
    to unnecessary attempts, when there are no free ranges
    available. Adding fix to track recently failed iova address size and
    allow further attempts, only if requested size is lesser than a failed
    size. The size is updated when any replenish happens.
    
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 928442dda565..0b93bf96693e 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -75,6 +75,7 @@ struct iova_domain {
 	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
+	unsigned long	max32_alloc_size; /* Size of last failed allocation */
 	struct iova	anchor;		/* rbtree lookup anchor */
 	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
 

commit 538d5b333216c3daa7a5821307164f10af73ec8c
Author: Tomasz Nowicki <tomasz.nowicki@caviumnetworks.com>
Date:   Wed Sep 20 10:52:02 2017 +0200

    iommu/iova: Make rcache flush optional on IOVA allocation failure
    
    Since IOVA allocation failure is not unusual case we need to flush
    CPUs' rcache in hope we will succeed in next round.
    
    However, it is useful to decide whether we need rcache flush step because
    of two reasons:
    - Not scalability. On large system with ~100 CPUs iterating and flushing
      rcache for each CPU becomes serious bottleneck so we may want to defer it.
    - free_cpu_cached_iovas() does not care about max PFN we are interested in.
      Thus we may flush our rcaches and still get no new IOVA like in the
      commonly used scenario:
    
        if (dma_limit > DMA_BIT_MASK(32) && dev_is_pci(dev))
            iova = alloc_iova_fast(iovad, iova_len, DMA_BIT_MASK(32) >> shift);
    
        if (!iova)
            iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift);
    
       1. First alloc_iova_fast() call is limited to DMA_BIT_MASK(32) to get
          PCI devices a SAC address
       2. alloc_iova() fails due to full 32-bit space
       3. rcaches contain PFNs out of 32-bit space so free_cpu_cached_iovas()
          throws entries away for nothing and alloc_iova() fails again
       4. Next alloc_iova_fast() call cannot take advantage of rcache since we
          have just defeated caches. In this case we pick the slowest option
          to proceed.
    
    This patch reworks flushed_rcache local flag to be additional function
    argument instead and control rcache flush step. Also, it updates all users
    to do the flush as the last chance.
    
    Signed-off-by: Tomasz Nowicki <Tomasz.Nowicki@caviumnetworks.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Tested-by: Nate Watterson <nwatters@codeaurora.org>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index c696ee81054e..928442dda565 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -150,7 +150,7 @@ void queue_iova(struct iova_domain *iovad,
 		unsigned long pfn, unsigned long pages,
 		unsigned long data);
 unsigned long alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
-			      unsigned long limit_pfn);
+			      unsigned long limit_pfn, bool flush_rcache);
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
@@ -212,7 +212,8 @@ static inline void queue_iova(struct iova_domain *iovad,
 
 static inline unsigned long alloc_iova_fast(struct iova_domain *iovad,
 					    unsigned long size,
-					    unsigned long limit_pfn)
+					    unsigned long limit_pfn,
+					    bool flush_rcache)
 {
 	return 0;
 }

commit bb68b2fbfbd643d4407541f9c7a16a2c9b3a57c7
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Thu Sep 21 16:52:46 2017 +0100

    iommu/iova: Add rbtree anchor node
    
    Add a permanent dummy IOVA reservation to the rbtree, such that we can
    always access the top of the address space instantly. The immediate
    benefit is that we remove the overhead of the rb_last() traversal when
    not using the cached node, but it also paves the way for further
    simplifications.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 953cfd20f152..c696ee81054e 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -75,6 +75,7 @@ struct iova_domain {
 	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
+	struct iova	anchor;		/* rbtree lookup anchor */
 	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
 
 	iova_flush_cb	flush_cb;	/* Call-Back function to flush IOMMU

commit aa3ac9469c1850ed00741955b975c3a19029763a
Author: Zhen Lei <thunder.leizhen@huawei.com>
Date:   Thu Sep 21 16:52:45 2017 +0100

    iommu/iova: Make dma_32bit_pfn implicit
    
    Now that the cached node optimisation can apply to all allocations, the
    couple of users which were playing tricks with dma_32bit_pfn in order to
    benefit from it can stop doing so. Conversely, there is also no need for
    all the other users to explicitly calculate a 'real' 32-bit PFN, when
    init_iova_domain() can happily do that itself from the page granularity.
    
    CC: Thierry Reding <thierry.reding@gmail.com>
    CC: Jonathan Hunter <jonathanh@nvidia.com>
    CC: David Airlie <airlied@linux.ie>
    CC: Sudeep Dutt <sudeep.dutt@intel.com>
    CC: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Zhen Lei <thunder.leizhen@huawei.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Zhen Lei <thunder.leizhen@huawei.com>
    Tested-by: Nate Watterson <nwatters@codeaurora.org>
    [rm: use iova_shift(), rewrote commit message]
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 69ea3e258ff2..953cfd20f152 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -154,7 +154,7 @@ struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
 void init_iova_domain(struct iova_domain *iovad, unsigned long granule,
-	unsigned long start_pfn, unsigned long pfn_32bit);
+	unsigned long start_pfn);
 int init_iova_flush_queue(struct iova_domain *iovad,
 			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
@@ -230,8 +230,7 @@ static inline void copy_reserved_iova(struct iova_domain *from,
 
 static inline void init_iova_domain(struct iova_domain *iovad,
 				    unsigned long granule,
-				    unsigned long start_pfn,
-				    unsigned long pfn_32bit)
+				    unsigned long start_pfn)
 {
 }
 

commit e60aa7b53845a261dd419652f12ab9f89e668843
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Thu Sep 21 16:52:44 2017 +0100

    iommu/iova: Extend rbtree node caching
    
    The cached node mechanism provides a significant performance benefit for
    allocations using a 32-bit DMA mask, but in the case of non-PCI devices
    or where the 32-bit space is full, the loss of this benefit can be
    significant - on large systems there can be many thousands of entries in
    the tree, such that walking all the way down to find free space every
    time becomes increasingly awful.
    
    Maintain a similar cached node for the whole IOVA space as a superset of
    the 32-bit space so that performance can remain much more consistent.
    
    Inspired by work by Zhen Lei <thunder.leizhen@huawei.com>.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Zhen Lei <thunder.leizhen@huawei.com>
    Tested-by: Nate Watterson <nwatters@codeaurora.org>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index d179b9bf7814..69ea3e258ff2 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -70,7 +70,8 @@ struct iova_fq {
 struct iova_domain {
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
 	struct rb_root	rbroot;		/* iova domain rbtree root */
-	struct rb_node	*cached32_node; /* Save last alloced node */
+	struct rb_node	*cached_node;	/* Save last alloced node */
+	struct rb_node	*cached32_node; /* Save last 32-bit alloced node */
 	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;

commit 9a005a800ae817c2c90ef117d7cd77614d866777
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Aug 10 16:58:18 2017 +0200

    iommu/iova: Add flush timer
    
    Add a timer to flush entries from the Flush-Queues every
    10ms. This makes sure that no stale TLB entries remain for
    too long after an IOVA has been unmapped.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 913a690cd4b0..d179b9bf7814 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -48,6 +48,9 @@ typedef void (* iova_entry_dtor)(unsigned long data);
 /* Number of entries per Flush Queue */
 #define IOVA_FQ_SIZE	256
 
+/* Timeout (in ms) after which entries are flushed from the Flush-Queue */
+#define IOVA_FQ_TIMEOUT	10
+
 /* Flush Queue entry for defered flushing */
 struct iova_fq_entry {
 	unsigned long iova_pfn;
@@ -86,6 +89,11 @@ struct iova_domain {
 
 	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
 						   have been finished */
+
+	struct timer_list fq_timer;		/* Timer to regularily empty the
+						   flush-queues */
+	atomic_t fq_timer_on;			/* 1 when timer is active, 0
+						   when not */
 };
 
 static inline unsigned long iova_size(struct iova *iova)

commit 8109c2a2f8463852dddd6a1c3fcf262047c0c124
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Aug 10 16:31:17 2017 +0200

    iommu/iova: Add locking to Flush-Queues
    
    The lock is taken from the same CPU most of the time. But
    having it allows to flush the queue also from another CPU if
    necessary.
    
    This will be used by a timer to regularily flush any pending
    IOVAs from the Flush-Queues.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 985b8008999e..913a690cd4b0 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -60,6 +60,7 @@ struct iova_fq_entry {
 struct iova_fq {
 	struct iova_fq_entry entries[IOVA_FQ_SIZE];
 	unsigned head, tail;
+	spinlock_t lock;
 };
 
 /* holds all the iova translations for a domain */

commit fb418dab8a4f01dde0c025d15145c589ec02796b
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Aug 10 16:14:59 2017 +0200

    iommu/iova: Add flush counters to Flush-Queue implementation
    
    There are two counters:
    
            * fq_flush_start_cnt  - Increased when a TLB flush
                                    is started.
    
            * fq_flush_finish_cnt - Increased when a TLB flush
                                    is finished.
    
    The fq_flush_start_cnt is assigned to every Flush-Queue
    entry on its creation. When freeing entries from the
    Flush-Queue, the value in the entry is compared to the
    fq_flush_finish_cnt. The entry can only be freed when its
    value is less than the value of fq_flush_finish_cnt.
    
    The reason for these counters it to take advantage of IOMMU
    TLB flushes that happened on other CPUs. These already
    flushed the TLB for Flush-Queue entries on other CPUs so
    that they can already be freed without flushing the TLB
    again.
    
    This makes it less likely that the Flush-Queue is full and
    saves IOMMU TLB flushes.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 1ae85248ec50..985b8008999e 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -14,6 +14,7 @@
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/rbtree.h>
+#include <linux/atomic.h>
 #include <linux/dma-mapping.h>
 
 /* iova structure */
@@ -52,6 +53,7 @@ struct iova_fq_entry {
 	unsigned long iova_pfn;
 	unsigned long pages;
 	unsigned long data;
+	u64 counter; /* Flush counter when this entrie was added */
 };
 
 /* Per-CPU Flush Queue structure */
@@ -77,6 +79,12 @@ struct iova_domain {
 					   iova entry */
 
 	struct iova_fq __percpu *fq;	/* Flush Queue */
+
+	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
+						   have been started */
+
+	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
+						   have been finished */
 };
 
 static inline unsigned long iova_size(struct iova *iova)

commit 1928210107edd4fa786199fef6b875d3af3bef88
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Aug 10 15:49:44 2017 +0200

    iommu/iova: Implement Flush-Queue ring buffer
    
    Add a function to add entries to the Flush-Queue ring
    buffer. If the buffer is full, call the flush-callback and
    free the entries.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 8aa10896150e..1ae85248ec50 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -127,6 +127,9 @@ struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
 	bool size_aligned);
 void free_iova_fast(struct iova_domain *iovad, unsigned long pfn,
 		    unsigned long size);
+void queue_iova(struct iova_domain *iovad,
+		unsigned long pfn, unsigned long pages,
+		unsigned long data);
 unsigned long alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
 			      unsigned long limit_pfn);
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
@@ -182,6 +185,12 @@ static inline void free_iova_fast(struct iova_domain *iovad,
 {
 }
 
+static inline void queue_iova(struct iova_domain *iovad,
+			      unsigned long pfn, unsigned long pages,
+			      unsigned long data)
+{
+}
+
 static inline unsigned long alloc_iova_fast(struct iova_domain *iovad,
 					    unsigned long size,
 					    unsigned long limit_pfn)

commit 42f87e71c3df12d8f29ec1bb7b47772ffaeaf1ee
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Aug 10 14:44:28 2017 +0200

    iommu/iova: Add flush-queue data structures
    
    This patch adds the basic data-structures to implement
    flush-queues in the generic IOVA code. It also adds the
    initialization and destroy routines for these data
    structures.
    
    The initialization routine is designed so that the use of
    this feature is optional for the users of IOVA code.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index e0a892ae45c0..8aa10896150e 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -36,6 +36,30 @@ struct iova_rcache {
 	struct iova_cpu_rcache __percpu *cpu_rcaches;
 };
 
+struct iova_domain;
+
+/* Call-Back from IOVA code into IOMMU drivers */
+typedef void (* iova_flush_cb)(struct iova_domain *domain);
+
+/* Destructor for per-entry data */
+typedef void (* iova_entry_dtor)(unsigned long data);
+
+/* Number of entries per Flush Queue */
+#define IOVA_FQ_SIZE	256
+
+/* Flush Queue entry for defered flushing */
+struct iova_fq_entry {
+	unsigned long iova_pfn;
+	unsigned long pages;
+	unsigned long data;
+};
+
+/* Per-CPU Flush Queue structure */
+struct iova_fq {
+	struct iova_fq_entry entries[IOVA_FQ_SIZE];
+	unsigned head, tail;
+};
+
 /* holds all the iova translations for a domain */
 struct iova_domain {
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
@@ -45,6 +69,14 @@ struct iova_domain {
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
 	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
+
+	iova_flush_cb	flush_cb;	/* Call-Back function to flush IOMMU
+					   TLBs */
+
+	iova_entry_dtor entry_dtor;	/* IOMMU driver specific destructor for
+					   iova entry */
+
+	struct iova_fq __percpu *fq;	/* Flush Queue */
 };
 
 static inline unsigned long iova_size(struct iova *iova)
@@ -102,6 +134,8 @@ struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
 void init_iova_domain(struct iova_domain *iovad, unsigned long granule,
 	unsigned long start_pfn, unsigned long pfn_32bit);
+int init_iova_flush_queue(struct iova_domain *iovad,
+			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
 struct iova *split_and_remove_iova(struct iova_domain *iovad,
@@ -174,6 +208,13 @@ static inline void init_iova_domain(struct iova_domain *iovad,
 {
 }
 
+static inline int init_iova_flush_queue(struct iova_domain *iovad,
+					iova_flush_cb flush_cb,
+					iova_entry_dtor entry_dtor)
+{
+	return -ENODEV;
+}
+
 static inline struct iova *find_iova(struct iova_domain *iovad,
 				     unsigned long pfn)
 {

commit b4d8c7aea15efa8c6272c58d78296f8b017c4c6a
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Mar 23 00:06:17 2017 +0100

    iommu/iova: Fix compile error with CONFIG_IOMMU_IOVA=m
    
    The #ifdef in iova.h only catches the CONFIG_IOMMU_IOVA=y
    case, so that compilation as a module fails with duplicate
    function definition errors. Fix it by catching both cases in
    the #if.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 548982ad5f2f..e0a892ae45c0 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -82,7 +82,7 @@ static inline unsigned long iova_pfn(struct iova_domain *iovad, dma_addr_t iova)
 	return iova >> iova_shift(iovad);
 }
 
-#ifdef CONFIG_IOMMU_IOVA
+#if IS_ENABLED(CONFIG_IOMMU_IOVA)
 int iova_cache_get(void);
 void iova_cache_put(void);
 

commit 21aff52ab2c831c2f07d48e2fa8d4bab26a66992
Author: Thierry Reding <treding@nvidia.com>
Date:   Mon Mar 20 20:11:28 2017 +0100

    iommu: Add dummy implementations for !IOMMU_IOVA
    
    Currently, building code which uses the API guarded by the IOMMU_IOVA
    will fail to link if IOMMU_IOVA is not enabled. Often this code will be
    using the API provided by the IOMMU_API Kconfig symbol, but support for
    this can be optional, with code falling back to contiguous memory. This
    commit implements dummy functions for the IOVA API so that it can be
    compiled out.
    
    With both IOMMU_API and IOMMU_IOVA optional, code can now be built with
    or without support for IOMMU without having to resort to #ifdefs in the
    user code.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index f27bb2c62fca..548982ad5f2f 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -82,6 +82,7 @@ static inline unsigned long iova_pfn(struct iova_domain *iovad, dma_addr_t iova)
 	return iova >> iova_shift(iovad);
 }
 
+#ifdef CONFIG_IOMMU_IOVA
 int iova_cache_get(void);
 void iova_cache_put(void);
 
@@ -106,5 +107,95 @@ void put_iova_domain(struct iova_domain *iovad);
 struct iova *split_and_remove_iova(struct iova_domain *iovad,
 	struct iova *iova, unsigned long pfn_lo, unsigned long pfn_hi);
 void free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad);
+#else
+static inline int iova_cache_get(void)
+{
+	return -ENOTSUPP;
+}
+
+static inline void iova_cache_put(void)
+{
+}
+
+static inline struct iova *alloc_iova_mem(void)
+{
+	return NULL;
+}
+
+static inline void free_iova_mem(struct iova *iova)
+{
+}
+
+static inline void free_iova(struct iova_domain *iovad, unsigned long pfn)
+{
+}
+
+static inline void __free_iova(struct iova_domain *iovad, struct iova *iova)
+{
+}
+
+static inline struct iova *alloc_iova(struct iova_domain *iovad,
+				      unsigned long size,
+				      unsigned long limit_pfn,
+				      bool size_aligned)
+{
+	return NULL;
+}
+
+static inline void free_iova_fast(struct iova_domain *iovad,
+				  unsigned long pfn,
+				  unsigned long size)
+{
+}
+
+static inline unsigned long alloc_iova_fast(struct iova_domain *iovad,
+					    unsigned long size,
+					    unsigned long limit_pfn)
+{
+	return 0;
+}
+
+static inline struct iova *reserve_iova(struct iova_domain *iovad,
+					unsigned long pfn_lo,
+					unsigned long pfn_hi)
+{
+	return NULL;
+}
+
+static inline void copy_reserved_iova(struct iova_domain *from,
+				      struct iova_domain *to)
+{
+}
+
+static inline void init_iova_domain(struct iova_domain *iovad,
+				    unsigned long granule,
+				    unsigned long start_pfn,
+				    unsigned long pfn_32bit)
+{
+}
+
+static inline struct iova *find_iova(struct iova_domain *iovad,
+				     unsigned long pfn)
+{
+	return NULL;
+}
+
+static inline void put_iova_domain(struct iova_domain *iovad)
+{
+}
+
+static inline struct iova *split_and_remove_iova(struct iova_domain *iovad,
+						 struct iova *iova,
+						 unsigned long pfn_lo,
+						 unsigned long pfn_hi)
+{
+	return NULL;
+}
+
+static inline void free_cpu_cached_iovas(unsigned int cpu,
+					 struct iova_domain *iovad)
+{
+}
+#endif
 
 #endif

commit 9257b4a206fc0229dd5f84b78e4d1ebf3f91d270
Author: Omer Peleg <omer@cs.technion.ac.il>
Date:   Wed Apr 20 11:34:11 2016 +0300

    iommu/iova: introduce per-cpu caching to iova allocation
    
    IOVA allocation has two problems that impede high-throughput I/O.
    First, it can do a linear search over the allocated IOVA ranges.
    Second, the rbtree spinlock that serializes IOVA allocations becomes
    contended.
    
    Address these problems by creating an API for caching allocated IOVA
    ranges, so that the IOVA allocator isn't accessed frequently.  This
    patch adds a per-CPU cache, from which CPUs can alloc/free IOVAs
    without taking the rbtree spinlock.  The per-CPU caches are backed by
    a global cache, to avoid invoking the (linear-time) IOVA allocator
    without needing to make the per-CPU cache size excessive.  This design
    is based on magazines, as described in "Magazines and Vmem: Extending
    the Slab Allocator to Many CPUs and Arbitrary Resources" (currently
    available at https://www.usenix.org/legacy/event/usenix01/bonwick.html)
    
    Adding caching on top of the existing rbtree allocator maintains the
    property that IOVAs are densely packed in the IO virtual address space,
    which is important for keeping IOMMU page table usage low.
    
    To keep the cache size reasonable, we bound the IOVA space a CPU can
    cache by 32 MiB (we cache a bounded number of IOVA ranges, and only
    ranges of size <= 128 KiB).  The shared global cache is bounded at
    4 MiB of IOVA space.
    
    Signed-off-by: Omer Peleg <omer@cs.technion.ac.il>
    [mad@cs.technion.ac.il: rebased, cleaned up and reworded the commit message]
    Signed-off-by: Adam Morrison <mad@cs.technion.ac.il>
    Reviewed-by: Shaohua Li <shli@fb.com>
    Reviewed-by: Ben Serebrin <serebrin@google.com>
    [dwmw2: split out VT-d part into a separate patch]
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 92f7177db2ce..f27bb2c62fca 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -19,8 +19,21 @@
 /* iova structure */
 struct iova {
 	struct rb_node	node;
-	unsigned long	pfn_hi; /* IOMMU dish out addr hi */
-	unsigned long	pfn_lo; /* IOMMU dish out addr lo */
+	unsigned long	pfn_hi; /* Highest allocated pfn */
+	unsigned long	pfn_lo; /* Lowest allocated pfn */
+};
+
+struct iova_magazine;
+struct iova_cpu_rcache;
+
+#define IOVA_RANGE_CACHE_MAX_SIZE 6	/* log of max cached IOVA range size (in pages) */
+#define MAX_GLOBAL_MAGS 32	/* magazines per bin */
+
+struct iova_rcache {
+	spinlock_t lock;
+	unsigned long depot_size;
+	struct iova_magazine *depot[MAX_GLOBAL_MAGS];
+	struct iova_cpu_rcache __percpu *cpu_rcaches;
 };
 
 /* holds all the iova translations for a domain */
@@ -31,6 +44,7 @@ struct iova_domain {
 	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
+	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
 };
 
 static inline unsigned long iova_size(struct iova *iova)
@@ -78,6 +92,10 @@ void __free_iova(struct iova_domain *iovad, struct iova *iova);
 struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
 	unsigned long limit_pfn,
 	bool size_aligned);
+void free_iova_fast(struct iova_domain *iovad, unsigned long pfn,
+		    unsigned long size);
+unsigned long alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
+			      unsigned long limit_pfn);
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
@@ -87,5 +105,6 @@ struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
 struct iova *split_and_remove_iova(struct iova_domain *iovad,
 	struct iova *iova, unsigned long pfn_lo, unsigned long pfn_hi);
+void free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad);
 
 #endif

commit ae1ff3d623905947158fd3394854c23026337810
Author: Sakari Ailus <sakari.ailus@linux.intel.com>
Date:   Mon Jul 13 14:31:28 2015 +0300

    iommu: iova: Move iova cache management to the iova library
    
    This is necessary to separate intel-iommu from the iova library.
    
    Signed-off-by: Sakari Ailus <sakari.ailus@linux.intel.com>
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 3920a19d8194..92f7177db2ce 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -68,8 +68,8 @@ static inline unsigned long iova_pfn(struct iova_domain *iovad, dma_addr_t iova)
 	return iova >> iova_shift(iovad);
 }
 
-int iommu_iova_cache_init(void);
-void iommu_iova_cache_destroy(void);
+int iova_cache_get(void);
+void iova_cache_put(void);
 
 struct iova *alloc_iova_mem(void);
 void free_iova_mem(struct iova *iova);

commit 0fb5fe874c42942e16c450ae05da453e13a1c09e
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jan 12 17:51:16 2015 +0000

    iommu: Make IOVA domain page size explicit
    
    Systems may contain heterogeneous IOMMUs supporting differing minimum
    page sizes, which may also not be common with the CPU page size.
    Thus it is practical to have an explicit notion of IOVA granularity
    to simplify handling of mapping and allocation constraints.
    
    As an initial step, move the IOVA page granularity from an implicit
    compile-time constant to a per-domain property so we can make use
    of it in IOVA domain context at runtime. To keep the abstraction tidy,
    extend the little API of inline iova_* helpers to parallel some of the
    equivalent PAGE_* macros.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 591b19626b46..3920a19d8194 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -28,6 +28,7 @@ struct iova_domain {
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
 	struct rb_root	rbroot;		/* iova domain rbtree root */
 	struct rb_node	*cached32_node; /* Save last alloced node */
+	unsigned long	granule;	/* pfn granularity for this domain */
 	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
 };
@@ -37,6 +38,36 @@ static inline unsigned long iova_size(struct iova *iova)
 	return iova->pfn_hi - iova->pfn_lo + 1;
 }
 
+static inline unsigned long iova_shift(struct iova_domain *iovad)
+{
+	return __ffs(iovad->granule);
+}
+
+static inline unsigned long iova_mask(struct iova_domain *iovad)
+{
+	return iovad->granule - 1;
+}
+
+static inline size_t iova_offset(struct iova_domain *iovad, dma_addr_t iova)
+{
+	return iova & iova_mask(iovad);
+}
+
+static inline size_t iova_align(struct iova_domain *iovad, size_t size)
+{
+	return ALIGN(size, iovad->granule);
+}
+
+static inline dma_addr_t iova_dma_addr(struct iova_domain *iovad, struct iova *iova)
+{
+	return (dma_addr_t)iova->pfn_lo << iova_shift(iovad);
+}
+
+static inline unsigned long iova_pfn(struct iova_domain *iovad, dma_addr_t iova)
+{
+	return iova >> iova_shift(iovad);
+}
+
 int iommu_iova_cache_init(void);
 void iommu_iova_cache_destroy(void);
 
@@ -50,8 +81,8 @@ struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
-void init_iova_domain(struct iova_domain *iovad, unsigned long start_pfn,
-	unsigned long pfn_32bit);
+void init_iova_domain(struct iova_domain *iovad, unsigned long granule,
+	unsigned long start_pfn, unsigned long pfn_32bit);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
 struct iova *split_and_remove_iova(struct iova_domain *iovad,

commit 1b72250076dde4276acecf3a7da722b185703e78
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jan 12 17:51:15 2015 +0000

    iommu: Make IOVA domain low limit flexible
    
    To share the IOVA allocator with other architectures, it needs to
    accommodate more general aperture restrictions; move the lower limit
    from a compile-time constant to a runtime domain property to allow
    IOVA domains with different requirements to co-exist.
    
    Also reword the slightly unclear description of alloc_iova since we're
    touching it anyway.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index ad0507c61cc7..591b19626b46 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -16,9 +16,6 @@
 #include <linux/rbtree.h>
 #include <linux/dma-mapping.h>
 
-/* IO virtual address start page frame number */
-#define IOVA_START_PFN		(1)
-
 /* iova structure */
 struct iova {
 	struct rb_node	node;
@@ -31,6 +28,7 @@ struct iova_domain {
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
 	struct rb_root	rbroot;		/* iova domain rbtree root */
 	struct rb_node	*cached32_node; /* Save last alloced node */
+	unsigned long	start_pfn;	/* Lower limit for this domain */
 	unsigned long	dma_32bit_pfn;
 };
 
@@ -52,7 +50,8 @@ struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
 struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
 	unsigned long pfn_hi);
 void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
-void init_iova_domain(struct iova_domain *iovad, unsigned long pfn_32bit);
+void init_iova_domain(struct iova_domain *iovad, unsigned long start_pfn,
+	unsigned long pfn_32bit);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
 struct iova *split_and_remove_iova(struct iova_domain *iovad,

commit 85b4545629663486b7f71047ce3b54fa0ad3eb28
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jan 12 17:51:14 2015 +0000

    iommu: Consolidate IOVA allocator code
    
    In order to share the IOVA allocator with other architectures, break
    the unnecssary dependency on the Intel IOMMU driver and move the
    remaining IOVA internals to iova.c
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 19e81d5ccb6d..ad0507c61cc7 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -39,6 +39,9 @@ static inline unsigned long iova_size(struct iova *iova)
 	return iova->pfn_hi - iova->pfn_lo + 1;
 }
 
+int iommu_iova_cache_init(void);
+void iommu_iova_cache_destroy(void);
+
 struct iova *alloc_iova_mem(void);
 void free_iova_mem(struct iova *iova);
 void free_iova(struct iova_domain *iovad, unsigned long pfn);

commit a156ef99e874f3701367cc192aa604bcf8c0a236
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Fri Jul 11 14:19:36 2014 +0800

    iommu/vt-d: Introduce helper function iova_size() to improve code readability
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 3277f4711349..19e81d5ccb6d 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -34,6 +34,11 @@ struct iova_domain {
 	unsigned long	dma_32bit_pfn;
 };
 
+static inline unsigned long iova_size(struct iova *iova)
+{
+	return iova->pfn_hi - iova->pfn_lo + 1;
+}
+
 struct iova *alloc_iova_mem(void);
 void free_iova_mem(struct iova *iova);
 void free_iova(struct iova_domain *iovad, unsigned long pfn);

commit 75f05569d0e51f6332a291c82abbeb7c8262e32d
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Wed Feb 19 14:07:37 2014 +0800

    iommu/vt-d: Update IOMMU state when memory hotplug happens
    
    If static identity domain is created, IOMMU driver needs to update
    si_domain page table when memory hotplug event happens. Otherwise
    PCI device DMA operations can't access the hot-added memory regions.
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Signed-off-by: Joerg Roedel <joro@8bytes.org>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 76a0759e88ec..3277f4711349 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -47,5 +47,7 @@ void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
 void init_iova_domain(struct iova_domain *iovad, unsigned long pfn_32bit);
 struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
 void put_iova_domain(struct iova_domain *iovad);
+struct iova *split_and_remove_iova(struct iova_domain *iovad,
+	struct iova *iova, unsigned long pfn_lo, unsigned long pfn_hi);
 
 #endif

commit 3d39cecc4841e8d4c4abdb401d10180f5faaded0
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Wed Jul 8 15:23:30 2009 +0100

    intel-iommu: Remove superfluous iova_alloc_lock from IOVA code
    
    We only ever obtain this lock immediately before the iova_rbtree_lock,
    and release it immediately after the iova_rbtree_lock. So ditch it and
    just use iova_rbtree_lock.
    
    [v2: Remove the lockdep bits this time too]
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>

diff --git a/include/linux/iova.h b/include/linux/iova.h
index 228f6c94b69c..76a0759e88ec 100644
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@ -28,7 +28,6 @@ struct iova {
 
 /* holds all the iova translations for a domain */
 struct iova_domain {
-	spinlock_t	iova_alloc_lock;/* Lock to protect iova  allocation */
 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
 	struct rb_root	rbroot;		/* iova domain rbtree root */
 	struct rb_node	*cached32_node; /* Save last alloced node */

commit 387179464257921eb9aa3d15cc3ff194f6945a7c
Author: Kay, Allen M <allen.m.kay@intel.com>
Date:   Tue Sep 9 18:37:29 2008 +0300

    VT-d: Changes to support KVM
    
    This patch extends the VT-d driver to support KVM
    
    [Ben: fixed memory pinning]
    [avi: move dma_remapping.h as well]
    
    Signed-off-by: Kay, Allen M <allen.m.kay@intel.com>
    Signed-off-by: Weidong Han <weidong.han@intel.com>
    Signed-off-by: Ben-Ami Yassour <benami@il.ibm.com>
    Signed-off-by: Amit Shah <amit.shah@qumranet.com>
    Acked-by: Mark Gross <mgross@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/iova.h b/include/linux/iova.h
new file mode 100644
index 000000000000..228f6c94b69c
--- /dev/null
+++ b/include/linux/iova.h
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2006, Intel Corporation.
+ *
+ * This file is released under the GPLv2.
+ *
+ * Copyright (C) 2006-2008 Intel Corporation
+ * Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
+ *
+ */
+
+#ifndef _IOVA_H_
+#define _IOVA_H_
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/rbtree.h>
+#include <linux/dma-mapping.h>
+
+/* IO virtual address start page frame number */
+#define IOVA_START_PFN		(1)
+
+/* iova structure */
+struct iova {
+	struct rb_node	node;
+	unsigned long	pfn_hi; /* IOMMU dish out addr hi */
+	unsigned long	pfn_lo; /* IOMMU dish out addr lo */
+};
+
+/* holds all the iova translations for a domain */
+struct iova_domain {
+	spinlock_t	iova_alloc_lock;/* Lock to protect iova  allocation */
+	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
+	struct rb_root	rbroot;		/* iova domain rbtree root */
+	struct rb_node	*cached32_node; /* Save last alloced node */
+	unsigned long	dma_32bit_pfn;
+};
+
+struct iova *alloc_iova_mem(void);
+void free_iova_mem(struct iova *iova);
+void free_iova(struct iova_domain *iovad, unsigned long pfn);
+void __free_iova(struct iova_domain *iovad, struct iova *iova);
+struct iova *alloc_iova(struct iova_domain *iovad, unsigned long size,
+	unsigned long limit_pfn,
+	bool size_aligned);
+struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
+	unsigned long pfn_hi);
+void copy_reserved_iova(struct iova_domain *from, struct iova_domain *to);
+void init_iova_domain(struct iova_domain *iovad, unsigned long pfn_32bit);
+struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
+void put_iova_domain(struct iova_domain *iovad);
+
+#endif
