commit 31d8fcac00fcf4007f3921edc69ab4dcb3abcd4d
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jun 25 20:30:31 2020 -0700

    mm: workingset: age nonresident information alongside anonymous pages
    
    Patch series "fix for "mm: balance LRU lists based on relative
    thrashing" patchset"
    
    This patchset fixes some problems of the patchset, "mm: balance LRU
    lists based on relative thrashing", which is now merged on the mainline.
    
    Patch "mm: workingset: let cache workingset challenge anon fix" is the
    result of discussion with Johannes.  See following link.
    
      http://lkml.kernel.org/r/20200520232525.798933-6-hannes@cmpxchg.org
    
    And, the other two are minor things which are found when I try to rebase
    my patchset.
    
    This patch (of 3):
    
    After ("mm: workingset: let cache workingset challenge anon fix"), we
    compare refault distances to active_file + anon.  But age of the
    non-resident information is only driven by the file LRU.  As a result,
    we may overestimate the recency of any incoming refaults and activate
    them too eagerly, causing unnecessary LRU churn in certain situations.
    
    Make anon aging drive nonresident age as well to address that.
    
    Link: http://lkml.kernel.org/r/1592288204-27734-1-git-send-email-iamjoonsoo.kim@lge.com
    Link: http://lkml.kernel.org/r/1592288204-27734-2-git-send-email-iamjoonsoo.kim@lge.com
    Fixes: 34e58cac6d8f2a ("mm: workingset: let cache workingset challenge anon")
    Reported-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c4c37fd12104..f6f884970511 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -257,8 +257,8 @@ struct lruvec {
 	 */
 	unsigned long			anon_cost;
 	unsigned long			file_cost;
-	/* Evictions & activations on the inactive file list */
-	atomic_long_t			inactive_age;
+	/* Non-resident age, driven by LRU movement */
+	atomic_long_t			nonresident_age;
 	/* Refaults at the time of last reclaim cycle */
 	unsigned long			refaults;
 	/* Various lruvec state flags (enum lruvec_flags) */

commit 496df3d3ab8a407f83330fb8d7160a5f91898c55
Author: Ben Widawsky <ben.widawsky@intel.com>
Date:   Wed Jun 10 18:41:50 2020 -0700

    mm: add comments on pglist_data zones
    
    While making other modifications it was easy to confuse the two struct
    members node_zones and node_zonelists.  For those already familiar with
    the code, this might seem to be a silly patch, but it's quite helpful to
    disambiguate the similar-sounding fields
    
    While here, add a small comment on why nr_zones isn't simply MAX_NR_ZONES
    
    Signed-off-by: Ben Widawsky <ben.widawsky@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20200520205443.2757414-1-ben.widawsky@intel.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index df1f08486d81..c4c37fd12104 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -660,9 +660,21 @@ struct deferred_split {
  * per-zone basis.
  */
 typedef struct pglist_data {
+	/*
+	 * node_zones contains just the zones for THIS node. Not all of the
+	 * zones may be populated, but it is the full list. It is referenced by
+	 * this node's node_zonelists as well as other node's node_zonelists.
+	 */
 	struct zone node_zones[MAX_NR_ZONES];
+
+	/*
+	 * node_zonelists contains references to all zones in all nodes.
+	 * Generally the first zones will be references to this node's
+	 * node_zones.
+	 */
 	struct zonelist node_zonelists[MAX_ZONELISTS];
-	int nr_zones;
+
+	int nr_zones; /* number of populated zones in this node */
 #ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
 	struct page *node_mem_map;
 #ifdef CONFIG_PAGE_EXTENSION

commit ee01c4d72adffb7d424535adf630f2955748fa8b
Merge: c444eb564fb1 09587a09ada2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 20:24:15 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
     "More mm/ work, plenty more to come
    
      Subsystems affected by this patch series: slub, memcg, gup, kasan,
      pagealloc, hugetlb, vmscan, tools, mempolicy, memblock, hugetlbfs,
      thp, mmap, kconfig"
    
    * akpm: (131 commits)
      arm64: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      x86: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      riscv: support DEBUG_WX
      mm: add DEBUG_WX support
      drivers/base/memory.c: cache memory blocks in xarray to accelerate lookup
      mm/thp: rename pmd_mknotpresent() as pmd_mkinvalid()
      powerpc/mm: drop platform defined pmd_mknotpresent()
      mm: thp: don't need to drain lru cache when splitting and mlocking THP
      hugetlbfs: get unmapped area below TASK_UNMAPPED_BASE for hugetlbfs
      sparc32: register memory occupied by kernel as memblock.memory
      include/linux/memblock.h: fix minor typo and unclear comment
      mm, mempolicy: fix up gup usage in lookup_node
      tools/vm/page_owner_sort.c: filter out unneeded line
      mm: swap: memcg: fix memcg stats for huge pages
      mm: swap: fix vmstats for huge pages
      mm: vmscan: limit the range of LRU type balancing
      mm: vmscan: reclaim writepage is IO cost
      mm: vmscan: determine anon/file pressure balance at the reclaim root
      mm: balance LRU lists based on relative thrashing
      mm: only count actual rotations as LRU reclaim cost
      ...

commit 1431d4d11abb265e79cd44bed2f5ea93f1bcc57b
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jun 3 16:02:53 2020 -0700

    mm: base LRU balancing on an explicit cost model
    
    Currently, scan pressure between the anon and file LRU lists is balanced
    based on a mixture of reclaim efficiency and a somewhat vague notion of
    "value" of having certain pages in memory over others.  That concept of
    value is problematic, because it has caused us to count any event that
    remotely makes one LRU list more or less preferrable for reclaim, even
    when these events are not directly comparable and impose very different
    costs on the system.  One example is referenced file pages that we still
    deactivate and referenced anonymous pages that we actually rotate back to
    the head of the list.
    
    There is also conceptual overlap with the LRU algorithm itself.  By
    rotating recently used pages instead of reclaiming them, the algorithm
    already biases the applied scan pressure based on page value.  Thus, when
    rebalancing scan pressure due to rotations, we should think of reclaim
    cost, and leave assessing the page value to the LRU algorithm.
    
    Lastly, considering both value-increasing as well as value-decreasing
    events can sometimes cause the same type of event to be counted twice,
    i.e.  how rotating a page increases the LRU value, while reclaiming it
    succesfully decreases the value.  In itself this will balance out fine,
    but it quietly skews the impact of events that are only recorded once.
    
    The abstract metric of "value", the murky relationship with the LRU
    algorithm, and accounting both negative and positive events make the
    current pressure balancing model hard to reason about and modify.
    
    This patch switches to a balancing model of accounting the concrete,
    actually observed cost of reclaiming one LRU over another.  For now, that
    cost includes pages that are scanned but rotated back to the list head.
    Subsequent patches will add consideration for IO caused by refaulting of
    recently evicted pages.
    
    Replace struct zone_reclaim_stat with two cost counters in the lruvec, and
    make everything that affects cost go through a new lru_note_cost()
    function.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/20200520232525.798933-9-hannes@cmpxchg.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2f79ff4477ba..e57248ccb63d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -242,19 +242,6 @@ static inline bool is_active_lru(enum lru_list lru)
 	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
 }
 
-struct zone_reclaim_stat {
-	/*
-	 * The pageout code in vmscan.c keeps track of how many of the
-	 * mem/swap backed and file backed pages are referenced.
-	 * The higher the rotated/scanned ratio, the more valuable
-	 * that cache is.
-	 *
-	 * The anon LRU stats live in [0], file LRU stats in [1]
-	 */
-	unsigned long		recent_rotated[2];
-	unsigned long		recent_scanned[2];
-};
-
 enum lruvec_flags {
 	LRUVEC_CONGESTED,		/* lruvec has many dirty pages
 					 * backed by a congested BDI
@@ -263,7 +250,13 @@ enum lruvec_flags {
 
 struct lruvec {
 	struct list_head		lists[NR_LRU_LISTS];
-	struct zone_reclaim_stat	reclaim_stat;
+	/*
+	 * These track the cost of reclaiming one LRU - file or anon -
+	 * over the other. As the observed cost of reclaiming one LRU
+	 * increases, the reclaim scan balance tips toward the other.
+	 */
+	unsigned long			anon_cost;
+	unsigned long			file_cost;
 	/* Evictions & activations on the inactive file list */
 	atomic_long_t			inactive_age;
 	/* Refaults at the time of last reclaim cycle */

commit 3d060856adfc59afb9d029c233141334cfaba418
Author: Pavel Tatashin <pasha.tatashin@soleen.com>
Date:   Wed Jun 3 15:59:24 2020 -0700

    mm: initialize deferred pages with interrupts enabled
    
    Initializing struct pages is a long task and keeping interrupts disabled
    for the duration of this operation introduces a number of problems.
    
    1. jiffies are not updated for long period of time, and thus incorrect time
       is reported. See proposed solution and discussion here:
       lkml/20200311123848.118638-1-shile.zhang@linux.alibaba.com
    2. It prevents farther improving deferred page initialization by allowing
       intra-node multi-threading.
    
    We are keeping interrupts disabled to solve a rather theoretical problem
    that was never observed in real world (See 3a2d7fa8a3d5).
    
    Let's keep interrupts enabled. In case we ever encounter a scenario where
    an interrupt thread wants to allocate large amount of memory this early in
    boot we can deal with that by growing zone (see deferred_grow_zone()) by
    the needed amount before starting deferred_init_memmap() threads.
    
    Before:
    [    1.232459] node 0 initialised, 12058412 pages in 1ms
    
    After:
    [    1.632580] node 0 initialised, 12051227 pages in 436ms
    
    Fixes: 3a2d7fa8a3d5 ("mm: disable interrupts while initializing deferred pages")
    Reported-by: Shile Zhang <shile.zhang@linux.alibaba.com>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Yiqian Wei <yiwei@redhat.com>
    Cc: <stable@vger.kernel.org>    [4.17+]
    Link: http://lkml.kernel.org/r/20200403140952.17177-3-pasha.tatashin@soleen.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cd8bd5f90552..2f79ff4477ba 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -680,6 +680,8 @@ typedef struct pglist_data {
 	/*
 	 * Must be held any time you expect node_start_pfn,
 	 * node_present_pages, node_spanned_pages or nr_zones to stay constant.
+	 * Also synchronizes pgdat->first_deferred_pfn during deferred page
+	 * init.
 	 *
 	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
 	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG

commit 97a225e69a1f880886f33d2e65a7ace13f152caa
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Wed Jun 3 15:59:01 2020 -0700

    mm/page_alloc: integrate classzone_idx and high_zoneidx
    
    classzone_idx is just different name for high_zoneidx now.  So, integrate
    them and add some comment to struct alloc_context in order to reduce
    future confusion about the meaning of this variable.
    
    The accessor, ac_classzone_idx() is also removed since it isn't needed
    after integration.
    
    In addition to integration, this patch also renames high_zoneidx to
    highest_zoneidx since it represents more precise meaning.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Ye Xiaolong <xiaolong.ye@intel.com>
    Link: http://lkml.kernel.org/r/1587095923-7515-3-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0c575c3d7feb..cd8bd5f90552 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -699,13 +699,13 @@ typedef struct pglist_data {
 	struct task_struct *kswapd;	/* Protected by
 					   mem_hotplug_begin/end() */
 	int kswapd_order;
-	enum zone_type kswapd_classzone_idx;
+	enum zone_type kswapd_highest_zoneidx;
 
 	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */
 
 #ifdef CONFIG_COMPACTION
 	int kcompactd_max_order;
-	enum zone_type kcompactd_classzone_idx;
+	enum zone_type kcompactd_highest_zoneidx;
 	wait_queue_head_t kcompactd_wait;
 	struct task_struct *kcompactd;
 #endif
@@ -783,15 +783,15 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 
 void build_all_zonelists(pg_data_t *pgdat);
 void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
-		   enum zone_type classzone_idx);
+		   enum zone_type highest_zoneidx);
 bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
-			 int classzone_idx, unsigned int alloc_flags,
+			 int highest_zoneidx, unsigned int alloc_flags,
 			 long free_pages);
 bool zone_watermark_ok(struct zone *z, unsigned int order,
-		unsigned long mark, int classzone_idx,
+		unsigned long mark, int highest_zoneidx,
 		unsigned int alloc_flags);
 bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
-		unsigned long mark, int classzone_idx);
+		unsigned long mark, int highest_zoneidx);
 enum memmap_context {
 	MEMMAP_EARLY,
 	MEMMAP_HOTPLUG,

commit 3f08a302f533f74ad2e909e7a61274aa7eebc0ab
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:57:02 2020 -0700

    mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option
    
    CONFIG_HAVE_MEMBLOCK_NODE_MAP is used to differentiate initialization of
    nodes and zones structures between the systems that have region to node
    mapping in memblock and those that don't.
    
    Currently all the NUMA architectures enable this option and for the
    non-NUMA systems we can presume that all the memory belongs to node 0 and
    therefore the compile time configuration option is not required.
    
    The remaining few architectures that use DISCONTIGMEM without NUMA are
    easily updated to use memblock_add_node() instead of memblock_add() and
    thus have proper correspondence of memblock regions to NUMA nodes.
    
    Still, free_area_init_node() must have a backward compatible version
    because its semantics with and without CONFIG_HAVE_MEMBLOCK_NODE_MAP is
    different.  Once all the architectures will use the new semantics, the
    entire compatibility layer can be dropped.
    
    To avoid addition of extra run time memory to store node id for
    architectures that keep memblock but have only a single node, the node id
    field of the memblock_region is guarded by CONFIG_NEED_MULTIPLE_NODES and
    the corresponding accessors presume that in those cases it is always 0.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c3a77eb85b42..0c575c3d7feb 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -876,7 +876,7 @@ extern int movable_zone;
 #ifdef CONFIG_HIGHMEM
 static inline int zone_movable_is_highmem(void)
 {
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 	return movable_zone == ZONE_HIGHMEM;
 #else
 	return (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;

commit 6f24fbd38c4e05f7905814791806c01dc6c4b9de
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:56:57 2020 -0700

    mm: make early_pfn_to_nid() and related defintions close to each other
    
    early_pfn_to_nid() and its helper __early_pfn_to_nid() are spread around
    include/linux/mm.h, include/linux/mmzone.h and mm/page_alloc.c.
    
    Drop unused stub for __early_pfn_to_nid() and move its actual generic
    implementation close to its users.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fdd9beb5efed..c3a77eb85b42 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1080,15 +1080,6 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 #include <asm/sparsemem.h>
 #endif
 
-#if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) && \
-	!defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
-static inline unsigned long early_pfn_to_nid(unsigned long pfn)
-{
-	BUILD_BUG_ON(IS_ENABLED(CONFIG_NUMA));
-	return 0;
-}
-#endif
-
 #ifdef CONFIG_FLATMEM
 #define pfn_to_nid(pfn)		(0)
 #endif

commit cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2
Merge: 2e63f6ce7ed2 065fcfd49763
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 16:27:18 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
    
     1) Allow setting bluetooth L2CAP modes via socket option, from Luiz
        Augusto von Dentz.
    
     2) Add GSO partial support to igc, from Sasha Neftin.
    
     3) Several cleanups and improvements to r8169 from Heiner Kallweit.
    
     4) Add IF_OPER_TESTING link state and use it when ethtool triggers a
        device self-test. From Andrew Lunn.
    
     5) Start moving away from custom driver versions, use the globally
        defined kernel version instead, from Leon Romanovsky.
    
     6) Support GRO vis gro_cells in DSA layer, from Alexander Lobakin.
    
     7) Allow hard IRQ deferral during NAPI, from Eric Dumazet.
    
     8) Add sriov and vf support to hinic, from Luo bin.
    
     9) Support Media Redundancy Protocol (MRP) in the bridging code, from
        Horatiu Vultur.
    
    10) Support netmap in the nft_nat code, from Pablo Neira Ayuso.
    
    11) Allow UDPv6 encapsulation of ESP in the ipsec code, from Sabrina
        Dubroca. Also add ipv6 support for espintcp.
    
    12) Lots of ReST conversions of the networking documentation, from Mauro
        Carvalho Chehab.
    
    13) Support configuration of ethtool rxnfc flows in bcmgenet driver,
        from Doug Berger.
    
    14) Allow to dump cgroup id and filter by it in inet_diag code, from
        Dmitry Yakunin.
    
    15) Add infrastructure to export netlink attribute policies to
        userspace, from Johannes Berg.
    
    16) Several optimizations to sch_fq scheduler, from Eric Dumazet.
    
    17) Fallback to the default qdisc if qdisc init fails because otherwise
        a packet scheduler init failure will make a device inoperative. From
        Jesper Dangaard Brouer.
    
    18) Several RISCV bpf jit optimizations, from Luke Nelson.
    
    19) Correct the return type of the ->ndo_start_xmit() method in several
        drivers, it's netdev_tx_t but many drivers were using
        'int'. From Yunjian Wang.
    
    20) Add an ethtool interface for PHY master/slave config, from Oleksij
        Rempel.
    
    21) Add BPF iterators, from Yonghang Song.
    
    22) Add cable test infrastructure, including ethool interfaces, from
        Andrew Lunn. Marvell PHY driver is the first to support this
        facility.
    
    23) Remove zero-length arrays all over, from Gustavo A. R. Silva.
    
    24) Calculate and maintain an explicit frame size in XDP, from Jesper
        Dangaard Brouer.
    
    25) Add CAP_BPF, from Alexei Starovoitov.
    
    26) Support terse dumps in the packet scheduler, from Vlad Buslov.
    
    27) Support XDP_TX bulking in dpaa2 driver, from Ioana Ciornei.
    
    28) Add devm_register_netdev(), from Bartosz Golaszewski.
    
    29) Minimize qdisc resets, from Cong Wang.
    
    30) Get rid of kernel_getsockopt and kernel_setsockopt in order to
        eliminate set_fs/get_fs calls. From Christoph Hellwig.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2517 commits)
      selftests: net: ip_defrag: ignore EPERM
      net_failover: fixed rollback in net_failover_open()
      Revert "tipc: Fix potential tipc_aead refcnt leak in tipc_crypto_rcv"
      Revert "tipc: Fix potential tipc_node refcnt leak in tipc_rcv"
      vmxnet3: allow rx flow hash ops only when rss is enabled
      hinic: add set_channels ethtool_ops support
      selftests/bpf: Add a default $(CXX) value
      tools/bpf: Don't use $(COMPILE.c)
      bpf, selftests: Use bpf_probe_read_kernel
      s390/bpf: Use bcr 0,%0 as tail call nop filler
      s390/bpf: Maintain 8-byte stack alignment
      selftests/bpf: Fix verifier test
      selftests/bpf: Fix sample_cnt shared between two threads
      bpf, selftests: Adapt cls_redirect to call csum_level helper
      bpf: Add csum_level helper for fixing up csum levels
      bpf: Fix up bpf_skb_adjust_room helper's skb csum setting
      sfc: add missing annotation for efx_ef10_try_update_nic_stats_vf()
      crypto/chtls: IPv6 support for inline TLS
      Crypto/chcr: Fixes a coccinile check error
      Crypto/chcr: Fixes compilations warnings
      ...

commit 94709049fb8442fb2f7b91fbec3c2897a75e18df
Merge: 17839856fd58 4fba37586e4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 12:21:36 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
     "A few little subsystems and a start of a lot of MM patches.
    
      Subsystems affected by this patch series: squashfs, ocfs2, parisc,
      vfs. With mm subsystems: slab-generic, slub, debug, pagecache, gup,
      swap, memcg, pagemap, memory-failure, vmalloc, kasan"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (128 commits)
      kasan: move kasan_report() into report.c
      mm/mm_init.c: report kasan-tag information stored in page->flags
      ubsan: entirely disable alignment checks under UBSAN_TRAP
      kasan: fix clang compilation warning due to stack protector
      x86/mm: remove vmalloc faulting
      mm: remove vmalloc_sync_(un)mappings()
      x86/mm/32: implement arch_sync_kernel_mappings()
      x86/mm/64: implement arch_sync_kernel_mappings()
      mm/ioremap: track which page-table levels were modified
      mm/vmalloc: track which page-table levels were modified
      mm: add functions to track page directory modifications
      s390: use __vmalloc_node in stack_alloc
      powerpc: use __vmalloc_node in alloc_vm_stack
      arm64: use __vmalloc_node in arch_alloc_vmap_stack
      mm: remove vmalloc_user_node_flags
      mm: switch the test_vmalloc module to use __vmalloc_node
      mm: remove __vmalloc_node_flags_caller
      mm: remove both instances of __vmalloc_node_flags
      mm: remove the prot argument to __vmalloc_node
      mm: remove the pgprot argument to __vmalloc
      ...

commit 8d92890bd6b8502d6aee4b37430ae6444ade7a8c
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jun 1 21:48:21 2020 -0700

    mm/writeback: discard NR_UNSTABLE_NFS, use NR_WRITEBACK instead
    
    After an NFS page has been written it is considered "unstable" until a
    COMMIT request succeeds.  If the COMMIT fails, the page will be
    re-written.
    
    These "unstable" pages are currently accounted as "reclaimable", either
    in WB_RECLAIMABLE, or in NR_UNSTABLE_NFS which is included in a
    'reclaimable' count.  This might have made sense when sending the COMMIT
    required a separate action by the VFS/MM (e.g.  releasepage() used to
    send a COMMIT).  However now that all writes generated by ->writepages()
    will automatically be followed by a COMMIT (since commit 919e3bd9a875
    ("NFS: Ensure we commit after writeback is complete")) it makes more
    sense to treat them as writeback pages.
    
    So this patch removes NR_UNSTABLE_NFS and accounts unstable pages in
    NR_WRITEBACK and WB_WRITEBACK.
    
    A particular effect of this change is that when
    wb_check_background_flush() calls wb_over_bg_threshold(), the latter
    will report 'true' a lot less often as the 'unstable' pages are no
    longer considered 'dirty' (as there is nothing that writeback can do
    about them anyway).
    
    Currently wb_check_background_flush() will trigger writeback to NFS even
    when there are relatively few dirty pages (if there are lots of unstable
    pages), this can result in small writes going to the server (10s of
    Kilobytes rather than a Megabyte) which hurts throughput.  With this
    patch, there are fewer writes which are each larger on average.
    
    Where the NR_UNSTABLE_NFS count was included in statistics
    virtual-files, the entry is retained, but the value is hard-coded as
    zero.  static trace points and warning printks which mentioned this
    counter no longer report it.
    
    [akpm@linux-foundation.org: re-layout comment]
    [akpm@linux-foundation.org: fix printk warning]
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Trond Myklebust <trond.myklebust@hammerspace.com>
    Acked-by: Michal Hocko <mhocko@suse.com>        [mm]
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Link: http://lkml.kernel.org/r/87d06j7gqa.fsf@notabene.neil.brown.name
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1b9de7d220fb..a89f47515eb1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -193,7 +193,6 @@ enum node_stat_item {
 	NR_FILE_THPS,
 	NR_FILE_PMDMAPPED,
 	NR_ANON_THPS,
-	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_VMSCAN_WRITE,
 	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
 	NR_DIRTIED,		/* page dirtyings since bootup */

commit 628d06a48f57c36abdc2a024930212e654a501b7
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Apr 27 09:00:08 2020 -0700

    scs: Add page accounting for shadow call stack allocations
    
    This change adds accounting for the memory allocated for shadow stacks.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1b9de7d220fb..acffc3bc6178 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -156,6 +156,9 @@ enum zone_stat_item {
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_PAGETABLE,		/* used for pagetables */
 	NR_KERNEL_STACK_KB,	/* measured in KiB */
+#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)
+	NR_KERNEL_SCS_KB,	/* measured in KiB */
+#endif
 	/* Second 128 byte cacheline */
 	NR_BOUNCE,
 #if IS_ENABLED(CONFIG_ZSMALLOC)

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b2af594ef0f7..93cf20f41e26 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -910,22 +910,21 @@ static inline int is_highmem(struct zone *zone)
 /* These two functions are used to setup the per zone pages min values */
 struct ctl_table;
 
-int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
-int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
+int min_free_kbytes_sysctl_handler(struct ctl_table *, int, void *, size_t *,
+		loff_t *);
+int watermark_scale_factor_sysctl_handler(struct ctl_table *, int, void *,
+		size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];
-int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
+int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, void *,
+		size_t *, loff_t *);
 int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
+		void *, size_t *, loff_t *);
 int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
-			void __user *, size_t *, loff_t *);
+		void *, size_t *, loff_t *);
 int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
-			void __user *, size_t *, loff_t *);
-
-extern int numa_zonelist_order_handler(struct ctl_table *, int,
-			void __user *, size_t *, loff_t *);
+		void *, size_t *, loff_t *);
+int numa_zonelist_order_handler(struct ctl_table *, int,
+		void *, size_t *, loff_t *);
 extern int percpu_pagelist_fraction;
 extern char numa_zonelist_order[];
 #define NUMA_ZONELIST_ORDER_LEN	16

commit 2374c09b1c8a883bb9b4b2fc3756703eeb618f4a
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:36 2020 +0200

    sysctl: remove all extern declaration from sysctl.c
    
    Extern declarations in .c files are a bad style and can lead to
    mismatches.  Use existing definitions in headers where they exist,
    and otherwise move the external declarations to suitable header
    files.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f37bb8f187fc..b2af594ef0f7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -909,6 +909,7 @@ static inline int is_highmem(struct zone *zone)
 
 /* These two functions are used to setup the per zone pages min values */
 struct ctl_table;
+
 int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
@@ -925,6 +926,7 @@ int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
 
 extern int numa_zonelist_order_handler(struct ctl_table *, int,
 			void __user *, size_t *, loff_t *);
+extern int percpu_pagelist_fraction;
 extern char numa_zonelist_order[];
 #define NUMA_ZONELIST_ORDER_LEN	16
 

commit 26363af5643490a817272e1cc6f1d3f1d550a699
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:35 2020 +0200

    mm: remove watermark_boost_factor_sysctl_handler
    
    watermark_boost_factor_sysctl_handler is just a pointless wrapper for
    proc_dointvec_minmax, so remove it and use proc_dointvec_minmax
    directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1b9de7d220fb..f37bb8f187fc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -911,8 +911,6 @@ static inline int is_highmem(struct zone *zone)
 struct ctl_table;
 int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
-int watermark_boost_factor_sysctl_handler(struct ctl_table *, int,
-					void __user *, size_t *, loff_t *);
 int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];

commit 9b06860d7c1f1f4cb7d70f92e47dfa4a91bd5007
Merge: 0906d8b975ff f6d2b802f80d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 8 21:03:40 2020 -0700

    Merge tag 'libnvdimm-for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm and dax updates from Dan Williams:
     "There were multiple touches outside of drivers/nvdimm/ this round to
      add cross arch compatibility to the devm_memremap_pages() interface,
      enhance numa information for persistent memory ranges, and add a
      zero_page_range() dax operation.
    
      This cycle I switched from the patchwork api to Konstantin's b4 script
      for collecting tags (from x86, PowerPC, filesystem, and device-mapper
      folks), and everything looks to have gone ok there. This has all
      appeared in -next with no reported issues.
    
      Summary:
    
       - Add support for region alignment configuration and enforcement to
         fix compatibility across architectures and PowerPC page size
         configurations.
    
       - Introduce 'zero_page_range' as a dax operation. This facilitates
         filesystem-dax operation without a block-device.
    
       - Introduce phys_to_target_node() to facilitate drivers that want to
         know resulting numa node if a given reserved address range was
         onlined.
    
       - Advertise a persistence-domain for of_pmem and papr_scm. The
         persistence domain indicates where cpu-store cycles need to reach
         in the platform-memory subsystem before the platform will consider
         them power-fail protected.
    
       - Promote numa_map_to_online_node() to a cross-kernel generic
         facility.
    
       - Save x86 numa information to allow for node-id lookups for reserved
         memory ranges, deploy that capability for the e820-pmem driver.
    
       - Pick up some miscellaneous minor fixes, that missed v5.6-final,
         including a some smatch reports in the ioctl path and some unit
         test compilation fixups.
    
       - Fixup some flexible-array declarations"
    
    * tag 'libnvdimm-for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (29 commits)
      dax: Move mandatory ->zero_page_range() check in alloc_dax()
      dax,iomap: Add helper dax_iomap_zero() to zero a range
      dax: Use new dax zero page method for zeroing a page
      dm,dax: Add dax zero_page_range operation
      s390,dcssblk,dax: Add dax zero_page_range operation to dcssblk driver
      dax, pmem: Add a dax operation zero_page_range
      pmem: Add functions for reading/writing page to/from pmem
      libnvdimm: Update persistence domain value for of_pmem and papr_scm device
      tools/test/nvdimm: Fix out of tree build
      libnvdimm/region: Fix build error
      libnvdimm/region: Replace zero-length array with flexible-array member
      libnvdimm/label: Replace zero-length array with flexible-array member
      ACPI: NFIT: Replace zero-length array with flexible-array member
      libnvdimm/region: Introduce an 'align' attribute
      libnvdimm/region: Introduce NDD_LABELING
      libnvdimm/namespace: Enforce memremap_compat_align()
      libnvdimm/pfn: Prevent raw mode fallback if pfn-infoblock valid
      libnvdimm: Out of bounds read in __nd_ioctl()
      acpi/nfit: improve bounds checking for 'func'
      mm/memremap_pages: Introduce memremap_compat_align()
      ...

commit 6218d740ac1bc723d57900b865d3e52b83550c2b
Author: Waiman Long <longman@redhat.com>
Date:   Mon Apr 6 20:08:52 2020 -0700

    mm: remove dummy struct bootmem_data/bootmem_data_t
    
    Both bootmem_data and bootmem_data_t structures are no longer defined.
    Remove the dummy forward declarations.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Link: http://lkml.kernel.org/r/20200326022617.26208-1-longman@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f3f264826423..e9892bf9eba9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -664,7 +664,6 @@ struct deferred_split {
  * Memory statistics and page replacement data structures are maintained on a
  * per-zone basis.
  */
-struct bootmem_data;
 typedef struct pglist_data {
 	struct zone node_zones[MAX_NR_ZONES];
 	struct zonelist node_zonelists[MAX_ZONELISTS];

commit 0a9f9f62316606ee827fa3318e95a1c489d9acf5
Author: Baoquan He <bhe@redhat.com>
Date:   Mon Apr 6 20:07:06 2020 -0700

    mm/sparse.c: only use subsection map in VMEMMAP case
    
    Currently, to support subsection aligned memory region adding for pmem,
    subsection map is added to track which subsection is present.
    
    However, config ZONE_DEVICE depends on SPARSEMEM_VMEMMAP.  It means
    subsection map only makes sense when SPARSEMEM_VMEMMAP enabled.  For the
    classic sparse, it's meaningless.  Even worse, it may confuse people when
    checking code related to the classic sparse.
    
    About the classic sparse which doesn't support subsection hotplug, Dan
    said it's more because the effort and maintenance burden outweighs the
    benefit.  Besides, the current 64 bit ARCHes all enable
    SPARSEMEM_VMEMMAP_ENABLE by default.
    
    Combining the above reasons, no need to provide subsection map and the
    relevant handling for the classic sparse.  Let's remove them.
    
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Link: http://lkml.kernel.org/r/20200312124414.439-4-bhe@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 42b77d3b68e8..f3f264826423 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1143,7 +1143,9 @@ static inline unsigned long section_nr_to_pfn(unsigned long sec)
 #define SUBSECTION_ALIGN_DOWN(pfn) ((pfn) & PAGE_SUBSECTION_MASK)
 
 struct mem_section_usage {
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
 	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);
+#endif
 	/* See declaration of similar field in struct zone */
 	unsigned long pageblock_flags[0];
 };

commit 6ab0136310961ebf4b5ecb565f0bf52c233dc093
Author: Alexander Duyck <alexander.h.duyck@linux.intel.com>
Date:   Mon Apr 6 20:04:49 2020 -0700

    mm: use zone and order instead of free area in free_list manipulators
    
    In order to enable the use of the zone from the list manipulator functions
    I will need access to the zone pointer.  As it turns out most of the
    accessors were always just being directly passed &zone->free_area[order]
    anyway so it would make sense to just fold that into the function itself
    and pass the zone and order as arguments instead of the free area.
    
    In order to be able to reference the zone we need to move the declaration
    of the functions down so that we have the zone defined before we define
    the list manipulation functions.  Since the functions are only used in the
    file mm/page_alloc.c we can just move them there to reduce noise in the
    header.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pankaj Gupta <pagupta@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wei Wang <wei.w.wang@intel.com>
    Cc: Yang Zhang <yang.zhang.wz@gmail.com>
    Cc: wei qi <weiqi4@huawei.com>
    Link: http://lkml.kernel.org/r/20200211224613.29318.43080.stgit@localhost.localdomain
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c023d7968b14..42b77d3b68e8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -100,29 +100,6 @@ struct free_area {
 	unsigned long		nr_free;
 };
 
-/* Used for pages not on another list */
-static inline void add_to_free_area(struct page *page, struct free_area *area,
-			     int migratetype)
-{
-	list_add(&page->lru, &area->free_list[migratetype]);
-	area->nr_free++;
-}
-
-/* Used for pages not on another list */
-static inline void add_to_free_area_tail(struct page *page, struct free_area *area,
-				  int migratetype)
-{
-	list_add_tail(&page->lru, &area->free_list[migratetype]);
-	area->nr_free++;
-}
-
-/* Used for pages which are on another list */
-static inline void move_to_free_area(struct page *page, struct free_area *area,
-			     int migratetype)
-{
-	list_move(&page->lru, &area->free_list[migratetype]);
-}
-
 static inline struct page *get_page_from_free_area(struct free_area *area,
 					    int migratetype)
 {
@@ -130,15 +107,6 @@ static inline struct page *get_page_from_free_area(struct free_area *area,
 					struct page, lru);
 }
 
-static inline void del_page_from_free_area(struct page *page,
-		struct free_area *area)
-{
-	list_del(&page->lru);
-	__ClearPageBuddy(page);
-	set_page_private(page, 0);
-	area->nr_free--;
-}
-
 static inline bool free_area_empty(struct free_area *area, int migratetype)
 {
 	return list_empty(&area->free_list[migratetype]);

commit a2129f24798a993abde9b4bf8b3713b52d56c121
Author: Alexander Duyck <alexander.h.duyck@linux.intel.com>
Date:   Mon Apr 6 20:04:45 2020 -0700

    mm: adjust shuffle code to allow for future coalescing
    
    Patch series "mm / virtio: Provide support for free page reporting", v17.
    
    This series provides an asynchronous means of reporting free guest pages
    to a hypervisor so that the memory associated with those pages can be
    dropped and reused by other processes and/or guests on the host.  Using
    this it is possible to avoid unnecessary I/O to disk and greatly improve
    performance in the case of memory overcommit on the host.
    
    When enabled we will be performing a scan of free memory every 2 seconds
    while pages of sufficiently high order are being freed.  In each pass at
    least one sixteenth of each free list will be reported.  By doing this we
    avoid racing against other threads that may be causing a high amount of
    memory churn.
    
    The lowest page order currently scanned when reporting pages is
    pageblock_order so that this feature will not interfere with the use of
    Transparent Huge Pages in the case of virtualization.
    
    Currently this is only in use by virtio-balloon however there is the hope
    that at some point in the future other hypervisors might be able to make
    use of it.  In the virtio-balloon/QEMU implementation the hypervisor is
    currently using MADV_DONTNEED to indicate to the host kernel that the page
    is currently free.  It will be zeroed and faulted back into the guest the
    next time the page is accessed.
    
    To track if a page is reported or not the Uptodate flag was repurposed and
    used as a Reported flag for Buddy pages.  We walk though the free list
    isolating pages and adding them to the scatterlist until we either
    encounter the end of the list or have processed at least one sixteenth of
    the pages that were listed in nr_free prior to us starting.  If we fill
    the scatterlist before we reach the end of the list we rotate the list so
    that the first unreported page we encounter is moved to the head of the
    list as that is where we will resume after we have freed the reported
    pages back into the tail of the list.
    
    Below are the results from various benchmarks.  I primarily focused on two
    tests.  The first is the will-it-scale/page_fault2 test, and the other is
    a modified version of will-it-scale/page_fault1 that was enabled to use
    THP.  I did this as it allows for better visibility into different parts
    of the memory subsystem.  The guest is running with 32G for RAM on one
    node of a E5-2630 v3.  The host has had some features such as CPU turbo
    disabled in the BIOS.
    
    Test                   page_fault1 (THP)    page_fault2
    Name            tasks  Process Iter  STDEV  Process Iter  STDEV
    Baseline            1    1012402.50  0.14%     361855.25  0.81%
                       16    8827457.25  0.09%    3282347.00  0.34%
    
    Patches Applied     1    1007897.00  0.23%     361887.00  0.26%
                       16    8784741.75  0.39%    3240669.25  0.48%
    
    Patches Enabled     1    1010227.50  0.39%     359749.25  0.56%
                       16    8756219.00  0.24%    3226608.75  0.97%
    
    Patches Enabled     1    1050982.00  4.26%     357966.25  0.14%
     page shuffle      16    8672601.25  0.49%    3223177.75  0.40%
    
    Patches enabled     1    1003238.00  0.22%     360211.00  0.22%
     shuffle w/ RFC    16    8767010.50  0.32%    3199874.00  0.71%
    
    The results above are for a baseline with a linux-next-20191219 kernel,
    that kernel with this patch set applied but page reporting disabled in
    virtio-balloon, the patches applied and page reporting fully enabled, the
    patches enabled with page shuffling enabled, and the patches applied with
    page shuffling enabled and an RFC patch that makes used of MADV_FREE in
    QEMU.  These results include the deviation seen between the average value
    reported here versus the high and/or low value.  I observed that during
    the test memory usage for the first three tests never dropped whereas with
    the patches fully enabled the VM would drop to using only a few GB of the
    host's memory when switching from memhog to page fault tests.
    
    Any of the overhead visible with this patch set enabled seems due to page
    faults caused by accessing the reported pages and the host zeroing the
    page before giving it back to the guest.  This overhead is much more
    visible when using THP than with standard 4K pages.  In addition page
    shuffling seemed to increase the amount of faults generated due to an
    increase in memory churn.  The overehad is reduced when using MADV_FREE as
    we can avoid the extra zeroing of the pages when they are reintroduced to
    the host, as can be seen when the RFC is applied with shuffling enabled.
    
    The overall guest size is kept fairly small to only a few GB while the
    test is running.  If the host memory were oversubscribed this patch set
    should result in a performance improvement as swapping memory in the host
    can be avoided.
    
    A brief history on the background of free page reporting can be found at:
    https://lore.kernel.org/lkml/29f43d5796feed0dec8e8bb98b187d9dac03b900.camel@linux.intel.com/
    
    This patch (of 9):
    
    Move the head/tail adding logic out of the shuffle code and into the
    __free_one_page function since ultimately that is where it is really
    needed anyway.  By doing this we should be able to reduce the overhead and
    can consolidate all of the list addition bits in one spot.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: David Hildenbrand <david@redhat.com>
    Cc: Yang Zhang <yang.zhang.wz@gmail.com>
    Cc: Pankaj Gupta <pagupta@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Wei Wang <wei.w.wang@intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: wei qi <weiqi4@huawei.com>
    Link: http://lkml.kernel.org/r/20200211224602.29318.84523.stgit@localhost.localdomain
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e84d448988b6..c023d7968b14 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -116,18 +116,6 @@ static inline void add_to_free_area_tail(struct page *page, struct free_area *ar
 	area->nr_free++;
 }
 
-#ifdef CONFIG_SHUFFLE_PAGE_ALLOCATOR
-/* Used to preserve page allocation order entropy */
-void add_to_free_area_random(struct page *page, struct free_area *area,
-		int migratetype);
-#else
-static inline void add_to_free_area_random(struct page *page,
-		struct free_area *area, int migratetype)
-{
-	add_to_free_area(page, area, migratetype);
-}
-#endif
-
 /* Used for pages which are on another list */
 static inline void move_to_free_area(struct page *page, struct free_area *area,
 			     int migratetype)

commit e03d1f78341e8a16f6cb5be5dfcd37ddc31a6839
Author: Pingfan Liu <kernelfans@gmail.com>
Date:   Wed Apr 1 21:09:27 2020 -0700

    mm/sparse: rename pfn_present() to pfn_in_present_section()
    
    After introducing mem sub section concept, pfn_present() loses its literal
    meaning, and will not be necessary a truth on partial populated mem
    section.
    
    Since all of the callers use it to judge an absent section, it is better
    to rename pfn_present() as pfn_in_present_section().
    
    Signed-off-by: Pingfan Liu <kernelfans@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>         [powerpc]
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Leonardo Bras <leonardo@linux.ibm.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Cc: Nathan Lynch <nathanl@linux.ibm.com>
    Link: http://lkml.kernel.org/r/1581919110-29575-1-git-send-email-kernelfans@gmail.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4bca42eeb439..e84d448988b6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1374,7 +1374,7 @@ static inline int pfn_valid(unsigned long pfn)
 }
 #endif
 
-static inline int pfn_present(unsigned long pfn)
+static inline int pfn_in_present_section(unsigned long pfn)
 {
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
 		return 0;
@@ -1411,7 +1411,7 @@ void sparse_init(void);
 #else
 #define sparse_init()	do {} while (0)
 #define sparse_index_init(_sec, _nid)  do {} while (0)
-#define pfn_present pfn_valid
+#define pfn_in_present_section pfn_valid
 #define subsection_map_init(_pfn, _nr_pages) do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 

commit 1970dc6f5226416957ad0cc70ab47386ed3195a6
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Wed Apr 1 21:05:37 2020 -0700

    mm/gup: /proc/vmstat: pin_user_pages (FOLL_PIN) reporting
    
    Now that pages are "DMA-pinned" via pin_user_page*(), and unpinned via
    unpin_user_pages*(), we need some visibility into whether all of this is
    working correctly.
    
    Add two new fields to /proc/vmstat:
    
        nr_foll_pin_acquired
        nr_foll_pin_released
    
    These are documented in Documentation/core-api/pin_user_pages.rst.  They
    represent the number of pages (since boot time) that have been pinned
    ("nr_foll_pin_acquired") and unpinned ("nr_foll_pin_released"), via
    pin_user_pages*() and unpin_user_pages*().
    
    In the absence of long-running DMA or RDMA operations that hold pages
    pinned, the above two fields will normally be equal to each other.
    
    Also: update Documentation/core-api/pin_user_pages.rst, to remove an
    earlier (now confirmed untrue) claim about a performance problem with
    /proc/vmstat.
    
    Also: update Documentation/core-api/pin_user_pages.rst to rename the new
    /proc/vmstat entries, to the names listed here.
    
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/20200211001536.1027652-9-jhubbard@nvidia.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 462f6873905a..4bca42eeb439 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -243,6 +243,8 @@ enum node_stat_item {
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
 	NR_KERNEL_MISC_RECLAIMABLE,	/* reclaimable non-slab kernel pages */
+	NR_FOLL_PIN_ACQUIRED,	/* via: pin_user_page(), gup flag: FOLL_PIN */
+	NR_FOLL_PIN_RELEASED,	/* pages returned via unpin_user_page() */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit 9ffc1d19fc4a6dfcfe06c91c2861ad6d44fdd92d
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jan 30 12:06:07 2020 -0800

    mm/memremap_pages: Introduce memremap_compat_align()
    
    The "sub-section memory hotplug" facility allows memremap_pages() users
    like libnvdimm to compensate for hardware platforms like x86 that have a
    section size larger than their hardware memory mapping granularity.  The
    compensation that sub-section support affords is being tolerant of
    physical memory resources shifting by units smaller (64MiB on x86) than
    the memory-hotplug section size (128 MiB). Where the platform
    physical-memory mapping granularity is limited by the number and
    capability of address-decode-registers in the memory controller.
    
    While the sub-section support allows memremap_pages() to operate on
    sub-section (2MiB) granularity, the Power architecture may still
    require 16MiB alignment on "!radix_enabled()" platforms.
    
    In order for libnvdimm to be able to detect and manage this per-arch
    limitation, introduce memremap_compat_align() as a common minimum
    alignment across all driver-facing memory-mapping interfaces, and let
    Power override it to 16MiB in the "!radix_enabled()" case.
    
    The assumption / requirement for 16MiB to be a viable
    memremap_compat_align() value is that Power does not have platforms
    where its equivalent of address-decode-registers never hardware remaps a
    persistent memory resource on smaller than 16MiB boundaries. Note that I
    tried my best to not add a new Kconfig symbol, but header include
    entanglements defeated the #ifndef memremap_compat_align design pattern
    and the need to export it defeats the __weak design pattern for arch
    overrides.
    
    Based on an initial patch by Aneesh.
    
    Link: http://lore.kernel.org/r/CAPcyv4gBGNP95APYaBcsocEa50tQj9b5h__83vgngjq3ouGX_Q@mail.gmail.com
    Reported-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reported-by: Jeff Moyer <jmoyer@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 462f6873905a..6b77f7239af5 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1170,6 +1170,7 @@ static inline unsigned long section_nr_to_pfn(unsigned long sec)
 #define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)
 
 #define SUBSECTION_SHIFT 21
+#define SUBSECTION_SIZE (1UL << SUBSECTION_SHIFT)
 
 #define PFN_SUBSECTION_SHIFT (SUBSECTION_SHIFT - PAGE_SHIFT)
 #define PAGES_PER_SUBSECTION (1UL << PFN_SUBSECTION_SHIFT)

commit 4c6058814ec4460c25111e29452ef596acdcd61b
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Feb 3 17:34:02 2020 -0800

    mm: factor out next_present_section_nr()
    
    Let's move it to the header and use the shorter variant from
    mm/page_alloc.c (the original one will also check
    "__highest_present_section_nr + 1", which is not necessary).  While at
    it, make the section_nr in next_pfn() const.
    
    In next_pfn(), we now return section_nr_to_pfn(-1) instead of -1 once we
    exceed __highest_present_section_nr, which doesn't make a difference in
    the caller as it is big enough (>= all sane end_pfn).
    
    Link: http://lkml.kernel.org/r/20200113144035.10848-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: "Jin, Zhi" <zhi.jin@intel.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c2bc309d1634..462f6873905a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1379,6 +1379,16 @@ static inline int pfn_present(unsigned long pfn)
 	return present_section(__nr_to_section(pfn_to_section_nr(pfn)));
 }
 
+static inline unsigned long next_present_section_nr(unsigned long section_nr)
+{
+	while (++section_nr <= __highest_present_section_nr) {
+		if (present_section_nr(section_nr))
+			return section_nr;
+	}
+
+	return -1;
+}
+
 /*
  * These are _only_ used during initialisation, therefore they
  * can use __initdata ...  They could have names to indicate

commit 0a3c57729768e08de690ba872dd52ee9c3c11e8b
Author: Hao Lee <haolee.swjtu@gmail.com>
Date:   Thu Jan 30 22:15:19 2020 -0800

    mm: fix comments related to node reclaim
    
    As zone reclaim has been replaced by node reclaim, this patch fixes
    related comments.
    
    Link: http://lkml.kernel.org/r/20191126141346.GA22665@haolee.github.io
    Signed-off-by: Hao Lee <haolee.swjtu@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5334ad8fc7bd..c2bc309d1634 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -758,7 +758,7 @@ typedef struct pglist_data {
 
 #ifdef CONFIG_NUMA
 	/*
-	 * zone reclaim becomes active if more unmapped pages exist.
+	 * node reclaim becomes active if more unmapped pages exist.
 	 */
 	unsigned long		min_unmapped_pages;
 	unsigned long		min_slab_pages;

commit 4a87e2a25dc27131c3cce5e94421622193305638
Author: Roman Gushchin <guro@fb.com>
Date:   Mon Jan 13 16:29:16 2020 -0800

    mm: memcg/slab: fix percpu slab vmstats flushing
    
    Currently slab percpu vmstats are flushed twice: during the memcg
    offlining and just before freeing the memcg structure.  Each time percpu
    counters are summed, added to the atomic counterparts and propagated up
    by the cgroup tree.
    
    The second flushing is required due to how recursive vmstats are
    implemented: counters are batched in percpu variables on a local level,
    and once a percpu value is crossing some predefined threshold, it spills
    over to atomic values on the local and each ascendant levels.  It means
    that without flushing some numbers cached in percpu variables will be
    dropped on floor each time a cgroup is destroyed.  And with uptime the
    error on upper levels might become noticeable.
    
    The first flushing aims to make counters on ancestor levels more
    precise.  Dying cgroups may resume in the dying state for a long time.
    After kmem_cache reparenting which is performed during the offlining
    slab counters of the dying cgroup don't have any chances to be updated,
    because any slab operations will be performed on the parent level.  It
    means that the inaccuracy caused by percpu batching will not decrease up
    to the final destruction of the cgroup.  By the original idea flushing
    slab counters during the offlining should minimize the visible
    inaccuracy of slab counters on the parent level.
    
    The problem is that percpu counters are not zeroed after the first
    flushing.  So every cached percpu value is summed twice.  It creates a
    small error (up to 32 pages per cpu, but usually less) which accumulates
    on parent cgroup level.  After creating and destroying of thousands of
    child cgroups, slab counter on parent level can be way off the real
    value.
    
    For now, let's just stop flushing slab counters on memcg offlining.  It
    can't be done correctly without scheduling a work on each cpu: reading
    and zeroing it during css offlining can race with an asynchronous
    update, which doesn't expect values to be changed underneath.
    
    With this change, slab counters on parent level will become eventually
    consistent.  Once all dying children are gone, values are correct.  And
    if not, the error is capped by 32 * NR_CPUS pages per dying cgroup.
    
    It's not perfect, as slab are reparented, so any updates after the
    reparenting will happen on the parent level.  It means that if a slab
    page was allocated, a counter on child level was bumped, then the page
    was reparented and freed, the annihilation of positive and negative
    counter values will not happen until the child cgroup is released.  It
    makes slab counters different from others, and it might want us to
    implement flushing in a correct form again.  But it's also a question of
    performance: scheduling a work on each cpu isn't free, and it's an open
    question if the benefit of having more accurate counters is worth it.
    
    We might also consider flushing all counters on offlining, not only slab
    counters.
    
    So let's fix the main problem now: make the slab counters eventually
    consistent, so at least the error won't grow with uptime (or more
    precisely the number of created and destroyed cgroups).  And think about
    the accuracy of counters separately.
    
    Link: http://lkml.kernel.org/r/20191220042728.1045881-1-guro@fb.com
    Fixes: bee07b33db78 ("mm: memcontrol: flush percpu slab vmstats on kmem offlining")
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 89d8ff06c9ce..5334ad8fc7bd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -215,9 +215,8 @@ enum node_stat_item {
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
-	NR_SLAB_RECLAIMABLE,	/* Please do not reorder this item */
-	NR_SLAB_UNRECLAIMABLE,	/* and this one without looking at
-				 * memcg_flush_percpu_vmstats() first. */
+	NR_SLAB_RECLAIMABLE,
+	NR_SLAB_UNRECLAIMABLE,
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	WORKINGSET_NODES,

commit 84218b552e0a591ac706a926d5e1e8eaf0d5a03a
Author: Hao Lee <haolee.swjtu@gmail.com>
Date:   Sat Nov 30 17:58:14 2019 -0800

    mm: fix struct member name in function comments
    
    The member in struct zonelist is _zonerefs instead of zones.
    
    Link: http://lkml.kernel.org/r/20190927144049.GA29622@haolee.github.io
    Signed-off-by: Hao Lee <haolee.swjtu@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richardw.yang@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d9e62b0b584e..89d8ff06c9ce 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1085,7 +1085,7 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 /**
  * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
  * @zone - The current zone in the iterator
- * @z - The current pointer within zonelist->zones being iterated
+ * @z - The current pointer within zonelist->_zonerefs being iterated
  * @zlist - The zonelist being iterated
  * @highidx - The zone index of the highest zone to return
  * @nodemask - Nodemask allowed by the allocator

commit b91ac374346ba206cfd568bb0ab830af6b205cfd
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Sat Nov 30 17:56:02 2019 -0800

    mm: vmscan: enforce inactive:active ratio at the reclaim root
    
    We split the LRU lists into inactive and an active parts to maximize
    workingset protection while allowing just enough inactive cache space to
    faciltate readahead and writeback for one-off file accesses (e.g.  a
    linear scan through a file, or logging); or just enough inactive anon to
    maintain recent reference information when reclaim needs to swap.
    
    With cgroups and their nested LRU lists, we currently don't do this
    correctly.  While recursive cgroup reclaim establishes a relative LRU
    order among the pages of all involved cgroups, inactive:active size
    decisions are done on a per-cgroup level.  As a result, we'll reclaim a
    cgroup's workingset when it doesn't have cold pages, even when one of its
    siblings has plenty of it that should be reclaimed first.
    
    For example: workload A has 50M worth of hot cache but doesn't do any
    one-off file accesses; meanwhile, parallel workload B scans files and
    rarely accesses the same page twice.
    
    If these workloads were to run in an uncgrouped system, A would be
    protected from the high rate of cache faults from B.  But if they were put
    in parallel cgroups for memory accounting purposes, B's fast cache fault
    rate would push out the hot cache pages of A.  This is unexpected and
    undesirable - the "scan resistance" of the page cache is broken.
    
    This patch moves inactive:active size balancing decisions to the root of
    reclaim - the same level where the LRU order is established.
    
    It does this by looking at the recursive size of the inactive and the
    active file sets of the cgroup subtree at the beginning of the reclaim
    cycle, and then making a decision - scan or skip active pages - that
    applies throughout the entire run and to every cgroup involved.
    
    With that in place, in the test above, the VM will recognize that there
    are plenty of inactive pages in the combined cache set of workloads A and
    B and prefer the one-off cache in B over the hot pages in A.  The scan
    resistance of the cache is restored.
    
    [cai@lca.pw: fix some -Wenum-conversion warnings]
      Link: http://lkml.kernel.org/r/1573848697-29262-1-git-send-email-cai@lca.pw
    Link: http://lkml.kernel.org/r/20191107205334.158354-4-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Suren Baghdasaryan <surenb@google.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ddee00e91a22..d9e62b0b584e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -273,12 +273,12 @@ enum lru_list {
 
 #define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)
 
-static inline int is_file_lru(enum lru_list lru)
+static inline bool is_file_lru(enum lru_list lru)
 {
 	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
 }
 
-static inline int is_active_lru(enum lru_list lru)
+static inline bool is_active_lru(enum lru_list lru)
 {
 	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
 }

commit 1b05117df78e035afb5f66ef50bf8750d976ef08
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Sat Nov 30 17:55:52 2019 -0800

    mm: vmscan: harmonize writeback congestion tracking for nodes & memcgs
    
    The current writeback congestion tracking has separate flags for kswapd
    reclaim (node level) and cgroup limit reclaim (memcg-node level).  This is
    unnecessarily complicated: the lruvec is an existing abstraction layer for
    that node-memcg intersection.
    
    Introduce lruvec->flags and LRUVEC_CONGESTED.  Then track that at the
    reclaim root level, which is either the NUMA node for global reclaim, or
    the cgroup-node intersection for cgroup reclaim.
    
    Link: http://lkml.kernel.org/r/20191022144803.302233-9-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Roman Gushchin <guro@fb.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cc8232a100bd..ddee00e91a22 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -296,6 +296,12 @@ struct zone_reclaim_stat {
 	unsigned long		recent_scanned[2];
 };
 
+enum lruvec_flags {
+	LRUVEC_CONGESTED,		/* lruvec has many dirty pages
+					 * backed by a congested BDI
+					 */
+};
+
 struct lruvec {
 	struct list_head		lists[NR_LRU_LISTS];
 	struct zone_reclaim_stat	reclaim_stat;
@@ -303,6 +309,8 @@ struct lruvec {
 	atomic_long_t			inactive_age;
 	/* Refaults at the time of last reclaim cycle */
 	unsigned long			refaults;
+	/* Various lruvec state flags (enum lruvec_flags) */
+	unsigned long			flags;
 #ifdef CONFIG_MEMCG
 	struct pglist_data *pgdat;
 #endif
@@ -572,9 +580,6 @@ struct zone {
 } ____cacheline_internodealigned_in_smp;
 
 enum pgdat_flags {
-	PGDAT_CONGESTED,		/* pgdat has many dirty pages backed by
-					 * a congested BDI
-					 */
 	PGDAT_DIRTY,			/* reclaim scanning has recently found
 					 * many dirty file pages at the tail
 					 * of the LRU.

commit 867e5e1de14b2b2bde324cdfeec3f3f83eb21424
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Sat Nov 30 17:55:34 2019 -0800

    mm: clean up and clarify lruvec lookup procedure
    
    There is a per-memcg lruvec and a NUMA node lruvec.  Which one is being
    used is somewhat confusing right now, and it's easy to make mistakes -
    especially when it comes to global reclaim.
    
    How it works: when memory cgroups are enabled, we always use the
    root_mem_cgroup's per-node lruvecs.  When memory cgroups are not compiled
    in or disabled at runtime, we use pgdat->lruvec.
    
    Document that in a comment.
    
    Due to the way the reclaim code is generalized, all lookups use the
    mem_cgroup_lruvec() helper function, and nobody should have to find the
    right lruvec manually right now.  But to avoid future mistakes, rename the
    pgdat->lruvec member to pgdat->__lruvec and delete the convenience wrapper
    that suggests it's a commonly accessed member.
    
    While in this area, swap the mem_cgroup_lruvec() argument order.  The name
    suggests a memcg operation, yet it takes a pgdat first and a memcg second.
    I have to double take every time I call this.  Fix that.
    
    Link: http://lkml.kernel.org/r/20191022144803.302233-3-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Cc: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c7fb21f19edd..cc8232a100bd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -777,7 +777,13 @@ typedef struct pglist_data {
 #endif
 
 	/* Fields commonly accessed by the page reclaim scanner */
-	struct lruvec		lruvec;
+
+	/*
+	 * NOTE: THIS IS UNUSED IF MEMCG IS ENABLED.
+	 *
+	 * Use mem_cgroup_lruvec() to look up lruvecs.
+	 */
+	struct lruvec		__lruvec;
 
 	unsigned long		flags;
 
@@ -800,11 +806,6 @@ typedef struct pglist_data {
 #define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)
 #define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))
 
-static inline struct lruvec *node_lruvec(struct pglist_data *pgdat)
-{
-	return &pgdat->lruvec;
-}
-
 static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)
 {
 	return pgdat->node_start_pfn + pgdat->node_spanned_pages;
@@ -842,7 +843,7 @@ static inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
 #ifdef CONFIG_MEMCG
 	return lruvec->pgdat;
 #else
-	return container_of(lruvec, struct pglist_data, lruvec);
+	return container_of(lruvec, struct pglist_data, __lruvec);
 #endif
 }
 

commit 653e003d7f37716f84c17edcad3c228497888bfc
Author: Hao Lee <haolee.swjtu@gmail.com>
Date:   Sat Nov 30 17:55:18 2019 -0800

    include/linux/mmzone.h: fix comment for ISOLATE_UNMAPPED macro
    
    Both file-backed pages and anonymous pages can be unmapped.
    ISOLATE_UNMAPPED is not just for file-backed pages.
    
    Link: http://lkml.kernel.org/r/20191024151621.GA20400@haolee.github.io
    Signed-off-by: Hao Lee <haolee.swjtu@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b0a36d1580b6..c7fb21f19edd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -308,7 +308,7 @@ struct lruvec {
 #endif
 };
 
-/* Isolate unmapped file */
+/* Isolate unmapped pages */
 #define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)
 /* Isolate for asynchronous migration */
 #define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x4)

commit 734f9246e791d8da278957b2c326d7709b2a97c0
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Wed Sep 11 20:25:46 2019 +0200

    mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
    
    These zones usage has evolved with time and the comments were outdated.
    This joins both ZONE_DMA and ZONE_DMA32 explanation and gives up to date
    examples on how they are used on different architectures.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bda20282746b..b0a36d1580b6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -359,33 +359,40 @@ struct per_cpu_nodestat {
 #endif /* !__GENERATING_BOUNDS.H */
 
 enum zone_type {
-#ifdef CONFIG_ZONE_DMA
 	/*
-	 * ZONE_DMA is used when there are devices that are not able
-	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
-	 * carve out the portion of memory that is needed for these devices.
-	 * The range is arch specific.
+	 * ZONE_DMA and ZONE_DMA32 are used when there are peripherals not able
+	 * to DMA to all of the addressable memory (ZONE_NORMAL).
+	 * On architectures where this area covers the whole 32 bit address
+	 * space ZONE_DMA32 is used. ZONE_DMA is left for the ones with smaller
+	 * DMA addressing constraints. This distinction is important as a 32bit
+	 * DMA mask is assumed when ZONE_DMA32 is defined. Some 64-bit
+	 * platforms may need both zones as they support peripherals with
+	 * different DMA addressing limitations.
+	 *
+	 * Some examples:
+	 *
+	 *  - i386 and x86_64 have a fixed 16M ZONE_DMA and ZONE_DMA32 for the
+	 *    rest of the lower 4G.
+	 *
+	 *  - arm only uses ZONE_DMA, the size, up to 4G, may vary depending on
+	 *    the specific device.
+	 *
+	 *  - arm64 has a fixed 1G ZONE_DMA and ZONE_DMA32 for the rest of the
+	 *    lower 4G.
 	 *
-	 * Some examples
+	 *  - powerpc only uses ZONE_DMA, the size, up to 2G, may vary
+	 *    depending on the specific device.
 	 *
-	 * Architecture		Limit
-	 * ---------------------------
-	 * parisc, ia64, sparc	<4G
-	 * s390, powerpc	<2G
-	 * arm			Various
-	 * alpha		Unlimited or 0-16MB.
+	 *  - s390 uses ZONE_DMA fixed to the lower 2G.
 	 *
-	 * i386, x86_64 and multiple other arches
-	 * 			<16M.
+	 *  - ia64 and riscv only use ZONE_DMA32.
+	 *
+	 *  - parisc uses neither.
 	 */
+#ifdef CONFIG_ZONE_DMA
 	ZONE_DMA,
 #endif
 #ifdef CONFIG_ZONE_DMA32
-	/*
-	 * x86_64 needs two ZONE_DMAs because it supports devices that are
-	 * only able to do DMA to the lower 16M but also 32 bit devices that
-	 * can only do DMA areas below 4G.
-	 */
 	ZONE_DMA32,
 #endif
 	/*

commit 364c1eebe453f06f0c1e837eb155a5725c9cd272
Author: Yang Shi <yang.shi@linux.alibaba.com>
Date:   Mon Sep 23 15:38:06 2019 -0700

    mm: thp: extract split_queue_* into a struct
    
    Patch series "Make deferred split shrinker memcg aware", v6.
    
    Currently THP deferred split shrinker is not memcg aware, this may cause
    premature OOM with some configuration.  For example the below test would
    run into premature OOM easily:
    
    $ cgcreate -g memory:thp
    $ echo 4G > /sys/fs/cgroup/memory/thp/memory/limit_in_bytes
    $ cgexec -g memory:thp transhuge-stress 4000
    
    transhuge-stress comes from kernel selftest.
    
    It is easy to hit OOM, but there are still a lot THP on the deferred split
    queue, memcg direct reclaim can't touch them since the deferred split
    shrinker is not memcg aware.
    
    Convert deferred split shrinker memcg aware by introducing per memcg
    deferred split queue.  The THP should be on either per node or per memcg
    deferred split queue if it belongs to a memcg.  When the page is
    immigrated to the other memcg, it will be immigrated to the target memcg's
    deferred split queue too.
    
    Reuse the second tail page's deferred_list for per memcg list since the
    same THP can't be on multiple deferred split queues.
    
    Make deferred split shrinker not depend on memcg kmem since it is not
    slab.  It doesn't make sense to not shrink THP even though memcg kmem is
    disabled.
    
    With the above change the test demonstrated above doesn't trigger OOM even
    though with cgroup.memory=nokmem.
    
    This patch (of 4):
    
    Put split_queue, split_queue_lock and split_queue_len into a struct in
    order to reduce code duplication when we convert deferred_split to memcg
    aware in the later patches.
    
    Link: http://lkml.kernel.org/r/1565144277-36240-2-git-send-email-yang.shi@linux.alibaba.com
    Signed-off-by: Yang Shi <yang.shi@linux.alibaba.com>
    Suggested-by: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index aafb7c38c627..bda20282746b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -679,6 +679,14 @@ struct zonelist {
 extern struct page *mem_map;
 #endif
 
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+struct deferred_split {
+	spinlock_t split_queue_lock;
+	struct list_head split_queue;
+	unsigned long split_queue_len;
+};
+#endif
+
 /*
  * On NUMA machines, each NUMA node would have a pg_data_t to describe
  * it's memory layout. On UMA machines there is a single pglist_data which
@@ -758,9 +766,7 @@ typedef struct pglist_data {
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
-	spinlock_t split_queue_lock;
-	struct list_head split_queue;
-	unsigned long split_queue_len;
+	struct deferred_split deferred_split_queue;
 #endif
 
 	/* Fields commonly accessed by the page reclaim scanner */

commit 60fbf0ab5da1c360e02b7f7d882bf1c0d8f7e32a
Author: Song Liu <songliubraving@fb.com>
Date:   Mon Sep 23 15:37:54 2019 -0700

    mm,thp: stats for file backed THP
    
    In preparation for non-shmem THP, this patch adds a few stats and exposes
    them in /proc/meminfo, /sys/bus/node/devices/<node>/meminfo, and
    /proc/<pid>/task/<tid>/smaps.
    
    This patch is mostly a rewrite of Kirill A.  Shutemov's earlier version:
    https://lkml.kernel.org/r/20170126115819.58875-5-kirill.shutemov@linux.intel.com/
    
    Link: http://lkml.kernel.org/r/20190801184244.3169074-5-songliubraving@fb.com
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Acked-by: Rik van Riel <riel@surriel.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: William Kucharski <william.kucharski@oracle.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3f38c30d2f13..aafb7c38c627 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -235,6 +235,8 @@ enum node_stat_item {
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_SHMEM_THPS,
 	NR_SHMEM_PMDMAPPED,
+	NR_FILE_THPS,
+	NR_FILE_PMDMAPPED,
 	NR_ANON_THPS,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_VMSCAN_WRITE,

commit bee07b33db78d4ee7ed6a2fe810b9473d5471fe4
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Aug 30 16:04:32 2019 -0700

    mm: memcontrol: flush percpu slab vmstats on kmem offlining
    
    I've noticed that the "slab" value in memory.stat is sometimes 0, even
    if some children memory cgroups have a non-zero "slab" value.  The
    following investigation showed that this is the result of the kmem_cache
    reparenting in combination with the per-cpu batching of slab vmstats.
    
    At the offlining some vmstat value may leave in the percpu cache, not
    being propagated upwards by the cgroup hierarchy.  It means that stats
    on ancestor levels are lower than actual.  Later when slab pages are
    released, the precise number of pages is substracted on the parent
    level, making the value negative.  We don't show negative values, 0 is
    printed instead.
    
    To fix this issue, let's flush percpu slab memcg and lruvec stats on
    memcg offlining.  This guarantees that numbers on all ancestor levels
    are accurate and match the actual number of outstanding slab pages.
    
    Link: http://lkml.kernel.org/r/20190819202338.363363-3-guro@fb.com
    Fixes: fb2f2b0adb98 ("mm: memcg/slab: reparent memcg kmem_caches on cgroup removal")
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d77d717c620c..3f38c30d2f13 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -215,8 +215,9 @@ enum node_stat_item {
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
-	NR_SLAB_RECLAIMABLE,
-	NR_SLAB_UNRECLAIMABLE,
+	NR_SLAB_RECLAIMABLE,	/* Please do not reorder this item */
+	NR_SLAB_UNRECLAIMABLE,	/* and this one without looking at
+				 * memcg_flush_percpu_vmstats() first. */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	WORKINGSET_NODES,

commit a3619190d62ed9d66416891be2416f6bea2b3ca4
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:40 2019 -0700

    libnvdimm/pfn: stop padding pmem namespaces to section alignment
    
    Now that the mm core supports section-unaligned hotplug of ZONE_DEVICE
    memory, we no longer need to add padding at pfn/dax device creation
    time.  The kernel will still honor padding established by older kernels.
    
    Link: http://lkml.kernel.org/r/156092356588.979959.6793371748950931916.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Jeff Moyer <jmoyer@redhat.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8331e76677c0..d77d717c620c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1160,6 +1160,9 @@ static inline unsigned long section_nr_to_pfn(unsigned long sec)
 #define SUBSECTIONS_PER_SECTION (1UL << (SECTION_SIZE_BITS - SUBSECTION_SHIFT))
 #endif
 
+#define SUBSECTION_ALIGN_UP(pfn) ALIGN((pfn), PAGES_PER_SUBSECTION)
+#define SUBSECTION_ALIGN_DOWN(pfn) ((pfn) & PAGE_SUBSECTION_MASK)
+
 struct mem_section_usage {
 	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);
 	/* See declaration of similar field in struct zone */

commit 46d945aeab4d7dd837bd0724662de2caf712f047
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:18 2019 -0700

    mm: kill is_dev_zone() helper
    
    Given there are no more usages of is_dev_zone() outside of 'ifdef
    CONFIG_ZONE_DEVICE' protection, kill off the compilation helper.
    
    Link: http://lkml.kernel.org/r/156092353211.979959.1489004866360828964.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Reviewed-by: Wei Yang <richardw.yang@linux.intel.com>
    Acked-by: David Hildenbrand <david@redhat.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7747ec9de588..8331e76677c0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -855,18 +855,6 @@ static inline int local_memory_node(int node_id) { return node_id; };
  */
 #define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
 
-#ifdef CONFIG_ZONE_DEVICE
-static inline bool is_dev_zone(const struct zone *zone)
-{
-	return zone_idx(zone) == ZONE_DEVICE;
-}
-#else
-static inline bool is_dev_zone(const struct zone *zone)
-{
-	return false;
-}
-#endif
-
 /*
  * Returns true if a zone has pages managed by the buddy allocator.
  * All the reclaim decisions have to use this function rather than

commit f46edbd1b1516da1fb34c917775168d5df576f78
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:04 2019 -0700

    mm/sparsemem: add helpers track active portions of a section at boot
    
    Prepare for hot{plug,remove} of sub-ranges of a section by tracking a
    sub-section active bitmask, each bit representing a PMD_SIZE span of the
    architecture's memory hotplug section size.
    
    The implications of a partially populated section is that pfn_valid()
    needs to go beyond a valid_section() check and either determine that the
    section is an "early section", or read the sub-section active ranges
    from the bitmask.  The expectation is that the bitmask (subsection_map)
    fits in the same cacheline as the valid_section() / early_section()
    data, so the incremental performance overhead to pfn_valid() should be
    negligible.
    
    The rationale for using early_section() to short-ciruit the
    subsection_map check is that there are legacy code paths that use
    pfn_valid() at section granularity before validating the pfn against
    pgdat data.  So, the early_section() check allows those traditional
    assumptions to persist while also permitting subsection_map to tell the
    truth for purposes of populating the unused portions of early sections
    with PMEM and other ZONE_DEVICE mappings.
    
    Link: http://lkml.kernel.org/r/156092350874.979959.18185938451405518285.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Qian Cai <cai@lca.pw>
    Tested-by: Jane Chu <jane.chu@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4be40634238b..7747ec9de588 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1178,6 +1178,8 @@ struct mem_section_usage {
 	unsigned long pageblock_flags[0];
 };
 
+void subsection_map_init(unsigned long pfn, unsigned long nr_pages);
+
 struct page;
 struct page_ext;
 struct mem_section {
@@ -1321,12 +1323,40 @@ static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 
 extern unsigned long __highest_present_section_nr;
 
+static inline int subsection_map_index(unsigned long pfn)
+{
+	return (pfn & ~(PAGE_SECTION_MASK)) / PAGES_PER_SUBSECTION;
+}
+
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
+static inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)
+{
+	int idx = subsection_map_index(pfn);
+
+	return test_bit(idx, ms->usage->subsection_map);
+}
+#else
+static inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)
+{
+	return 1;
+}
+#endif
+
 #ifndef CONFIG_HAVE_ARCH_PFN_VALID
 static inline int pfn_valid(unsigned long pfn)
 {
+	struct mem_section *ms;
+
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
 		return 0;
-	return valid_section(__nr_to_section(pfn_to_section_nr(pfn)));
+	ms = __nr_to_section(pfn_to_section_nr(pfn));
+	if (!valid_section(ms))
+		return 0;
+	/*
+	 * Traditionally early sections always returned pfn_valid() for
+	 * the entire section-sized span.
+	 */
+	return early_section(ms) || pfn_section_valid(ms, pfn);
 }
 #endif
 
@@ -1358,6 +1388,7 @@ void sparse_init(void);
 #define sparse_init()	do {} while (0)
 #define sparse_index_init(_sec, _nid)  do {} while (0)
 #define pfn_present pfn_valid
+#define subsection_map_init(_pfn, _nr_pages) do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
 /*

commit 326e1b8f83a4318b09033ef754f40c785aed5e68
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:00 2019 -0700

    mm/sparsemem: introduce a SECTION_IS_EARLY flag
    
    In preparation for sub-section hotplug, track whether a given section
    was created during early memory initialization, or later via memory
    hotplug.  This distinction is needed to maintain the coarse expectation
    that pfn_valid() returns true for any pfn within a given section even if
    that section has pages that are reserved from the page allocator.
    
    For example one of the of goals of subsection hotplug is to support
    cases where the system physical memory layout collides System RAM and
    PMEM within a section.  Several pfn_valid() users expect to just check
    if a section is valid, but they are not careful to check if the given
    pfn is within a "System RAM" boundary and instead expect pgdat
    information to further validate the pfn.
    
    Rather than unwind those paths to make their pfn_valid() queries more
    precise a follow on patch uses the SECTION_IS_EARLY flag to maintain the
    traditional expectation that pfn_valid() returns true for all early
    sections.
    
    Link: https://lore.kernel.org/lkml/1560366952-10660-1-git-send-email-cai@lca.pw/
    Link: http://lkml.kernel.org/r/156092350358.979959.5817209875548072819.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: Qian Cai <cai@lca.pw>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2520336bdfd1..4be40634238b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1260,7 +1260,8 @@ extern size_t mem_section_usage_size(void);
 #define	SECTION_MARKED_PRESENT	(1UL<<0)
 #define SECTION_HAS_MEM_MAP	(1UL<<1)
 #define SECTION_IS_ONLINE	(1UL<<2)
-#define SECTION_MAP_LAST_BIT	(1UL<<3)
+#define SECTION_IS_EARLY	(1UL<<3)
+#define SECTION_MAP_LAST_BIT	(1UL<<4)
 #define SECTION_MAP_MASK	(~(SECTION_MAP_LAST_BIT-1))
 #define SECTION_NID_SHIFT	3
 
@@ -1286,6 +1287,11 @@ static inline int valid_section(struct mem_section *section)
 	return (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));
 }
 
+static inline int early_section(struct mem_section *section)
+{
+	return (section && (section->section_mem_map & SECTION_IS_EARLY));
+}
+
 static inline int valid_section_nr(unsigned long nr)
 {
 	return valid_section(__nr_to_section(nr));

commit f1eca35a0dc7cb3cdb00c88c8c5e5138a65face0
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:57:57 2019 -0700

    mm/sparsemem: introduce struct mem_section_usage
    
    Patch series "mm: Sub-section memory hotplug support", v10.
    
    The memory hotplug section is an arbitrary / convenient unit for memory
    hotplug.  'Section-size' units have bled into the user interface
    ('memblock' sysfs) and can not be changed without breaking existing
    userspace.  The section-size constraint, while mostly benign for typical
    memory hotplug, has and continues to wreak havoc with 'device-memory'
    use cases, persistent memory (pmem) in particular.  Recall that pmem
    uses devm_memremap_pages(), and subsequently arch_add_memory(), to
    allocate a 'struct page' memmap for pmem.  However, it does not use the
    'bottom half' of memory hotplug, i.e.  never marks pmem pages online and
    never exposes the userspace memblock interface for pmem.  This leaves an
    opening to redress the section-size constraint.
    
    To date, the libnvdimm subsystem has attempted to inject padding to
    satisfy the internal constraints of arch_add_memory().  Beyond
    complicating the code, leading to bugs [2], wasting memory, and limiting
    configuration flexibility, the padding hack is broken when the platform
    changes this physical memory alignment of pmem from one boot to the
    next.  Device failure (intermittent or permanent) and physical
    reconfiguration are events that can cause the platform firmware to
    change the physical placement of pmem on a subsequent boot, and device
    failure is an everyday event in a data-center.
    
    It turns out that sections are only a hard requirement of the
    user-facing interface for memory hotplug and with a bit more
    infrastructure sub-section arch_add_memory() support can be added for
    kernel internal usages like devm_memremap_pages().  Here is an analysis
    of the current design assumptions in the current code and how they are
    addressed in the new implementation:
    
    Current design assumptions:
    
     - Sections that describe boot memory (early sections) are never
       unplugged / removed.
    
     - pfn_valid(), in the CONFIG_SPARSEMEM_VMEMMAP=y, case devolves to a
       valid_section() check
    
     - __add_pages() and helper routines assume all operations occur in
       PAGES_PER_SECTION units.
    
     - The memblock sysfs interface only comprehends full sections
    
    New design assumptions:
    
     - Sections are instrumented with a sub-section bitmask to track (on
       x86) individual 2MB sub-divisions of a 128MB section.
    
     - Partially populated early sections can be extended with additional
       sub-sections, and those sub-sections can be removed with
       arch_remove_memory(). With this in place we no longer lose usable
       memory capacity to padding.
    
     - pfn_valid() is updated to look deeper than valid_section() to also
       check the active-sub-section mask. This indication is in the same
       cacheline as the valid_section() so the performance impact is
       expected to be negligible. So far the lkp robot has not reported any
       regressions.
    
     - Outside of the core vmemmap population routines which are replaced,
       other helper routines like shrink_{zone,pgdat}_span() are updated to
       handle the smaller granularity. Core memory hotplug routines that
       deal with online memory are not touched.
    
     - The existing memblock sysfs user api guarantees / assumptions are not
       touched since this capability is limited to !online
       !memblock-sysfs-accessible sections.
    
    Meanwhile the issue reports continue to roll in from users that do not
    understand when and how the 128MB constraint will bite them.  The current
    implementation relied on being able to support at least one misaligned
    namespace, but that immediately falls over on any moderately complex
    namespace creation attempt.  Beyond the initial problem of 'System RAM'
    colliding with pmem, and the unsolvable problem of physical alignment
    changes, Linux is now being exposed to platforms that collide pmem ranges
    with other pmem ranges by default [3].  In short, devm_memremap_pages()
    has pushed the venerable section-size constraint past the breaking point,
    and the simplicity of section-aligned arch_add_memory() is no longer
    tenable.
    
    These patches are exposed to the kbuild robot on a subsection-v10 branch
    [4], and a preview of the unit test for this functionality is available
    on the 'subsection-pending' branch of ndctl [5].
    
    [2]: https://lore.kernel.org/r/155000671719.348031.2347363160141119237.stgit@dwillia2-desk3.amr.corp.intel.com
    [3]: https://github.com/pmem/ndctl/issues/76
    [4]: https://git.kernel.org/pub/scm/linux/kernel/git/djbw/nvdimm.git/log/?h=subsection-v10
    [5]: https://github.com/pmem/ndctl/commit/7c59b4867e1c
    
    This patch (of 13):
    
    Towards enabling memory hotplug to track partial population of a section,
    introduce 'struct mem_section_usage'.
    
    A pointer to a 'struct mem_section_usage' instance replaces the existing
    pointer to a 'pageblock_flags' bitmap.  Effectively it adds one more
    'unsigned long' beyond the 'pageblock_flags' (usemap) allocation to house
    a new 'subsection_map' bitmap.  The new bitmap enables the memory
    hot{plug,remove} implementation to act on incremental sub-divisions of a
    section.
    
    SUBSECTION_SHIFT is defined as global constant instead of per-architecture
    value like SECTION_SIZE_BITS in order to allow cross-arch compatibility of
    subsection users.  Specifically a common subsection size allows for the
    possibility that persistent memory namespace configurations be made
    compatible across architectures.
    
    The primary motivation for this functionality is to support platforms that
    mix "System RAM" and "Persistent Memory" within a single section, or
    multiple PMEM ranges with different mapping lifetimes within a single
    section.  The section restriction for hotplug has caused an ongoing saga
    of hacks and bugs for devm_memremap_pages() users.
    
    Beyond the fixups to teach existing paths how to retrieve the 'usemap'
    from a section, and updates to usemap allocation path, there are no
    expected behavior changes.
    
    Link: http://lkml.kernel.org/r/156092349845.979959.73333291612799019.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Wei Yang <richardw.yang@linux.intel.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 298d1c3e4c2e..2520336bdfd1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1160,6 +1160,24 @@ static inline unsigned long section_nr_to_pfn(unsigned long sec)
 #define SECTION_ALIGN_UP(pfn)	(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)
 #define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)
 
+#define SUBSECTION_SHIFT 21
+
+#define PFN_SUBSECTION_SHIFT (SUBSECTION_SHIFT - PAGE_SHIFT)
+#define PAGES_PER_SUBSECTION (1UL << PFN_SUBSECTION_SHIFT)
+#define PAGE_SUBSECTION_MASK (~(PAGES_PER_SUBSECTION-1))
+
+#if SUBSECTION_SHIFT > SECTION_SIZE_BITS
+#error Subsection size exceeds section size
+#else
+#define SUBSECTIONS_PER_SECTION (1UL << (SECTION_SIZE_BITS - SUBSECTION_SHIFT))
+#endif
+
+struct mem_section_usage {
+	DECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);
+	/* See declaration of similar field in struct zone */
+	unsigned long pageblock_flags[0];
+};
+
 struct page;
 struct page_ext;
 struct mem_section {
@@ -1177,8 +1195,7 @@ struct mem_section {
 	 */
 	unsigned long section_mem_map;
 
-	/* See declaration of similar field in struct zone */
-	unsigned long *pageblock_flags;
+	struct mem_section_usage *usage;
 #ifdef CONFIG_PAGE_EXTENSION
 	/*
 	 * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use
@@ -1209,6 +1226,11 @@ extern struct mem_section **mem_section;
 extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
 #endif
 
+static inline unsigned long *section_to_usemap(struct mem_section *ms)
+{
+	return ms->usage->pageblock_flags;
+}
+
 static inline struct mem_section *__nr_to_section(unsigned long nr)
 {
 #ifdef CONFIG_SPARSEMEM_EXTREME
@@ -1220,7 +1242,7 @@ static inline struct mem_section *__nr_to_section(unsigned long nr)
 	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
 }
 extern unsigned long __section_nr(struct mem_section *ms);
-extern unsigned long usemap_size(void);
+extern size_t mem_section_usage_size(void);
 
 /*
  * We use the lower bits of the mem_map pointer to store

commit 2491f0a2c0b117b9097e9c9eee0c21f2e5f716d7
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:57:37 2019 -0700

    mm: section numbers use the type "unsigned long"
    
    Patch series "mm: Further memory block device cleanups", v1.
    
    Some further cleanups around memory block devices.  Especially, clean up
    and simplify walk_memory_range().  Including some other minor cleanups.
    
    This patch (of 6):
    
    We are using a mixture of "int" and "unsigned long".  Let's make this
    consistent by using "unsigned long" everywhere.  We'll do the same with
    memory block ids next.
    
    While at it, turn the "unsigned long i" in removable_show() into an int
    - sections_per_block is an int.
    
    [akpm@linux-foundation.org: s/unsigned long i/unsigned long nr/]
    [david@redhat.com: v3]
      Link: http://lkml.kernel.org/r/20190620183139.4352-2-david@redhat.com
    Link: http://lkml.kernel.org/r/20190614100114.311-2-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Baoquan He <bhe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 70394cabaf4e..298d1c3e4c2e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1219,7 +1219,7 @@ static inline struct mem_section *__nr_to_section(unsigned long nr)
 		return NULL;
 	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
 }
-extern int __section_nr(struct mem_section* ms);
+extern unsigned long __section_nr(struct mem_section *ms);
 extern unsigned long usemap_size(void);
 
 /*
@@ -1291,7 +1291,7 @@ static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 	return __nr_to_section(pfn_to_section_nr(pfn));
 }
 
-extern int __highest_present_section_nr;
+extern unsigned long __highest_present_section_nr;
 
 #ifndef CONFIG_HAVE_ARCH_PFN_VALID
 static inline int pfn_valid(unsigned long pfn)

commit 97500a4a54876d3d6d2d1b8419223eb4e69b32d8
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue May 14 15:41:35 2019 -0700

    mm: maintain randomization of page free lists
    
    When freeing a page with an order >= shuffle_page_order randomly select
    the front or back of the list for insertion.
    
    While the mm tries to defragment physical pages into huge pages this can
    tend to make the page allocator more predictable over time.  Inject the
    front-back randomness to preserve the initial randomness established by
    shuffle_free_memory() when the kernel was booted.
    
    The overhead of this manipulation is constrained by only being applied
    for MAX_ORDER sized pages by default.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/154899812788.3165233.9066631950746578517.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Robert Elliott <elliott@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index acd6f9cb01bd..70394cabaf4e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -116,6 +116,18 @@ static inline void add_to_free_area_tail(struct page *page, struct free_area *ar
 	area->nr_free++;
 }
 
+#ifdef CONFIG_SHUFFLE_PAGE_ALLOCATOR
+/* Used to preserve page allocation order entropy */
+void add_to_free_area_random(struct page *page, struct free_area *area,
+		int migratetype);
+#else
+static inline void add_to_free_area_random(struct page *page,
+		struct free_area *area, int migratetype)
+{
+	add_to_free_area(page, area, migratetype);
+}
+#endif
+
 /* Used for pages which are on another list */
 static inline void move_to_free_area(struct page *page, struct free_area *area,
 			     int migratetype)

commit b03641af680959df57c275a80ff0dc116627c7ae
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue May 14 15:41:32 2019 -0700

    mm: move buddy list manipulations into helpers
    
    In preparation for runtime randomization of the zone lists, take all
    (well, most of) the list_*() functions in the buddy allocator and put
    them in helper functions.  Provide a common control point for injecting
    additional behavior when freeing pages.
    
    [dan.j.williams@intel.com: fix buddy list helpers]
      Link: http://lkml.kernel.org/r/155033679702.1773410.13041474192173212653.stgit@dwillia2-desk3.amr.corp.intel.com
    [vbabka@suse.cz: remove del_page_from_free_area() migratetype parameter]
      Link: http://lkml.kernel.org/r/4672701b-6775-6efd-0797-b6242591419e@suse.cz
    Link: http://lkml.kernel.org/r/154899812264.3165233.5219320056406926223.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Tested-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Robert Elliott <elliott@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1fb5a04530aa..acd6f9cb01bd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -18,6 +18,8 @@
 #include <linux/pageblock-flags.h>
 #include <linux/page-flags-layout.h>
 #include <linux/atomic.h>
+#include <linux/mm_types.h>
+#include <linux/page-flags.h>
 #include <asm/page.h>
 
 /* Free memory management - zoned buddy allocator.  */
@@ -98,6 +100,50 @@ struct free_area {
 	unsigned long		nr_free;
 };
 
+/* Used for pages not on another list */
+static inline void add_to_free_area(struct page *page, struct free_area *area,
+			     int migratetype)
+{
+	list_add(&page->lru, &area->free_list[migratetype]);
+	area->nr_free++;
+}
+
+/* Used for pages not on another list */
+static inline void add_to_free_area_tail(struct page *page, struct free_area *area,
+				  int migratetype)
+{
+	list_add_tail(&page->lru, &area->free_list[migratetype]);
+	area->nr_free++;
+}
+
+/* Used for pages which are on another list */
+static inline void move_to_free_area(struct page *page, struct free_area *area,
+			     int migratetype)
+{
+	list_move(&page->lru, &area->free_list[migratetype]);
+}
+
+static inline struct page *get_page_from_free_area(struct free_area *area,
+					    int migratetype)
+{
+	return list_first_entry_or_null(&area->free_list[migratetype],
+					struct page, lru);
+}
+
+static inline void del_page_from_free_area(struct page *page,
+		struct free_area *area)
+{
+	list_del(&page->lru);
+	__ClearPageBuddy(page);
+	set_page_private(page, 0);
+	area->nr_free--;
+}
+
+static inline bool free_area_empty(struct free_area *area, int migratetype)
+{
+	return list_empty(&area->free_list[migratetype]);
+}
+
 struct pglist_data;
 
 /*

commit e900a918b0984ec8f2eb150b8477a47b75d17692
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue May 14 15:41:28 2019 -0700

    mm: shuffle initial free memory to improve memory-side-cache utilization
    
    Patch series "mm: Randomize free memory", v10.
    
    This patch (of 3):
    
    Randomization of the page allocator improves the average utilization of
    a direct-mapped memory-side-cache.  Memory side caching is a platform
    capability that Linux has been previously exposed to in HPC
    (high-performance computing) environments on specialty platforms.  In
    that instance it was a smaller pool of high-bandwidth-memory relative to
    higher-capacity / lower-bandwidth DRAM.  Now, this capability is going
    to be found on general purpose server platforms where DRAM is a cache in
    front of higher latency persistent memory [1].
    
    Robert offered an explanation of the state of the art of Linux
    interactions with memory-side-caches [2], and I copy it here:
    
        It's been a problem in the HPC space:
        http://www.nersc.gov/research-and-development/knl-cache-mode-performance-coe/
    
        A kernel module called zonesort is available to try to help:
        https://software.intel.com/en-us/articles/xeon-phi-software
    
        and this abandoned patch series proposed that for the kernel:
        https://lkml.kernel.org/r/20170823100205.17311-1-lukasz.daniluk@intel.com
    
        Dan's patch series doesn't attempt to ensure buffers won't conflict, but
        also reduces the chance that the buffers will. This will make performance
        more consistent, albeit slower than "optimal" (which is near impossible
        to attain in a general-purpose kernel).  That's better than forcing
        users to deploy remedies like:
            "To eliminate this gradual degradation, we have added a Stream
             measurement to the Node Health Check that follows each job;
             nodes are rebooted whenever their measured memory bandwidth
             falls below 300 GB/s."
    
    A replacement for zonesort was merged upstream in commit cc9aec03e58f
    ("x86/numa_emulation: Introduce uniform split capability").  With this
    numa_emulation capability, memory can be split into cache sized
    ("near-memory" sized) numa nodes.  A bind operation to such a node, and
    disabling workloads on other nodes, enables full cache performance.
    However, once the workload exceeds the cache size then cache conflicts
    are unavoidable.  While HPC environments might be able to tolerate
    time-scheduling of cache sized workloads, for general purpose server
    platforms, the oversubscribed cache case will be the common case.
    
    The worst case scenario is that a server system owner benchmarks a
    workload at boot with an un-contended cache only to see that performance
    degrade over time, even below the average cache performance due to
    excessive conflicts.  Randomization clips the peaks and fills in the
    valleys of cache utilization to yield steady average performance.
    
    Here are some performance impact details of the patches:
    
    1/ An Intel internal synthetic memory bandwidth measurement tool, saw a
       3X speedup in a contrived case that tries to force cache conflicts.
       The contrived cased used the numa_emulation capability to force an
       instance of the benchmark to be run in two of the near-memory sized
       numa nodes.  If both instances were placed on the same emulated they
       would fit and cause zero conflicts.  While on separate emulated nodes
       without randomization they underutilized the cache and conflicted
       unnecessarily due to the in-order allocation per node.
    
    2/ A well known Java server application benchmark was run with a heap
       size that exceeded cache size by 3X.  The cache conflict rate was 8%
       for the first run and degraded to 21% after page allocator aging.  With
       randomization enabled the rate levelled out at 11%.
    
    3/ A MongoDB workload did not observe measurable difference in
       cache-conflict rates, but the overall throughput dropped by 7% with
       randomization in one case.
    
    4/ Mel Gorman ran his suite of performance workloads with randomization
       enabled on platforms without a memory-side-cache and saw a mix of some
       improvements and some losses [3].
    
    While there is potentially significant improvement for applications that
    depend on low latency access across a wide working-set, the performance
    may be negligible to negative for other workloads.  For this reason the
    shuffle capability defaults to off unless a direct-mapped
    memory-side-cache is detected.  Even then, the page_alloc.shuffle=0
    parameter can be specified to disable the randomization on those systems.
    
    Outside of memory-side-cache utilization concerns there is potentially
    security benefit from randomization.  Some data exfiltration and
    return-oriented-programming attacks rely on the ability to infer the
    location of sensitive data objects.  The kernel page allocator, especially
    early in system boot, has predictable first-in-first out behavior for
    physical pages.  Pages are freed in physical address order when first
    onlined.
    
    Quoting Kees:
        "While we already have a base-address randomization
         (CONFIG_RANDOMIZE_MEMORY), attacks against the same hardware and
         memory layouts would certainly be using the predictability of
         allocation ordering (i.e. for attacks where the base address isn't
         important: only the relative positions between allocated memory).
         This is common in lots of heap-style attacks. They try to gain
         control over ordering by spraying allocations, etc.
    
         I'd really like to see this because it gives us something similar
         to CONFIG_SLAB_FREELIST_RANDOM but for the page allocator."
    
    While SLAB_FREELIST_RANDOM reduces the predictability of some local slab
    caches it leaves vast bulk of memory to be predictably in order allocated.
    However, it should be noted, the concrete security benefits are hard to
    quantify, and no known CVE is mitigated by this randomization.
    
    Introduce shuffle_free_memory(), and its helper shuffle_zone(), to perform
    a Fisher-Yates shuffle of the page allocator 'free_area' lists when they
    are initially populated with free memory at boot and at hotplug time.  Do
    this based on either the presence of a page_alloc.shuffle=Y command line
    parameter, or autodetection of a memory-side-cache (to be added in a
    follow-on patch).
    
    The shuffling is done in terms of CONFIG_SHUFFLE_PAGE_ORDER sized free
    pages where the default CONFIG_SHUFFLE_PAGE_ORDER is MAX_ORDER-1 i.e.  10,
    4MB this trades off randomization granularity for time spent shuffling.
    MAX_ORDER-1 was chosen to be minimally invasive to the page allocator
    while still showing memory-side cache behavior improvements, and the
    expectation that the security implications of finer granularity
    randomization is mitigated by CONFIG_SLAB_FREELIST_RANDOM.  The
    performance impact of the shuffling appears to be in the noise compared to
    other memory initialization work.
    
    This initial randomization can be undone over time so a follow-on patch is
    introduced to inject entropy on page free decisions.  It is reasonable to
    ask if the page free entropy is sufficient, but it is not enough due to
    the in-order initial freeing of pages.  At the start of that process
    putting page1 in front or behind page0 still keeps them close together,
    page2 is still near page1 and has a high chance of being adjacent.  As
    more pages are added ordering diversity improves, but there is still high
    page locality for the low address pages and this leads to no significant
    impact to the cache conflict rate.
    
    [1]: https://itpeernetwork.intel.com/intel-optane-dc-persistent-memory-operating-modes/
    [2]: https://lkml.kernel.org/r/AT5PR8401MB1169D656C8B5E121752FC0F8AB120@AT5PR8401MB1169.NAMPRD84.PROD.OUTLOOK.COM
    [3]: https://lkml.org/lkml/2018/10/12/309
    
    [dan.j.williams@intel.com: fix shuffle enable]
      Link: http://lkml.kernel.org/r/154943713038.3858443.4125180191382062871.stgit@dwillia2-desk3.amr.corp.intel.com
    [cai@lca.pw: fix SHUFFLE_PAGE_ALLOCATOR help texts]
      Link: http://lkml.kernel.org/r/20190425201300.75650-1-cai@lca.pw
    Link: http://lkml.kernel.org/r/154899811738.3165233.12325692939590944259.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Robert Elliott <elliott@hpe.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5a4aedc160bd..1fb5a04530aa 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1271,6 +1271,7 @@ void sparse_init(void);
 #else
 #define sparse_init()	do {} while (0)
 #define sparse_index_init(_sec, _nid)  do {} while (0)
+#define pfn_present pfn_valid
 #endif /* CONFIG_SPARSEMEM */
 
 /*

commit 113b7dfd827175977ea71cc4a29c1ac24acb9fce
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Mon May 13 17:18:11 2019 -0700

    mm: memcontrol: quarantine the mem_cgroup_[node_]nr_lru_pages() API
    
    Only memcg_numa_stat_show() uses those wrappers and the lru bitmasks,
    group them together.
    
    Link: http://lkml.kernel.org/r/20190228163020.24100-7-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Roman Gushchin <guro@fb.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fba7741533be..5a4aedc160bd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -247,11 +247,6 @@ struct lruvec {
 #endif
 };
 
-/* Mask used at gathering information at once (see memcontrol.c) */
-#define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
-#define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
-#define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
-
 /* Isolate unmapped file */
 #define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)
 /* Isolate for asynchronous migration */

commit f4b7e272b5c0425915e2115068e0a5a20a3a628e
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Tue Mar 5 15:49:39 2019 -0800

    mm: remove zone_lru_lock() function, access ->lru_lock directly
    
    We have common pattern to access lru_lock from a page pointer:
            zone_lru_lock(page_zone(page))
    
    Which is silly, because it unfolds to this:
            &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)]->zone_pgdat->lru_lock
    while we can simply do
            &NODE_DATA(page_to_nid(page))->lru_lock
    
    Remove zone_lru_lock() function, since it's only complicate things.  Use
    'page_pgdat(page)->lru_lock' pattern instead.
    
    [aryabinin@virtuozzo.com: a slightly better version of __split_huge_page()]
      Link: http://lkml.kernel.org/r/20190301121651.7741-1-aryabinin@virtuozzo.com
    Link: http://lkml.kernel.org/r/20190228083329.31892-2-aryabinin@virtuozzo.com
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: William Kucharski <william.kucharski@oracle.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6d3290cd1f6f..fba7741533be 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -730,10 +730,6 @@ typedef struct pglist_data {
 
 #define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)
 #define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))
-static inline spinlock_t *zone_lru_lock(struct zone *zone)
-{
-	return &zone->zone_pgdat->lru_lock;
-}
 
 static inline struct lruvec *node_lruvec(struct pglist_data *pgdat)
 {

commit 8bb4e7a2ee26c05a94ae6cb0aec2f82a3523cf35
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Tue Mar 5 15:46:22 2019 -0800

    mm: fix some typos in mm directory
    
    No functional change.
    
    Link: http://lkml.kernel.org/r/20190118235123.27843-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 90c13cdeefb5..6d3290cd1f6f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1301,7 +1301,7 @@ void memory_present(int nid, unsigned long start, unsigned long end);
 
 /*
  * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we
- * need to check pfn validility within that MAX_ORDER_NR_PAGES block.
+ * need to check pfn validity within that MAX_ORDER_NR_PAGES block.
  * pfn_valid_within() should be used in this case; we optimise this away
  * when we have no holes within a MAX_ORDER_NR_PAGES block.
  */

commit e332f741a8dd1ec9a6dc8aa997296ecbfe64323e
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Tue Mar 5 15:45:38 2019 -0800

    mm, compaction: be selective about what pageblocks to clear skip hints
    
    Pageblock hints are cleared when compaction restarts or kswapd makes
    enough progress that it can sleep but it's over-eager in that the bit is
    cleared for migration sources with no LRU pages and migration targets
    with no free pages.  As pageblock skip hint flushes are relatively rare
    and out-of-band with respect to kswapd, this patch makes a few more
    expensive checks to see if it's appropriate to even clear the bit.
    Every pageblock that is not cleared will avoid 512 pages being scanned
    unnecessarily on x86-64.
    
    The impact is variable with different workloads showing small
    differences in latency, success rates and scan rates.  This is expected
    as clearing the hints is not that common but doing a small amount of
    work out-of-band to avoid a large amount of work in-band later is
    generally a good thing.
    
    Link: http://lkml.kernel.org/r/20190118175136.31341-22-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: YueHaibing <yuehaibing@huawei.com>
    [cai@lca.pw: no stuck in __reset_isolation_pfn()]
      Link: http://lkml.kernel.org/r/20190206034732.75687-1-cai@lca.pw
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 842f9189537b..90c13cdeefb5 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -480,6 +480,8 @@ struct zone {
 	unsigned long		compact_cached_free_pfn;
 	/* pfn where async and sync compaction migration scanner should start */
 	unsigned long		compact_cached_migrate_pfn[2];
+	unsigned long		compact_init_migrate_pfn;
+	unsigned long		compact_init_free_pfn;
 #endif
 
 #ifdef CONFIG_COMPACTION

commit 73444bc4d8f92e46a20cb6bd3342fc2ea75c6787
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Tue Jan 8 15:23:39 2019 -0800

    mm, page_alloc: do not wake kswapd with zone lock held
    
    syzbot reported the following regression in the latest merge window and
    it was confirmed by Qian Cai that a similar bug was visible from a
    different context.
    
      ======================================================
      WARNING: possible circular locking dependency detected
      4.20.0+ #297 Not tainted
      ------------------------------------------------------
      syz-executor0/8529 is trying to acquire lock:
      000000005e7fb829 (&pgdat->kswapd_wait){....}, at:
      __wake_up_common_lock+0x19e/0x330 kernel/sched/wait.c:120
    
      but task is already holding lock:
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at: spin_lock
      include/linux/spinlock.h:329 [inline]
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at: rmqueue_bulk
      mm/page_alloc.c:2548 [inline]
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at: __rmqueue_pcplist
      mm/page_alloc.c:3021 [inline]
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at: rmqueue_pcplist
      mm/page_alloc.c:3050 [inline]
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at: rmqueue
      mm/page_alloc.c:3072 [inline]
      000000009bb7bae0 (&(&zone->lock)->rlock){-.-.}, at:
      get_page_from_freelist+0x1bae/0x52a0 mm/page_alloc.c:3491
    
    It appears to be a false positive in that the only way the lock ordering
    should be inverted is if kswapd is waking itself and the wakeup
    allocates debugging objects which should already be allocated if it's
    kswapd doing the waking.  Nevertheless, the possibility exists and so
    it's best to avoid the problem.
    
    This patch flags a zone as needing a kswapd using the, surprisingly,
    unused zone flag field.  The flag is read without the lock held to do
    the wakeup.  It's possible that the flag setting context is not the same
    as the flag clearing context or for small races to occur.  However, each
    race possibility is harmless and there is no visible degredation in
    fragmentation treatment.
    
    While zone->flag could have continued to be unused, there is potential
    for moving some existing fields into the flags field instead.
    Particularly read-mostly ones like zone->initialized and
    zone->contiguous.
    
    Link: http://lkml.kernel.org/r/20190103225712.GJ31517@techsingularity.net
    Fixes: 1c30844d2dfe ("mm: reclaim small amounts of memory when an external fragmentation event occurs")
    Reported-by: syzbot+93d94a001cfbce9e60e1@syzkaller.appspotmail.com
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Tested-by: Qian Cai <cai@lca.pw>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cc4a507d7ca4..842f9189537b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -520,6 +520,12 @@ enum pgdat_flags {
 	PGDAT_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 };
 
+enum zone_flags {
+	ZONE_BOOSTED_WATERMARK,		/* zone recently boosted watermarks.
+					 * Cleared when kswapd is woken.
+					 */
+};
+
 static inline unsigned long zone_managed_pages(struct zone *zone)
 {
 	return (unsigned long)atomic_long_read(&zone->managed_pages);

commit fa004ab7365ffa1e17e6b267d64798afccb94946
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:37:10 2018 -0800

    mm, hotplug: move init_currently_empty_zone() under zone_span_lock protection
    
    During online_pages phase, pgdat->nr_zones will be updated in case this
    zone is empty.
    
    Currently the online_pages phase is protected by the global locks
    (device_device_hotplug_lock and mem_hotplug_lock), which ensures there is
    no contention during the update of nr_zones.
    
    These global locks introduces scalability issues (especially the second
    one), which slow down code relying on get_online_mems().  This is also a
    preparation for not having to rely on get_online_mems() but instead some
    more fine grained locks.
    
    The patch moves init_currently_empty_zone under both zone_span_writelock
    and pgdat_resize_lock because both the pgdat state is changed (nr_zones)
    and the zone's start_pfn.  Also this patch changes the documentation of
    node_size_lock to include the protection of nr_zones.
    
    Link: http://lkml.kernel.org/r/20181203205016.14123-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fc4b5cdb6c2d..cc4a507d7ca4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -636,8 +636,8 @@ typedef struct pglist_data {
 #endif
 #if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
 	/*
-	 * Must be held any time you expect node_start_pfn, node_present_pages
-	 * or node_spanned_pages stay constant.
+	 * Must be held any time you expect node_start_pfn,
+	 * node_present_pages, node_spanned_pages or nr_zones to stay constant.
 	 *
 	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
 	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG

commit 83af658898cb292a32d8b6cd9b51266d7cfc4b6a
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:37:02 2018 -0800

    mm, sparse: drop pgdat_resize_lock in sparse_add/remove_one_section()
    
    pgdat_resize_lock is used to protect pgdat's memory region information
    like: node_start_pfn, node_present_pages, etc.  While in function
    sparse_add/remove_one_section(), pgdat_resize_lock is used to protect
    initialization/release of one mem_section.  This looks not proper.
    
    These code paths are currently protected by mem_hotplug_lock currently but
    should there ever be any reason for locking at the sparse layer a
    dedicated lock should be introduced.
    
    Following is the current call trace of sparse_add/remove_one_section()
    
        mem_hotplug_begin()
        arch_add_memory()
           add_pages()
               __add_pages()
                   __add_section()
                       sparse_add_one_section()
        mem_hotplug_done()
    
        mem_hotplug_begin()
        arch_remove_memory()
            __remove_pages()
                __remove_section()
                    sparse_remove_one_section()
        mem_hotplug_done()
    
    The comment above the pgdat_resize_lock also mentions "Holding this will
    also guarantee that any pfn_valid() stays that way.", which is true with
    the current implementation and false after this patch.  But current
    implementation doesn't meet this comment.  There isn't any pfn walkers to
    take the lock so this looks like a relict from the past.  This patch also
    removes this comment.
    
    [richard.weiyang@gmail.com: v4]
      Link: http://lkml.kernel.org/r/20181204085657.20472-1-richard.weiyang@gmail.com
    [mhocko@suse.com: changelog suggestion]
    Link: http://lkml.kernel.org/r/20181128091243.19249-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a6e300732ec7..fc4b5cdb6c2d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -637,8 +637,7 @@ typedef struct pglist_data {
 #if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
 	/*
 	 * Must be held any time you expect node_start_pfn, node_present_pages
-	 * or node_spanned_pages stay constant.  Holding this will also
-	 * guarantee that any pfn_valid() stays that way.
+	 * or node_spanned_pages stay constant.
 	 *
 	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
 	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG

commit 23b68cfaae0ea40a9509fad37b756a6916dec54e
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:36:18 2018 -0800

    mm: check nr_initialised with PAGES_PER_SECTION directly in defer_init()
    
    When DEFERRED_STRUCT_PAGE_INIT is configured, only the first section of
    each node's highest zone is initialized before defer stage.
    
    static_init_pgcnt is used to store the number of pages like this:
    
        pgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,
                                                  pgdat->node_spanned_pages);
    
    because we don't want to overflow zone's range.
    
    But this is not necessary, since defer_init() is called like this:
    
      memmap_init_zone()
        for pfn in [start_pfn, end_pfn)
          defer_init(pfn, end_pfn)
    
    In case (pgdat->node_spanned_pages < PAGES_PER_SECTION), the loop would
    stop before calling defer_init().
    
    BTW, comparing PAGES_PER_SECTION with node_spanned_pages is not correct,
    since nr_initialised is zone based instead of node based.  Even
    node_spanned_pages is bigger than PAGES_PER_SECTION, its highest zone
    would have pages less than PAGES_PER_SECTION.
    
    Link: http://lkml.kernel.org/r/20181122094807.6985-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e0c3bc2edbbd..a6e300732ec7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -692,8 +692,6 @@ typedef struct pglist_data {
 	 * is the first PFN that needs to be initialised.
 	 */
 	unsigned long first_deferred_pfn;
-	/* Number of non-deferred pages */
-	unsigned long static_init_pgcnt;
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE

commit c999fbd3dcc6535b1e298b016665ec23ac2b0a9a
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Dec 28 00:35:55 2018 -0800

    mm/mmzone.c: make "migratetype_names" const char *
    
    Those strings are immutable in fact.
    
    Link: http://lkml.kernel.org/r/20181124090327.GA10877@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5b4bfb90fb94..e0c3bc2edbbd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -65,7 +65,7 @@ enum migratetype {
 };
 
 /* In mm/page_alloc.c; keep in sync also with show_migration_types() there */
-extern char * const migratetype_names[MIGRATE_TYPES];
+extern const char * const migratetype_names[MIGRATE_TYPES];
 
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)

commit 1c30844d2dfe272d58c8fc000960b835d13aa2ac
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Dec 28 00:35:52 2018 -0800

    mm: reclaim small amounts of memory when an external fragmentation event occurs
    
    An external fragmentation event was previously described as
    
        When the page allocator fragments memory, it records the event using
        the mm_page_alloc_extfrag event. If the fallback_order is smaller
        than a pageblock order (order-9 on 64-bit x86) then it's considered
        an event that will cause external fragmentation issues in the future.
    
    The kernel reduces the probability of such events by increasing the
    watermark sizes by calling set_recommended_min_free_kbytes early in the
    lifetime of the system.  This works reasonably well in general but if
    there are enough sparsely populated pageblocks then the problem can still
    occur as enough memory is free overall and kswapd stays asleep.
    
    This patch introduces a watermark_boost_factor sysctl that allows a zone
    watermark to be temporarily boosted when an external fragmentation causing
    events occurs.  The boosting will stall allocations that would decrease
    free memory below the boosted low watermark and kswapd is woken if the
    calling context allows to reclaim an amount of memory relative to the size
    of the high watermark and the watermark_boost_factor until the boost is
    cleared.  When kswapd finishes, it wakes kcompactd at the pageblock order
    to clean some of the pageblocks that may have been affected by the
    fragmentation event.  kswapd avoids any writeback, slab shrinkage and swap
    from reclaim context during this operation to avoid excessive system
    disruption in the name of fragmentation avoidance.  Care is taken so that
    kswapd will do normal reclaim work if the system is really low on memory.
    
    This was evaluated using the same workloads as "mm, page_alloc: Spread
    allocations across zones before introducing fragmentation".
    
    1-socket Skylake machine
    config-global-dhp__workload_thpfioscale XFS (no special madvise)
    4 fio threads, 1 THP allocating thread
    --------------------------------------
    
    4.20-rc3 extfrag events < order 9:   804694
    4.20-rc3+patch:                      408912 (49% reduction)
    4.20-rc3+patch1-4:                    18421 (98% reduction)
    
                                       4.20.0-rc3             4.20.0-rc3
                                     lowzone-v5r8             boost-v5r8
    Amean     fault-base-1      653.58 (   0.00%)      652.71 (   0.13%)
    Amean     fault-huge-1        0.00 (   0.00%)      178.93 * -99.00%*
    
                                  4.20.0-rc3             4.20.0-rc3
                                lowzone-v5r8             boost-v5r8
    Percentage huge-1        0.00 (   0.00%)        5.12 ( 100.00%)
    
    Note that external fragmentation causing events are massively reduced by
    this path whether in comparison to the previous kernel or the vanilla
    kernel.  The fault latency for huge pages appears to be increased but that
    is only because THP allocations were successful with the patch applied.
    
    1-socket Skylake machine
    global-dhp__workload_thpfioscale-madvhugepage-xfs (MADV_HUGEPAGE)
    -----------------------------------------------------------------
    
    4.20-rc3 extfrag events < order 9:  291392
    4.20-rc3+patch:                     191187 (34% reduction)
    4.20-rc3+patch1-4:                   13464 (95% reduction)
    
    thpfioscale Fault Latencies
                                       4.20.0-rc3             4.20.0-rc3
                                     lowzone-v5r8             boost-v5r8
    Min       fault-base-1      912.00 (   0.00%)      905.00 (   0.77%)
    Min       fault-huge-1      127.00 (   0.00%)      135.00 (  -6.30%)
    Amean     fault-base-1     1467.55 (   0.00%)     1481.67 (  -0.96%)
    Amean     fault-huge-1     1127.11 (   0.00%)     1063.88 *   5.61%*
    
                                  4.20.0-rc3             4.20.0-rc3
                                lowzone-v5r8             boost-v5r8
    Percentage huge-1       77.64 (   0.00%)       83.46 (   7.49%)
    
    As before, massive reduction in external fragmentation events, some jitter
    on latencies and an increase in THP allocation success rates.
    
    2-socket Haswell machine
    config-global-dhp__workload_thpfioscale XFS (no special madvise)
    4 fio threads, 5 THP allocating threads
    ----------------------------------------------------------------
    
    4.20-rc3 extfrag events < order 9:  215698
    4.20-rc3+patch:                     200210 (7% reduction)
    4.20-rc3+patch1-4:                   14263 (93% reduction)
    
                                       4.20.0-rc3             4.20.0-rc3
                                     lowzone-v5r8             boost-v5r8
    Amean     fault-base-5     1346.45 (   0.00%)     1306.87 (   2.94%)
    Amean     fault-huge-5     3418.60 (   0.00%)     1348.94 (  60.54%)
    
                                  4.20.0-rc3             4.20.0-rc3
                                lowzone-v5r8             boost-v5r8
    Percentage huge-5        0.78 (   0.00%)        7.91 ( 910.64%)
    
    There is a 93% reduction in fragmentation causing events, there is a big
    reduction in the huge page fault latency and allocation success rate is
    higher.
    
    2-socket Haswell machine
    global-dhp__workload_thpfioscale-madvhugepage-xfs (MADV_HUGEPAGE)
    -----------------------------------------------------------------
    
    4.20-rc3 extfrag events < order 9: 166352
    4.20-rc3+patch:                    147463 (11% reduction)
    4.20-rc3+patch1-4:                  11095 (93% reduction)
    
    thpfioscale Fault Latencies
                                       4.20.0-rc3             4.20.0-rc3
                                     lowzone-v5r8             boost-v5r8
    Amean     fault-base-5     6217.43 (   0.00%)     7419.67 * -19.34%*
    Amean     fault-huge-5     3163.33 (   0.00%)     3263.80 (  -3.18%)
    
                                  4.20.0-rc3             4.20.0-rc3
                                lowzone-v5r8             boost-v5r8
    Percentage huge-5       95.14 (   0.00%)       87.98 (  -7.53%)
    
    There is a large reduction in fragmentation events with some jitter around
    the latencies and success rates.  As before, the high THP allocation
    success rate does mean the system is under a lot of pressure.  However, as
    the fragmentation events are reduced, it would be expected that the
    long-term allocation success rate would be higher.
    
    Link: http://lkml.kernel.org/r/20181123114528.28802-5-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dcf1b66a96ab..5b4bfb90fb94 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -269,10 +269,10 @@ enum zone_watermarks {
 	NR_WMARK
 };
 
-#define min_wmark_pages(z) (z->_watermark[WMARK_MIN])
-#define low_wmark_pages(z) (z->_watermark[WMARK_LOW])
-#define high_wmark_pages(z) (z->_watermark[WMARK_HIGH])
-#define wmark_pages(z, i) (z->_watermark[i])
+#define min_wmark_pages(z) (z->_watermark[WMARK_MIN] + z->watermark_boost)
+#define low_wmark_pages(z) (z->_watermark[WMARK_LOW] + z->watermark_boost)
+#define high_wmark_pages(z) (z->_watermark[WMARK_HIGH] + z->watermark_boost)
+#define wmark_pages(z, i) (z->_watermark[i] + z->watermark_boost)
 
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
@@ -364,6 +364,7 @@ struct zone {
 
 	/* zone watermarks, access with *_wmark_pages(zone) macros */
 	unsigned long _watermark[NR_WMARK];
+	unsigned long watermark_boost;
 
 	unsigned long nr_reserved_highatomic;
 
@@ -890,6 +891,8 @@ static inline int is_highmem(struct zone *zone)
 struct ctl_table;
 int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
+int watermark_boost_factor_sysctl_handler(struct ctl_table *, int,
+					void __user *, size_t *, loff_t *);
 int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];

commit a921444382b49cc7fdeca3fba3e278bc09484a27
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Dec 28 00:35:44 2018 -0800

    mm: move zone watermark accesses behind an accessor
    
    This is a preparation patch only, no functional change.
    
    Link: http://lkml.kernel.org/r/20181123114528.28802-3-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Zi Yan <zi.yan@cs.rutgers.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bc0990c1f1c3..dcf1b66a96ab 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -269,9 +269,10 @@ enum zone_watermarks {
 	NR_WMARK
 };
 
-#define min_wmark_pages(z) (z->watermark[WMARK_MIN])
-#define low_wmark_pages(z) (z->watermark[WMARK_LOW])
-#define high_wmark_pages(z) (z->watermark[WMARK_HIGH])
+#define min_wmark_pages(z) (z->_watermark[WMARK_MIN])
+#define low_wmark_pages(z) (z->_watermark[WMARK_LOW])
+#define high_wmark_pages(z) (z->_watermark[WMARK_HIGH])
+#define wmark_pages(z, i) (z->_watermark[i])
 
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
@@ -362,7 +363,7 @@ struct zone {
 	/* Read-mostly fields */
 
 	/* zone watermarks, access with *_wmark_pages(zone) macros */
-	unsigned long watermark[NR_WMARK];
+	unsigned long _watermark[NR_WMARK];
 
 	unsigned long nr_reserved_highatomic;
 

commit 476567e8735a0d06225f3873a86dfa0efd95f3a5
Author: Arun KS <arunks@codeaurora.org>
Date:   Fri Dec 28 00:34:32 2018 -0800

    mm: remove managed_page_count_lock spinlock
    
    Now that totalram_pages and managed_pages are atomic varibles, no need of
    managed_page_count spinlock.  The lock had really a weak consistency
    guarantee.  It hasn't been used for anything but the update but no reader
    actually cares about all the values being updated to be in sync.
    
    Link: http://lkml.kernel.org/r/1542090790-21750-5-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a23e34e21178..bc0990c1f1c3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -428,12 +428,6 @@ struct zone {
 	 * Write access to present_pages at runtime should be protected by
 	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
 	 * present_pages should get_online_mems() to get a stable value.
-	 *
-	 * Read access to managed_pages should be safe because it's unsigned
-	 * long. Write access to zone->managed_pages and totalram_pages are
-	 * protected by managed_page_count_lock at runtime. Idealy only
-	 * adjust_managed_page_count() should be used instead of directly
-	 * touching zone->managed_pages and totalram_pages.
 	 */
 	atomic_long_t		managed_pages;
 	unsigned long		spanned_pages;

commit 9705bea5f833f4fc21d5bef5fce7348427f76ea4
Author: Arun KS <arunks@codeaurora.org>
Date:   Fri Dec 28 00:34:24 2018 -0800

    mm: convert zone->managed_pages to atomic variable
    
    totalram_pages, zone->managed_pages and totalhigh_pages updates are
    protected by managed_page_count_lock, but readers never care about it.
    Convert these variables to atomic to avoid readers potentially seeing a
    store tear.
    
    This patch converts zone->managed_pages.  Subsequent patches will convert
    totalram_panges, totalhigh_pages and eventually managed_page_count_lock
    will be removed.
    
    Main motivation was that managed_page_count_lock handling was complicating
    things.  It was discussed in length here,
    https://lore.kernel.org/patchwork/patch/995739/#1181785 So it seemes
    better to remove the lock and convert variables to atomic, with preventing
    poteintial store-to-read tearing as a bonus.
    
    Link: http://lkml.kernel.org/r/1542090790-21750-3-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Suggested-by: Michal Hocko <mhocko@suse.com>
    Suggested-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 077d797d1f60..a23e34e21178 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -435,7 +435,7 @@ struct zone {
 	 * adjust_managed_page_count() should be used instead of directly
 	 * touching zone->managed_pages and totalram_pages.
 	 */
-	unsigned long		managed_pages;
+	atomic_long_t		managed_pages;
 	unsigned long		spanned_pages;
 	unsigned long		present_pages;
 
@@ -524,6 +524,11 @@ enum pgdat_flags {
 	PGDAT_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 };
 
+static inline unsigned long zone_managed_pages(struct zone *zone)
+{
+	return (unsigned long)atomic_long_read(&zone->managed_pages);
+}
+
 static inline unsigned long zone_end_pfn(const struct zone *zone)
 {
 	return zone->zone_start_pfn + zone->spanned_pages;
@@ -820,7 +825,7 @@ static inline bool is_dev_zone(const struct zone *zone)
  */
 static inline bool managed_zone(struct zone *zone)
 {
-	return zone->managed_pages;
+	return zone_managed_pages(zone);
 }
 
 /* Returns true if a zone has memory */

commit 8d6973327ee84c2f40dd9efd8928d4a1186c96e2
Merge: 6d101ba6be2a 12526b0d6c58
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 27 10:43:24 2018 -0800

    Merge tag 'powerpc-4.21-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Mitigations for Spectre v2 on some Freescale (NXP) CPUs.
    
       - A large series adding support for pass-through of Nvidia V100 GPUs
         to guests on Power9.
    
       - Another large series to enable hardware assistance for TLB table
         walk on MPC8xx CPUs.
    
       - Some preparatory changes to our DMA code, to make way for further
         cleanups from Christoph.
    
       - Several fixes for our Transactional Memory handling discovered by
         fuzzing the signal return path.
    
       - Support for generating our system call table(s) from a text file
         like other architectures.
    
       - A fix to our page fault handler so that instead of generating a
         WARN_ON_ONCE, user accesses of kernel addresses instead print a
         ratelimited and appropriately scary warning.
    
       - A cosmetic change to make our unhandled page fault messages more
         similar to other arches and also more compact and informative.
    
       - Freescale updates from Scott:
           "Highlights include elimination of legacy clock bindings use from
            dts files, an 83xx watchdog handler, fixes to old dts interrupt
            errors, and some minor cleanup."
    
      And many clean-ups, reworks and minor fixes etc.
    
      Thanks to: Alexandre Belloni, Alexey Kardashevskiy, Andrew Donnellan,
      Aneesh Kumar K.V, Arnd Bergmann, Benjamin Herrenschmidt, Breno Leitao,
      Christian Lamparter, Christophe Leroy, Christoph Hellwig, Daniel
      Axtens, Darren Stevens, David Gibson, Diana Craciun, Dmitry V. Levin,
      Firoz Khan, Geert Uytterhoeven, Greg Kurz, Gustavo Romero, Hari
      Bathini, Joel Stanley, Kees Cook, Madhavan Srinivasan, Mahesh
      Salgaonkar, Markus Elfring, Mathieu Malaterre, Michal Suchnek, Naveen
      N. Rao, Nick Desaulniers, Oliver O'Halloran, Paul Mackerras, Ram Pai,
      Ravi Bangoria, Rob Herring, Russell Currey, Sabyasachi Gupta, Sam
      Bobroff, Satheesh Rajendran, Scott Wood, Segher Boessenkool, Stephen
      Rothwell, Tang Yuantian, Thiago Jung Bauermann, Yangtao Li, Yuantian
      Tang, Yue Haibing"
    
    * tag 'powerpc-4.21-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (201 commits)
      Revert "powerpc/fsl_pci: simplify fsl_pci_dma_set_mask"
      powerpc/zImage: Also check for stdout-path
      powerpc: Fix HMIs on big-endian with CONFIG_RELOCATABLE=y
      macintosh: Use of_node_name_{eq, prefix} for node name comparisons
      ide: Use of_node_name_eq for node name comparisons
      powerpc: Use of_node_name_eq for node name comparisons
      powerpc/pseries/pmem: Convert to %pOFn instead of device_node.name
      powerpc/mm: Remove very old comment in hash-4k.h
      powerpc/pseries: Fix node leak in update_lmb_associativity_index()
      powerpc/configs/85xx: Enable CONFIG_DEBUG_KERNEL
      powerpc/dts/fsl: Fix dtc-flagged interrupt errors
      clk: qoriq: add more compatibles strings
      powerpc/fsl: Use new clockgen binding
      powerpc/83xx: handle machine check caused by watchdog timer
      powerpc/fsl-rio: fix spelling mistake "reserverd" -> "reserved"
      powerpc/fsl_pci: simplify fsl_pci_dma_set_mask
      arch/powerpc/fsl_rmu: Use dma_zalloc_coherent
      vfio_pci: Add NVIDIA GV100GL [Tesla V100 SXM2] subdriver
      vfio_pci: Allow regions to add own capabilities
      vfio_pci: Allow mapping extra regions
      ...

commit 25078dc1f74be16b858e914f52cc8f4d03c2271a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 16 17:53:49 2018 +0100

    powerpc: use mm zones more sensibly
    
    Powerpc has somewhat odd usage where ZONE_DMA is used for all memory on
    common 64-bit configfs, and ZONE_DMA32 is used for 31-bit schemes.
    
    Move to a scheme closer to what other architectures use (and I dare to
    say the intent of the system):
    
     - ZONE_DMA: optionally for memory < 31-bit (64-bit embedded only)
     - ZONE_NORMAL: everything addressable by the kernel
     - ZONE_HIGHMEM: memory > 32-bit for 32-bit kernels
    
    Also provide information on how ZONE_DMA is used by defining
    ARCH_ZONE_DMA_BITS.
    
    Contains various fixes from Benjamin Herrenschmidt.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 847705a6d0ec..e2d01ccd071d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -314,7 +314,7 @@ enum zone_type {
 	 * Architecture		Limit
 	 * ---------------------------
 	 * parisc, ia64, sparc	<4G
-	 * s390			<2G
+	 * s390, powerpc	<2G
 	 * arm			Various
 	 * alpha		Unlimited or 0-16MB.
 	 *

commit 9def36e0fa9a0d9c5393c039db59f1f2d3a388b3
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Dec 14 14:16:57 2018 -0800

    mm/sparse: add common helper to mark all memblocks present
    
    Presently the arches arm64, arm and sh have a function which loops
    through each memblock and calls memory present.  riscv will require a
    similar function.
    
    Introduce a common memblocks_present() function that can be used by all
    the arches.  Subsequent patches will cleanup the arches that make use of
    this.
    
    Link: http://lkml.kernel.org/r/20181107205433.3875-3-logang@deltatee.com
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 847705a6d0ec..db023a92f3a4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -783,6 +783,12 @@ void memory_present(int nid, unsigned long start, unsigned long end);
 static inline void memory_present(int nid, unsigned long start, unsigned long end) {}
 #endif
 
+#if defined(CONFIG_SPARSEMEM)
+void memblocks_present(void);
+#else
+static inline void memblocks_present(void) {}
+#endif
+
 #ifdef CONFIG_HAVE_MEMORYLESS_NODES
 int local_memory_node(int node_id);
 #else

commit b4a991ec584b3ce333342c431c3cc4fef8a690d7
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:07:40 2018 -0700

    mm: remove CONFIG_NO_BOOTMEM
    
    All achitectures select NO_BOOTMEM which essentially becomes 'Y' for any
    kernel configuration and therefore it can be removed.
    
    [alexander.h.duyck@linux.intel.com: remove now defunct NO_BOOTMEM from depends list for deferred init]
      Link: http://lkml.kernel.org/r/20180925201814.3576.15105.stgit@localhost.localdomain
    Link: http://lkml.kernel.org/r/1536927045-23536-3-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9f0caccd5833..847705a6d0ec 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -633,9 +633,6 @@ typedef struct pglist_data {
 	struct page_ext *node_page_ext;
 #endif
 #endif
-#ifndef CONFIG_NO_BOOTMEM
-	struct bootmem_data *bdata;
-#endif
 #if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
 	/*
 	 * Must be held any time you expect node_start_pfn, node_present_pages
@@ -869,7 +866,7 @@ static inline int is_highmem_idx(enum zone_type idx)
 }
 
 /**
- * is_highmem - helper function to quickly check if a struct zone is a 
+ * is_highmem - helper function to quickly check if a struct zone is a
  *              highmem zone or not.  This is an attempt to keep references
  *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
  * @zone - pointer to struct zone variable

commit 68d48e6a2df575b935edd420396c3cb8b6aa6ad3
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:39 2018 -0700

    mm: workingset: add vmstat counter for shadow nodes
    
    Make it easier to catch bugs in the shadow node shrinker by adding a
    counter for the shadow nodes in circulation.
    
    [akpm@linux-foundation.org: assert that irqs are disabled, for __inc_lruvec_page_state()]
    [akpm@linux-foundation.org: s/WARN_ON_ONCE/VM_WARN_ON_ONCE/, per Johannes]
    Link: http://lkml.kernel.org/r/20181009184732.762-4-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ba51d5bf7af1..9f0caccd5833 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -161,6 +161,7 @@ enum node_stat_item {
 	NR_SLAB_UNRECLAIMABLE,
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
+	WORKINGSET_NODES,
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
 	WORKINGSET_RESTORE,

commit 1899ad18c6072d689896badafb81267b0a1092a4
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:04 2018 -0700

    mm: workingset: tell cache transitions from workingset thrashing
    
    Refaults happen during transitions between workingsets as well as in-place
    thrashing.  Knowing the difference between the two has a range of
    applications, including measuring the impact of memory shortage on the
    system performance, as well as the ability to smarter balance pressure
    between the filesystem cache and the swap-backed workingset.
    
    During workingset transitions, inactive cache refaults and pushes out
    established active cache.  When that active cache isn't stale, however,
    and also ends up refaulting, that's bonafide thrashing.
    
    Introduce a new page flag that tells on eviction whether the page has been
    active or not in its lifetime.  This bit is then stored in the shadow
    entry, to classify refaults as transitioning or thrashing.
    
    How many page->flags does this leave us with on 32-bit?
    
            20 bits are always page flags
    
            21 if you have an MMU
    
            23 with the zone bits for DMA, Normal, HighMem, Movable
    
            29 with the sparsemem section bits
    
            30 if PAE is enabled
    
            31 with this patch.
    
    So on 32-bit PAE, that leaves 1 bit for distinguishing two NUMA nodes.  If
    that's not enough, the system can switch to discontigmem and re-gain the 6
    or 7 sparsemem section bits.
    
    Link: http://lkml.kernel.org/r/20180828172258.3185-3-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7bbeba21f6a3..ba51d5bf7af1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -163,6 +163,7 @@ enum node_stat_item {
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
+	WORKINGSET_RESTORE,
 	WORKINGSET_NODERECLAIM,
 	NR_ANON_MAPPED,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.

commit b29940c1abd7a4c3abeb926df0a5ec84d6902d47
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Fri Oct 26 15:05:46 2018 -0700

    mm: rename and change semantics of nr_indirectly_reclaimable_bytes
    
    The vmstat counter NR_INDIRECTLY_RECLAIMABLE_BYTES was introduced by
    commit eb59254608bc ("mm: introduce NR_INDIRECTLY_RECLAIMABLE_BYTES") with
    the goal of accounting objects that can be reclaimed, but cannot be
    allocated via a SLAB_RECLAIM_ACCOUNT cache.  This is now possible via
    kmalloc() with __GFP_RECLAIMABLE flag, and the dcache external names user
    is converted.
    
    The counter is however still useful for accounting direct page allocations
    (i.e.  not slab) with a shrinker, such as the ION page pool.  So keep it,
    and:
    
    - change granularity to pages to be more like other counters; sub-page
      allocations should be able to use kmalloc
    - rename the counter to NR_KERNEL_MISC_RECLAIMABLE
    - expose the counter again in vmstat as "nr_kernel_misc_reclaimable"; we can
      again remove the check for not printing "hidden" counters
    
    Link: http://lkml.kernel.org/r/20180731090649.16028-5-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Cc: Vijayanand Jitta <vjitta@codeaurora.org>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d4b0c79d2924..7bbeba21f6a3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -180,7 +180,7 @@ enum node_stat_item {
 	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
-	NR_INDIRECTLY_RECLAIMABLE_BYTES, /* measured in bytes */
+	NR_KERNEL_MISC_RECLAIMABLE,	/* reclaimable non-slab kernel pages */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit e054637597ba36d3729ba6a3a3dd7aad8e2a3003
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Sat Oct 6 16:53:19 2018 +0530

    mm, sched/numa: Remove remaining traces of NUMA rate-limiting
    
    Remove the leftover pglist_data::numabalancing_migrate_lock and its
    initialization, we stopped using this lock with:
    
      efaffc5e40ae ("mm, sched/numa: Remove rate-limiting of automatic NUMA balancing migration")
    
    [ mingo: Rewrote the changelog. ]
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Linux-MM <linux-mm@kvack.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1538824999-31230-1-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3f4c0b167333..d4b0c79d2924 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -667,10 +667,6 @@ typedef struct pglist_data {
 	enum zone_type kcompactd_classzone_idx;
 	wait_queue_head_t kcompactd_wait;
 	struct task_struct *kcompactd;
-#endif
-#ifdef CONFIG_NUMA_BALANCING
-	/* Lock serializing the migrate rate limiting window */
-	spinlock_t numabalancing_migrate_lock;
 #endif
 	/*
 	 * This is a per-node reserve of pages that are not available

commit efaffc5e40aeced0bcb497ed7a0a5b8c14abfcdf
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Mon Oct 1 11:05:24 2018 +0100

    mm, sched/numa: Remove rate-limiting of automatic NUMA balancing migration
    
    Rate limiting of page migrations due to automatic NUMA balancing was
    introduced to mitigate the worst-case scenario of migrating at high
    frequency due to false sharing or slowly ping-ponging between nodes.
    Since then, a lot of effort was spent on correctly identifying these
    pages and avoiding unnecessary migrations and the safety net may no longer
    be required.
    
    Jirka Hladky reported a regression in 4.17 due to a scheduler patch that
    avoids spreading STREAM tasks wide prematurely. However, once the task
    was properly placed, it delayed migrating the memory due to rate limiting.
    Increasing the limit fixed the problem for him.
    
    Currently, the limit is hard-coded and does not account for the real
    capabilities of the hardware. Even if an estimate was attempted, it would
    not properly account for the number of memory controllers and it could
    not account for the amount of bandwidth used for normal accesses. Rather
    than fudging, this patch simply eliminates the rate limiting.
    
    However, Jirka reports that a STREAM configuration using multiple
    processes achieved similar performance to 4.16. In local tests, this patch
    improved performance of STREAM relative to the baseline but it is somewhat
    machine-dependent. Most workloads show little or not performance difference
    implying that there is not a heavily reliance on the throttling mechanism
    and it is safe to remove.
    
    STREAM on 2-socket machine
                             4.19.0-rc5             4.19.0-rc5
                             numab-v1r1       noratelimit-v1r1
    MB/sec copy     43298.52 (   0.00%)    44673.38 (   3.18%)
    MB/sec scale    30115.06 (   0.00%)    31293.06 (   3.91%)
    MB/sec add      32825.12 (   0.00%)    34883.62 (   6.27%)
    MB/sec triad    32549.52 (   0.00%)    34906.60 (   7.24%
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jirka Hladky <jhladky@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Linux-MM <linux-mm@kvack.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20181001100525.29789-2-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1e22d96734e0..3f4c0b167333 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -671,12 +671,6 @@ typedef struct pglist_data {
 #ifdef CONFIG_NUMA_BALANCING
 	/* Lock serializing the migrate rate limiting window */
 	spinlock_t numabalancing_migrate_lock;
-
-	/* Rate limiting time interval */
-	unsigned long numabalancing_migrate_next_window;
-
-	/* Number of pages migrated during the rate limiting time interval */
-	unsigned long numabalancing_migrate_nr_pages;
 #endif
 	/*
 	 * This is a per-node reserve of pages that are not available

commit c1093b746c0576ed81c4d568d1e39cab651d37e6
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Tue Aug 21 21:53:32 2018 -0700

    mm: access zone->node via zone_to_nid() and zone_set_nid()
    
    zone->node is configured only when CONFIG_NUMA=y, so it is a good idea to
    have inline functions to access this field in order to avoid ifdef's in c
    files.
    
    Link: http://lkml.kernel.org/r/20180730101757.28058-3-osalvador@techadventures.net
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Pasha Tatashin <Pavel.Tatashin@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3d4577a3ae7e..1e22d96734e0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -834,6 +834,25 @@ static inline bool populated_zone(struct zone *zone)
 	return zone->present_pages;
 }
 
+#ifdef CONFIG_NUMA
+static inline int zone_to_nid(struct zone *zone)
+{
+	return zone->node;
+}
+
+static inline void zone_set_nid(struct zone *zone, int nid)
+{
+	zone->node = nid;
+}
+#else
+static inline int zone_to_nid(struct zone *zone)
+{
+	return 0;
+}
+
+static inline void zone_set_nid(struct zone *zone, int nid) {}
+#endif
+
 extern int movable_zone;
 
 #ifdef CONFIG_HIGHMEM
@@ -949,12 +968,7 @@ static inline int zonelist_zone_idx(struct zoneref *zoneref)
 
 static inline int zonelist_node_idx(struct zoneref *zoneref)
 {
-#ifdef CONFIG_NUMA
-	/* zone_to_nid not available in this context */
-	return zoneref->zone->node;
-#else
-	return 0;
-#endif /* CONFIG_NUMA */
+	return zone_to_nid(zoneref->zone);
 }
 
 struct zoneref *__next_zones_zonelist(struct zoneref *z,

commit 89696701ea847786f88ccc1c580fb4c3c20c4a3a
Author: Oscar Salvador <osalvador@suse.de>
Date:   Tue Aug 21 21:53:24 2018 -0700

    mm: remove zone_id() and make use of zone_idx() in is_dev_zone()
    
    is_dev_zone() is using zone_id() to check if the zone is ZONE_DEVICE.
    zone_id() looks pretty much the same as zone_idx(), and while the use of
    zone_idx() is quite spread in the kernel, zone_id() is only being used by
    is_dev_zone().
    
    This patch removes zone_id() and makes is_dev_zone() use zone_idx() to
    check the zone, so we do not have two things with the same functionality
    around.
    
    Link: http://lkml.kernel.org/r/20180730133718.28683-1-osalvador@techadventures.net
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Pasha Tatashin <Pavel.Tatashin@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 32699b2dc52a..3d4577a3ae7e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -755,25 +755,6 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 	return !pgdat->node_start_pfn && !pgdat->node_spanned_pages;
 }
 
-static inline int zone_id(const struct zone *zone)
-{
-	struct pglist_data *pgdat = zone->zone_pgdat;
-
-	return zone - pgdat->node_zones;
-}
-
-#ifdef CONFIG_ZONE_DEVICE
-static inline bool is_dev_zone(const struct zone *zone)
-{
-	return zone_id(zone) == ZONE_DEVICE;
-}
-#else
-static inline bool is_dev_zone(const struct zone *zone)
-{
-	return false;
-}
-#endif
-
 #include <linux/memory_hotplug.h>
 
 void build_all_zonelists(pg_data_t *pgdat);
@@ -824,6 +805,18 @@ static inline int local_memory_node(int node_id) { return node_id; };
  */
 #define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
 
+#ifdef CONFIG_ZONE_DEVICE
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return zone_idx(zone) == ZONE_DEVICE;
+}
+#else
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return false;
+}
+#endif
+
 /*
  * Returns true if a zone has pages managed by the buddy allocator.
  * All the reclaim decisions have to use this function rather than

commit d3cda2337bbc9edd2a26b83cb00eaa8c048ff274
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Apr 10 16:30:11 2018 -0700

    mm/page_alloc: don't reserve ZONE_HIGHMEM for ZONE_MOVABLE request
    
    Freepage on ZONE_HIGHMEM doesn't work for kernel memory so it's not that
    important to reserve.  When ZONE_MOVABLE is used, this problem would
    theorectically cause to decrease usable memory for GFP_HIGHUSER_MOVABLE
    allocation request which is mainly used for page cache and anon page
    allocation.  So, fix it by setting 0 to
    sysctl_lowmem_reserve_ratio[ZONE_HIGHMEM].
    
    And, defining sysctl_lowmem_reserve_ratio array by MAX_NR_ZONES - 1 size
    makes code complex.  For example, if there is highmem system, following
    reserve ratio is activated for *NORMAL ZONE* which would be easyily
    misleading people.
    
     #ifdef CONFIG_HIGHMEM
     32
     #endif
    
    This patch also fixes this situation by defining
    sysctl_lowmem_reserve_ratio array by MAX_NR_ZONES and place "#ifdef" to
    right place.
    
    Link: http://lkml.kernel.org/r/1504672525-17915-1-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Tested-by: Tony Lindgren <tony@atomide.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <linux-api@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a0c9e45a859a..32699b2dc52a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -885,7 +885,7 @@ int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
-extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1];
+extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];
 int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int,

commit eb59254608bc1d42c4c6afdcdce9c0d3ce02b318
Author: Roman Gushchin <guro@fb.com>
Date:   Tue Apr 10 16:27:36 2018 -0700

    mm: introduce NR_INDIRECTLY_RECLAIMABLE_BYTES
    
    Patch series "indirectly reclaimable memory", v2.
    
    This patchset introduces the concept of indirectly reclaimable memory
    and applies it to fix the issue of when a big number of dentries with
    external names can significantly affect the MemAvailable value.
    
    This patch (of 3):
    
    Introduce a concept of indirectly reclaimable memory and adds the
    corresponding memory counter and /proc/vmstat item.
    
    Indirectly reclaimable memory is any sort of memory, used by the kernel
    (except of reclaimable slabs), which is actually reclaimable, i.e.  will
    be released under memory pressure.
    
    The counter is in bytes, as it's not always possible to count such
    objects in pages.  The name contains BYTES by analogy to
    NR_KERNEL_STACK_KB.
    
    Link: http://lkml.kernel.org/r/20180305133743.12746-2-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f11ae29005f1..a0c9e45a859a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -180,6 +180,7 @@ enum node_stat_item {
 	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
+	NR_INDIRECTLY_RECLAIMABLE_BYTES, /* measured in bytes */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit 5ecd9d403ad081ed2de7b118c1e96124d4e0ba6c
Author: David Rientjes <rientjes@google.com>
Date:   Thu Apr 5 16:25:16 2018 -0700

    mm, page_alloc: wakeup kcompactd even if kswapd cannot free more memory
    
    Kswapd will not wakeup if per-zone watermarks are not failing or if too
    many previous attempts at background reclaim have failed.
    
    This can be true if there is a lot of free memory available.  For high-
    order allocations, kswapd is responsible for waking up kcompactd for
    background compaction.  If the zone is not below its watermarks or
    reclaim has recently failed (lots of free memory, nothing left to
    reclaim), kcompactd does not get woken up.
    
    When __GFP_DIRECT_RECLAIM is not allowed, allow kcompactd to still be
    woken up even if kswapd will not reclaim.  This allows high-order
    allocations, such as thp, to still trigger background compaction even
    when the zone has an abundance of free memory.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1803111659420.209721@chino.kir.corp.google.com
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5d935411d3c4..f11ae29005f1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -776,7 +776,8 @@ static inline bool is_dev_zone(const struct zone *zone)
 #include <linux/memory_hotplug.h>
 
 void build_all_zonelists(pg_data_t *pgdat);
-void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
+void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
+		   enum zone_type classzone_idx);
 bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 			 int classzone_idx, unsigned int alloc_flags,
 			 long free_pages);

commit 3a2d7fa8a3d5ae740bd0c21d933acc6220857ed0
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Apr 5 16:22:27 2018 -0700

    mm: disable interrupts while initializing deferred pages
    
    Vlastimil Babka reported about a window issue during which when deferred
    pages are initialized, and the current version of on-demand
    initialization is finished, allocations may fail.  While this is highly
    unlikely scenario, since this kind of allocation request must be large,
    and must come from interrupt handler, we still want to cover it.
    
    We solve this by initializing deferred pages with interrupts disabled,
    and holding node_size_lock spin lock while pages in the node are being
    initialized.  The on-demand deferred page initialization that comes
    later will use the same lock, and thus synchronize with
    deferred_init_memmap().
    
    It is unlikely for threads that initialize deferred pages to be
    interrupted.  They run soon after smp_init(), but before modules are
    initialized, and long before user space programs.  This is why there is
    no adverse effect of having these threads running with interrupts
    disabled.
    
    [pasha.tatashin@oracle.com: v6]
      Link: http://lkml.kernel.org/r/20180313182355.17669-2-pasha.tatashin@oracle.com
    Link: http://lkml.kernel.org/r/20180309220807.24961-2-pasha.tatashin@oracle.com
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven Sistare <steven.sistare@oracle.com>
    Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Gioh Kim <gi-oh.kim@profitbricks.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Miles Chen <miles.chen@mediatek.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a2db4576e499..5d935411d3c4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -633,14 +633,15 @@ typedef struct pglist_data {
 #ifndef CONFIG_NO_BOOTMEM
 	struct bootmem_data *bdata;
 #endif
-#ifdef CONFIG_MEMORY_HOTPLUG
+#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
 	/*
 	 * Must be held any time you expect node_start_pfn, node_present_pages
 	 * or node_spanned_pages stay constant.  Holding this will also
 	 * guarantee that any pfn_valid() stays that way.
 	 *
 	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
-	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG.
+	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG
+	 * or CONFIG_DEFERRED_STRUCT_PAGE_INIT.
 	 *
 	 * Nests above zone->lock and zone->span_seqlock
 	 */

commit fc5d1073cae299de4517755a910df4f12a6a438f
Author: David Rientjes <rientjes@google.com>
Date:   Mon Mar 26 23:27:21 2018 -0700

    x86/mm/32: Remove unused node_memmap_size_bytes() & CONFIG_NEED_NODE_MEMMAP_SIZE logic
    
    node_memmap_size_bytes() has been unused since the v3.9 kernel, so remove it.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mm@kvack.org
    Fixes: f03574f2d5b2 ("x86-32, mm: Rip out x86_32 NUMA remapping code")
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1803262325540.256524@chino.kir.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7522a6987595..a2db4576e499 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -816,10 +816,6 @@ int local_memory_node(int node_id);
 static inline int local_memory_node(int node_id) { return node_id; };
 #endif
 
-#ifdef CONFIG_NEED_NODE_MEMMAP_SIZE
-unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
-#endif
-
 /*
  * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
  */
@@ -1289,7 +1285,6 @@ struct mminit_pfnnid_cache {
 #endif
 
 void memory_present(int nid, unsigned long start, unsigned long end);
-unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 
 /*
  * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we

commit def9b71ee651a6fee93a10734b94f93a69cdb2d4
Author: Petr Tesarik <ptesarik@suse.com>
Date:   Wed Jan 31 16:20:26 2018 -0800

    include/linux/mmzone.h: fix explanation of lower bits in the SPARSEMEM mem_map pointer
    
    The comment is confusing.  On the one hand, it refers to 32-bit
    alignment (struct page alignment on 32-bit platforms), but this would
    only guarantee that the 2 lowest bits must be zero.  On the other hand,
    it claims that at least 3 bits are available, and 3 bits are actually
    used.
    
    This is not broken, because there is a stronger alignment guarantee,
    just less obvious.  Let's fix the comment to make it clear how many bits
    are available and why.
    
    Although memmap arrays are allocated in various places, the resulting
    pointer is encoded eventually, so I am adding a BUG_ON() here to enforce
    at runtime that all expected bits are indeed available.
    
    I have also added a BUILD_BUG_ON to check that PFN_SECTION_SHIFT is
    sufficient, because this part of the calculation can be easily checked
    at build time.
    
    [ptesarik@suse.com: v2]
      Link: http://lkml.kernel.org/r/20180125100516.589ea6af@ezekiel.suse.cz
    Link: http://lkml.kernel.org/r/20180119080908.3a662e6f@ezekiel.suse.cz
    Signed-off-by: Petr Tesarik <ptesarik@suse.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kemi Wang <kemi.wang@intel.com>
    Cc: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 67f2e3c38939..7522a6987595 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1166,8 +1166,16 @@ extern unsigned long usemap_size(void);
 
 /*
  * We use the lower bits of the mem_map pointer to store
- * a little bit of information.  There should be at least
- * 3 bits here due to 32-bit alignment.
+ * a little bit of information.  The pointer is calculated
+ * as mem_map - section_nr_to_pfn(pnum).  The result is
+ * aligned to the minimum alignment of the two values:
+ *   1. All mem_map arrays are page-aligned.
+ *   2. section_nr_to_pfn() always clears PFN_SECTION_SHIFT
+ *      lowest bits.  PFN_SECTION_SHIFT is arch-specific
+ *      (equal SECTION_SIZE_BITS - PAGE_SHIFT), and the
+ *      worst combination is powerpc with 256k pages,
+ *      which results in PFN_SECTION_SHIFT equal 6.
+ * To sum it up, at least 6 bits are available.
  */
 #define	SECTION_MARKED_PRESENT	(1UL<<0)
 #define SECTION_HAS_MEM_MAP	(1UL<<1)

commit d135e5750205a21a212a19dbb05aeb339e2cbea7
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Wed Nov 15 17:38:41 2017 -0800

    mm/page_alloc.c: broken deferred calculation
    
    In reset_deferred_meminit() we determine number of pages that must not
    be deferred.  We initialize pages for at least 2G of memory, but also
    pages for reserved memory in this node.
    
    The reserved memory is determined in this function:
    memblock_reserved_memory_within(), which operates over physical
    addresses, and returns size in bytes.  However, reset_deferred_meminit()
    assumes that that this function operates with pfns, and returns page
    count.
    
    The result is that in the best case machine boots slower than expected
    due to initializing more pages than needed in single thread, and in the
    worst case panics because fewer than needed pages are initialized early.
    
    Link: http://lkml.kernel.org/r/20171021011707.15191-1-pasha.tatashin@oracle.com
    Fixes: 864b9a393dcb ("mm: consider memblock reservations for deferred memory initialization sizing")
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 39ccaa9c428b..67f2e3c38939 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -700,7 +700,8 @@ typedef struct pglist_data {
 	 * is the first PFN that needs to be initialised.
 	 */
 	unsigned long first_deferred_pfn;
-	unsigned long static_init_size;
+	/* Number of non-deferred pages */
+	unsigned long static_init_pgcnt;
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE

commit 3a50d14d0df5776e002a8683a290c87eeac93a21
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Wed Nov 15 17:34:15 2017 -0800

    mm: remove unused pgdat->inactive_ratio
    
    Since commit 59dc76b0d4df ("mm: vmscan: reduce size of inactive file
    list") 'pgdat->inactive_ratio' is not used, except for printing
    "node_inactive_ratio: 0" in /proc/zoneinfo output.
    
    Remove it.
    
    Link: http://lkml.kernel.org/r/20171003152611.27483-1-aryabinin@virtuozzo.com
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a507f43ad221..39ccaa9c428b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -712,12 +712,6 @@ typedef struct pglist_data {
 	/* Fields commonly accessed by the page reclaim scanner */
 	struct lruvec		lruvec;
 
-	/*
-	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
-	 * this node's LRU.  Maintained by the pageout code.
-	 */
-	unsigned int inactive_ratio;
-
 	unsigned long		flags;
 
 	ZONE_PADDING(_pad2_)

commit b3d9a136815ca9284ade2a897a3b7d2b0084c33c
Merge: c7da092a1f24 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:53:06 2017 +0100

    Merge branch 'linus' into x86/asm, to pick up fixes and resolve conflicts
    
    Conflicts:
            arch/x86/kernel/cpu/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c8f89417740b..c9c4a81b9767 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_MMZONE_H
 #define _LINUX_MMZONE_H
 

commit 83e3c48729d9ebb7af5a31a504f3fd6aff0348c4
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Sep 29 17:08:16 2017 +0300

    mm/sparsemem: Allocate mem_section at runtime for CONFIG_SPARSEMEM_EXTREME=y
    
    Size of the mem_section[] array depends on the size of the physical address space.
    
    In preparation for boot-time switching between paging modes on x86-64
    we need to make the allocation of mem_section[] dynamic, because otherwise
    we waste a lot of RAM: with CONFIG_NODE_SHIFT=10, mem_section[] size is 32kB
    for 4-level paging and 2MB for 5-level paging mode.
    
    The patch allocates the array on the first call to sparse_memory_present_with_active_regions().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170929140821.37654-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c8f89417740b..e796edf1296f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1150,13 +1150,17 @@ struct mem_section {
 #define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)
 
 #ifdef CONFIG_SPARSEMEM_EXTREME
-extern struct mem_section *mem_section[NR_SECTION_ROOTS];
+extern struct mem_section **mem_section;
 #else
 extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
 #endif
 
 static inline struct mem_section *__nr_to_section(unsigned long nr)
 {
+#ifdef CONFIG_SPARSEMEM_EXTREME
+	if (!mem_section)
+		return NULL;
+#endif
 	if (!mem_section[SECTION_NR_TO_ROOT(nr)])
 		return NULL;
 	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];

commit 1dd2bfc86818ddbc95f98e312e7704350223fd7d
Author: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
Date:   Tue Oct 3 16:16:29 2017 -0700

    mm/memory_hotplug: change pfn_to_section_nr/section_nr_to_pfn macro to inline function
    
    pfn_to_section_nr() and section_nr_to_pfn() are defined as macro.
    pfn_to_section_nr() has no issue even if it is defined as macro.  But
    section_nr_to_pfn() has overflow issue if sec is defined as int.
    
    section_nr_to_pfn() just shifts sec by PFN_SECTION_SHIFT.  If sec is
    defined as unsigned long, section_nr_to_pfn() returns pfn as 64 bit value.
    But if sec is defined as int, section_nr_to_pfn() returns pfn as 32 bit
    value.
    
    __remove_section() calculates start_pfn using section_nr_to_pfn() and
    scn_nr defined as int.  So if hot-removed memory address is over 16TB,
    overflow issue occurs and section_nr_to_pfn() does not calculate correct
    pfn.
    
    To make callers use proper arg, the patch changes the macros to inline
    functions.
    
    Fixes: 815121d2b5cd ("memory_hotplug: clear zone when removing the memory")
    Link: http://lkml.kernel.org/r/e643a387-e573-6bbf-d418-c60c8ee3d15e@gmail.com
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 356a814e7c8e..c8f89417740b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1094,8 +1094,14 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #error Allocator MAX_ORDER exceeds SECTION_SIZE
 #endif
 
-#define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
-#define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
+static inline unsigned long pfn_to_section_nr(unsigned long pfn)
+{
+	return pfn >> PFN_SECTION_SHIFT;
+}
+static inline unsigned long section_nr_to_pfn(unsigned long sec)
+{
+	return sec << PFN_SECTION_SHIFT;
+}
 
 #define SECTION_ALIGN_UP(pfn)	(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)
 #define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)

commit 1d90ca897cb05cf38bd62f36756d219e02913b7d
Author: Kemi Wang <kemi.wang@intel.com>
Date:   Fri Sep 8 16:12:52 2017 -0700

    mm: update NUMA counter threshold size
    
    There is significant overhead in cache bouncing caused by zone counters
    (NUMA associated counters) update in parallel in multi-threaded page
    allocation (suggested by Dave Hansen).
    
    This patch updates NUMA counter threshold to a fixed size of MAX_U16 - 2,
    as a small threshold greatly increases the update frequency of the global
    counter from local per cpu counter(suggested by Ying Huang).
    
    The rationality is that these statistics counters don't affect the
    kernel's decision, unlike other VM counters, so it's not a problem to use
    a large threshold.
    
    With this patchset, we see 31.3% drop of CPU cycles(537-->369) for per
    single page allocation and reclaim on Jesper's page_bench03 benchmark.
    
    Benchmark provided by Jesper D Brouer(increase loop times to 10000000):
    https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/
    bench
    
     Threshold   CPU cycles    Throughput(88 threads)
         32          799         241760478
         64          640         301628829
         125         537         358906028 <==> system by default (base)
         256         468         412397590
         512         428         450550704
         4096        399         482520943
         20000       394         489009617
         30000       395         488017817
         65533       369(-31.3%) 521661345(+45.3%) <==> with this patchset
         N/A         342(-36.3%) 562900157(+56.8%) <==> disable zone_statistics
    
    Link: http://lkml.kernel.org/r/1503568801-21305-3-git-send-email-kemi.wang@intel.com
    Signed-off-by: Kemi Wang <kemi.wang@intel.com>
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Suggested-by: Ying Huang <ying.huang@intel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e65d91c02e30..356a814e7c8e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -282,8 +282,7 @@ struct per_cpu_pageset {
 	struct per_cpu_pages pcp;
 #ifdef CONFIG_NUMA
 	s8 expire;
-	s8 numa_stat_threshold;
-	s8 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];
+	u16 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];
 #endif
 #ifdef CONFIG_SMP
 	s8 stat_threshold;

commit 3a321d2a3dde812142e06ab5c2f062ed860182a5
Author: Kemi Wang <kemi.wang@intel.com>
Date:   Fri Sep 8 16:12:48 2017 -0700

    mm: change the call sites of numa statistics items
    
    Patch series "Separate NUMA statistics from zone statistics", v2.
    
    Each page allocation updates a set of per-zone statistics with a call to
    zone_statistics().  As discussed in 2017 MM summit, these are a
    substantial source of overhead in the page allocator and are very rarely
    consumed.  This significant overhead in cache bouncing caused by zone
    counters (NUMA associated counters) update in parallel in multi-threaded
    page allocation (pointed out by Dave Hansen).
    
    A link to the MM summit slides:
      http://people.netfilter.org/hawk/presentations/MM-summit2017/MM-summit2017-JesperBrouer.pdf
    
    To mitigate this overhead, this patchset separates NUMA statistics from
    zone statistics framework, and update NUMA counter threshold to a fixed
    size of MAX_U16 - 2, as a small threshold greatly increases the update
    frequency of the global counter from local per cpu counter (suggested by
    Ying Huang).  The rationality is that these statistics counters don't
    need to be read often, unlike other VM counters, so it's not a problem
    to use a large threshold and make readers more expensive.
    
    With this patchset, we see 31.3% drop of CPU cycles(537-->369, see
    below) for per single page allocation and reclaim on Jesper's
    page_bench03 benchmark.  Meanwhile, this patchset keeps the same style
    of virtual memory statistics with little end-user-visible effects (only
    move the numa stats to show behind zone page stats, see the first patch
    for details).
    
    I did an experiment of single page allocation and reclaim concurrently
    using Jesper's page_bench03 benchmark on a 2-Socket Broadwell-based
    server (88 processors with 126G memory) with different size of threshold
    of pcp counter.
    
    Benchmark provided by Jesper D Brouer(increase loop times to 10000000):
      https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/bench
    
       Threshold   CPU cycles    Throughput(88 threads)
          32        799         241760478
          64        640         301628829
          125       537         358906028 <==> system by default
          256       468         412397590
          512       428         450550704
          4096      399         482520943
          20000     394         489009617
          30000     395         488017817
          65533     369(-31.3%) 521661345(+45.3%) <==> with this patchset
          N/A       342(-36.3%) 562900157(+56.8%) <==> disable zone_statistics
    
    This patch (of 3):
    
    In this patch, NUMA statistics is separated from zone statistics
    framework, all the call sites of NUMA stats are changed to use
    numa-stats-specific functions, it does not have any functionality change
    except that the number of NUMA stats is shown behind zone page stats
    when users *read* the zone info.
    
    E.g. cat /proc/zoneinfo
        ***Base***                           ***With this patch***
    nr_free_pages 3976                         nr_free_pages 3976
    nr_zone_inactive_anon 0                    nr_zone_inactive_anon 0
    nr_zone_active_anon 0                      nr_zone_active_anon 0
    nr_zone_inactive_file 0                    nr_zone_inactive_file 0
    nr_zone_active_file 0                      nr_zone_active_file 0
    nr_zone_unevictable 0                      nr_zone_unevictable 0
    nr_zone_write_pending 0                    nr_zone_write_pending 0
    nr_mlock     0                             nr_mlock     0
    nr_page_table_pages 0                      nr_page_table_pages 0
    nr_kernel_stack 0                          nr_kernel_stack 0
    nr_bounce    0                             nr_bounce    0
    nr_zspages   0                             nr_zspages   0
    numa_hit 0                                *nr_free_cma  0*
    numa_miss 0                                numa_hit     0
    numa_foreign 0                             numa_miss    0
    numa_interleave 0                          numa_foreign 0
    numa_local   0                             numa_interleave 0
    numa_other   0                             numa_local   0
    *nr_free_cma 0*                            numa_other 0
        ...                                        ...
    vm stats threshold: 10                     vm stats threshold: 10
        ...                                        ...
    
    The next patch updates the numa stats counter size and threshold.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/1503568801-21305-2-git-send-email-kemi.wang@intel.com
    Signed-off-by: Kemi Wang <kemi.wang@intel.com>
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Ying Huang <ying.huang@intel.com>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e7e92c8f4883..e65d91c02e30 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -114,6 +114,20 @@ struct zone_padding {
 #define ZONE_PADDING(name)
 #endif
 
+#ifdef CONFIG_NUMA
+enum numa_stat_item {
+	NUMA_HIT,		/* allocated in intended node */
+	NUMA_MISS,		/* allocated in non intended node */
+	NUMA_FOREIGN,		/* was intended here, hit elsewhere */
+	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
+	NUMA_LOCAL,		/* allocation from local node */
+	NUMA_OTHER,		/* allocation from other node */
+	NR_VM_NUMA_STAT_ITEMS
+};
+#else
+#define NR_VM_NUMA_STAT_ITEMS 0
+#endif
+
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
@@ -131,14 +145,6 @@ enum zone_stat_item {
 	NR_BOUNCE,
 #if IS_ENABLED(CONFIG_ZSMALLOC)
 	NR_ZSPAGES,		/* allocated in zsmalloc */
-#endif
-#ifdef CONFIG_NUMA
-	NUMA_HIT,		/* allocated in intended node */
-	NUMA_MISS,		/* allocated in non intended node */
-	NUMA_FOREIGN,		/* was intended here, hit elsewhere */
-	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
-	NUMA_LOCAL,		/* allocation from local node */
-	NUMA_OTHER,		/* allocation from other node */
 #endif
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
@@ -276,6 +282,8 @@ struct per_cpu_pageset {
 	struct per_cpu_pages pcp;
 #ifdef CONFIG_NUMA
 	s8 expire;
+	s8 numa_stat_threshold;
+	s8 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];
 #endif
 #ifdef CONFIG_SMP
 	s8 stat_threshold;
@@ -496,6 +504,7 @@ struct zone {
 	ZONE_PADDING(_pad3_)
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
+	atomic_long_t		vm_numa_stat[NR_VM_NUMA_STAT_ITEMS];
 } ____cacheline_internodealigned_in_smp;
 
 enum pgdat_flags {

commit b93e0f329e24f3615aa551fd9b99a75fb7c9195f
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 6 16:20:37 2017 -0700

    mm, memory_hotplug: get rid of zonelists_mutex
    
    zonelists_mutex was introduced by commit 4eaf3f64397c ("mem-hotplug: fix
    potential race while building zonelist for new populated zone") to
    protect zonelist building from races.  This is no longer needed though
    because both memory online and offline are fully serialized.  New users
    have grown since then.
    
    Notably setup_per_zone_wmarks wants to prevent from races between memory
    hotplug, khugepaged setup and manual min_free_kbytes update via sysctl
    (see cfd3da1e49bb ("mm: Serialize access to min_free_kbytes").  Let's
    add a private lock for that purpose.  This will not prevent from seeing
    halfway through memory hotplug operation but that shouldn't be a big
    deal becuse memory hotplug will update watermarks explicitly so we will
    eventually get a full picture.  The lock just makes sure we won't race
    when updating watermarks leading to weird results.
    
    Also __build_all_zonelists manipulates global data so add a private lock
    for it as well.  This doesn't seem to be necessary today but it is more
    robust to have a lock there.
    
    While we are at it make sure we document that memory online/offline
    depends on a full serialization either via mem_hotplug_begin() or
    device_lock.
    
    Link: http://lkml.kernel.org/r/20170721143915.14161-9-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Haicheng Li <haicheng.li@linux.intel.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 551f68bec2fa..e7e92c8f4883 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -770,7 +770,6 @@ static inline bool is_dev_zone(const struct zone *zone)
 
 #include <linux/memory_hotplug.h>
 
-extern struct mutex zonelists_mutex;
 void build_all_zonelists(pg_data_t *pgdat);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,

commit 72675e131eb418c78980c1e683c0c25a25b61221
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 6 16:20:24 2017 -0700

    mm, memory_hotplug: drop zone from build_all_zonelists
    
    build_all_zonelists gets a zone parameter to initialize zone's pagesets.
    There is only a single user which gives a non-NULL zone parameter and
    that one doesn't really need the rest of the build_all_zonelists (see
    commit 6dcd73d7011b ("memory-hotplug: allocate zone's pcp before
    onlining pages")).
    
    Therefore remove setup_zone_pageset from build_all_zonelists and call it
    from its only user directly.  This will also remove a pointless zonlists
    rebuilding which is always good.
    
    Link: http://lkml.kernel.org/r/20170721143915.14161-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bfdc37b77d88..551f68bec2fa 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -771,7 +771,7 @@ static inline bool is_dev_zone(const struct zone *zone)
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;
-void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
+void build_all_zonelists(pg_data_t *pgdat);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 			 int classzone_idx, unsigned int alloc_flags,

commit c9bff3eebc09be23fbc868f5e6731666d23cbea3
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 6 16:20:13 2017 -0700

    mm, page_alloc: rip out ZONELIST_ORDER_ZONE
    
    Patch series "cleanup zonelists initialization", v1.
    
    This is aimed at cleaning up the zonelists initialization code we have
    but the primary motivation was bug report [2] which got resolved but the
    usage of stop_machine is just too ugly to live.  Most patches are
    straightforward but 3 of them need a special consideration.
    
    Patch 1 removes zone ordered zonelists completely.  I am CCing linux-api
    because this is a user visible change.  As I argue in the patch
    description I do not think we have a strong usecase for it these days.
    I have kept sysctl in place and warn into the log if somebody tries to
    configure zone lists ordering.  If somebody has a real usecase for it we
    can revert this patch but I do not expect anybody will actually notice
    runtime differences.  This patch is not strictly needed for the rest but
    it made patch 6 easier to implement.
    
    Patch 7 removes stop_machine from build_all_zonelists without adding any
    special synchronization between iterators and updater which I _believe_
    is acceptable as explained in the changelog.  I hope I am not missing
    anything.
    
    Patch 8 then removes zonelists_mutex which is kind of ugly as well and
    not really needed AFAICS but a care should be taken when double checking
    my thinking.
    
    This patch (of 9):
    
    Supporting zone ordered zonelists costs us just a lot of code while the
    usefulness is arguable if existent at all.  Mel has already made node
    ordering default on 64b systems.  32b systems are still using
    ZONELIST_ORDER_ZONE because it is considered better to fallback to a
    different NUMA node rather than consume precious lowmem zones.
    
    This argument is, however, weaken by the fact that the memory reclaim
    has been reworked to be node rather than zone oriented.  This means that
    lowmem requests have to skip over all highmem pages on LRUs already and
    so zone ordering doesn't save the reclaim time much.  So the only
    advantage of the zone ordering is under a light memory pressure when
    highmem requests do not ever hit into lowmem zones and the lowmem
    pressure doesn't need to reclaim.
    
    Considering that 32b NUMA systems are rather suboptimal already and it
    is generally advisable to use 64b kernel on such a HW I believe we
    should rather care about the code maintainability and just get rid of
    ZONELIST_ORDER_ZONE altogether.  Keep systcl in place and warn if
    somebody tries to set zone ordering either from kernel command line or
    the sysctl.
    
    [mhocko@suse.com: reading vm.numa_zonelist_order will never terminate]
    Link: http://lkml.kernel.org/r/20170721143915.14161-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Cc: <linux-api@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fc14b8b3f6ce..bfdc37b77d88 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -896,7 +896,7 @@ int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
 extern int numa_zonelist_order_handler(struct ctl_table *, int,
 			void __user *, size_t *, loff_t *);
 extern char numa_zonelist_order[];
-#define NUMA_ZONELIST_ORDER_LEN 16	/* string buffer size */
+#define NUMA_ZONELIST_ORDER_LEN	16
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 

commit 9d1f4b3f5b29bea431525e528a3ff2dc806ad904
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:50:12 2017 -0700

    mm: disallow early_pfn_to_nid on configurations which do not implement it
    
    early_pfn_to_nid will return node 0 if both HAVE_ARCH_EARLY_PFN_TO_NID
    and HAVE_MEMBLOCK_NODE_MAP are disabled.  It seems we are safe now
    because all architectures which support NUMA define one of them (with an
    exception of alpha which however has CONFIG_NUMA marked as broken) so
    this works as expected.  It can get silently and subtly broken too
    easily, though.  Make sure we fail the compilation if NUMA is enabled
    and there is no proper implementation for this function.  If that ever
    happens we know that either the specific configuration is invalid and
    the fix should either disable NUMA or enable one of the above configs.
    
    Link: http://lkml.kernel.org/r/20170704075803.15979-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Yang Shi <yang.shi@linaro.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 16532fa0bb64..fc14b8b3f6ce 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1055,6 +1055,7 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 	!defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
 static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 {
+	BUILD_BUG_ON(IS_ENABLED(CONFIG_NUMA));
 	return 0;
 }
 #endif

commit 618b8c20d03c9ea06711bd36d906322ba35c0add
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Mon Jul 10 15:49:32 2017 -0700

    include/linux/mmzone.h: remove ancient/ambiguous comment
    
    Currently pg_data_t is just a struct which describes a NUMA node memory
    layout.  Let's keep the comment simple and remove ambiguity.
    
    Link: http://lkml.kernel.org/r/1498220534-22717-1-git-send-email-nborisov@suse.com
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7e8f100cb56d..16532fa0bb64 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -603,12 +603,9 @@ extern struct page *mem_map;
 #endif
 
 /*
- * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
- * (mostly NUMA machines?) to denote a higher-level memory zone than the
- * zone denotes.
- *
  * On NUMA machines, each NUMA node would have a pg_data_t to describe
- * it's memory layout.
+ * it's memory layout. On UMA machines there is a single pglist_data which
+ * describes the whole memory.
  *
  * Memory statistics and page replacement data structures are maintained on a
  * per-zone basis.

commit 385386cff4c6f047907655e05791d88198c4c523
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jul 6 15:40:43 2017 -0700

    mm: vmstat: move slab statistics from zone to node counters
    
    Patch series "mm: per-lruvec slab stats"
    
    Josef is working on a new approach to balancing slab caches and the page
    cache.  For this to work, he needs slab cache statistics on the lruvec
    level.  These patches implement that by adding infrastructure that
    allows updating and reading generic VM stat items per lruvec, then
    switches some existing VM accounting sites, including the slab
    accounting ones, to this new cgroup-aware API.
    
    I'll follow up with more patches on this, because there is actually
    substantial simplification that can be done to the memory controller
    when we replace private memcg accounting with making the existing VM
    accounting sites cgroup-aware.  But this is enough for Josef to base his
    slab reclaim work on, so here goes.
    
    This patch (of 5):
    
    To re-implement slab cache vs.  page cache balancing, we'll need the
    slab counters at the lruvec level, which, ever since lru reclaim was
    moved from the zone to the node, is the intersection of the node, not
    the zone, and the memcg.
    
    We could retain the per-zone counters for when the page allocator dumps
    its memory information on failures, and have counters on both levels -
    which on all but NUMA node 0 is usually redundant.  But let's keep it
    simple for now and just move them.  If anybody complains we can restore
    the per-zone counters.
    
    [hannes@cmpxchg.org: fix oops]
      Link: http://lkml.kernel.org/r/20170605183511.GA8915@cmpxchg.org
    Link: http://lkml.kernel.org/r/20170530181724.27197-3-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index abc1641011f2..7e8f100cb56d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -125,8 +125,6 @@ enum zone_stat_item {
 	NR_ZONE_UNEVICTABLE,
 	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
-	NR_SLAB_RECLAIMABLE,
-	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
 	NR_KERNEL_STACK_KB,	/* measured in KiB */
 	/* Second 128 byte cacheline */
@@ -152,6 +150,8 @@ enum node_stat_item {
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
+	NR_SLAB_RECLAIMABLE,
+	NR_SLAB_UNRECLAIMABLE,
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	WORKINGSET_REFAULT,

commit f1dd2cd13c4bbbc9a7c4617b3b034fa643de98fe
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:11 2017 -0700

    mm, memory_hotplug: do not associate hotadded memory to zones until online
    
    The current memory hotplug implementation relies on having all the
    struct pages associate with a zone/node during the physical hotplug
    phase (arch_add_memory->__add_pages->__add_section->__add_zone).  In the
    vast majority of cases this means that they are added to ZONE_NORMAL.
    This has been so since 9d99aaa31f59 ("[PATCH] x86_64: Support memory
    hotadd without sparsemem") and it wasn't a big deal back then because
    movable onlining didn't exist yet.
    
    Much later memory hotplug wanted to (ab)use ZONE_MOVABLE for movable
    onlining 511c2aba8f07 ("mm, memory-hotplug: dynamic configure movable
    memory and portion memory") and then things got more complicated.
    Rather than reconsidering the zone association which was no longer
    needed (because the memory hotplug already depended on SPARSEMEM) a
    convoluted semantic of zone shifting has been developed.  Only the
    currently last memblock or the one adjacent to the zone_movable can be
    onlined movable.  This essentially means that the online type changes as
    the new memblocks are added.
    
    Let's simulate memory hot online manually
      $ echo 0x100000000 > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory32/valid_zones
      Normal Movable
    
      $ echo $((0x100000000+(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      $ echo $((0x100000000+2*(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      $ echo online_movable > /sys/devices/system/memory/memory34/state
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable Normal
    
    This is an awkward semantic because an udev event is sent as soon as the
    block is onlined and an udev handler might want to online it based on
    some policy (e.g.  association with a node) but it will inherently race
    with new blocks showing up.
    
    This patch changes the physical online phase to not associate pages with
    any zone at all.  All the pages are just marked reserved and wait for
    the onlining phase to be associated with the zone as per the online
    request.  There are only two requirements
    
            - existing ZONE_NORMAL and ZONE_MOVABLE cannot overlap
    
            - ZONE_NORMAL precedes ZONE_MOVABLE in physical addresses
    
    the latter one is not an inherent requirement and can be changed in the
    future.  It preserves the current behavior and made the code slightly
    simpler.  This is subject to change in future.
    
    This means that the same physical online steps as above will lead to the
    following state: Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
    
    Implementation:
    The current move_pfn_range is reimplemented to check the above
    requirements (allow_online_pfn_range) and then updates the respective
    zone (move_pfn_range_to_zone), the pgdat and links all the pages in the
    pfn range with the zone/node.  __add_pages is updated to not require the
    zone and only initializes sections in the range.  This allowed to
    simplify the arch_add_memory code (s390 could get rid of quite some of
    code).
    
    devm_memremap_pages is the only user of arch_add_memory which relies on
    the zone association because it only hooks into the memory hotplug only
    half way.  It uses it to associate the new memory with ZONE_DEVICE but
    doesn't allow it to be {on,off}lined via sysfs.  This means that this
    particular code path has to call move_pfn_range_to_zone explicitly.
    
    The original zone shifting code is kept in place and will be removed in
    the follow up patch for an easier review.
    
    Please note that this patch also changes the original behavior when
    offlining a memory block adjacent to another zone (Normal vs.  Movable)
    used to allow to change its movable type.  This will be handled later.
    
    [richard.weiyang@gmail.com: simplify zone_intersects()]
      Link: http://lkml.kernel.org/r/20170616092335.5177-1-richard.weiyang@gmail.com
    [richard.weiyang@gmail.com: remove duplicate call for set_page_links]
      Link: http://lkml.kernel.org/r/20170616092335.5177-2-richard.weiyang@gmail.com
    [akpm@linux-foundation.org: remove unused local `i']
    Link: http://lkml.kernel.org/r/20170515085827.16474-12-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # For s390 bits
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2aaf7e08c5a8..abc1641011f2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -532,6 +532,22 @@ static inline bool zone_is_empty(struct zone *zone)
 	return zone->spanned_pages == 0;
 }
 
+/*
+ * Return true if [start_pfn, start_pfn + nr_pages) range has a non-empty
+ * intersection with the given zone
+ */
+static inline bool zone_intersects(struct zone *zone,
+		unsigned long start_pfn, unsigned long nr_pages)
+{
+	if (zone_is_empty(zone))
+		return false;
+	if (start_pfn >= zone_end_pfn(zone) ||
+	    start_pfn + nr_pages <= zone->zone_start_pfn)
+		return false;
+
+	return true;
+}
+
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the

commit 2d070eab2e8270c8a84d480bb91e4f739315f03d
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:37:56 2017 -0700

    mm: consider zone which is not fully populated to have holes
    
    __pageblock_pfn_to_page has two users currently, set_zone_contiguous
    which checks whether the given zone contains holes and
    pageblock_pfn_to_page which then carefully returns a first valid page
    from the given pfn range for the given zone.  This doesn't handle zones
    which are not fully populated though.  Memory pageblocks can be offlined
    or might not have been onlined yet.  In such a case the zone should be
    considered to have holes otherwise pfn walkers can touch and play with
    offline pages.
    
    Current callers of pageblock_pfn_to_page in compaction seem to work
    properly right now because they only isolate PageBuddy
    (isolate_freepages_block) or PageLRU resp.  __PageMovable
    (isolate_migratepages_block) which will be always false for these pages.
    It would be safer to skip these pages altogether, though.
    
    In order to do this patch adds a new memory section state
    (SECTION_IS_ONLINE) which is set in memory_present (during boot time) or
    in online_pages_range during the memory hotplug.  Similarly
    offline_mem_sections clears the bit and it is called when the memory
    range is offlined.
    
    pfn_to_online_page helper is then added which check the mem section and
    only returns a page if it is onlined already.
    
    Use the new helper in __pageblock_pfn_to_page and skip the whole page
    block in such a case.
    
    [mhocko@suse.com: check valid section number in pfn_to_online_page (Vlastimil),
     mark sections online after all struct pages are initialized in
     online_pages_range (Vlastimil)]
      Link: http://lkml.kernel.org/r/20170518164210.GD18333@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/20170515085827.16474-8-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 976a1202bec1..2aaf7e08c5a8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1144,9 +1144,10 @@ extern unsigned long usemap_size(void);
  */
 #define	SECTION_MARKED_PRESENT	(1UL<<0)
 #define SECTION_HAS_MEM_MAP	(1UL<<1)
-#define SECTION_MAP_LAST_BIT	(1UL<<2)
+#define SECTION_IS_ONLINE	(1UL<<2)
+#define SECTION_MAP_LAST_BIT	(1UL<<3)
 #define SECTION_MAP_MASK	(~(SECTION_MAP_LAST_BIT-1))
-#define SECTION_NID_SHIFT	2
+#define SECTION_NID_SHIFT	3
 
 static inline struct page *__section_mem_map_addr(struct mem_section *section)
 {
@@ -1175,6 +1176,23 @@ static inline int valid_section_nr(unsigned long nr)
 	return valid_section(__nr_to_section(nr));
 }
 
+static inline int online_section(struct mem_section *section)
+{
+	return (section && (section->section_mem_map & SECTION_IS_ONLINE));
+}
+
+static inline int online_section_nr(unsigned long nr)
+{
+	return online_section(__nr_to_section(nr));
+}
+
+#ifdef CONFIG_MEMORY_HOTPLUG
+void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);
+#ifdef CONFIG_MEMORY_HOTREMOVE
+void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);
+#endif
+#endif
+
 static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 {
 	return __nr_to_section(pfn_to_section_nr(pfn));
@@ -1253,10 +1271,15 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 #ifdef CONFIG_ARCH_HAS_HOLES_MEMORYMODEL
 /*
  * pfn_valid() is meant to be able to tell if a given PFN has valid memmap
- * associated with it or not. In FLATMEM, it is expected that holes always
- * have valid memmap as long as there is valid PFNs either side of the hole.
- * In SPARSEMEM, it is assumed that a valid section has a memmap for the
- * entire section.
+ * associated with it or not. This means that a struct page exists for this
+ * pfn. The caller cannot assume the page is fully initialized in general.
+ * Hotplugable pages might not have been onlined yet. pfn_to_online_page()
+ * will ensure the struct page is fully online and initialized. Special pages
+ * (e.g. ZONE_DEVICE) are never onlined and should be treated accordingly.
+ *
+ * In FLATMEM, it is expected that holes always have valid memmap as long as
+ * there is valid PFNs either side of the hole. In SPARSEMEM, it is assumed
+ * that a valid section has a memmap for the entire section.
  *
  * However, an ARM, and maybe other embedded architectures in the future
  * free memmap backing holes to save memory on the assumption the memmap is

commit dc0bbf3b7fb9ed2246f62bba4379070589e2135c
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:37:35 2017 -0700

    mm: remove return value from init_currently_empty_zone
    
    Patch series "mm: make movable onlining suck less", v4.
    
    Movable onlining is a real hack with many downsides - mainly
    reintroduction of lowmem/highmem issues we used to have on 32b systems -
    but it is the only way to make the memory hotremove more reliable which
    is something that people are asking for.
    
    The current semantic of memory movable onlinening is really cumbersome,
    however.  The main reason for this is that the udev driven approach is
    basically unusable because udev races with the memory probing while only
    the last memory block or the one adjacent to the existing zone_movable
    are allowed to be onlined movable.  In short the criterion for the
    successful online_movable changes under udev's feet.  A reliable udev
    approach would require a 2 phase approach where the first successful
    movable online would have to check all the previous blocks and online
    them in descending order.  This is hard to be considered sane.
    
    This patchset aims at making the onlining semantic more usable.  First
    of all it allows to online memory movable as long as it doesn't clash
    with the existing ZONE_NORMAL.  That means that ZONE_NORMAL and
    ZONE_MOVABLE cannot overlap.  Currently I preserve the original ordering
    semantic so the zone always precedes the movable zone but I have plans
    to remove this restriction in future because it is not really necessary.
    
    First 3 patches are cleanups which should be ready to be merged right
    away (unless I have missed something subtle of course).
    
    Patch 4 deals with ZONE_DEVICE dependencies down the __add_pages path.
    
    Patch 5 deals with implicit assumptions of register_one_node on pgdat
    initialization.
    
    Patches 6-10 deal with offline holes in the zone for pfn walkers.  I
    hope I got all of them right but people familiar with compaction should
    double check this.
    
    Patch 11 is the core of the change.  In order to make it easier to
    review I have tried it to be as minimalistic as possible and the large
    code removal is moved to patch 14.
    
    Patch 12 is a trivial follow up cleanup.  Patch 13 fixes sparse warnings
    and finally patch 14 removes the unused code.
    
    I have tested the patches in kvm:
      # qemu-system-x86_64 -enable-kvm -monitor pty -m 2G,slots=4,maxmem=4G -numa node,mem=1G -numa node,mem=1G ...
    
    and then probed the additional memory by
      (qemu) object_add memory-backend-ram,id=mem1,size=1G
      (qemu) device_add pc-dimm,id=dimm1,memdev=mem1
    
    Then I have used this simple script to probe the memory block by hand
      # cat probe_memblock.sh
      #!/bin/sh
    
      BLOCK_NR=$1
    
      # echo $((0x100000000+$BLOCK_NR*(128<<20))) > /sys/devices/system/memory/probe
    
      # for i in $(seq 10); do sh probe_memblock.sh $i; done
      # grep . /sys/devices/system/memory/memory3?/valid_zones 2>/dev/null
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
      /sys/devices/system/memory/memory35/valid_zones:Normal Movable
      /sys/devices/system/memory/memory36/valid_zones:Normal Movable
      /sys/devices/system/memory/memory37/valid_zones:Normal Movable
      /sys/devices/system/memory/memory38/valid_zones:Normal Movable
      /sys/devices/system/memory/memory39/valid_zones:Normal Movable
    
    The main difference to the original implementation is that all new
    memblocks can be both online_kernel and online_movable initially because
    there is no clash obviously.  For the comparison the original
    implementation would have
    
      /sys/devices/system/memory/memory33/valid_zones:Normal
      /sys/devices/system/memory/memory34/valid_zones:Normal
      /sys/devices/system/memory/memory35/valid_zones:Normal
      /sys/devices/system/memory/memory36/valid_zones:Normal
      /sys/devices/system/memory/memory37/valid_zones:Normal
      /sys/devices/system/memory/memory38/valid_zones:Normal
      /sys/devices/system/memory/memory39/valid_zones:Normal Movable
    
    Now
      # echo online_movable > /sys/devices/system/memory/memory34/state
      # grep . /sys/devices/system/memory/memory3?/valid_zones 2>/dev/null
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
      /sys/devices/system/memory/memory35/valid_zones:Movable
      /sys/devices/system/memory/memory36/valid_zones:Movable
      /sys/devices/system/memory/memory37/valid_zones:Movable
      /sys/devices/system/memory/memory38/valid_zones:Movable
      /sys/devices/system/memory/memory39/valid_zones:Movable
    
    Block 33 can still be online both kernel and movable while all
    the remaining can be only movable.
    
    /proc/zonelist says
      Node 0, zone   Normal
        pages free     0
              min      0
              low      0
              high     0
              spanned  0
              present  0
      --
      Node 0, zone  Movable
        pages free     32753
              min      85
              low      117
              high     149
              spanned  32768
              present  32768
    
    A new memblock at a lower address will result in a new memblock (32)
    which will still allow both Normal and Movable.
    
      # sh probe_memblock.sh 0
      # grep . /sys/devices/system/memory/memory3[2-5]/valid_zones 2>/dev/null
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
      /sys/devices/system/memory/memory35/valid_zones:Movable
    
    and online_kernel will convert it to the zone normal properly
    while 33 can be still onlined both ways.
    
      # echo online_kernel > /sys/devices/system/memory/memory32/state
      # grep . /sys/devices/system/memory/memory3[2-5]/valid_zones 2>/dev/null
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
      /sys/devices/system/memory/memory35/valid_zones:Movable
    
    /proc/zoneinfo will now tell
      Node 0, zone   Normal
        pages free     65441
              min      165
              low      230
              high     295
              spanned  65536
              present  65536
      --
      Node 0, zone  Movable
        pages free     32740
              min      82
              low      114
              high     146
              spanned  32768
              present  32768
    
    so both zones have one memblock spanned and present.
    
    Onlining 39 should associate this block to the movable zone
    
      # echo online > /sys/devices/system/memory/memory39/state
    
    /proc/zoneinfo will now tell
      Node 0, zone   Normal
        pages free     32765
              min      80
              low      112
              high     144
              spanned  32768
              present  32768
      --
      Node 0, zone  Movable
        pages free     65501
              min      160
              low      225
              high     290
              spanned  196608
              present  65536
    
    so we will have a movable zone which spans 6 memblocks, 2 present and 4
    representing a hole.
    
    Offlining both movable blocks will lead to the zone with no present
    pages which is the expected behavior I believe.
    
      # echo offline > /sys/devices/system/memory/memory39/state
      # echo offline > /sys/devices/system/memory/memory34/state
      # grep -A6 "Movable\|Normal" /proc/zoneinfo
      Node 0, zone   Normal
        pages free     32735
              min      90
              low      122
              high     154
              spanned  32768
              present  32768
      --
      Node 0, zone  Movable
        pages free     0
              min      0
              low      0
              high     0
              spanned  196608
              present  0
    
    As a bonus we will get a nice cleanup in the memory hotplug codebase.
    
    This patch (of 16):
    
    init_currently_empty_zone doesn't have any error to return yet it is
    still an int and callers try to be defensive and try to handle potential
    error.  Remove this nonsense and simplify all callers.
    
    This patch shouldn't have any visible effect
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fc39f85d273c..976a1202bec1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -772,7 +772,7 @@ enum memmap_context {
 	MEMMAP_EARLY,
 	MEMMAP_HOTPLUG,
 };
-extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
+extern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
 				     unsigned long size);
 
 extern void lruvec_init(struct lruvec *lruvec);

commit c4e1be9ec1130fff4d691cdc0e0f9d666009f9ae
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jul 6 15:36:44 2017 -0700

    mm, sparsemem: break out of loops early
    
    There are a number of times that we loop over NR_MEM_SECTIONS, looking
    for section_present() on each section.  But, when we have very large
    physical address spaces (large MAX_PHYSMEM_BITS), NR_MEM_SECTIONS
    becomes very large, making the loops quite long.
    
    With MAX_PHYSMEM_BITS=46 and a section size of 128MB, the current loops
    are 512k iterations, which we barely notice on modern hardware.  But,
    raising MAX_PHYSMEM_BITS higher (like we will see on systems that
    support 5-level paging) makes this 64x longer and we start to notice,
    especially on slower systems like simulators.  A 10-second delay for
    512k iterations is annoying.  But, a 640- second delay is crippling.
    
    This does not help if we have extremely sparse physical address spaces,
    but those are quite rare.  We expect that most of the "slow" systems
    where this matters will also be quite small and non-sparse.
    
    To fix this, we track the highest section we've ever encountered.  This
    lets us know when we will *never* see another section_present(), and
    lets us break out of the loops earlier.
    
    Doing the whole for_each_present_section_nr() macro is probably
    overkill, but it will ensure that any future loop iterations that we
    grow are more likely to be correct.
    
    Kirrill said "It shaved almost 40 seconds from boot time in qemu with
    5-level paging enabled for me".
    
    Link: http://lkml.kernel.org/r/20170504174434.C45A4735@viggo.jf.intel.com
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Tested-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ef6a13b7bd3e..fc39f85d273c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1180,6 +1180,8 @@ static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 	return __nr_to_section(pfn_to_section_nr(pfn));
 }
 
+extern int __highest_present_section_nr;
+
 #ifndef CONFIG_HAVE_ARCH_PFN_VALID
 static inline int pfn_valid(unsigned long pfn)
 {

commit 864b9a393dcb5aed09b8fd31b9bbda0fdda99374
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Jun 2 14:46:49 2017 -0700

    mm: consider memblock reservations for deferred memory initialization sizing
    
    We have seen an early OOM killer invocation on ppc64 systems with
    crashkernel=4096M:
    
            kthreadd invoked oom-killer: gfp_mask=0x16040c0(GFP_KERNEL|__GFP_COMP|__GFP_NOTRACK), nodemask=7, order=0, oom_score_adj=0
            kthreadd cpuset=/ mems_allowed=7
            CPU: 0 PID: 2 Comm: kthreadd Not tainted 4.4.68-1.gd7fe927-default #1
            Call Trace:
              dump_stack+0xb0/0xf0 (unreliable)
              dump_header+0xb0/0x258
              out_of_memory+0x5f0/0x640
              __alloc_pages_nodemask+0xa8c/0xc80
              kmem_getpages+0x84/0x1a0
              fallback_alloc+0x2a4/0x320
              kmem_cache_alloc_node+0xc0/0x2e0
              copy_process.isra.25+0x260/0x1b30
              _do_fork+0x94/0x470
              kernel_thread+0x48/0x60
              kthreadd+0x264/0x330
              ret_from_kernel_thread+0x5c/0xa4
    
            Mem-Info:
            active_anon:0 inactive_anon:0 isolated_anon:0
             active_file:0 inactive_file:0 isolated_file:0
             unevictable:0 dirty:0 writeback:0 unstable:0
             slab_reclaimable:5 slab_unreclaimable:73
             mapped:0 shmem:0 pagetables:0 bounce:0
             free:0 free_pcp:0 free_cma:0
            Node 7 DMA free:0kB min:0kB low:0kB high:0kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:52428800kB managed:110016kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:320kB slab_unreclaimable:4672kB kernel_stack:1152kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes
            lowmem_reserve[]: 0 0 0 0
            Node 7 DMA: 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB 0*8192kB 0*16384kB = 0kB
            0 total pagecache pages
            0 pages in swap cache
            Swap cache stats: add 0, delete 0, find 0/0
            Free swap  = 0kB
            Total swap = 0kB
            819200 pages RAM
            0 pages HighMem/MovableOnly
            817481 pages reserved
            0 pages cma reserved
            0 pages hwpoisoned
    
    the reason is that the managed memory is too low (only 110MB) while the
    rest of the the 50GB is still waiting for the deferred intialization to
    be done.  update_defer_init estimates the initial memoty to initialize
    to 2GB at least but it doesn't consider any memory allocated in that
    range.  In this particular case we've had
    
            Reserving 4096MB of memory at 128MB for crashkernel (System RAM: 51200MB)
    
    so the low 2GB is mostly depleted.
    
    Fix this by considering memblock allocations in the initial static
    initialization estimation.  Move the max_initialise to
    reset_deferred_meminit and implement a simple memblock_reserved_memory
    helper which iterates all reserved blocks and sums the size of all that
    start below the given address.  The cumulative size is than added on top
    of the initial estimation.  This is still not ideal because
    reset_deferred_meminit doesn't consider holes and so reservation might
    be above the initial estimation whihch we ignore but let's make the
    logic simpler until we really need to handle more complicated cases.
    
    Fixes: 3a80a7fa7989 ("mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set")
    Link: http://lkml.kernel.org/r/20170531104010.GI27783@dhcp22.suse.cz
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org>    [4.2+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ebaccd4e7d8c..ef6a13b7bd3e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -678,6 +678,7 @@ typedef struct pglist_data {
 	 * is the first PFN that needs to be initialised.
 	 */
 	unsigned long first_deferred_pfn;
+	unsigned long static_init_size;
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE

commit b682debd97153706ffbe2fe3f8ec30a7ee11f9e1
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 8 15:54:43 2017 -0700

    mm, compaction: change migrate_async_suitable() to suitable_migration_source()
    
    Preparation for making the decisions more complex and depending on
    compact_control flags.  No functional change.
    
    Link: http://lkml.kernel.org/r/20170307131545.28577-6-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e0c3c5e3d8a0..ebaccd4e7d8c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -74,6 +74,11 @@ extern char * const migratetype_names[MIGRATE_TYPES];
 #  define is_migrate_cma_page(_page) false
 #endif
 
+static inline bool is_migrate_movable(int mt)
+{
+	return is_migrate_cma(mt) || mt == MIGRATE_MOVABLE;
+}
+
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \
 		for (type = 0; type < MIGRATE_TYPES; type++)

commit 2a2e48854d704214dac7546e87ae0e4daa0e61a0
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed May 3 14:55:03 2017 -0700

    mm: vmscan: fix IO/refault regression in cache workingset transition
    
    Since commit 59dc76b0d4df ("mm: vmscan: reduce size of inactive file
    list") we noticed bigger IO spikes during changes in cache access
    patterns.
    
    The patch in question shrunk the inactive list size to leave more room
    for the current workingset in the presence of streaming IO.  However,
    workingset transitions that previously happened on the inactive list are
    now pushed out of memory and incur more refaults to complete.
    
    This patch disables active list protection when refaults are being
    observed.  This accelerates workingset transitions, and allows more of
    the new set to establish itself from memory, without eating into the
    ability to protect the established workingset during stable periods.
    
    The workloads that were measurably affected for us were hit pretty bad
    by it, with refault/majfault rates doubling and tripling during cache
    transitions, and the machines sustaining half-hour periods of 100% IO
    utilization, where they'd previously have sub-minute peaks at 60-90%.
    
    Stateful services that handle user data tend to be more conservative
    with kernel upgrades.  As a result we hit most page cache issues with
    some delay, as was the case here.
    
    The severity seemed to warrant a stable tag.
    
    Fixes: 59dc76b0d4df ("mm: vmscan: reduce size of inactive file list")
    Link: http://lkml.kernel.org/r/20170404220052.27593-1-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: <stable@vger.kernel.org>    [4.7+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 446cf68c1c09..e0c3c5e3d8a0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -225,6 +225,8 @@ struct lruvec {
 	struct zone_reclaim_stat	reclaim_stat;
 	/* Evictions & activations on the inactive file list */
 	atomic_long_t			inactive_age;
+	/* Refaults at the time of last reclaim cycle */
+	unsigned long			refaults;
 #ifdef CONFIG_MEMCG
 	struct pglist_data *pgdat;
 #endif

commit a6ffdc07847e74cc244c02ab6d0351a4a5d77281
Author: Xishi Qiu <qiuxishi@huawei.com>
Date:   Wed May 3 14:52:52 2017 -0700

    mm: use is_migrate_highatomic() to simplify the code
    
    Introduce two helpers, is_migrate_highatomic() and is_migrate_highatomic_page().
    
    Simplify the code, no functional changes.
    
    [akpm@linux-foundation.org: use static inlines rather than macros, per mhocko]
    Link: http://lkml.kernel.org/r/58B94F15.6060606@huawei.com
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 04e0969966f6..446cf68c1c09 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -35,7 +35,7 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
-enum {
+enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
 	MIGRATE_RECLAIMABLE,

commit c822f6223d03c2c5b026a21da09c6b6d523258cd
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed May 3 14:52:10 2017 -0700

    mm: delete NR_PAGES_SCANNED and pgdat_reclaimable()
    
    NR_PAGES_SCANNED counts number of pages scanned since the last page free
    event in the allocator.  This was used primarily to measure the
    reclaimability of zones and nodes, and determine when reclaim should
    give up on them.  In that role, it has been replaced in the preceding
    patches by a different mechanism.
    
    Being implemented as an efficient vmstat counter, it was automatically
    exported to userspace as well.  It's however unlikely that anyone
    outside the kernel is using this counter in any meaningful way.
    
    Remove the counter and the unused pgdat_reclaimable().
    
    Link: http://lkml.kernel.org/r/20170228214007.5621-8-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Jia He <hejianet@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d2c50ab6ae40..04e0969966f6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -149,7 +149,6 @@ enum node_stat_item {
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
-	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
 	WORKINGSET_NODERECLAIM,

commit c73322d098e4b6f5f0f0fa1330bf57e218775539
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed May 3 14:51:51 2017 -0700

    mm: fix 100% CPU kswapd busyloop on unreclaimable nodes
    
    Patch series "mm: kswapd spinning on unreclaimable nodes - fixes and
    cleanups".
    
    Jia reported a scenario in which the kswapd of a node indefinitely spins
    at 100% CPU usage.  We have seen similar cases at Facebook.
    
    The kernel's current method of judging its ability to reclaim a node (or
    whether to back off and sleep) is based on the amount of scanned pages
    in proportion to the amount of reclaimable pages.  In Jia's and our
    scenarios, there are no reclaimable pages in the node, however, and the
    condition for backing off is never met.  Kswapd busyloops in an attempt
    to restore the watermarks while having nothing to work with.
    
    This series reworks the definition of an unreclaimable node based not on
    scanning but on whether kswapd is able to actually reclaim pages in
    MAX_RECLAIM_RETRIES (16) consecutive runs.  This is the same criteria
    the page allocator uses for giving up on direct reclaim and invoking the
    OOM killer.  If it cannot free any pages, kswapd will go to sleep and
    leave further attempts to direct reclaim invocations, which will either
    make progress and re-enable kswapd, or invoke the OOM killer.
    
    Patch #1 fixes the immediate problem Jia reported, the remainder are
    smaller fixlets, cleanups, and overall phasing out of the old method.
    
    Patch #6 is the odd one out.  It's a nice cleanup to get_scan_count(),
    and directly related to #5, but in itself not relevant to the series.
    
    If the whole series is too ambitious for 4.11, I would consider the
    first three patches fixes, the rest cleanups.
    
    This patch (of 9):
    
    Jia He reports a problem with kswapd spinning at 100% CPU when
    requesting more hugepages than memory available in the system:
    
    $ echo 4000 >/proc/sys/vm/nr_hugepages
    
    top - 13:42:59 up  3:37,  1 user,  load average: 1.09, 1.03, 1.01
    Tasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie
    %Cpu(s):  0.0 us, 12.5 sy,  0.0 ni, 85.5 id,  2.0 wa,  0.0 hi,  0.0 si,  0.0 st
    KiB Mem:  31371520 total, 30915136 used,   456384 free,      320 buffers
    KiB Swap:  6284224 total,   115712 used,  6168512 free.    48192 cached Mem
    
      PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
       76 root      20   0       0      0      0 R 100.0 0.000 217:17.29 kswapd3
    
    At that time, there are no reclaimable pages left in the node, but as
    kswapd fails to restore the high watermarks it refuses to go to sleep.
    
    Kswapd needs to back away from nodes that fail to balance.  Up until
    commit 1d82de618ddd ("mm, vmscan: make kswapd reclaim in terms of
    nodes") kswapd had such a mechanism.  It considered zones whose
    theoretically reclaimable pages it had reclaimed six times over as
    unreclaimable and backed away from them.  This guard was erroneously
    removed as the patch changed the definition of a balanced node.
    
    However, simply restoring this code wouldn't help in the case reported
    here: there *are* no reclaimable pages that could be scanned until the
    threshold is met.  Kswapd would stay awake anyway.
    
    Introduce a new and much simpler way of backing off.  If kswapd runs
    through MAX_RECLAIM_RETRIES (16) cycles without reclaiming a single
    page, make it back off from the node.  This is the same number of shots
    direct reclaim takes before declaring OOM.  Kswapd will go to sleep on
    that node until a direct reclaimer manages to reclaim some pages, thus
    proving the node reclaimable again.
    
    [hannes@cmpxchg.org: check kswapd failure against the cumulative nr_reclaimed count]
      Link: http://lkml.kernel.org/r/20170306162410.GB2090@cmpxchg.org
    [shakeelb@google.com: fix condition for throttle_direct_reclaim]
      Link: http://lkml.kernel.org/r/20170314183228.20152-1-shakeelb@google.com
    Link: http://lkml.kernel.org/r/20170228214007.5621-2-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Reported-by: Jia He <hejianet@gmail.com>
    Tested-by: Jia He <hejianet@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8e02b3750fe0..d2c50ab6ae40 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -630,6 +630,8 @@ typedef struct pglist_data {
 	int kswapd_order;
 	enum zone_type kswapd_classzone_idx;
 
+	int kswapd_failures;		/* Number of 'reclaimed == 0' runs */
+
 #ifdef CONFIG_COMPACTION
 	int kcompactd_max_order;
 	enum zone_type kcompactd_classzone_idx;

commit 1276ad68e2491d1ceeb65f55d790f9277593c459
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Feb 24 14:56:11 2017 -0800

    mm: vmscan: scan dirty pages even in laptop mode
    
    Patch series "mm: vmscan: fix kswapd writeback regression".
    
    We noticed a regression on multiple hadoop workloads when moving from
    3.10 to 4.0 and 4.6, which involves kswapd getting tangled up in page
    writeout, causing direct reclaim herds that also don't make progress.
    
    I tracked it down to the thrash avoidance efforts after 3.10 that make
    the kernel better at keeping use-once cache and use-many cache sorted on
    the inactive and active list, with more aggressive protection of the
    active list as long as there is inactive cache.  Unfortunately, our
    workload's use-once cache is mostly from streaming writes.  Waiting for
    writes to avoid potential reloads in the future is not a good tradeoff.
    
    These patches do the following:
    
    1. Wake the flushers when kswapd sees a lump of dirty pages. It's
       possible to be below the dirty background limit and still have cache
       velocity push them through the LRU. So start a-flushin'.
    
    2. Let kswapd only write pages that have been rotated twice. This makes
       sure we really tried to get all the clean pages on the inactive list
       before resorting to horrible LRU-order writeback.
    
    3. Move rotating dirty pages off the inactive list. Instead of churning
       or waiting on page writeback, we'll go after clean active cache. This
       might lead to thrashing, but in this state memory demand outstrips IO
       speed anyway, and reads are faster than writes.
    
    Mel backported the series to 4.10-rc5 with one minor conflict and ran a
    couple of tests on it.  Mix of read/write random workload didn't show
    anything interesting.  Write-only database didn't show much difference
    in performance but there were slight reductions in IO -- probably in the
    noise.
    
    simoop did show big differences although not as big as Mel expected.
    This is Chris Mason's workload that similate the VM activity of hadoop.
    Mel won't go through the full details but over the samples measured
    during an hour it reported
    
                                             4.10.0-rc5            4.10.0-rc5
                                                vanilla         johannes-v1r1
    Amean    p50-Read             21346531.56 (  0.00%) 21697513.24 ( -1.64%)
    Amean    p95-Read             24700518.40 (  0.00%) 25743268.98 ( -4.22%)
    Amean    p99-Read             27959842.13 (  0.00%) 28963271.11 ( -3.59%)
    Amean    p50-Write                1138.04 (  0.00%)      989.82 ( 13.02%)
    Amean    p95-Write             1106643.48 (  0.00%)    12104.00 ( 98.91%)
    Amean    p99-Write             1569213.22 (  0.00%)    36343.38 ( 97.68%)
    Amean    p50-Allocation          85159.82 (  0.00%)    79120.70 (  7.09%)
    Amean    p95-Allocation         204222.58 (  0.00%)   129018.43 ( 36.82%)
    Amean    p99-Allocation         278070.04 (  0.00%)   183354.43 ( 34.06%)
    Amean    final-p50-Read       21266432.00 (  0.00%) 21921792.00 ( -3.08%)
    Amean    final-p95-Read       24870912.00 (  0.00%) 26116096.00 ( -5.01%)
    Amean    final-p99-Read       28147712.00 (  0.00%) 29523968.00 ( -4.89%)
    Amean    final-p50-Write          1130.00 (  0.00%)      977.00 ( 13.54%)
    Amean    final-p95-Write       1033216.00 (  0.00%)     2980.00 ( 99.71%)
    Amean    final-p99-Write       1517568.00 (  0.00%)    32672.00 ( 97.85%)
    Amean    final-p50-Allocation    86656.00 (  0.00%)    78464.00 (  9.45%)
    Amean    final-p95-Allocation   211712.00 (  0.00%)   116608.00 ( 44.92%)
    Amean    final-p99-Allocation   287232.00 (  0.00%)   168704.00 ( 41.27%)
    
    The latencies are actually completely horrific in comparison to 4.4 (and
    4.10-rc5 is worse than 4.9 according to historical data for reasons Mel
    hasn't analysed yet).
    
    Still, 95% of write latency (p95-write) is halved by the series and
    allocation latency is way down.  Direct reclaim activity is one fifth of
    what it was according to vmstats.  Kswapd activity is higher but this is
    not necessarily surprising.  Kswapd efficiency is unchanged at 99% (99%
    of pages scanned were reclaimed) but direct reclaim efficiency went from
    77% to 99%
    
    In the vanilla kernel, 627MB of data was written back from reclaim
    context.  With the series, no data was written back.  With or without
    the patch, pages are being immediately reclaimed after writeback
    completes.  However, with the patch, only 1/8th of the pages are
    reclaimed like this.
    
    This patch (of 5):
    
    We have an elaborate dirty/writeback throttling mechanism inside the
    reclaim scanner, but for that to work the pages have to go through
    shrink_page_list() and get counted for what they are.  Otherwise, we
    mess up the LRU order and don't match reclaim speed to writeback.
    
    Especially during deactivation, there is never a reason to skip dirty
    pages; nothing is even trying to write them out from there.  Don't mess
    up the LRU order for nothing, shuffle these pages along.
    
    Link: http://lkml.kernel.org/r/20170123181641.23938-2-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 82fc632fd11d..8e02b3750fe0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -236,8 +236,6 @@ struct lruvec {
 #define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
 #define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
 
-/* Isolate clean file */
-#define ISOLATE_CLEAN		((__force isolate_mode_t)0x1)
 /* Isolate unmapped file */
 #define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)
 /* Isolate for asynchronous migration */

commit fd538803731e50367b7c59ce4ad3454426a3d671
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Feb 22 15:45:58 2017 -0800

    mm, vmscan: cleanup lru size claculations
    
    lruvec_lru_size returns the full size of the LRU list while we sometimes
    need a value reduced only to eligible zones (e.g.  for lowmem requests).
    inactive_list_is_low is one such user.  Later patches will add more of
    them.  Add a new parameter to lruvec_lru_size and allow it filter out
    zones which are not eligible for the given context.
    
    Link: http://lkml.kernel.org/r/20170117103702.28542-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f4aac87adcc3..82fc632fd11d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -779,7 +779,7 @@ static inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
 #endif
 }
 
-extern unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru);
+extern unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru, int zone_idx);
 
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);

commit ea57485af8f4221312a5a95d63c382b45e7840dc
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Jan 24 15:18:32 2017 -0800

    mm, page_alloc: fix check for NULL preferred_zone
    
    Patch series "fix premature OOM regression in 4.7+ due to cpuset races".
    
    This is v2 of my attempt to fix the recent report based on LTP cpuset
    stress test [1].  The intention is to go to stable 4.9 LTSS with this,
    as triggering repeated OOMs is not nice.  That's why the patches try to
    be not too intrusive.
    
    Unfortunately why investigating I found that modifying the testcase to
    use per-VMA policies instead of per-task policies will bring the OOM's
    back, but that seems to be much older and harder to fix problem.  I have
    posted a RFC [2] but I believe that fixing the recent regressions has a
    higher priority.
    
    Longer-term we might try to think how to fix the cpuset mess in a better
    and less error prone way.  I was for example very surprised to learn,
    that cpuset updates change not only task->mems_allowed, but also
    nodemask of mempolicies.  Until now I expected the parameter to
    alloc_pages_nodemask() to be stable.  I wonder why do we then treat
    cpusets specially in get_page_from_freelist() and distinguish HARDWALL
    etc, when there's unconditional intersection between mempolicy and
    cpuset.  I would expect the nodemask adjustment for saving overhead in
    g_p_f(), but that clearly doesn't happen in the current form.  So we
    have both crazy complexity and overhead, AFAICS.
    
    [1] https://lkml.kernel.org/r/CAFpQJXUq-JuEP=QPidy4p_=FN0rkH5Z-kfB4qBvsf6jMS87Edg@mail.gmail.com
    [2] https://lkml.kernel.org/r/7c459f26-13a6-a817-e508-b65b903a8378@suse.cz
    
    This patch (of 4):
    
    Since commit c33d6c06f60f ("mm, page_alloc: avoid looking up the first
    zone in a zonelist twice") we have a wrong check for NULL preferred_zone,
    which can theoretically happen due to concurrent cpuset modification.  We
    check the zoneref pointer which is never NULL and we should check the zone
    pointer.  Also document this in first_zones_zonelist() comment per Michal
    Hocko.
    
    Fixes: c33d6c06f60f ("mm, page_alloc: avoid looking up the first zone in a zonelist twice")
    Link: http://lkml.kernel.org/r/20170120103843.24587-2-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Ganapatrao Kulkarni <gpkulkarni@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 36d9896fbc1e..f4aac87adcc3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -972,12 +972,16 @@ static __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,
  * @zonelist - The zonelist to search for a suitable zone
  * @highest_zoneidx - The zone index of the highest zone to return
  * @nodes - An optional nodemask to filter the zonelist with
- * @zone - The first suitable zone found is returned via this parameter
+ * @return - Zoneref pointer for the first suitable zone found (see below)
  *
  * This function returns the first zone at or below a given zone index that is
  * within the allowed nodemask. The zoneref returned is a cursor that can be
  * used to iterate the zonelist with next_zones_zonelist by advancing it by
  * one before calling.
+ *
+ * When no eligible zone is found, zoneref->zone is NULL (zoneref itself is
+ * never NULL). This may happen either genuinely, or due to concurrent nodemask
+ * update due to cpuset modification.
  */
 static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 					enum zone_type highest_zoneidx,

commit 9efeccacd3a486128d3add611dd4cefb5b60a58c
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Dec 11 06:34:53 2016 +0200

    linux: drop __bitwise__ everywhere
    
    __bitwise__ used to mean "yes, please enable sparse checks
    unconditionally", but now that we dropped __CHECK_ENDIAN__
    __bitwise is exactly the same.
    There aren't many users, replace it by __bitwise everywhere.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Stefan Schmidt <stefan@osg.samsung.com>
    Acked-by: Krzysztof Kozlowski <krzk@kernel.org>
    Akced-by: Lee Duncan <lduncan@suse.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0f088f3a2fed..36d9896fbc1e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -246,7 +246,7 @@ struct lruvec {
 #define ISOLATE_UNEVICTABLE	((__force isolate_mode_t)0x8)
 
 /* LRU Isolation modes. */
-typedef unsigned __bitwise__ isolate_mode_t;
+typedef unsigned __bitwise isolate_mode_t;
 
 enum zone_watermarks {
 	WMARK_MIN,

commit 9dcb8b685fc30813b35ab4b4bf39244430753190
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 10:15:30 2016 -0700

    mm: remove per-zone hashtable of bitlock waitqueues
    
    The per-zone waitqueues exist because of a scalability issue with the
    page waitqueues on some NUMA machines, but it turns out that they hurt
    normal loads, and now with the vmalloced stacks they also end up
    breaking gfs2 that uses a bit_wait on a stack object:
    
         wait_on_bit(&gh->gh_iflags, HIF_WAIT, TASK_UNINTERRUPTIBLE)
    
    where 'gh' can be a reference to the local variable 'mount_gh' on the
    stack of fill_super().
    
    The reason the per-zone hash table breaks for this case is that there is
    no "zone" for virtual allocations, and trying to look up the physical
    page to get at it will fail (with a BUG_ON()).
    
    It turns out that I actually complained to the mm people about the
    per-zone hash table for another reason just a month ago: the zone lookup
    also hurts the regular use of "unlock_page()" a lot, because the zone
    lookup ends up forcing several unnecessary cache misses and generates
    horrible code.
    
    As part of that earlier discussion, we had a much better solution for
    the NUMA scalability issue - by just making the page lock have a
    separate contention bit, the waitqueue doesn't even have to be looked at
    for the normal case.
    
    Peter Zijlstra already has a patch for that, but let's see if anybody
    even notices.  In the meantime, let's fix the actual gfs2 breakage by
    simplifying the bitlock waitqueues and removing the per-zone issue.
    
    Reported-by: Andreas Gruenbacher <agruenba@redhat.com>
    Tested-by: Bob Peterson <rpeterso@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7f2ae99e5daf..0f088f3a2fed 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -440,33 +440,7 @@ struct zone {
 	seqlock_t		span_seqlock;
 #endif
 
-	/*
-	 * wait_table		-- the array holding the hash table
-	 * wait_table_hash_nr_entries	-- the size of the hash table array
-	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
-	 *
-	 * The purpose of all these is to keep track of the people
-	 * waiting for a page to become available and make them
-	 * runnable again when possible. The trouble is that this
-	 * consumes a lot of space, especially when so few things
-	 * wait on pages at a given time. So instead of using
-	 * per-page waitqueues, we use a waitqueue hash table.
-	 *
-	 * The bucket discipline is to sleep on the same queue when
-	 * colliding and wake all in that wait queue when removing.
-	 * When something wakes, it must check to be sure its page is
-	 * truly available, a la thundering herd. The cost of a
-	 * collision is great, but given the expected load of the
-	 * table, they should be so rare as to be outweighed by the
-	 * benefits from the saved space.
-	 *
-	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
-	 * primary users of these fields, and in mm/page_alloc.c
-	 * free_area_init_core() performs the initialization of them.
-	 */
-	wait_queue_head_t	*wait_table;
-	unsigned long		wait_table_hash_nr_entries;
-	unsigned long		wait_table_bits;
+	int initialized;
 
 	/* Write-intensive fields used from the page allocator */
 	ZONE_PADDING(_pad1_)
@@ -546,7 +520,7 @@ static inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)
 
 static inline bool zone_is_initialized(struct zone *zone)
 {
-	return !!zone->wait_table;
+	return zone->initialized;
 }
 
 static inline bool zone_is_empty(struct zone *zone)

commit 6aa303defb7454a2520c4ddcdf6b081f62a15890
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Sep 1 16:14:55 2016 -0700

    mm, vmscan: only allocate and reclaim from zones with pages managed by the buddy allocator
    
    Firmware Assisted Dump (FA_DUMP) on ppc64 reserves substantial amounts
    of memory when booting a secondary kernel.  Srikar Dronamraju reported
    that multiple nodes may have no memory managed by the buddy allocator
    but still return true for populated_zone().
    
    Commit 1d82de618ddd ("mm, vmscan: make kswapd reclaim in terms of
    nodes") was reported to cause kswapd to spin at 100% CPU usage when
    fadump was enabled.  The old code happened to deal with the situation of
    a populated node with zero free pages by co-incidence but the current
    code tries to reclaim populated zones without realising that is
    impossible.
    
    We cannot just convert populated_zone() as many existing users really
    need to check for present_pages.  This patch introduces a managed_zone()
    helper and uses it in the few cases where it is critical that the check
    is made for managed pages -- zonelist construction and page reclaim.
    
    Link: http://lkml.kernel.org/r/20160831195104.GB8119@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reported-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Tested-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d572b78b65e1..7f2ae99e5daf 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -828,9 +828,21 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
  */
 #define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
 
-static inline int populated_zone(struct zone *zone)
+/*
+ * Returns true if a zone has pages managed by the buddy allocator.
+ * All the reclaim decisions have to use this function rather than
+ * populated_zone(). If the whole zone is reserved then we can easily
+ * end up with populated_zone() && !managed_zone().
+ */
+static inline bool managed_zone(struct zone *zone)
+{
+	return zone->managed_pages;
+}
+
+/* Returns true if a zone has memory */
+static inline bool populated_zone(struct zone *zone)
 {
-	return (!!zone->present_pages);
+	return zone->present_pages;
 }
 
 extern int movable_zone;

commit 1eccfa090eaea22558570054bbdc147817e1df5e
Merge: 1bd4403d86a1 ed18adc1cdd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 8 14:48:14 2016 -0700

    Merge tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull usercopy protection from Kees Cook:
     "Tbhis implements HARDENED_USERCOPY verification of copy_to_user and
      copy_from_user bounds checking for most architectures on SLAB and
      SLUB"
    
    * tag 'usercopy-v4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      mm: SLUB hardened usercopy support
      mm: SLAB hardened usercopy support
      s390/uaccess: Enable hardened usercopy
      sparc/uaccess: Enable hardened usercopy
      powerpc/uaccess: Enable hardened usercopy
      ia64/uaccess: Enable hardened usercopy
      arm64/uaccess: Enable hardened usercopy
      ARM: uaccess: Enable hardened usercopy
      x86/uaccess: Enable hardened usercopy
      mm: Hardened usercopy
      mm: Implement stack frame object validation
      mm: Add is_migrate_cma_page

commit d30dd8be06a5ae640766b20ea9ae288832bd12ac
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 28 15:48:14 2016 -0700

    mm: track NR_KERNEL_STACK in KiB instead of number of stacks
    
    Currently, NR_KERNEL_STACK tracks the number of kernel stacks in a zone.
    This only makes sense if each kernel stack exists entirely in one zone,
    and allowing vmapped stacks could break this assumption.
    
    Since frv has THREAD_SIZE < PAGE_SIZE, we need to track kernel stack
    allocations in a unit that divides both THREAD_SIZE and PAGE_SIZE on all
    architectures.  Keep it simple and use KiB.
    
    Link: http://lkml.kernel.org/r/083c71e642c5fa5f1b6898902e1b2db7b48940d4.1468523549.git.luto@kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ca0fbc483441..f2e4e90621ec 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -121,7 +121,7 @@ enum zone_stat_item {
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
-	NR_KERNEL_STACK,
+	NR_KERNEL_STACK_KB,	/* measured in KiB */
 	/* Second 128 byte cacheline */
 	NR_BOUNCE,
 #if IS_ENABLED(CONFIG_ZSMALLOC)

commit 5a1c84b404a7176b8b36e2a0041b6f0adb3151a3
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:47:31 2016 -0700

    mm: remove reclaim and compaction retry approximations
    
    If per-zone LRU accounting is available then there is no point
    approximating whether reclaim and compaction should retry based on pgdat
    statistics.  This is effectively a revert of "mm, vmstat: remove zone
    and node double accounting by approximating retries" with the difference
    that inactive/active stats are still available.  This preserves the
    history of why the approximation was retried and why it had to be
    reverted to handle OOM kills on 32-bit systems.
    
    Link: http://lkml.kernel.org/r/1469110261-7365-4-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1a813ad335f4..ca0fbc483441 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -116,6 +116,7 @@ enum zone_stat_item {
 	NR_ZONE_INACTIVE_FILE,
 	NR_ZONE_ACTIVE_FILE,
 	NR_ZONE_UNEVICTABLE,
+	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,

commit 71c799f4982d340fff86e751898841322f07f235
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jul 28 15:47:26 2016 -0700

    mm: add per-zone lru list stat
    
    When I did stress test with hackbench, I got OOM message frequently
    which didn't ever happen in zone-lru.
    
      gfp_mask=0x26004c0(GFP_KERNEL|__GFP_REPEAT|__GFP_NOTRACK), order=0
      ..
      ..
       __alloc_pages_nodemask+0xe52/0xe60
       ? new_slab+0x39c/0x3b0
       new_slab+0x39c/0x3b0
       ___slab_alloc.constprop.87+0x6da/0x840
       ? __alloc_skb+0x3c/0x260
       ? _raw_spin_unlock_irq+0x27/0x60
       ? trace_hardirqs_on_caller+0xec/0x1b0
       ? finish_task_switch+0xa6/0x220
       ? poll_select_copy_remaining+0x140/0x140
       __slab_alloc.isra.81.constprop.86+0x40/0x6d
       ? __alloc_skb+0x3c/0x260
       kmem_cache_alloc+0x22c/0x260
       ? __alloc_skb+0x3c/0x260
       __alloc_skb+0x3c/0x260
       alloc_skb_with_frags+0x4e/0x1a0
       sock_alloc_send_pskb+0x16a/0x1b0
       ? wait_for_unix_gc+0x31/0x90
       ? alloc_set_pte+0x2ad/0x310
       unix_stream_sendmsg+0x28d/0x340
       sock_sendmsg+0x2d/0x40
       sock_write_iter+0x6c/0xc0
       __vfs_write+0xc0/0x120
       vfs_write+0x9b/0x1a0
       ? __might_fault+0x49/0xa0
       SyS_write+0x44/0x90
       do_fast_syscall_32+0xa6/0x1e0
       sysenter_past_esp+0x45/0x74
    
      Mem-Info:
      active_anon:104698 inactive_anon:105791 isolated_anon:192
       active_file:433 inactive_file:283 isolated_file:22
       unevictable:0 dirty:0 writeback:296 unstable:0
       slab_reclaimable:6389 slab_unreclaimable:78927
       mapped:474 shmem:0 pagetables:101426 bounce:0
       free:10518 free_pcp:334 free_cma:0
      Node 0 active_anon:418792kB inactive_anon:423164kB active_file:1732kB inactive_file:1132kB unevictable:0kB isolated(anon):768kB isolated(file):88kB mapped:1896kB dirty:0kB writeback:1184kB shmem:0kB writeback_tmp:0kB unstable:0kB pages_scanned:1478632 all_unreclaimable? yes
      DMA free:3304kB min:68kB low:84kB high:100kB present:15992kB managed:15916kB mlocked:0kB slab_reclaimable:0kB slab_unreclaimable:4088kB kernel_stack:0kB pagetables:2480kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB
      lowmem_reserve[]: 0 809 1965 1965
      Normal free:3436kB min:3604kB low:4504kB high:5404kB present:897016kB managed:858460kB mlocked:0kB slab_reclaimable:25556kB slab_unreclaimable:311712kB kernel_stack:164608kB pagetables:30844kB bounce:0kB free_pcp:620kB local_pcp:104kB free_cma:0kB
      lowmem_reserve[]: 0 0 9247 9247
      HighMem free:33808kB min:512kB low:1796kB high:3080kB present:1183736kB managed:1183736kB mlocked:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:372252kB bounce:0kB free_pcp:428kB local_pcp:72kB free_cma:0kB
      lowmem_reserve[]: 0 0 0 0
      DMA: 2*4kB (UM) 2*8kB (UM) 0*16kB 1*32kB (U) 1*64kB (U) 2*128kB (UM) 1*256kB (U) 1*512kB (M) 0*1024kB 1*2048kB (U) 0*4096kB = 3192kB
      Normal: 33*4kB (MH) 79*8kB (ME) 11*16kB (M) 4*32kB (M) 2*64kB (ME) 2*128kB (EH) 7*256kB (EH) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 3244kB
      HighMem: 2590*4kB (UM) 1568*8kB (UM) 491*16kB (UM) 60*32kB (UM) 6*64kB (M) 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 33064kB
      Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB
      25121 total pagecache pages
      24160 pages in swap cache
      Swap cache stats: add 86371, delete 62211, find 42865/60187
      Free swap  = 4015560kB
      Total swap = 4192252kB
      524186 pages RAM
      295934 pages HighMem/MovableOnly
      9658 pages reserved
      0 pages cma reserved
    
    The order-0 allocation for normal zone failed while there are a lot of
    reclaimable memory(i.e., anonymous memory with free swap).  I wanted to
    analyze the problem but it was hard because we removed per-zone lru stat
    so I couldn't know how many of anonymous memory there are in normal/dma
    zone.
    
    When we investigate OOM problem, reclaimable memory count is crucial
    stat to find a problem.  Without it, it's hard to parse the OOM message
    so I believe we should keep it.
    
    With per-zone lru stat,
    
      gfp_mask=0x26004c0(GFP_KERNEL|__GFP_REPEAT|__GFP_NOTRACK), order=0
      Mem-Info:
      active_anon:101103 inactive_anon:102219 isolated_anon:0
       active_file:503 inactive_file:544 isolated_file:0
       unevictable:0 dirty:0 writeback:34 unstable:0
       slab_reclaimable:6298 slab_unreclaimable:74669
       mapped:863 shmem:0 pagetables:100998 bounce:0
       free:23573 free_pcp:1861 free_cma:0
      Node 0 active_anon:404412kB inactive_anon:409040kB active_file:2012kB inactive_file:2176kB unevictable:0kB isolated(anon):0kB isolated(file):0kB mapped:3452kB dirty:0kB writeback:136kB shmem:0kB writeback_tmp:0kB unstable:0kB pages_scanned:1320845 all_unreclaimable? yes
      DMA free:3296kB min:68kB low:84kB high:100kB active_anon:5540kB inactive_anon:0kB active_file:0kB inactive_file:0kB present:15992kB managed:15916kB mlocked:0kB slab_reclaimable:248kB slab_unreclaimable:2628kB kernel_stack:792kB pagetables:2316kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB
      lowmem_reserve[]: 0 809 1965 1965
      Normal free:3600kB min:3604kB low:4504kB high:5404kB active_anon:86304kB inactive_anon:0kB active_file:160kB inactive_file:376kB present:897016kB managed:858524kB mlocked:0kB slab_reclaimable:24944kB slab_unreclaimable:296048kB kernel_stack:163832kB pagetables:35892kB bounce:0kB free_pcp:3076kB local_pcp:656kB free_cma:0kB
      lowmem_reserve[]: 0 0 9247 9247
      HighMem free:86156kB min:512kB low:1796kB high:3080kB active_anon:312852kB inactive_anon:410024kB active_file:1924kB inactive_file:2012kB present:1183736kB managed:1183736kB mlocked:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:365784kB bounce:0kB free_pcp:3868kB local_pcp:720kB free_cma:0kB
      lowmem_reserve[]: 0 0 0 0
      DMA: 8*4kB (UM) 8*8kB (UM) 4*16kB (M) 2*32kB (UM) 2*64kB (UM) 1*128kB (M) 3*256kB (UME) 2*512kB (UE) 1*1024kB (E) 0*2048kB 0*4096kB = 3296kB
      Normal: 240*4kB (UME) 160*8kB (UME) 23*16kB (ME) 3*32kB (UE) 3*64kB (UME) 2*128kB (ME) 1*256kB (U) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 3408kB
      HighMem: 10942*4kB (UM) 3102*8kB (UM) 866*16kB (UM) 76*32kB (UM) 11*64kB (UM) 4*128kB (UM) 1*256kB (M) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 86344kB
      Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB
      54409 total pagecache pages
      53215 pages in swap cache
      Swap cache stats: add 300982, delete 247765, find 157978/226539
      Free swap  = 3803244kB
      Total swap = 4192252kB
      524186 pages RAM
      295934 pages HighMem/MovableOnly
      9642 pages reserved
      0 pages cma reserved
    
    With that, we can see normal zone has a 86M reclaimable memory so we can
    know something goes wrong(I will fix the problem in next patch) in
    reclaim.
    
    [mgorman@techsingularity.net: rename zone LRU stats in /proc/vmstat]
     Link: http://lkml.kernel.org/r/20160725072300.GK10438@techsingularity.net
    Link: http://lkml.kernel.org/r/1469110261-7365-2-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a3b7f45aac56..1a813ad335f4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -110,6 +110,12 @@ struct zone_padding {
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
+	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
+	NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,
+	NR_ZONE_ACTIVE_ANON,
+	NR_ZONE_INACTIVE_FILE,
+	NR_ZONE_ACTIVE_FILE,
+	NR_ZONE_UNEVICTABLE,
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,

commit bca6759258dbef378bcf5b872177bcd2259ceb68
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:47:05 2016 -0700

    mm, vmstat: remove zone and node double accounting by approximating retries
    
    The number of LRU pages, dirty pages and writeback pages must be
    accounted for on both zones and nodes because of the reclaim retry
    logic, compaction retry logic and highmem calculations all depending on
    per-zone stats.
    
    Many lowmem allocations are immune from OOM kill due to a check in
    __alloc_pages_may_oom for (ac->high_zoneidx < ZONE_NORMAL) since commit
    03668b3ceb0c ("oom: avoid oom killer for lowmem allocations").  The
    exception is costly high-order allocations or allocations that cannot
    fail.  If the __alloc_pages_may_oom avoids OOM-kill for low-order lowmem
    allocations then it would fall through to __alloc_pages_direct_compact.
    
    This patch will blindly retry reclaim for zone-constrained allocations
    in should_reclaim_retry up to MAX_RECLAIM_RETRIES.  This is not ideal
    but without per-zone stats there are not many alternatives.  The impact
    it that zone-constrained allocations may delay before considering the
    OOM killer.
    
    As there is no guarantee enough memory can ever be freed to satisfy
    compaction, this patch avoids retrying compaction for zone-contrained
    allocations.
    
    In combination, that means that the per-node stats can be used when
    deciding whether to continue reclaim using a rough approximation.  While
    it is possible this will make the wrong decision on occasion, it will
    not infinite loop as the number of reclaim attempts is capped by
    MAX_RECLAIM_RETRIES.
    
    The final step is calculating the number of dirtyable highmem pages.  As
    those calculations only care about the global count of file pages in
    highmem.  This patch uses a global counter used instead of per-zone
    stats as it is sufficient.
    
    In combination, this allows the per-zone LRU and dirty state counters to
    be removed.
    
    [mgorman@techsingularity.net: fix acct_highmem_file_pages()]
      Link: http://lkml.kernel.org/r/1468853426-12858-4-git-send-email-mgorman@techsingularity.netLink: http://lkml.kernel.org/r/1467970510-21195-35-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Suggested by: Michal Hocko <mhocko@kernel.org>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bd33e6f1bed0..a3b7f45aac56 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -110,10 +110,6 @@ struct zone_padding {
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
-	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
-	NR_ZONE_LRU_ANON = NR_ZONE_LRU_BASE,
-	NR_ZONE_LRU_FILE,
-	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,

commit e6cbd7f2efb433d717af72aa8510a9db6f7a7e05
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:50 2016 -0700

    mm, page_alloc: remove fair zone allocation policy
    
    The fair zone allocation policy interleaves allocation requests between
    zones to avoid an age inversion problem whereby new pages are reclaimed
    to balance a zone.  Reclaim is now node-based so this should no longer
    be an issue and the fair zone allocation policy is not free.  This patch
    removes it.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-30-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e19c081c794e..bd33e6f1bed0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -110,7 +110,6 @@ struct zone_padding {
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
-	NR_ALLOC_BATCH,
 	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
 	NR_ZONE_LRU_ANON = NR_ZONE_LRU_BASE,
 	NR_ZONE_LRU_FILE,
@@ -516,10 +515,6 @@ struct zone {
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
 } ____cacheline_internodealigned_in_smp;
 
-enum zone_flags {
-	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
-};
-
 enum pgdat_flags {
 	PGDAT_CONGESTED,		/* pgdat has many dirty pages backed by
 					 * a congested BDI

commit a5f5f91da6ad647fb0cc7fce0e17343c0d1c5a9a
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:32 2016 -0700

    mm: convert zone_reclaim to node_reclaim
    
    As reclaim is now per-node based, convert zone_reclaim to be
    node_reclaim.  It is possible that a node will be reclaimed multiple
    times if it has multiple zones but this is unavoidable without caching
    all nodes traversed so far.  The documentation and interface to
    userspace is the same from a configuration perspective and will will be
    similar in behaviour unless the node-local allocation requests were also
    limited to lower zones.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-24-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e3d6d42722a0..e19c081c794e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -372,14 +372,6 @@ struct zone {
 	unsigned long		*pageblock_flags;
 #endif /* CONFIG_SPARSEMEM */
 
-#ifdef CONFIG_NUMA
-	/*
-	 * zone reclaim becomes active if more unmapped pages exist.
-	 */
-	unsigned long		min_unmapped_pages;
-	unsigned long		min_slab_pages;
-#endif /* CONFIG_NUMA */
-
 	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 	unsigned long		zone_start_pfn;
 
@@ -525,7 +517,6 @@ struct zone {
 } ____cacheline_internodealigned_in_smp;
 
 enum zone_flags {
-	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
 };
 
@@ -540,6 +531,7 @@ enum pgdat_flags {
 	PGDAT_WRITEBACK,		/* reclaim scanning has recently found
 					 * many pages under writeback
 					 */
+	PGDAT_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 };
 
 static inline unsigned long zone_end_pfn(const struct zone *zone)
@@ -688,6 +680,14 @@ typedef struct pglist_data {
 	 */
 	unsigned long		totalreserve_pages;
 
+#ifdef CONFIG_NUMA
+	/*
+	 * zone reclaim becomes active if more unmapped pages exist.
+	 */
+	unsigned long		min_unmapped_pages;
+	unsigned long		min_slab_pages;
+#endif /* CONFIG_NUMA */
+
 	/* Write-intensive fields used by page reclaim */
 	ZONE_PADDING(_pad1_)
 	spinlock_t		lru_lock;

commit c4a25635b60d08853a3e4eaae3ab34419a36cfa2
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:23 2016 -0700

    mm: move vmscan writes and file write accounting to the node
    
    As reclaim is now node-based, it follows that page write activity due to
    page reclaim should also be accounted for on the node.  For consistency,
    also account page writes and page dirtying on a per-node basis.
    
    After this patch, there are a few remaining zone counters that may appear
    strange but are fine.  NUMA stats are still per-zone as this is a
    user-space interface that tools consume.  NR_MLOCK, NR_SLAB_*,
    NR_PAGETABLE, NR_KERNEL_STACK and NR_BOUNCE are all allocations that
    potentially pin low memory and cannot trivially be reclaimed on demand.
    This information is still useful for debugging a page allocation failure
    warning.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-21-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index acd4665c3025..e3d6d42722a0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -122,10 +122,6 @@ enum zone_stat_item {
 	NR_KERNEL_STACK,
 	/* Second 128 byte cacheline */
 	NR_BOUNCE,
-	NR_VMSCAN_WRITE,
-	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
-	NR_DIRTIED,		/* page dirtyings since bootup */
-	NR_WRITTEN,		/* page writings since bootup */
 #if IS_ENABLED(CONFIG_ZSMALLOC)
 	NR_ZSPAGES,		/* allocated in zsmalloc */
 #endif
@@ -165,6 +161,10 @@ enum node_stat_item {
 	NR_SHMEM_PMDMAPPED,
 	NR_ANON_THPS,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
+	NR_VMSCAN_WRITE,
+	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
+	NR_DIRTIED,		/* page dirtyings since bootup */
+	NR_WRITTEN,		/* page writings since bootup */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit 11fb998986a72aa7e997d96d63d52582a01228c5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:20 2016 -0700

    mm: move most file-based accounting to the node
    
    There are now a number of accounting oddities such as mapped file pages
    being accounted for on the node while the total number of file pages are
    accounted on the zone.  This can be coped with to some extent but it's
    confusing so this patch moves the relevant file-based accounted.  Due to
    throttling logic in the page allocator for reliable OOM detection, it is
    still necessary to track dirty and writeback pages on a per-zone basis.
    
    [mgorman@techsingularity.net: fix NR_ZONE_WRITE_PENDING accounting]
      Link: http://lkml.kernel.org/r/1468404004-5085-5-git-send-email-mgorman@techsingularity.net
    Link: http://lkml.kernel.org/r/1467970510-21195-20-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2d4a8804eafa..acd4665c3025 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -114,21 +114,16 @@ enum zone_stat_item {
 	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
 	NR_ZONE_LRU_ANON = NR_ZONE_LRU_BASE,
 	NR_ZONE_LRU_FILE,
+	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
-	NR_FILE_PAGES,
-	NR_FILE_DIRTY,
-	NR_WRITEBACK,
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
 	NR_KERNEL_STACK,
 	/* Second 128 byte cacheline */
-	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
 	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
-	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
-	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
 #if IS_ENABLED(CONFIG_ZSMALLOC)
@@ -142,9 +137,6 @@ enum zone_stat_item {
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
 #endif
-	NR_ANON_THPS,
-	NR_SHMEM_THPS,
-	NR_SHMEM_PMDMAPPED,
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
@@ -164,6 +156,15 @@ enum node_stat_item {
 	NR_ANON_MAPPED,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
+	NR_FILE_PAGES,
+	NR_FILE_DIRTY,
+	NR_WRITEBACK,
+	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
+	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
+	NR_SHMEM_THPS,
+	NR_SHMEM_PMDMAPPED,
+	NR_ANON_THPS,
+	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit 4b9d0fab7166c9323f06d708518a35cf3a90426c
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:17 2016 -0700

    mm: rename NR_ANON_PAGES to NR_ANON_MAPPED
    
    NR_FILE_PAGES  is the number of        file pages.
    NR_FILE_MAPPED is the number of mapped file pages.
    NR_ANON_PAGES  is the number of mapped anon pages.
    
    This is unhelpful naming as it's easy to confuse NR_FILE_MAPPED and
    NR_ANON_PAGES for mapped pages.  This patch renames NR_ANON_PAGES so we
    have
    
    NR_FILE_PAGES  is the number of        file pages.
    NR_FILE_MAPPED is the number of mapped file pages.
    NR_ANON_MAPPED is the number of mapped anon pages.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-19-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 95d34d1e1fb5..2d4a8804eafa 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -161,7 +161,7 @@ enum node_stat_item {
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
 	WORKINGSET_NODERECLAIM,
-	NR_ANON_PAGES,	/* Mapped anonymous pages */
+	NR_ANON_MAPPED,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_VM_NODE_STAT_ITEMS

commit 50658e2e04c12d5cd628381c1b9cb69d0093a9c0
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:14 2016 -0700

    mm: move page mapped accounting to the node
    
    Reclaim makes decisions based on the number of pages that are mapped but
    it's mixing node and zone information.  Account NR_FILE_MAPPED and
    NR_ANON_PAGES pages on the node.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-18-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fae2fe3c6942..95d34d1e1fb5 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -115,9 +115,6 @@ enum zone_stat_item {
 	NR_ZONE_LRU_ANON = NR_ZONE_LRU_BASE,
 	NR_ZONE_LRU_FILE,
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
-	NR_ANON_PAGES,	/* Mapped anonymous pages */
-	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
-			   only modified from process context */
 	NR_FILE_PAGES,
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
@@ -164,6 +161,9 @@ enum node_stat_item {
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
 	WORKINGSET_NODERECLAIM,
+	NR_ANON_PAGES,	/* Mapped anonymous pages */
+	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
+			   only modified from process context */
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit 281e37265f2826ed401d84d6790226448ef3f0e8
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:11 2016 -0700

    mm, page_alloc: consider dirtyable memory in terms of nodes
    
    Historically dirty pages were spread among zones but now that LRUs are
    per-node it is more appropriate to consider dirty pages in a node.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-17-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 62f477d6cfe8..fae2fe3c6942 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -363,12 +363,6 @@ struct zone {
 	struct pglist_data	*zone_pgdat;
 	struct per_cpu_pageset __percpu *pageset;
 
-	/*
-	 * This is a per-zone reserve of pages that are not available
-	 * to userspace allocations.
-	 */
-	unsigned long		totalreserve_pages;
-
 #ifndef CONFIG_SPARSEMEM
 	/*
 	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
@@ -687,6 +681,12 @@ typedef struct pglist_data {
 	/* Number of pages migrated during the rate limiting time interval */
 	unsigned long numabalancing_migrate_nr_pages;
 #endif
+	/*
+	 * This is a per-node reserve of pages that are not available
+	 * to userspace allocations.
+	 */
+	unsigned long		totalreserve_pages;
+
 	/* Write-intensive fields used by page reclaim */
 	ZONE_PADDING(_pad1_)
 	spinlock_t		lru_lock;

commit 1e6b10857f91685c60c341703ece4ae9bb775cf3
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:08 2016 -0700

    mm, workingset: make working set detection node-aware
    
    Working set and refault detection is still zone-based, fix it.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-16-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 895c365e3259..62f477d6cfe8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -145,9 +145,6 @@ enum zone_stat_item {
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
 #endif
-	WORKINGSET_REFAULT,
-	WORKINGSET_ACTIVATE,
-	WORKINGSET_NODERECLAIM,
 	NR_ANON_THPS,
 	NR_SHMEM_THPS,
 	NR_SHMEM_PMDMAPPED,
@@ -164,6 +161,9 @@ enum node_stat_item {
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
+	WORKINGSET_REFAULT,
+	WORKINGSET_ACTIVATE,
+	WORKINGSET_NODERECLAIM,
 	NR_VM_NODE_STAT_ITEMS
 };
 

commit a9dd0a83104c01269ea36a9b4ec42b51edf85427
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:02 2016 -0700

    mm, vmscan: make shrink_node decisions more node-centric
    
    Earlier patches focused on having direct reclaim and kswapd use data
    that is node-centric for reclaiming but shrink_node() itself still uses
    too much zone information.  This patch removes unnecessary zone-based
    information with the most important decision being whether to continue
    reclaim or not.  Some memcg APIs are adjusted as a result even though
    memcg itself still uses some zone information.
    
    [mgorman@techsingularity.net: optimization]
      Link: http://lkml.kernel.org/r/1468588165-12461-2-git-send-email-mgorman@techsingularity.net
    Link: http://lkml.kernel.org/r/1467970510-21195-14-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4062fa74526f..895c365e3259 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -739,9 +739,9 @@ static inline spinlock_t *zone_lru_lock(struct zone *zone)
 	return &zone->zone_pgdat->lru_lock;
 }
 
-static inline struct lruvec *zone_lruvec(struct zone *zone)
+static inline struct lruvec *node_lruvec(struct pglist_data *pgdat)
 {
-	return &zone->zone_pgdat->lruvec;
+	return &pgdat->lruvec;
 }
 
 static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)

commit 38087d9b0360987a6db46c2c2c4ece37cd048abe
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:49 2016 -0700

    mm, vmscan: simplify the logic deciding whether kswapd sleeps
    
    kswapd goes through some complex steps trying to figure out if it should
    stay awake based on the classzone_idx and the requested order.  It is
    unnecessarily complex and passes in an invalid classzone_idx to
    balance_pgdat().  What matters most of all is whether a larger order has
    been requsted and whether kswapd successfully reclaimed at the previous
    order.  This patch irons out the logic to check just that and the end
    result is less headache inducing.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-10-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index edafdaf62e90..4062fa74526f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -668,8 +668,9 @@ typedef struct pglist_data {
 	wait_queue_head_t pfmemalloc_wait;
 	struct task_struct *kswapd;	/* Protected by
 					   mem_hotplug_begin/end() */
-	int kswapd_max_order;
-	enum zone_type classzone_idx;
+	int kswapd_order;
+	enum zone_type kswapd_classzone_idx;
+
 #ifdef CONFIG_COMPACTION
 	int kcompactd_max_order;
 	enum zone_type kcompactd_classzone_idx;

commit 0f66114893997f781029c109b0974b7f61130df7
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:34 2016 -0700

    mm, mmzone: clarify the usage of zone padding
    
    Zone padding separates write-intensive fields used by page allocation,
    compaction and vmstats but the comments are a little misleading and need
    clarification.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-5-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d4f5cac0a8c3..edafdaf62e90 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -477,20 +477,21 @@ struct zone {
 	unsigned long		wait_table_hash_nr_entries;
 	unsigned long		wait_table_bits;
 
+	/* Write-intensive fields used from the page allocator */
 	ZONE_PADDING(_pad1_)
+
 	/* free areas of different sizes */
 	struct free_area	free_area[MAX_ORDER];
 
 	/* zone flags, see below */
 	unsigned long		flags;
 
-	/* Write-intensive fields used from the page allocator */
+	/* Primarily protects free_area */
 	spinlock_t		lock;
 
+	/* Write-intensive fields used by compaction and vmstats. */
 	ZONE_PADDING(_pad2_)
 
-	/* Write-intensive fields used by page reclaim */
-
 	/*
 	 * When free pages are below this point, additional steps are taken
 	 * when reading the number of free pages to avoid per-cpu counter

commit 599d0c954f91d0689c9bb421b5bc04ea02437a41
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:31 2016 -0700

    mm, vmscan: move LRU lists to node
    
    This moves the LRU lists from the zone to the node and related data such
    as counters, tracing, congestion tracking and writeback tracking.
    
    Unfortunately, due to reclaim and compaction retry logic, it is
    necessary to account for the number of LRU pages on both zone and node
    logic.  Most reclaim logic is based on the node counters but the retry
    logic uses the zone counters which do not distinguish inactive and
    active sizes.  It would be possible to leave the LRU counters on a
    per-zone basis but it's a heavier calculation across multiple cache
    lines that is much more frequent than the retry checks.
    
    Other than the LRU counters, this is mostly a mechanical patch but note
    that it introduces a number of anomalies.  For example, the scans are
    per-zone but using per-node counters.  We also mark a node as congested
    when a zone is congested.  This causes weird problems that are fixed
    later but is easier to review.
    
    In the event that there is excessive overhead on 32-bit systems due to
    the nodes being on LRU then there are two potential solutions
    
    1. Long-term isolation of highmem pages when reclaim is lowmem
    
       When pages are skipped, they are immediately added back onto the LRU
       list. If lowmem reclaim persisted for long periods of time, the same
       highmem pages get continually scanned. The idea would be that lowmem
       keeps those pages on a separate list until a reclaim for highmem pages
       arrives that splices the highmem pages back onto the LRU. It potentially
       could be implemented similar to the UNEVICTABLE list.
    
       That would reduce the skip rate with the potential corner case is that
       highmem pages have to be scanned and reclaimed to free lowmem slab pages.
    
    2. Linear scan lowmem pages if the initial LRU shrink fails
    
       This will break LRU ordering but may be preferable and faster during
       memory pressure than skipping LRU pages.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-4-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cfa870107abe..d4f5cac0a8c3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -111,12 +111,9 @@ enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
 	NR_ALLOC_BATCH,
-	NR_LRU_BASE,
-	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
-	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
-	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
-	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
-	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
+	NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
+	NR_ZONE_LRU_ANON = NR_ZONE_LRU_BASE,
+	NR_ZONE_LRU_FILE,
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
@@ -134,12 +131,9 @@ enum zone_stat_item {
 	NR_VMSCAN_WRITE,
 	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
-	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
-	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
-	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
 #if IS_ENABLED(CONFIG_ZSMALLOC)
 	NR_ZSPAGES,		/* allocated in zsmalloc */
 #endif
@@ -161,6 +155,15 @@ enum zone_stat_item {
 	NR_VM_ZONE_STAT_ITEMS };
 
 enum node_stat_item {
+	NR_LRU_BASE,
+	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
+	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
+	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
+	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
+	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
+	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
+	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
+	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
 	NR_VM_NODE_STAT_ITEMS
 };
 
@@ -219,7 +222,7 @@ struct lruvec {
 	/* Evictions & activations on the inactive file list */
 	atomic_long_t			inactive_age;
 #ifdef CONFIG_MEMCG
-	struct zone			*zone;
+	struct pglist_data *pgdat;
 #endif
 };
 
@@ -357,13 +360,6 @@ struct zone {
 #ifdef CONFIG_NUMA
 	int node;
 #endif
-
-	/*
-	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
-	 * this zone's LRU.  Maintained by the pageout code.
-	 */
-	unsigned int inactive_ratio;
-
 	struct pglist_data	*zone_pgdat;
 	struct per_cpu_pageset __percpu *pageset;
 
@@ -495,9 +491,6 @@ struct zone {
 
 	/* Write-intensive fields used by page reclaim */
 
-	/* Fields commonly accessed by the page reclaim scanner */
-	struct lruvec		lruvec;
-
 	/*
 	 * When free pages are below this point, additional steps are taken
 	 * when reading the number of free pages to avoid per-cpu counter
@@ -537,17 +530,20 @@ struct zone {
 
 enum zone_flags {
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
-	ZONE_CONGESTED,			/* zone has many dirty pages backed by
+	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
+};
+
+enum pgdat_flags {
+	PGDAT_CONGESTED,		/* pgdat has many dirty pages backed by
 					 * a congested BDI
 					 */
-	ZONE_DIRTY,			/* reclaim scanning has recently found
+	PGDAT_DIRTY,			/* reclaim scanning has recently found
 					 * many dirty file pages at the tail
 					 * of the LRU.
 					 */
-	ZONE_WRITEBACK,			/* reclaim scanning has recently found
+	PGDAT_WRITEBACK,		/* reclaim scanning has recently found
 					 * many pages under writeback
 					 */
-	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
 };
 
 static inline unsigned long zone_end_pfn(const struct zone *zone)
@@ -707,6 +703,19 @@ typedef struct pglist_data {
 	unsigned long split_queue_len;
 #endif
 
+	/* Fields commonly accessed by the page reclaim scanner */
+	struct lruvec		lruvec;
+
+	/*
+	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
+	 * this node's LRU.  Maintained by the pageout code.
+	 */
+	unsigned int inactive_ratio;
+
+	unsigned long		flags;
+
+	ZONE_PADDING(_pad2_)
+
 	/* Per-node vmstats */
 	struct per_cpu_nodestat __percpu *per_cpu_nodestats;
 	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];
@@ -728,6 +737,11 @@ static inline spinlock_t *zone_lru_lock(struct zone *zone)
 	return &zone->zone_pgdat->lru_lock;
 }
 
+static inline struct lruvec *zone_lruvec(struct zone *zone)
+{
+	return &zone->zone_pgdat->lruvec;
+}
+
 static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)
 {
 	return pgdat->node_start_pfn + pgdat->node_spanned_pages;
@@ -779,12 +793,12 @@ extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
 
 extern void lruvec_init(struct lruvec *lruvec);
 
-static inline struct zone *lruvec_zone(struct lruvec *lruvec)
+static inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
 {
 #ifdef CONFIG_MEMCG
-	return lruvec->zone;
+	return lruvec->pgdat;
 #else
-	return container_of(lruvec, struct zone, lruvec);
+	return container_of(lruvec, struct pglist_data, lruvec);
 #endif
 }
 

commit a52633d8e9c35832f1409dc5fa166019048a3f1f
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:28 2016 -0700

    mm, vmscan: move lru_lock to the node
    
    Node-based reclaim requires node-based LRUs and locking.  This is a
    preparation patch that just moves the lru_lock to the node so later
    patches are easier to review.  It is a mechanical change but note this
    patch makes contention worse because the LRU lock is hotter and direct
    reclaim and kswapd can contend on the same lock even when reclaiming
    from different zones.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-3-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 078ecb81e209..cfa870107abe 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -93,7 +93,7 @@ struct free_area {
 struct pglist_data;
 
 /*
- * zone->lock and zone->lru_lock are two of the hottest locks in the kernel.
+ * zone->lock and the zone lru_lock are two of the hottest locks in the kernel.
  * So add a wild amount of padding here to ensure that they fall into separate
  * cachelines.  There are very few zone structures in the machine, so space
  * consumption is not a concern here.
@@ -496,7 +496,6 @@ struct zone {
 	/* Write-intensive fields used by page reclaim */
 
 	/* Fields commonly accessed by the page reclaim scanner */
-	spinlock_t		lru_lock;
 	struct lruvec		lruvec;
 
 	/*
@@ -690,6 +689,9 @@ typedef struct pglist_data {
 	/* Number of pages migrated during the rate limiting time interval */
 	unsigned long numabalancing_migrate_nr_pages;
 #endif
+	/* Write-intensive fields used by page reclaim */
+	ZONE_PADDING(_pad1_)
+	spinlock_t		lru_lock;
 
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
 	/*
@@ -721,6 +723,10 @@ typedef struct pglist_data {
 
 #define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)
 #define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))
+static inline spinlock_t *zone_lru_lock(struct zone *zone)
+{
+	return &zone->zone_pgdat->lru_lock;
+}
 
 static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)
 {

commit 75ef7184053989118d3814c558a9af62e7376a58
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:24 2016 -0700

    mm, vmstat: add infrastructure for per-node vmstats
    
    Patchset: "Move LRU page reclaim from zones to nodes v9"
    
    This series moves LRUs from the zones to the node.  While this is a
    current rebase, the test results were based on mmotm as of June 23rd.
    Conceptually, this series is simple but there are a lot of details.
    Some of the broad motivations for this are;
    
    1. The residency of a page partially depends on what zone the page was
       allocated from.  This is partially combatted by the fair zone allocation
       policy but that is a partial solution that introduces overhead in the
       page allocator paths.
    
    2. Currently, reclaim on node 0 behaves slightly different to node 1. For
       example, direct reclaim scans in zonelist order and reclaims even if
       the zone is over the high watermark regardless of the age of pages
       in that LRU. Kswapd on the other hand starts reclaim on the highest
       unbalanced zone. A difference in distribution of file/anon pages due
       to when they were allocated results can result in a difference in
       again. While the fair zone allocation policy mitigates some of the
       problems here, the page reclaim results on a multi-zone node will
       always be different to a single-zone node.
       it was scheduled on as a result.
    
    3. kswapd and the page allocator scan zones in the opposite order to
       avoid interfering with each other but it's sensitive to timing.  This
       mitigates the page allocator using pages that were allocated very recently
       in the ideal case but it's sensitive to timing. When kswapd is allocating
       from lower zones then it's great but during the rebalancing of the highest
       zone, the page allocator and kswapd interfere with each other. It's worse
       if the highest zone is small and difficult to balance.
    
    4. slab shrinkers are node-based which makes it harder to identify the exact
       relationship between slab reclaim and LRU reclaim.
    
    The reason we have zone-based reclaim is that we used to have
    large highmem zones in common configurations and it was necessary
    to quickly find ZONE_NORMAL pages for reclaim. Today, this is much
    less of a concern as machines with lots of memory will (or should) use
    64-bit kernels. Combinations of 32-bit hardware and 64-bit hardware are
    rare. Machines that do use highmem should have relatively low highmem:lowmem
    ratios than we worried about in the past.
    
    Conceptually, moving to node LRUs should be easier to understand. The
    page allocator plays fewer tricks to game reclaim and reclaim behaves
    similarly on all nodes.
    
    The series has been tested on a 16 core UMA machine and a 2-socket 48
    core NUMA machine. The UMA results are presented in most cases as the NUMA
    machine behaved similarly.
    
    pagealloc
    ---------
    
    This is a microbenchmark that shows the benefit of removing the fair zone
    allocation policy. It was tested uip to order-4 but only orders 0 and 1 are
    shown as the other orders were comparable.
    
                                               4.7.0-rc4                  4.7.0-rc4
                                          mmotm-20160623                 nodelru-v9
    Min      total-odr0-1               490.00 (  0.00%)           457.00 (  6.73%)
    Min      total-odr0-2               347.00 (  0.00%)           329.00 (  5.19%)
    Min      total-odr0-4               288.00 (  0.00%)           273.00 (  5.21%)
    Min      total-odr0-8               251.00 (  0.00%)           239.00 (  4.78%)
    Min      total-odr0-16              234.00 (  0.00%)           222.00 (  5.13%)
    Min      total-odr0-32              223.00 (  0.00%)           211.00 (  5.38%)
    Min      total-odr0-64              217.00 (  0.00%)           208.00 (  4.15%)
    Min      total-odr0-128             214.00 (  0.00%)           204.00 (  4.67%)
    Min      total-odr0-256             250.00 (  0.00%)           230.00 (  8.00%)
    Min      total-odr0-512             271.00 (  0.00%)           269.00 (  0.74%)
    Min      total-odr0-1024            291.00 (  0.00%)           282.00 (  3.09%)
    Min      total-odr0-2048            303.00 (  0.00%)           296.00 (  2.31%)
    Min      total-odr0-4096            311.00 (  0.00%)           309.00 (  0.64%)
    Min      total-odr0-8192            316.00 (  0.00%)           314.00 (  0.63%)
    Min      total-odr0-16384           317.00 (  0.00%)           315.00 (  0.63%)
    Min      total-odr1-1               742.00 (  0.00%)           712.00 (  4.04%)
    Min      total-odr1-2               562.00 (  0.00%)           530.00 (  5.69%)
    Min      total-odr1-4               457.00 (  0.00%)           433.00 (  5.25%)
    Min      total-odr1-8               411.00 (  0.00%)           381.00 (  7.30%)
    Min      total-odr1-16              381.00 (  0.00%)           356.00 (  6.56%)
    Min      total-odr1-32              372.00 (  0.00%)           346.00 (  6.99%)
    Min      total-odr1-64              372.00 (  0.00%)           343.00 (  7.80%)
    Min      total-odr1-128             375.00 (  0.00%)           351.00 (  6.40%)
    Min      total-odr1-256             379.00 (  0.00%)           351.00 (  7.39%)
    Min      total-odr1-512             385.00 (  0.00%)           355.00 (  7.79%)
    Min      total-odr1-1024            386.00 (  0.00%)           358.00 (  7.25%)
    Min      total-odr1-2048            390.00 (  0.00%)           362.00 (  7.18%)
    Min      total-odr1-4096            390.00 (  0.00%)           362.00 (  7.18%)
    Min      total-odr1-8192            388.00 (  0.00%)           363.00 (  6.44%)
    
    This shows a steady improvement throughout. The primary benefit is from
    reduced system CPU usage which is obvious from the overall times;
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623nodelru-v8
    User          189.19      191.80
    System       2604.45     2533.56
    Elapsed      2855.30     2786.39
    
    The vmstats also showed that the fair zone allocation policy was definitely
    removed as can be seen here;
    
                                 4.7.0-rc3   4.7.0-rc3
                             mmotm-20160623 nodelru-v8
    DMA32 allocs               28794729769           0
    Normal allocs              48432501431 77227309877
    Movable allocs                       0           0
    
    tiobench on ext4
    ----------------
    
    tiobench is a benchmark that artifically benefits if old pages remain resident
    while new pages get reclaimed. The fair zone allocation policy mitigates this
    problem so pages age fairly. While the benchmark has problems, it is important
    that tiobench performance remains constant as it implies that page aging
    problems that the fair zone allocation policy fixes are not re-introduced.
    
                                             4.7.0-rc4             4.7.0-rc4
                                        mmotm-20160623            nodelru-v9
    Min      PotentialReadSpeed        89.65 (  0.00%)       90.21 (  0.62%)
    Min      SeqRead-MB/sec-1          82.68 (  0.00%)       82.01 ( -0.81%)
    Min      SeqRead-MB/sec-2          72.76 (  0.00%)       72.07 ( -0.95%)
    Min      SeqRead-MB/sec-4          75.13 (  0.00%)       74.92 ( -0.28%)
    Min      SeqRead-MB/sec-8          64.91 (  0.00%)       65.19 (  0.43%)
    Min      SeqRead-MB/sec-16         62.24 (  0.00%)       62.22 ( -0.03%)
    Min      RandRead-MB/sec-1          0.88 (  0.00%)        0.88 (  0.00%)
    Min      RandRead-MB/sec-2          0.95 (  0.00%)        0.92 ( -3.16%)
    Min      RandRead-MB/sec-4          1.43 (  0.00%)        1.34 ( -6.29%)
    Min      RandRead-MB/sec-8          1.61 (  0.00%)        1.60 ( -0.62%)
    Min      RandRead-MB/sec-16         1.80 (  0.00%)        1.90 (  5.56%)
    Min      SeqWrite-MB/sec-1         76.41 (  0.00%)       76.85 (  0.58%)
    Min      SeqWrite-MB/sec-2         74.11 (  0.00%)       73.54 ( -0.77%)
    Min      SeqWrite-MB/sec-4         80.05 (  0.00%)       80.13 (  0.10%)
    Min      SeqWrite-MB/sec-8         72.88 (  0.00%)       73.20 (  0.44%)
    Min      SeqWrite-MB/sec-16        75.91 (  0.00%)       76.44 (  0.70%)
    Min      RandWrite-MB/sec-1         1.18 (  0.00%)        1.14 ( -3.39%)
    Min      RandWrite-MB/sec-2         1.02 (  0.00%)        1.03 (  0.98%)
    Min      RandWrite-MB/sec-4         1.05 (  0.00%)        0.98 ( -6.67%)
    Min      RandWrite-MB/sec-8         0.89 (  0.00%)        0.92 (  3.37%)
    Min      RandWrite-MB/sec-16        0.92 (  0.00%)        0.93 (  1.09%)
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623 approx-v9
    User          645.72      525.90
    System        403.85      331.75
    Elapsed      6795.36     6783.67
    
    This shows that the series has little or not impact on tiobench which is
    desirable and a reduction in system CPU usage. It indicates that the fair
    zone allocation policy was removed in a manner that didn't reintroduce
    one class of page aging bug. There were only minor differences in overall
    reclaim activity
    
                                 4.7.0-rc4   4.7.0-rc4
                              mmotm-20160623nodelru-v8
    Minor Faults                    645838      647465
    Major Faults                       573         640
    Swap Ins                             0           0
    Swap Outs                            0           0
    DMA allocs                           0           0
    DMA32 allocs                  46041453    44190646
    Normal allocs                 78053072    79887245
    Movable allocs                       0           0
    Allocation stalls                   24          67
    Stall zone DMA                       0           0
    Stall zone DMA32                     0           0
    Stall zone Normal                    0           2
    Stall zone HighMem                   0           0
    Stall zone Movable                   0          65
    Direct pages scanned             10969       30609
    Kswapd pages scanned          93375144    93492094
    Kswapd pages reclaimed        93372243    93489370
    Direct pages reclaimed           10969       30609
    Kswapd efficiency                  99%         99%
    Kswapd velocity              13741.015   13781.934
    Direct efficiency                 100%        100%
    Direct velocity                  1.614       4.512
    Percentage direct scans             0%          0%
    
    kswapd activity was roughly comparable. There were differences in direct
    reclaim activity but negligible in the context of the overall workload
    (velocity of 4 pages per second with the patches applied, 1.6 pages per
    second in the baseline kernel).
    
    pgbench read-only large configuration on ext4
    ---------------------------------------------
    
    pgbench is a database benchmark that can be sensitive to page reclaim
    decisions. This also checks if removing the fair zone allocation policy
    is safe
    
    pgbench Transactions
                            4.7.0-rc4             4.7.0-rc4
                       mmotm-20160623            nodelru-v8
    Hmean    1       188.26 (  0.00%)      189.78 (  0.81%)
    Hmean    5       330.66 (  0.00%)      328.69 ( -0.59%)
    Hmean    12      370.32 (  0.00%)      380.72 (  2.81%)
    Hmean    21      368.89 (  0.00%)      369.00 (  0.03%)
    Hmean    30      382.14 (  0.00%)      360.89 ( -5.56%)
    Hmean    32      428.87 (  0.00%)      432.96 (  0.95%)
    
    Negligible differences again. As with tiobench, overall reclaim activity
    was comparable.
    
    bonnie++ on ext4
    ----------------
    
    No interesting performance difference, negligible differences on reclaim
    stats.
    
    paralleldd on ext4
    ------------------
    
    This workload uses varying numbers of dd instances to read large amounts of
    data from disk.
    
                                   4.7.0-rc3             4.7.0-rc3
                              mmotm-20160623            nodelru-v9
    Amean    Elapsd-1       186.04 (  0.00%)      189.41 ( -1.82%)
    Amean    Elapsd-3       192.27 (  0.00%)      191.38 (  0.46%)
    Amean    Elapsd-5       185.21 (  0.00%)      182.75 (  1.33%)
    Amean    Elapsd-7       183.71 (  0.00%)      182.11 (  0.87%)
    Amean    Elapsd-12      180.96 (  0.00%)      181.58 ( -0.35%)
    Amean    Elapsd-16      181.36 (  0.00%)      183.72 ( -1.30%)
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623 nodelru-v9
    User         1548.01     1552.44
    System       8609.71     8515.08
    Elapsed      3587.10     3594.54
    
    There is little or no change in performance but some drop in system CPU usage.
    
                                 4.7.0-rc3   4.7.0-rc3
                            mmotm-20160623  nodelru-v9
    Minor Faults                    362662      367360
    Major Faults                      1204        1143
    Swap Ins                            22           0
    Swap Outs                         2855        1029
    DMA allocs                           0           0
    DMA32 allocs                  31409797    28837521
    Normal allocs                 46611853    49231282
    Movable allocs                       0           0
    Direct pages scanned                 0           0
    Kswapd pages scanned          40845270    40869088
    Kswapd pages reclaimed        40830976    40855294
    Direct pages reclaimed               0           0
    Kswapd efficiency                  99%         99%
    Kswapd velocity              11386.711   11369.769
    Direct efficiency                 100%        100%
    Direct velocity                  0.000       0.000
    Percentage direct scans             0%          0%
    Page writes by reclaim            2855        1029
    Page writes file                     0           0
    Page writes anon                  2855        1029
    Page reclaim immediate             771        1628
    Sector Reads                 293312636   293536360
    Sector Writes                 18213568    18186480
    Page rescued immediate               0           0
    Slabs scanned                   128257      132747
    Direct inode steals                181          56
    Kswapd inode steals                 59        1131
    
    It basically shows that kswapd was active at roughly the same rate in
    both kernels. There was also comparable slab scanning activity and direct
    reclaim was avoided in both cases. There appears to be a large difference
    in numbers of inodes reclaimed but the workload has few active inodes and
    is likely a timing artifact.
    
    stutter
    -------
    
    stutter simulates a simple workload. One part uses a lot of anonymous
    memory, a second measures mmap latency and a third copies a large file.
    The primary metric is checking for mmap latency.
    
    stutter
                                 4.7.0-rc4             4.7.0-rc4
                            mmotm-20160623            nodelru-v8
    Min         mmap     16.6283 (  0.00%)     13.4258 ( 19.26%)
    1st-qrtle   mmap     54.7570 (  0.00%)     34.9121 ( 36.24%)
    2nd-qrtle   mmap     57.3163 (  0.00%)     46.1147 ( 19.54%)
    3rd-qrtle   mmap     58.9976 (  0.00%)     47.1882 ( 20.02%)
    Max-90%     mmap     59.7433 (  0.00%)     47.4453 ( 20.58%)
    Max-93%     mmap     60.1298 (  0.00%)     47.6037 ( 20.83%)
    Max-95%     mmap     73.4112 (  0.00%)     82.8719 (-12.89%)
    Max-99%     mmap     92.8542 (  0.00%)     88.8870 (  4.27%)
    Max         mmap   1440.6569 (  0.00%)    121.4201 ( 91.57%)
    Mean        mmap     59.3493 (  0.00%)     42.2991 ( 28.73%)
    Best99%Mean mmap     57.2121 (  0.00%)     41.8207 ( 26.90%)
    Best95%Mean mmap     55.9113 (  0.00%)     39.9620 ( 28.53%)
    Best90%Mean mmap     55.6199 (  0.00%)     39.3124 ( 29.32%)
    Best50%Mean mmap     53.2183 (  0.00%)     33.1307 ( 37.75%)
    Best10%Mean mmap     45.9842 (  0.00%)     20.4040 ( 55.63%)
    Best5%Mean  mmap     43.2256 (  0.00%)     17.9654 ( 58.44%)
    Best1%Mean  mmap     32.9388 (  0.00%)     16.6875 ( 49.34%)
    
    This shows a number of improvements with the worst-case outlier greatly
    improved.
    
    Some of the vmstats are interesting
    
                                 4.7.0-rc4   4.7.0-rc4
                              mmotm-20160623nodelru-v8
    Swap Ins                           163         502
    Swap Outs                            0           0
    DMA allocs                           0           0
    DMA32 allocs                 618719206  1381662383
    Normal allocs                891235743   564138421
    Movable allocs                       0           0
    Allocation stalls                 2603           1
    Direct pages scanned            216787           2
    Kswapd pages scanned          50719775    41778378
    Kswapd pages reclaimed        41541765    41777639
    Direct pages reclaimed          209159           0
    Kswapd efficiency                  81%         99%
    Kswapd velocity              16859.554   14329.059
    Direct efficiency                  96%          0%
    Direct velocity                 72.061       0.001
    Percentage direct scans             0%          0%
    Page writes by reclaim         6215049           0
    Page writes file               6215049           0
    Page writes anon                     0           0
    Page reclaim immediate           70673          90
    Sector Reads                  81940800    81680456
    Sector Writes                100158984    98816036
    Page rescued immediate               0           0
    Slabs scanned                  1366954       22683
    
    While this is not guaranteed in all cases, this particular test showed
    a large reduction in direct reclaim activity. It's also worth noting
    that no page writes were issued from reclaim context.
    
    This series is not without its hazards. There are at least three areas
    that I'm concerned with even though I could not reproduce any problems in
    that area.
    
    1. Reclaim/compaction is going to be affected because the amount of reclaim is
       no longer targetted at a specific zone. Compaction works on a per-zone basis
       so there is no guarantee that reclaiming a few THP's worth page pages will
       have a positive impact on compaction success rates.
    
    2. The Slab/LRU reclaim ratio is affected because the frequency the shrinkers
       are called is now different. This may or may not be a problem but if it
       is, it'll be because shrinkers are not called enough and some balancing
       is required.
    
    3. The anon/file reclaim ratio may be affected. Pages about to be dirtied are
       distributed between zones and the fair zone allocation policy used to do
       something very similar for anon. The distribution is now different but not
       necessarily in any way that matters but it's still worth bearing in mind.
    
    VM statistic counters for reclaim decisions are zone-based.  If the kernel
    is to reclaim on a per-node basis then we need to track per-node
    statistics but there is no infrastructure for that.  The most notable
    change is that the old node_page_state is renamed to
    sum_zone_node_page_state.  The new node_page_state takes a pglist_data and
    uses per-node stats but none exist yet.  There is some renaming such as
    vm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming
    of mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical
    patch with no functional change.  There is a lot of similarity between the
    node and zone helpers which is unfortunate but there was no obvious way of
    reusing the code and maintaining type safety.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-2-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 19425e988bdc..078ecb81e209 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -160,6 +160,10 @@ enum zone_stat_item {
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
+enum node_stat_item {
+	NR_VM_NODE_STAT_ITEMS
+};
+
 /*
  * We do arithmetic on the LRU lists in various places in the code,
  * so it is important to keep the active lists LRU_ACTIVE higher in
@@ -267,6 +271,11 @@ struct per_cpu_pageset {
 #endif
 };
 
+struct per_cpu_nodestat {
+	s8 stat_threshold;
+	s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];
+};
+
 #endif /* !__GENERATING_BOUNDS.H */
 
 enum zone_type {
@@ -695,6 +704,10 @@ typedef struct pglist_data {
 	struct list_head split_queue;
 	unsigned long split_queue_len;
 #endif
+
+	/* Per-node vmstats */
+	struct per_cpu_nodestat __percpu *per_cpu_nodestats;
+	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)

commit 65c453778aea374a46597f4d9826274d1eaf7338
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Jul 26 15:26:10 2016 -0700

    mm, rmap: account shmem thp pages
    
    Let's add ShmemHugePages and ShmemPmdMapped fields into meminfo and
    smaps.  It indicates how many times we allocate and map shmem THP.
    
    NR_ANON_TRANSPARENT_HUGEPAGES is renamed to NR_ANON_THPS.
    
    Link: http://lkml.kernel.org/r/1466021202-61880-27-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3d7ab30d4940..19425e988bdc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -154,7 +154,9 @@ enum zone_stat_item {
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
 	WORKINGSET_NODERECLAIM,
-	NR_ANON_TRANSPARENT_HUGEPAGES,
+	NR_ANON_THPS,
+	NR_SHMEM_THPS,
+	NR_SHMEM_PMDMAPPED,
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 

commit 91537fee001361b1a4d485f1af65d8efa03d49b5
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 26 15:24:45 2016 -0700

    mm: add NR_ZSMALLOC to vmstat
    
    zram is very popular for some of the embedded world (e.g., TV, mobile
    phones).  On those system, zsmalloc's consumed memory size is never
    trivial (one of example from real product system, total memory: 800M,
    zsmalloc consumed: 150M), so we have used this out of tree patch to
    monitor system memory behavior via /proc/vmstat.
    
    With zsmalloc in vmstat, it helps in tracking down system behavior due
    to memory usage.
    
    [minchan@kernel.org: zsmalloc: follow up zsmalloc vmstat]
      Link: http://lkml.kernel.org/r/20160607091737.GC23435@bbox
    [akpm@linux-foundation.org: fix build with CONFIG_ZSMALLOC=m]
    Link: http://lkml.kernel.org/r/1464919731-13255-1-git-send-email-minchan@kernel.org
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Sangseok Lee <sangseok.lee@lge.com>
    Cc: Chanho Min <chanho.min@lge.com>
    Cc: Chan Gyun Jeong <chan.jeong@lge.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3388ccbab7d6..3d7ab30d4940 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -140,6 +140,9 @@ enum zone_stat_item {
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
 	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
+#if IS_ENABLED(CONFIG_ZSMALLOC)
+	NR_ZSPAGES,		/* allocated in zsmalloc */
+#endif
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */

commit 798fd756952c4b6cb7dfe6f6437e9f02da79a5bc
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Tue Jul 26 15:22:30 2016 -0700

    mm: zap ZONE_OOM_LOCKED
    
    Not used since oom_lock was instroduced.
    
    Link: http://lkml.kernel.org/r/1464358093-22663-1-git-send-email-vdavydov@virtuozzo.com
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 02069c23486d..3388ccbab7d6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -524,7 +524,6 @@ struct zone {
 
 enum zone_flags {
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
-	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
 	ZONE_CONGESTED,			/* zone has many dirty pages backed by
 					 * a congested BDI
 					 */

commit 7c15d9bb8231f998ae7dc0b72415f5215459f7fb
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jul 19 15:00:04 2016 -0700

    mm: Add is_migrate_cma_page
    
    Code such as hardened user copy[1] needs a way to tell if a
    page is CMA or not. Add is_migrate_cma_page in a similar way
    to is_migrate_isolate_page.
    
    [1]http://article.gmane.org/gmane.linux.kernel.mm/155238
    
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 02069c23486d..c8478b29f070 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -68,8 +68,10 @@ extern char * const migratetype_names[MIGRATE_TYPES];
 
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
+#  define is_migrate_cma_page(_page) (get_pageblock_migratetype(_page) == MIGRATE_CMA)
 #else
 #  define is_migrate_cma(migratetype) false
+#  define is_migrate_cma_page(_page) false
 #endif
 
 #define for_each_migratetype_order(order, type) \

commit 0c9ad804f178eae02a34045bb0916fa0e31623d5
Author: Weijie Yang <weijie.yang@samsung.com>
Date:   Fri May 20 16:58:04 2016 -0700

    mm fix commmets: if SPARSEMEM, pgdata doesn't have page_ext
    
    If SPARSEMEM, use page_ext in mem_section
    if !SPARSEMEM, use page_ext in pgdata
    
    Signed-off-by: Weijie Yang <weijie.yang@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8dd0333b01dc..02069c23486d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1063,7 +1063,7 @@ struct mem_section {
 	unsigned long *pageblock_flags;
 #ifdef CONFIG_PAGE_EXTENSION
 	/*
-	 * If !SPARSEMEM, pgdat doesn't have page_ext pointer. We use
+	 * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use
 	 * section. (see page_ext.h about this.)
 	 */
 	struct page_ext *page_ext;

commit 86a294a81f93d6f36d00ec3ff779d36d218f852d
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri May 20 16:57:12 2016 -0700

    mm, oom, compaction: prevent from should_compact_retry looping for ever for costly orders
    
    "mm: consider compaction feedback also for costly allocation" has
    removed the upper bound for the reclaim/compaction retries based on the
    number of reclaimed pages for costly orders.  While this is desirable
    the patch did miss a mis interaction between reclaim, compaction and the
    retry logic.  The direct reclaim tries to get zones over min watermark
    while compaction backs off and returns COMPACT_SKIPPED when all zones
    are below low watermark + 1<<order gap.  If we are getting really close
    to OOM then __compaction_suitable can keep returning COMPACT_SKIPPED a
    high order request (e.g.  hugetlb order-9) while the reclaim is not able
    to release enough pages to get us over low watermark.  The reclaim is
    still able to make some progress (usually trashing over few remaining
    pages) so we are not able to break out from the loop.
    
    I have seen this happening with the same test described in "mm: consider
    compaction feedback also for costly allocation" on a swapless system.
    The original problem got resolved by "vmscan: consider classzone_idx in
    compaction_ready" but it shows how things might go wrong when we
    approach the oom event horizont.
    
    The reason why compaction requires being over low rather than min
    watermark is not clear to me.  This check was there essentially since
    56de7263fcf3 ("mm: compaction: direct compact when a high-order
    allocation fails").  It is clearly an implementation detail though and
    we shouldn't pull it into the generic retry logic while we should be
    able to cope with such eventuality.  The only place in
    should_compact_retry where we retry without any upper bound is for
    compaction_withdrawn() case.
    
    Introduce compaction_zonelist_suitable function which checks the given
    zonelist and returns true only if there is at least one zone which would
    would unblock __compaction_suitable if more memory got reclaimed.  In
    this implementation it checks __compaction_suitable with NR_FREE_PAGES
    plus part of the reclaimable memory as the target for the watermark
    check.  The reclaimable memory is reduced linearly by the allocation
    order.  The idea is that we do not want to reclaim all the remaining
    memory for a single allocation request just unblock
    __compaction_suitable which doesn't guarantee we will make a further
    progress.
    
    The new helper is then used if compaction_withdrawn() feedback was
    provided so we do not retry if there is no outlook for a further
    progress.  !costly requests shouldn't be affected much - e.g.  order-2
    pages would require to have at least 64kB on the reclaimable LRUs while
    order-9 would need at least 32M which should be enough to not lock up.
    
    [vbabka@suse.cz: fix classzone_idx vs. high_zoneidx usage in compaction_zonelist_suitable]
    [akpm@linux-foundation.org: fix it for Mel's mm-page_alloc-remove-field-from-alloc_context.patch]
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c60db2096fd8..8dd0333b01dc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -739,6 +739,9 @@ static inline bool is_dev_zone(const struct zone *zone)
 extern struct mutex zonelists_mutex;
 void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
+bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
+			 int classzone_idx, unsigned int alloc_flags,
+			 long free_pages);
 bool zone_watermark_ok(struct zone *z, unsigned int order,
 		unsigned long mark, int classzone_idx,
 		unsigned int alloc_flags);

commit 0b423ca22f95a867f789aab1fe57ee4e378df43b
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu May 19 17:14:27 2016 -0700

    mm, page_alloc: inline pageblock lookup in page free fast paths
    
    The function call overhead of get_pfnblock_flags_mask() is measurable in
    the page free paths.  This patch uses an inlined version that is faster.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4b28d2f8125e..c60db2096fd8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -85,13 +85,6 @@ extern int page_group_by_mobility_disabled;
 	get_pfnblock_flags_mask(page, page_to_pfn(page),		\
 			PB_migrate_end, MIGRATETYPE_MASK)
 
-static inline int get_pfnblock_migratetype(struct page *page, unsigned long pfn)
-{
-	BUILD_BUG_ON(PB_migrate_end - PB_migrate != 2);
-	return get_pfnblock_flags_mask(page, pfn, PB_migrate_end,
-					MIGRATETYPE_MASK);
-}
-
 struct free_area {
 	struct list_head	free_list[MIGRATE_TYPES];
 	unsigned long		nr_free;

commit c33d6c06f60f710f0305ae792773e1c2560e1e51
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu May 19 17:14:10 2016 -0700

    mm, page_alloc: avoid looking up the first zone in a zonelist twice
    
    The allocator fast path looks up the first usable zone in a zonelist and
    then get_page_from_freelist does the same job in the zonelist iterator.
    This patch preserves the necessary information.
    
                                                 4.6.0-rc2                  4.6.0-rc2
                                            fastmark-v1r20             initonce-v1r20
      Min      alloc-odr0-1               364.00 (  0.00%)           359.00 (  1.37%)
      Min      alloc-odr0-2               262.00 (  0.00%)           260.00 (  0.76%)
      Min      alloc-odr0-4               214.00 (  0.00%)           214.00 (  0.00%)
      Min      alloc-odr0-8               186.00 (  0.00%)           186.00 (  0.00%)
      Min      alloc-odr0-16              173.00 (  0.00%)           173.00 (  0.00%)
      Min      alloc-odr0-32              165.00 (  0.00%)           165.00 (  0.00%)
      Min      alloc-odr0-64              161.00 (  0.00%)           162.00 ( -0.62%)
      Min      alloc-odr0-128             159.00 (  0.00%)           161.00 ( -1.26%)
      Min      alloc-odr0-256             168.00 (  0.00%)           170.00 ( -1.19%)
      Min      alloc-odr0-512             180.00 (  0.00%)           181.00 ( -0.56%)
      Min      alloc-odr0-1024            190.00 (  0.00%)           190.00 (  0.00%)
      Min      alloc-odr0-2048            196.00 (  0.00%)           196.00 (  0.00%)
      Min      alloc-odr0-4096            202.00 (  0.00%)           202.00 (  0.00%)
      Min      alloc-odr0-8192            206.00 (  0.00%)           205.00 (  0.49%)
      Min      alloc-odr0-16384           206.00 (  0.00%)           205.00 (  0.49%)
    
    The benefit is negligible and the results are within the noise but each
    cycle counts.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 327f0fa1e1ce..4b28d2f8125e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -959,13 +959,10 @@ static __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,
  */
 static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 					enum zone_type highest_zoneidx,
-					nodemask_t *nodes,
-					struct zone **zone)
+					nodemask_t *nodes)
 {
-	struct zoneref *z = next_zones_zonelist(zonelist->_zonerefs,
+	return next_zones_zonelist(zonelist->_zonerefs,
 							highest_zoneidx, nodes);
-	*zone = zonelist_zone(z);
-	return z;
 }
 
 /**
@@ -980,10 +977,17 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
  * within a given nodemask
  */
 #define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
-	for (z = first_zones_zonelist(zlist, highidx, nodemask, &zone);	\
+	for (z = first_zones_zonelist(zlist, highidx, nodemask), zone = zonelist_zone(z);	\
 		zone;							\
 		z = next_zones_zonelist(++z, highidx, nodemask),	\
-			zone = zonelist_zone(z))			\
+			zone = zonelist_zone(z))
+
+#define for_next_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
+	for (zone = z->zone;	\
+		zone;							\
+		z = next_zones_zonelist(++z, highidx, nodemask),	\
+			zone = zonelist_zone(z))
+
 
 /**
  * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index

commit c603844bdcb5238980de8d58b393f52d7729d651
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu May 19 17:13:38 2016 -0700

    mm, page_alloc: convert alloc_flags to unsigned
    
    alloc_flags is a bitmask of flags but it is signed which does not
    necessarily generate the best code depending on the compiler.  Even
    without an impact, it makes more sense that this be unsigned.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cfcd7723edb6..327f0fa1e1ce 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -747,7 +747,8 @@ extern struct mutex zonelists_mutex;
 void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool zone_watermark_ok(struct zone *z, unsigned int order,
-		unsigned long mark, int classzone_idx, int alloc_flags);
+		unsigned long mark, int classzone_idx,
+		unsigned int alloc_flags);
 bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
 		unsigned long mark, int classzone_idx);
 enum memmap_context {

commit 682a3385e7734fa3abbd504cbeb5fe91793f1827
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu May 19 17:13:30 2016 -0700

    mm, page_alloc: inline the fast path of the zonelist iterator
    
    The page allocator iterates through a zonelist for zones that match the
    addressing limitations and nodemask of the caller but many allocations
    will not be restricted.  Despite this, there is always functional call
    overhead which builds up.
    
    This patch inlines the optimistic basic case and only calls the iterator
    function for the complex case.  A hindrance was the fact that
    cpuset_current_mems_allowed is used in the fastpath as the allowed
    nodemask even though all nodes are allowed on most systems.  The patch
    handles this by only considering cpuset_current_mems_allowed if a cpuset
    exists.  As well as being faster in the fast-path, this removes some
    junk in the slowpath.
    
    The performance difference on a page allocator microbenchmark is;
    
                                                 4.6.0-rc2                  4.6.0-rc2
                                          statinline-v1r20              optiter-v1r20
      Min      alloc-odr0-1               412.00 (  0.00%)           382.00 (  7.28%)
      Min      alloc-odr0-2               301.00 (  0.00%)           282.00 (  6.31%)
      Min      alloc-odr0-4               247.00 (  0.00%)           233.00 (  5.67%)
      Min      alloc-odr0-8               215.00 (  0.00%)           203.00 (  5.58%)
      Min      alloc-odr0-16              199.00 (  0.00%)           188.00 (  5.53%)
      Min      alloc-odr0-32              191.00 (  0.00%)           182.00 (  4.71%)
      Min      alloc-odr0-64              187.00 (  0.00%)           177.00 (  5.35%)
      Min      alloc-odr0-128             185.00 (  0.00%)           175.00 (  5.41%)
      Min      alloc-odr0-256             193.00 (  0.00%)           184.00 (  4.66%)
      Min      alloc-odr0-512             207.00 (  0.00%)           197.00 (  4.83%)
      Min      alloc-odr0-1024            213.00 (  0.00%)           203.00 (  4.69%)
      Min      alloc-odr0-2048            220.00 (  0.00%)           209.00 (  5.00%)
      Min      alloc-odr0-4096            226.00 (  0.00%)           214.00 (  5.31%)
      Min      alloc-odr0-8192            229.00 (  0.00%)           218.00 (  4.80%)
      Min      alloc-odr0-16384           229.00 (  0.00%)           219.00 (  4.37%)
    
    perf indicated that next_zones_zonelist disappeared in the profile and
    __next_zones_zonelist did not appear.  This is expected as the
    micro-benchmark would hit the inlined fast-path every time.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 150c6049f961..cfcd7723edb6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -919,6 +919,10 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
 #endif /* CONFIG_NUMA */
 }
 
+struct zoneref *__next_zones_zonelist(struct zoneref *z,
+					enum zone_type highest_zoneidx,
+					nodemask_t *nodes);
+
 /**
  * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
  * @z - The cursor used as a starting point for the search
@@ -931,9 +935,14 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
  * being examined. It should be advanced by one before calling
  * next_zones_zonelist again.
  */
-struct zoneref *next_zones_zonelist(struct zoneref *z,
+static __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,
 					enum zone_type highest_zoneidx,
-					nodemask_t *nodes);
+					nodemask_t *nodes)
+{
+	if (likely(!nodes && zonelist_zone_idx(z) <= highest_zoneidx))
+		return z;
+	return __next_zones_zonelist(z, highest_zoneidx, nodes);
+}
 
 /**
  * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist

commit 29f9cb53d25cd9916537b44b0af7f0b95a2e4438
Author: Chanho Min <chanho.min@lge.com>
Date:   Thu May 19 17:11:57 2016 -0700

    mm/highmem: simplify is_highmem()
    
    is_highmem() can be simplified by use of is_highmem_idx().  This patch
    removes redundant code and will make it easier to maintain if the zone
    policy is changed or a new zone is added.
    
    (akpm: saves me 25 bytes of text per is_highmem() callsite)
    
    Signed-off-by: Chanho Min <chanho.min@lge.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c60df9257cc7..150c6049f961 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -828,10 +828,7 @@ static inline int is_highmem_idx(enum zone_type idx)
 static inline int is_highmem(struct zone *zone)
 {
 #ifdef CONFIG_HIGHMEM
-	int zone_off = (char *)zone - (char *)zone->zone_pgdat->node_zones;
-	return zone_off == ZONE_HIGHMEM * sizeof(*zone) ||
-	       (zone_off == ZONE_MOVABLE * sizeof(*zone) &&
-		zone_movable_is_highmem());
+	return is_highmem_idx(zone_idx(zone));
 #else
 	return 0;
 #endif

commit 795ae7a0de6b834a0cc202aa55c190ef81496665
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Mar 17 14:19:14 2016 -0700

    mm: scale kswapd watermarks in proportion to memory
    
    In machines with 140G of memory and enterprise flash storage, we have
    seen read and write bursts routinely exceed the kswapd watermarks and
    cause thundering herds in direct reclaim.  Unfortunately, the only way
    to tune kswapd aggressiveness is through adjusting min_free_kbytes - the
    system's emergency reserves - which is entirely unrelated to the
    system's latency requirements.  In order to get kswapd to maintain a
    250M buffer of free memory, the emergency reserves need to be set to 1G.
    That is a lot of memory wasted for no good reason.
    
    On the other hand, it's reasonable to assume that allocation bursts and
    overall allocation concurrency scale with memory capacity, so it makes
    sense to make kswapd aggressiveness a function of that as well.
    
    Change the kswapd watermark scale factor from the currently fixed 25% of
    the tunable emergency reserve to a tunable 0.1% of memory.
    
    Beyond 1G of memory, this will produce bigger watermark steps than the
    current formula in default settings.  Ensure that the new formula never
    chooses steps smaller than that, i.e.  25% of the emergency reserve.
    
    On a 140G machine, this raises the default watermark steps - the
    distance between min and low, and low and high - from 16M to 143M.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bdd9a270a813..c60df9257cc7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -841,6 +841,8 @@ static inline int is_highmem(struct zone *zone)
 struct ctl_table;
 int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
+int watermark_scale_factor_sysctl_handler(struct ctl_table *, int,
+					void __user *, size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1];
 int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);

commit 698b1b30642f1ff0ea10ef1de9745ab633031377
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Mar 17 14:18:08 2016 -0700

    mm, compaction: introduce kcompactd
    
    Memory compaction can be currently performed in several contexts:
    
     - kswapd balancing a zone after a high-order allocation failure
     - direct compaction to satisfy a high-order allocation, including THP
       page fault attemps
     - khugepaged trying to collapse a hugepage
     - manually from /proc
    
    The purpose of compaction is two-fold.  The obvious purpose is to
    satisfy a (pending or future) high-order allocation, and is easy to
    evaluate.  The other purpose is to keep overal memory fragmentation low
    and help the anti-fragmentation mechanism.  The success wrt the latter
    purpose is more
    
    The current situation wrt the purposes has a few drawbacks:
    
     - compaction is invoked only when a high-order page or hugepage is not
       available (or manually).  This might be too late for the purposes of
       keeping memory fragmentation low.
     - direct compaction increases latency of allocations.  Again, it would
       be better if compaction was performed asynchronously to keep
       fragmentation low, before the allocation itself comes.
     - (a special case of the previous) the cost of compaction during THP
       page faults can easily offset the benefits of THP.
     - kswapd compaction appears to be complex, fragile and not working in
       some scenarios.  It could also end up compacting for a high-order
       allocation request when it should be reclaiming memory for a later
       order-0 request.
    
    To improve the situation, we should be able to benefit from an
    equivalent of kswapd, but for compaction - i.e. a background thread
    which responds to fragmentation and the need for high-order allocations
    (including hugepages) somewhat proactively.
    
    One possibility is to extend the responsibilities of kswapd, which could
    however complicate its design too much.  It should be better to let
    kswapd handle reclaim, as order-0 allocations are often more critical
    than high-order ones.
    
    Another possibility is to extend khugepaged, but this kthread is a
    single instance and tied to THP configs.
    
    This patch goes with the option of a new set of per-node kthreads called
    kcompactd, and lays the foundations, without introducing any new
    tunables.  The lifecycle mimics kswapd kthreads, including the memory
    hotplug hooks.
    
    For compaction, kcompactd uses the standard compaction_suitable() and
    ompact_finished() criteria and the deferred compaction functionality.
    Unlike direct compaction, it uses only sync compaction, as there's no
    allocation latency to minimize.
    
    This patch doesn't yet add a call to wakeup_kcompactd.  The kswapd
    compact/reclaim loop for high-order pages will be replaced by waking up
    kcompactd in the next patch with the description of what's wrong with
    the old approach.
    
    Waking up of the kcompactd threads is also tied to kswapd activity and
    follows these rules:
     - we don't want to affect any fastpaths, so wake up kcompactd only from
       the slowpath, as it's done for kswapd
     - if kswapd is doing reclaim, it's more important than compaction, so
       don't invoke kcompactd until kswapd goes to sleep
     - the target order used for kswapd is passed to kcompactd
    
    Future possible future uses for kcompactd include the ability to wake up
    kcompactd on demand in special situations, such as when hugepages are
    not available (currently not done due to __GFP_NO_KSWAPD) or when a
    fragmentation event (i.e.  __rmqueue_fallback()) occurs.  It's also
    possible to perform periodic compaction with kcompactd.
    
    [arnd@arndb.de: fix build errors with kcompactd]
    [paul.gortmaker@windriver.com: don't use modular references for non modular code]
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6de02ac378a0..bdd9a270a813 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -668,6 +668,12 @@ typedef struct pglist_data {
 					   mem_hotplug_begin/end() */
 	int kswapd_max_order;
 	enum zone_type classzone_idx;
+#ifdef CONFIG_COMPACTION
+	int kcompactd_max_order;
+	enum zone_type kcompactd_classzone_idx;
+	wait_queue_head_t kcompactd_wait;
+	struct task_struct *kcompactd;
+#endif
 #ifdef CONFIG_NUMA_BALANCING
 	/* Lock serializing the migrate rate limiting window */
 	spinlock_t numabalancing_migrate_lock;

commit 7cf91a98e607c2f935dbcc177d70011e95b8faff
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Mar 15 14:57:51 2016 -0700

    mm/compaction: speed up pageblock_pfn_to_page() when zone is contiguous
    
    There is a performance drop report due to hugepage allocation and in
    there half of cpu time are spent on pageblock_pfn_to_page() in
    compaction [1].
    
    In that workload, compaction is triggered to make hugepage but most of
    pageblocks are un-available for compaction due to pageblock type and
    skip bit so compaction usually fails.  Most costly operations in this
    case is to find valid pageblock while scanning whole zone range.  To
    check if pageblock is valid to compact, valid pfn within pageblock is
    required and we can obtain it by calling pageblock_pfn_to_page().  This
    function checks whether pageblock is in a single zone and return valid
    pfn if possible.  Problem is that we need to check it every time before
    scanning pageblock even if we re-visit it and this turns out to be very
    expensive in this workload.
    
    Although we have no way to skip this pageblock check in the system where
    hole exists at arbitrary position, we can use cached value for zone
    continuity and just do pfn_to_page() in the system where hole doesn't
    exist.  This optimization considerably speeds up in above workload.
    
    Before vs After
      Max: 1096 MB/s vs 1325 MB/s
      Min: 635 MB/s 1015 MB/s
      Avg: 899 MB/s 1194 MB/s
    
    Avg is improved by roughly 30% [2].
    
    [1]: http://www.spinics.net/lists/linux-mm/msg97378.html
    [2]: https://lkml.org/lkml/2015/12/9/23
    
    [akpm@linux-foundation.org: don't forget to restore zone->contiguous on error path, per Vlastimil]
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reported-by: Aaron Lu <aaron.lu@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Tested-by: Aaron Lu <aaron.lu@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 03cbdd906f55..6de02ac378a0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -522,6 +522,8 @@ struct zone {
 	bool			compact_blockskip_flush;
 #endif
 
+	bool			contiguous;
+
 	ZONE_PADDING(_pad3_)
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];

commit 23047a96d7cfcfca1a6d026ecaec526ea4803e9e
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Mar 15 14:57:16 2016 -0700

    mm: workingset: per-cgroup cache thrash detection
    
    Cache thrash detection (see a528910e12ec "mm: thrash detection-based
    file cache sizing" for details) currently only works on the system
    level, not inside cgroups.  Worse, as the refaults are compared to the
    global number of active cache, cgroups might wrongfully get all their
    refaults activated when their pages are hotter than those of others.
    
    Move the refault machinery from the zone to the lruvec, and then tag
    eviction entries with the memcg ID.  This makes the thrash detection
    work correctly inside cgroups.
    
    [sergey.senozhatsky@gmail.com: do not return from workingset_activation() with locked rcu and page]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9fc23ab550a7..03cbdd906f55 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -212,10 +212,12 @@ struct zone_reclaim_stat {
 };
 
 struct lruvec {
-	struct list_head lists[NR_LRU_LISTS];
-	struct zone_reclaim_stat reclaim_stat;
+	struct list_head		lists[NR_LRU_LISTS];
+	struct zone_reclaim_stat	reclaim_stat;
+	/* Evictions & activations on the inactive file list */
+	atomic_long_t			inactive_age;
 #ifdef CONFIG_MEMCG
-	struct zone *zone;
+	struct zone			*zone;
 #endif
 };
 
@@ -490,9 +492,6 @@ struct zone {
 	spinlock_t		lru_lock;
 	struct lruvec		lruvec;
 
-	/* Evictions & activations on the inactive file list */
-	atomic_long_t		inactive_age;
-
 	/*
 	 * When free pages are below this point, additional steps are taken
 	 * when reading the number of free pages to avoid per-cpu counter
@@ -761,6 +760,8 @@ static inline struct zone *lruvec_zone(struct lruvec *lruvec)
 #endif
 }
 
+extern unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru);
+
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);
 #else

commit 60f30350fd69a3e4d5f0f45937d3274c22565134
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Mar 15 14:56:08 2016 -0700

    mm, page_owner: print migratetype of page and pageblock, symbolic flags
    
    The information in /sys/kernel/debug/page_owner includes the migratetype
    of the pageblock the page belongs to.  This is also checked against the
    page's migratetype (as declared by gfp_flags during its allocation), and
    the page is reported as Fallback if its migratetype differs from the
    pageblock's one.  t This is somewhat misleading because in fact fallback
    allocation is not the only reason why these two can differ.  It also
    doesn't direcly provide the page's migratetype, although it's possible
    to derive that from the gfp_flags.
    
    It's arguably better to print both page and pageblock's migratetype and
    leave the interpretation to the consumer than to suggest fallback
    allocation as the only possible reason.  While at it, we can print the
    migratetypes as string the same way as /proc/pagetypeinfo does, as some
    of the numeric values depend on kernel configuration.  For that, this
    patch moves the migratetype_names array from #ifdef CONFIG_PROC_FS part
    of mm/vmstat.c to mm/page_alloc.c and exports it.
    
    With the new format strings for flags, we can now also provide symbolic
    page and gfp flags in the /sys/kernel/debug/page_owner file.  This
    replaces the positional printing of page flags as single letters, which
    might have looked nicer, but was limited to a subset of flags, and
    required the user to remember the letters.
    
    Example page_owner entry after the patch:
    
      Page allocated via order 0, mask 0x24213ca(GFP_HIGHUSER_MOVABLE|__GFP_COLD|__GFP_NOWARN|__GFP_NORETRY)
      PFN 520 type Movable Block 1 type Movable Flags 0xfffff8001006c(referenced|uptodate|lru|active|mappedtodisk)
       [<ffffffff811682c4>] __alloc_pages_nodemask+0x134/0x230
       [<ffffffff811b4058>] alloc_pages_current+0x88/0x120
       [<ffffffff8115e386>] __page_cache_alloc+0xe6/0x120
       [<ffffffff8116ba6c>] __do_page_cache_readahead+0xdc/0x240
       [<ffffffff8116bd05>] ondemand_readahead+0x135/0x260
       [<ffffffff8116bfb1>] page_cache_sync_readahead+0x31/0x50
       [<ffffffff81160523>] generic_file_read_iter+0x453/0x760
       [<ffffffff811e0d57>] __vfs_read+0xa7/0xd0
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7b6c2cfee390..9fc23ab550a7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -63,6 +63,9 @@ enum {
 	MIGRATE_TYPES
 };
 
+/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */
+extern char * const migratetype_names[MIGRATE_TYPES];
+
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
 #else

commit a3d0a918502cc73af4f60da2cc4c5cac5573f183
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Feb 2 16:57:08 2016 -0800

    thp: make split_queue per-node
    
    Andrea Arcangeli suggested to make split queue per-node to improve
    scalability.  Let's do it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 33bb1b19273e..7b6c2cfee390 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -682,6 +682,12 @@ typedef struct pglist_data {
 	 */
 	unsigned long first_deferred_pfn;
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	spinlock_t split_queue_lock;
+	struct list_head split_queue;
+	unsigned long split_queue_len;
+#endif
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)

commit a8d0143730d7b42c9fe6d1435d92ecce6863a62a
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:20:15 2016 -0800

    mm: page_alloc: generalize the dirty balance reserve
    
    The dirty balance reserve that dirty throttling has to consider is
    merely memory not available to userspace allocations.  There is nothing
    writeback-specific about it.  Generalize the name so that it's reusable
    outside of that context.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3b6fb71bbeb3..33bb1b19273e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -356,10 +356,10 @@ struct zone {
 	struct per_cpu_pageset __percpu *pageset;
 
 	/*
-	 * This is a per-zone reserve of pages that should not be
-	 * considered dirtyable memory.
+	 * This is a per-zone reserve of pages that are not available
+	 * to userspace allocations.
 	 */
-	unsigned long		dirty_balance_reserve;
+	unsigned long		totalreserve_pages;
 
 #ifndef CONFIG_SPARSEMEM
 	/*

commit 5b80287a65da927742c6d43b1369bd5ed133aad1
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu Jan 14 15:19:11 2016 -0800

    mm/mmzone.c: memmap_valid_within() can be boolean
    
    Make memmap_valid_within return bool due to this particular function
    only using either one or zero as its return value.
    
    No functional change.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 12c98dfc31b1..3b6fb71bbeb3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1200,13 +1200,13 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
  * the zone and PFN linkages are still valid. This is expensive, but walkers
  * of the full memmap are extremely rare.
  */
-int memmap_valid_within(unsigned long pfn,
+bool memmap_valid_within(unsigned long pfn,
 					struct page *page, struct zone *zone);
 #else
-static inline int memmap_valid_within(unsigned long pfn,
+static inline bool memmap_valid_within(unsigned long pfn,
 					struct page *page, struct zone *zone)
 {
-	return 1;
+	return true;
 }
 #endif /* CONFIG_ARCH_HAS_HOLES_MEMORYMODEL */
 

commit c00eb15a8914b8ba84032a36044a5aaf7f71709d
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu Jan 14 15:19:00 2016 -0800

    mm/zonelist: enumerate zonelists array index
    
    Hardcoding index to zonelists array in gfp_zonelist() is not a good
    idea, let's enumerate it to improve readability.
    
    No functional change.
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: fix CONFIG_NUMA=n build]
    [n-horiguchi@ah.jp.nec.com: fix warning in comparing enumerator]
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 996384672c73..12c98dfc31b1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -571,19 +571,17 @@ static inline bool zone_is_empty(struct zone *zone)
 /* Maximum number of zones on a zonelist */
 #define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)
 
+enum {
+	ZONELIST_FALLBACK,	/* zonelist with fallback */
 #ifdef CONFIG_NUMA
-
-/*
- * The NUMA zonelists are doubled because we need zonelists that restrict the
- * allocations to a single node for __GFP_THISNODE.
- *
- * [0]	: Zonelist with fallback
- * [1]	: No fallback (__GFP_THISNODE)
- */
-#define MAX_ZONELISTS 2
-#else
-#define MAX_ZONELISTS 1
+	/*
+	 * The NUMA zonelists are doubled because we need zonelists that
+	 * restrict the allocations to a single node for __GFP_THISNODE.
+	 */
+	ZONELIST_NOFALLBACK,	/* zonelist without fallback (__GFP_THISNODE) */
 #endif
+	MAX_ZONELISTS
+};
 
 /*
  * This struct contains information about a zone in a zonelist. It is stored

commit 06640290bfc6688062387f915c5df094e9872133
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu Jan 14 15:18:57 2016 -0800

    include/linux/mmzone.h: remove unused is_unevictable_lru()
    
    Since commit a0b8cab3b9b2 ("mm: remove lru parameter from
    __pagevec_lru_add and remove parts of pagevec API") there's no
    user of this function anymore, so remove it.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e23a9e704536..996384672c73 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -195,11 +195,6 @@ static inline int is_active_lru(enum lru_list lru)
 	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
 }
 
-static inline int is_unevictable_lru(enum lru_list lru)
-{
-	return (lru == LRU_UNEVICTABLE);
-}
-
 struct zone_reclaim_stat {
 	/*
 	 * The pageout code in vmscan.c keeps track of how many of the

commit 89903327607232de32f05100cf03f9390b858e0b
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Nov 6 16:28:46 2015 -0800

    include/linux/mmzone.h: reflow comment
    
    Someone has an 86 column display.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d3bafe4ff32b..e23a9e704536 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -337,12 +337,13 @@ struct zone {
 	unsigned long nr_reserved_highatomic;
 
 	/*
-	 * We don't know if the memory that we're going to allocate will be freeable
-	 * or/and it will be released eventually, so to avoid totally wasting several
-	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
-	 * to run OOM on the lower zones despite there's tons of freeable ram
-	 * on the higher zones). This array is recalculated at runtime if the
-	 * sysctl_lowmem_reserve_ratio sysctl changes.
+	 * We don't know if the memory that we're going to allocate will be
+	 * freeable or/and it will be released eventually, so to avoid totally
+	 * wasting several GB of ram we must reserve some of the lower zone
+	 * memory (otherwise we risk to run OOM on the lower zones despite
+	 * there being tons of freeable ram on the higher zones).  This array is
+	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
+	 * changes.
 	 */
 	long lowmem_reserve[MAX_NR_ZONES];
 

commit 0aaa29a56e4fb0fc9e24edb649e2733a672ca099
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:37 2015 -0800

    mm, page_alloc: reserve pageblocks for high-order atomic allocations on demand
    
    High-order watermark checking exists for two reasons -- kswapd high-order
    awareness and protection for high-order atomic requests.  Historically the
    kernel depended on MIGRATE_RESERVE to preserve min_free_kbytes as
    high-order free pages for as long as possible.  This patch introduces
    MIGRATE_HIGHATOMIC that reserves pageblocks for high-order atomic
    allocations on demand and avoids using those blocks for order-0
    allocations.  This is more flexible and reliable than MIGRATE_RESERVE was.
    
    A MIGRATE_HIGHORDER pageblock is created when an atomic high-order
    allocation request steals a pageblock but limits the total number to 1% of
    the zone.  Callers that speculatively abuse atomic allocations for
    long-lived high-order allocations to access the reserve will quickly fail.
     Note that SLUB is currently not such an abuser as it reclaims at least
    once.  It is possible that the pageblock stolen has few suitable
    high-order pages and will need to steal again in the near future but there
    would need to be strong justification to search all pageblocks for an
    ideal candidate.
    
    The pageblocks are unreserved if an allocation fails after a direct
    reclaim attempt.
    
    The watermark checks account for the reserved pageblocks when the
    allocation request is not a high-order atomic allocation.
    
    The reserved pageblocks can not be used for order-0 allocations.  This may
    allow temporary wastage until a failed reclaim reassigns the pageblock.
    This is deliberate as the intent of the reservation is to satisfy a
    limited number of atomic high-order short-lived requests if the system
    requires them.
    
    The stutter benchmark was used to evaluate this but while it was running
    there was a systemtap script that randomly allocated between 1 high-order
    page and 12.5% of memory's worth of order-3 pages using GFP_ATOMIC.  This
    is much larger than the potential reserve and it does not attempt to be
    realistic.  It is intended to stress random high-order allocations from an
    unknown source, show that there is a reduction in failures without
    introducing an anomaly where atomic allocations are more reliable than
    regular allocations.  The amount of memory reserved varied throughout the
    workload as reserves were created and reclaimed under memory pressure.
    The allocation failures once the workload warmed up were as follows;
    
    4.2-rc5-vanilla         70%
    4.2-rc5-atomic-reserve  56%
    
    The failure rate was also measured while building multiple kernels.  The
    failure rate was 14% but is 6% with this patch applied.
    
    Overall, this is a small reduction but the reserves are small relative to
    the number of allocation requests.  In early versions of the patch, the
    failure rate reduced by a much larger amount but that required much larger
    reserves and perversely made atomic allocations seem more reliable than
    regular allocations.
    
    [yalin.wang2010@gmail.com: fix redundant check and a memory leak]
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: yalin wang <yalin.wang2010@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b86cfa3313cf..d3bafe4ff32b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -39,6 +39,8 @@ enum {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
 	MIGRATE_RECLAIMABLE,
+	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
+	MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,
 #ifdef CONFIG_CMA
 	/*
 	 * MIGRATE_CMA migration type is designed to mimic the way
@@ -61,8 +63,6 @@ enum {
 	MIGRATE_TYPES
 };
 
-#define MIGRATE_PCPTYPES (MIGRATE_RECLAIMABLE+1)
-
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
 #else
@@ -334,6 +334,8 @@ struct zone {
 	/* zone watermarks, access with *_wmark_pages(zone) macros */
 	unsigned long watermark[NR_WMARK];
 
+	unsigned long nr_reserved_highatomic;
+
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several

commit 974a786e63c96a2401a78ddba926f34c128474f1
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:34 2015 -0800

    mm, page_alloc: remove MIGRATE_RESERVE
    
    MIGRATE_RESERVE preserves an old property of the buddy allocator that
    existed prior to fragmentation avoidance -- min_free_kbytes worth of pages
    tended to remain contiguous until the only alternative was to fail the
    allocation.  At the time it was discovered that high-order atomic
    allocations relied on this property so MIGRATE_RESERVE was introduced.  A
    later patch will introduce an alternative MIGRATE_HIGHATOMIC so this patch
    deletes MIGRATE_RESERVE and supporting code so it'll be easier to review.
    Note that this patch in isolation may look like a false regression if
    someone was bisecting high-order atomic allocation failures.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1e88aae329ff..b86cfa3313cf 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -39,8 +39,6 @@ enum {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
 	MIGRATE_RECLAIMABLE,
-	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
-	MIGRATE_RESERVE = MIGRATE_PCPTYPES,
 #ifdef CONFIG_CMA
 	/*
 	 * MIGRATE_CMA migration type is designed to mimic the way
@@ -63,6 +61,8 @@ enum {
 	MIGRATE_TYPES
 };
 
+#define MIGRATE_PCPTYPES (MIGRATE_RECLAIMABLE+1)
+
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
 #else
@@ -429,12 +429,6 @@ struct zone {
 
 	const char		*name;
 
-	/*
-	 * Number of MIGRATE_RESERVE page block. To maintain for just
-	 * optimization. Protected by zone->lock.
-	 */
-	int			nr_migrate_reserve_block;
-
 #ifdef CONFIG_MEMORY_ISOLATION
 	/*
 	 * Number of isolated pageblock. It is used to solve incorrect

commit f77cf4e4cc9d40310a7224a1a67c733aeec78836
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:31 2015 -0800

    mm, page_alloc: delete the zonelist_cache
    
    The zonelist cache (zlc) was introduced to skip over zones that were
    recently known to be full.  This avoided expensive operations such as the
    cpuset checks, watermark calculations and zone_reclaim.  The situation
    today is different and the complexity of zlc is harder to justify.
    
    1) The cpuset checks are no-ops unless a cpuset is active and in general
       are a lot cheaper.
    
    2) zone_reclaim is now disabled by default and I suspect that was a large
       source of the cost that zlc wanted to avoid. When it is enabled, it's
       known to be a major source of stalling when nodes fill up and it's
       unwise to hit every other user with the overhead.
    
    3) Watermark checks are expensive to calculate for high-order
       allocation requests. Later patches in this series will reduce the cost
       of the watermark checking.
    
    4) The most important issue is that in the current implementation it
       is possible for a failed THP allocation to mark a zone full for order-0
       allocations and cause a fallback to remote nodes.
    
    The last issue could be addressed with additional complexity but as the
    benefit of zlc is questionable, it is better to remove it.  If stalls due
    to zone_reclaim are ever reported then an alternative would be to
    introduce deferring logic based on a timeout inside zone_reclaim itself
    and leave the page allocator fast paths alone.
    
    The impact on page-allocator microbenchmarks is negligible as they don't
    hit the paths where the zlc comes into play.  Most page-reclaim related
    workloads showed no noticeable difference as a result of the removal.
    
    The impact was noticeable in a workload called "stutter".  One part uses a
    lot of anonymous memory, a second measures mmap latency and a third copies
    a large file.  In an ideal world the latency application would not notice
    the mmap latency.  On a 2-node machine the results of this patch are
    
    stutter
                                 4.3.0-rc1             4.3.0-rc1
                                  baseline              nozlc-v4
    Min         mmap     20.9243 (  0.00%)     20.7716 (  0.73%)
    1st-qrtle   mmap     22.0612 (  0.00%)     22.0680 ( -0.03%)
    2nd-qrtle   mmap     22.3291 (  0.00%)     22.3809 ( -0.23%)
    3rd-qrtle   mmap     25.2244 (  0.00%)     25.2396 ( -0.06%)
    Max-90%     mmap     48.0995 (  0.00%)     28.3713 ( 41.02%)
    Max-93%     mmap     52.5557 (  0.00%)     36.0170 ( 31.47%)
    Max-95%     mmap     55.8173 (  0.00%)     47.3163 ( 15.23%)
    Max-99%     mmap     67.3781 (  0.00%)     70.1140 ( -4.06%)
    Max         mmap  24447.6375 (  0.00%)  12915.1356 ( 47.17%)
    Mean        mmap     33.7883 (  0.00%)     27.7944 ( 17.74%)
    Best99%Mean mmap     27.7825 (  0.00%)     25.2767 (  9.02%)
    Best95%Mean mmap     26.3912 (  0.00%)     23.7994 (  9.82%)
    Best90%Mean mmap     24.9886 (  0.00%)     23.2251 (  7.06%)
    Best50%Mean mmap     22.0157 (  0.00%)     22.0261 ( -0.05%)
    Best10%Mean mmap     21.6705 (  0.00%)     21.6083 (  0.29%)
    Best5%Mean  mmap     21.5581 (  0.00%)     21.4611 (  0.45%)
    Best1%Mean  mmap     21.3079 (  0.00%)     21.1631 (  0.68%)
    
    Note that the maximum stall latency went from 24 seconds to 12 which is
    still bad but an improvement.  The milage varies considerably 2-node
    machine on an earlier test went from 494 seconds to 47 seconds and a
    4-node machine that tested an earlier version of this patch went from a
    worst case stall time of 6 seconds to 67ms.  The nature of the benchmark
    is inherently unpredictable as it is hammering the system and the milage
    will vary between machines.
    
    There is a secondary impact with potentially more direct reclaim because
    zones are now being considered instead of being skipped by zlc.  In this
    particular test run it did not occur so will not be described.  However,
    in at least one test the following was observed
    
    1. Direct reclaim rates were higher. This was likely due to direct reclaim
      being entered instead of the zlc disabling a zone and busy looping.
      Busy looping may have the effect of allowing kswapd to make more
      progress and in some cases may be better overall. If this is found then
      the correct action is to put direct reclaimers to sleep on a waitqueue
      and allow kswapd make forward progress. Busy looping on the zlc is even
      worse than when the allocator used to blindly call congestion_wait().
    
    2. There was higher swap activity as direct reclaim was active.
    
    3. Direct reclaim efficiency was lower. This is related to 1 as more
      scanning activity also encountered more pages that could not be
      immediately reclaimed
    
    In that case, the direct page scan and reclaim rates are noticeable but
    it is not considered a problem for a few reasons
    
    1. The test is primarily concerned with latency. The mmap attempts are also
       faulted which means there are THP allocation requests. The ZLC could
       cause zones to be disabled causing the process to busy loop instead
       of reclaiming.  This looks like elevated direct reclaim activity but
       it's the correct action to take based on what processes requested.
    
    2. The test hammers reclaim and compaction heavily. The number of successful
       THP faults is highly variable but affects the reclaim stats. It's not a
       realistic or reasonable measure of page reclaim activity.
    
    3. No other page-reclaim intensive workload that was tested showed a problem.
    
    4. If a workload is identified that benefitted from the busy looping then it
       should be fixed by having direct reclaimers sleep on a wait queue until
       woken by kswapd instead of busy looping. We had this class of problem before
       when congestion_waits() with a fixed timeout was a brain damaged decision
       but happened to benefit some workloads.
    
    If a workload is identified that relied on the zlc to busy loop then it
    should be fixed correctly and have a direct reclaimer sleep on a waitqueue
    until woken by kswapd.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 38bed71758ab..1e88aae329ff 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -589,75 +589,8 @@ static inline bool zone_is_empty(struct zone *zone)
  * [1]	: No fallback (__GFP_THISNODE)
  */
 #define MAX_ZONELISTS 2
-
-
-/*
- * We cache key information from each zonelist for smaller cache
- * footprint when scanning for free pages in get_page_from_freelist().
- *
- * 1) The BITMAP fullzones tracks which zones in a zonelist have come
- *    up short of free memory since the last time (last_fullzone_zap)
- *    we zero'd fullzones.
- * 2) The array z_to_n[] maps each zone in the zonelist to its node
- *    id, so that we can efficiently evaluate whether that node is
- *    set in the current tasks mems_allowed.
- *
- * Both fullzones and z_to_n[] are one-to-one with the zonelist,
- * indexed by a zones offset in the zonelist zones[] array.
- *
- * The get_page_from_freelist() routine does two scans.  During the
- * first scan, we skip zones whose corresponding bit in 'fullzones'
- * is set or whose corresponding node in current->mems_allowed (which
- * comes from cpusets) is not set.  During the second scan, we bypass
- * this zonelist_cache, to ensure we look methodically at each zone.
- *
- * Once per second, we zero out (zap) fullzones, forcing us to
- * reconsider nodes that might have regained more free memory.
- * The field last_full_zap is the time we last zapped fullzones.
- *
- * This mechanism reduces the amount of time we waste repeatedly
- * reexaming zones for free memory when they just came up low on
- * memory momentarilly ago.
- *
- * The zonelist_cache struct members logically belong in struct
- * zonelist.  However, the mempolicy zonelists constructed for
- * MPOL_BIND are intentionally variable length (and usually much
- * shorter).  A general purpose mechanism for handling structs with
- * multiple variable length members is more mechanism than we want
- * here.  We resort to some special case hackery instead.
- *
- * The MPOL_BIND zonelists don't need this zonelist_cache (in good
- * part because they are shorter), so we put the fixed length stuff
- * at the front of the zonelist struct, ending in a variable length
- * zones[], as is needed by MPOL_BIND.
- *
- * Then we put the optional zonelist cache on the end of the zonelist
- * struct.  This optional stuff is found by a 'zlcache_ptr' pointer in
- * the fixed length portion at the front of the struct.  This pointer
- * both enables us to find the zonelist cache, and in the case of
- * MPOL_BIND zonelists, (which will just set the zlcache_ptr to NULL)
- * to know that the zonelist cache is not there.
- *
- * The end result is that struct zonelists come in two flavors:
- *  1) The full, fixed length version, shown below, and
- *  2) The custom zonelists for MPOL_BIND.
- * The custom MPOL_BIND zonelists have a NULL zlcache_ptr and no zlcache.
- *
- * Even though there may be multiple CPU cores on a node modifying
- * fullzones or last_full_zap in the same zonelist_cache at the same
- * time, we don't lock it.  This is just hint data - if it is wrong now
- * and then, the allocator will still function, perhaps a bit slower.
- */
-
-
-struct zonelist_cache {
-	unsigned short z_to_n[MAX_ZONES_PER_ZONELIST];		/* zone->nid */
-	DECLARE_BITMAP(fullzones, MAX_ZONES_PER_ZONELIST);	/* zone full? */
-	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
-};
 #else
 #define MAX_ZONELISTS 1
-struct zonelist_cache;
 #endif
 
 /*
@@ -675,9 +608,6 @@ struct zoneref {
  * allocation, the other zones are fallback zones, in decreasing
  * priority.
  *
- * If zlcache_ptr is not NULL, then it is just the address of zlcache,
- * as explained above.  If zlcache_ptr is NULL, there is no zlcache.
- * *
  * To speed the reading of the zonelist, the zonerefs contain the zone index
  * of the entry being read. Helper functions to access information given
  * a struct zoneref are
@@ -687,11 +617,7 @@ struct zoneref {
  * zonelist_node_idx()	- Return the index of the node for an entry
  */
 struct zonelist {
-	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
 	struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];
-#ifdef CONFIG_NUMA
-	struct zonelist_cache zlcache;			     // optional ...
-#endif
 };
 
 #ifndef CONFIG_DISCONTIGMEM

commit 016c13daa5c9e4827eca703e2f0621c131f2cca3
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:18 2015 -0800

    mm, page_alloc: use masks and shifts when converting GFP flags to migrate types
    
    This patch redefines which GFP bits are used for specifying mobility and
    the order of the migrate types.  Once redefined it's possible to convert
    GFP flags to a migrate type with a simple mask and shift.  The only
    downside is that readers of OOM kill messages and allocation failures may
    have been used to the existing values but scripts/gfp-translate will help.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e326843c995a..38bed71758ab 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -37,8 +37,8 @@
 
 enum {
 	MIGRATE_UNMOVABLE,
-	MIGRATE_RECLAIMABLE,
 	MIGRATE_MOVABLE,
+	MIGRATE_RECLAIMABLE,
 	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
 	MIGRATE_RESERVE = MIGRATE_PCPTYPES,
 #ifdef CONFIG_CMA

commit e2b19197ff9dc46f3e3888f273c4395f9e5a9856
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:09 2015 -0800

    mm, page_alloc: remove unnecessary parameter from zone_watermark_ok_safe
    
    Overall, the intent of this series is to remove the zonelist cache which
    was introduced to avoid high overhead in the page allocator.  Once this is
    done, it is necessary to reduce the cost of watermark checks.
    
    The series starts with minor micro-optimisations.
    
    Next it notes that GFP flags that affect watermark checks are abused.
    __GFP_WAIT historically identified callers that could not sleep and could
    access reserves.  This was later abused to identify callers that simply
    prefer to avoid sleeping and have other options.  A patch distinguishes
    between atomic callers, high-priority callers and those that simply wish
    to avoid sleep.
    
    The zonelist cache has been around for a long time but it is of dubious
    merit with a lot of complexity and some issues that are explained.  The
    most important issue is that a failed THP allocation can cause a zone to
    be treated as "full".  This potentially causes unnecessary stalls, reclaim
    activity or remote fallbacks.  The issues could be fixed but it's not
    worth it.  The series places a small number of other micro-optimisations
    on top before examining GFP flags watermarks.
    
    High-order watermarks enforcement can cause high-order allocations to fail
    even though pages are free.  The watermark checks both protect high-order
    atomic allocations and make kswapd aware of high-order pages but there is
    a much better way that can be handled using migrate types.  This series
    uses page grouping by mobility to reserve pageblocks for high-order
    allocations with the size of the reservation depending on demand.  kswapd
    awareness is maintained by examining the free lists.  By patch 12 in this
    series, there are no high-order watermark checks while preserving the
    properties that motivated the introduction of the watermark checks.
    
    This patch (of 10):
    
    No user of zone_watermark_ok_safe() specifies alloc_flags.  This patch
    removes the unnecessary parameter.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2d7e660cdefe..e326843c995a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -817,7 +817,7 @@ void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool zone_watermark_ok(struct zone *z, unsigned int order,
 		unsigned long mark, int classzone_idx, int alloc_flags);
 bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
-		unsigned long mark, int classzone_idx, int alloc_flags);
+		unsigned long mark, int classzone_idx);
 enum memmap_context {
 	MEMMAP_EARLY,
 	MEMMAP_HOTPLUG,

commit b171e4093017d4d6e411f5e97823e5e4a21266a2
Author: Yaowei Bai <bywxiaobai@163.com>
Date:   Thu Nov 5 18:47:06 2015 -0800

    mm/page_alloc: remove unused parameter in init_currently_empty_zone()
    
    Commit a2f3aa025766 ("[PATCH] Fix sparsemem on Cell") fixed an oops
    experienced on the Cell architecture when init-time functions,
    early_*(), are called at runtime by introducing an 'enum memmap_context'
    parameter to memmap_init_zone() and init_currently_empty_zone().  This
    parameter is intended to be used to tell whether the call of these two
    functions is being made on behalf of a hotplug event, or happening at
    boot-time.  However, init_currently_empty_zone() does not use this
    parameter at all, so remove it.
    
    Signed-off-by: Yaowei Bai <bywxiaobai@163.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d94347737292..2d7e660cdefe 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -823,8 +823,7 @@ enum memmap_context {
 	MEMMAP_HOTPLUG,
 };
 extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
-				     unsigned long size,
-				     enum memmap_context context);
+				     unsigned long size);
 
 extern void lruvec_init(struct lruvec *lruvec);
 

commit 12f03ee606914317e7e6a0815e53a48205c31dae
Merge: d9241b22b58e 004f1afbe199
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 14:35:59 2015 -0700

    Merge tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This update has successfully completed a 0day-kbuild run and has
      appeared in a linux-next release.  The changes outside of the typical
      drivers/nvdimm/ and drivers/acpi/nfit.[ch] paths are related to the
      removal of IORESOURCE_CACHEABLE, the introduction of memremap(), and
      the introduction of ZONE_DEVICE + devm_memremap_pages().
    
      Summary:
    
       - Introduce ZONE_DEVICE and devm_memremap_pages() as a generic
         mechanism for adding device-driver-discovered memory regions to the
         kernel's direct map.
    
         This facility is used by the pmem driver to enable pfn_to_page()
         operations on the page frames returned by DAX ('direct_access' in
         'struct block_device_operations').
    
         For now, the 'memmap' allocation for these "device" pages comes
         from "System RAM".  Support for allocating the memmap from device
         memory will arrive in a later kernel.
    
       - Introduce memremap() to replace usages of ioremap_cache() and
         ioremap_wt().  memremap() drops the __iomem annotation for these
         mappings to memory that do not have i/o side effects.  The
         replacement of ioremap_cache() with memremap() is limited to the
         pmem driver to ease merging the api change in v4.3.
    
         Completion of the conversion is targeted for v4.4.
    
       - Similar to the usage of memcpy_to_pmem() + wmb_pmem() in the pmem
         driver, update the VFS DAX implementation and PMEM api to provide
         persistence guarantees for kernel operations on a DAX mapping.
    
       - Convert the ACPI NFIT 'BLK' driver to map the block apertures as
         cacheable to improve performance.
    
       - Miscellaneous updates and fixes to libnvdimm including support for
         issuing "address range scrub" commands, clarifying the optimal
         'sector size' of pmem devices, a clarification of the usage of the
         ACPI '_STA' (status) property for DIMM devices, and other minor
         fixes"
    
    * tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (34 commits)
      libnvdimm, pmem: direct map legacy pmem by default
      libnvdimm, pmem: 'struct page' for pmem
      libnvdimm, pfn: 'struct page' provider infrastructure
      x86, pmem: clarify that ARCH_HAS_PMEM_API implies PMEM mapped WB
      add devm_memremap_pages
      mm: ZONE_DEVICE for "device memory"
      mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
      dax: drop size parameter to ->direct_access()
      nd_blk: change aperture mapping from WC to WB
      nvdimm: change to use generic kvfree()
      pmem, dax: have direct_access use __pmem annotation
      dax: update I/O path to do proper PMEM flushing
      pmem: add copy_from_iter_pmem() and clear_pmem()
      pmem, x86: clean up conditional pmem includes
      pmem: remove layer when calling arch_has_wmb_pmem()
      pmem, x86: move x86 PMEM API to new pmem.h header
      libnvdimm, e820: make CONFIG_X86_PMEM_LEGACY a tristate option
      pmem: switch to devm_ allocations
      devres: add devm_memremap
      libnvdimm, btt: write and validate parent_uuid
      ...

commit 4e6dab4233f667c0ae465e5cb46603b49b4f6d74
Author: minkyung88.kim <minkyung88.kim@lge.com>
Date:   Fri Sep 4 15:48:16 2015 -0700

    mm: remove struct node_active_region
    
    struct node_active_region is not used anymore.  Remove it.
    
    Signed-off-by: minkyung88.kim <minkyung88.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 754c25966a0a..ac00e2050943 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -690,14 +690,6 @@ struct zonelist {
 #endif
 };
 
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
-struct node_active_region {
-	unsigned long start_pfn;
-	unsigned long end_pfn;
-	int nid;
-};
-#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
-
 #ifndef CONFIG_DISCONTIGMEM
 /* The array of struct pages - for discontigmem use pgdat->lmem_map */
 extern struct page *mem_map;

commit 033fbae988fcb67e5077203512181890848b8e90
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Aug 9 15:29:06 2015 -0400

    mm: ZONE_DEVICE for "device memory"
    
    While pmem is usable as a block device or via DAX mappings to userspace
    there are several usage scenarios that can not target pmem due to its
    lack of struct page coverage. In preparation for "hot plugging" pmem
    into the vmemmap add ZONE_DEVICE as a new zone to tag these pages
    separately from the ones that are subject to standard page allocations.
    Importantly "device memory" can be removed at will by userspace
    unbinding the driver of the device.
    
    Having a separate zone prevents allocation and otherwise marks these
    pages that are distinct from typical uniform memory.  Device memory has
    different lifetime and performance characteristics than RAM.  However,
    since we have run out of ZONES_SHIFT bits this functionality currently
    depends on sacrificing ZONE_DMA.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Jerome Glisse <j.glisse@gmail.com>
    [hch: various simplifications in the arch interface]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 754c25966a0a..9217fd93c25b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -319,7 +319,11 @@ enum zone_type {
 	ZONE_HIGHMEM,
 #endif
 	ZONE_MOVABLE,
+#ifdef CONFIG_ZONE_DEVICE
+	ZONE_DEVICE,
+#endif
 	__MAX_NR_ZONES
+
 };
 
 #ifndef __GENERATING_BOUNDS_H
@@ -794,6 +798,25 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 	return !pgdat->node_start_pfn && !pgdat->node_spanned_pages;
 }
 
+static inline int zone_id(const struct zone *zone)
+{
+	struct pglist_data *pgdat = zone->zone_pgdat;
+
+	return zone - pgdat->node_zones;
+}
+
+#ifdef CONFIG_ZONE_DEVICE
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return zone_id(zone) == ZONE_DEVICE;
+}
+#else
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return false;
+}
+#endif
+
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;

commit 3a80a7fa7989fbb6aa56bb6ad31811b62cf99e60
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jun 30 14:57:02 2015 -0700

    mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set
    
    This patch initalises all low memory struct pages and 2G of the highest
    zone on each node during memory initialisation if
    CONFIG_DEFERRED_STRUCT_PAGE_INIT is set.  That config option cannot be set
    but will be available in a later patch.  Parallel initialisation of struct
    page depends on some features from memory hotplug and it is necessary to
    alter alter section annotations.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Nate Zimmer <nzimmer@sgi.com>
    Tested-by: Waiman Long <waiman.long@hp.com>
    Tested-by: Daniel J Blueman <daniel@numascale.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Nate Zimmer <nzimmer@sgi.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1e05dc7449cd..754c25966a0a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -762,6 +762,14 @@ typedef struct pglist_data {
 	/* Number of pages migrated during the rate limiting time interval */
 	unsigned long numabalancing_migrate_nr_pages;
 #endif
+
+#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+	/*
+	 * If memory initialisation on large machines is deferred then this
+	 * is the first PFN that needs to be initialised.
+	 */
+	unsigned long first_deferred_pfn;
+#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)

commit 75a592a47129dcfc1aec40e7d3cdf239a767d441
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jun 30 14:56:59 2015 -0700

    mm: meminit: inline some helper functions
    
    early_pfn_in_nid() and meminit_pfn_in_nid() are small functions that are
    unnecessarily visible outside memory initialisation.  As well as
    unnecessary visibility, it's unnecessary function call overhead when
    initialising pages.  This patch moves the helpers inline.
    
    [akpm@linux-foundation.org: fix build]
    [mhocko@suse.cz: fix build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Nate Zimmer <nzimmer@sgi.com>
    Tested-by: Waiman Long <waiman.long@hp.com>
    Tested-by: Daniel J Blueman <daniel@numascale.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Nate Zimmer <nzimmer@sgi.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b2473d822549..1e05dc7449cd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1227,15 +1227,6 @@ struct mminit_pfnnid_cache {
 	int last_nid;
 };
 
-#ifdef CONFIG_NODES_SPAN_OTHER_NODES
-bool early_pfn_in_nid(unsigned long pfn, int nid);
-bool meminit_pfn_in_nid(unsigned long pfn, int node,
-			struct mminit_pfnnid_cache *state);
-#else
-#define early_pfn_in_nid(pfn, nid)		(1)
-#define meminit_pfn_in_nid(pfn, nid, state)	(1)
-#endif
-
 #ifndef early_pfn_valid
 #define early_pfn_valid(pfn)	(1)
 #endif

commit 8a942fdea560d4ac0e9d9fabcd5201ad20e0c382
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jun 30 14:56:55 2015 -0700

    mm: meminit: make __early_pfn_to_nid SMP-safe and introduce meminit_pfn_in_nid
    
    __early_pfn_to_nid() use static variables to cache recent lookups as
    memblock lookups are very expensive but it assumes that memory
    initialisation is single-threaded.  Parallel initialisation of struct
    pages will break that assumption so this patch makes __early_pfn_to_nid()
    SMP-safe by requiring the caller to cache recent search information.
    early_pfn_to_nid() keeps the same interface but is only safe to use early
    in boot due to the use of a global static variable.  meminit_pfn_in_nid()
    is an SMP-safe version that callers must maintain their own state for.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Nate Zimmer <nzimmer@sgi.com>
    Tested-by: Waiman Long <waiman.long@hp.com>
    Tested-by: Daniel J Blueman <daniel@numascale.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Nate Zimmer <nzimmer@sgi.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 54d74f6eb233..b2473d822549 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1216,10 +1216,24 @@ void sparse_init(void);
 #define sparse_index_init(_sec, _nid)  do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
+/*
+ * During memory init memblocks map pfns to nids. The search is expensive and
+ * this caches recent lookups. The implementation of __early_pfn_to_nid
+ * may treat start/end as pfns or sections.
+ */
+struct mminit_pfnnid_cache {
+	unsigned long last_start;
+	unsigned long last_end;
+	int last_nid;
+};
+
 #ifdef CONFIG_NODES_SPAN_OTHER_NODES
 bool early_pfn_in_nid(unsigned long pfn, int nid);
+bool meminit_pfn_in_nid(unsigned long pfn, int node,
+			struct mminit_pfnnid_cache *state);
 #else
-#define early_pfn_in_nid(pfn, nid)	(1)
+#define early_pfn_in_nid(pfn, nid)		(1)
+#define meminit_pfn_in_nid(pfn, nid, state)	(1)
 #endif
 
 #ifndef early_pfn_valid

commit d7e4a2ea51c1b0ac0b2d53f5ab4a2e878b1c4aec
Author: Zhang Zhen <zhenzhang.zhang@huawei.com>
Date:   Wed Apr 15 16:12:57 2015 -0700

    mm: refactor zone_movable_is_highmem()
    
    All callers of zone_movable_is_highmem are under #ifdef CONFIG_HIGHMEM,
    so the else branch return 0 is not needed.
    
    Signed-off-by: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2782df47101e..54d74f6eb233 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -842,16 +842,16 @@ static inline int populated_zone(struct zone *zone)
 
 extern int movable_zone;
 
+#ifdef CONFIG_HIGHMEM
 static inline int zone_movable_is_highmem(void)
 {
-#if defined(CONFIG_HIGHMEM) && defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 	return movable_zone == ZONE_HIGHMEM;
-#elif defined(CONFIG_HIGHMEM)
-	return (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;
 #else
-	return 0;
+	return (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;
 #endif
 }
+#endif
 
 static inline int is_highmem_idx(enum zone_type idx)
 {

commit a368ab67aa55615a03b2c9c00fb965bee3ebeaa4
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Apr 7 14:26:41 2015 -0700

    mm: move zone lock to a different cache line than order-0 free page lists
    
    Huang Ying reported the following problem due to commit 3484b2de9499 ("mm:
    rearrange zone fields into read-only, page alloc, statistics and page
    reclaim lines") from the Intel performance tests
    
        24b7e5819ad5cbef  3484b2de9499df23c4604a513b
        ----------------  --------------------------
                 %stddev     %change         %stddev
                     \          |                \
            152288 \261  0%     -46.2%      81911 \261  0%  aim7.jobs-per-min
               237 \261  0%     +85.6%        440 \261  0%  aim7.time.elapsed_time
               237 \261  0%     +85.6%        440 \261  0%  aim7.time.elapsed_time.max
             25026 \261  0%     +70.7%      42712 \261  0%  aim7.time.system_time
           2186645 \261  5%     +32.0%    2885949 \261  4%  aim7.time.voluntary_context_switches
           4576561 \261  1%     +24.9%    5715773 \261  0%  aim7.time.involuntary_context_switches
    
    The problem is specific to very large machines under stress.  It was not
    reproducible with the machines I had used to justify the original patch
    because large numbers of CPUs are required.  When pressure is high enough,
    the cache line is bouncing between CPUs trying to acquire the lock and the
    holder of the lock adjusting free lists.  The intention was that the
    acquirer of the lock would automatically have the cache line holding the
    free lists but according to Huang, this is not a universal win.
    
    One possibility is to move the zone lock to its own cache line but it
    increases the size of the zone.  This patch moves the lock to the other
    end of the free lists where they do not contend under high pressure.  It
    does mean the page allocator paths now require more cache lines but Huang
    reports that it restores performance to previous levels on large machines
    
                 %stddev     %change         %stddev
                     \          |                \
             84568 \261  1%     +94.3%     164280 \261  1%  aim7.jobs-per-min
           2881944 \261  2%     -35.1%    1870386 \261  8%  aim7.time.voluntary_context_switches
               681 \261  1%      -3.4%        658 \261  0%  aim7.time.user_time
           5538139 \261  0%     -12.1%    4867884 \261  0%  aim7.time.involuntary_context_switches
             44174 \261  1%     -46.0%      23848 \261  1%  aim7.time.system_time
               426 \261  1%     -48.4%        219 \261  1%  aim7.time.elapsed_time
               426 \261  1%     -48.4%        219 \261  1%  aim7.time.elapsed_time.max
               468 \261  1%     -43.1%        266 \261  2%  uptime.boot
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-by: Huang Ying <ying.huang@intel.com>
    Tested-by: Huang Ying <ying.huang@intel.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f279d9c158cd..2782df47101e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -474,16 +474,15 @@ struct zone {
 	unsigned long		wait_table_bits;
 
 	ZONE_PADDING(_pad1_)
-
-	/* Write-intensive fields used from the page allocator */
-	spinlock_t		lock;
-
 	/* free areas of different sizes */
 	struct free_area	free_area[MAX_ORDER];
 
 	/* zone flags, see below */
 	unsigned long		flags;
 
+	/* Write-intensive fields used from the page allocator */
+	spinlock_t		lock;
+
 	ZONE_PADDING(_pad2_)
 
 	/* Write-intensive fields used by page reclaim */

commit 05891fb06517d19ae5357c9dc44e96bbe0300a3c
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Feb 11 15:25:47 2015 -0800

    mm: microoptimize zonelist operations
    
    next_zones_zonelist() returns a zoneref pointer, as well as a zone pointer
    via extra parameter.  Since the latter can be trivially obtained by
    dereferencing the former, the overhead of the extra parameter is
    unjustified.
    
    This patch thus removes the zone parameter from next_zones_zonelist().
    Both callers happen to be in the same header file, so it's simple to add
    the zoneref dereference inline.  We save some bytes of code size.
    
    add/remove: 0/0 grow/shrink: 0/3 up/down: 0/-105 (-105)
    function                                     old     new   delta
    nr_free_zone_pages                           129     115     -14
    __alloc_pages_nodemask                      2300    2285     -15
    get_page_from_freelist                      2652    2576     -76
    
    add/remove: 0/0 grow/shrink: 1/0 up/down: 10/0 (10)
    function                                     old     new   delta
    try_to_compact_pages                         569     579     +10
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b41829701334..f279d9c158cd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -970,7 +970,6 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
  * @z - The cursor used as a starting point for the search
  * @highest_zoneidx - The zone index of the highest zone to return
  * @nodes - An optional nodemask to filter the zonelist with
- * @zone - The first suitable zone found is returned via this parameter
  *
  * This function returns the next zone at or below a given zone index that is
  * within the allowed nodemask using a cursor as the starting point for the
@@ -980,8 +979,7 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
  */
 struct zoneref *next_zones_zonelist(struct zoneref *z,
 					enum zone_type highest_zoneidx,
-					nodemask_t *nodes,
-					struct zone **zone);
+					nodemask_t *nodes);
 
 /**
  * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
@@ -1000,8 +998,10 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 					nodemask_t *nodes,
 					struct zone **zone)
 {
-	return next_zones_zonelist(zonelist->_zonerefs, highest_zoneidx, nodes,
-								zone);
+	struct zoneref *z = next_zones_zonelist(zonelist->_zonerefs,
+							highest_zoneidx, nodes);
+	*zone = zonelist_zone(z);
+	return z;
 }
 
 /**
@@ -1018,7 +1018,8 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 #define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
 	for (z = first_zones_zonelist(zlist, highidx, nodemask, &zone);	\
 		zone;							\
-		z = next_zones_zonelist(++z, highidx, nodemask, &zone))	\
+		z = next_zones_zonelist(++z, highidx, nodemask),	\
+			zone = zonelist_zone(z))			\
 
 /**
  * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index

commit 44628d9755e249aab9a6e1a17407d2f4278047ee
Author: Baoquan He <bhe@redhat.com>
Date:   Wed Feb 11 15:25:10 2015 -0800

    mm: fix typo of MIGRATE_RESERVE in comment
    
    Found it when I want to jump to the definition of MIGRATE_RESERVE ctags.
    
    Signed-off-by: Baoquan He <bhe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2f0856d14b21..b41829701334 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -426,7 +426,7 @@ struct zone {
 	const char		*name;
 
 	/*
-	 * Number of MIGRATE_RESEVE page block. To maintain for just
+	 * Number of MIGRATE_RESERVE page block. To maintain for just
 	 * optimization. Protected by zone->lock.
 	 */
 	int			nr_migrate_reserve_block;

commit eefa864b701d78dc9753c70a3540a2e9ae192595
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Fri Dec 12 16:55:46 2014 -0800

    mm/page_ext: resurrect struct page extending code for debugging
    
    When we debug something, we'd like to insert some information to every
    page.  For this purpose, we sometimes modify struct page itself.  But,
    this has drawbacks.  First, it requires re-compile.  This makes us
    hesitate to use the powerful debug feature so development process is
    slowed down.  And, second, sometimes it is impossible to rebuild the
    kernel due to third party module dependency.  At third, system behaviour
    would be largely different after re-compile, because it changes size of
    struct page greatly and this structure is accessed by every part of
    kernel.  Keeping this as it is would be better to reproduce errornous
    situation.
    
    This feature is intended to overcome above mentioned problems.  This
    feature allocates memory for extended data per page in certain place
    rather than the struct page itself.  This memory can be accessed by the
    accessor functions provided by this code.  During the boot process, it
    checks whether allocation of huge chunk of memory is needed or not.  If
    not, it avoids allocating memory at all.  With this advantage, we can
    include this feature into the kernel in default and can avoid rebuild and
    solve related problems.
    
    Until now, memcg uses this technique.  But, now, memcg decides to embed
    their variable to struct page itself and it's code to extend struct page
    has been removed.  I'd like to use this code to develop debug feature, so
    this patch resurrect it.
    
    To help these things to work well, this patch introduces two callbacks for
    clients.  One is the need callback which is mandatory if user wants to
    avoid useless memory allocation at boot-time.  The other is optional, init
    callback, which is used to do proper initialization after memory is
    allocated.  Detailed explanation about purpose of these functions is in
    code comment.  Please refer it.
    
    Others are completely same with previous extension code in memcg.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Jungsoo Son <jungsoo.son@lge.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3879d7664dfc..2f0856d14b21 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -722,6 +722,9 @@ typedef struct pglist_data {
 	int nr_zones;
 #ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
 	struct page *node_mem_map;
+#ifdef CONFIG_PAGE_EXTENSION
+	struct page_ext *node_page_ext;
+#endif
 #endif
 #ifndef CONFIG_NO_BOOTMEM
 	struct bootmem_data *bdata;
@@ -1075,6 +1078,7 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)
 
 struct page;
+struct page_ext;
 struct mem_section {
 	/*
 	 * This is, logically, a pointer to an array of struct
@@ -1092,6 +1096,14 @@ struct mem_section {
 
 	/* See declaration of similar field in struct zone */
 	unsigned long *pageblock_flags;
+#ifdef CONFIG_PAGE_EXTENSION
+	/*
+	 * If !SPARSEMEM, pgdat doesn't have page_ext pointer. We use
+	 * section. (see page_ext.h about this.)
+	 */
+	struct page_ext *page_ext;
+	unsigned long pad;
+#endif
 	/*
 	 * WARNING: mem_section must be a power-of-2 in size for the
 	 * calculation and use of SECTION_ROOT_MASK to make sense.

commit 1306a85aed3ec3db98945aafb7dfbe5648a1203c
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Dec 10 15:44:52 2014 -0800

    mm: embed the memcg pointer directly into struct page
    
    Memory cgroups used to have 5 per-page pointers.  To allow users to
    disable that amount of overhead during runtime, those pointers were
    allocated in a separate array, with a translation layer between them and
    struct page.
    
    There is now only one page pointer remaining: the memcg pointer, that
    indicates which cgroup the page is associated with when charged.  The
    complexity of runtime allocation and the runtime translation overhead is
    no longer justified to save that *potential* 0.19% of memory.  With
    CONFIG_SLUB, page->mem_cgroup actually sits in the doubleword padding
    after the page->private member and doesn't even increase struct page,
    and then this patch actually saves space.  Remaining users that care can
    still compile their kernels without CONFIG_MEMCG.
    
         text    data     bss     dec     hex     filename
      8828345 1725264  983040 11536649 b00909  vmlinux.old
      8827425 1725264  966656 11519345 afc571  vmlinux.new
    
    [mhocko@suse.cz: update Documentation/cgroups/memory.txt]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ffe66e381c04..3879d7664dfc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -722,9 +722,6 @@ typedef struct pglist_data {
 	int nr_zones;
 #ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
 	struct page *node_mem_map;
-#ifdef CONFIG_MEMCG
-	struct page_cgroup *node_page_cgroup;
-#endif
 #endif
 #ifndef CONFIG_NO_BOOTMEM
 	struct bootmem_data *bdata;
@@ -1078,7 +1075,6 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)
 
 struct page;
-struct page_cgroup;
 struct mem_section {
 	/*
 	 * This is, logically, a pointer to an array of struct
@@ -1096,14 +1092,6 @@ struct mem_section {
 
 	/* See declaration of similar field in struct zone */
 	unsigned long *pageblock_flags;
-#ifdef CONFIG_MEMCG
-	/*
-	 * If !SPARSEMEM, pgdat doesn't have page_cgroup pointer. We use
-	 * section. (see memcontrol.h/page_cgroup.h about this.)
-	 */
-	struct page_cgroup *page_cgroup;
-	unsigned long pad;
-#endif
 	/*
 	 * WARNING: mem_section must be a power-of-2 in size for the
 	 * calculation and use of SECTION_ROOT_MASK to make sense.

commit ad53f92eb416d81e469fa8ea57153e59455e7175
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Nov 13 15:19:11 2014 -0800

    mm/page_alloc: fix incorrect isolation behavior by rechecking migratetype
    
    Before describing bugs itself, I first explain definition of freepage.
    
     1. pages on buddy list are counted as freepage.
     2. pages on isolate migratetype buddy list are *not* counted as freepage.
     3. pages on cma buddy list are counted as CMA freepage, too.
    
    Now, I describe problems and related patch.
    
    Patch 1: There is race conditions on getting pageblock migratetype that
    it results in misplacement of freepages on buddy list, incorrect
    freepage count and un-availability of freepage.
    
    Patch 2: Freepages on pcp list could have stale cached information to
    determine migratetype of buddy list to go.  This causes misplacement of
    freepages on buddy list and incorrect freepage count.
    
    Patch 4: Merging between freepages on different migratetype of
    pageblocks will cause freepages accouting problem.  This patch fixes it.
    
    Without patchset [3], above problem doesn't happens on my CMA allocation
    test, because CMA reserved pages aren't used at all.  So there is no
    chance for above race.
    
    With patchset [3], I did simple CMA allocation test and get below
    result:
    
     - Virtual machine, 4 cpus, 1024 MB memory, 256 MB CMA reservation
     - run kernel build (make -j16) on background
     - 30 times CMA allocation(8MB * 30 = 240MB) attempts in 5 sec interval
     - Result: more than 5000 freepage count are missed
    
    With patchset [3] and this patchset, I found that no freepage count are
    missed so that I conclude that problems are solved.
    
    On my simple memory offlining test, these problems also occur on that
    environment, too.
    
    This patch (of 4):
    
    There are two paths to reach core free function of buddy allocator,
    __free_one_page(), one is free_one_page()->__free_one_page() and the
    other is free_hot_cold_page()->free_pcppages_bulk()->__free_one_page().
    Each paths has race condition causing serious problems.  At first, this
    patch is focused on first type of freepath.  And then, following patch
    will solve the problem in second type of freepath.
    
    In the first type of freepath, we got migratetype of freeing page
    without holding the zone lock, so it could be racy.  There are two cases
    of this race.
    
     1. pages are added to isolate buddy list after restoring orignal
        migratetype
    
        CPU1                                   CPU2
    
        get migratetype => return MIGRATE_ISOLATE
        call free_one_page() with MIGRATE_ISOLATE
    
                                    grab the zone lock
                                    unisolate pageblock
                                    release the zone lock
    
        grab the zone lock
        call __free_one_page() with MIGRATE_ISOLATE
        freepage go into isolate buddy list,
        although pageblock is already unisolated
    
    This may cause two problems.  One is that we can't use this page anymore
    until next isolation attempt of this pageblock, because freepage is on
    isolate buddy list.  The other is that freepage accouting could be wrong
    due to merging between different buddy list.  Freepages on isolate buddy
    list aren't counted as freepage, but ones on normal buddy list are
    counted as freepage.  If merge happens, buddy freepage on normal buddy
    list is inevitably moved to isolate buddy list without any consideration
    of freepage accouting so it could be incorrect.
    
     2. pages are added to normal buddy list while pageblock is isolated.
        It is similar with above case.
    
    This also may cause two problems.  One is that we can't keep these
    freepages from being allocated.  Although this pageblock is isolated,
    freepage would be added to normal buddy list so that it could be
    allocated without any restriction.  And the other problem is same as
    case 1, that it, incorrect freepage accouting.
    
    This race condition would be prevented by checking migratetype again
    with holding the zone lock.  Because it is somewhat heavy operation and
    it isn't needed in common case, we want to avoid rechecking as much as
    possible.  So this patch introduce new variable, nr_isolate_pageblock in
    struct zone to check if there is isolated pageblock.  With this, we can
    avoid to re-check migratetype in common case and do it only if there is
    isolated pageblock or migratetype is MIGRATE_ISOLATE.  This solve above
    mentioned problems.
    
    Changes from v3:
    Add one more check in free_one_page() that checks whether migratetype is
    MIGRATE_ISOLATE or not. Without this, abovementioned case 1 could happens.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Heesub Shin <heesub.shin@samsung.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Ritesh Harjani <ritesh.list@gmail.com>
    Cc: Gioh Kim <gioh.kim@lge.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 48bf12ef6620..ffe66e381c04 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -431,6 +431,15 @@ struct zone {
 	 */
 	int			nr_migrate_reserve_block;
 
+#ifdef CONFIG_MEMORY_ISOLATION
+	/*
+	 * Number of isolated pageblock. It is used to solve incorrect
+	 * freepage counting problem due to racy retrieving migratetype
+	 * of pageblock. Protected by zone->lock.
+	 */
+	unsigned long		nr_isolate_pageblock;
+#endif
+
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;

commit 5705465174686d007473e017b76c4b64b44aa690
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Oct 9 15:28:17 2014 -0700

    mm: clean up zone flags
    
    Page reclaim tests zone_is_reclaim_dirty(), but the site that actually
    sets this state does zone_set_flag(zone, ZONE_TAIL_LRU_DIRTY), sending the
    reader through layers indirection just to track down a simple bit.
    
    Remove all zone flag wrappers and just use bitops against zone->flags
    directly.  It's just as readable and the lines are barely any longer.
    
    Also rename ZONE_TAIL_LRU_DIRTY to ZONE_DIRTY to match ZONE_WRITEBACK, and
    remove the zone_flags_t typedef.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 318df7051850..48bf12ef6620 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -521,13 +521,13 @@ struct zone {
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
 } ____cacheline_internodealigned_in_smp;
 
-typedef enum {
+enum zone_flags {
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
 	ZONE_CONGESTED,			/* zone has many dirty pages backed by
 					 * a congested BDI
 					 */
-	ZONE_TAIL_LRU_DIRTY,		/* reclaim scanning has recently found
+	ZONE_DIRTY,			/* reclaim scanning has recently found
 					 * many dirty file pages at the tail
 					 * of the LRU.
 					 */
@@ -535,52 +535,7 @@ typedef enum {
 					 * many pages under writeback
 					 */
 	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
-} zone_flags_t;
-
-static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
-{
-	set_bit(flag, &zone->flags);
-}
-
-static inline int zone_test_and_set_flag(struct zone *zone, zone_flags_t flag)
-{
-	return test_and_set_bit(flag, &zone->flags);
-}
-
-static inline void zone_clear_flag(struct zone *zone, zone_flags_t flag)
-{
-	clear_bit(flag, &zone->flags);
-}
-
-static inline int zone_is_reclaim_congested(const struct zone *zone)
-{
-	return test_bit(ZONE_CONGESTED, &zone->flags);
-}
-
-static inline int zone_is_reclaim_dirty(const struct zone *zone)
-{
-	return test_bit(ZONE_TAIL_LRU_DIRTY, &zone->flags);
-}
-
-static inline int zone_is_reclaim_writeback(const struct zone *zone)
-{
-	return test_bit(ZONE_WRITEBACK, &zone->flags);
-}
-
-static inline int zone_is_reclaim_locked(const struct zone *zone)
-{
-	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);
-}
-
-static inline int zone_is_fair_depleted(const struct zone *zone)
-{
-	return test_bit(ZONE_FAIR_DEPLETED, &zone->flags);
-}
-
-static inline int zone_is_oom_locked(const struct zone *zone)
-{
-	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
-}
+};
 
 static inline unsigned long zone_end_pfn(const struct zone *zone)
 {

commit 4ffeaf3560a52b4a69cc7909873d08c0ef5909d4
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 16:07:22 2014 -0700

    mm: page_alloc: reduce cost of the fair zone allocation policy
    
    The fair zone allocation policy round-robins allocations between zones
    within a node to avoid age inversion problems during reclaim.  If the
    first allocation fails, the batch counts are reset and a second attempt
    made before entering the slow path.
    
    One assumption made with this scheme is that batches expire at roughly
    the same time and the resets each time are justified.  This assumption
    does not hold when zones reach their low watermark as the batches will
    be consumed at uneven rates.  Allocation failure due to watermark
    depletion result in additional zonelist scans for the reset and another
    watermark check before hitting the slowpath.
    
    On UMA, the benefit is negligible -- around 0.25%.  On 4-socket NUMA
    machine it's variable due to the variability of measuring overhead with
    the vmstat changes.  The system CPU overhead comparison looks like
    
              3.16.0-rc3  3.16.0-rc3  3.16.0-rc3
                 vanilla   vmstat-v5 lowercost-v5
    User          746.94      774.56      802.00
    System      65336.22    32847.27    40852.33
    Elapsed     27553.52    27415.04    27368.46
    
    However it is worth noting that the overall benchmark still completed
    faster and intuitively it makes sense to take as few passes as possible
    through the zonelists.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0bd77f730b38..318df7051850 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -534,6 +534,7 @@ typedef enum {
 	ZONE_WRITEBACK,			/* reclaim scanning has recently found
 					 * many pages under writeback
 					 */
+	ZONE_FAIR_DEPLETED,		/* fair zone policy batch depleted */
 } zone_flags_t;
 
 static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
@@ -571,6 +572,11 @@ static inline int zone_is_reclaim_locked(const struct zone *zone)
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);
 }
 
+static inline int zone_is_fair_depleted(const struct zone *zone)
+{
+	return test_bit(ZONE_FAIR_DEPLETED, &zone->flags);
+}
+
 static inline int zone_is_oom_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);

commit 0d5d823ab4e608ec7b52ac4410de4cb74bbe0edd
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 16:07:16 2014 -0700

    mm: move zone->pages_scanned into a vmstat counter
    
    zone->pages_scanned is a write-intensive cache line during page reclaim
    and it's also updated during page free.  Move the counter into vmstat to
    take advantage of the per-cpu updates and do not update it in the free
    paths unless necessary.
    
    On a small UMA machine running tiobench the difference is marginal.  On
    a 4-node machine the overhead is more noticable.  Note that automatic
    NUMA balancing was disabled for this test as otherwise the system CPU
    overhead is unpredictable.
    
              3.16.0-rc3  3.16.0-rc3  3.16.0-rc3
                 vanillarearrange-v5   vmstat-v5
    User          746.94      759.78      774.56
    System      65336.22    58350.98    32847.27
    Elapsed     27553.52    27282.02    27415.04
    
    Note that the overhead reduction will vary depending on where exactly
    pages are allocated and freed.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ed0876bb902c..0bd77f730b38 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -143,6 +143,7 @@ enum zone_stat_item {
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
+	NR_PAGES_SCANNED,	/* pages scanned since last reclaim */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */
@@ -480,7 +481,6 @@ struct zone {
 
 	/* Fields commonly accessed by the page reclaim scanner */
 	spinlock_t		lru_lock;
-	unsigned long		pages_scanned;	   /* since last reclaim */
 	struct lruvec		lruvec;
 
 	/* Evictions & activations on the inactive file list */

commit 3484b2de9499df23c4604a513b36f96326ae81ad
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 16:07:14 2014 -0700

    mm: rearrange zone fields into read-only, page alloc, statistics and page reclaim lines
    
    The arrangement of struct zone has changed over time and now it has
    reached the point where there is some inappropriate sharing going on.
    On x86-64 for example
    
    o The zone->node field is shared with the zone lock and zone->node is
      accessed frequently from the page allocator due to the fair zone
      allocation policy.
    
    o span_seqlock is almost never used by shares a line with free_area
    
    o Some zone statistics share a cache line with the LRU lock so
      reclaim-intensive and allocator-intensive workloads can bounce the cache
      line on a stat update
    
    This patch rearranges struct zone to put read-only and read-mostly
    fields together and then splits the page allocator intensive fields, the
    zone statistics and the page reclaim intensive fields into their own
    cache lines.  Note that the type of lowmem_reserve changes due to the
    watermark calculations being signed and avoiding a signed/unsigned
    conversion there.
    
    On the test configuration I used the overall size of struct zone shrunk
    by one cache line.  On smaller machines, this is not likely to be
    noticable.  However, on a 4-node NUMA machine running tiobench the
    system CPU overhead is reduced by this patch.
    
              3.16.0-rc3  3.16.0-rc3
                 vanillarearrange-v5r9
    User          746.94      759.78
    System      65336.22    58350.98
    Elapsed     27553.52    27282.02
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 559e659288fc..ed0876bb902c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -324,18 +324,11 @@ enum zone_type {
 #ifndef __GENERATING_BOUNDS_H
 
 struct zone {
-	/* Fields commonly accessed by the page allocator */
+	/* Read-mostly fields */
 
 	/* zone watermarks, access with *_wmark_pages(zone) macros */
 	unsigned long watermark[NR_WMARK];
 
-	/*
-	 * When free pages are below this point, additional steps are taken
-	 * when reading the number of free pages to avoid per-cpu counter
-	 * drift allowing watermarks to be breached
-	 */
-	unsigned long percpu_drift_mark;
-
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several
@@ -344,41 +337,26 @@ struct zone {
 	 * on the higher zones). This array is recalculated at runtime if the
 	 * sysctl_lowmem_reserve_ratio sysctl changes.
 	 */
-	unsigned long		lowmem_reserve[MAX_NR_ZONES];
-
-	/*
-	 * This is a per-zone reserve of pages that should not be
-	 * considered dirtyable memory.
-	 */
-	unsigned long		dirty_balance_reserve;
+	long lowmem_reserve[MAX_NR_ZONES];
 
 #ifdef CONFIG_NUMA
 	int node;
+#endif
+
 	/*
-	 * zone reclaim becomes active if more unmapped pages exist.
+	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
+	 * this zone's LRU.  Maintained by the pageout code.
 	 */
-	unsigned long		min_unmapped_pages;
-	unsigned long		min_slab_pages;
-#endif
+	unsigned int inactive_ratio;
+
+	struct pglist_data	*zone_pgdat;
 	struct per_cpu_pageset __percpu *pageset;
+
 	/*
-	 * free areas of different sizes
+	 * This is a per-zone reserve of pages that should not be
+	 * considered dirtyable memory.
 	 */
-	spinlock_t		lock;
-#if defined CONFIG_COMPACTION || defined CONFIG_CMA
-	/* Set to true when the PG_migrate_skip bits should be cleared */
-	bool			compact_blockskip_flush;
-
-	/* pfn where compaction free scanner should start */
-	unsigned long		compact_cached_free_pfn;
-	/* pfn where async and sync compaction migration scanner should start */
-	unsigned long		compact_cached_migrate_pfn[2];
-#endif
-#ifdef CONFIG_MEMORY_HOTPLUG
-	/* see spanned/present_pages for more description */
-	seqlock_t		span_seqlock;
-#endif
-	struct free_area	free_area[MAX_ORDER];
+	unsigned long		dirty_balance_reserve;
 
 #ifndef CONFIG_SPARSEMEM
 	/*
@@ -388,74 +366,14 @@ struct zone {
 	unsigned long		*pageblock_flags;
 #endif /* CONFIG_SPARSEMEM */
 
-#ifdef CONFIG_COMPACTION
-	/*
-	 * On compaction failure, 1<<compact_defer_shift compactions
-	 * are skipped before trying again. The number attempted since
-	 * last failure is tracked with compact_considered.
-	 */
-	unsigned int		compact_considered;
-	unsigned int		compact_defer_shift;
-	int			compact_order_failed;
-#endif
-
-	ZONE_PADDING(_pad1_)
-
-	/* Fields commonly accessed by the page reclaim scanner */
-	spinlock_t		lru_lock;
-	struct lruvec		lruvec;
-
-	/* Evictions & activations on the inactive file list */
-	atomic_long_t		inactive_age;
-
-	unsigned long		pages_scanned;	   /* since last reclaim */
-	unsigned long		flags;		   /* zone flags, see below */
-
-	/* Zone statistics */
-	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
-
-	/*
-	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
-	 * this zone's LRU.  Maintained by the pageout code.
-	 */
-	unsigned int inactive_ratio;
-
-
-	ZONE_PADDING(_pad2_)
-	/* Rarely used or read-mostly fields */
-
+#ifdef CONFIG_NUMA
 	/*
-	 * wait_table		-- the array holding the hash table
-	 * wait_table_hash_nr_entries	-- the size of the hash table array
-	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
-	 *
-	 * The purpose of all these is to keep track of the people
-	 * waiting for a page to become available and make them
-	 * runnable again when possible. The trouble is that this
-	 * consumes a lot of space, especially when so few things
-	 * wait on pages at a given time. So instead of using
-	 * per-page waitqueues, we use a waitqueue hash table.
-	 *
-	 * The bucket discipline is to sleep on the same queue when
-	 * colliding and wake all in that wait queue when removing.
-	 * When something wakes, it must check to be sure its page is
-	 * truly available, a la thundering herd. The cost of a
-	 * collision is great, but given the expected load of the
-	 * table, they should be so rare as to be outweighed by the
-	 * benefits from the saved space.
-	 *
-	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
-	 * primary users of these fields, and in mm/page_alloc.c
-	 * free_area_init_core() performs the initialization of them.
+	 * zone reclaim becomes active if more unmapped pages exist.
 	 */
-	wait_queue_head_t	* wait_table;
-	unsigned long		wait_table_hash_nr_entries;
-	unsigned long		wait_table_bits;
+	unsigned long		min_unmapped_pages;
+	unsigned long		min_slab_pages;
+#endif /* CONFIG_NUMA */
 
-	/*
-	 * Discontig memory support fields.
-	 */
-	struct pglist_data	*zone_pgdat;
 	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 	unsigned long		zone_start_pfn;
 
@@ -500,9 +418,11 @@ struct zone {
 	 * adjust_managed_page_count() should be used instead of directly
 	 * touching zone->managed_pages and totalram_pages.
 	 */
+	unsigned long		managed_pages;
 	unsigned long		spanned_pages;
 	unsigned long		present_pages;
-	unsigned long		managed_pages;
+
+	const char		*name;
 
 	/*
 	 * Number of MIGRATE_RESEVE page block. To maintain for just
@@ -510,10 +430,95 @@ struct zone {
 	 */
 	int			nr_migrate_reserve_block;
 
+#ifdef CONFIG_MEMORY_HOTPLUG
+	/* see spanned/present_pages for more description */
+	seqlock_t		span_seqlock;
+#endif
+
 	/*
-	 * rarely used fields:
+	 * wait_table		-- the array holding the hash table
+	 * wait_table_hash_nr_entries	-- the size of the hash table array
+	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
+	 *
+	 * The purpose of all these is to keep track of the people
+	 * waiting for a page to become available and make them
+	 * runnable again when possible. The trouble is that this
+	 * consumes a lot of space, especially when so few things
+	 * wait on pages at a given time. So instead of using
+	 * per-page waitqueues, we use a waitqueue hash table.
+	 *
+	 * The bucket discipline is to sleep on the same queue when
+	 * colliding and wake all in that wait queue when removing.
+	 * When something wakes, it must check to be sure its page is
+	 * truly available, a la thundering herd. The cost of a
+	 * collision is great, but given the expected load of the
+	 * table, they should be so rare as to be outweighed by the
+	 * benefits from the saved space.
+	 *
+	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
+	 * primary users of these fields, and in mm/page_alloc.c
+	 * free_area_init_core() performs the initialization of them.
 	 */
-	const char		*name;
+	wait_queue_head_t	*wait_table;
+	unsigned long		wait_table_hash_nr_entries;
+	unsigned long		wait_table_bits;
+
+	ZONE_PADDING(_pad1_)
+
+	/* Write-intensive fields used from the page allocator */
+	spinlock_t		lock;
+
+	/* free areas of different sizes */
+	struct free_area	free_area[MAX_ORDER];
+
+	/* zone flags, see below */
+	unsigned long		flags;
+
+	ZONE_PADDING(_pad2_)
+
+	/* Write-intensive fields used by page reclaim */
+
+	/* Fields commonly accessed by the page reclaim scanner */
+	spinlock_t		lru_lock;
+	unsigned long		pages_scanned;	   /* since last reclaim */
+	struct lruvec		lruvec;
+
+	/* Evictions & activations on the inactive file list */
+	atomic_long_t		inactive_age;
+
+	/*
+	 * When free pages are below this point, additional steps are taken
+	 * when reading the number of free pages to avoid per-cpu counter
+	 * drift allowing watermarks to be breached
+	 */
+	unsigned long percpu_drift_mark;
+
+#if defined CONFIG_COMPACTION || defined CONFIG_CMA
+	/* pfn where compaction free scanner should start */
+	unsigned long		compact_cached_free_pfn;
+	/* pfn where async and sync compaction migration scanner should start */
+	unsigned long		compact_cached_migrate_pfn[2];
+#endif
+
+#ifdef CONFIG_COMPACTION
+	/*
+	 * On compaction failure, 1<<compact_defer_shift compactions
+	 * are skipped before trying again. The number attempted since
+	 * last failure is tracked with compact_considered.
+	 */
+	unsigned int		compact_considered;
+	unsigned int		compact_defer_shift;
+	int			compact_order_failed;
+#endif
+
+#if defined CONFIG_COMPACTION || defined CONFIG_CMA
+	/* Set to true when the PG_migrate_skip bits should be cleared */
+	bool			compact_blockskip_flush;
+#endif
+
+	ZONE_PADDING(_pad3_)
+	/* Zone statistics */
+	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
 } ____cacheline_internodealigned_in_smp;
 
 typedef enum {

commit 1a4dc5bc7cb5659a8004d105afeb0571126f8f56
Author: Wang Nan <wangnan0@huawei.com>
Date:   Wed Aug 6 16:06:08 2014 -0700

    mem-hotplug: improve zone_movable_is_highmem logic
    
    In original code, zone_movable_is_highmem() assumes ZONE_MOVABLE not
    highmem if CONFIG_HAVE_MEMBLOCK_NODE_MAP is not set.  In online_pages,
    it extracts pages from the previous zone before ZONE_MOVABLE.  Which is
    logically inconsistent:
    
    If HAVE_MEMBLOCK_NODE_MAP is turned off but HIGHMEM is on,
    zone_movable_is_highmem() makes movable zone not highmem, but
    online_pages() extracts pages from ZONE_HIGHMEM.
    
    This inconsistency doesn't cause real problem currently, because all
    architectures support online_pages also have HAVE_MEMBLOCK_NODE_MAP.
    However, fixing it makes code clear, and also helps futher coding.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Cc: Zhang Zhen <zhangzhen@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6cbd1b6c3d20..559e659288fc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -872,6 +872,8 @@ static inline int zone_movable_is_highmem(void)
 {
 #if defined(CONFIG_HIGHMEM) && defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
 	return movable_zone == ZONE_HIGHMEM;
+#elif defined(CONFIG_HIGHMEM)
+	return (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;
 #else
 	return 0;
 #endif

commit 7aeb09f9104b760fc53c98cb7d20d06640baf9e6
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:21 2014 -0700

    mm: page_alloc: use unsigned int for order in more places
    
    X86 prefers the use of unsigned types for iterators and there is a
    tendency to mix whether a signed or unsigned type if used for page order.
    This converts a number of sites in mm/page_alloc.c to use unsigned int for
    order where possible.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 472426ac96ae..6cbd1b6c3d20 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -817,10 +817,10 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 extern struct mutex zonelists_mutex;
 void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
-bool zone_watermark_ok(struct zone *z, int order, unsigned long mark,
-		int classzone_idx, int alloc_flags);
-bool zone_watermark_ok_safe(struct zone *z, int order, unsigned long mark,
-		int classzone_idx, int alloc_flags);
+bool zone_watermark_ok(struct zone *z, unsigned int order,
+		unsigned long mark, int classzone_idx, int alloc_flags);
+bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
+		unsigned long mark, int classzone_idx, int alloc_flags);
 enum memmap_context {
 	MEMMAP_EARLY,
 	MEMMAP_HOTPLUG,

commit dc4b0caff24d9b2918e9f27bc65499ee63187eba
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:17 2014 -0700

    mm: page_alloc: reduce number of times page_to_pfn is called
    
    In the free path we calculate page_to_pfn multiple times. Reduce that.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8ef1e3f71e0f..472426ac96ae 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -78,10 +78,15 @@ extern int page_group_by_mobility_disabled;
 #define NR_MIGRATETYPE_BITS (PB_migrate_end - PB_migrate + 1)
 #define MIGRATETYPE_MASK ((1UL << NR_MIGRATETYPE_BITS) - 1)
 
-static inline int get_pageblock_migratetype(struct page *page)
+#define get_pageblock_migratetype(page)					\
+	get_pfnblock_flags_mask(page, page_to_pfn(page),		\
+			PB_migrate_end, MIGRATETYPE_MASK)
+
+static inline int get_pfnblock_migratetype(struct page *page, unsigned long pfn)
 {
 	BUILD_BUG_ON(PB_migrate_end - PB_migrate != 2);
-	return get_pageblock_flags_mask(page, PB_migrate_end, MIGRATETYPE_MASK);
+	return get_pfnblock_flags_mask(page, pfn, PB_migrate_end,
+					MIGRATETYPE_MASK);
 }
 
 struct free_area {

commit e58469bafd0524e848c3733bc3918d854595e20f
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:16 2014 -0700

    mm: page_alloc: use word-based accesses for get/set pageblock bitmaps
    
    The test_bit operations in get/set pageblock flags are expensive.  This
    patch reads the bitmap on a word basis and use shifts and masks to isolate
    the bits of interest.  Similarly masks are used to set a local copy of the
    bitmap and then use cmpxchg to update the bitmap if there have been no
    other changes made in parallel.
    
    In a test running dd onto tmpfs the overhead of the pageblock-related
    functions went from 1.27% in profiles to 0.5%.
    
    In addition to the performance benefits, this patch closes races that are
    possible between:
    
    a) get_ and set_pageblock_migratetype(), where get_pageblock_migratetype()
       reads part of the bits before and other part of the bits after
       set_pageblock_migratetype() has updated them.
    
    b) set_pageblock_migratetype() and set_pageblock_skip(), where the non-atomic
       read-modify-update set bit operation in set_pageblock_skip() will cause
       lost updates to some bits changed in the set_pageblock_migratetype().
    
    Joonsoo Kim first reported the case a) via code inspection.  Vlastimil
    Babka's testing with a debug patch showed that either a) or b) occurs
    roughly once per mmtests' stress-highalloc benchmark (although not
    necessarily in the same pageblock).  Furthermore during development of
    unrelated compaction patches, it was observed that frequent calls to
    {start,undo}_isolate_page_range() the race occurs several thousands of
    times and has resulted in NULL pointer dereferences in move_freepages()
    and free_one_page() in places where free_list[migratetype] is
    manipulated by e.g.  list_move().  Further debugging confirmed that
    migratetype had invalid value of 6, causing out of bounds access to the
    free_list array.
    
    That confirmed that the race exist, although it may be extremely rare,
    and currently only fatal where page isolation is performed due to
    memory hot remove.  Races on pageblocks being updated by
    set_pageblock_migratetype(), where both old and new migratetype are
    lower MIGRATE_RESERVE, currently cannot result in an invalid value
    being observed, although theoretically they may still lead to
    unexpected creation or destruction of MIGRATE_RESERVE pageblocks.
    Furthermore, things could get suddenly worse when memory isolation is
    used more, or when new migratetypes are added.
    
    After this patch, the race has no longer been observed in testing.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reported-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reported-and-tested-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 10a96ee68311..8ef1e3f71e0f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -75,9 +75,13 @@ enum {
 
 extern int page_group_by_mobility_disabled;
 
+#define NR_MIGRATETYPE_BITS (PB_migrate_end - PB_migrate + 1)
+#define MIGRATETYPE_MASK ((1UL << NR_MIGRATETYPE_BITS) - 1)
+
 static inline int get_pageblock_migratetype(struct page *page)
 {
-	return get_pageblock_flags_group(page, PB_migrate, PB_migrate_end);
+	BUILD_BUG_ON(PB_migrate_end - PB_migrate != 2);
+	return get_pageblock_flags_mask(page, PB_migrate_end, MIGRATETYPE_MASK);
 }
 
 struct free_area {

commit 35979ef3393110ff3c12c6b94552208d3bdf1a36
Author: David Rientjes <rientjes@google.com>
Date:   Wed Jun 4 16:08:27 2014 -0700

    mm, compaction: add per-zone migration pfn cache for async compaction
    
    Each zone has a cached migration scanner pfn for memory compaction so that
    subsequent calls to memory compaction can start where the previous call
    left off.
    
    Currently, the compaction migration scanner only updates the per-zone
    cached pfn when pageblocks were not skipped for async compaction.  This
    creates a dependency on calling sync compaction to avoid having subsequent
    calls to async compaction from scanning an enormous amount of non-MOVABLE
    pageblocks each time it is called.  On large machines, this could be
    potentially very expensive.
    
    This patch adds a per-zone cached migration scanner pfn only for async
    compaction.  It is updated everytime a pageblock has been scanned in its
    entirety and when no pages from it were successfully isolated.  The cached
    migration scanner pfn for sync compaction is updated only when called for
    sync compaction.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ae693e1ad0f9..10a96ee68311 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -360,9 +360,10 @@ struct zone {
 	/* Set to true when the PG_migrate_skip bits should be cleared */
 	bool			compact_blockskip_flush;
 
-	/* pfns where compaction scanners should start */
+	/* pfn where compaction free scanner should start */
 	unsigned long		compact_cached_free_pfn;
-	unsigned long		compact_cached_migrate_pfn;
+	/* pfn where async and sync compaction migration scanner should start */
+	unsigned long		compact_cached_migrate_pfn[2];
 #endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */

commit bfc8c90139ebd049b9801a951db3b9a4a00bed9c
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:07:18 2014 -0700

    mem-hotplug: implement get/put_online_mems
    
    kmem_cache_{create,destroy,shrink} need to get a stable value of
    cpu/node online mask, because they init/destroy/access per-cpu/node
    kmem_cache parts, which can be allocated or destroyed on cpu/mem
    hotplug.  To protect against cpu hotplug, these functions use
    {get,put}_online_cpus.  However, they do nothing to synchronize with
    memory hotplug - taking the slab_mutex does not eliminate the
    possibility of race as described in patch 2.
    
    What we need there is something like get_online_cpus, but for memory.
    We already have lock_memory_hotplug, which serves for the purpose, but
    it's a bit of a hammer right now, because it's backed by a mutex.  As a
    result, it imposes some limitations to locking order, which are not
    desirable, and can't be used just like get_online_cpus.  That's why in
    patch 1 I substitute it with get/put_online_mems, which work exactly
    like get/put_online_cpus except they block not cpu, but memory hotplug.
    
    [ v1 can be found at https://lkml.org/lkml/2014/4/6/68.  I NAK'ed it by
      myself, because it used an rw semaphore for get/put_online_mems,
      making them dead lock prune.  ]
    
    This patch (of 2):
    
    {un}lock_memory_hotplug, which is used to synchronize against memory
    hotplug, is currently backed by a mutex, which makes it a bit of a
    hammer - threads that only want to get a stable value of online nodes
    mask won't be able to proceed concurrently.  Also, it imposes some
    strong locking ordering rules on it, which narrows down the set of its
    usage scenarios.
    
    This patch introduces get/put_online_mems, which are the same as
    get/put_online_cpus, but for memory hotplug, i.e.  executing a code
    inside a get/put_online_mems section will guarantee a stable value of
    online nodes, present pages, etc.
    
    lock_memory_hotplug()/unlock_memory_hotplug() are removed altogether.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c1dbe0ba9f82..ae693e1ad0f9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -481,9 +481,8 @@ struct zone {
 	 * give them a chance of being in the same cacheline.
 	 *
 	 * Write access to present_pages at runtime should be protected by
-	 * lock_memory_hotplug()/unlock_memory_hotplug().  Any reader who can't
-	 * tolerant drift of present_pages should hold memory hotplug lock to
-	 * get a stable value.
+	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
+	 * present_pages should get_online_mems() to get a stable value.
 	 *
 	 * Read access to managed_pages should be safe because it's unsigned
 	 * long. Write access to zone->managed_pages and totalram_pages are
@@ -765,7 +764,8 @@ typedef struct pglist_data {
 	int node_id;
 	wait_queue_head_t kswapd_wait;
 	wait_queue_head_t pfmemalloc_wait;
-	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */
+	struct task_struct *kswapd;	/* Protected by
+					   mem_hotplug_begin/end() */
 	int kswapd_max_order;
 	enum zone_type classzone_idx;
 #ifdef CONFIG_NUMA_BALANCING

commit 5f7a75acdb24c7b9c436b3a0a66eec12e101d19c
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:07:15 2014 -0700

    mm: page_alloc: do not cache reclaim distances
    
    pgdat->reclaim_nodes tracks if a remote node is allowed to be reclaimed
    by zone_reclaim due to its distance.  As it is expected that
    zone_reclaim_mode will be rarely enabled it is unreasonable for all
    machines to take a penalty.  Fortunately, the zone_reclaim_mode() path
    is already slow and it is the path that takes the hit.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fac5509c18f0..c1dbe0ba9f82 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -763,7 +763,6 @@ typedef struct pglist_data {
 	unsigned long node_spanned_pages; /* total size of physical page
 					     range, including holes */
 	int node_id;
-	nodemask_t reclaim_nodes;	/* Nodes allowed to reclaim from */
 	wait_queue_head_t kswapd_wait;
 	wait_queue_head_t pfmemalloc_wait;
 	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */

commit 449dd6984d0e47643c04c807f609dd56d48d5bcc
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Apr 3 14:47:56 2014 -0700

    mm: keep page cache radix tree nodes in check
    
    Previously, page cache radix tree nodes were freed after reclaim emptied
    out their page pointers.  But now reclaim stores shadow entries in their
    place, which are only reclaimed when the inodes themselves are
    reclaimed.  This is problematic for bigger files that are still in use
    after they have a significant amount of their cache reclaimed, without
    any of those pages actually refaulting.  The shadow entries will just
    sit there and waste memory.  In the worst case, the shadow entries will
    accumulate until the machine runs out of memory.
    
    To get this under control, the VM will track radix tree nodes
    exclusively containing shadow entries on a per-NUMA node list.  Per-NUMA
    rather than global because we expect the radix tree nodes themselves to
    be allocated node-locally and we want to reduce cross-node references of
    otherwise independent cache workloads.  A simple shrinker will then
    reclaim these nodes on memory pressure.
    
    A few things need to be stored in the radix tree node to implement the
    shadow node LRU and allow tree deletions coming from the list:
    
    1. There is no index available that would describe the reverse path
       from the node up to the tree root, which is needed to perform a
       deletion.  To solve this, encode in each node its offset inside the
       parent.  This can be stored in the unused upper bits of the same
       member that stores the node's height at no extra space cost.
    
    2. The number of shadow entries needs to be counted in addition to the
       regular entries, to quickly detect when the node is ready to go to
       the shadow node LRU list.  The current entry count is an unsigned
       int but the maximum number of entries is 64, so a shadow counter
       can easily be stored in the unused upper bits.
    
    3. Tree modification needs tree lock and tree root, which are located
       in the address space, so store an address_space backpointer in the
       node.  The parent pointer of the node is in a union with the 2-word
       rcu_head, so the backpointer comes at no extra cost as well.
    
    4. The node needs to be linked to an LRU list, which requires a list
       head inside the node.  This does increase the size of the node, but
       it does not change the number of objects that fit into a slab page.
    
    [akpm@linux-foundation.org: export the right function]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Metin Doslu <metin@citusdata.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Ozgun Erdogan <ozgun@citusdata.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roman Gushchin <klamm@yandex-team.ru>
    Cc: Ryan Mallon <rmallon@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f25db1d74a21..fac5509c18f0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -144,6 +144,7 @@ enum zone_stat_item {
 #endif
 	WORKINGSET_REFAULT,
 	WORKINGSET_ACTIVATE,
+	WORKINGSET_NODERECLAIM,
 	NR_ANON_TRANSPARENT_HUGEPAGES,
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };

commit a528910e12ec7ee203095eb1711468a66b9b60b0
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Apr 3 14:47:51 2014 -0700

    mm: thrash detection-based file cache sizing
    
    The VM maintains cached filesystem pages on two types of lists.  One
    list holds the pages recently faulted into the cache, the other list
    holds pages that have been referenced repeatedly on that first list.
    The idea is to prefer reclaiming young pages over those that have shown
    to benefit from caching in the past.  We call the recently usedbut
    ultimately was not significantly better than a FIFO policy and still
    thrashed cache based on eviction speed, rather than actual demand for
    cache.
    
    This patch solves one half of the problem by decoupling the ability to
    detect working set changes from the inactive list size.  By maintaining
    a history of recently evicted file pages it can detect frequently used
    pages with an arbitrarily small inactive list size, and subsequently
    apply pressure on the active list based on actual demand for cache, not
    just overall eviction speed.
    
    Every zone maintains a counter that tracks inactive list aging speed.
    When a page is evicted, a snapshot of this counter is stored in the
    now-empty page cache radix tree slot.  On refault, the minimum access
    distance of the page can be assessed, to evaluate whether the page
    should be part of the active list or not.
    
    This fixes the VM's blindness towards working set changes in excess of
    the inactive list.  And it's the foundation to further improve the
    protection ability and reduce the minimum inactive list size of 50%.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Bob Liu <bob.liu@oracle.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Metin Doslu <metin@citusdata.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Ozgun Erdogan <ozgun@citusdata.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roman Gushchin <klamm@yandex-team.ru>
    Cc: Ryan Mallon <rmallon@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9b61b9bf81ac..f25db1d74a21 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -142,6 +142,8 @@ enum zone_stat_item {
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
 #endif
+	WORKINGSET_REFAULT,
+	WORKINGSET_ACTIVATE,
 	NR_ANON_TRANSPARENT_HUGEPAGES,
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
@@ -392,6 +394,9 @@ struct zone {
 	spinlock_t		lru_lock;
 	struct lruvec		lruvec;
 
+	/* Evictions & activations on the inactive file list */
+	atomic_long_t		inactive_age;
+
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */
 

commit e97ca8e5b864f88b028c1759ba8536fa827d6d96
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Mon Mar 10 15:49:43 2014 -0700

    mm: fix GFP_THISNODE callers and clarify
    
    GFP_THISNODE is for callers that implement their own clever fallback to
    remote nodes.  It restricts the allocation to the specified node and
    does not invoke reclaim, assuming that the caller will take care of it
    when the fallback fails, e.g.  through a subsequent allocation request
    without GFP_THISNODE set.
    
    However, many current GFP_THISNODE users only want the node exclusive
    aspect of the flag, without actually implementing their own fallback or
    triggering reclaim if necessary.  This results in things like page
    migration failing prematurely even when there is easily reclaimable
    memory available, unless kswapd happens to be running already or a
    concurrent allocation attempt triggers the necessary reclaim.
    
    Convert all callsites that don't implement their own fallback strategy
    to __GFP_THISNODE.  This restricts the allocation a single node too, but
    at the same time allows the allocator to enter the slowpath, wake
    kswapd, and invoke direct reclaim if necessary, to make the allocation
    happen when memory is full.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Jan Stancek <jstancek@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5f2052c83154..9b61b9bf81ac 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -590,10 +590,10 @@ static inline bool zone_is_empty(struct zone *zone)
 
 /*
  * The NUMA zonelists are doubled because we need zonelists that restrict the
- * allocations to a single node for GFP_THISNODE.
+ * allocations to a single node for __GFP_THISNODE.
  *
  * [0]	: Zonelist with fallback
- * [1]	: No fallback (GFP_THISNODE)
+ * [1]	: No fallback (__GFP_THISNODE)
  */
 #define MAX_ZONELISTS 2
 

commit 1c5e9c27cbd966c7f0038698d5dcd5ada3574f47
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 15:50:59 2014 -0800

    mm: numa: limit scope of lock for NUMA migrate rate limiting
    
    NUMA migrate rate limiting protects a migration counter and window using
    a lock but in some cases this can be a contended lock.  It is not
    critical that the number of pages be perfect, lost updates are
    acceptable.  Reduce the importance of this lock.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 67ab5febabf7..5f2052c83154 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -764,10 +764,7 @@ typedef struct pglist_data {
 	int kswapd_max_order;
 	enum zone_type classzone_idx;
 #ifdef CONFIG_NUMA_BALANCING
-	/*
-	 * Lock serializing the per destination node AutoNUMA memory
-	 * migration rate limiting data.
-	 */
+	/* Lock serializing the migrate rate limiting window */
 	spinlock_t numabalancing_migrate_lock;
 
 	/* Rate limiting time interval */

commit 943dca1a1fcbccb58de944669b833fd38a6c809b
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Tue Jan 21 15:49:06 2014 -0800

    mm: get rid of unnecessary pageblock scanning in setup_zone_migrate_reserve
    
    Yasuaki Ishimatsu reported memory hot-add spent more than 5 _hours_ on
    9TB memory machine since onlining memory sections is too slow.  And we
    found out setup_zone_migrate_reserve spent >90% of the time.
    
    The problem is, setup_zone_migrate_reserve scans all pageblocks
    unconditionally, but it is only necessary if the number of reserved
    block was reduced (i.e.  memory hot remove).
    
    Moreover, maximum MIGRATE_RESERVE per zone is currently 2.  It means
    that the number of reserved pageblocks is almost always unchanged.
    
    This patch adds zone->nr_migrate_reserve_block to maintain the number of
    MIGRATE_RESERVE pageblocks and it reduces the overhead of
    setup_zone_migrate_reserve dramatically.  The following table shows time
    of onlining a memory section.
    
      Amount of memory     | 128GB | 192GB | 256GB|
      ---------------------------------------------
      linux-3.12           |  23.9 |  31.4 | 44.5 |
      This patch           |   8.3 |   8.3 |  8.6 |
      Mel's proposal patch |  10.9 |  19.2 | 31.3 |
      ---------------------------------------------
                                       (millisecond)
    
      128GB : 4 nodes and each node has 32GB of memory
      192GB : 6 nodes and each node has 32GB of memory
      256GB : 8 nodes and each node has 32GB of memory
    
      (*1) Mel proposed his idea by the following threads.
           https://lkml.org/lkml/2013/10/30/272
    
    [akpm@linux-foundation.org: tweak comment]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reported-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Tested-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bd791e452ad7..67ab5febabf7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -489,6 +489,12 @@ struct zone {
 	unsigned long		present_pages;
 	unsigned long		managed_pages;
 
+	/*
+	 * Number of MIGRATE_RESEVE page block. To maintain for just
+	 * optimization. Protected by zone->lock.
+	 */
+	int			nr_migrate_reserve_block;
+
 	/*
 	 * rarely used fields:
 	 */

commit 6e543d5780e36ff5ee56c44d7e2e30db3457a7ed
Author: Lisa Du <cldu@marvell.com>
Date:   Wed Sep 11 14:22:36 2013 -0700

    mm: vmscan: fix do_try_to_free_pages() livelock
    
    This patch is based on KOSAKI's work and I add a little more description,
    please refer https://lkml.org/lkml/2012/6/14/74.
    
    Currently, I found system can enter a state that there are lots of free
    pages in a zone but only order-0 and order-1 pages which means the zone is
    heavily fragmented, then high order allocation could make direct reclaim
    path's long stall(ex, 60 seconds) especially in no swap and no compaciton
    enviroment.  This problem happened on v3.4, but it seems issue still lives
    in current tree, the reason is do_try_to_free_pages enter live lock:
    
    kswapd will go to sleep if the zones have been fully scanned and are still
    not balanced.  As kswapd thinks there's little point trying all over again
    to avoid infinite loop.  Instead it changes order from high-order to
    0-order because kswapd think order-0 is the most important.  Look at
    73ce02e9 in detail.  If watermarks are ok, kswapd will go back to sleep
    and may leave zone->all_unreclaimable =3D 0.  It assume high-order users
    can still perform direct reclaim if they wish.
    
    Direct reclaim continue to reclaim for a high order which is not a
    COSTLY_ORDER without oom-killer until kswapd turn on
    zone->all_unreclaimble= .  This is because to avoid too early oom-kill.
    So it means direct_reclaim depends on kswapd to break this loop.
    
    In worst case, direct-reclaim may continue to page reclaim forever when
    kswapd sleeps forever until someone like watchdog detect and finally kill
    the process.  As described in:
    http://thread.gmane.org/gmane.linux.kernel.mm/103737
    
    We can't turn on zone->all_unreclaimable from direct reclaim path because
    direct reclaim path don't take any lock and this way is racy.  Thus this
    patch removes zone->all_unreclaimable field completely and recalculates
    zone reclaimable state every time.
    
    Note: we can't take the idea that direct-reclaim see zone->pages_scanned
    directly and kswapd continue to use zone->all_unreclaimable.  Because, it
    is racy.  commit 929bea7c71 (vmscan: all_unreclaimable() use
    zone->all_unreclaimable as a name) describes the detail.
    
    [akpm@linux-foundation.org: uninline zone_reclaimable_pages() and zone_reclaimable()]
    Cc: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Nick Piggin <npiggin@gmail.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Bob Liu <lliubbo@gmail.com>
    Cc: Neil Zhang <zhangwm@marvell.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Lisa Du <cldu@marvell.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ac1ea796ec0f..bd791e452ad7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -353,7 +353,6 @@ struct zone {
 	 * free areas of different sizes
 	 */
 	spinlock_t		lock;
-	int                     all_unreclaimable; /* All pages pinned */
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
 	/* Set to true when the PG_migrate_skip bits should be cleared */
 	bool			compact_blockskip_flush;

commit 81c0a2bb515fd4daae8cab64352877480792b515
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Sep 11 14:20:47 2013 -0700

    mm: page_alloc: fair zone allocator policy
    
    Each zone that holds userspace pages of one workload must be aged at a
    speed proportional to the zone size.  Otherwise, the time an individual
    page gets to stay in memory depends on the zone it happened to be
    allocated in.  Asymmetry in the zone aging creates rather unpredictable
    aging behavior and results in the wrong pages being reclaimed, activated
    etc.
    
    But exactly this happens right now because of the way the page allocator
    and kswapd interact.  The page allocator uses per-node lists of all zones
    in the system, ordered by preference, when allocating a new page.  When
    the first iteration does not yield any results, kswapd is woken up and the
    allocator retries.  Due to the way kswapd reclaims zones below the high
    watermark while a zone can be allocated from when it is above the low
    watermark, the allocator may keep kswapd running while kswapd reclaim
    ensures that the page allocator can keep allocating from the first zone in
    the zonelist for extended periods of time.  Meanwhile the other zones
    rarely see new allocations and thus get aged much slower in comparison.
    
    The result is that the occasional page placed in lower zones gets
    relatively more time in memory, even gets promoted to the active list
    after its peers have long been evicted.  Meanwhile, the bulk of the
    working set may be thrashing on the preferred zone even though there may
    be significant amounts of memory available in the lower zones.
    
    Even the most basic test -- repeatedly reading a file slightly bigger than
    memory -- shows how broken the zone aging is.  In this scenario, no single
    page should be able stay in memory long enough to get referenced twice and
    activated, but activation happens in spades:
    
      $ grep active_file /proc/zoneinfo
          nr_inactive_file 0
          nr_active_file 0
          nr_inactive_file 0
          nr_active_file 8
          nr_inactive_file 1582
          nr_active_file 11994
      $ cat data data data data >/dev/null
      $ grep active_file /proc/zoneinfo
          nr_inactive_file 0
          nr_active_file 70
          nr_inactive_file 258753
          nr_active_file 443214
          nr_inactive_file 149793
          nr_active_file 12021
    
    Fix this with a very simple round robin allocator.  Each zone is allowed a
    batch of allocations that is proportional to the zone's size, after which
    it is treated as full.  The batch counters are reset when all zones have
    been tried and the allocator enters the slowpath and kicks off kswapd
    reclaim.  Allocation and reclaim is now fairly spread out to all
    available/allowable zones:
    
      $ grep active_file /proc/zoneinfo
          nr_inactive_file 0
          nr_active_file 0
          nr_inactive_file 174
          nr_active_file 4865
          nr_inactive_file 53
          nr_active_file 860
      $ cat data data data data >/dev/null
      $ grep active_file /proc/zoneinfo
          nr_inactive_file 0
          nr_active_file 0
          nr_inactive_file 666622
          nr_active_file 4988
          nr_inactive_file 190969
          nr_active_file 937
    
    When zone_reclaim_mode is enabled, allocations will now spread out to all
    zones on the local node, not just the first preferred zone (which on a 4G
    node might be a tiny Normal zone).
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Paul Bolle <paul.bollee@gmail.com>
    Cc: Zlatko Calusic <zcalusic@bitsync.net>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index af4a3b77a8de..ac1ea796ec0f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -105,6 +105,7 @@ struct zone_padding {
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
+	NR_ALLOC_BATCH,
 	NR_LRU_BASE,
 	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
 	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */

commit b21fbccd4b8aba805cbc231998ec7bf83616a79e
Author: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
Date:   Mon Jul 8 16:00:07 2013 -0700

    mm: remove unused functions is_{normal_idx, normal, dma32, dma}
    
    These functions are nowhere used, so remove them.
    
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ae19af5ec02c..af4a3b77a8de 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -869,11 +869,6 @@ static inline int is_highmem_idx(enum zone_type idx)
 #endif
 }
 
-static inline int is_normal_idx(enum zone_type idx)
-{
-	return (idx == ZONE_NORMAL);
-}
-
 /**
  * is_highmem - helper function to quickly check if a struct zone is a 
  *              highmem zone or not.  This is an attempt to keep references
@@ -892,29 +887,6 @@ static inline int is_highmem(struct zone *zone)
 #endif
 }
 
-static inline int is_normal(struct zone *zone)
-{
-	return zone == zone->zone_pgdat->node_zones + ZONE_NORMAL;
-}
-
-static inline int is_dma32(struct zone *zone)
-{
-#ifdef CONFIG_ZONE_DMA32
-	return zone == zone->zone_pgdat->node_zones + ZONE_DMA32;
-#else
-	return 0;
-#endif
-}
-
-static inline int is_dma(struct zone *zone)
-{
-#ifdef CONFIG_ZONE_DMA
-	return zone == zone->zone_pgdat->node_zones + ZONE_DMA;
-#else
-	return 0;
-#endif
-}
-
 /* These two functions are used to setup the per zone pages min values */
 struct ctl_table;
 int min_free_kbytes_sysctl_handler(struct ctl_table *, int,

commit 55878e88c59221c3187e1c24ec3b15eb79c374c0
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:04:44 2013 -0700

    sparsemem: add BUILD_BUG_ON when sizeof mem_section is non-power-of-2
    
    Instead of leaving a hidden trap for the next person who comes along and
    wants to add something to mem_section, add a big fat warning about it
    needing to be a power-of-2, and insert a BUILD_BUG_ON() in sparse_init()
    to catch mistakes.
    
    Right now non-power-of-2 mem_sections cause a number of WARNs at boot
    (which don't clearly point to the size of mem_section as an issue), but
    the system limps on (temporarily, at least).
    
    This is based upon Dave Hansen's earlier RFC where he ran into the same
    issue:
            "sparsemem: fix boot when SECTIONS_PER_ROOT is not power-of-2"
            http://lkml.indiana.edu/hypermail/linux/kernel/1205.2/03077.html
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Acked-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 09d381b71fd8..ae19af5ec02c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1137,6 +1137,10 @@ struct mem_section {
 	struct page_cgroup *page_cgroup;
 	unsigned long pad;
 #endif
+	/*
+	 * WARNING: mem_section must be a power-of-2 in size for the
+	 * calculation and use of SECTION_ROOT_MASK to make sense.
+	 */
 };
 
 #ifdef CONFIG_SPARSEMEM_EXTREME

commit c3d5f5f0c2bc4eabeaf49f1a21e1aeb965246cd2
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:14 2013 -0700

    mm: use a dedicated lock to protect totalram_pages and zone->managed_pages
    
    Currently lock_memory_hotplug()/unlock_memory_hotplug() are used to
    protect totalram_pages and zone->managed_pages.  Other than the memory
    hotplug driver, totalram_pages and zone->managed_pages may also be
    modified at runtime by other drivers, such as Xen balloon,
    virtio_balloon etc.  For those cases, memory hotplug lock is a little
    too heavy, so introduce a dedicated lock to protect totalram_pages and
    zone->managed_pages.
    
    Now we have a simplified locking rules totalram_pages and
    zone->managed_pages as:
    
    1) no locking for read accesses because they are unsigned long.
    2) no locking for write accesses at boot time in single-threaded context.
    3) serialize write accesses at runtime by acquiring the dedicated
       managed_page_count_lock.
    
    Also adjust zone->managed_pages when freeing reserved pages into the
    buddy system, to keep totalram_pages and zone->managed_pages in
    consistence.
    
    [akpm@linux-foundation.org: don't export adjust_managed_page_count to modules (for now)]
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e511f9429f1e..09d381b71fd8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -474,10 +474,16 @@ struct zone {
 	 * frequently read in proximity to zone->lock.  It's good to
 	 * give them a chance of being in the same cacheline.
 	 *
-	 * Write access to present_pages and managed_pages at runtime should
-	 * be protected by lock_memory_hotplug()/unlock_memory_hotplug().
-	 * Any reader who can't tolerant drift of present_pages and
-	 * managed_pages should hold memory hotplug lock to get a stable value.
+	 * Write access to present_pages at runtime should be protected by
+	 * lock_memory_hotplug()/unlock_memory_hotplug().  Any reader who can't
+	 * tolerant drift of present_pages should hold memory hotplug lock to
+	 * get a stable value.
+	 *
+	 * Read access to managed_pages should be safe because it's unsigned
+	 * long. Write access to zone->managed_pages and totalram_pages are
+	 * protected by managed_page_count_lock at runtime. Idealy only
+	 * adjust_managed_page_count() should be used instead of directly
+	 * touching zone->managed_pages and totalram_pages.
 	 */
 	unsigned long		spanned_pages;
 	unsigned long		present_pages;

commit 114d4b79f7e561fd76fb98eba79e18df7cdd60f0
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:02:09 2013 -0700

    mmzone: note that node_size_lock should be manipulated via pgdat_resize_lock()
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9ec7cffcae21..e511f9429f1e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -733,6 +733,9 @@ typedef struct pglist_data {
 	 * or node_spanned_pages stay constant.  Holding this will also
 	 * guarantee that any pfn_valid() stays that way.
 	 *
+	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
+	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG.
+	 *
 	 * Nests above zone->lock and zone->span_seqlock
 	 */
 	spinlock_t node_size_lock;

commit 72c3b51bda557ab38d6c5b2af750c27cba15f828
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:02:08 2013 -0700

    mm: fix comment referring to non-existent size_seqlock, change to span_seqlock
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fce64afba042..9ec7cffcae21 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -733,7 +733,7 @@ typedef struct pglist_data {
 	 * or node_spanned_pages stay constant.  Holding this will also
 	 * guarantee that any pfn_valid() stays that way.
 	 *
-	 * Nests above zone->lock and zone->size_seqlock.
+	 * Nests above zone->lock and zone->span_seqlock
 	 */
 	spinlock_t node_size_lock;
 #endif

commit 283aba9f9e0e4882bf09bd37a2983379a6fae805
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jul 3 15:01:51 2013 -0700

    mm: vmscan: block kswapd if it is encountering pages under writeback
    
    Historically, kswapd used to congestion_wait() at higher priorities if
    it was not making forward progress.  This made no sense as the failure
    to make progress could be completely independent of IO.  It was later
    replaced by wait_iff_congested() and removed entirely by commit 258401a6
    (mm: don't wait on congested zones in balance_pgdat()) as it was
    duplicating logic in shrink_inactive_list().
    
    This is problematic.  If kswapd encounters many pages under writeback
    and it continues to scan until it reaches the high watermark then it
    will quickly skip over the pages under writeback and reclaim clean young
    pages or push applications out to swap.
    
    The use of wait_iff_congested() is not suited to kswapd as it will only
    stall if the underlying BDI is really congested or a direct reclaimer
    was unable to write to the underlying BDI.  kswapd bypasses the BDI
    congestion as it sets PF_SWAPWRITE but even if this was taken into
    account then it would cause direct reclaimers to stall on writeback
    which is not desirable.
    
    This patch sets a ZONE_WRITEBACK flag if direct reclaim or kswapd is
    encountering too many pages under writeback.  If this flag is set and
    kswapd encounters a PageReclaim page under writeback then it'll assume
    that the LRU lists are being recycled too quickly before IO can complete
    and block waiting for some IO to complete.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Tested-by: Zlatko Calusic <zcalusic@bitsync.net>
    Cc: dormando <dormando@rydia.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2aaf72f7e345..fce64afba042 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -499,6 +499,9 @@ typedef enum {
 					 * many dirty file pages at the tail
 					 * of the LRU.
 					 */
+	ZONE_WRITEBACK,			/* reclaim scanning has recently found
+					 * many pages under writeback
+					 */
 } zone_flags_t;
 
 static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
@@ -526,6 +529,11 @@ static inline int zone_is_reclaim_dirty(const struct zone *zone)
 	return test_bit(ZONE_TAIL_LRU_DIRTY, &zone->flags);
 }
 
+static inline int zone_is_reclaim_writeback(const struct zone *zone)
+{
+	return test_bit(ZONE_WRITEBACK, &zone->flags);
+}
+
 static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);

commit d43006d503ac921c7df4f94d13c17db6f13c9d26
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jul 3 15:01:50 2013 -0700

    mm: vmscan: have kswapd writeback pages based on dirty pages encountered, not priority
    
    Currently kswapd queues dirty pages for writeback if scanning at an
    elevated priority but the priority kswapd scans at is not related to the
    number of unqueued dirty encountered.  Since commit "mm: vmscan: Flatten
    kswapd priority loop", the priority is related to the size of the LRU
    and the zone watermark which is no indication as to whether kswapd
    should write pages or not.
    
    This patch tracks if an excessive number of unqueued dirty pages are
    being encountered at the end of the LRU.  If so, it indicates that dirty
    pages are being recycled before flusher threads can clean them and flags
    the zone so that kswapd will start writing pages until the zone is
    balanced.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Tested-by: Zlatko Calusic <zcalusic@bitsync.net>
    Cc: dormando <dormando@rydia.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5c76737d836b..2aaf72f7e345 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -495,6 +495,10 @@ typedef enum {
 	ZONE_CONGESTED,			/* zone has many dirty pages backed by
 					 * a congested BDI
 					 */
+	ZONE_TAIL_LRU_DIRTY,		/* reclaim scanning has recently found
+					 * many dirty file pages at the tail
+					 * of the LRU.
+					 */
 } zone_flags_t;
 
 static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
@@ -517,6 +521,11 @@ static inline int zone_is_reclaim_congested(const struct zone *zone)
 	return test_bit(ZONE_CONGESTED, &zone->flags);
 }
 
+static inline int zone_is_reclaim_dirty(const struct zone *zone)
+{
+	return test_bit(ZONE_TAIL_LRU_DIRTY, &zone->flags);
+}
+
 static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);

commit 5d434fcb255dec99189f1c58a06e4f56e12bf77d
Merge: 5a5a1bf099d6 071361d3473e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 09:36:50 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree updates from Jiri Kosina:
     "Usual stuff, mostly comment fixes, typo fixes, printk fixes and small
      code cleanups"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (45 commits)
      mm: Convert print_symbol to %pSR
      gfs2: Convert print_symbol to %pSR
      m32r: Convert print_symbol to %pSR
      iostats.txt: add easy-to-find description for field 6
      x86 cmpxchg.h: fix wrong comment
      treewide: Fix typo in printk and comments
      doc: devicetree: Fix various typos
      docbook: fix 8250 naming in device-drivers
      pata_pdc2027x: Fix compiler warning
      treewide: Fix typo in printks
      mei: Fix comments in drivers/misc/mei
      treewide: Fix typos in kernel messages
      pm44xx: Fix comment for "CONFIG_CPU_IDLE"
      doc: Fix typo "CONFIG_CGROUP_CGROUP_MEMCG_SWAP"
      mmzone: correct "pags" to "pages" in comment.
      kernel-parameters: remove outdated 'noresidual' parameter
      Remove spurious _H suffixes from ifdef comments
      sound: Remove stray pluses from Kconfig file
      radio-shark: Fix printk "CONFIG_LED_CLASS"
      doc: put proper reference to CONFIG_MODULE_SIG_ENFORCE
      ...

commit 8761e31c227f9751327196f170eba2b519eab48f
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Tue Mar 26 10:30:44 2013 -0700

    mmzone: correct "pags" to "pages" in comment.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ede274957e05..2570216b844a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -450,7 +450,7 @@ struct zone {
 	 *
 	 * present_pages is physical pages existing within the zone, which
 	 * is calculated as:
-	 *	present_pages = spanned_pages - absent_pages(pags in holes);
+	 *	present_pages = spanned_pages - absent_pages(pages in holes);
 	 *
 	 * managed_pages is present pages managed by the buddy system, which
 	 * is calculated as (reserved_pages includes pages allocated by the

commit f9228b204f789493117e458d2fefae937edb7272
Author: Russ Anderson <rja@sgi.com>
Date:   Fri Mar 22 15:04:43 2013 -0700

    mm: zone_end_pfn is too small
    
    Booting with 32 TBytes memory hits BUG at mm/page_alloc.c:552! (output
    below).
    
    The key hint is "page 4294967296 outside zone".
    4294967296 = 0x100000000 (bit 32 is set).
    
    The problem is in include/linux/mmzone.h:
    
      530 static inline unsigned zone_end_pfn(const struct zone *zone)
      531 {
      532         return zone->zone_start_pfn + zone->spanned_pages;
      533 }
    
    zone_end_pfn is "unsigned" (32 bits).  Changing it to "unsigned long"
    (64 bits) fixes the problem.
    
    zone_end_pfn() was added recently in commit 108bcc96ef70 ("mm: add & use
    zone_end_pfn() and zone_spans_pfn()")
    
    Output from the failure.
    
      No AGP bridge found
      page 4294967296 outside zone [ 4294967296 - 4327469056 ]
      ------------[ cut here ]------------
      kernel BUG at mm/page_alloc.c:552!
      invalid opcode: 0000 [#1] SMP
      Modules linked in:
      CPU 0
      Pid: 0, comm: swapper Not tainted 3.9.0-rc2.dtp+ #10
      RIP: free_one_page+0x382/0x430
      Process swapper (pid: 0, threadinfo ffffffff81942000, task ffffffff81955420)
      Call Trace:
        __free_pages_ok+0x96/0xb0
        __free_pages+0x25/0x50
        __free_pages_bootmem+0x8a/0x8c
        __free_memory_core+0xea/0x131
        free_low_memory_core_early+0x4a/0x98
        free_all_bootmem+0x45/0x47
        mem_init+0x7b/0x14c
        start_kernel+0x216/0x433
        x86_64_start_reservations+0x2a/0x2c
        x86_64_start_kernel+0x144/0x153
      Code: 89 f1 ba 01 00 00 00 31 f6 d3 e2 4c 89 ef e8 66 a4 01 00 e9 2c fe ff ff 0f 0b eb fe 0f 0b 66 66 2e 0f 1f 84 00 00 00 00 00 eb f3 <0f> 0b eb fe 0f 0b 0f 1f 84 00 00 00 00 00 eb f6 0f 0b eb fe 49
    
    Signed-off-by: Russ Anderson <rja@sgi.com>
    Reported-by: George Beshers <gbeshers@sgi.com>
    Acked-by: Hedi Berriche <hedi@sgi.com>
    Cc: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ede274957e05..c74092eebf5c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -527,7 +527,7 @@ static inline int zone_is_oom_locked(const struct zone *zone)
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
 }
 
-static inline unsigned zone_end_pfn(const struct zone *zone)
+static inline unsigned long zone_end_pfn(const struct zone *zone)
 {
 	return zone->zone_start_pfn + zone->spanned_pages;
 }

commit da3649e133948d8b7d8c57b05a33faf62ac2cc7e
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Fri Feb 22 16:35:27 2013 -0800

    mmzone: add pgdat_{end_pfn,is_empty}() helpers & consolidate.
    
    Add pgdat_end_pfn() and pgdat_is_empty() helpers which match the similar
    zone_*() functions.
    
    Change node_end_pfn() to be a wrapper of pgdat_end_pfn().
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: David Hansen <dave@linux.vnet.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cf8925962b68..ede274957e05 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -756,11 +756,17 @@ typedef struct pglist_data {
 #define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))
 
 #define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)
+#define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))
 
-#define node_end_pfn(nid) ({\
-	pg_data_t *__pgdat = NODE_DATA(nid);\
-	__pgdat->node_start_pfn + __pgdat->node_spanned_pages;\
-})
+static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)
+{
+	return pgdat->node_start_pfn + pgdat->node_spanned_pages;
+}
+
+static inline bool pgdat_is_empty(pg_data_t *pgdat)
+{
+	return !pgdat->node_start_pfn && !pgdat->node_spanned_pages;
+}
 
 #include <linux/memory_hotplug.h>
 

commit 2a6e3ebee2edcade56f836390a5f0c7b76ff5f9e
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Fri Feb 22 16:35:24 2013 -0800

    mm: add zone_is_empty() and zone_is_initialized()
    
    Factoring out these 2 checks makes it more clear what we are actually
    checking for.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: David Hansen <dave@linux.vnet.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 34343f51e211..cf8925962b68 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -537,6 +537,16 @@ static inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)
 	return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);
 }
 
+static inline bool zone_is_initialized(struct zone *zone)
+{
+	return !!zone->wait_table;
+}
+
+static inline bool zone_is_empty(struct zone *zone)
+{
+	return zone->spanned_pages == 0;
+}
+
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the

commit 108bcc96ef7047c02cad4d229f04da38186a3f3f
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Fri Feb 22 16:35:23 2013 -0800

    mm: add & use zone_end_pfn() and zone_spans_pfn()
    
    Add 2 helpers (zone_end_pfn() and zone_spans_pfn()) to reduce code
    duplication.
    
    This also switches to using them in compaction (where an additional
    variable needed to be renamed), page_alloc, vmstat, memory_hotplug, and
    kmemleak.
    
    Note that in compaction.c I avoid calling zone_end_pfn() repeatedly
    because I expect at some point the sycronization issues with start_pfn &
    spanned_pages will need fixing, either by actually using the seqlock or
    clever memory barrier usage.
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: David Hansen <dave@linux.vnet.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6c80d0ac14dd..34343f51e211 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -527,6 +527,16 @@ static inline int zone_is_oom_locked(const struct zone *zone)
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
 }
 
+static inline unsigned zone_end_pfn(const struct zone *zone)
+{
+	return zone->zone_start_pfn + zone->spanned_pages;
+}
+
+static inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)
+{
+	return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);
+}
+
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the

commit bbeae5b05ef6e40bf54db05ceb8635824153b9e2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Feb 22 16:34:30 2013 -0800

    mm: move page flags layout to separate header
    
    This is a preparation patch for moving page->_last_nid into page->flags
    that moves page flag layout information to a separate header.  This
    patch is necessary because otherwise there would be a circular
    dependency between mm_types.h and mm.h.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Simon Jeons <simon.jeons@gmail.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4f4c8c26fa9d..6c80d0ac14dd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -15,7 +15,7 @@
 #include <linux/seqlock.h>
 #include <linux/nodemask.h>
 #include <linux/pageblock-flags.h>
-#include <generated/bounds.h>
+#include <linux/page-flags-layout.h>
 #include <linux/atomic.h>
 #include <asm/page.h>
 
@@ -310,24 +310,6 @@ enum zone_type {
 
 #ifndef __GENERATING_BOUNDS_H
 
-/*
- * When a memory allocation must conform to specific limitations (such
- * as being suitable for DMA) the caller will pass in hints to the
- * allocator in the gfp_mask, in the zone modifier bits.  These bits
- * are used to select a priority ordered list of memory zones which
- * match the requested limits. See gfp_zone() in include/linux/gfp.h
- */
-
-#if MAX_NR_ZONES < 2
-#define ZONES_SHIFT 0
-#elif MAX_NR_ZONES <= 2
-#define ZONES_SHIFT 1
-#elif MAX_NR_ZONES <= 4
-#define ZONES_SHIFT 2
-#else
-#error ZONES_SHIFT -- too many zones configured adjust calculation
-#endif
-
 struct zone {
 	/* Fields commonly accessed by the page allocator */
 
@@ -1055,8 +1037,6 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
  * PA_SECTION_SHIFT		physical address to/from section number
  * PFN_SECTION_SHIFT		pfn to/from section number
  */
-#define SECTIONS_SHIFT		(MAX_PHYSMEM_BITS - SECTION_SIZE_BITS)
-
 #define PA_SECTION_SHIFT	(SECTION_SIZE_BITS)
 #define PFN_SECTION_SHIFT	(SECTION_SIZE_BITS - PAGE_SHIFT)
 

commit 194159fbcc0d6ac1351837d3cd7a27a4af0219a6
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Feb 22 16:33:58 2013 -0800

    mm: remove MIGRATE_ISOLATE check in hotpath
    
    Several functions test MIGRATE_ISOLATE and some of those are hotpath but
    MIGRATE_ISOLATE is used only if we enable CONFIG_MEMORY_ISOLATION(ie,
    CMA, memory-hotplug and memory-failure) which are not common config
    option.  So let's not add unnecessary overhead and code when we don't
    enable CONFIG_MEMORY_ISOLATION.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 73b64a38b984..4f4c8c26fa9d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -57,7 +57,9 @@ enum {
 	 */
 	MIGRATE_CMA,
 #endif
+#ifdef CONFIG_MEMORY_ISOLATION
 	MIGRATE_ISOLATE,	/* can't allocate from here */
+#endif
 	MIGRATE_TYPES
 };
 

commit a458431e176ddb27e8ef8b98c2a681b217337393
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Fri Jan 4 15:35:08 2013 -0800

    mm: fix zone_watermark_ok_safe() accounting of isolated pages
    
    Commit 702d1a6e0766 ("memory-hotplug: fix kswapd looping forever
    problem") added an isolated pageblocks counter (nr_pageblock_isolate in
    struct zone) and used it to adjust free pages counter in
    zone_watermark_ok_safe() to prevent kswapd looping forever problem.
    
    Then later, commit 2139cbe627b8 ("cma: fix counting of isolated pages")
    fixed accounting of isolated pages in global free pages counter.  It
    made the previous zone_watermark_ok_safe() fix unnecessary and
    potentially harmful (cause now isolated pages may be accounted twice
    making free pages counter incorrect).
    
    This patch removes the special isolated pageblocks counter altogether
    which fixes zone_watermark_ok_safe() free pages check.
    
    Reported-by: Tomasz Stanislawski <t.stanislaws@samsung.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4bec5be82cab..73b64a38b984 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -503,14 +503,6 @@ struct zone {
 	 * rarely used fields:
 	 */
 	const char		*name;
-#ifdef CONFIG_MEMORY_ISOLATION
-	/*
-	 * the number of MIGRATE_ISOLATE *pageblock*.
-	 * We need this for free page counting. Look at zone_watermark_ok_safe.
-	 * It's protected by zone->lock
-	 */
-	int		nr_pageblock_isolate;
-#endif
 } ____cacheline_internodealigned_in_smp;
 
 typedef enum {

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 9feedc9d831e18ae6d0d15aa562e5e46ba53647b
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Dec 12 13:52:12 2012 -0800

    mm: introduce new field "managed_pages" to struct zone
    
    Currently a zone's present_pages is calcuated as below, which is
    inaccurate and may cause trouble to memory hotplug.
    
            spanned_pages - absent_pages - memmap_pages - dma_reserve.
    
    During fixing bugs caused by inaccurate zone->present_pages, we found
    zone->present_pages has been abused.  The field zone->present_pages may
    have different meanings in different contexts:
    
    1) pages existing in a zone.
    2) pages managed by the buddy system.
    
    For more discussions about the issue, please refer to:
      http://lkml.org/lkml/2012/11/5/866
      https://patchwork.kernel.org/patch/1346751/
    
    This patchset tries to introduce a new field named "managed_pages" to
    struct zone, which counts "pages managed by the buddy system".  And revert
    zone->present_pages to count "physical pages existing in a zone", which
    also keep in consistence with pgdat->node_present_pages.
    
    We will set an initial value for zone->managed_pages in function
    free_area_init_core() and will adjust it later if the initial value is
    inaccurate.
    
    For DMA/normal zones, the initial value is set to:
    
            (spanned_pages - absent_pages - memmap_pages - dma_reserve)
    
    Later zone->managed_pages will be adjusted to the accurate value when the
    bootmem allocator frees all free pages to the buddy system in function
    free_all_bootmem_node() and free_all_bootmem().
    
    The bootmem allocator doesn't touch highmem pages, so highmem zones'
    managed_pages is set to the accurate value "spanned_pages - absent_pages"
    in function free_area_init_core() and won't be updated anymore.
    
    This patch also adds a new field "managed_pages" to /proc/zoneinfo
    and sysrq showmem.
    
    [akpm@linux-foundation.org: small comment tweaks]
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Maciej Rutecki <maciej.rutecki@gmail.com>
    Tested-by: Chris Clayton <chris2553@googlemail.com>
    Cc: "Rafael J . Wysocki" <rjw@sisk.pl>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0c0b1d608a69..cd55dad56aac 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -460,17 +460,44 @@ struct zone {
 	unsigned long		zone_start_pfn;
 
 	/*
-	 * zone_start_pfn, spanned_pages and present_pages are all
-	 * protected by span_seqlock.  It is a seqlock because it has
-	 * to be read outside of zone->lock, and it is done in the main
-	 * allocator path.  But, it is written quite infrequently.
+	 * spanned_pages is the total pages spanned by the zone, including
+	 * holes, which is calculated as:
+	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
 	 *
-	 * The lock is declared along with zone->lock because it is
+	 * present_pages is physical pages existing within the zone, which
+	 * is calculated as:
+	 *	present_pages = spanned_pages - absent_pages(pags in holes);
+	 *
+	 * managed_pages is present pages managed by the buddy system, which
+	 * is calculated as (reserved_pages includes pages allocated by the
+	 * bootmem allocator):
+	 *	managed_pages = present_pages - reserved_pages;
+	 *
+	 * So present_pages may be used by memory hotplug or memory power
+	 * management logic to figure out unmanaged pages by checking
+	 * (present_pages - managed_pages). And managed_pages should be used
+	 * by page allocator and vm scanner to calculate all kinds of watermarks
+	 * and thresholds.
+	 *
+	 * Locking rules:
+	 *
+	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
+	 * It is a seqlock because it has to be read outside of zone->lock,
+	 * and it is done in the main allocator path.  But, it is written
+	 * quite infrequently.
+	 *
+	 * The span_seq lock is declared along with zone->lock because it is
 	 * frequently read in proximity to zone->lock.  It's good to
 	 * give them a chance of being in the same cacheline.
+	 *
+	 * Write access to present_pages and managed_pages at runtime should
+	 * be protected by lock_memory_hotplug()/unlock_memory_hotplug().
+	 * Any reader who can't tolerant drift of present_pages and
+	 * managed_pages should hold memory hotplug lock to get a stable value.
 	 */
-	unsigned long		spanned_pages;	/* total size, including holes */
-	unsigned long		present_pages;	/* amount of memory (excluding holes) */
+	unsigned long		spanned_pages;
+	unsigned long		present_pages;
+	unsigned long		managed_pages;
 
 	/*
 	 * rarely used fields:

commit bc357f431c836c6631751e3ef7dfe7882394ad67
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Dec 11 16:02:59 2012 -0800

    mm: cma: remove watermark hacks
    
    Commits 2139cbe627b8 ("cma: fix counting of isolated pages") and
    d95ea5d18e69 ("cma: fix watermark checking") introduced a reliable
    method of free page accounting when memory is being allocated from CMA
    regions, so the workaround introduced earlier by commit 49f223a9cd96
    ("mm: trigger page reclaim in alloc_contig_range() to stabilise
    watermarks") can be finally removed.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a23923ba8263..0c0b1d608a69 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -63,10 +63,8 @@ enum {
 
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
-#  define cma_wmark_pages(zone)	zone->min_cma_pages
 #else
 #  define is_migrate_cma(migratetype) false
-#  define cma_wmark_pages(zone) 0
 #endif
 
 #define for_each_migratetype_order(order, type) \
@@ -382,13 +380,6 @@ struct zone {
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;
-#endif
-#ifdef CONFIG_CMA
-	/*
-	 * CMA needs to increase watermark levels during the allocation
-	 * process to make sure that the system is not starved.
-	 */
-	unsigned long		min_cma_pages;
 #endif
 	struct free_area	free_area[MAX_ORDER];
 

commit 8177a420ed7c16c171ed3c3aec5b0676db38c247
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Fri Mar 23 20:56:34 2012 +0100

    mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
    
    This defines the per-node data used by Migrate On Fault in order to
    rate limit the migration. The rate limiting is applied independently
    to each destination node.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a23923ba8263..28601fdfcdb2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -717,6 +717,19 @@ typedef struct pglist_data {
 	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */
 	int kswapd_max_order;
 	enum zone_type classzone_idx;
+#ifdef CONFIG_NUMA_BALANCING
+	/*
+	 * Lock serializing the per destination node AutoNUMA memory
+	 * migration rate limiting data.
+	 */
+	spinlock_t numabalancing_migrate_lock;
+
+	/* Rate limiting time interval */
+	unsigned long numabalancing_migrate_next_window;
+
+	/* Number of pages migrated during the rate limiting time interval */
+	unsigned long numabalancing_migrate_nr_pages;
+#endif
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)

commit bea8c150a7efbc0f204e709b7274fe273f55e0d3
Author: Hugh Dickins <hughd@google.com>
Date:   Fri Nov 16 14:14:54 2012 -0800

    memcg: fix hotplugged memory zone oops
    
    When MEMCG is configured on (even when it's disabled by boot option),
    when adding or removing a page to/from its lru list, the zone pointer
    used for stats updates is nowadays taken from the struct lruvec.  (On
    many configurations, calculating zone from page is slower.)
    
    But we have no code to update all the lruvecs (per zone, per memcg) when
    a memory node is hotadded.  Here's an extract from the oops which
    results when running numactl to bind a program to a newly onlined node:
    
      BUG: unable to handle kernel NULL pointer dereference at 0000000000000f60
      IP:  __mod_zone_page_state+0x9/0x60
      Pid: 1219, comm: numactl Not tainted 3.6.0-rc5+ #180 Bochs Bochs
      Process numactl (pid: 1219, threadinfo ffff880039abc000, task ffff8800383c4ce0)
      Call Trace:
        __pagevec_lru_add_fn+0xdf/0x140
        pagevec_lru_move_fn+0xb1/0x100
        __pagevec_lru_add+0x1c/0x30
        lru_add_drain_cpu+0xa3/0x130
        lru_add_drain+0x2f/0x40
       ...
    
    The natural solution might be to use a memcg callback whenever memory is
    hotadded; but that solution has not been scoped out, and it happens that
    we do have an easy location at which to update lruvec->zone.  The lruvec
    pointer is discovered either by mem_cgroup_zone_lruvec() or by
    mem_cgroup_page_lruvec(), and both of those do know the right zone.
    
    So check and set lruvec->zone in those; and remove the inadequate
    attempt to set lruvec->zone from lruvec_init(), which is called before
    NODE_DATA(node) has been allocated in such cases.
    
    Ah, there was one exceptionr.  For no particularly good reason,
    mem_cgroup_force_empty_list() has its own code for deciding lruvec.
    Change it to use the standard mem_cgroup_zone_lruvec() and
    mem_cgroup_get_lru_size() too.  In fact it was already safe against such
    an oops (the lru lists in danger could only be empty), but we're better
    proofed against future changes this way.
    
    I've marked this for stable (3.6) since we introduced the problem in 3.5
    (now closed to stable); but I have no idea if this is the only fix
    needed to get memory hotadd working with memcg in 3.6, and received no
    answer when I enquired twice before.
    
    Reported-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 50aaca81f63d..a23923ba8263 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -752,7 +752,7 @@ extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
 				     unsigned long size,
 				     enum memmap_context context);
 
-extern void lruvec_init(struct lruvec *lruvec, struct zone *zone);
+extern void lruvec_init(struct lruvec *lruvec);
 
 static inline struct zone *lruvec_zone(struct lruvec *lruvec)
 {

commit e46a28790e594c0876d1a84270926abf75460f61
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:48 2012 -0700

    CMA: migrate mlocked pages
    
    Presently CMA cannot migrate mlocked pages so it ends up failing to allocate
    contiguous memory space.
    
    This patch makes mlocked pages be migrated out.  Of course, it can affect
    realtime processes but in CMA usecase, contiguous memory allocation failing
    is far worse than access latency to an mlocked page being variable while
    CMA is running.  If someone wants to make the system realtime, he shouldn't
    enable CMA because stalls can still happen at random times.
    
    [akpm@linux-foundation.org: tweak comment text, per Mel]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a5578871d033..50aaca81f63d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -218,6 +218,8 @@ struct lruvec {
 #define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)
 /* Isolate for asynchronous migration */
 #define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x4)
+/* Isolate unevictable pages */
+#define ISOLATE_UNEVICTABLE	((__force isolate_mode_t)0x8)
 
 /* LRU Isolation modes. */
 typedef unsigned __bitwise__ isolate_mode_t;

commit 957f822a0ab95e88b146638bad6209bbc315bedd
Author: David Rientjes <rientjes@google.com>
Date:   Mon Oct 8 16:33:24 2012 -0700

    mm, numa: reclaim from all nodes within reclaim distance
    
    RECLAIM_DISTANCE represents the distance between nodes at which it is
    deemed too costly to allocate from; it's preferred to try to reclaim from
    a local zone before falling back to allocating on a remote node with such
    a distance.
    
    To do this, zone_reclaim_mode is set if the distance between any two
    nodes on the system is greather than this distance.  This, however, ends
    up causing the page allocator to reclaim from every zone regardless of
    its affinity.
    
    What we really want is to reclaim only from zones that are closer than
    RECLAIM_DISTANCE.  This patch adds a nodemask to each node that
    represents the set of nodes that are within this distance.  During the
    zone iteration, if the bit for a zone's node is set for the local node,
    then reclaim is attempted; otherwise, the zone is skipped.
    
    [akpm@linux-foundation.org: fix CONFIG_NUMA=n build]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d240efa8f846..a5578871d033 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -709,6 +709,7 @@ typedef struct pglist_data {
 	unsigned long node_spanned_pages; /* total size of physical page
 					     range, including holes */
 	int node_id;
+	nodemask_t reclaim_nodes;	/* Nodes allowed to reclaim from */
 	wait_queue_head_t kswapd_wait;
 	wait_queue_head_t pfmemalloc_wait;
 	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */

commit 62997027ca5b3d4618198ed8b1aba40b61b1137b
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:47 2012 -0700

    mm: compaction: clear PG_migrate_skip based on compaction and reclaim activity
    
    Compaction caches if a pageblock was scanned and no pages were isolated so
    that the pageblocks can be skipped in the future to reduce scanning.  This
    information is not cleared by the page allocator based on activity due to
    the impact it would have to the page allocator fast paths.  Hence there is
    a requirement that something clear the cache or pageblocks will be skipped
    forever.  Currently the cache is cleared if there were a number of recent
    allocation failures and it has not been cleared within the last 5 seconds.
    Time-based decisions like this are terrible as they have no relationship
    to VM activity and is basically a big hammer.
    
    Unfortunately, accurate heuristics would add cost to some hot paths so
    this patch implements a rough heuristic.  There are two cases where the
    cache is cleared.
    
    1. If a !kswapd process completes a compaction cycle (migrate and free
       scanner meet), the zone is marked compact_blockskip_flush. When kswapd
       goes to sleep, it will clear the cache. This is expected to be the
       common case where the cache is cleared. It does not really matter if
       kswapd happens to be asleep or going to sleep when the flag is set as
       it will be woken on the next allocation request.
    
    2. If there have been multiple failures recently and compaction just
       finished being deferred then a process will clear the cache and start a
       full scan.  This situation happens if there are multiple high-order
       allocation requests under heavy memory pressure.
    
    The clearing of the PG_migrate_skip bits and other scans is inherently
    racy but the race is harmless.  For allocations that can fail such as THP,
    they will simply fail.  For requests that cannot fail, they will retry the
    allocation.  Tests indicated that scanning rates were roughly similar to
    when the time-based heuristic was used and the allocation success rates
    were similar.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Rafael Aquini <aquini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c8b3abc97a1e..d240efa8f846 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -370,7 +370,8 @@ struct zone {
 	spinlock_t		lock;
 	int                     all_unreclaimable; /* All pages pinned */
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
-	unsigned long		compact_blockskip_expire;
+	/* Set to true when the PG_migrate_skip bits should be cleared */
+	bool			compact_blockskip_flush;
 
 	/* pfns where compaction scanners should start */
 	unsigned long		compact_cached_free_pfn;

commit c89511ab2f8fe2b47585e60da8af7fd213ec877e
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:45 2012 -0700

    mm: compaction: Restart compaction from near where it left off
    
    This is almost entirely based on Rik's previous patches and discussions
    with him about how this might be implemented.
    
    Order > 0 compaction stops when enough free pages of the correct page
    order have been coalesced.  When doing subsequent higher order
    allocations, it is possible for compaction to be invoked many times.
    
    However, the compaction code always starts out looking for things to
    compact at the start of the zone, and for free pages to compact things to
    at the end of the zone.
    
    This can cause quadratic behaviour, with isolate_freepages starting at the
    end of the zone each time, even though previous invocations of the
    compaction code already filled up all free memory on that end of the zone.
     This can cause isolate_freepages to take enormous amounts of CPU with
    certain workloads on larger memory systems.
    
    This patch caches where the migration and free scanner should start from
    on subsequent compaction invocations using the pageblock-skip information.
     When compaction starts it begins from the cached restart points and will
    update the cached restart points until a page is isolated or a pageblock
    is skipped that would have been scanned by synchronous compaction.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f85ecc9cfa1b..c8b3abc97a1e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -371,6 +371,10 @@ struct zone {
 	int                     all_unreclaimable; /* All pages pinned */
 #if defined CONFIG_COMPACTION || defined CONFIG_CMA
 	unsigned long		compact_blockskip_expire;
+
+	/* pfns where compaction scanners should start */
+	unsigned long		compact_cached_free_pfn;
+	unsigned long		compact_cached_migrate_pfn;
 #endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */

commit bb13ffeb9f6bfeb301443994dfbf29f91117dfb3
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:41 2012 -0700

    mm: compaction: cache if a pageblock was scanned and no pages were isolated
    
    When compaction was implemented it was known that scanning could
    potentially be excessive.  The ideal was that a counter be maintained for
    each pageblock but maintaining this information would incur a severe
    penalty due to a shared writable cache line.  It has reached the point
    where the scanning costs are a serious problem, particularly on
    long-lived systems where a large process starts and allocates a large
    number of THPs at the same time.
    
    Instead of using a shared counter, this patch adds another bit to the
    pageblock flags called PG_migrate_skip.  If a pageblock is scanned by
    either migrate or free scanner and 0 pages were isolated, the pageblock is
    marked to be skipped in the future.  When scanning, this bit is checked
    before any scanning takes place and the block skipped if set.
    
    The main difficulty with a patch like this is "when to ignore the cached
    information?" If it's ignored too often, the scanning rates will still be
    excessive.  If the information is too stale then allocations will fail
    that might have otherwise succeeded.  In this patch
    
    o CMA always ignores the information
    o If the migrate and free scanner meet then the cached information will
      be discarded if it's at least 5 seconds since the last time the cache
      was discarded
    o If there are a large number of allocation failures, discard the cache.
    
    The time-based heuristic is very clumsy but there are few choices for a
    better event.  Depending solely on multiple allocation failures still
    allows excessive scanning when THP allocations are failing in quick
    succession due to memory pressure.  Waiting until memory pressure is
    relieved would cause compaction to continually fail instead of using
    reclaim/compaction to try allocate the page.  The time-based mechanism is
    clumsy but a better option is not obvious.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 16a4cc2950a0..f85ecc9cfa1b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -369,6 +369,9 @@ struct zone {
 	 */
 	spinlock_t		lock;
 	int                     all_unreclaimable; /* All pages pinned */
+#if defined CONFIG_COMPACTION || defined CONFIG_CMA
+	unsigned long		compact_blockskip_expire;
+#endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;

commit 753341a4b85ff337487b9959c71c529f522004f4
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:40 2012 -0700

    revert "mm: have order > 0 compaction start off where it left"
    
    This reverts commit 7db8889ab05b ("mm: have order > 0 compaction start
    off where it left") and commit de74f1cc ("mm: have order > 0 compaction
    start near a pageblock with free pages").  These patches were a good
    idea and tests confirmed that they massively reduced the amount of
    scanning but the implementation is complex and tricky to understand.  A
    later patch will cache what pageblocks should be skipped and
    reimplements the concept of compact_cached_free_pfn on top for both
    migration and free scanners.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 85ac67aa5770..16a4cc2950a0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -369,10 +369,6 @@ struct zone {
 	 */
 	spinlock_t		lock;
 	int                     all_unreclaimable; /* All pages pinned */
-#if defined CONFIG_COMPACTION || defined CONFIG_CMA
-	/* pfn where the last incremental compaction isolated free pages */
-	unsigned long		compact_cached_free_pfn;
-#endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;

commit d1ce749a0db12202b711d1aba1d29e823034648d
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Mon Oct 8 16:32:02 2012 -0700

    cma: count free CMA pages
    
    Add NR_FREE_CMA_PAGES counter to be later used for checking watermark in
    __zone_watermark_ok().  For simplicity and to avoid #ifdef hell make this
    counter always available (not only when CONFIG_CMA=y).
    
    [akpm@linux-foundation.org: use conventional migratetype naming]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2daa54f55db7..85ac67aa5770 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -142,6 +142,7 @@ enum zone_stat_item {
 	NUMA_OTHER,		/* allocation from other node */
 #endif
 	NR_ANON_TRANSPARENT_HUGEPAGES,
+	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
 /*

commit 5515061d22f0f9976ae7815864bfd22042d36848
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:35 2012 -0700

    mm: throttle direct reclaimers if PF_MEMALLOC reserves are low and swap is backed by network storage
    
    If swap is backed by network storage such as NBD, there is a risk that a
    large number of reclaimers can hang the system by consuming all
    PF_MEMALLOC reserves.  To avoid these hangs, the administrator must tune
    min_free_kbytes in advance which is a bit fragile.
    
    This patch throttles direct reclaimers if half the PF_MEMALLOC reserves
    are in use.  If the system is routinely getting throttled the system
    administrator can increase min_free_kbytes so degradation is smoother but
    the system will keep running.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 64b2c3a48286..2daa54f55db7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -705,6 +705,7 @@ typedef struct pglist_data {
 					     range, including holes */
 	int node_id;
 	wait_queue_head_t kswapd_wait;
+	wait_queue_head_t pfmemalloc_wait;
 	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */
 	int kswapd_max_order;
 	enum zone_type classzone_idx;

commit 702d1a6e0766d45642c934444fd41f658d251305
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:56 2012 -0700

    memory-hotplug: fix kswapd looping forever problem
    
    When hotplug offlining happens on zone A, it starts to mark freed page as
    MIGRATE_ISOLATE type in buddy for preventing further allocation.
    (MIGRATE_ISOLATE is very irony type because it's apparently on buddy but
    we can't allocate them).
    
    When the memory shortage happens during hotplug offlining, current task
    starts to reclaim, then wake up kswapd.  Kswapd checks watermark, then go
    sleep because current zone_watermark_ok_safe doesn't consider
    MIGRATE_ISOLATE freed page count.  Current task continue to reclaim in
    direct reclaim path without kswapd's helping.  The problem is that
    zone->all_unreclaimable is set by only kswapd so that current task would
    be looping forever like below.
    
    __alloc_pages_slowpath
    restart:
            wake_all_kswapd
    rebalance:
            __alloc_pages_direct_reclaim
                    do_try_to_free_pages
                            if global_reclaim && !all_unreclaimable
                                    return 1; /* It means we did did_some_progress */
            skip __alloc_pages_may_oom
            should_alloc_retry
                    goto rebalance;
    
    If we apply KOSAKI's patch[1] which doesn't depends on kswapd about
    setting zone->all_unreclaimable, we can solve this problem by killing some
    task in direct reclaim path.  But it doesn't wake up kswapd, still.  It
    could be a problem still if other subsystem needs GFP_ATOMIC request.  So
    kswapd should consider MIGRATE_ISOLATE when it calculate free pages BEFORE
    going sleep.
    
    This patch counts the number of MIGRATE_ISOLATE page block and
    zone_watermark_ok_safe will consider it if the system has such blocks
    (fortunately, it's very rare so no problem in POV overhead and kswapd is
    never hotpath).
    
    Copy/modify from Mel's quote
    "
    Ideal solution would be "allocating" the pageblock.
    It would keep the free space accounting as it is but historically,
    memory hotplug didn't allocate pages because it would be difficult to
    detect if a pageblock was isolated or if part of some balloon.
    Allocating just full pageblocks would work around this, However,
    it would play very badly with CMA.
    "
    
    [1] http://lkml.org/lkml/2012/6/14/74
    
    [akpm@linux-foundation.org: simplify nr_zone_isolate_freepages(), rework zone_watermark_ok_safe() comment, simplify set_pageblock_isolate() and restore_pageblock_isolate()]
    [akpm@linux-foundation.org: fix CONFIG_MEMORY_ISOLATION=n build]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Suggested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Tested-by: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 98f079bcf399..64b2c3a48286 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -478,6 +478,14 @@ struct zone {
 	 * rarely used fields:
 	 */
 	const char		*name;
+#ifdef CONFIG_MEMORY_ISOLATION
+	/*
+	 * the number of MIGRATE_ISOLATE *pageblock*.
+	 * We need this for free page counting. Look at zone_watermark_ok_safe.
+	 * It's protected by zone->lock
+	 */
+	int		nr_pageblock_isolate;
+#endif
 } ____cacheline_internodealigned_in_smp;
 
 typedef enum {

commit 9adb62a5df9c0fbef7b4665919329f73a34651ed
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Tue Jul 31 16:43:28 2012 -0700

    mm/hotplug: correctly setup fallback zonelists when creating new pgdat
    
    When hotadd_new_pgdat() is called to create new pgdat for a new node, a
    fallback zonelist should be created for the new node.  There's code to try
    to achieve that in hotadd_new_pgdat() as below:
    
            /*
             * The node we allocated has no zone fallback lists. For avoiding
             * to access not-initialized zonelist, build here.
             */
            mutex_lock(&zonelists_mutex);
            build_all_zonelists(pgdat, NULL);
            mutex_unlock(&zonelists_mutex);
    
    But it doesn't work as expected.  When hotadd_new_pgdat() is called, the
    new node is still in offline state because node_set_online(nid) hasn't
    been called yet.  And build_all_zonelists() only builds zonelists for
    online nodes as:
    
            for_each_online_node(nid) {
                    pg_data_t *pgdat = NODE_DATA(nid);
    
                    build_zonelists(pgdat);
                    build_zonelist_cache(pgdat);
            }
    
    Though we hope to create zonelist for the new pgdat, but it doesn't.  So
    add a new parameter "pgdat" the build_all_zonelists() to build pgdat for
    the new pgdat too.
    
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Keping Chen <chenkeping@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f64afa5929fe..98f079bcf399 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -721,7 +721,7 @@ typedef struct pglist_data {
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;
-void build_all_zonelists(void *data);
+void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);

commit fe03025db3f4ade1f231b174938e0fe224722759
Author: Rabin Vincent <rabin@rab.in>
Date:   Tue Jul 31 16:43:14 2012 -0700

    mm: CONFIG_HAVE_MEMBLOCK_NODE -> CONFIG_HAVE_MEMBLOCK_NODE_MAP
    
    0ee332c14518699 ("memblock: Kill early_node_map[]") wanted to replace
    CONFIG_ARCH_POPULATES_NODE_MAP with CONFIG_HAVE_MEMBLOCK_NODE_MAP but
    ended up replacing one occurence with a reference to the non-existent
    symbol CONFIG_HAVE_MEMBLOCK_NODE.
    
    The resulting omission of code would probably have been causing problems
    to 32-bit machines with memory hotplug.
    
    Signed-off-by: Rabin Vincent <rabin@rab.in>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1aeadce4d56e..f64afa5929fe 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -776,7 +776,7 @@ extern int movable_zone;
 
 static inline int zone_movable_is_highmem(void)
 {
-#if defined(CONFIG_HIGHMEM) && defined(CONFIG_HAVE_MEMBLOCK_NODE)
+#if defined(CONFIG_HIGHMEM) && defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
 	return movable_zone == ZONE_HIGHMEM;
 #else
 	return 0;

commit 7db8889ab05b57200158432755af318fb68854a2
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Jul 31 16:43:12 2012 -0700

    mm: have order > 0 compaction start off where it left
    
    Order > 0 compaction stops when enough free pages of the correct page
    order have been coalesced.  When doing subsequent higher order
    allocations, it is possible for compaction to be invoked many times.
    
    However, the compaction code always starts out looking for things to
    compact at the start of the zone, and for free pages to compact things to
    at the end of the zone.
    
    This can cause quadratic behaviour, with isolate_freepages starting at the
    end of the zone each time, even though previous invocations of the
    compaction code already filled up all free memory on that end of the zone.
    
    This can cause isolate_freepages to take enormous amounts of CPU with
    certain workloads on larger memory systems.
    
    The obvious solution is to have isolate_freepages remember where it left
    off last time, and continue at that point the next time it gets invoked
    for an order > 0 compaction.  This could cause compaction to fail if
    cc->free_pfn and cc->migrate_pfn are close together initially, in that
    case we restart from the end of the zone and try once more.
    
    Forced full (order == -1) compactions are left alone.
    
    [akpm@linux-foundation.org: checkpatch fixes]
    [akpm@linux-foundation.org: s/laste/last/, use 80 cols]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reported-by: Jim Schutt <jaschut@sandia.gov>
    Tested-by: Jim Schutt <jaschut@sandia.gov>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1495d952e641..1aeadce4d56e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -368,6 +368,10 @@ struct zone {
 	 */
 	spinlock_t		lock;
 	int                     all_unreclaimable; /* All pages pinned */
+#if defined CONFIG_COMPACTION || defined CONFIG_CMA
+	/* pfn where the last incremental compaction isolated free pages */
+	unsigned long		compact_cached_free_pfn;
+#endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;

commit ca28ddc908fcfef0e5c1b6e5df632db7fc26de10
Author: Wanpeng Li <liwp@linux.vnet.ibm.com>
Date:   Tue Jul 31 16:43:04 2012 -0700

    mm: remove unused LRU_ALL_EVICTABLE
    
    Signed-off-by: Wanpeng Li <liwp.linux@gmail.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3bdfa15b2012..1495d952e641 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -209,7 +209,6 @@ struct lruvec {
 /* Mask used at gathering information at once (see memcontrol.c) */
 #define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
 #define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
-#define LRU_ALL_EVICTABLE (LRU_ALL_FILE | LRU_ALL_ANON)
 #define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
 
 /* Isolate clean file */

commit c255a458055e459f65eb7b7f51dc5dbdd0caf1d8
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Jul 31 16:43:02 2012 -0700

    memcg: rename config variables
    
    Sanity:
    
    CONFIG_CGROUP_MEM_RES_CTLR -> CONFIG_MEMCG
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP -> CONFIG_MEMCG_SWAP
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP_ENABLED -> CONFIG_MEMCG_SWAP_ENABLED
    CONFIG_CGROUP_MEM_RES_CTLR_KMEM -> CONFIG_MEMCG_KMEM
    
    [mhocko@suse.cz: fix missed bits]
    Cc: Glauber Costa <glommer@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 458988bd55a1..3bdfa15b2012 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -201,7 +201,7 @@ struct zone_reclaim_stat {
 struct lruvec {
 	struct list_head lists[NR_LRU_LISTS];
 	struct zone_reclaim_stat reclaim_stat;
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+#ifdef CONFIG_MEMCG
 	struct zone *zone;
 #endif
 };
@@ -671,7 +671,7 @@ typedef struct pglist_data {
 	int nr_zones;
 #ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
 	struct page *node_mem_map;
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+#ifdef CONFIG_MEMCG
 	struct page_cgroup *node_page_cgroup;
 #endif
 #endif
@@ -736,7 +736,7 @@ extern void lruvec_init(struct lruvec *lruvec, struct zone *zone);
 
 static inline struct zone *lruvec_zone(struct lruvec *lruvec)
 {
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+#ifdef CONFIG_MEMCG
 	return lruvec->zone;
 #else
 	return container_of(lruvec, struct zone, lruvec);
@@ -1052,7 +1052,7 @@ struct mem_section {
 
 	/* See declaration of similar field in struct zone */
 	unsigned long *pageblock_flags;
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+#ifdef CONFIG_MEMCG
 	/*
 	 * If !SPARSEMEM, pgdat doesn't have page_cgroup pointer. We use
 	 * section. (see memcontrol.h/page_cgroup.h about this.)

commit d14b7a419a664cd7c1c585c9e7fffee9e9051d53
Merge: e8ff13b0bf88 a58b3a4aba2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 13:34:56 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree from Jiri Kosina:
     "Trivial updates all over the place as usual."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (29 commits)
      Fix typo in include/linux/clk.h .
      pci: hotplug: Fix typo in pci
      iommu: Fix typo in iommu
      video: Fix typo in drivers/video
      Documentation: Add newline at end-of-file to files lacking one
      arm,unicore32: Remove obsolete "select MISC_DEVICES"
      module.c: spelling s/postition/position/g
      cpufreq: Fix typo in cpufreq driver
      trivial: typo in comment in mksysmap
      mach-omap2: Fix typo in debug message and comment
      scsi: aha152x: Fix sparse warning and make printing pointer address more portable.
      Change email address for Steve Glendinning
      Btrfs: fix typo in convert_extent_bit
      via: Remove bogus if check
      netprio_cgroup.c: fix comment typo
      backlight: fix memory leak on obscure error path
      Documentation: asus-laptop.txt references an obsolete Kconfig item
      Documentation: ManagementStyle: fixed typo
      mm/vmscan: cleanup comment error in balance_pgdat
      mm: cleanup on the comments of zone_reclaim_stat
      ...

commit d8adde17e5f858427504725218c56aef90e90fc7
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Wed Jul 11 14:01:52 2012 -0700

    memory hotplug: fix invalid memory access caused by stale kswapd pointer
    
    kswapd_stop() is called to destroy the kswapd work thread when all memory
    of a NUMA node has been offlined.  But kswapd_stop() only terminates the
    work thread without resetting NODE_DATA(nid)->kswapd to NULL.  The stale
    pointer will prevent kswapd_run() from creating a new work thread when
    adding memory to the memory-less NUMA node again.  Eventually the stale
    pointer may cause invalid memory access.
    
    An example stack dump as below. It's reproduced with 2.6.32, but latest
    kernel has the same issue.
    
      BUG: unable to handle kernel NULL pointer dereference at (null)
      IP: [<ffffffff81051a94>] exit_creds+0x12/0x78
      PGD 0
      Oops: 0000 [#1] SMP
      last sysfs file: /sys/devices/system/memory/memory391/state
      CPU 11
      Modules linked in: cpufreq_conservative cpufreq_userspace cpufreq_powersave acpi_cpufreq microcode fuse loop dm_mod tpm_tis rtc_cmos i2c_i801 rtc_core tpm serio_raw pcspkr sg tpm_bios igb i2c_core iTCO_wdt rtc_lib mptctl iTCO_vendor_support button dca bnx2 usbhid hid uhci_hcd ehci_hcd usbcore sd_mod crc_t10dif edd ext3 mbcache jbd fan ide_pci_generic ide_core ata_generic ata_piix libata thermal processor thermal_sys hwmon mptsas mptscsih mptbase scsi_transport_sas scsi_mod
      Pid: 7949, comm: sh Not tainted 2.6.32.12-qiuxishi-5-default #92 Tecal RH2285
      RIP: 0010:exit_creds+0x12/0x78
      RSP: 0018:ffff8806044f1d78  EFLAGS: 00010202
      RAX: 0000000000000000 RBX: ffff880604f22140 RCX: 0000000000019502
      RDX: 0000000000000000 RSI: 0000000000000202 RDI: 0000000000000000
      RBP: ffff880604f22150 R08: 0000000000000000 R09: ffffffff81a4dc10
      R10: 00000000000032a0 R11: ffff880006202500 R12: 0000000000000000
      R13: 0000000000c40000 R14: 0000000000008000 R15: 0000000000000001
      FS:  00007fbc03d066f0(0000) GS:ffff8800282e0000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
      CR2: 0000000000000000 CR3: 000000060f029000 CR4: 00000000000006e0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
      Process sh (pid: 7949, threadinfo ffff8806044f0000, task ffff880603d7c600)
      Stack:
       ffff880604f22140 ffffffff8103aac5 ffff880604f22140 ffffffff8104d21e
       ffff880006202500 0000000000008000 0000000000c38000 ffffffff810bd5b1
       0000000000000000 ffff880603d7c600 00000000ffffdd29 0000000000000003
      Call Trace:
        __put_task_struct+0x5d/0x97
        kthread_stop+0x50/0x58
        offline_pages+0x324/0x3da
        memory_block_change_state+0x179/0x1db
        store_mem_state+0x9e/0xbb
        sysfs_write_file+0xd0/0x107
        vfs_write+0xad/0x169
        sys_write+0x45/0x6e
        system_call_fastpath+0x16/0x1b
      Code: ff 4d 00 0f 94 c0 84 c0 74 08 48 89 ef e8 1f fd ff ff 5b 5d 31 c0 41 5c c3 53 48 8b 87 20 06 00 00 48 89 fb 48 8b bf 18 06 00 00 <8b> 00 48 c7 83 18 06 00 00 00 00 00 00 f0 ff 0f 0f 94 c0 84 c0
      RIP  exit_creds+0x12/0x78
       RSP <ffff8806044f1d78>
      CR2: 0000000000000000
    
    [akpm@linux-foundation.org: add pglist_data.kswapd locking comments]
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2427706f78b4..68c569fcbb66 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -694,7 +694,7 @@ typedef struct pglist_data {
 					     range, including holes */
 	int node_id;
 	wait_queue_head_t kswapd_wait;
-	struct task_struct *kswapd;
+	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */
 	int kswapd_max_order;
 	enum zone_type classzone_idx;
 } pg_data_t;

commit 59f91e5dd0504dc0ebfaa0b6f3a55e6931f96266
Merge: 57bdfdd80077 89abfab133ef
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Fri Jun 29 14:45:58 2012 +0200

    Merge branch 'master' into for-next
    
    Conflicts:
            include/linux/mmzone.h
    
    Synced with Linus' tree so that trivial patch can be applied
    on top of up-to-date code properly.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>

commit 46028e6d10cbf9ccd5fb49aa0c23a430f314144c
Author: Wanpeng Li <liwp@linux.vnet.ibm.com>
Date:   Fri Jun 15 16:52:29 2012 +0800

    mm: cleanup on the comments of zone_reclaim_stat
    
    Signed-off-by: Wanpeng Li <liwp.linux@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4871e31ae277..54631776dff2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -316,7 +316,7 @@ enum zone_type {
 struct zone_reclaim_stat {
 	/*
 	 * The pageout code in vmscan.c keeps track of how many of the
-	 * mem/swap backed and file backed pages are refeferenced.
+	 * mem/swap backed and file backed pages are referenced.
 	 * The higher the rotated/scanned ratio, the more valuable
 	 * that cache is.
 	 *

commit 7f5e86c2ccc1480946d2c869d7f7d5278e828092
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:58 2012 -0700

    mm: add link from struct lruvec to struct zone
    
    This is the first stage of struct mem_cgroup_zone removal.  Further
    patches replace struct mem_cgroup_zone with a pointer to struct lruvec.
    
    If CONFIG_CGROUP_MEM_RES_CTLR=n lruvec_zone() is just container_of().
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5c4880bc027a..2427706f78b4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -201,6 +201,9 @@ struct zone_reclaim_stat {
 struct lruvec {
 	struct list_head lists[NR_LRU_LISTS];
 	struct zone_reclaim_stat reclaim_stat;
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+	struct zone *zone;
+#endif
 };
 
 /* Mask used at gathering information at once (see memcontrol.c) */
@@ -729,6 +732,17 @@ extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
 				     unsigned long size,
 				     enum memmap_context context);
 
+extern void lruvec_init(struct lruvec *lruvec, struct zone *zone);
+
+static inline struct zone *lruvec_zone(struct lruvec *lruvec)
+{
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+	return lruvec->zone;
+#else
+	return container_of(lruvec, struct zone, lruvec);
+#endif
+}
+
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);
 #else

commit 89abfab133ef1f5902abafb744df72793213ac19
Author: Hugh Dickins <hughd@google.com>
Date:   Tue May 29 15:06:53 2012 -0700

    mm/memcg: move reclaim_stat into lruvec
    
    With mem_cgroup_disabled() now explicit, it becomes clear that the
    zone_reclaim_stat structure actually belongs in lruvec, per-zone when
    memcg is disabled but per-memcg per-zone when it's enabled.
    
    We can delete mem_cgroup_get_reclaim_stat(), and change
    update_page_reclaim_stat() to update just the one set of stats, the one
    which get_scan_count() will actually use.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4871e31ae277..1b89861eedc0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -185,8 +185,22 @@ static inline int is_unevictable_lru(enum lru_list lru)
 	return (lru == LRU_UNEVICTABLE);
 }
 
+struct zone_reclaim_stat {
+	/*
+	 * The pageout code in vmscan.c keeps track of how many of the
+	 * mem/swap backed and file backed pages are refeferenced.
+	 * The higher the rotated/scanned ratio, the more valuable
+	 * that cache is.
+	 *
+	 * The anon LRU stats live in [0], file LRU stats in [1]
+	 */
+	unsigned long		recent_rotated[2];
+	unsigned long		recent_scanned[2];
+};
+
 struct lruvec {
 	struct list_head lists[NR_LRU_LISTS];
+	struct zone_reclaim_stat reclaim_stat;
 };
 
 /* Mask used at gathering information at once (see memcontrol.c) */
@@ -313,19 +327,6 @@ enum zone_type {
 #error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
 
-struct zone_reclaim_stat {
-	/*
-	 * The pageout code in vmscan.c keeps track of how many of the
-	 * mem/swap backed and file backed pages are refeferenced.
-	 * The higher the rotated/scanned ratio, the more valuable
-	 * that cache is.
-	 *
-	 * The anon LRU stats live in [0], file LRU stats in [1]
-	 */
-	unsigned long		recent_rotated[2];
-	unsigned long		recent_scanned[2];
-};
-
 struct zone {
 	/* Fields commonly accessed by the page allocator */
 
@@ -407,8 +408,6 @@ struct zone {
 	spinlock_t		lru_lock;
 	struct lruvec		lruvec;
 
-	struct zone_reclaim_stat reclaim_stat;
-
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */
 

commit f3fd4a61928a5edf5b033a417e761b488b43e203
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Tue May 29 15:06:54 2012 -0700

    mm: remove lru type checks from __isolate_lru_page()
    
    After patch "mm: forbid lumpy-reclaim in shrink_active_list()" we can
    completely remove anon/file and active/inactive lru type filters from
    __isolate_lru_page(), because isolation for 0-order reclaim always
    isolates pages from right lru list.  And pages-isolation for lumpy
    shrink_inactive_list() or memory-compaction anyway allowed to isolate
    pages from all evictable lru lists.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1b89861eedc0..5c4880bc027a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -209,16 +209,12 @@ struct lruvec {
 #define LRU_ALL_EVICTABLE (LRU_ALL_FILE | LRU_ALL_ANON)
 #define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
 
-/* Isolate inactive pages */
-#define ISOLATE_INACTIVE	((__force isolate_mode_t)0x1)
-/* Isolate active pages */
-#define ISOLATE_ACTIVE		((__force isolate_mode_t)0x2)
 /* Isolate clean file */
-#define ISOLATE_CLEAN		((__force isolate_mode_t)0x4)
+#define ISOLATE_CLEAN		((__force isolate_mode_t)0x1)
 /* Isolate unmapped file */
-#define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x8)
+#define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x2)
 /* Isolate for asynchronous migration */
-#define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x10)
+#define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x4)
 
 /* LRU Isolation modes. */
 typedef unsigned __bitwise__ isolate_mode_t;

commit d484864dd96e1830e7689510597707c1df8cd681
Merge: be87cfb47c5c 0f51596bd39a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 25 09:18:59 2012 -0700

    Merge branch 'for-linus' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping
    
    Pull CMA and ARM DMA-mapping updates from Marek Szyprowski:
     "These patches contain two major updates for DMA mapping subsystem
      (mainly for ARM architecture).  First one is Contiguous Memory
      Allocator (CMA) which makes it possible for device drivers to allocate
      big contiguous chunks of memory after the system has booted.
    
      The main difference from the similar frameworks is the fact that CMA
      allows to transparently reuse the memory region reserved for the big
      chunk allocation as a system memory, so no memory is wasted when no
      big chunk is allocated.  Once the alloc request is issued, the
      framework migrates system pages to create space for the required big
      chunk of physically contiguous memory.
    
      For more information one can refer to nice LWN articles:
    
       - 'A reworked contiguous memory allocator':
                    http://lwn.net/Articles/447405/
    
       - 'CMA and ARM':
                    http://lwn.net/Articles/450286/
    
       - 'A deep dive into CMA':
                    http://lwn.net/Articles/486301/
    
       - and the following thread with the patches and links to all previous
         versions:
                    https://lkml.org/lkml/2012/4/3/204
    
      The main client for this new framework is ARM DMA-mapping subsystem.
    
      The second part provides a complete redesign in ARM DMA-mapping
      subsystem.  The core implementation has been changed to use common
      struct dma_map_ops based infrastructure with the recent updates for
      new dma attributes merged in v3.4-rc2.  This allows to use more than
      one implementation of dma-mapping calls and change/select them on the
      struct device basis.  The first client of this new infractructure is
      dmabounce implementation which has been completely cut out of the
      core, common code.
    
      The last patch of this redesign update introduces a new, experimental
      implementation of dma-mapping calls on top of generic IOMMU framework.
      This lets ARM sub-platform to transparently use IOMMU for DMA-mapping
      calls if one provides required IOMMU hardware.
    
      For more information please refer to the following thread:
                    http://www.spinics.net/lists/arm-kernel/msg175729.html
    
      The last patch merges changes from both updates and provides a
      resolution for the conflicts which cannot be avoided when patches have
      been applied on the same files (mainly arch/arm/mm/dma-mapping.c)."
    
    Acked by Andrew Morton <akpm@linux-foundation.org>:
     "Yup, this one please.  It's had much work, plenty of review and I
      think even Russell is happy with it."
    
    * 'for-linus' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping: (28 commits)
      ARM: dma-mapping: use PMD size for section unmap
      cma: fix migration mode
      ARM: integrate CMA with DMA-mapping subsystem
      X86: integrate CMA with DMA-mapping subsystem
      drivers: add Contiguous Memory Allocator
      mm: trigger page reclaim in alloc_contig_range() to stabilise watermarks
      mm: extract reclaim code from __alloc_pages_direct_reclaim()
      mm: Serialize access to min_free_kbytes
      mm: page_isolation: MIGRATE_CMA isolation functions added
      mm: mmzone: MIGRATE_CMA migration type added
      mm: page_alloc: change fallbacks array handling
      mm: page_alloc: introduce alloc_contig_range()
      mm: compaction: export some of the functions
      mm: compaction: introduce isolate_freepages_range()
      mm: compaction: introduce map_pages()
      mm: compaction: introduce isolate_migratepages_range()
      mm: page_alloc: remove trailing whitespace
      ARM: dma-mapping: add support for IOMMU mapper
      ARM: dma-mapping: use alloc, mmap, free from dma_ops
      ARM: dma-mapping: remove redundant code and do the cleanup
      ...
    
    Conflicts:
            arch/x86/include/asm/dma-mapping.h

commit 49f223a9cd96c7293d7258ff88c2bdf83065f69c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jan 25 12:49:24 2012 +0100

    mm: trigger page reclaim in alloc_contig_range() to stabilise watermarks
    
    alloc_contig_range() performs memory allocation so it also should keep
    track on keeping the correct level of memory watermarks. This commit adds
    a call to *_slowpath style reclaim to grab enough pages to make sure that
    the final collection of contiguous pages from freelists will not starve
    the system.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    CC: Michal Nazarewicz <mina86@mina86.com>
    Tested-by: Rob Clark <rob.clark@linaro.org>
    Tested-by: Ohad Ben-Cohen <ohad@wizery.com>
    Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org>
    Tested-by: Robert Nelson <robertcnelson@gmail.com>
    Tested-by: Barry Song <Baohua.Song@csr.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8c1335f3c3a3..26f2040b8b04 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -63,8 +63,10 @@ enum {
 
 #ifdef CONFIG_CMA
 #  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
+#  define cma_wmark_pages(zone)	zone->min_cma_pages
 #else
 #  define is_migrate_cma(migratetype) false
+#  define cma_wmark_pages(zone) 0
 #endif
 
 #define for_each_migratetype_order(order, type) \
@@ -370,6 +372,13 @@ struct zone {
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;
+#endif
+#ifdef CONFIG_CMA
+	/*
+	 * CMA needs to increase watermark levels during the allocation
+	 * process to make sure that the system is not starved.
+	 */
+	unsigned long		min_cma_pages;
 #endif
 	struct free_area	free_area[MAX_ORDER];
 

commit 47118af076f64844b4f423bc2f545b2da9dab50d
Author: Michal Nazarewicz <mina86@mina86.com>
Date:   Thu Dec 29 13:09:50 2011 +0100

    mm: mmzone: MIGRATE_CMA migration type added
    
    The MIGRATE_CMA migration type has two main characteristics:
    (i) only movable pages can be allocated from MIGRATE_CMA
    pageblocks and (ii) page allocator will never change migration
    type of MIGRATE_CMA pageblocks.
    
    This guarantees (to some degree) that page in a MIGRATE_CMA page
    block can always be migrated somewhere else (unless there's no
    memory left in the system).
    
    It is designed to be used for allocating big chunks (eg. 10MiB)
    of physically contiguous memory.  Once driver requests
    contiguous memory, pages from MIGRATE_CMA pageblocks may be
    migrated away to create a contiguous block.
    
    To minimise number of migrations, MIGRATE_CMA migration type
    is the last type tried when page allocator falls back to other
    migration types when requested.
    
    Signed-off-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Tested-by: Rob Clark <rob.clark@linaro.org>
    Tested-by: Ohad Ben-Cohen <ohad@wizery.com>
    Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org>
    Tested-by: Robert Nelson <robertcnelson@gmail.com>
    Tested-by: Barry Song <Baohua.Song@csr.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dff711509661..8c1335f3c3a3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -35,13 +35,37 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
-#define MIGRATE_UNMOVABLE     0
-#define MIGRATE_RECLAIMABLE   1
-#define MIGRATE_MOVABLE       2
-#define MIGRATE_PCPTYPES      3 /* the number of types on the pcp lists */
-#define MIGRATE_RESERVE       3
-#define MIGRATE_ISOLATE       4 /* can't allocate from here */
-#define MIGRATE_TYPES         5
+enum {
+	MIGRATE_UNMOVABLE,
+	MIGRATE_RECLAIMABLE,
+	MIGRATE_MOVABLE,
+	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
+	MIGRATE_RESERVE = MIGRATE_PCPTYPES,
+#ifdef CONFIG_CMA
+	/*
+	 * MIGRATE_CMA migration type is designed to mimic the way
+	 * ZONE_MOVABLE works.  Only movable pages can be allocated
+	 * from MIGRATE_CMA pageblocks and page allocator never
+	 * implicitly change migration type of MIGRATE_CMA pageblock.
+	 *
+	 * The way to use it is to change migratetype of a range of
+	 * pageblocks to MIGRATE_CMA which can be done by
+	 * __free_pageblock_cma() function.  What is important though
+	 * is that a range of pageblocks must be aligned to
+	 * MAX_ORDER_NR_PAGES should biggest page be bigger then
+	 * a single pageblock.
+	 */
+	MIGRATE_CMA,
+#endif
+	MIGRATE_ISOLATE,	/* can't allocate from here */
+	MIGRATE_TYPES
+};
+
+#ifdef CONFIG_CMA
+#  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)
+#else
+#  define is_migrate_cma(migratetype) false
+#endif
 
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \

commit 35fca53e15a696adbea300a981df4bbfb09a76d6
Author: Wang YanQing <udknight@gmail.com>
Date:   Sun Apr 15 20:42:28 2012 +0800

    mmzone: fix comment typo coelesce -> coalesce
    
    Signed-off-by: Wang YanQing <udknight@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dff711509661..41aa49b74821 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -30,7 +30,7 @@
 /*
  * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
  * costly to service.  That is between allocation orders which should
- * coelesce naturally under reasonable reclaim pressure and those which
+ * coalesce naturally under reasonable reclaim pressure and those which
  * will not.
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3

commit aff622495c9a0b56148192e53bdec539f5e147f2
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Mar 21 16:33:52 2012 -0700

    vmscan: only defer compaction for failed order and higher
    
    Currently a failed order-9 (transparent hugepage) compaction can lead to
    memory compaction being temporarily disabled for a memory zone.  Even if
    we only need compaction for an order 2 allocation, eg.  for jumbo frames
    networking.
    
    The fix is relatively straightforward: keep track of the highest order at
    which compaction is succeeding, and only defer compaction for orders at
    which compaction is failing.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 650ba2fb3301..dff711509661 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -365,6 +365,7 @@ struct zone {
 	 */
 	unsigned int		compact_considered;
 	unsigned int		compact_defer_shift;
+	int			compact_order_failed;
 #endif
 
 	ZONE_PADDING(_pad1_)

commit 4111304dab198c687bc60f2e235a9f7ee92c47c8
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jan 12 17:20:01 2012 -0800

    mm: enum lru_list lru
    
    Mostly we use "enum lru_list lru": change those few "l"s to "lru"s.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2038b90ca6e3..650ba2fb3301 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -140,23 +140,23 @@ enum lru_list {
 	NR_LRU_LISTS
 };
 
-#define for_each_lru(l) for (l = 0; l < NR_LRU_LISTS; l++)
+#define for_each_lru(lru) for (lru = 0; lru < NR_LRU_LISTS; lru++)
 
-#define for_each_evictable_lru(l) for (l = 0; l <= LRU_ACTIVE_FILE; l++)
+#define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)
 
-static inline int is_file_lru(enum lru_list l)
+static inline int is_file_lru(enum lru_list lru)
 {
-	return (l == LRU_INACTIVE_FILE || l == LRU_ACTIVE_FILE);
+	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
 }
 
-static inline int is_active_lru(enum lru_list l)
+static inline int is_active_lru(enum lru_list lru)
 {
-	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE);
+	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
 }
 
-static inline int is_unevictable_lru(enum lru_list l)
+static inline int is_unevictable_lru(enum lru_list lru)
 {
-	return (l == LRU_UNEVICTABLE);
+	return (lru == LRU_UNEVICTABLE);
 }
 
 struct lruvec {

commit c82449352854ff09e43062246af86bdeb628f0c3
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Jan 12 17:19:38 2012 -0800

    mm: compaction: make isolate_lru_page() filter-aware again
    
    Commit 39deaf85 ("mm: compaction: make isolate_lru_page() filter-aware")
    noted that compaction does not migrate dirty or writeback pages and that
    is was meaningless to pick the page and re-add it to the LRU list.  This
    had to be partially reverted because some dirty pages can be migrated by
    compaction without blocking.
    
    This patch updates "mm: compaction: make isolate_lru_page" by skipping
    over pages that migration has no possibility of migrating to minimise LRU
    disruption.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel<riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Andy Isaacson <adi@hexapodia.org>
    Cc: Nai Xia <nai.xia@gmail.com>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 42e544cd4c9f..2038b90ca6e3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -177,6 +177,8 @@ struct lruvec {
 #define ISOLATE_CLEAN		((__force isolate_mode_t)0x4)
 /* Isolate unmapped file */
 #define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x8)
+/* Isolate for asynchronous migration */
+#define ISOLATE_ASYNC_MIGRATE	((__force isolate_mode_t)0x10)
 
 /* LRU Isolation modes. */
 typedef unsigned __bitwise__ isolate_mode_t;

commit 6290df545814990ca2663baf6e894669132d5f73
Author: Johannes Weiner <jweiner@redhat.com>
Date:   Thu Jan 12 17:18:10 2012 -0800

    mm: collect LRU list heads into struct lruvec
    
    Having a unified structure with a LRU list set for both global zones and
    per-memcg zones allows to keep that code simple which deals with LRU
    lists and does not care about the container itself.
    
    Once the per-memcg LRU lists directly link struct pages, the isolation
    function and all other list manipulations are shared between the memcg
    case and the global LRU case.
    
    Signed-off-by: Johannes Weiner <jweiner@redhat.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ca6ca92418a6..42e544cd4c9f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -159,6 +159,10 @@ static inline int is_unevictable_lru(enum lru_list l)
 	return (l == LRU_UNEVICTABLE);
 }
 
+struct lruvec {
+	struct list_head lists[NR_LRU_LISTS];
+};
+
 /* Mask used at gathering information at once (see memcontrol.c) */
 #define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
 #define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
@@ -364,10 +368,8 @@ struct zone {
 	ZONE_PADDING(_pad1_)
 
 	/* Fields commonly accessed by the page reclaim scanner */
-	spinlock_t		lru_lock;	
-	struct zone_lru {
-		struct list_head list;
-	} lru[NR_LRU_LISTS];
+	spinlock_t		lru_lock;
+	struct lruvec		lruvec;
 
 	struct zone_reclaim_stat reclaim_stat;
 

commit ab8fabd46f811d5153d8a0cd2fac9a0d41fb593d
Author: Johannes Weiner <jweiner@redhat.com>
Date:   Tue Jan 10 15:07:42 2012 -0800

    mm: exclude reserved pages from dirtyable memory
    
    Per-zone dirty limits try to distribute page cache pages allocated for
    writing across zones in proportion to the individual zone sizes, to reduce
    the likelihood of reclaim having to write back individual pages from the
    LRU lists in order to make progress.
    
    This patch:
    
    The amount of dirtyable pages should not include the full number of free
    pages: there is a number of reserved pages that the page allocator and
    kswapd always try to keep free.
    
    The closer (reclaimable pages - dirty pages) is to the number of reserved
    pages, the more likely it becomes for reclaim to run into dirty pages:
    
           +----------+ ---
           |   anon   |  |
           +----------+  |
           |          |  |
           |          |  -- dirty limit new    -- flusher new
           |   file   |  |                     |
           |          |  |                     |
           |          |  -- dirty limit old    -- flusher old
           |          |                        |
           +----------+                       --- reclaim
           | reserved |
           +----------+
           |  kernel  |
           +----------+
    
    This patch introduces a per-zone dirty reserve that takes both the lowmem
    reserve as well as the high watermark of the zone into account, and a
    global sum of those per-zone values that is subtracted from the global
    amount of dirtyable pages.  The lowmem reserve is unavailable to page
    cache allocations and kswapd tries to keep the high watermark free.  We
    don't want to end up in a situation where reclaim has to clean pages in
    order to balance zones.
    
    Not treating reserved pages as dirtyable on a global level is only a
    conceptual fix.  In reality, dirty pages are not distributed equally
    across zones and reclaim runs into dirty pages on a regular basis.
    
    But it is important to get this right before tackling the problem on a
    per-zone level, where the distance between reclaim and the dirty pages is
    mostly much smaller in absolute numbers.
    
    [akpm@linux-foundation.org: fix highmem build]
    Signed-off-by: Johannes Weiner <jweiner@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3ac040f19369..ca6ca92418a6 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -317,6 +317,12 @@ struct zone {
 	 */
 	unsigned long		lowmem_reserve[MAX_NR_ZONES];
 
+	/*
+	 * This is a per-zone reserve of pages that should not be
+	 * considered dirtyable memory.
+	 */
+	unsigned long		dirty_balance_reserve;
+
 #ifdef CONFIG_NUMA
 	int node;
 	/*

commit 0ee332c1451869963626bf9cac88f165a90990e1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Dec 8 10:22:09 2011 -0800

    memblock: Kill early_node_map[]
    
    Now all ARCH_POPULATES_NODE_MAP archs select HAVE_MEBLOCK_NODE_MAP -
    there's no user of early_node_map[] left.  Kill early_node_map[] and
    replace ARCH_POPULATES_NODE_MAP with HAVE_MEMBLOCK_NODE_MAP.  Also,
    relocate for_each_mem_pfn_range() and helper from mm.h to memblock.h
    as page_alloc.c would no longer host an alternative implementation.
    
    This change is ultimately one to one mapping and shouldn't cause any
    observable difference; however, after the recent changes, there are
    some functions which now would fit memblock.c better than page_alloc.c
    and dependency on HAVE_MEMBLOCK_NODE_MAP instead of HAVE_MEMBLOCK
    doesn't make much sense on some of them.  Further cleanups for
    functions inside HAVE_MEMBLOCK_NODE_MAP in mm.h would be nice.
    
    -v2: Fix compile bug introduced by mis-spelling
     CONFIG_HAVE_MEMBLOCK_NODE_MAP to CONFIG_MEMBLOCK_HAVE_NODE_MAP in
     mmzone.h.  Reported by Stephen Rothwell.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 188cb2ffe8db..3ac040f19369 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -598,13 +598,13 @@ struct zonelist {
 #endif
 };
 
-#ifdef CONFIG_ARCH_POPULATES_NODE_MAP
+#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 struct node_active_region {
 	unsigned long start_pfn;
 	unsigned long end_pfn;
 	int nid;
 };
-#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
+#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 
 #ifndef CONFIG_DISCONTIGMEM
 /* The array of struct pages - for discontigmem use pgdat->lmem_map */
@@ -720,7 +720,7 @@ extern int movable_zone;
 
 static inline int zone_movable_is_highmem(void)
 {
-#if defined(CONFIG_HIGHMEM) && defined(CONFIG_ARCH_POPULATES_NODE_MAP)
+#if defined(CONFIG_HIGHMEM) && defined(CONFIG_HAVE_MEMBLOCK_NODE)
 	return movable_zone == ZONE_HIGHMEM;
 #else
 	return 0;
@@ -938,7 +938,7 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 #endif
 
 #if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) && \
-	!defined(CONFIG_ARCH_POPULATES_NODE_MAP)
+	!defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
 static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 {
 	return 0;

commit 49ea7eb65e7c5060807fb9312b1ad4c3eab82e2c
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 31 17:07:59 2011 -0700

    mm: vmscan: immediately reclaim end-of-LRU dirty pages when writeback completes
    
    When direct reclaim encounters a dirty page, it gets recycled around the
    LRU for another cycle.  This patch marks the page PageReclaim similar to
    deactivate_page() so that the page gets reclaimed almost immediately after
    the page gets cleaned.  This is to avoid reclaiming clean pages that are
    younger than a dirty page encountered at the end of the LRU that might
    have been something like a use-once page.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Johannes Weiner <jweiner@redhat.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Alex Elder <aelder@sgi.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2c41b2c1943b..188cb2ffe8db 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -100,7 +100,7 @@ enum zone_stat_item {
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
-	NR_VMSCAN_WRITE_SKIP,
+	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */

commit ee72886d8ed5d9de3fa0ed3b99a7ca7702576a96
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Oct 31 17:07:38 2011 -0700

    mm: vmscan: do not writeback filesystem pages in direct reclaim
    
    Testing from the XFS folk revealed that there is still too much I/O from
    the end of the LRU in kswapd.  Previously it was considered acceptable by
    VM people for a small number of pages to be written back from reclaim with
    testing generally showing about 0.3% of pages reclaimed were written back
    (higher if memory was low).  That writing back a small number of pages is
    ok has been heavily disputed for quite some time and Dave Chinner
    explained it well;
    
            It doesn't have to be a very high number to be a problem. IO
            is orders of magnitude slower than the CPU time it takes to
            flush a page, so the cost of making a bad flush decision is
            very high. And single page writeback from the LRU is almost
            always a bad flush decision.
    
    To complicate matters, filesystems respond very differently to requests
    from reclaim according to Christoph Hellwig;
    
            xfs tries to write it back if the requester is kswapd
            ext4 ignores the request if it's a delayed allocation
            btrfs ignores the request
    
    As a result, each filesystem has different performance characteristics
    when under memory pressure and there are many pages being dirtied.  In
    some cases, the request is ignored entirely so the VM cannot depend on the
    IO being dispatched.
    
    The objective of this series is to reduce writing of filesystem-backed
    pages from reclaim, play nicely with writeback that is already in progress
    and throttle reclaim appropriately when writeback pages are encountered.
    The assumption is that the flushers will always write pages faster than if
    reclaim issues the IO.
    
    A secondary goal is to avoid the problem whereby direct reclaim splices
    two potentially deep call stacks together.
    
    There is a potential new problem as reclaim has less control over how long
    before a page in a particularly zone or container is cleaned and direct
    reclaimers depend on kswapd or flusher threads to do the necessary work.
    However, as filesystems sometimes ignore direct reclaim requests already,
    it is not expected to be a serious issue.
    
    Patch 1 disables writeback of filesystem pages from direct reclaim
            entirely. Anonymous pages are still written.
    
    Patch 2 removes dead code in lumpy reclaim as it is no longer able
            to synchronously write pages. This hurts lumpy reclaim but
            there is an expectation that compaction is used for hugepage
            allocations these days and lumpy reclaim's days are numbered.
    
    Patches 3-4 add warnings to XFS and ext4 if called from
            direct reclaim. With patch 1, this "never happens" and is
            intended to catch regressions in this logic in the future.
    
    Patch 5 disables writeback of filesystem pages from kswapd unless
            the priority is raised to the point where kswapd is considered
            to be in trouble.
    
    Patch 6 throttles reclaimers if too many dirty pages are being
            encountered and the zones or backing devices are congested.
    
    Patch 7 invalidates dirty pages found at the end of the LRU so they
            are reclaimed quickly after being written back rather than
            waiting for a reclaimer to find them
    
    I consider this series to be orthogonal to the writeback work but it is
    worth noting that the writeback work affects the viability of patch 8 in
    particular.
    
    I tested this on ext4 and xfs using fs_mark, a simple writeback test based
    on dd and a micro benchmark that does a streaming write to a large mapping
    (exercises use-once LRU logic) followed by streaming writes to a mix of
    anonymous and file-backed mappings.  The command line for fs_mark when
    botted with 512M looked something like
    
    ./fs_mark -d  /tmp/fsmark-2676  -D  100  -N  150  -n  150  -L  25  -t  1  -S0  -s  10485760
    
    The number of files was adjusted depending on the amount of available
    memory so that the files created was about 3xRAM.  For multiple threads,
    the -d switch is specified multiple times.
    
    The test machine is x86-64 with an older generation of AMD processor with
    4 cores.  The underlying storage was 4 disks configured as RAID-0 as this
    was the best configuration of storage I had available.  Swap is on a
    separate disk.  Dirty ratio was tuned to 40% instead of the default of
    20%.
    
    Testing was run with and without monitors to both verify that the patches
    were operating as expected and that any performance gain was real and not
    due to interference from monitors.
    
    Here is a summary of results based on testing XFS.
    
    512M1P-xfs           Files/s  mean                 32.69 ( 0.00%)     34.44 ( 5.08%)
    512M1P-xfs           Elapsed Time fsmark                    51.41     48.29
    512M1P-xfs           Elapsed Time simple-wb                114.09    108.61
    512M1P-xfs           Elapsed Time mmap-strm                113.46    109.34
    512M1P-xfs           Kswapd efficiency fsmark                 62%       63%
    512M1P-xfs           Kswapd efficiency simple-wb              56%       61%
    512M1P-xfs           Kswapd efficiency mmap-strm              44%       42%
    512M-xfs             Files/s  mean                 30.78 ( 0.00%)     35.94 (14.36%)
    512M-xfs             Elapsed Time fsmark                    56.08     48.90
    512M-xfs             Elapsed Time simple-wb                112.22     98.13
    512M-xfs             Elapsed Time mmap-strm                219.15    196.67
    512M-xfs             Kswapd efficiency fsmark                 54%       56%
    512M-xfs             Kswapd efficiency simple-wb              54%       55%
    512M-xfs             Kswapd efficiency mmap-strm              45%       44%
    512M-4X-xfs          Files/s  mean                 30.31 ( 0.00%)     33.33 ( 9.06%)
    512M-4X-xfs          Elapsed Time fsmark                    63.26     55.88
    512M-4X-xfs          Elapsed Time simple-wb                100.90     90.25
    512M-4X-xfs          Elapsed Time mmap-strm                261.73    255.38
    512M-4X-xfs          Kswapd efficiency fsmark                 49%       50%
    512M-4X-xfs          Kswapd efficiency simple-wb              54%       56%
    512M-4X-xfs          Kswapd efficiency mmap-strm              37%       36%
    512M-16X-xfs         Files/s  mean                 60.89 ( 0.00%)     65.22 ( 6.64%)
    512M-16X-xfs         Elapsed Time fsmark                    67.47     58.25
    512M-16X-xfs         Elapsed Time simple-wb                103.22     90.89
    512M-16X-xfs         Elapsed Time mmap-strm                237.09    198.82
    512M-16X-xfs         Kswapd efficiency fsmark                 45%       46%
    512M-16X-xfs         Kswapd efficiency simple-wb              53%       55%
    512M-16X-xfs         Kswapd efficiency mmap-strm              33%       33%
    
    Up until 512-4X, the FSmark improvements were statistically significant.
    For the 4X and 16X tests the results were within standard deviations but
    just barely.  The time to completion for all tests is improved which is an
    important result.  In general, kswapd efficiency is not affected by
    skipping dirty pages.
    
    1024M1P-xfs          Files/s  mean                 39.09 ( 0.00%)     41.15 ( 5.01%)
    1024M1P-xfs          Elapsed Time fsmark                    84.14     80.41
    1024M1P-xfs          Elapsed Time simple-wb                210.77    184.78
    1024M1P-xfs          Elapsed Time mmap-strm                162.00    160.34
    1024M1P-xfs          Kswapd efficiency fsmark                 69%       75%
    1024M1P-xfs          Kswapd efficiency simple-wb              71%       77%
    1024M1P-xfs          Kswapd efficiency mmap-strm              43%       44%
    1024M-xfs            Files/s  mean                 35.45 ( 0.00%)     37.00 ( 4.19%)
    1024M-xfs            Elapsed Time fsmark                    94.59     91.00
    1024M-xfs            Elapsed Time simple-wb                229.84    195.08
    1024M-xfs            Elapsed Time mmap-strm                405.38    440.29
    1024M-xfs            Kswapd efficiency fsmark                 79%       71%
    1024M-xfs            Kswapd efficiency simple-wb              74%       74%
    1024M-xfs            Kswapd efficiency mmap-strm              39%       42%
    1024M-4X-xfs         Files/s  mean                 32.63 ( 0.00%)     35.05 ( 6.90%)
    1024M-4X-xfs         Elapsed Time fsmark                   103.33     97.74
    1024M-4X-xfs         Elapsed Time simple-wb                204.48    178.57
    1024M-4X-xfs         Elapsed Time mmap-strm                528.38    511.88
    1024M-4X-xfs         Kswapd efficiency fsmark                 81%       70%
    1024M-4X-xfs         Kswapd efficiency simple-wb              73%       72%
    1024M-4X-xfs         Kswapd efficiency mmap-strm              39%       38%
    1024M-16X-xfs        Files/s  mean                 42.65 ( 0.00%)     42.97 ( 0.74%)
    1024M-16X-xfs        Elapsed Time fsmark                   103.11     99.11
    1024M-16X-xfs        Elapsed Time simple-wb                200.83    178.24
    1024M-16X-xfs        Elapsed Time mmap-strm                397.35    459.82
    1024M-16X-xfs        Kswapd efficiency fsmark                 84%       69%
    1024M-16X-xfs        Kswapd efficiency simple-wb              74%       73%
    1024M-16X-xfs        Kswapd efficiency mmap-strm              39%       40%
    
    All FSMark tests up to 16X had statistically significant improvements.
    For the most part, tests are completing faster with the exception of the
    streaming writes to a mixture of anonymous and file-backed mappings which
    were slower in two cases
    
    In the cases where the mmap-strm tests were slower, there was more
    swapping due to dirty pages being skipped.  The number of additional pages
    swapped is almost identical to the fewer number of pages written from
    reclaim.  In other words, roughly the same number of pages were reclaimed
    but swapping was slower.  As the test is a bit unrealistic and stresses
    memory heavily, the small shift is acceptable.
    
    4608M1P-xfs          Files/s  mean                 29.75 ( 0.00%)     30.96 ( 3.91%)
    4608M1P-xfs          Elapsed Time fsmark                   512.01    492.15
    4608M1P-xfs          Elapsed Time simple-wb                618.18    566.24
    4608M1P-xfs          Elapsed Time mmap-strm                488.05    465.07
    4608M1P-xfs          Kswapd efficiency fsmark                 93%       86%
    4608M1P-xfs          Kswapd efficiency simple-wb              88%       84%
    4608M1P-xfs          Kswapd efficiency mmap-strm              46%       45%
    4608M-xfs            Files/s  mean                 27.60 ( 0.00%)     28.85 ( 4.33%)
    4608M-xfs            Elapsed Time fsmark                   555.96    532.34
    4608M-xfs            Elapsed Time simple-wb                659.72    571.85
    4608M-xfs            Elapsed Time mmap-strm               1082.57   1146.38
    4608M-xfs            Kswapd efficiency fsmark                 89%       91%
    4608M-xfs            Kswapd efficiency simple-wb              88%       82%
    4608M-xfs            Kswapd efficiency mmap-strm              48%       46%
    4608M-4X-xfs         Files/s  mean                 26.00 ( 0.00%)     27.47 ( 5.35%)
    4608M-4X-xfs         Elapsed Time fsmark                   592.91    564.00
    4608M-4X-xfs         Elapsed Time simple-wb                616.65    575.07
    4608M-4X-xfs         Elapsed Time mmap-strm               1773.02   1631.53
    4608M-4X-xfs         Kswapd efficiency fsmark                 90%       94%
    4608M-4X-xfs         Kswapd efficiency simple-wb              87%       82%
    4608M-4X-xfs         Kswapd efficiency mmap-strm              43%       43%
    4608M-16X-xfs        Files/s  mean                 26.07 ( 0.00%)     26.42 ( 1.32%)
    4608M-16X-xfs        Elapsed Time fsmark                   602.69    585.78
    4608M-16X-xfs        Elapsed Time simple-wb                606.60    573.81
    4608M-16X-xfs        Elapsed Time mmap-strm               1549.75   1441.86
    4608M-16X-xfs        Kswapd efficiency fsmark                 98%       98%
    4608M-16X-xfs        Kswapd efficiency simple-wb              88%       82%
    4608M-16X-xfs        Kswapd efficiency mmap-strm              44%       42%
    
    Unlike the other tests, the fsmark results are not statistically
    significant but the min and max times are both improved and for the most
    part, tests completed faster.
    
    There are other indications that this is an improvement as well.  For
    example, in the vast majority of cases, there were fewer pages scanned by
    direct reclaim implying in many cases that stalls due to direct reclaim
    are reduced.  KSwapd is scanning more due to skipping dirty pages which is
    unfortunate but the CPU usage is still acceptable
    
    In an earlier set of tests, I used blktrace and in almost all cases
    throughput throughout the entire test was higher.  However, I ended up
    discarding those results as recording blktrace data was too heavy for my
    liking.
    
    On a laptop, I plugged in a USB stick and ran a similar tests of tests
    using it as backing storage.  A desktop environment was running and for
    the entire duration of the tests, firefox and gnome terminal were
    launching and exiting to vaguely simulate a user.
    
    1024M-xfs            Files/s  mean               0.41 ( 0.00%)        0.44 ( 6.82%)
    1024M-xfs            Elapsed Time fsmark               2053.52   1641.03
    1024M-xfs            Elapsed Time simple-wb            1229.53    768.05
    1024M-xfs            Elapsed Time mmap-strm            4126.44   4597.03
    1024M-xfs            Kswapd efficiency fsmark              84%       85%
    1024M-xfs            Kswapd efficiency simple-wb           92%       81%
    1024M-xfs            Kswapd efficiency mmap-strm           60%       51%
    1024M-xfs            Avg wait ms fsmark                5404.53     4473.87
    1024M-xfs            Avg wait ms simple-wb             2541.35     1453.54
    1024M-xfs            Avg wait ms mmap-strm             3400.25     3852.53
    
    The mmap-strm results were hurt because firefox launching had a tendency
    to push the test out of memory.  On the postive side, firefox launched
    marginally faster with the patches applied.  Time to completion for many
    tests was faster but more importantly - the "Avg wait" time as measured by
    iostat was far lower implying the system would be more responsive.  It was
    also the case that "Avg wait ms" on the root filesystem was lower.  I
    tested it manually and while the system felt slightly more responsive
    while copying data to a USB stick, it was marginal enough that it could be
    my imagination.
    
    This patch: do not writeback filesystem pages in direct reclaim.
    
    When kswapd is failing to keep zones above the min watermark, a process
    will enter direct reclaim in the same manner kswapd does.  If a dirty page
    is encountered during the scan, this page is written to backing storage
    using mapping->writepage.
    
    This causes two problems.  First, it can result in very deep call stacks,
    particularly if the target storage or filesystem are complex.  Some
    filesystems ignore write requests from direct reclaim as a result.  The
    second is that a single-page flush is inefficient in terms of IO.  While
    there is an expectation that the elevator will merge requests, this does
    not always happen.  Quoting Christoph Hellwig;
    
            The elevator has a relatively small window it can operate on,
            and can never fix up a bad large scale writeback pattern.
    
    This patch prevents direct reclaim writing back filesystem pages by
    checking if current is kswapd.  Anonymous pages are still written to swap
    as there is not the equivalent of a flusher thread for anonymous pages.
    If the dirty pages cannot be written back, they are placed back on the LRU
    lists.  There is now a direct dependency on dirty page balancing to
    prevent too many pages in the system being dirtied which would prevent
    reclaim making forward progress.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Johannes Weiner <jweiner@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Alex Elder <aelder@sgi.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ec57779c5a57..2c41b2c1943b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -100,6 +100,7 @@ enum zone_stat_item {
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
+	NR_VMSCAN_WRITE_SKIP,
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */

commit f80c0673610e36ae29d63e3297175e22f70dde5f
Author: Minchan Kim <minchan.kim@gmail.com>
Date:   Mon Oct 31 17:06:55 2011 -0700

    mm: zone_reclaim: make isolate_lru_page() filter-aware
    
    In __zone_reclaim case, we don't want to shrink mapped page.  Nonetheless,
    we have isolated mapped page and re-add it into LRU's head.  It's
    unnecessary CPU overhead and makes LRU churning.
    
    Of course, when we isolate the page, the page might be mapped but when we
    try to migrate the page, the page would be not mapped.  So it could be
    migrated.  But race is rare and although it happens, it's no big deal.
    
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 80da968798ea..ec57779c5a57 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -170,6 +170,8 @@ static inline int is_unevictable_lru(enum lru_list l)
 #define ISOLATE_ACTIVE		((__force isolate_mode_t)0x2)
 /* Isolate clean file */
 #define ISOLATE_CLEAN		((__force isolate_mode_t)0x4)
+/* Isolate unmapped file */
+#define ISOLATE_UNMAPPED	((__force isolate_mode_t)0x8)
 
 /* LRU Isolation modes. */
 typedef unsigned __bitwise__ isolate_mode_t;

commit 39deaf8585152f1a35c1676d3d7dc6ae0fb65967
Author: Minchan Kim <minchan.kim@gmail.com>
Date:   Mon Oct 31 17:06:51 2011 -0700

    mm: compaction: make isolate_lru_page() filter-aware
    
    In async mode, compaction doesn't migrate dirty or writeback pages.  So,
    it's meaningless to pick the page and re-add it to lru list.
    
    Of course, when we isolate the page in compaction, the page might be dirty
    or writeback but when we try to migrate the page, the page would be not
    dirty, writeback.  So it could be migrated.  But it's very unlikely as
    isolate and migration cycle is much faster than writeout.
    
    So, this patch helps cpu overhead and prevent unnecessary LRU churning.
    
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 436ce6e7a446..80da968798ea 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -168,6 +168,8 @@ static inline int is_unevictable_lru(enum lru_list l)
 #define ISOLATE_INACTIVE	((__force isolate_mode_t)0x1)
 /* Isolate active pages */
 #define ISOLATE_ACTIVE		((__force isolate_mode_t)0x2)
+/* Isolate clean file */
+#define ISOLATE_CLEAN		((__force isolate_mode_t)0x4)
 
 /* LRU Isolation modes. */
 typedef unsigned __bitwise__ isolate_mode_t;

commit 4356f21d09283dc6d39a6f7287a65ddab61e2808
Author: Minchan Kim <minchan.kim@gmail.com>
Date:   Mon Oct 31 17:06:47 2011 -0700

    mm: change isolate mode from #define to bitwise type
    
    Change ISOLATE_XXX macro with bitwise isolate_mode_t type.  Normally,
    macro isn't recommended as it's type-unsafe and making debugging harder as
    symbol cannot be passed throught to the debugger.
    
    Quote from Johannes
    " Hmm, it would probably be cleaner to fully convert the isolation mode
    into independent flags.  INACTIVE, ACTIVE, BOTH is currently a
    tri-state among flags, which is a bit ugly."
    
    This patch moves isolate mode from swap.h to mmzone.h by memcontrol.h
    
    Signed-off-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index be1ac8d7789b..436ce6e7a446 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -164,6 +164,14 @@ static inline int is_unevictable_lru(enum lru_list l)
 #define LRU_ALL_EVICTABLE (LRU_ALL_FILE | LRU_ALL_ANON)
 #define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
 
+/* Isolate inactive pages */
+#define ISOLATE_INACTIVE	((__force isolate_mode_t)0x1)
+/* Isolate active pages */
+#define ISOLATE_ACTIVE		((__force isolate_mode_t)0x2)
+
+/* LRU Isolation modes. */
+typedef unsigned __bitwise__ isolate_mode_t;
+
 enum zone_watermarks {
 	WMARK_MIN,
 	WMARK_LOW,

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0a2d3d620feb..be1ac8d7789b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -16,7 +16,7 @@
 #include <linux/nodemask.h>
 #include <linux/pageblock-flags.h>
 #include <generated/bounds.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/page.h>
 
 /* Free memory management - zoned buddy allocator.  */

commit bb2a0de92c891b8feeedc0178acb3ae009d899a8
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Jul 26 16:08:22 2011 -0700

    memcg: consolidate memory cgroup lru stat functions
    
    In mm/memcontrol.c, there are many lru stat functions as..
    
      mem_cgroup_zone_nr_lru_pages
      mem_cgroup_node_nr_file_lru_pages
      mem_cgroup_nr_file_lru_pages
      mem_cgroup_node_nr_anon_lru_pages
      mem_cgroup_nr_anon_lru_pages
      mem_cgroup_node_nr_unevictable_lru_pages
      mem_cgroup_nr_unevictable_lru_pages
      mem_cgroup_node_nr_lru_pages
      mem_cgroup_nr_lru_pages
      mem_cgroup_get_local_zonestat
    
    Some of them are under #ifdef MAX_NUMNODES >1 and others are not.
    This seems bad. This patch consolidates all functions into
    
      mem_cgroup_zone_nr_lru_pages()
      mem_cgroup_node_nr_lru_pages()
      mem_cgroup_nr_lru_pages()
    
    For these functions, "which LRU?" information is passed by a mask.
    
    example:
      mem_cgroup_nr_lru_pages(mem, BIT(LRU_ACTIVE_ANON))
    
    And I added some macro as ALL_LRU, ALL_LRU_FILE, ALL_LRU_ANON.
    
    example:
      mem_cgroup_nr_lru_pages(mem, ALL_LRU)
    
    BTW, considering layout of NUMA memory placement of counters, this patch seems
    to be better.
    
    Now, when we gather all LRU information, we scan in following orer
        for_each_lru -> for_each_node -> for_each_zone.
    
    This means we'll touch cache lines in different node in turn.
    
    After patch, we'll scan
        for_each_node -> for_each_zone -> for_each_lru(mask)
    
    Then, we'll gather information in the same cacheline at once.
    
    [akpm@linux-foundation.org: fix warnigns, build error]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Ying Han <yinghan@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9f7c3ebcbbad..0a2d3d620feb 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -158,6 +158,12 @@ static inline int is_unevictable_lru(enum lru_list l)
 	return (l == LRU_UNEVICTABLE);
 }
 
+/* Mask used at gathering information at once (see memcontrol.c) */
+#define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
+#define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
+#define LRU_ALL_EVICTABLE (LRU_ALL_FILE | LRU_ALL_ANON)
+#define LRU_ALL	     ((1 << NR_LRU_LISTS) - 1)
+
 enum zone_watermarks {
 	WMARK_MIN,
 	WMARK_LOW,

commit c6830c22603aaecf65405af23f6da2d55892f9cb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Thu Jun 16 17:28:07 2011 +0900

    Fix node_start/end_pfn() definition for mm/page_cgroup.c
    
    commit 21a3c96 uses node_start/end_pfn(nid) for detection start/end
    of nodes. But, it's not defined in linux/mmzone.h but defined in
    /arch/???/include/mmzone.h which is included only under
    CONFIG_NEED_MULTIPLE_NODES=y.
    
    Then, we see
      mm/page_cgroup.c: In function 'page_cgroup_init':
      mm/page_cgroup.c:308: error: implicit declaration of function 'node_start_pfn'
      mm/page_cgroup.c:309: error: implicit declaration of function 'node_end_pfn'
    
    So, fixiing page_cgroup.c is an idea...
    
    But node_start_pfn()/node_end_pfn() is a very generic macro and
    should be implemented in the same manner for all archs.
    (m32r has different implementation...)
    
    This patch removes definitions of node_start/end_pfn() in each archs
    and defines a unified one in linux/mmzone.h. It's not under
    CONFIG_NEED_MULTIPLE_NODES, now.
    
    A result of macro expansion is here (mm/page_cgroup.c)
    
    for !NUMA
     start_pfn = ((&contig_page_data)->node_start_pfn);
      end_pfn = ({ pg_data_t *__pgdat = (&contig_page_data); __pgdat->node_start_pfn + __pgdat->node_spanned_pages;});
    
    for NUMA (x86-64)
      start_pfn = ((node_data[nid])->node_start_pfn);
      end_pfn = ({ pg_data_t *__pgdat = (node_data[nid]); __pgdat->node_start_pfn + __pgdat->node_spanned_pages;});
    
    Changelog:
     - fixed to avoid using "nid" twice in node_end_pfn() macro.
    
    Reported-and-acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Reported-and-tested-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c928dac6cad0..9f7c3ebcbbad 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -647,6 +647,13 @@ typedef struct pglist_data {
 #endif
 #define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))
 
+#define node_start_pfn(nid)	(NODE_DATA(nid)->node_start_pfn)
+
+#define node_end_pfn(nid) ({\
+	pg_data_t *__pgdat = NODE_DATA(nid);\
+	__pgdat->node_start_pfn + __pgdat->node_spanned_pages;\
+})
+
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;

commit 2a56d2220284b0e4dd8569fa475d7053f1c40a63
Merge: 46f2cc80514e 239df0fd5ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 27 19:51:32 2011 -0700

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (45 commits)
      ARM: 6945/1: Add unwinding support for division functions
      ARM: kill pmd_off()
      ARM: 6944/1: mm: allow ASID 0 to be allocated to tasks
      ARM: 6943/1: mm: use TTBR1 instead of reserved context ID
      ARM: 6942/1: mm: make TTBR1 always point to swapper_pg_dir on ARMv6/7
      ARM: 6941/1: cache: ensure MVA is cacheline aligned in flush_kern_dcache_area
      ARM: add sendmmsg syscall
      ARM: 6863/1: allow hotplug on msm
      ARM: 6832/1: mmci: support for ST-Ericsson db8500v2
      ARM: 6830/1: mach-ux500: force PrimeCell revisions
      ARM: 6829/1: amba: make hardcoded periphid override hardware
      ARM: 6828/1: mach-ux500: delete SSP PrimeCell ID
      ARM: 6827/1: mach-netx: delete hardcoded periphid
      ARM: 6940/1: fiq: Briefly document driver responsibilities for suspend/resume
      ARM: 6938/1: fiq: Refactor {get,set}_fiq_regs() for Thumb-2
      ARM: 6914/1: sparsemem: fix highmem detection when using SPARSEMEM
      ARM: 6913/1: sparsemem: allow pfn_valid to be overridden when using SPARSEMEM
      at91: drop at572d940hf support
      at91rm9200: introduce at91rm9200_set_type to specficy cpu package
      at91: drop boot_params and PLAT_PHYS_OFFSET
      ...

commit 246e87a9393448c20873bc5dee64be68ed559e24
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Thu May 26 16:25:34 2011 -0700

    memcg: fix get_scan_count() for small targets
    
    During memory reclaim we determine the number of pages to be scanned per
    zone as
    
            (anon + file) >> priority.
    Assume
            scan = (anon + file) >> priority.
    
    If scan < SWAP_CLUSTER_MAX, the scan will be skipped for this time and
    priority gets higher.  This has some problems.
    
      1. This increases priority as 1 without any scan.
         To do scan in this priority, amount of pages should be larger than 512M.
         If pages>>priority < SWAP_CLUSTER_MAX, it's recorded and scan will be
         batched, later. (But we lose 1 priority.)
         If memory size is below 16M, pages >> priority is 0 and no scan in
         DEF_PRIORITY forever.
    
      2. If zone->all_unreclaimabe==true, it's scanned only when priority==0.
         So, x86's ZONE_DMA will never be recoverred until the user of pages
         frees memory by itself.
    
      3. With memcg, the limit of memory can be small. When using small memcg,
         it gets priority < DEF_PRIORITY-2 very easily and need to call
         wait_iff_congested().
         For doing scan before priorty=9, 64MB of memory should be used.
    
    Then, this patch tries to scan SWAP_CLUSTER_MAX of pages in force...when
    
      1. the target is enough small.
      2. it's kswapd or memcg reclaim.
    
    Then we can avoid rapid priority drop and may be able to recover
    all_unreclaimable in a small zones.  And this patch removes nr_saved_scan.
     This will allow scanning in this priority even when pages >> priority is
    very small.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Ying Han <yinghan@google.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 217bcf6bca77..29312bdf119f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -273,11 +273,6 @@ struct zone_reclaim_stat {
 	 */
 	unsigned long		recent_rotated[2];
 	unsigned long		recent_scanned[2];
-
-	/*
-	 * accumulated for batching
-	 */
-	unsigned long		nr_saved_scan[NR_LRU_LISTS];
 };
 
 struct zone {

commit 7b7bf499f79de3f6c85a340c8453a78789523f85
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu May 19 13:21:14 2011 +0100

    ARM: 6913/1: sparsemem: allow pfn_valid to be overridden when using SPARSEMEM
    
    In commit eb33575c ("[ARM] Double check memmap is actually valid with a
    memmap has unexpected holes V2"), a new function, memmap_valid_within,
    was introduced to mmzone.h so that holes in the memmap which pass
    pfn_valid in SPARSEMEM configurations can be detected and avoided.
    
    The fix to this problem checks that the pfn <-> page linkages are
    correct by calculating the page for the pfn and then checking that
    page_to_pfn on that page returns the original pfn. Unfortunately, in
    SPARSEMEM configurations, this results in reading from the page flags to
    determine the correct section. Since the memmap here has been freed,
    junk is read from memory and the check is no longer robust.
    
    In the best case, reading from /proc/pagetypeinfo will give you the
    wrong answer. In the worst case, you get SEGVs, Kernel OOPses and hung
    CPUs. Furthermore, ioremap implementations that use pfn_valid to
    disallow the remapping of normal memory will break.
    
    This patch allows architectures to provide their own pfn_valid function
    instead of using the default implementation used by sparsemem. The
    architecture-specific version is aware of the memmap state and will
    return false when passed a pfn for a freed page within a valid section.
    
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 217bcf6bca77..261f299c9441 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1056,12 +1056,14 @@ static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 	return __nr_to_section(pfn_to_section_nr(pfn));
 }
 
+#ifndef CONFIG_HAVE_ARCH_PFN_VALID
 static inline int pfn_valid(unsigned long pfn)
 {
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
 		return 0;
 	return valid_section(__nr_to_section(pfn_to_section_nr(pfn)));
 }
+#endif
 
 static inline int pfn_present(unsigned long pfn)
 {

commit a539f3533b78e39a22723d6d3e1e11b6c14454d9
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Tue May 24 17:12:51 2011 -0700

    mm: add SECTION_ALIGN_UP() and SECTION_ALIGN_DOWN() macro
    
    Add SECTION_ALIGN_UP() and SECTION_ALIGN_DOWN() macro which aligns given
    pfn to upper section and lower section boundary accordingly.
    
    Required for the latest memory hotplug support for the Xen balloon driver.
    
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d7152002e18d..217bcf6bca77 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -956,6 +956,9 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
 #define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
 
+#define SECTION_ALIGN_UP(pfn)	(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)
+#define SECTION_ALIGN_DOWN(pfn)	((pfn) & PAGE_SECTION_MASK)
+
 struct page;
 struct page_cgroup;
 struct mem_section {

commit e3c40f379a144f35e53864a2cd970e238071afd7
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Tue May 24 17:12:33 2011 -0700

    mm: pfn_to_section_nr()/section_nr_to_pfn() is valid only in CONFIG_SPARSEMEM context
    
    pfn_to_section_nr()/section_nr_to_pfn() is valid only in CONFIG_SPARSEMEM
    context.  Move it to proper place.
    
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e56f835274c9..d7152002e18d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -928,9 +928,6 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #define pfn_to_nid(pfn)		(0)
 #endif
 
-#define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
-#define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
-
 #ifdef CONFIG_SPARSEMEM
 
 /*
@@ -956,6 +953,9 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #error Allocator MAX_ORDER exceeds SECTION_SIZE
 #endif
 
+#define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
+#define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
+
 struct page;
 struct page_cgroup;
 struct mem_section {

commit 0a9d59a2461477bd9ed143c01af9df3f8f00fa81
Merge: a23ce6da9677 795abaf1e4e1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Tue Feb 15 10:24:31 2011 +0100

    Merge branch 'master' into for-next

commit 25a64ec1e7d0cfe172832d06a31215d458dfea7f
Author: Pete Zaitcev <zaitcev@kotori.zaitcev.us>
Date:   Thu Feb 3 22:43:48 2011 -0700

    fix comment spelling becausse => because
    
    Signed-off-by: Pete Zaitcev <zaitcev@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 39c24ebe9cfd..52f96d78c0e8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -477,7 +477,7 @@ unsigned long zone_nr_free_pages(struct zone *zone);
 #ifdef CONFIG_NUMA
 
 /*
- * The NUMA zonelists are doubled becausse we need zonelists that restrict the
+ * The NUMA zonelists are doubled because we need zonelists that restrict the
  * allocations to a single node for GFP_THISNODE.
  *
  * [0]	: Zonelist with fallback

commit 79134171df238171daa4c024a42b77b401ccb00b
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:46:58 2011 -0800

    thp: transparent hugepage vmstat
    
    Add hugepage stat information to /proc/vmstat and /proc/meminfo.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dad3612a7e39..02ecb0189b1d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -114,6 +114,7 @@ enum zone_stat_item {
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
 #endif
+	NR_ANON_TRANSPARENT_HUGEPAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
 /*

commit 9950474883e027e6e728cbcff25f7f2bf0c96530
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:46:20 2011 -0800

    mm: kswapd: stop high-order balancing when any suitable zone is balanced
    
    Simon Kirby reported the following problem
    
       We're seeing cases on a number of servers where cache never fully
       grows to use all available memory.  Sometimes we see servers with 4 GB
       of memory that never seem to have less than 1.5 GB free, even with a
       constantly-active VM.  In some cases, these servers also swap out while
       this happens, even though they are constantly reading the working set
       into memory.  We have been seeing this happening for a long time; I
       don't think it's anything recent, and it still happens on 2.6.36.
    
    After some debugging work by Simon, Dave Hansen and others, the prevaling
    theory became that kswapd is reclaiming order-3 pages requested by SLUB
    too aggressive about it.
    
    There are two apparent problems here.  On the target machine, there is a
    small Normal zone in comparison to DMA32.  As kswapd tries to balance all
    zones, it would continually try reclaiming for Normal even though DMA32
    was balanced enough for callers.  The second problem is that
    sleeping_prematurely() does not use the same logic as balance_pgdat() when
    deciding whether to sleep or not.  This keeps kswapd artifically awake.
    
    A number of tests were run and the figures from previous postings will
    look very different for a few reasons.  One, the old figures were forcing
    my network card to use GFP_ATOMIC in attempt to replicate Simon's problem.
     Second, I previous specified slub_min_order=3 again in an attempt to
    reproduce Simon's problem.  In this posting, I'm depending on Simon to say
    whether his problem is fixed or not and these figures are to show the
    impact to the ordinary cases.  Finally, the "vmscan" figures are taken
    from /proc/vmstat instead of the tracepoints.  There is less information
    but recording is less disruptive.
    
    The first test of relevance was postmark with a process running in the
    background reading a large amount of anonymous memory in blocks.  The
    objective was to vaguely simulate what was happening on Simon's machine
    and it's memory intensive enough to have kswapd awake.
    
    POSTMARK
                                                traceonly          kanyzone
    Transactions per second:              156.00 ( 0.00%)   153.00 (-1.96%)
    Data megabytes read per second:        21.51 ( 0.00%)    21.52 ( 0.05%)
    Data megabytes written per second:     29.28 ( 0.00%)    29.11 (-0.58%)
    Files created alone per second:       250.00 ( 0.00%)   416.00 (39.90%)
    Files create/transact per second:      79.00 ( 0.00%)    76.00 (-3.95%)
    Files deleted alone per second:       520.00 ( 0.00%)   420.00 (-23.81%)
    Files delete/transact per second:      79.00 ( 0.00%)    76.00 (-3.95%)
    
    MMTests Statistics: duration
    User/Sys Time Running Test (seconds)         16.58      17.4
    Total Elapsed Time (seconds)                218.48    222.47
    
    VMstat Reclaim Statistics: vmscan
    Direct reclaims                                  0          4
    Direct reclaim pages scanned                     0        203
    Direct reclaim pages reclaimed                   0        184
    Kswapd pages scanned                        326631     322018
    Kswapd pages reclaimed                      312632     309784
    Kswapd low wmark quickly                         1          4
    Kswapd high wmark quickly                      122        475
    Kswapd skip congestion_wait                      1          0
    Pages activated                             700040     705317
    Pages deactivated                           212113     203922
    Pages written                                 9875       6363
    
    Total pages scanned                         326631    322221
    Total pages reclaimed                       312632    309968
    %age total pages scanned/reclaimed          95.71%    96.20%
    %age total pages scanned/written             3.02%     1.97%
    
    proc vmstat: Faults
    Major Faults                                   300       254
    Minor Faults                                645183    660284
    Page ins                                    493588    486704
    Page outs                                  4960088   4986704
    Swap ins                                      1230       661
    Swap outs                                     9869      6355
    
    Performance is mildly affected because kswapd is no longer doing as much
    work and the background memory consumer process is getting in the way.
    Note that kswapd scanned and reclaimed fewer pages as it's less aggressive
    and overall fewer pages were scanned and reclaimed.  Swap in/out is
    particularly reduced again reflecting kswapd throwing out fewer pages.
    
    The slight performance impact is unfortunate here but it looks like a
    direct result of kswapd being less aggressive.  As the bug report is about
    too many pages being freed by kswapd, it may have to be accepted for now.
    
    The second test is a streaming IO benchmark that was previously used by
    Johannes to show regressions in page reclaim.
    
    MICRO
                                             traceonly  kanyzone
    User/Sys Time Running Test (seconds)         29.29     28.87
    Total Elapsed Time (seconds)                492.18    488.79
    
    VMstat Reclaim Statistics: vmscan
    Direct reclaims                               2128       1460
    Direct reclaim pages scanned               2284822    1496067
    Direct reclaim pages reclaimed              148919     110937
    Kswapd pages scanned                      15450014   16202876
    Kswapd pages reclaimed                     8503697    8537897
    Kswapd low wmark quickly                      3100       3397
    Kswapd high wmark quickly                     1860       7243
    Kswapd skip congestion_wait                    708        801
    Pages activated                               9635       9573
    Pages deactivated                             1432       1271
    Pages written                                  223       1130
    
    Total pages scanned                       17734836  17698943
    Total pages reclaimed                      8652616   8648834
    %age total pages scanned/reclaimed          48.79%    48.87%
    %age total pages scanned/written             0.00%     0.01%
    
    proc vmstat: Faults
    Major Faults                                   165       221
    Minor Faults                               9655785   9656506
    Page ins                                      3880      7228
    Page outs                                 37692940  37480076
    Swap ins                                         0        69
    Swap outs                                       19        15
    
    Again fewer pages are scanned and reclaimed as expected and this time the
    test completed faster.  Note that kswapd is hitting its watermarks faster
    (low and high wmark quickly) which I expect is due to kswapd reclaiming
    fewer pages.
    
    I also ran fs-mark, iozone and sysbench but there is nothing interesting
    to report in the figures.  Performance is not significantly changed and
    the reclaim statistics look reasonable.
    
    Tgis patch:
    
    When the allocator enters its slow path, kswapd is woken up to balance the
    node.  It continues working until all zones within the node are balanced.
    For order-0 allocations, this makes perfect sense but for higher orders it
    can have unintended side-effects.  If the zone sizes are imbalanced,
    kswapd may reclaim heavily within a smaller zone discarding an excessive
    number of pages.  The user-visible behaviour is that kswapd is awake and
    reclaiming even though plenty of pages are free from a suitable zone.
    
    This patch alters the "balance" logic for high-order reclaim allowing
    kswapd to stop if any suitable zone becomes balanced to reduce the number
    of pages it reclaims from other zones.  kswapd still tries to ensure that
    order-0 watermarks for all zones are met before sleeping.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Eric B Munson <emunson@mgebm.net>
    Cc: Simon Kirby <sim@hostway.ca>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 48906629335c..dad3612a7e39 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -639,6 +639,7 @@ typedef struct pglist_data {
 	wait_queue_head_t kswapd_wait;
 	struct task_struct *kswapd;
 	int kswapd_max_order;
+	enum zone_type classzone_idx;
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
@@ -654,7 +655,7 @@ typedef struct pglist_data {
 
 extern struct mutex zonelists_mutex;
 void build_all_zonelists(void *data);
-void wakeup_kswapd(struct zone *zone, int order);
+void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);
 bool zone_watermark_ok_safe(struct zone *z, int order, unsigned long mark,

commit 88f5acf88ae6a9778f6d25d0d5d7ec2d57764a97
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:45:41 2011 -0800

    mm: page allocator: adjust the per-cpu counter threshold when memory is low
    
    Commit aa45484 ("calculate a better estimate of NR_FREE_PAGES when memory
    is low") noted that watermarks were based on the vmstat NR_FREE_PAGES.  To
    avoid synchronization overhead, these counters are maintained on a per-cpu
    basis and drained both periodically and when a threshold is above a
    threshold.  On large CPU systems, the difference between the estimate and
    real value of NR_FREE_PAGES can be very high.  The system can get into a
    case where pages are allocated far below the min watermark potentially
    causing livelock issues.  The commit solved the problem by taking a better
    reading of NR_FREE_PAGES when memory was low.
    
    Unfortately, as reported by Shaohua Li this accurate reading can consume a
    large amount of CPU time on systems with many sockets due to cache line
    bouncing.  This patch takes a different approach.  For large machines
    where counter drift might be unsafe and while kswapd is awake, the per-cpu
    thresholds for the target pgdat are reduced to limit the level of drift to
    what should be a safe level.  This incurs a performance penalty in heavy
    memory pressure by a factor that depends on the workload and the machine
    but the machine should function correctly without accidentally exhausting
    all memory on a node.  There is an additional cost when kswapd wakes and
    sleeps but the event is not expected to be frequent - in Shaohua's test
    case, there was one recorded sleep and wake event at least.
    
    To ensure that kswapd wakes up, a safe version of zone_watermark_ok() is
    introduced that takes a more accurate reading of NR_FREE_PAGES when called
    from wakeup_kswapd, when deciding whether it is really safe to go back to
    sleep in sleeping_prematurely() and when deciding if a zone is really
    balanced or not in balance_pgdat().  We are still using an expensive
    function but limiting how often it is called.
    
    When the test case is reproduced, the time spent in the watermark
    functions is reduced.  The following report is on the percentage of time
    spent cumulatively spent in the functions zone_nr_free_pages(),
    zone_watermark_ok(), __zone_watermark_ok(), zone_watermark_ok_safe(),
    zone_page_state_snapshot(), zone_page_state().
    
    vanilla                      11.6615%
    disable-threshold            0.2584%
    
    David said:
    
    : We had to pull aa454840 "mm: page allocator: calculate a better estimate
    : of NR_FREE_PAGES when memory is low and kswapd is awake" from 2.6.36
    : internally because tests showed that it would cause the machine to stall
    : as the result of heavy kswapd activity.  I merged it back with this fix as
    : it is pending in the -mm tree and it solves the issue we were seeing, so I
    : definitely think this should be pushed to -stable (and I would seriously
    : consider it for 2.6.37 inclusion even at this late date).
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reported-by: Shaohua Li <shaohua.li@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Tested-by: Nicolas Bareil <nico@chdir.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: <stable@kernel.org>         [2.6.37.1, 2.6.36.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 39c24ebe9cfd..48906629335c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -458,12 +458,6 @@ static inline int zone_is_oom_locked(const struct zone *zone)
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
 }
 
-#ifdef CONFIG_SMP
-unsigned long zone_nr_free_pages(struct zone *zone);
-#else
-#define zone_nr_free_pages(zone) zone_page_state(zone, NR_FREE_PAGES)
-#endif /* CONFIG_SMP */
-
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
@@ -661,7 +655,9 @@ typedef struct pglist_data {
 extern struct mutex zonelists_mutex;
 void build_all_zonelists(void *data);
 void wakeup_kswapd(struct zone *zone, int order);
-int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
+bool zone_watermark_ok(struct zone *z, int order, unsigned long mark,
+		int classzone_idx, int alloc_flags);
+bool zone_watermark_ok_safe(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);
 enum memmap_context {
 	MEMMAP_EARLY,

commit 0e093d99763eb4cea09f8ca4f1d01f34e121d10b
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 26 14:21:45 2010 -0700

    writeback: do not sleep on the congestion queue if there are no congested BDIs or if significant congestion is not being encountered in the current zone
    
    If congestion_wait() is called with no BDI congested, the caller will
    sleep for the full timeout and this may be an unnecessary sleep.  This
    patch adds a wait_iff_congested() that checks congestion and only sleeps
    if a BDI is congested else, it calls cond_resched() to ensure the caller
    is not hogging the CPU longer than its quota but otherwise will not sleep.
    
    This is aimed at reducing some of the major desktop stalls reported during
    IO.  For example, while kswapd is operating, it calls congestion_wait()
    but it could just have been reclaiming clean page cache pages with no
    congestion.  Without this patch, it would sleep for a full timeout but
    after this patch, it'll just call schedule() if it has been on the CPU too
    long.  Similar logic applies to direct reclaimers that are not making
    enough progress.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c3c17fb675ee..39c24ebe9cfd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -423,6 +423,9 @@ struct zone {
 typedef enum {
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
+	ZONE_CONGESTED,			/* zone has many dirty pages backed by
+					 * a congested BDI
+					 */
 } zone_flags_t;
 
 static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
@@ -440,6 +443,11 @@ static inline void zone_clear_flag(struct zone *zone, zone_flags_t flag)
 	clear_bit(flag, &zone->flags);
 }
 
+static inline int zone_is_reclaim_congested(const struct zone *zone)
+{
+	return test_bit(ZONE_CONGESTED, &zone->flags);
+}
+
 static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);

commit ea941f0e2a8c02ae876cd73deb4e1557248f258c
Author: Michael Rubin <mrubin@google.com>
Date:   Tue Oct 26 14:21:35 2010 -0700

    writeback: add nr_dirtied and nr_written to /proc/vmstat
    
    To help developers and applications gain visibility into writeback
    behaviour adding two entries to vm_stat_items and /proc/vmstat.  This will
    allow us to track the "written" and "dirtied" counts.
    
       # grep nr_dirtied /proc/vmstat
       nr_dirtied 3747
       # grep nr_written /proc/vmstat
       nr_written 3618
    
    Signed-off-by: Michael Rubin <mrubin@google.com>
    Reviewed-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3984c4eb41fd..c3c17fb675ee 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -104,6 +104,8 @@ enum zone_stat_item {
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
+	NR_DIRTIED,		/* page dirtyings since bootup */
+	NR_WRITTEN,		/* page writings since bootup */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */

commit aa45484031ddee09b06350ab8528bfe5b2c76d1c
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Sep 9 16:38:17 2010 -0700

    mm: page allocator: calculate a better estimate of NR_FREE_PAGES when memory is low and kswapd is awake
    
    Ordinarily watermark checks are based on the vmstat NR_FREE_PAGES as it is
    cheaper than scanning a number of lists.  To avoid synchronization
    overhead, counter deltas are maintained on a per-cpu basis and drained
    both periodically and when the delta is above a threshold.  On large CPU
    systems, the difference between the estimated and real value of
    NR_FREE_PAGES can be very high.  If NR_FREE_PAGES is much higher than
    number of real free page in buddy, the VM can allocate pages below min
    watermark, at worst reducing the real number of pages to zero.  Even if
    the OOM killer kills some victim for freeing memory, it may not free
    memory if the exit path requires a new page resulting in livelock.
    
    This patch introduces a zone_page_state_snapshot() function (courtesy of
    Christoph) that takes a slightly more accurate view of an arbitrary vmstat
    counter.  It is used to read NR_FREE_PAGES while kswapd is awake to avoid
    the watermark being accidentally broken.  The estimate is not perfect and
    may result in cache line bounces but is expected to be lighter than the
    IPI calls necessary to continually drain the per-cpu counters while kswapd
    is awake.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6e6e62648a4d..3984c4eb41fd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -283,6 +283,13 @@ struct zone {
 	/* zone watermarks, access with *_wmark_pages(zone) macros */
 	unsigned long watermark[NR_WMARK];
 
+	/*
+	 * When free pages are below this point, additional steps are taken
+	 * when reading the number of free pages to avoid per-cpu counter
+	 * drift allowing watermarks to be breached
+	 */
+	unsigned long percpu_drift_mark;
+
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several
@@ -441,6 +448,12 @@ static inline int zone_is_oom_locked(const struct zone *zone)
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
 }
 
+#ifdef CONFIG_SMP
+unsigned long zone_nr_free_pages(struct zone *zone);
+#else
+#define zone_nr_free_pages(zone) zone_page_state(zone, NR_FREE_PAGES)
+#endif /* CONFIG_SMP */
+
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the

commit 25edde0332916ae706ccf83de688be57bcc844b7
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Aug 9 17:19:27 2010 -0700

    vmscan: kill prev_priority completely
    
    Since 2.6.28 zone->prev_priority is unused. Then it can be removed
    safely. It reduce stack usage slightly.
    
    Now I have to say that I'm sorry. 2 years ago, I thought prev_priority
    can be integrate again, it's useful. but four (or more) times trying
    haven't got good performance number. Thus I give up such approach.
    
    The rest of this changelog is notes on prev_priority and why it existed in
    the first place and why it might be not necessary any more. This information
    is based heavily on discussions between Andrew Morton, Rik van Riel and
    Kosaki Motohiro who is heavily quotes from.
    
    Historically prev_priority was important because it determined when the VM
    would start unmapping PTE pages. i.e. there are no balances of note within
    the VM, Anon vs File and Mapped vs Unmapped. Without prev_priority, there
    is a potential risk of unnecessarily increasing minor faults as a large
    amount of read activity of use-once pages could push mapped pages to the
    end of the LRU and get unmapped.
    
    There is no proof this is still a problem but currently it is not considered
    to be. Active files are not deactivated if the active file list is smaller
    than the inactive list reducing the liklihood that file-mapped pages are
    being pushed off the LRU and referenced executable pages are kept on the
    active list to avoid them getting pushed out by read activity.
    
    Even if it is a problem, prev_priority prev_priority wouldn't works
    nowadays. First of all, current vmscan still a lot of UP centric code. it
    expose some weakness on some dozens CPUs machine. I think we need more and
    more improvement.
    
    The problem is, current vmscan mix up per-system-pressure, per-zone-pressure
    and per-task-pressure a bit. example, prev_priority try to boost priority to
    other concurrent priority. but if the another task have mempolicy restriction,
    it is unnecessary, but also makes wrong big latency and exceeding reclaim.
    per-task based priority + prev_priority adjustment make the emulation of
    per-system pressure. but it have two issue 1) too rough and brutal emulation
    2) we need per-zone pressure, not per-system.
    
    Another example, currently DEF_PRIORITY is 12. it mean the lru rotate about
    2 cycle (1/4096 + 1/2048 + 1/1024 + .. + 1) before invoking OOM-Killer.
    but if 10,0000 thrreads enter DEF_PRIORITY reclaim at the same time, the
    system have higher memory pressure than priority==0 (1/4096*10,000 > 2).
    prev_priority can't solve such multithreads workload issue. In other word,
    prev_priority concept assume the sysmtem don't have lots threads."
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michael Rubin <mrubin@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9ed9c459b14c..6e6e62648a4d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -347,21 +347,6 @@ struct zone {
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
 
-	/*
-	 * prev_priority holds the scanning priority for this zone.  It is
-	 * defined as the scanning priority at which we achieved our reclaim
-	 * target at the previous try_to_free_pages() or balance_pgdat()
-	 * invocation.
-	 *
-	 * We use prev_priority as a measure of how much stress page reclaim is
-	 * under - it drives the swappiness decision: whether to unmap mapped
-	 * pages.
-	 *
-	 * Access to both this field is quite racy even on uniprocessor.  But
-	 * it is expected to average out OK.
-	 */
-	int prev_priority;
-
 	/*
 	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
 	 * this zone's LRU.  Maintained by the pageout code.

commit b645bd1286f2fbcd2eb4ab3bed5884f63c42e363
Author: Alexander Nevenchannyy <a.nevenchannyy@gmail.com>
Date:   Mon Aug 9 17:19:00 2010 -0700

    mmzone.h: remove dead prototype
    
    get_zone_counts() was dropped from kernel tree, see:
    http://www.mail-archive.com/mm-commits@vger.kernel.org/msg07313.html but
    its prototype remains.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b4d109e389b8..9ed9c459b14c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -651,8 +651,6 @@ typedef struct pglist_data {
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;
-void get_zone_counts(unsigned long *active, unsigned long *inactive,
-			unsigned long *free);
 void build_all_zonelists(void *data);
 void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,

commit 7aac789885512388a66d47280d7e7777ffba1e59
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Wed May 26 14:45:00 2010 -0700

    numa: introduce numa_mem_id()- effective local memory node id
    
    Introduce numa_mem_id(), based on generic percpu variable infrastructure
    to track "nearest node with memory" for archs that support memoryless
    nodes.
    
    Define API in <linux/topology.h> when CONFIG_HAVE_MEMORYLESS_NODES
    defined, else stubs.  Architectures will define HAVE_MEMORYLESS_NODES
    if/when they support them.
    
    Archs can override definitions of:
    
    numa_mem_id() - returns node number of "local memory" node
    set_numa_mem() - initialize [this cpus'] per cpu variable 'numa_mem'
    cpu_to_mem()  - return numa_mem for specified cpu; may be used as lvalue
    
    Generic initialization of 'numa_mem' occurs in __build_all_zonelists().
    This will initialize the boot cpu at boot time, and all cpus on change of
    numa_zonelist_order, or when node or memory hot-plug requires zonelist
    rebuild.  Archs that support memoryless nodes will need to initialize
    'numa_mem' for secondary cpus as they're brought on-line.
    
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0fa491326c4a..b4d109e389b8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -671,6 +671,12 @@ void memory_present(int nid, unsigned long start, unsigned long end);
 static inline void memory_present(int nid, unsigned long start, unsigned long end) {}
 #endif
 
+#ifdef CONFIG_HAVE_MEMORYLESS_NODES
+int local_memory_node(int node_id);
+#else
+static inline int local_memory_node(int node_id) { return node_id; };
+#endif
+
 #ifdef CONFIG_NEED_NODE_MEMMAP_SIZE
 unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 #endif

commit 4eaf3f64397c3db3c5785eee508270d62a9fabd9
Author: Haicheng Li <haicheng.li@linux.intel.com>
Date:   Mon May 24 14:32:52 2010 -0700

    mem-hotplug: fix potential race while building zonelist for new populated zone
    
    Add global mutex zonelists_mutex to fix the possible race:
    
         CPU0                                  CPU1                    CPU2
    (1) zone->present_pages += online_pages;
    (2)                                       build_all_zonelists();
    (3)                                                               alloc_page();
    (4)                                                               free_page();
    (5) build_all_zonelists();
    (6)   __build_all_zonelists();
    (7)     zone->pageset = alloc_percpu();
    
    In step (3,4), zone->pageset still points to boot_pageset, so bad
    things may happen if 2+ nodes are in this state. Even if only 1 node
    is accessing the boot_pageset, (3) may still consume too much memory
    to fail the memory allocations in step (7).
    
    Besides, atomic operation ensures alloc_percpu() in step (7) will never fail
    since there is a new fresh memory block added in step(6).
    
    [haicheng.li@linux.intel.com: hold zonelists_mutex when build_all_zonelists]
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: Andi Kleen <andi.kleen@intel.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a367ed5bb3fe..0fa491326c4a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -650,6 +650,7 @@ typedef struct pglist_data {
 
 #include <linux/memory_hotplug.h>
 
+extern struct mutex zonelists_mutex;
 void get_zone_counts(unsigned long *active, unsigned long *inactive,
 			unsigned long *free);
 void build_all_zonelists(void *data);

commit 1f522509c77a5dea8dc384b735314f03908a6415
Author: Haicheng Li <haicheng.li@linux.intel.com>
Date:   Mon May 24 14:32:51 2010 -0700

    mem-hotplug: avoid multiple zones sharing same boot strapping boot_pageset
    
    For each new populated zone of hotadded node, need to update its pagesets
    with dynamically allocated per_cpu_pageset struct for all possible CPUs:
    
        1) Detach zone->pageset from the shared boot_pageset
           at end of __build_all_zonelists().
    
        2) Use mutex to protect zone->pageset when it's still
           shared in onlined_pages()
    
    Otherwises, multiple zones of different nodes would share same boot strapping
    boot_pageset for same CPU, which will finally cause below kernel panic:
    
      ------------[ cut here ]------------
      kernel BUG at mm/page_alloc.c:1239!
      invalid opcode: 0000 [#1] SMP
      ...
      Call Trace:
       [<ffffffff811300c1>] __alloc_pages_nodemask+0x131/0x7b0
       [<ffffffff81162e67>] alloc_pages_current+0x87/0xd0
       [<ffffffff81128407>] __page_cache_alloc+0x67/0x70
       [<ffffffff811325f0>] __do_page_cache_readahead+0x120/0x260
       [<ffffffff81132751>] ra_submit+0x21/0x30
       [<ffffffff811329c6>] ondemand_readahead+0x166/0x2c0
       [<ffffffff81132ba0>] page_cache_async_readahead+0x80/0xa0
       [<ffffffff8112a0e4>] generic_file_aio_read+0x364/0x670
       [<ffffffff81266cfa>] nfs_file_read+0xca/0x130
       [<ffffffff8117b20a>] do_sync_read+0xfa/0x140
       [<ffffffff8117bf75>] vfs_read+0xb5/0x1a0
       [<ffffffff8117c151>] sys_read+0x51/0x80
       [<ffffffff8103c032>] system_call_fastpath+0x16/0x1b
      RIP  [<ffffffff8112ff13>] get_page_from_freelist+0x883/0x900
       RSP <ffff88000d1e78a8>
      ---[ end trace 4bda28328b9990db ]
    
    [akpm@linux-foundation.org: merge fix]
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: Andi Kleen <andi.kleen@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f6f2c505fa7e..a367ed5bb3fe 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -652,7 +652,7 @@ typedef struct pglist_data {
 
 void get_zone_counts(unsigned long *active, unsigned long *inactive,
 			unsigned long *free);
-void build_all_zonelists(void);
+void build_all_zonelists(void *data);
 void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);

commit 0faa56389c793cda7f967117415717bbab24fe4e
Author: Marcelo Roberto Jimenez <mroberto@cpti.cetuc.puc-rio.br>
Date:   Mon May 24 14:32:47 2010 -0700

    mm: fix NR_SECTION_ROOTS == 0 when using using sparsemem extreme.
    
    Got this while compiling for ARM/SA1100:
    
    mm/sparse.c: In function '__section_nr':
    mm/sparse.c:135: warning: 'root' is used uninitialized in this function
    
    This patch follows Russell King's suggestion for a new calculation for
    NR_SECTION_ROOTS.  Thanks also to Sergei Shtylyov for pointing out the
    existence of the macro DIV_ROUND_UP.
    
    Atsushi Nemoto observed:
    : This fix doesn't just silence the warning - it fixes a real problem.
    :
    : Without this fix, mem_section[] might have 0 size so mem_section[0]
    : will share other variable area.  For example, I got:
    :
    : c030c700 b __warned.16478
    : c030c700 B mem_section
    : c030c701 b __warned.16483
    :
    : This might cause very strange behavior.  Your patch actually fixes it.
    
    Signed-off-by: Marcelo Roberto Jimenez <mroberto@cpti.cetuc.puc-rio.br>
    Cc: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Sergei Shtylyov <sshtylyov@mvista.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fd55f725a09e..f6f2c505fa7e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -981,7 +981,7 @@ struct mem_section {
 #endif
 
 #define SECTION_NR_TO_ROOT(sec)	((sec) / SECTIONS_PER_ROOT)
-#define NR_SECTION_ROOTS	(NR_MEM_SECTIONS / SECTIONS_PER_ROOT)
+#define NR_SECTION_ROOTS	DIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)
 #define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)
 
 #ifdef CONFIG_SPARSEMEM_EXTREME

commit 4f92e2586b43a2402e116055d4edda704f911b5b
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon May 24 14:32:32 2010 -0700

    mm: compaction: defer compaction using an exponential backoff when compaction fails
    
    The fragmentation index may indicate that a failure is due to external
    fragmentation but after a compaction run completes, it is still possible
    for an allocation to fail.  There are two obvious reasons as to why
    
      o Page migration cannot move all pages so fragmentation remains
      o A suitable page may exist but watermarks are not met
    
    In the event of compaction followed by an allocation failure, this patch
    defers further compaction in the zone (1 << compact_defer_shift) times.
    If the next compaction attempt also fails, compact_defer_shift is
    increased up to a maximum of 6.  If compaction succeeds, the defer
    counters are reset again.
    
    The zone that is deferred is the first zone in the zonelist - i.e.  the
    preferred zone.  To defer compaction in the other zones, the information
    would need to be stored in the zonelist or implemented similar to the
    zonelist_cache.  This would impact the fast-paths and is not justified at
    this time.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index cf9e458e96b0..fd55f725a09e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -321,6 +321,15 @@ struct zone {
 	unsigned long		*pageblock_flags;
 #endif /* CONFIG_SPARSEMEM */
 
+#ifdef CONFIG_COMPACTION
+	/*
+	 * On compaction failure, 1<<compact_defer_shift compactions
+	 * are skipped before trying again. The number attempted since
+	 * last failure is tracked with compact_considered.
+	 */
+	unsigned int		compact_considered;
+	unsigned int		compact_defer_shift;
+#endif
 
 	ZONE_PADDING(_pad1_)
 

commit 318ae2edc3b29216abd8a2510f3f80b764f06858
Merge: 25cf84cf377c 3e58974027b0
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Mar 8 16:55:37 2010 +0100

    Merge branch 'for-next' into for-linus
    
    Conflicts:
            Documentation/filesystems/proc.txt
            arch/arm/mach-u300/include/mach/debug-macro.S
            drivers/net/qlge/qlge_ethtool.c
            drivers/net/qlge/qlge_main.c
            drivers/net/typhoon.c

commit 93e4a89a8c987189b168a530a331ef6d0fcf07a7
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Fri Mar 5 13:41:55 2010 -0800

    mm: restore zone->all_unreclaimable to independence word
    
    commit e815af95 ("change all_unreclaimable zone member to flags") changed
    all_unreclaimable member to bit flag.  But it had an undesireble side
    effect.  free_one_page() is one of most hot path in linux kernel and
    increasing atomic ops in it can reduce kernel performance a bit.
    
    Thus, this patch revert such commit partially. at least
    all_unreclaimable shouldn't share memory word with other zone flags.
    
    [akpm@linux-foundation.org: fix patch interaction]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Huang Shijie <shijie8@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a01a103341bd..bc209d8b7b5c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -306,6 +306,7 @@ struct zone {
 	 * free areas of different sizes
 	 */
 	spinlock_t		lock;
+	int                     all_unreclaimable; /* All pages pinned */
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/* see spanned/present_pages for more description */
 	seqlock_t		span_seqlock;
@@ -417,7 +418,6 @@ struct zone {
 } ____cacheline_internodealigned_in_smp;
 
 typedef enum {
-	ZONE_ALL_UNRECLAIMABLE,		/* all pages pinned */
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
 } zone_flags_t;
@@ -437,11 +437,6 @@ static inline void zone_clear_flag(struct zone *zone, zone_flags_t flag)
 	clear_bit(flag, &zone->flags);
 }
 
-static inline int zone_is_all_unreclaimable(const struct zone *zone)
-{
-	return test_bit(ZONE_ALL_UNRECLAIMABLE, &zone->flags);
-}
-
 static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);

commit a626b46e17d0762d664ce471d40bc506b6e721ab
Merge: c1dcb4bb1e3e dce46a04d55d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 3 08:15:05 2010 -0800

    Merge branch 'x86-bootmem-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-bootmem-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (30 commits)
      early_res: Need to save the allocation name in drop_range_partial()
      sparsemem: Fix compilation on PowerPC
      early_res: Add free_early_partial()
      x86: Fix non-bootmem compilation on PowerPC
      core: Move early_res from arch/x86 to kernel/
      x86: Add find_fw_memmap_area
      Move round_up/down to kernel.h
      x86: Make 32bit support NO_BOOTMEM
      early_res: Enhance check_and_double_early_res
      x86: Move back find_e820_area to e820.c
      x86: Add find_early_area_size
      x86: Separate early_res related code from e820.c
      x86: Move bios page reserve early to head32/64.c
      sparsemem: Put mem map for one node together.
      sparsemem: Put usemap for one node together
      x86: Make 64 bit use early_res instead of bootmem before slab
      x86: Only call dma32_reserve_bootmem 64bit !CONFIG_NUMA
      x86: Make early_node_mem get mem > 4 GB if possible
      x86: Dynamically increase early_res array size
      x86: Introduce max_early_res and early_res_count
      ...

commit 43cf38eb5cea91245502df3fcee4dbfc1c74dd1c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 2 14:38:57 2010 +0900

    percpu: add __percpu sparse annotations to core kernel subsystems
    
    Add __percpu sparse annotations to core subsystems.
    
    These annotations are to make sparse consider percpu variables to be
    in a different address space and warn if accessed without going
    through percpu accessors.  This patch doesn't affect normal builds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-mm@kvack.org
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Eric Biederman <ebiederm@xmission.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7874201a3556..41acd4bf7664 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -301,7 +301,7 @@ struct zone {
 	unsigned long		min_unmapped_pages;
 	unsigned long		min_slab_pages;
 #endif
-	struct per_cpu_pageset	*pageset;
+	struct per_cpu_pageset __percpu *pageset;
 	/*
 	 * free areas of different sizes
 	 */

commit 08677214e318297f228237be0042aac754f48f1d
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Wed Feb 10 01:20:20 2010 -0800

    x86: Make 64 bit use early_res instead of bootmem before slab
    
    Finally we can use early_res to replace bootmem for x86_64 now.
    
    Still can use CONFIG_NO_BOOTMEM to enable it or not.
    
    -v2: fix 32bit compiling about MAX_DMA32_PFN
    -v3: folded bug fix from LKML message below
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <4B747239.4070907@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 30fe668c2542..eae8387b6007 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -620,7 +620,9 @@ typedef struct pglist_data {
 	struct page_cgroup *node_page_cgroup;
 #endif
 #endif
+#ifndef CONFIG_NO_BOOTMEM
 	struct bootmem_data *bdata;
+#endif
 #ifdef CONFIG_MEMORY_HOTPLUG
 	/*
 	 * Must be held any time you expect node_start_pfn, node_present_pages

commit 2a61aa401638529cd4231f6106980d307fba98fa
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Fri Dec 11 16:35:40 2009 -0500

    Fix misspellings of "invocation" in comments.
    
    Some comments misspell "invocation"; this fixes them. No code
    changes.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 30fe668c2542..e60a340fe890 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -349,7 +349,7 @@ struct zone {
 	 * prev_priority holds the scanning priority for this zone.  It is
 	 * defined as the scanning priority at which we achieved our reclaim
 	 * target at the previous try_to_free_pages() or balance_pgdat()
-	 * invokation.
+	 * invocation.
 	 *
 	 * We use prev_priority as a measure of how much stress page reclaim is
 	 * under - it drives the swappiness decision: whether to unmap mapped

commit 99dcc3e5a94ed491fbef402831d8c0bbb267f995
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Tue Jan 5 15:34:51 2010 +0900

    this_cpu: Page allocator conversion
    
    Use the per cpu allocator functionality to avoid per cpu arrays in struct zone.
    
    This drastically reduces the size of struct zone for systems with large
    amounts of processors and allows placement of critical variables of struct
    zone in one cacheline even on very large systems.
    
    Another effect is that the pagesets of one processor are placed near one
    another. If multiple pagesets from different zones fit into one cacheline
    then additional cacheline fetches can be avoided on the hot paths when
    allocating memory from multiple zones.
    
    Bootstrap becomes simpler if we use the same scheme for UP, SMP, NUMA. #ifdefs
    are reduced and we can drop the zone_pcp macro.
    
    Hotplug handling is also simplified since cpu alloc can bring up and
    shut down cpu areas for a specific cpu as a whole. So there is no need to
    allocate or free individual pagesets.
    
    V7-V8:
    - Explain chicken egg dilemmna with percpu allocator.
    
    V4-V5:
    - Fix up cases where per_cpu_ptr is called before irq disable
    - Integrate the bootstrap logic that was separate before.
    
    tj: Build failure in pageset_cpuup_callback() due to missing ret
        variable fixed.
    
    Reviewed-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 30fe668c2542..7874201a3556 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -184,13 +184,7 @@ struct per_cpu_pageset {
 	s8 stat_threshold;
 	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
 #endif
-} ____cacheline_aligned_in_smp;
-
-#ifdef CONFIG_NUMA
-#define zone_pcp(__z, __cpu) ((__z)->pageset[(__cpu)])
-#else
-#define zone_pcp(__z, __cpu) (&(__z)->pageset[(__cpu)])
-#endif
+};
 
 #endif /* !__GENERATING_BOUNDS.H */
 
@@ -306,10 +300,8 @@ struct zone {
 	 */
 	unsigned long		min_unmapped_pages;
 	unsigned long		min_slab_pages;
-	struct per_cpu_pageset	*pageset[NR_CPUS];
-#else
-	struct per_cpu_pageset	pageset[NR_CPUS];
 #endif
+	struct per_cpu_pageset	*pageset;
 	/*
 	 * free areas of different sizes
 	 */

commit 01fc0ac198eabcbf460e1ed058860a935b6c2c9a
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Apr 19 21:57:19 2009 +0200

    kbuild: move bounds.h to include/generated
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6f7561730d88..30fe668c2542 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -15,7 +15,7 @@
 #include <linux/seqlock.h>
 #include <linux/nodemask.h>
 #include <linux/pageblock-flags.h>
-#include <linux/bounds.h>
+#include <generated/bounds.h>
 #include <asm/atomic.h>
 #include <asm/page.h>
 

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 652ef01be582..6f7561730d88 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -755,21 +755,20 @@ static inline int is_dma(struct zone *zone)
 
 /* These two functions are used to setup the per zone pages min values */
 struct ctl_table;
-struct file;
-int min_free_kbytes_sysctl_handler(struct ctl_table *, int, struct file *, 
+int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1];
-int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
+int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
-int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int, struct file *,
+int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
 int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
-			struct file *, void __user *, size_t *, loff_t *);
+			void __user *, size_t *, loff_t *);
 int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
-			struct file *, void __user *, size_t *, loff_t *);
+			void __user *, size_t *, loff_t *);
 
 extern int numa_zonelist_order_handler(struct ctl_table *, int,
-			struct file *, void __user *, size_t *, loff_t *);
+			void __user *, size_t *, loff_t *);
 extern char numa_zonelist_order[];
 #define NUMA_ZONELIST_ORDER_LEN 16	/* string buffer size */
 

commit 5f8dcc21211a3d4e3a7a5ca366b469fb88117f61
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Sep 21 17:03:19 2009 -0700

    page-allocator: split per-cpu list into one-list-per-migrate-type
    
    The following two patches remove searching in the page allocator fast-path
    by maintaining multiple free-lists in the per-cpu structure.  At the time
    the search was introduced, increasing the per-cpu structures would waste a
    lot of memory as per-cpu structures were statically allocated at
    compile-time.  This is no longer the case.
    
    The patches are as follows. They are based on mmotm-2009-08-27.
    
    Patch 1 adds multiple lists to struct per_cpu_pages, one per
            migratetype that can be stored on the PCP lists.
    
    Patch 2 notes that the pcpu drain path check empty lists multiple times. The
            patch reduces the number of checks by maintaining a count of free
            lists encountered. Lists containing pages will then free multiple
            pages in batch
    
    The patches were tested with kernbench, netperf udp/tcp, hackbench and
    sysbench.  The netperf tests were not bound to any CPU in particular and
    were run such that the results should be 99% confidence that the reported
    results are within 1% of the estimated mean.  sysbench was run with a
    postgres background and read-only tests.  Similar to netperf, it was run
    multiple times so that it's 99% confidence results are within 1%.  The
    patches were tested on x86, x86-64 and ppc64 as
    
    x86:    Intel Pentium D 3GHz with 8G RAM (no-brand machine)
            kernbench       - No significant difference, variance well within noise
            netperf-udp     - 1.34% to 2.28% gain
            netperf-tcp     - 0.45% to 1.22% gain
            hackbench       - Small variances, very close to noise
            sysbench        - Very small gains
    
    x86-64: AMD Phenom 9950 1.3GHz with 8G RAM (no-brand machine)
            kernbench       - No significant difference, variance well within noise
            netperf-udp     - 1.83% to 10.42% gains
            netperf-tcp     - No conclusive until buffer >= PAGE_SIZE
                                    4096    +15.83%
                                    8192    + 0.34% (not significant)
                                    16384   + 1%
            hackbench       - Small gains, very close to noise
            sysbench        - 0.79% to 1.6% gain
    
    ppc64:  PPC970MP 2.5GHz with 10GB RAM (it's a terrasoft powerstation)
            kernbench       - No significant difference, variance well within noise
            netperf-udp     - 2-3% gain for almost all buffer sizes tested
            netperf-tcp     - losses on small buffers, gains on larger buffers
                              possibly indicates some bad caching effect.
            hackbench       - No significant difference
            sysbench        - 2-4% gain
    
    This patch:
    
    Currently the per-cpu page allocator searches the PCP list for pages of
    the correct migrate-type to reduce the possibility of pages being
    inappropriate placed from a fragmentation perspective.  This search is
    potentially expensive in a fast-path and undesirable.  Splitting the
    per-cpu list into multiple lists increases the size of a per-cpu structure
    and this was potentially a major problem at the time the search was
    introduced.  These problem has been mitigated as now only the necessary
    number of structures is allocated for the running system.
    
    This patch replaces a list search in the per-cpu allocator with one list
    per migrate type.  The potential snag with this approach is when bulk
    freeing pages.  We round-robin free pages based on migrate type which has
    little bearing on the cache hotness of the page and potentially checks
    empty lists repeatedly in the event the majority of PCP pages are of one
    type.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Nick Piggin <npiggin@suse.de>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c188ea624c74..652ef01be582 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -38,6 +38,7 @@
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
+#define MIGRATE_PCPTYPES      3 /* the number of types on the pcp lists */
 #define MIGRATE_RESERVE       3
 #define MIGRATE_ISOLATE       4 /* can't allocate from here */
 #define MIGRATE_TYPES         5
@@ -169,7 +170,9 @@ struct per_cpu_pages {
 	int count;		/* number of pages in the list */
 	int high;		/* high watermark, emptying needed */
 	int batch;		/* chunk size for buddy add/remove */
-	struct list_head list;	/* the list of pages */
+
+	/* Lists of pages, one per migrate type stored on the pcp-lists */
+	struct list_head lists[MIGRATE_PCPTYPES];
 };
 
 struct per_cpu_pageset {

commit f86296317434b21585e229f6c49a33cb9ebab4d3
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Mon Sep 21 17:03:11 2009 -0700

    mm: do batched scans for mem_cgroup
    
    For mem_cgroup, shrink_zone() may call shrink_list() with nr_to_scan=1, in
    which case shrink_list() _still_ calls isolate_pages() with the much
    larger SWAP_CLUSTER_MAX.  It effectively scales up the inactive list scan
    rate by up to 32 times.
    
    For example, with 16k inactive pages and DEF_PRIORITY=12, (16k >> 12)=4.
    So when shrink_zone() expects to scan 4 pages in the active/inactive list,
    the active list will be scanned 4 pages, while the inactive list will be
    (over) scanned SWAP_CLUSTER_MAX=32 pages in effect.  And that could break
    the balance between the two lists.
    
    It can further impact the scan of anon active list, due to the anon
    active/inactive ratio rebalance logic in balance_pgdat()/shrink_zone():
    
    inactive anon list over scanned => inactive_anon_is_low() == TRUE
                                    => shrink_active_list()
                                    => active anon list over scanned
    
    So the end result may be
    
    - anon inactive  => over scanned
    - anon active    => over scanned (maybe not as much)
    - file inactive  => over scanned
    - file active    => under scanned (relatively)
    
    The accesses to nr_saved_scan are not lock protected and so not 100%
    accurate, however we can tolerate small errors and the resulted small
    imbalanced scan rates between zones.
    
    Cc: Rik van Riel <riel@redhat.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9c50309b30a1..c188ea624c74 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -273,6 +273,11 @@ struct zone_reclaim_stat {
 	 */
 	unsigned long		recent_rotated[2];
 	unsigned long		recent_scanned[2];
+
+	/*
+	 * accumulated for batching
+	 */
+	unsigned long		nr_saved_scan[NR_LRU_LISTS];
 };
 
 struct zone {
@@ -327,7 +332,6 @@ struct zone {
 	spinlock_t		lru_lock;	
 	struct zone_lru {
 		struct list_head list;
-		unsigned long nr_saved_scan;	/* accumulated for batching */
 	} lru[NR_LRU_LISTS];
 
 	struct zone_reclaim_stat reclaim_stat;

commit a731286de62294b63d8ceb3c5914ac52cc17e690
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:01:37 2009 -0700

    mm: vmstat: add isolate pages
    
    If the system is running a heavy load of processes then concurrent reclaim
    can isolate a large number of pages from the LRU. /proc/vmstat and the
    output generated for an OOM do not show how many pages were isolated.
    
    This has been observed during process fork bomb testing (mstctl11 in LTP).
    
    This patch shows the information about isolated pages.
    
    Reproduced via:
    
    -----------------------
    % ./hackbench 140 process 1000
       => OOM occur
    
    active_anon:146 inactive_anon:0 isolated_anon:49245
     active_file:79 inactive_file:18 isolated_file:113
     unevictable:0 dirty:0 writeback:0 unstable:0 buffer:39
     free:370 slab_reclaimable:309 slab_unreclaimable:5492
     mapped:53 shmem:15 pagetables:28140 bounce:0
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b3583b93b77e..9c50309b30a1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -100,6 +100,8 @@ enum zone_stat_item {
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
+	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
+	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */

commit 4b02108ac1b3354a22b0d83c684797692efdc395
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:01:33 2009 -0700

    mm: oom analysis: add shmem vmstat
    
    Recently we encountered OOM problems due to memory use of the GEM cache.
    Generally a large amuont of Shmem/Tmpfs pages tend to create a memory
    shortage problem.
    
    We often use the following calculation to determine the amount of shmem
    pages:
    
    shmem = NR_ACTIVE_ANON + NR_INACTIVE_ANON - NR_ANON_PAGES
    
    however the expression does not consider isolated and mlocked pages.
    
    This patch adds explicit accounting for pages used by shmem and tmpfs.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Wu Fengguang <fengguang.wu@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d9335b8de84a..b3583b93b77e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -100,6 +100,7 @@ enum zone_stat_item {
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
+	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */

commit c6a7f5728a1db45d30df55a01adc130b4ab0327c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:01:32 2009 -0700

    mm: oom analysis: Show kernel stack usage in /proc/meminfo and OOM log output
    
    The amount of memory allocated to kernel stacks can become significant and
    cause OOM conditions.  However, we do not display the amount of memory
    consumed by stacks.
    
    Add code to display the amount of memory used for stacks in /proc/meminfo.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 889598537370..d9335b8de84a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -94,10 +94,11 @@ enum zone_stat_item {
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
+	NR_KERNEL_STACK,
+	/* Second 128 byte cacheline */
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
-	/* Second 128 byte cacheline */
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */

commit 6837765963f1723e80ca97b1fae660f3a60d77df
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jun 16 15:32:51 2009 -0700

    mm: remove CONFIG_UNEVICTABLE_LRU config option
    
    Currently, nobody wants to turn UNEVICTABLE_LRU off.  Thus this
    configurability is unnecessary.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Acked-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index db976b9f8791..889598537370 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -83,13 +83,8 @@ enum zone_stat_item {
 	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
-#ifdef CONFIG_UNEVICTABLE_LRU
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
-#else
-	NR_UNEVICTABLE = NR_ACTIVE_FILE, /* avoid compiler errors in dead code */
-	NR_MLOCK = NR_ACTIVE_FILE,
-#endif
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
@@ -132,11 +127,7 @@ enum lru_list {
 	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
-#ifdef CONFIG_UNEVICTABLE_LRU
 	LRU_UNEVICTABLE,
-#else
-	LRU_UNEVICTABLE = LRU_ACTIVE_FILE, /* avoid compiler errors in dead code */
-#endif
 	NR_LRU_LISTS
 };
 
@@ -156,11 +147,7 @@ static inline int is_active_lru(enum lru_list l)
 
 static inline int is_unevictable_lru(enum lru_list l)
 {
-#ifdef CONFIG_UNEVICTABLE_LRU
 	return (l == LRU_UNEVICTABLE);
-#else
-	return 0;
-#endif
 }
 
 enum zone_watermarks {

commit 6e08a369ee10b361ac1cdcdf4fabd420fd08beb3
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Jun 16 15:32:29 2009 -0700

    vmscan: cleanup the scan batching code
    
    The vmscan batching logic is twisting.  Move it into a standalone function
    nr_scan_try_batch() and document it.  No behavior change.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dd8487f0442f..db976b9f8791 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -334,9 +334,9 @@ struct zone {
 
 	/* Fields commonly accessed by the page reclaim scanner */
 	spinlock_t		lru_lock;	
-	struct {
+	struct zone_lru {
 		struct list_head list;
-		unsigned long nr_scan;
+		unsigned long nr_saved_scan;	/* accumulated for batching */
 	} lru[NR_LRU_LISTS];
 
 	struct zone_reclaim_stat reclaim_stat;

commit 418589663d6011de9006425b6c5721e1544fb47a
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jun 16 15:32:12 2009 -0700

    page allocator: use allocation flags as an index to the zone watermark
    
    ALLOC_WMARK_MIN, ALLOC_WMARK_LOW and ALLOC_WMARK_HIGH determin whether
    pages_min, pages_low or pages_high is used as the zone watermark when
    allocating the pages.  Two branches in the allocator hotpath determine
    which watermark to use.
    
    This patch uses the flags as an array index into a watermark array that is
    indexed with WMARK_* defines accessed via helpers.  All call sites that
    use zone->pages_* are updated to use the helpers for accessing the values
    and the array offsets for setting.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0aa4445b0b8a..dd8487f0442f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -163,6 +163,17 @@ static inline int is_unevictable_lru(enum lru_list l)
 #endif
 }
 
+enum zone_watermarks {
+	WMARK_MIN,
+	WMARK_LOW,
+	WMARK_HIGH,
+	NR_WMARK
+};
+
+#define min_wmark_pages(z) (z->watermark[WMARK_MIN])
+#define low_wmark_pages(z) (z->watermark[WMARK_LOW])
+#define high_wmark_pages(z) (z->watermark[WMARK_HIGH])
+
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
 	int high;		/* high watermark, emptying needed */
@@ -275,7 +286,10 @@ struct zone_reclaim_stat {
 
 struct zone {
 	/* Fields commonly accessed by the page allocator */
-	unsigned long		pages_min, pages_low, pages_high;
+
+	/* zone watermarks, access with *_wmark_pages(zone) macros */
+	unsigned long watermark[NR_WMARK];
+
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several

commit 49255c619fbd482d704289b5eb2795f8e3b7ff2e
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jun 16 15:31:58 2009 -0700

    page allocator: move check for disabled anti-fragmentation out of fastpath
    
    On low-memory systems, anti-fragmentation gets disabled as there is
    nothing it can do and it would just incur overhead shuffling pages between
    lists constantly.  Currently the check is made in the free page fast path
    for every page.  This patch moves it to a slow path.  On machines with low
    memory, there will be small amount of additional overhead as pages get
    shuffled between lists but it should quickly settle.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a47c879e1304..0aa4445b0b8a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -50,9 +50,6 @@ extern int page_group_by_mobility_disabled;
 
 static inline int get_pageblock_migratetype(struct page *page)
 {
-	if (unlikely(page_group_by_mobility_disabled))
-		return MIGRATE_UNMOVABLE;
-
 	return get_pageblock_flags_group(page, PB_migrate, PB_migrate_end);
 }
 

commit eb33575cf67d3f35fa2510210ef92631266e2465
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed May 13 17:34:48 2009 +0100

    [ARM] Double check memmap is actually valid with a memmap has unexpected holes V2
    
    pfn_valid() is meant to be able to tell if a given PFN has valid memmap
    associated with it or not. In FLATMEM, it is expected that holes always
    have valid memmap as long as there is valid PFNs either side of the hole.
    In SPARSEMEM, it is assumed that a valid section has a memmap for the
    entire section.
    
    However, ARM and maybe other embedded architectures in the future free
    memmap backing holes to save memory on the assumption the memmap is never
    used. The page_zone linkages are then broken even though pfn_valid()
    returns true. A walker of the full memmap must then do this additional
    check to ensure the memmap they are looking at is sane by making sure the
    zone and PFN linkages are still valid. This is expensive, but walkers of
    the full memmap are extremely rare.
    
    This was caught before for FLATMEM and hacked around but it hits again for
    SPARSEMEM because the page_zone linkages can look ok where the PFN linkages
    are totally screwed. This looks like a hatchet job but the reality is that
    any clean solution would end up consumning all the memory saved by punching
    these unexpected holes in the memmap. For example, we tried marking the
    memmap within the section invalid but the section size exceeds the size of
    the hole in most cases so pfn_valid() starts returning false where valid
    memmap exists. Shrinking the size of the section would increase memory
    consumption offsetting the gains.
    
    This patch identifies when an architecture is punching unexpected holes
    in the memmap that the memory model cannot automatically detect and sets
    ARCH_HAS_HOLES_MEMORYMODEL. At the moment, this is restricted to EP93xx
    which is the model sub-architecture this has been reported on but may expand
    later. When set, walkers of the full memmap must call memmap_valid_within()
    for each PFN and passing in what it expects the page and zone to be for
    that PFN. If it finds the linkages to be broken, it assumes the memmap is
    invalid for that PFN.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 186ec6ab334d..a47c879e1304 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1097,6 +1097,32 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 #define pfn_valid_within(pfn) (1)
 #endif
 
+#ifdef CONFIG_ARCH_HAS_HOLES_MEMORYMODEL
+/*
+ * pfn_valid() is meant to be able to tell if a given PFN has valid memmap
+ * associated with it or not. In FLATMEM, it is expected that holes always
+ * have valid memmap as long as there is valid PFNs either side of the hole.
+ * In SPARSEMEM, it is assumed that a valid section has a memmap for the
+ * entire section.
+ *
+ * However, an ARM, and maybe other embedded architectures in the future
+ * free memmap backing holes to save memory on the assumption the memmap is
+ * never used. The page_zone linkages are then broken even though pfn_valid()
+ * returns true. A walker of the full memmap must then do this additional
+ * check to ensure the memmap they are looking at is sane by making sure
+ * the zone and PFN linkages are still valid. This is expensive, but walkers
+ * of the full memmap are extremely rare.
+ */
+int memmap_valid_within(unsigned long pfn,
+					struct page *page, struct zone *zone);
+#else
+static inline int memmap_valid_within(unsigned long pfn,
+					struct page *page, struct zone *zone)
+{
+	return 1;
+}
+#endif /* CONFIG_ARCH_HAS_HOLES_MEMORYMODEL */
+
 #endif /* !__GENERATING_BOUNDS.H */
 #endif /* !__ASSEMBLY__ */
 #endif /* _LINUX_MMZONE_H */

commit 90975ef71246c5c688ead04e8ff6f36dc92d28b3
Merge: cab4e4c43f92 558f6ab9106e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 5 10:33:07 2009 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux-2.6-cpumask
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux-2.6-cpumask: (36 commits)
      cpumask: remove cpumask allocation from idle_balance, fix
      numa, cpumask: move numa_node_id default implementation to topology.h, fix
      cpumask: remove cpumask allocation from idle_balance
      x86: cpumask: x86 mmio-mod.c use cpumask_var_t for downed_cpus
      x86: cpumask: update 32-bit APM not to mug current->cpus_allowed
      x86: microcode: cleanup
      x86: cpumask: use work_on_cpu in arch/x86/kernel/microcode_core.c
      cpumask: fix CONFIG_CPUMASK_OFFSTACK=y cpu hotunplug crash
      numa, cpumask: move numa_node_id default implementation to topology.h
      cpumask: convert node_to_cpumask_map[] to cpumask_var_t
      cpumask: remove x86 cpumask_t uses.
      cpumask: use cpumask_var_t in uv_flush_tlb_others.
      cpumask: remove cpumask_t assignment from vector_allocation_domain()
      cpumask: make Xen use the new operators.
      cpumask: clean up summit's send_IPI functions
      cpumask: use new cpumask functions throughout x86
      x86: unify cpu_callin_mask/cpu_callout_mask/cpu_initialized_mask/cpu_sibling_setup_mask
      cpumask: convert struct cpuinfo_x86's llc_shared_map to cpumask_var_t
      cpumask: convert node_to_cpumask_map[] to cpumask_var_t
      x86: unify 32 and 64-bit node_to_cpumask_map
      ...

commit ee99c71c59f897436ec65debb99372b3146f9985
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Mar 31 15:19:31 2009 -0700

    mm: introduce for_each_populated_zone() macro
    
    Impact: cleanup
    
    In almost cases, for_each_zone() is used with populated_zone().  It's
    because almost function doesn't need memoryless node information.
    Therefore, for_each_populated_zone() can help to make code simplify.
    
    This patch has no functional change.
    
    [akpm@linux-foundation.org: small cleanup]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1aca6cebbb78..26ef24076b76 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -806,6 +806,14 @@ extern struct zone *next_zone(struct zone *zone);
 	     zone;					\
 	     zone = next_zone(zone))
 
+#define for_each_populated_zone(zone)		        \
+	for (zone = (first_online_pgdat())->node_zones; \
+	     zone;					\
+	     zone = next_zone(zone))			\
+		if (!populated_zone(zone))		\
+			; /* do nothing */		\
+		else
+
 static inline struct zone *zonelist_zone(struct zoneref *zoneref)
 {
 	return zoneref->zone;

commit 082edb7bf443eb8eda15b482d16ad9dd8137ad24
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 13 23:43:37 2009 +1030

    numa, cpumask: move numa_node_id default implementation to topology.h
    
    Impact: cleanup, potential bugfix
    
    Not sure what changed to expose this, but clearly that numa_node_id()
    doesn't belong in mmzone.h (the inline in gfp.h is probably overkill, too).
    
    In file included from include/linux/topology.h:34,
                     from arch/x86/mm/numa.c:2:
    /home/rusty/patches-cpumask/linux-2.6/arch/x86/include/asm/topology.h:64:1: warning: "numa_node_id" redefined
    In file included from include/linux/topology.h:32,
                     from arch/x86/mm/numa.c:2:
    include/linux/mmzone.h:770:1: warning: this is the location of the previous definition
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Travis <travis@sgi.com>
    LKML-Reference: <200903132343.37661.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1aca6cebbb78..e6aacf77986a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -764,12 +764,6 @@ extern int numa_zonelist_order_handler(struct ctl_table *, int,
 extern char numa_zonelist_order[];
 #define NUMA_ZONELIST_ORDER_LEN 16	/* string buffer size */
 
-#include <linux/topology.h>
-/* Returns the number of the current Node. */
-#ifndef numa_node_id
-#define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
-#endif
-
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 
 extern struct pglist_data contig_page_data;

commit cc2559bccc72767cb446f79b071d96c30c26439b
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Wed Feb 18 14:48:33 2009 -0800

    mm: fix memmap init for handling memory hole
    
    Now, early_pfn_in_nid(PFN, NID) may returns false if PFN is a hole.
    and memmap initialization was not done. This was a trouble for
    sparc boot.
    
    To fix this, the PFN should be initialized and marked as PG_reserved.
    This patch changes early_pfn_in_nid() return true if PFN is a hole.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reported-by: David Miller <davem@davemlloft.net>
    Tested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: <stable@kernel.org>         [2.6.25.x, 2.6.26.x, 2.6.27.x, 2.6.28.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 09c14e213b63..1aca6cebbb78 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1071,7 +1071,7 @@ void sparse_init(void);
 #endif /* CONFIG_SPARSEMEM */
 
 #ifdef CONFIG_NODES_SPAN_OTHER_NODES
-#define early_pfn_in_nid(pfn, nid)	(early_pfn_to_nid(pfn) == (nid))
+bool early_pfn_in_nid(unsigned long pfn, int nid);
 #else
 #define early_pfn_in_nid(pfn, nid)	(1)
 #endif

commit 6e9015716ae9b59e9635d692fddfcfb9582c146c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Jan 7 18:08:15 2009 -0800

    mm: introduce zone_reclaim struct
    
    Add zone_reclam_stat struct for later enhancement.
    
    A later patch uses this.  This patch doesn't any behavior change (yet).
    
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 35a7b5e19465..09c14e213b63 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -263,6 +263,19 @@ enum zone_type {
 #error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
 
+struct zone_reclaim_stat {
+	/*
+	 * The pageout code in vmscan.c keeps track of how many of the
+	 * mem/swap backed and file backed pages are refeferenced.
+	 * The higher the rotated/scanned ratio, the more valuable
+	 * that cache is.
+	 *
+	 * The anon LRU stats live in [0], file LRU stats in [1]
+	 */
+	unsigned long		recent_rotated[2];
+	unsigned long		recent_scanned[2];
+};
+
 struct zone {
 	/* Fields commonly accessed by the page allocator */
 	unsigned long		pages_min, pages_low, pages_high;
@@ -315,16 +328,7 @@ struct zone {
 		unsigned long nr_scan;
 	} lru[NR_LRU_LISTS];
 
-	/*
-	 * The pageout code in vmscan.c keeps track of how many of the
-	 * mem/swap backed and file backed pages are refeferenced.
-	 * The higher the rotated/scanned ratio, the more valuable
-	 * that cache is.
-	 *
-	 * The anon LRU stats live in [0], file LRU stats in [1]
-	 */
-	unsigned long		recent_rotated[2];
-	unsigned long		recent_scanned[2];
+	struct zone_reclaim_stat reclaim_stat;
 
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */

commit 52d4b9ac0b985168009c2a57098324e67bae171f
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Sat Oct 18 20:28:16 2008 -0700

    memcg: allocate all page_cgroup at boot
    
    Allocate all page_cgroup at boot and remove page_cgroup poitner from
    struct page.  This patch adds an interface as
    
     struct page_cgroup *lookup_page_cgroup(struct page*)
    
    All FLATMEM/DISCONTIGMEM/SPARSEMEM  and MEMORY_HOTPLUG is supported.
    
    Remove page_cgroup pointer reduces the amount of memory by
     - 4 bytes per PAGE_SIZE.
     - 8 bytes per PAGE_SIZE
    if memory controller is disabled. (even if configured.)
    
    On usual 8GB x86-32 server, this saves 8MB of NORMAL_ZONE memory.
    On my x86-64 server with 48GB of memory, this saves 96MB of memory.
    I think this reduction makes sense.
    
    By pre-allocation, kmalloc/kfree in charge/uncharge are removed.
    This means
      - we're not necessary to be afraid of kmalloc faiulre.
        (this can happen because of gfp_mask type.)
      - we can avoid calling kmalloc/kfree.
      - we can avoid allocating tons of small objects which can be fragmented.
      - we can know what amount of memory will be used for this extra-lru handling.
    
    I added printk message as
    
            "allocated %ld bytes of page_cgroup"
            "please try cgroup_disable=memory option if you don't want"
    
    maybe enough informative for users.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index da2d053a95f1..35a7b5e19465 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -601,8 +601,11 @@ typedef struct pglist_data {
 	struct zone node_zones[MAX_NR_ZONES];
 	struct zonelist node_zonelists[MAX_ZONELISTS];
 	int nr_zones;
-#ifdef CONFIG_FLAT_NODE_MEM_MAP
+#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
 	struct page *node_mem_map;
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+	struct page_cgroup *node_page_cgroup;
+#endif
 #endif
 	struct bootmem_data *bdata;
 #ifdef CONFIG_MEMORY_HOTPLUG
@@ -931,6 +934,7 @@ static inline unsigned long early_pfn_to_nid(unsigned long pfn)
 #endif
 
 struct page;
+struct page_cgroup;
 struct mem_section {
 	/*
 	 * This is, logically, a pointer to an array of struct
@@ -948,6 +952,14 @@ struct mem_section {
 
 	/* See declaration of similar field in struct zone */
 	unsigned long *pageblock_flags;
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+	/*
+	 * If !SPARSEMEM, pgdat doesn't have page_cgroup pointer. We use
+	 * section. (see memcontrol.h/page_cgroup.h about this.)
+	 */
+	struct page_cgroup *page_cgroup;
+	unsigned long pad;
+#endif
 };
 
 #ifdef CONFIG_SPARSEMEM_EXTREME

commit 5344b7e648980cc2ca613ec03a56a8222ff48820
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:26:51 2008 -0700

    vmstat: mlocked pages statistics
    
    Add NR_MLOCK zone page state, which provides a (conservative) count of
    mlocked pages (actually, the number of mlocked pages moved off the LRU).
    
    Reworked by lts to fit in with the modified mlock page support in the
    Reclaim Scalability series.
    
    [kosaki.motohiro@jp.fujitsu.com: fix incorrect Mlocked field of /proc/meminfo]
    [lee.schermerhorn@hp.com: mlocked-pages: add event counting with statistics]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d1f60d5fe2ea..da2d053a95f1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -88,8 +88,10 @@ enum zone_stat_item {
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
 #ifdef CONFIG_UNEVICTABLE_LRU
 	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
+	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 #else
 	NR_UNEVICTABLE = NR_ACTIVE_FILE, /* avoid compiler errors in dead code */
+	NR_MLOCK = NR_ACTIVE_FILE,
 #endif
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.

commit 894bc310419ac95f4fa4142dc364401a7e607f65
Author: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Date:   Sat Oct 18 20:26:39 2008 -0700

    Unevictable LRU Infrastructure
    
    When the system contains lots of mlocked or otherwise unevictable pages,
    the pageout code (kswapd) can spend lots of time scanning over these
    pages.  Worse still, the presence of lots of unevictable pages can confuse
    kswapd into thinking that more aggressive pageout modes are required,
    resulting in all kinds of bad behaviour.
    
    Infrastructure to manage pages excluded from reclaim--i.e., hidden from
    vmscan.  Based on a patch by Larry Woodman of Red Hat.  Reworked to
    maintain "unevictable" pages on a separate per-zone LRU list, to "hide"
    them from vmscan.
    
    Kosaki Motohiro added the support for the memory controller unevictable
    lru list.
    
    Pages on the unevictable list have both PG_unevictable and PG_lru set.
    Thus, PG_unevictable is analogous to and mutually exclusive with
    PG_active--it specifies which LRU list the page is on.
    
    The unevictable infrastructure is enabled by a new mm Kconfig option
    [CONFIG_]UNEVICTABLE_LRU.
    
    A new function 'page_evictable(page, vma)' in vmscan.c tests whether or
    not a page may be evictable.  Subsequent patches will add the various
    !evictable tests.  We'll want to keep these tests light-weight for use in
    shrink_active_list() and, possibly, the fault path.
    
    To avoid races between tasks putting pages [back] onto an LRU list and
    tasks that might be moving the page from non-evictable to evictable state,
    the new function 'putback_lru_page()' -- inverse to 'isolate_lru_page()'
    -- tests the "evictability" of a page after placing it on the LRU, before
    dropping the reference.  If the page has become unevictable,
    putback_lru_page() will redo the 'putback', thus moving the page to the
    unevictable list.  This way, we avoid "stranding" evictable pages on the
    unevictable list.
    
    [akpm@linux-foundation.org: fix fallout from out-of-order merge]
    [riel@redhat.com: fix UNEVICTABLE_LRU and !PROC_PAGE_MONITOR build]
    [nishimura@mxp.nes.nec.co.jp: remove redundant mapping check]
    [kosaki.motohiro@jp.fujitsu.com: unevictable-lru-infrastructure: putback_lru_page()/unevictable page handling rework]
    [kosaki.motohiro@jp.fujitsu.com: kill unnecessary lock_page() in vmscan.c]
    [kosaki.motohiro@jp.fujitsu.com: revert migration change of unevictable lru infrastructure]
    [kosaki.motohiro@jp.fujitsu.com: revert to unevictable-lru-infrastructure-kconfig-fix.patch]
    [kosaki.motohiro@jp.fujitsu.com: restore patch failure of vmstat-unevictable-and-mlocked-pages-vm-events.patch]
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Debugged-by: Benjamin Kidwell <benjkidwell@yahoo.com>
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9c5111f49a32..d1f60d5fe2ea 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -86,6 +86,11 @@ enum zone_stat_item {
 	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
 	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
 	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
+#ifdef CONFIG_UNEVICTABLE_LRU
+	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
+#else
+	NR_UNEVICTABLE = NR_ACTIVE_FILE, /* avoid compiler errors in dead code */
+#endif
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
@@ -128,10 +133,18 @@ enum lru_list {
 	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
-	NR_LRU_LISTS };
+#ifdef CONFIG_UNEVICTABLE_LRU
+	LRU_UNEVICTABLE,
+#else
+	LRU_UNEVICTABLE = LRU_ACTIVE_FILE, /* avoid compiler errors in dead code */
+#endif
+	NR_LRU_LISTS
+};
 
 #define for_each_lru(l) for (l = 0; l < NR_LRU_LISTS; l++)
 
+#define for_each_evictable_lru(l) for (l = 0; l <= LRU_ACTIVE_FILE; l++)
+
 static inline int is_file_lru(enum lru_list l)
 {
 	return (l == LRU_INACTIVE_FILE || l == LRU_ACTIVE_FILE);
@@ -142,6 +155,15 @@ static inline int is_active_lru(enum lru_list l)
 	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE);
 }
 
+static inline int is_unevictable_lru(enum lru_list l)
+{
+#ifdef CONFIG_UNEVICTABLE_LRU
+	return (l == LRU_UNEVICTABLE);
+#else
+	return 0;
+#endif
+}
+
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
 	int high;		/* high watermark, emptying needed */

commit 556adecba110bf5f1db6c6b56416cfab5bcab698
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Oct 18 20:26:34 2008 -0700

    vmscan: second chance replacement for anonymous pages
    
    We avoid evicting and scanning anonymous pages for the most part, but
    under some workloads we can end up with most of memory filled with
    anonymous pages.  At that point, we suddenly need to clear the referenced
    bits on all of memory, which can take ages on very large memory systems.
    
    We can reduce the maximum number of pages that need to be scanned by not
    taking the referenced state into account when deactivating an anonymous
    page.  After all, every anonymous page starts out referenced, so why
    check?
    
    If an anonymous page gets referenced again before it reaches the end of
    the inactive list, we move it back to the active list.
    
    To keep the maximum amount of necessary work reasonable, we scale the
    active to inactive ratio with the size of memory, using the formula
    active:inactive ratio = sqrt(memory in GB * 10).
    
    Kswapd CPU use now seems to scale by the amount of pageout bandwidth,
    instead of by the amount of memory present in the system.
    
    [kamezawa.hiroyu@jp.fujitsu.com: fix OOM with memcg]
    [kamezawa.hiroyu@jp.fujitsu.com: memcg: lru scan fix]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 59a4c8fd6ebd..9c5111f49a32 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -323,6 +323,12 @@ struct zone {
 	 */
 	int prev_priority;
 
+	/*
+	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
+	 * this zone's LRU.  Maintained by the pageout code.
+	 */
+	unsigned int inactive_ratio;
+
 
 	ZONE_PADDING(_pad2_)
 	/* Rarely used or read-mostly fields */

commit 4f98a2fee8acdb4ac84545df98cccecfd130f8db
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Oct 18 20:26:32 2008 -0700

    vmscan: split LRU lists into anon & file sets
    
    Split the LRU lists in two, one set for pages that are backed by real file
    systems ("file") and one for pages that are backed by memory and swap
    ("anon").  The latter includes tmpfs.
    
    The advantage of doing this is that the VM will not have to scan over lots
    of anonymous pages (which we generally do not want to swap out), just to
    find the page cache pages that it should evict.
    
    This patch has the infrastructure and a basic policy to balance how much
    we scan the anon lists and how much we scan the file lists.  The big
    policy changes are in separate patches.
    
    [lee.schermerhorn@hp.com: collect lru meminfo statistics from correct offset]
    [kosaki.motohiro@jp.fujitsu.com: prevent incorrect oom under split_lru]
    [kosaki.motohiro@jp.fujitsu.com: fix pagevec_move_tail() doesn't treat unevictable page]
    [hugh@veritas.com: memcg swapbacked pages active]
    [hugh@veritas.com: splitlru: BDI_CAP_SWAP_BACKED]
    [akpm@linux-foundation.org: fix /proc/vmstat units]
    [nishimura@mxp.nes.nec.co.jp: memcg: fix handling of shmem migration]
    [kosaki.motohiro@jp.fujitsu.com: adjust Quicklists field of /proc/meminfo]
    [kosaki.motohiro@jp.fujitsu.com: fix style issue of get_scan_ratio()]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 156e18f3919b..59a4c8fd6ebd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -82,21 +82,23 @@ enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
 	NR_LRU_BASE,
-	NR_INACTIVE = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
-	NR_ACTIVE,	/*  "     "     "   "       "         */
+	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
+	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
+	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
+	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
-	/* Second 128 byte cacheline */
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
+	/* Second 128 byte cacheline */
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
@@ -108,17 +110,36 @@ enum zone_stat_item {
 #endif
 	NR_VM_ZONE_STAT_ITEMS };
 
+/*
+ * We do arithmetic on the LRU lists in various places in the code,
+ * so it is important to keep the active lists LRU_ACTIVE higher in
+ * the array than the corresponding inactive lists, and to keep
+ * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
+ *
+ * This has to be kept in sync with the statistics in zone_stat_item
+ * above and the descriptions in vmstat_text in mm/vmstat.c
+ */
+#define LRU_BASE 0
+#define LRU_ACTIVE 1
+#define LRU_FILE 2
+
 enum lru_list {
-	LRU_BASE,
-	LRU_INACTIVE=LRU_BASE,	/* must match order of NR_[IN]ACTIVE */
-	LRU_ACTIVE,		/*  "     "     "   "       "        */
+	LRU_INACTIVE_ANON = LRU_BASE,
+	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
+	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
+	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
 	NR_LRU_LISTS };
 
 #define for_each_lru(l) for (l = 0; l < NR_LRU_LISTS; l++)
 
+static inline int is_file_lru(enum lru_list l)
+{
+	return (l == LRU_INACTIVE_FILE || l == LRU_ACTIVE_FILE);
+}
+
 static inline int is_active_lru(enum lru_list l)
 {
-	return (l == LRU_ACTIVE);
+	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE);
 }
 
 struct per_cpu_pages {
@@ -269,6 +290,18 @@ struct zone {
 		struct list_head list;
 		unsigned long nr_scan;
 	} lru[NR_LRU_LISTS];
+
+	/*
+	 * The pageout code in vmscan.c keeps track of how many of the
+	 * mem/swap backed and file backed pages are refeferenced.
+	 * The higher the rotated/scanned ratio, the more valuable
+	 * that cache is.
+	 *
+	 * The anon LRU stats live in [0], file LRU stats in [1]
+	 */
+	unsigned long		recent_rotated[2];
+	unsigned long		recent_scanned[2];
+
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */
 

commit b69408e88bd86b98feb7b9a38fd865e1ddb29827
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Sat Oct 18 20:26:14 2008 -0700

    vmscan: Use an indexed array for LRU variables
    
    Currently we are defining explicit variables for the inactive and active
    list.  An indexed array can be more generic and avoid repeating similar
    code in several places in the reclaim code.
    
    We are saving a few bytes in terms of code size:
    
    Before:
    
       text    data     bss     dec     hex filename
    4097753  573120 4092484 8763357  85b7dd vmlinux
    
    After:
    
       text    data     bss     dec     hex filename
    4097729  573120 4092484 8763333  85b7c5 vmlinux
    
    Having an easy way to add new lru lists may ease future work on the
    reclaim code.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 428328a05fa1..156e18f3919b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -81,8 +81,9 @@ struct zone_padding {
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
-	NR_INACTIVE,
-	NR_ACTIVE,
+	NR_LRU_BASE,
+	NR_INACTIVE = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
+	NR_ACTIVE,	/*  "     "     "   "       "         */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
@@ -107,6 +108,19 @@ enum zone_stat_item {
 #endif
 	NR_VM_ZONE_STAT_ITEMS };
 
+enum lru_list {
+	LRU_BASE,
+	LRU_INACTIVE=LRU_BASE,	/* must match order of NR_[IN]ACTIVE */
+	LRU_ACTIVE,		/*  "     "     "   "       "        */
+	NR_LRU_LISTS };
+
+#define for_each_lru(l) for (l = 0; l < NR_LRU_LISTS; l++)
+
+static inline int is_active_lru(enum lru_list l)
+{
+	return (l == LRU_ACTIVE);
+}
+
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
 	int high;		/* high watermark, emptying needed */
@@ -251,10 +265,10 @@ struct zone {
 
 	/* Fields commonly accessed by the page reclaim scanner */
 	spinlock_t		lru_lock;	
-	struct list_head	active_list;
-	struct list_head	inactive_list;
-	unsigned long		nr_scan_active;
-	unsigned long		nr_scan_inactive;
+	struct {
+		struct list_head list;
+		unsigned long nr_scan;
+	} lru[NR_LRU_LISTS];
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */
 

commit 5bead2a0680687b9576d57c177988e8aa082b922
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Sat Sep 13 02:33:19 2008 -0700

    mm: mark the correct zone as full when scanning zonelists
    
    The iterator for_each_zone_zonelist() uses a struct zoneref *z cursor when
    scanning zonelists to keep track of where in the zonelist it is.  The
    zoneref that is returned corresponds to the the next zone that is to be
    scanned, not the current one.  It was intended to be treated as an opaque
    list.
    
    When the page allocator is scanning a zonelist, it marks elements in the
    zonelist corresponding to zones that are temporarily full.  As the
    zonelist is being updated, it uses the cursor here;
    
      if (NUMA_BUILD)
            zlc_mark_zone_full(zonelist, z);
    
    This is intended to prevent rescanning in the near future but the zoneref
    cursor does not correspond to the zone that has been found to be full.
    This is an easy misunderstanding to make so this patch corrects the
    problem by changing zoneref cursor to be the current zone being scanned
    instead of the next one.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: <stable@kernel.org>         [2.6.26.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 443bc7cd8c62..428328a05fa1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -751,8 +751,9 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
  *
  * This function returns the next zone at or below a given zone index that is
  * within the allowed nodemask using a cursor as the starting point for the
- * search. The zoneref returned is a cursor that is used as the next starting
- * point for future calls to next_zones_zonelist().
+ * search. The zoneref returned is a cursor that represents the current zone
+ * being examined. It should be advanced by one before calling
+ * next_zones_zonelist again.
  */
 struct zoneref *next_zones_zonelist(struct zoneref *z,
 					enum zone_type highest_zoneidx,
@@ -768,9 +769,8 @@ struct zoneref *next_zones_zonelist(struct zoneref *z,
  *
  * This function returns the first zone at or below a given zone index that is
  * within the allowed nodemask. The zoneref returned is a cursor that can be
- * used to iterate the zonelist with next_zones_zonelist. The cursor should
- * not be used by the caller as it does not match the value of the zone
- * returned.
+ * used to iterate the zonelist with next_zones_zonelist by advancing it by
+ * one before calling.
  */
 static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 					enum zone_type highest_zoneidx,
@@ -795,7 +795,7 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 #define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
 	for (z = first_zones_zonelist(zlist, highidx, nodemask, &zone);	\
 		zone;							\
-		z = next_zones_zonelist(z, highidx, nodemask, &zone))	\
+		z = next_zones_zonelist(++z, highidx, nodemask, &zone))	\
 
 /**
  * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index

commit 12d15f0d51d47cec39d1d7250e81573c5cbd8b5d
Author: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
Date:   Fri May 23 13:05:01 2008 -0700

    for_each_online_pgdat(): kerneldoc fix
    
    for_each_pgdat() was renamed to for_each_online_pgdat() and kerneldoc
    comments should be updated accordingly.
    
    Signed-off-by: Fernando Luis Vazquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c463cd8a15a4..443bc7cd8c62 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -703,7 +703,7 @@ extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
 extern struct zone *next_zone(struct zone *zone);
 
 /**
- * for_each_pgdat - helper macro to iterate over all nodes
+ * for_each_online_pgdat - helper macro to iterate over all online nodes
  * @pgdat - pointer to a pg_data_t variable
  */
 #define for_each_online_pgdat(pgdat)			\

commit 735643ee6cc5249bfac07fcad0946a5e7aff4423
Author: Robert P. J. Day <rpjday@crashcourse.ca>
Date:   Wed Apr 30 00:55:12 2008 -0700

    Remove "#ifdef __KERNEL__" checks from unexported headers
    
    Remove the "#ifdef __KERNEL__" tests from unexported header files in
    linux/include whose entire contents are wrapped in that preprocessor
    test.
    
    Signed-off-by: Robert P. J. Day <rpjday@crashcourse.ca>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ceb675d83a56..c463cd8a15a4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1,7 +1,6 @@
 #ifndef _LINUX_MMZONE_H
 #define _LINUX_MMZONE_H
 
-#ifdef __KERNEL__
 #ifndef __ASSEMBLY__
 #ifndef __GENERATING_BOUNDS_H
 
@@ -1005,5 +1004,4 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 
 #endif /* !__GENERATING_BOUNDS.H */
 #endif /* !__ASSEMBLY__ */
-#endif /* __KERNEL__ */
 #endif /* _LINUX_MMZONE_H */

commit fc3ba692a4d19019387c5acaea63131f9eab05dd
Author: Miklos Szeredi <mszeredi@suse.cz>
Date:   Wed Apr 30 00:54:38 2008 -0700

    mm: Add NR_WRITEBACK_TEMP counter
    
    Fuse will use temporary buffers to write back dirty data from memory mappings
    (normal writes are done synchronously).  This is needed, because there cannot
    be any guarantee about the time in which a write will complete.
    
    By using temporary buffers, from the MM's point if view the page is written
    back immediately.  If the writeout was due to memory pressure, this
    effectively migrates data from a full zone to a less full zone.
    
    This patch adds a new counter (NR_WRITEBACK_TEMP) for the number of pages used
    as temporary buffers.
    
    [Lee.Schermerhorn@hp.com: add vmstat_text for NR_WRITEBACK_TEMP]
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index aad98003176f..ceb675d83a56 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -97,6 +97,7 @@ enum zone_stat_item {
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
+	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */

commit 04753278769f3b6c3b79a080edb52f21d83bf6e2
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Mon Apr 28 02:13:31 2008 -0700

    memory hotplug: register section/node id to free
    
    This patch set is to free pages which is allocated by bootmem for
    memory-hotremove.  Some structures of memory management are allocated by
    bootmem.  ex) memmap, etc.
    
    To remove memory physically, some of them must be freed according to
    circumstance.  This patch set makes basis to free those pages, and free
    memmaps.
    
    Basic my idea is using remain members of struct page to remember information
    of users of bootmem (section number or node id).  When the section is
    removing, kernel can confirm it.  By this information, some issues can be
    solved.
    
      1) When the memmap of removing section is allocated on other
         section by bootmem, it should/can be free.
      2) When the memmap of removing section is allocated on the
         same section, it shouldn't be freed. Because the section has to be
         logical memory offlined already and all pages must be isolated against
         page allocater. If it is freed, page allocator may use it which will
         be removed physically soon.
      3) When removing section has other section's memmap,
         kernel will be able to show easily which section should be removed
         before it for user. (Not implemented yet)
      4) When the above case 2), the page isolation will be able to check and skip
         memmap's page when logical memory offline (offline_pages()).
         Current page isolation code fails in this case because this page is
         just reserved page and it can't distinguish this pages can be
         removed or not. But, it will be able to do by this patch.
         (Not implemented yet.)
      5) The node information like pgdat has similar issues. But, this
         will be able to be solved too by this.
         (Not implemented yet, but, remembering node id in the pages.)
    
    Fortunately, current bootmem allocator just keeps PageReserved flags,
    and doesn't use any other members of page struct. The users of
    bootmem doesn't use them too.
    
    This patch:
    
    This is to register information which is node or section's id.  Kernel can
    distinguish which node/section uses the pages allcated by bootmem.  This is
    basis for hot-remove sections or nodes.
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c3828497f41d..aad98003176f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -896,6 +896,7 @@ static inline struct mem_section *__nr_to_section(unsigned long nr)
 	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
 }
 extern int __section_nr(struct mem_section* ms);
+extern unsigned long usemap_size(void);
 
 /*
  * We use the lower bits of the mem_map pointer to store

commit 97965478a66fbdf0f4ad5e4ecc4828f0cb548a45
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 28 02:12:54 2008 -0700

    mm: Get rid of __ZONE_COUNT
    
    It was used to compensate because MAX_NR_ZONES was not available to the
    #ifdefs.  Export MAX_NR_ZONES via the new mechanism and get rid of
    __ZONE_COUNT.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c7a51dac441d..c3828497f41d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -3,6 +3,7 @@
 
 #ifdef __KERNEL__
 #ifndef __ASSEMBLY__
+#ifndef __GENERATING_BOUNDS_H
 
 #include <linux/spinlock.h>
 #include <linux/list.h>
@@ -15,6 +16,7 @@
 #include <linux/seqlock.h>
 #include <linux/nodemask.h>
 #include <linux/pageblock-flags.h>
+#include <linux/bounds.h>
 #include <asm/atomic.h>
 #include <asm/page.h>
 
@@ -129,6 +131,8 @@ struct per_cpu_pageset {
 #define zone_pcp(__z, __cpu) (&(__z)->pageset[(__cpu)])
 #endif
 
+#endif /* !__GENERATING_BOUNDS.H */
+
 enum zone_type {
 #ifdef CONFIG_ZONE_DMA
 	/*
@@ -177,9 +181,11 @@ enum zone_type {
 	ZONE_HIGHMEM,
 #endif
 	ZONE_MOVABLE,
-	MAX_NR_ZONES
+	__MAX_NR_ZONES
 };
 
+#ifndef __GENERATING_BOUNDS_H
+
 /*
  * When a memory allocation must conform to specific limitations (such
  * as being suitable for DMA) the caller will pass in hints to the
@@ -188,28 +194,15 @@ enum zone_type {
  * match the requested limits. See gfp_zone() in include/linux/gfp.h
  */
 
-/*
- * Count the active zones.  Note that the use of defined(X) outside
- * #if and family is not necessarily defined so ensure we cannot use
- * it later.  Use __ZONE_COUNT to work out how many shift bits we need.
- */
-#define __ZONE_COUNT (			\
-	  defined(CONFIG_ZONE_DMA)	\
-	+ defined(CONFIG_ZONE_DMA32)	\
-	+ 1				\
-	+ defined(CONFIG_HIGHMEM)	\
-	+ 1				\
-)
-#if __ZONE_COUNT < 2
+#if MAX_NR_ZONES < 2
 #define ZONES_SHIFT 0
-#elif __ZONE_COUNT <= 2
+#elif MAX_NR_ZONES <= 2
 #define ZONES_SHIFT 1
-#elif __ZONE_COUNT <= 4
+#elif MAX_NR_ZONES <= 4
 #define ZONES_SHIFT 2
 #else
 #error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
-#undef __ZONE_COUNT
 
 struct zone {
 	/* Fields commonly accessed by the page allocator */
@@ -1008,6 +1001,7 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 #define pfn_valid_within(pfn) (1)
 #endif
 
+#endif /* !__GENERATING_BOUNDS.H */
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MMZONE_H */

commit 9223b4190fa1297a59f292f3419fc0285321d0ea
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 28 02:12:48 2008 -0700

    pageflags: get rid of FLAGS_RESERVED
    
    NR_PAGEFLAGS specifies the number of page flags we are using.  From that we
    can calculate the number of bits leftover that can be used for zone, node (and
    maybe the sections id).  There is no need anymore for FLAGS_RESERVED if we use
    NR_PAGEFLAGS.
    
    Use the new methods to make NR_PAGEFLAGS available via the preprocessor.
    NR_PAGEFLAGS is used to calculate field boundaries in the page flags fields.
    These field widths have to be available to the preprocessor.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0aece6d8937e..c7a51dac441d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -820,25 +820,6 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 #include <asm/sparsemem.h>
 #endif
 
-#if BITS_PER_LONG == 32
-/*
- * with 32 bit page->flags field, we reserve 9 bits for node/zone info.
- * there are 4 zones (3 bits) and this leaves 9-3=6 bits for nodes.
- */
-#define FLAGS_RESERVED		9
-
-#elif BITS_PER_LONG == 64
-/*
- * with 64 bit flags field, there's plenty of room.
- */
-#define FLAGS_RESERVED		32
-
-#else
-
-#error BITS_PER_LONG not defined
-
-#endif
-
 #if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) && \
 	!defined(CONFIG_ARCH_POPULATES_NODE_MAP)
 static inline unsigned long early_pfn_to_nid(unsigned long pfn)

commit b45445684198a946b587732265692e6495993abf
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Apr 28 02:12:39 2008 -0700

    mm: make early_pfn_to_nid() a C function
    
    Fix this (sparc64)
    
    mm/sparse-vmemmap.c: In function `vmemmap_verify':
    mm/sparse-vmemmap.c:64: warning: unused variable `pfn'
    
    by switching to a C function which touches its arg.
    
    (reason 3,555 why macros are bad)
    
    Also, the `nid' arg was misnamed.
    
    Reviewed-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 498d6ceff2f4..0aece6d8937e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -841,7 +841,10 @@ static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 
 #if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) && \
 	!defined(CONFIG_ARCH_POPULATES_NODE_MAP)
-#define early_pfn_to_nid(nid)  (0UL)
+static inline unsigned long early_pfn_to_nid(unsigned long pfn)
+{
+	return 0;
+}
 #endif
 
 #ifdef CONFIG_FLATMEM

commit 19770b32609b6bf97a3dece2529089494cbfc549
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:18 2008 -0700

    mm: filter based on a nodemask as well as a gfp_mask
    
    The MPOL_BIND policy creates a zonelist that is used for allocations
    controlled by that mempolicy.  As the per-node zonelist is already being
    filtered based on a zone id, this patch adds a version of __alloc_pages() that
    takes a nodemask for further filtering.  This eliminates the need for
    MPOL_BIND to create a custom zonelist.
    
    A positive benefit of this is that allocations using MPOL_BIND now use the
    local node's distance-ordered zonelist instead of a custom node-id-ordered
    zonelist.  I.e., pages will be allocated from the closest allowed node with
    available memory.
    
    [Lee.Schermerhorn@hp.com: Mempolicy: update stale documentation and comments]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask rework]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d34b4c290017..498d6ceff2f4 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -749,36 +749,60 @@ static inline int zonelist_node_idx(struct zoneref *zoneref)
 #endif /* CONFIG_NUMA */
 }
 
-static inline void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)
-{
-	zoneref->zone = zone;
-	zoneref->zone_idx = zone_idx(zone);
-}
+/**
+ * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
+ * @z - The cursor used as a starting point for the search
+ * @highest_zoneidx - The zone index of the highest zone to return
+ * @nodes - An optional nodemask to filter the zonelist with
+ * @zone - The first suitable zone found is returned via this parameter
+ *
+ * This function returns the next zone at or below a given zone index that is
+ * within the allowed nodemask using a cursor as the starting point for the
+ * search. The zoneref returned is a cursor that is used as the next starting
+ * point for future calls to next_zones_zonelist().
+ */
+struct zoneref *next_zones_zonelist(struct zoneref *z,
+					enum zone_type highest_zoneidx,
+					nodemask_t *nodes,
+					struct zone **zone);
 
-/* Returns the first zone at or below highest_zoneidx in a zonelist */
+/**
+ * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
+ * @zonelist - The zonelist to search for a suitable zone
+ * @highest_zoneidx - The zone index of the highest zone to return
+ * @nodes - An optional nodemask to filter the zonelist with
+ * @zone - The first suitable zone found is returned via this parameter
+ *
+ * This function returns the first zone at or below a given zone index that is
+ * within the allowed nodemask. The zoneref returned is a cursor that can be
+ * used to iterate the zonelist with next_zones_zonelist. The cursor should
+ * not be used by the caller as it does not match the value of the zone
+ * returned.
+ */
 static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
-					enum zone_type highest_zoneidx)
+					enum zone_type highest_zoneidx,
+					nodemask_t *nodes,
+					struct zone **zone)
 {
-	struct zoneref *z;
-
-	/* Find the first suitable zone to use for the allocation */
-	z = zonelist->_zonerefs;
-	while (zonelist_zone_idx(z) > highest_zoneidx)
-		z++;
-
-	return z;
+	return next_zones_zonelist(zonelist->_zonerefs, highest_zoneidx, nodes,
+								zone);
 }
 
-/* Returns the next zone at or below highest_zoneidx in a zonelist */
-static inline struct zoneref *next_zones_zonelist(struct zoneref *z,
-					enum zone_type highest_zoneidx)
-{
-	/* Find the next suitable zone to use for the allocation */
-	while (zonelist_zone_idx(z) > highest_zoneidx)
-		z++;
-
-	return z;
-}
+/**
+ * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
+ * @zone - The current zone in the iterator
+ * @z - The current pointer within zonelist->zones being iterated
+ * @zlist - The zonelist being iterated
+ * @highidx - The zone index of the highest zone to return
+ * @nodemask - Nodemask allowed by the allocator
+ *
+ * This iterator iterates though all zones at or below a given zone index and
+ * within a given nodemask
+ */
+#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
+	for (z = first_zones_zonelist(zlist, highidx, nodemask, &zone);	\
+		zone;							\
+		z = next_zones_zonelist(z, highidx, nodemask, &zone))	\
 
 /**
  * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
@@ -790,11 +814,7 @@ static inline struct zoneref *next_zones_zonelist(struct zoneref *z,
  * This iterator iterates though all zones at or below a given zone index.
  */
 #define for_each_zone_zonelist(zone, z, zlist, highidx) \
-	for (z = first_zones_zonelist(zlist, highidx),			\
-					zone = zonelist_zone(z++);	\
-		zone;							\
-		z = next_zones_zonelist(z, highidx),			\
-					zone = zonelist_zone(z++))
+	for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, NULL)
 
 #ifdef CONFIG_SPARSEMEM
 #include <asm/sparsemem.h>

commit dd1a239f6f2d4d3eedd318583ec319aa145b324c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:17 2008 -0700

    mm: have zonelist contains structs with both a zone pointer and zone_idx
    
    Filtering zonelists requires very frequent use of zone_idx().  This is costly
    as it involves a lookup of another structure and a substraction operation.  As
    the zone_idx is often required, it should be quickly accessible.  The node idx
    could also be stored here if it was found that accessing zone->node is
    significant which may be the case on workloads where nodemasks are heavily
    used.
    
    This patch introduces a struct zoneref to store a zone pointer and a zone
    index.  The zonelist then consists of an array of these struct zonerefs which
    are looked up as necessary.  Helpers are given for accessing the zone index as
    well as the node index.
    
    [kamezawa.hiroyu@jp.fujitsu.com: Suggested struct zoneref instead of embedding information in pointers]
    [hugh@veritas.com: mm-have-zonelist: fix memcg ooms]
    [hugh@veritas.com: just return do_try_to_free_pages]
    [hugh@veritas.com: do_try_to_free_pages gfp_mask redundant]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d5c33a0b89e9..d34b4c290017 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -468,6 +468,15 @@ struct zonelist_cache {
 struct zonelist_cache;
 #endif
 
+/*
+ * This struct contains information about a zone in a zonelist. It is stored
+ * here to avoid dereferences into large structures and lookups of tables
+ */
+struct zoneref {
+	struct zone *zone;	/* Pointer to actual zone */
+	int zone_idx;		/* zone_idx(zoneref->zone) */
+};
+
 /*
  * One allocation request operates on a zonelist. A zonelist
  * is a list of zones, the first one is the 'goal' of the
@@ -476,11 +485,18 @@ struct zonelist_cache;
  *
  * If zlcache_ptr is not NULL, then it is just the address of zlcache,
  * as explained above.  If zlcache_ptr is NULL, there is no zlcache.
+ * *
+ * To speed the reading of the zonelist, the zonerefs contain the zone index
+ * of the entry being read. Helper functions to access information given
+ * a struct zoneref are
+ *
+ * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs
+ * zonelist_zone_idx()	- Return the index of the zone for an entry
+ * zonelist_node_idx()	- Return the index of the node for an entry
  */
-
 struct zonelist {
 	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
-	struct zone *zones[MAX_ZONES_PER_ZONELIST + 1];      // NULL delimited
+	struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];
 #ifdef CONFIG_NUMA
 	struct zonelist_cache zlcache;			     // optional ...
 #endif
@@ -713,26 +729,52 @@ extern struct zone *next_zone(struct zone *zone);
 	     zone;					\
 	     zone = next_zone(zone))
 
+static inline struct zone *zonelist_zone(struct zoneref *zoneref)
+{
+	return zoneref->zone;
+}
+
+static inline int zonelist_zone_idx(struct zoneref *zoneref)
+{
+	return zoneref->zone_idx;
+}
+
+static inline int zonelist_node_idx(struct zoneref *zoneref)
+{
+#ifdef CONFIG_NUMA
+	/* zone_to_nid not available in this context */
+	return zoneref->zone->node;
+#else
+	return 0;
+#endif /* CONFIG_NUMA */
+}
+
+static inline void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)
+{
+	zoneref->zone = zone;
+	zoneref->zone_idx = zone_idx(zone);
+}
+
 /* Returns the first zone at or below highest_zoneidx in a zonelist */
-static inline struct zone **first_zones_zonelist(struct zonelist *zonelist,
+static inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
 					enum zone_type highest_zoneidx)
 {
-	struct zone **z;
+	struct zoneref *z;
 
 	/* Find the first suitable zone to use for the allocation */
-	z = zonelist->zones;
-	while (*z && zone_idx(*z) > highest_zoneidx)
+	z = zonelist->_zonerefs;
+	while (zonelist_zone_idx(z) > highest_zoneidx)
 		z++;
 
 	return z;
 }
 
 /* Returns the next zone at or below highest_zoneidx in a zonelist */
-static inline struct zone **next_zones_zonelist(struct zone **z,
+static inline struct zoneref *next_zones_zonelist(struct zoneref *z,
 					enum zone_type highest_zoneidx)
 {
 	/* Find the next suitable zone to use for the allocation */
-	while (*z && zone_idx(*z) > highest_zoneidx)
+	while (zonelist_zone_idx(z) > highest_zoneidx)
 		z++;
 
 	return z;
@@ -748,9 +790,11 @@ static inline struct zone **next_zones_zonelist(struct zone **z,
  * This iterator iterates though all zones at or below a given zone index.
  */
 #define for_each_zone_zonelist(zone, z, zlist, highidx) \
-	for (z = first_zones_zonelist(zlist, highidx), zone = *z++;	\
+	for (z = first_zones_zonelist(zlist, highidx),			\
+					zone = zonelist_zone(z++);	\
 		zone;							\
-		z = next_zones_zonelist(z, highidx), zone = *z++)
+		z = next_zones_zonelist(z, highidx),			\
+					zone = zonelist_zone(z++))
 
 #ifdef CONFIG_SPARSEMEM
 #include <asm/sparsemem.h>

commit 54a6eb5c4765aa573a030ceeba2c14e3d2ea5706
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:16 2008 -0700

    mm: use two zonelist that are filtered by GFP mask
    
    Currently a node has two sets of zonelists, one for each zone type in the
    system and a second set for GFP_THISNODE allocations.  Based on the zones
    allowed by a gfp mask, one of these zonelists is selected.  All of these
    zonelists consume memory and occupy cache lines.
    
    This patch replaces the multiple zonelists per-node with two zonelists.  The
    first contains all populated zones in the system, ordered by distance, for
    fallback allocations when the target/preferred node has no free pages.  The
    second contains all populated zones in the node suitable for GFP_THISNODE
    allocations.
    
    An iterator macro is introduced called for_each_zone_zonelist() that interates
    through each zone allowed by the GFP flags in the selected zonelist.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 451eaa13bc28..d5c33a0b89e9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -393,10 +393,10 @@ static inline int zone_is_oom_locked(const struct zone *zone)
  * The NUMA zonelists are doubled becausse we need zonelists that restrict the
  * allocations to a single node for GFP_THISNODE.
  *
- * [0 .. MAX_NR_ZONES -1] 		: Zonelists with fallback
- * [MAZ_NR_ZONES ... MAZ_ZONELISTS -1]  : No fallback (GFP_THISNODE)
+ * [0]	: Zonelist with fallback
+ * [1]	: No fallback (GFP_THISNODE)
  */
-#define MAX_ZONELISTS (2 * MAX_NR_ZONES)
+#define MAX_ZONELISTS 2
 
 
 /*
@@ -464,7 +464,7 @@ struct zonelist_cache {
 	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
 };
 #else
-#define MAX_ZONELISTS MAX_NR_ZONES
+#define MAX_ZONELISTS 1
 struct zonelist_cache;
 #endif
 
@@ -486,24 +486,6 @@ struct zonelist {
 #endif
 };
 
-#ifdef CONFIG_NUMA
-/*
- * Only custom zonelists like MPOL_BIND need to be filtered as part of
- * policies. As described in the comment for struct zonelist_cache, these
- * zonelists will not have a zlcache so zlcache_ptr will not be set. Use
- * that to determine if the zonelists needs to be filtered or not.
- */
-static inline int alloc_should_filter_zonelist(struct zonelist *zonelist)
-{
-	return !zonelist->zlcache_ptr;
-}
-#else
-static inline int alloc_should_filter_zonelist(struct zonelist *zonelist)
-{
-	return 0;
-}
-#endif /* CONFIG_NUMA */
-
 #ifdef CONFIG_ARCH_POPULATES_NODE_MAP
 struct node_active_region {
 	unsigned long start_pfn;
@@ -731,6 +713,45 @@ extern struct zone *next_zone(struct zone *zone);
 	     zone;					\
 	     zone = next_zone(zone))
 
+/* Returns the first zone at or below highest_zoneidx in a zonelist */
+static inline struct zone **first_zones_zonelist(struct zonelist *zonelist,
+					enum zone_type highest_zoneidx)
+{
+	struct zone **z;
+
+	/* Find the first suitable zone to use for the allocation */
+	z = zonelist->zones;
+	while (*z && zone_idx(*z) > highest_zoneidx)
+		z++;
+
+	return z;
+}
+
+/* Returns the next zone at or below highest_zoneidx in a zonelist */
+static inline struct zone **next_zones_zonelist(struct zone **z,
+					enum zone_type highest_zoneidx)
+{
+	/* Find the next suitable zone to use for the allocation */
+	while (*z && zone_idx(*z) > highest_zoneidx)
+		z++;
+
+	return z;
+}
+
+/**
+ * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
+ * @zone - The current zone in the iterator
+ * @z - The current pointer within zonelist->zones being iterated
+ * @zlist - The zonelist being iterated
+ * @highidx - The zone index of the highest zone to return
+ *
+ * This iterator iterates though all zones at or below a given zone index.
+ */
+#define for_each_zone_zonelist(zone, z, zlist, highidx) \
+	for (z = first_zones_zonelist(zlist, highidx), zone = *z++;	\
+		zone;							\
+		z = next_zones_zonelist(z, highidx), zone = *z++)
+
 #ifdef CONFIG_SPARSEMEM
 #include <asm/sparsemem.h>
 #endif

commit ddc81ed2c5d47a078a3b02c5c3a4345bc2bc3c9b
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Mon Apr 28 02:12:07 2008 -0700

    remove sparse warning for mmzone.h
    
    include/linux/mmzone.h:640:22: warning: potentially expensive pointer subtraction
    
    Calculate the offset into the node_zones array rather than the index
    using casts to (char *) and comparing against the index * sizeof(struct zone).
    
    On X86_32 this saves a sar, but code size increases by one byte per
    is_highmem() use due to 32-bit cmps rather than 16 bit cmps.
    
    Before:
     207:   2b 80 8c 07 00 00       sub    0x78c(%eax),%eax
     20d:   c1 f8 0b                sar    $0xb,%eax
     210:   83 f8 02                cmp    $0x2,%eax
     213:   74 16                   je     22b <kmap_atomic_prot+0x144>
     215:   83 f8 03                cmp    $0x3,%eax
     218:   0f 85 8f 00 00 00       jne    2ad <kmap_atomic_prot+0x1c6>
     21e:   83 3d 00 00 00 00 02    cmpl   $0x2,0x0
     225:   0f 85 82 00 00 00       jne    2ad <kmap_atomic_prot+0x1c6>
     22b:   64 a1 00 00 00 00       mov    %fs:0x0,%eax
    
    After:
     207:   2b 80 8c 07 00 00       sub    0x78c(%eax),%eax
     20d:   3d 00 10 00 00          cmp    $0x1000,%eax
     212:   74 18                   je     22c <kmap_atomic_prot+0x145>
     214:   3d 00 18 00 00          cmp    $0x1800,%eax
     219:   0f 85 8f 00 00 00       jne    2ae <kmap_atomic_prot+0x1c7>
     21f:   83 3d 00 00 00 00 02    cmpl   $0x2,0x0
     226:   0f 85 82 00 00 00       jne    2ae <kmap_atomic_prot+0x1c7>
     22c:   64 a1 00 00 00 00       mov    %fs:0x0,%eax
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9f274a687c7e..451eaa13bc28 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -637,9 +637,10 @@ static inline int is_normal_idx(enum zone_type idx)
 static inline int is_highmem(struct zone *zone)
 {
 #ifdef CONFIG_HIGHMEM
-	int zone_idx = zone - zone->zone_pgdat->node_zones;
-	return zone_idx == ZONE_HIGHMEM ||
-		(zone_idx == ZONE_MOVABLE && zone_movable_is_highmem());
+	int zone_off = (char *)zone - (char *)zone->zone_pgdat->node_zones;
+	return zone_off == ZONE_HIGHMEM * sizeof(*zone) ||
+	       (zone_off == ZONE_MOVABLE * sizeof(*zone) &&
+		zone_movable_is_highmem());
 #else
 	return 0;
 #endif

commit 218ff137bc67252694420563d23d051ab9227f17
Author: Johannes Weiner <hannes@saeurebad.de>
Date:   Mon Apr 21 22:35:29 2008 +0000

    Remove unused MAX_NODES_SHIFT
    
    MAX_NODES_SHIFT is not referenced anywhere in the tree, so dump it.
    
    Signed-off-by: Johannes Weiner <hannes@saeurebad.de>
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8d8d1977736e..9f274a687c7e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -699,7 +699,6 @@ extern char numa_zonelist_order[];
 extern struct pglist_data contig_page_data;
 #define NODE_DATA(nid)		(&contig_page_data)
 #define NODE_MEM_MAP(nid)	mem_map
-#define MAX_NODES_SHIFT		1
 
 #else /* CONFIG_NEED_MULTIPLE_NODES */
 

commit 3dfa5721f12c3d5a441448086bee156887daa961
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Feb 4 22:29:19 2008 -0800

    Page allocator: get rid of the list of cold pages
    
    We have repeatedly discussed if the cold pages still have a point. There is
    one way to join the two lists: Use a single list and put the cold pages at the
    end and the hot pages at the beginning. That way a single list can serve for
    both types of allocations.
    
    The discussion of the RFC for this and Mel's measurements indicate that
    there may not be too much of a point left to having separate lists for
    hot and cold pages (see http://marc.info/?t=119492914200001&r=1&w=2).
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Martin Bligh <mbligh@mbligh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4c4522a51a3b..8d8d1977736e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -113,7 +113,7 @@ struct per_cpu_pages {
 };
 
 struct per_cpu_pageset {
-	struct per_cpu_pages pcp[2];	/* 0: hot.  1: cold */
+	struct per_cpu_pages pcp;
 #ifdef CONFIG_NUMA
 	s8 expire;
 #endif

commit d773ed6b856a96bd6d18b6e04455e3ced0876da4
Author: David Rientjes <rientjes@google.com>
Date:   Tue Oct 16 23:26:01 2007 -0700

    mm: test and set zone reclaim lock before starting reclaim
    
    Introduces new zone flag interface for testing and setting flags:
    
            int zone_test_and_set_flag(struct zone *zone, zone_flags_t flag)
    
    Instead of setting and clearing ZONE_RECLAIM_LOCKED each time shrink_zone() is
    called, this flag is test and set before starting zone reclaim.  Zone reclaim
    starts in __alloc_pages() when a zone's watermark fails and the system is in
    zone_reclaim_mode.  If it's already in reclaim, there's no need to start again
    so it is simply considered full for that allocation attempt.
    
    There is a change of behavior with regard to concurrent zone shrinking.  It is
    now possible for try_to_free_pages() or kswapd to already be shrinking a
    particular zone when __alloc_pages() starts zone reclaim.  In this case, it is
    possible for two concurrent threads to invoke shrink_zone() for a single zone.
    
    This change forbids a zone to be in zone reclaim twice, which was always the
    behavior, but allows for concurrent try_to_free_pages() or kswapd shrinking
    when starting zone reclaim.
    
    Cc: Andrea Arcangeli <andrea@suse.de>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9011505e740d..4c4522a51a3b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -351,6 +351,12 @@ static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
 {
 	set_bit(flag, &zone->flags);
 }
+
+static inline int zone_test_and_set_flag(struct zone *zone, zone_flags_t flag)
+{
+	return test_and_set_bit(flag, &zone->flags);
+}
+
 static inline void zone_clear_flag(struct zone *zone, zone_flags_t flag)
 {
 	clear_bit(flag, &zone->flags);
@@ -360,10 +366,12 @@ static inline int zone_is_all_unreclaimable(const struct zone *zone)
 {
 	return test_bit(ZONE_ALL_UNRECLAIMABLE, &zone->flags);
 }
+
 static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);
 }
+
 static inline int zone_is_oom_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_OOM_LOCKED, &zone->flags);

commit 098d7f128a4e53cb64930628915ac767785e0e60
Author: David Rientjes <rientjes@google.com>
Date:   Tue Oct 16 23:25:55 2007 -0700

    oom: add per-zone locking
    
    OOM killer synchronization should be done with zone granularity so that memory
    policy and cpuset allocations may have their corresponding zones locked and
    allow parallel kills for other OOM conditions that may exist elsewhere in the
    system.  DMA allocations can be targeted at the zone level, which would not be
    possible if locking was done in nodes or globally.
    
    Synchronization shall be done with a variation of "trylocks." The goal is to
    put the current task to sleep and restart the failed allocation attempt later
    if the trylock fails.  Otherwise, the OOM killer is invoked.
    
    Each zone in the zonelist that __alloc_pages() was called with is checked for
    the newly-introduced ZONE_OOM_LOCKED flag.  If any zone has this flag present,
    the "trylock" to serialize the OOM killer fails and returns zero.  Otherwise,
    all the zones have ZONE_OOM_LOCKED set and the try_set_zone_oom() function
    returns non-zero.
    
    Cc: Andrea Arcangeli <andrea@suse.de>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index bad9486ee0cc..9011505e740d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -344,6 +344,7 @@ struct zone {
 typedef enum {
 	ZONE_ALL_UNRECLAIMABLE,		/* all pages pinned */
 	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
+	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
 } zone_flags_t;
 
 static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
@@ -363,6 +364,10 @@ static inline int zone_is_reclaim_locked(const struct zone *zone)
 {
 	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);
 }
+static inline int zone_is_oom_locked(const struct zone *zone)
+{
+	return test_bit(ZONE_OOM_LOCKED, &zone->flags);
+}
 
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one

commit e815af95f94914993bbad279c71cf5fef9f4eaac
Author: David Rientjes <rientjes@google.com>
Date:   Tue Oct 16 23:25:54 2007 -0700

    oom: change all_unreclaimable zone member to flags
    
    Convert the int all_unreclaimable member of struct zone to unsigned long
    flags.  This can now be used to specify several different zone flags such as
    all_unreclaimable and reclaim_in_progress, which can now be removed and
    converted to a per-zone flag.
    
    Flags are set and cleared as follows:
    
            zone_set_flag(struct zone *zone, zone_flags_t flag)
            zone_clear_flag(struct zone *zone, zone_flags_t flag)
    
    Defines the first zone flags, ZONE_ALL_UNRECLAIMABLE and ZONE_RECLAIM_LOCKED,
    which have the same semantics as the old zone->all_unreclaimable and
    zone->reclaim_in_progress, respectively.  Also converts all current users that
    set or clear either flag to use the new interface.
    
    Helper functions are defined to test the flags:
    
            int zone_is_all_unreclaimable(const struct zone *zone)
            int zone_is_reclaim_locked(const struct zone *zone)
    
    All flag operators are of the atomic variety because there are currently
    readers that are implemented that do not take zone->lock.
    
    [akpm@linux-foundation.org: add needed include]
    Cc: Andrea Arcangeli <andrea@suse.de>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f4bfe824834f..bad9486ee0cc 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -7,6 +7,7 @@
 #include <linux/spinlock.h>
 #include <linux/list.h>
 #include <linux/wait.h>
+#include <linux/bitops.h>
 #include <linux/cache.h>
 #include <linux/threads.h>
 #include <linux/numa.h>
@@ -262,10 +263,7 @@ struct zone {
 	unsigned long		nr_scan_active;
 	unsigned long		nr_scan_inactive;
 	unsigned long		pages_scanned;	   /* since last reclaim */
-	int			all_unreclaimable; /* All pages pinned */
-
-	/* A count of how many reclaimers are scanning this zone */
-	atomic_t		reclaim_in_progress;
+	unsigned long		flags;		   /* zone flags, see below */
 
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
@@ -343,6 +341,29 @@ struct zone {
 	const char		*name;
 } ____cacheline_internodealigned_in_smp;
 
+typedef enum {
+	ZONE_ALL_UNRECLAIMABLE,		/* all pages pinned */
+	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
+} zone_flags_t;
+
+static inline void zone_set_flag(struct zone *zone, zone_flags_t flag)
+{
+	set_bit(flag, &zone->flags);
+}
+static inline void zone_clear_flag(struct zone *zone, zone_flags_t flag)
+{
+	clear_bit(flag, &zone->flags);
+}
+
+static inline int zone_is_all_unreclaimable(const struct zone *zone)
+{
+	return test_bit(ZONE_ALL_UNRECLAIMABLE, &zone->flags);
+}
+static inline int zone_is_reclaim_locked(const struct zone *zone)
+{
+	return test_bit(ZONE_RECLAIM_LOCKED, &zone->flags);
+}
+
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the

commit a5d76b54a3f3a40385d7f76069a2feac9f1bad63
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:11 2007 -0700

    memory unplug: page isolation
    
    Implement generic chunk-of-pages isolation method by using page grouping ops.
    
    This patch add MIGRATE_ISOLATE to MIGRATE_TYPES. By this
     - MIGRATE_TYPES increases.
     - bitmap for migratetype is enlarged.
    
    pages of MIGRATE_ISOLATE migratetype will not be allocated even if it is free.
    By this, you can isolated *freed* pages from users. How-to-free pages is not
    a purpose of this patch. You may use reclaim and migrate codes to free pages.
    
    If start_isolate_page_range(start,end) is called,
     - migratetype of the range turns to be MIGRATE_ISOLATE  if
       its type is MIGRATE_MOVABLE. (*) this check can be updated if other
       memory reclaiming works make progress.
     - MIGRATE_ISOLATE is not on migratetype fallback list.
     - All free pages and will-be-freed pages are isolated.
    To check all pages in the range are isolated or not,  use test_pages_isolated(),
    To cancel isolation, use undo_isolate_page_range().
    
    Changes V6 -> V7
     - removed unnecessary #ifdef
    
    There are HOLES_IN_ZONE handling codes...I'm glad if we can remove them..
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 20ea42c45e4a..f4bfe824834f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -37,7 +37,8 @@
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
 #define MIGRATE_RESERVE       3
-#define MIGRATE_TYPES         4
+#define MIGRATE_ISOLATE       4 /* can't allocate from here */
+#define MIGRATE_TYPES         5
 
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \

commit 467c996c1e1910633fa8e7adc9b052aa3ed5f97c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:26:02 2007 -0700

    Print out statistics in relation to fragmentation avoidance to /proc/pagetypeinfo
    
    This patch provides fragmentation avoidance statistics via /proc/pagetypeinfo.
     The information is collected only on request so there is no runtime overhead.
     The statistics are in three parts:
    
    The first part prints information on the size of blocks that pages are
    being grouped on and looks like
    
    Page block order: 10
    Pages per block:  1024
    
    The second part is a more detailed version of /proc/buddyinfo and looks like
    
    Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10
    Node    0, zone      DMA, type    Unmovable      0      0      0      0      0      0      0      0      0      0      0
    Node    0, zone      DMA, type  Reclaimable      1      0      0      0      0      0      0      0      0      0      0
    Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      0      0
    Node    0, zone      DMA, type      Reserve      0      4      4      0      0      0      0      1      0      1      0
    Node    0, zone   Normal, type    Unmovable    111      8      4      4      2      3      1      0      0      0      0
    Node    0, zone   Normal, type  Reclaimable    293     89      8      0      0      0      0      0      0      0      0
    Node    0, zone   Normal, type      Movable      1      6     13      9      7      6      3      0      0      0      0
    Node    0, zone   Normal, type      Reserve      0      0      0      0      0      0      0      0      0      0      4
    
    The third part looks like
    
    Number of blocks type     Unmovable  Reclaimable      Movable      Reserve
    Node 0, zone      DMA            0            1            2            1
    Node 0, zone   Normal            3           17           94            4
    
    To walk the zones within a node with interrupts disabled, walk_zones_in_node()
    is introduced and shared between /proc/buddyinfo, /proc/zoneinfo and
    /proc/pagetypeinfo to reduce code duplication.  It seems specific to what
    vmstat.c requires but could be broken out as a general utility function in
    mmzone.c if there were other other potential users.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 87a4045580f2..20ea42c45e4a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -43,6 +43,16 @@
 	for (order = 0; order < MAX_ORDER; order++) \
 		for (type = 0; type < MIGRATE_TYPES; type++)
 
+extern int page_group_by_mobility_disabled;
+
+static inline int get_pageblock_migratetype(struct page *page)
+{
+	if (unlikely(page_group_by_mobility_disabled))
+		return MIGRATE_UNMOVABLE;
+
+	return get_pageblock_flags_group(page, PB_migrate, PB_migrate_end);
+}
+
 struct free_area {
 	struct list_head	free_list[MIGRATE_TYPES];
 	unsigned long		nr_free;

commit d9c2340052278d8eb2ffb16b0484f8f794def4de
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:26:01 2007 -0700

    Do not depend on MAX_ORDER when grouping pages by mobility
    
    Currently mobility grouping works at the MAX_ORDER_NR_PAGES level.  This makes
    sense for the majority of users where this is also the huge page size.
    However, on platforms like ia64 where the huge page size is runtime
    configurable it is desirable to group at a lower order.  On x86_64 and
    occasionally on x86, the hugepage size may not always be MAX_ORDER_NR_PAGES.
    
    This patch groups pages together based on the value of HUGETLB_PAGE_ORDER.  It
    uses a compile-time constant if possible and a variable where the huge page
    size is runtime configurable.
    
    It is assumed that grouping should be done at the lowest sensible order and
    that the user would not want to override this.  If this is not true,
    page_block order could be forced to a variable initialised via a boot-time
    kernel parameter.
    
    One potential issue with this patch is that IA64 now parses hugepagesz with
    early_param() instead of __setup().  __setup() is called after the memory
    allocator has been initialised and the pageblock bitmaps already setup.  In
    tests on one IA64 there did not seem to be any problem with using
    early_param() and in fact may be more correct as it guarantees the parameter
    is handled before the parsing of hugepages=.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index fef08c6cf75e..87a4045580f2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -235,7 +235,7 @@ struct zone {
 
 #ifndef CONFIG_SPARSEMEM
 	/*
-	 * Flags for a MAX_ORDER_NR_PAGES block. See pageblock-flags.h.
+	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
 	 * In SPARSEMEM, this map is stored in struct mem_section
 	 */
 	unsigned long		*pageblock_flags;
@@ -740,7 +740,7 @@ extern struct zone *next_zone(struct zone *zone);
 #define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))
 
 #define SECTION_BLOCKFLAGS_BITS \
-		((1 << (PFN_SECTION_SHIFT - (MAX_ORDER-1))) * NR_PAGEBLOCK_BITS)
+	((1UL << (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)
 
 #if (MAX_ORDER - 1 + PAGE_SHIFT) > SECTION_SIZE_BITS
 #error Allocator MAX_ORDER exceeds SECTION_SIZE

commit 64c5e135bf5a2a7f0ededb3435a31adbe0202f0c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:59 2007 -0700

    don't group high order atomic allocations
    
    Grouping high-order atomic allocations together was intended to allow
    bursty users of atomic allocations to work such as e1000 in situations
    where their preallocated buffers were depleted.  This did not work in at
    least one case with a wireless network adapter needing order-1 allocations
    frequently.  To resolve that, the free pages used for min_free_kbytes were
    moved to separate contiguous blocks with the patch
    bias-the-location-of-pages-freed-for-min_free_kbytes-in-the-same-max_order_nr_pages-blocks.
    
    It is felt that keeping the free pages in the same contiguous blocks should
    be sufficient for bursty short-lived high-order atomic allocations to
    succeed, maybe even with the e1000.  Even if there is a failure, increasing
    the value of min_free_kbytes will free pages as contiguous bloks in
    contrast to the standard buddy allocator which makes no attempt to keep the
    minimum number of free pages contiguous.
    
    This patch backs out grouping high order atomic allocations together to
    determine if it is really needed or not.  If a new report comes in about
    high-order atomic allocations failing, the feature can be reintroduced to
    determine if it fixes the problem or not.  As a side-effect, this patch
    reduces by 1 the number of bits required to track the mobility type of
    pages within a MAX_ORDER_NR_PAGES block.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 09b2c4f50e38..fef08c6cf75e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -36,9 +36,8 @@
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
-#define MIGRATE_HIGHATOMIC    3
-#define MIGRATE_RESERVE       4
-#define MIGRATE_TYPES         5
+#define MIGRATE_RESERVE       3
+#define MIGRATE_TYPES         4
 
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \

commit ac0e5b7a6b93fb291b01fe1e951e3c16bcdd3503
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:58 2007 -0700

    remove PAGE_GROUP_BY_MOBILITY
    
    Grouping pages by mobility can be disabled at compile-time. This was
    considered undesirable by a number of people. However, in the current stack of
    patches, it is not a simple case of just dropping the configurable patch as it
    would cause merge conflicts.  This patch backs out the configuration option.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index afdec8117458..09b2c4f50e38 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -33,21 +33,12 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
-#ifdef CONFIG_PAGE_GROUP_BY_MOBILITY
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
 #define MIGRATE_HIGHATOMIC    3
 #define MIGRATE_RESERVE       4
 #define MIGRATE_TYPES         5
-#else
-#define MIGRATE_UNMOVABLE     0
-#define MIGRATE_UNRECLAIMABLE 0
-#define MIGRATE_MOVABLE       0
-#define MIGRATE_HIGHATOMIC    0
-#define MIGRATE_RESERVE       0
-#define MIGRATE_TYPES         1
-#endif
 
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \

commit 56fd56b868f19385c50af8941a4c78df433b2d32
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:58 2007 -0700

    Bias the location of pages freed for min_free_kbytes in the same MAX_ORDER_NR_PAGES blocks
    
    The standard buddy allocator always favours the smallest block of pages.
    The effect of this is that the pages free to satisfy min_free_kbytes tends
    to be preserved since boot time at the same location of memory ffor a very
    long time and as a contiguous block.  When an administrator sets the
    reserve at 16384 at boot time, it tends to be the same MAX_ORDER blocks
    that remain free.  This allows the occasional high atomic allocation to
    succeed up until the point the blocks are split.  In practice, it is
    difficult to split these blocks but when they do split, the benefit of
    having min_free_kbytes for contiguous blocks disappears.  Additionally,
    increasing min_free_kbytes once the system has been running for some time
    has no guarantee of creating contiguous blocks.
    
    On the other hand, CONFIG_PAGE_GROUP_BY_MOBILITY favours splitting large
    blocks when there are no free pages of the appropriate type available.  A
    side-effect of this is that all blocks in memory tends to be used up and
    the contiguous free blocks from boot time are not preserved like in the
    vanilla allocator.  This can cause a problem if a new caller is unwilling
    to reclaim or does not reclaim for long enough.
    
    A failure scenario was found for a wireless network device allocating
    order-1 atomic allocations but the allocations were not intense or frequent
    enough for a whole block of pages to be preserved for MIGRATE_HIGHALLOC.
    This was reproduced on a desktop by booting with mem=256mb, forcing the
    driver to allocate at order-1, running a bittorrent client (downloading a
    debian ISO) and building a kernel with -j2.
    
    This patch addresses the problem on the desktop machine booted with
    mem=256mb.  It works by setting aside a reserve of MAX_ORDER_NR_PAGES
    blocks, the number of which depends on the value of min_free_kbytes.  These
    blocks are only fallen back to when there is no other free pages.  Then the
    smallest possible page is used just like the normal buddy allocator instead
    of the largest possible page to preserve contiguous pages The pages in free
    lists in the reserve blocks are never taken for another migrate type.  The
    results is that even if min_free_kbytes is set to a low value, contiguous
    blocks will be preserved in the MIGRATE_RESERVE blocks.
    
    This works better than the vanilla allocator because if min_free_kbytes is
    increased, a new reserve block will be chosen based on the location of
    reclaimable pages and the block will free up as contiguous pages.  In the
    vanilla allocator, no effort is made to target a block of pages to free as
    contiguous pages and min_free_kbytes pages are scattered randomly.
    
    This effect has been observed on the test machine.  min_free_kbytes was set
    initially low but it was kept as a contiguous free block within
    MIGRATE_RESERVE.  min_free_kbytes was then set to a higher value and over a
    period of time, the free blocks were within the reserve and coalescing.
    How long it takes to free up depends on how quickly LRU is rotating.
    Amusingly, this means that more activity will free the blocks faster.
    
    This mechanism potentially replaces MIGRATE_HIGHALLOC as it may be more
    effective than grouping contiguous free pages together.  It all depends on
    whether the number of active atomic high allocations exceeds
    min_free_kbytes or not.  If the number of active allocations exceeds
    min_free_kbytes, it's worth it but maybe in that situation, min_free_kbytes
    should be set higher.  Once there are no more reports of allocation
    failures, a patch will be submitted that backs out MIGRATE_HIGHALLOC and
    see if the reports stay missing.
    
    Credit to Mariusz Kozlowski for discovering the problem, describing the
    failure scenario and testing patches and scenarios.
    
    [akpm@linux-foundation.org: cleanups]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9a5d5590bd39..afdec8117458 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -38,12 +38,14 @@
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
 #define MIGRATE_HIGHATOMIC    3
-#define MIGRATE_TYPES         4
+#define MIGRATE_RESERVE       4
+#define MIGRATE_TYPES         5
 #else
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_UNRECLAIMABLE 0
 #define MIGRATE_MOVABLE       0
 #define MIGRATE_HIGHATOMIC    0
+#define MIGRATE_RESERVE       0
 #define MIGRATE_TYPES         1
 #endif
 

commit 5c0e3066474b57c56ff0d88ca31d95bd14232fee
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:56 2007 -0700

    Fix corruption of memmap on IA64 SPARSEMEM when mem_section is not a power of 2
    
    There are problems in the use of SPARSEMEM and pageblock flags that causes
    problems on ia64.
    
    The first part of the problem is that units are incorrect in
    SECTION_BLOCKFLAGS_BITS computation.  This results in a map_section's
    section_mem_map being treated as part of a bitmap which isn't good.  This
    was evident with an invalid virtual address when mem_init attempted to free
    bootmem pages while relinquishing control from the bootmem allocator.
    
    The second part of the problem occurs because the pageblock flags bitmap is
    be located with the mem_section.  The SECTIONS_PER_ROOT computation using
    sizeof (mem_section) may not be a power of 2 depending on the size of the
    bitmap.  This renders masks and other such things not power of 2 base.
    This issue was seen with SPARSEMEM_EXTREME on ia64.  This patch moves the
    bitmap outside of mem_section and uses a pointer instead in the
    mem_section.  The bitmaps are allocated when the section is being
    initialised.
    
    Note that sparse_early_usemap_alloc() does not use alloc_remap() like
    sparse_early_mem_map_alloc().  The allocation required for the bitmap on
    x86, the only architecture that uses alloc_remap is typically smaller than
    a cache line.  alloc_remap() pads out allocations to the cache size which
    would be a needless waste.
    
    Credit to Bob Picco for identifying the original problem and effecting a
    fix for the SECTION_BLOCKFLAGS_BITS calculation.  Credit to Andy Whitcroft
    for devising the best way of allocating the bitmaps only when required for
    the section.
    
    [wli@holomorphy.com: warning fix]
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: William Irwin <bill.irwin@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a8140a9a65e8..9a5d5590bd39 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -748,7 +748,7 @@ extern struct zone *next_zone(struct zone *zone);
 #define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))
 
 #define SECTION_BLOCKFLAGS_BITS \
-		((SECTION_SIZE_BITS - (MAX_ORDER-1)) * NR_PAGEBLOCK_BITS)
+		((1 << (PFN_SECTION_SHIFT - (MAX_ORDER-1))) * NR_PAGEBLOCK_BITS)
 
 #if (MAX_ORDER - 1 + PAGE_SHIFT) > SECTION_SIZE_BITS
 #error Allocator MAX_ORDER exceeds SECTION_SIZE
@@ -769,7 +769,9 @@ struct mem_section {
 	 * before using it wrong.
 	 */
 	unsigned long section_mem_map;
-	DECLARE_BITMAP(pageblock_flags, SECTION_BLOCKFLAGS_BITS);
+
+	/* See declaration of similar field in struct zone */
+	unsigned long *pageblock_flags;
 };
 
 #ifdef CONFIG_SPARSEMEM_EXTREME

commit e010487dbe09d63cf916fd1b119d17abd0f48207
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:53 2007 -0700

    Group high-order atomic allocations
    
    In rare cases, the kernel needs to allocate a high-order block of pages
    without sleeping.  For example, this is the case with e1000 cards configured
    to use jumbo frames.  Migrating or reclaiming pages in this situation is not
    an option.
    
    This patch groups these allocations together as much as possible by adding a
    new MIGRATE_TYPE.  The MIGRATE_HIGHATOMIC type are exactly what they sound
    like.  Care is taken that pages of other migrate types do not use the same
    blocks as high-order atomic allocations.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4721e9aa3ced..a8140a9a65e8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -37,11 +37,13 @@
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_RECLAIMABLE   1
 #define MIGRATE_MOVABLE       2
-#define MIGRATE_TYPES         3
+#define MIGRATE_HIGHATOMIC    3
+#define MIGRATE_TYPES         4
 #else
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_UNRECLAIMABLE 0
 #define MIGRATE_MOVABLE       0
+#define MIGRATE_HIGHATOMIC    0
 #define MIGRATE_TYPES         1
 #endif
 

commit e12ba74d8ff3e2f73a583500d7095e406df4d093
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:52 2007 -0700

    Group short-lived and reclaimable kernel allocations
    
    This patch marks a number of allocations that are either short-lived such as
    network buffers or are reclaimable such as inode allocations.  When something
    like updatedb is called, long-lived and unmovable kernel allocations tend to
    be spread throughout the address space which increases fragmentation.
    
    This patch groups these allocations together as much as possible by adding a
    new MIGRATE_TYPE.  The MIGRATE_RECLAIMABLE type is for allocations that can be
    reclaimed on demand, but not moved.  i.e.  they can be migrated by deleting
    them and re-reading the information from elsewhere.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7d7e4fe0fda8..4721e9aa3ced 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -35,10 +35,12 @@
 
 #ifdef CONFIG_PAGE_GROUP_BY_MOBILITY
 #define MIGRATE_UNMOVABLE     0
-#define MIGRATE_MOVABLE       1
-#define MIGRATE_TYPES         2
+#define MIGRATE_RECLAIMABLE   1
+#define MIGRATE_MOVABLE       2
+#define MIGRATE_TYPES         3
 #else
 #define MIGRATE_UNMOVABLE     0
+#define MIGRATE_UNRECLAIMABLE 0
 #define MIGRATE_MOVABLE       0
 #define MIGRATE_TYPES         1
 #endif

commit b92a6edd4b77a8794adb497280beea5df5e59a14
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:50 2007 -0700

    Add a configure option to group pages by mobility
    
    The grouping mechanism has some memory overhead and a more complex allocation
    path.  This patch allows the strategy to be disabled for small memory systems
    or if it is known the workload is suffering because of the strategy.  It also
    acts to show where the page groupings strategy interacts with the standard
    buddy allocator.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Joel Schopp <jschopp@austin.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 57700038e669..7d7e4fe0fda8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -33,9 +33,15 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#ifdef CONFIG_PAGE_GROUP_BY_MOBILITY
 #define MIGRATE_UNMOVABLE     0
 #define MIGRATE_MOVABLE       1
 #define MIGRATE_TYPES         2
+#else
+#define MIGRATE_UNMOVABLE     0
+#define MIGRATE_MOVABLE       0
+#define MIGRATE_TYPES         1
+#endif
 
 #define for_each_migratetype_order(order, type) \
 	for (order = 0; order < MAX_ORDER; order++) \

commit b2a0ac8875a0a3b9f0739b60526f8c5977d2200f
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:48 2007 -0700

    Split the free lists for movable and unmovable allocations
    
    This patch adds the core of the fragmentation reduction strategy.  It works by
    grouping pages together based on their ability to migrate or be reclaimed.
    Basically, it works by breaking the list in zone->free_area list into
    MIGRATE_TYPES number of lists.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 322e8048463e..57700038e669 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -33,8 +33,16 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#define MIGRATE_UNMOVABLE     0
+#define MIGRATE_MOVABLE       1
+#define MIGRATE_TYPES         2
+
+#define for_each_migratetype_order(order, type) \
+	for (order = 0; order < MAX_ORDER; order++) \
+		for (type = 0; type < MIGRATE_TYPES; type++)
+
 struct free_area {
-	struct list_head	free_list;
+	struct list_head	free_list[MIGRATE_TYPES];
 	unsigned long		nr_free;
 };
 

commit 835c134ec4dd755e5c4470af566db226d1e96742
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:47 2007 -0700

    Add a bitmap that is used to track flags affecting a block of pages
    
    Here is the latest revision of the anti-fragmentation patches.  Of particular
    note in this version is special treatment of high-order atomic allocations.
    Care is taken to group them together and avoid grouping pages of other types
    near them.  Artifical tests imply that it works.  I'm trying to get the
    hardware together that would allow setting up of a "real" test.  If anyone
    already has a setup and test that can trigger the atomic-allocation problem,
    I'd appreciate a test of these patches and a report.  The second major change
    is that these patches will apply cleanly with patches that implement
    anti-fragmentation through zones.
    
    kernbench shows effectively no performance difference varying between -0.2%
    and +2% on a variety of test machines.  Success rates for huge page allocation
    are dramatically increased.  For example, on a ppc64 machine, the vanilla
    kernel was only able to allocate 1% of memory as a hugepage and this was due
    to a single hugepage reserved as min_free_kbytes.  With these patches applied,
    17% was allocatable as superpages.  With reclaim-related fixes from Andy
    Whitcroft, it was 40% and further reclaim-related improvements should increase
    this further.
    
    Changelog Since V28
    o Group high-order atomic allocations together
    o It is no longer required to set min_free_kbytes to 10% of memory. A value
      of 16384 in most cases will be sufficient
    o Now applied with zone-based anti-fragmentation
    o Fix incorrect VM_BUG_ON within buffered_rmqueue()
    o Reorder the stack so later patches do not back out work from earlier patches
    o Fix bug were journal pages were being treated as movable
    o Bias placement of non-movable pages to lower PFNs
    o More agressive clustering of reclaimable pages in reactions to workloads
      like updatedb that flood the size of inode caches
    
    Changelog Since V27
    
    o Renamed anti-fragmentation to Page Clustering. Anti-fragmentation was giving
      the mistaken impression that it was the 100% solution for high order
      allocations. Instead, it greatly increases the chances high-order
      allocations will succeed and lays the foundation for defragmentation and
      memory hot-remove to work properly
    o Redefine page groupings based on ability to migrate or reclaim instead of
      basing on reclaimability alone
    o Get rid of spurious inits
    o Per-cpu lists are no longer split up per-type. Instead the per-cpu list is
      searched for a page of the appropriate type
    o Added more explanation commentary
    o Fix up bug in pageblock code where bitmap was used before being initalised
    
    Changelog Since V26
    o Fix double init of lists in setup_pageset
    
    Changelog Since V25
    o Fix loop order of for_each_rclmtype_order so that order of loop matches args
    o gfpflags_to_rclmtype uses gfp_t instead of unsigned long
    o Rename get_pageblock_type() to get_page_rclmtype()
    o Fix alignment problem in move_freepages()
    o Add mechanism for assigning flags to blocks of pages instead of page->flags
    o On fallback, do not examine the preferred list of free pages a second time
    
    The purpose of these patches is to reduce external fragmentation by grouping
    pages of related types together.  When pages are migrated (or reclaimed under
    memory pressure), large contiguous pages will be freed.
    
    This patch works by categorising allocations by their ability to migrate;
    
    Movable - The pages may be moved with the page migration mechanism. These are
            generally userspace pages.
    
    Reclaimable - These are allocations for some kernel caches that are
            reclaimable or allocations that are known to be very short-lived.
    
    Unmovable - These are pages that are allocated by the kernel that
            are not trivially reclaimed. For example, the memory allocated for a
            loaded module would be in this category. By default, allocations are
            considered to be of this type
    
    HighAtomic - These are high-order allocations belonging to callers that
            cannot sleep or perform any IO. In practice, this is restricted to
            jumbo frame allocation for network receive. It is assumed that the
            allocations are short-lived
    
    Instead of having one MAX_ORDER-sized array of free lists in struct free_area,
    there is one for each type of reclaimability.  Once a 2^MAX_ORDER block of
    pages is split for a type of allocation, it is added to the free-lists for
    that type, in effect reserving it.  Hence, over time, pages of the different
    types can be clustered together.
    
    When the preferred freelists are expired, the largest possible block is taken
    from an alternative list.  Buddies that are split from that large block are
    placed on the preferred allocation-type freelists to mitigate fragmentation.
    
    This implementation gives best-effort for low fragmentation in all zones.
    Ideally, min_free_kbytes needs to be set to a value equal to 4 * (1 <<
    (MAX_ORDER-1)) pages in most cases.  This would be 16384 on x86 and x86_64 for
    example.
    
    Our tests show that about 60-70% of physical memory can be allocated on a
    desktop after a few days uptime.  In benchmarks and stress tests, we are
    finding that 80% of memory is available as contiguous blocks at the end of the
    test.  To compare, a standard kernel was getting < 1% of memory as large pages
    on a desktop and about 8-12% of memory as large pages at the end of stress
    tests.
    
    Following this email are 12 patches that implement thie page grouping feature.
     The first patch introduces a mechanism for storing flags related to a whole
    block of pages.  Then allocations are split between movable and all other
    allocations.  Following that are patches to deal with per-cpu pages and make
    the mechanism configurable.  The next patch moves free pages between lists
    when partially allocated blocks are used for pages of another migrate type.
    The second last patch groups reclaimable kernel allocations such as inode
    caches together.  The final patch related to groupings keeps high-order atomic
    allocations.
    
    The last two patches are more concerned with control of fragmentation.  The
    second last patch biases placement of non-movable allocations towards the
    start of memory.  This is with a view of supporting memory hot-remove of DIMMs
    with higher PFNs in the future.  The biasing could be enforced a lot heavier
    but it would cost.  The last patch agressively clusters reclaimable pages like
    inode caches together.
    
    The fragmentation reduction strategy needs to track if pages within a block
    can be moved or reclaimed so that pages are freed to the appropriate list.
    This patch adds a bitmap for flags affecting a whole a MAX_ORDER block of
    pages.
    
    In non-SPARSEMEM configurations, the bitmap is stored in the struct zone and
    allocated during initialisation.  SPARSEMEM statically allocates the bitmap in
    a struct mem_section so that bitmaps do not have to be resized during memory
    hotadd.  This wastes a small amount of memory per unused section (usually
    sizeof(unsigned long)) but the complexity of dynamically allocating the memory
    is quite high.
    
    Additional credit to Andy Whitcroft who reviewed up an earlier implementation
    of the mechanism an suggested how to make it a *lot* cleaner.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f6167f2fd7fb..322e8048463e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -13,6 +13,7 @@
 #include <linux/init.h>
 #include <linux/seqlock.h>
 #include <linux/nodemask.h>
+#include <linux/pageblock-flags.h>
 #include <asm/atomic.h>
 #include <asm/page.h>
 
@@ -222,6 +223,14 @@ struct zone {
 #endif
 	struct free_area	free_area[MAX_ORDER];
 
+#ifndef CONFIG_SPARSEMEM
+	/*
+	 * Flags for a MAX_ORDER_NR_PAGES block. See pageblock-flags.h.
+	 * In SPARSEMEM, this map is stored in struct mem_section
+	 */
+	unsigned long		*pageblock_flags;
+#endif /* CONFIG_SPARSEMEM */
+
 
 	ZONE_PADDING(_pad1_)
 
@@ -720,6 +729,9 @@ extern struct zone *next_zone(struct zone *zone);
 #define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)
 #define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))
 
+#define SECTION_BLOCKFLAGS_BITS \
+		((SECTION_SIZE_BITS - (MAX_ORDER-1)) * NR_PAGEBLOCK_BITS)
+
 #if (MAX_ORDER - 1 + PAGE_SHIFT) > SECTION_SIZE_BITS
 #error Allocator MAX_ORDER exceeds SECTION_SIZE
 #endif
@@ -739,6 +751,7 @@ struct mem_section {
 	 * before using it wrong.
 	 */
 	unsigned long section_mem_map;
+	DECLARE_BITMAP(pageblock_flags, SECTION_BLOCKFLAGS_BITS);
 };
 
 #ifdef CONFIG_SPARSEMEM_EXTREME

commit 523b945855a1427000ffc707c610abe5947ae607
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:25:37 2007 -0700

    Memoryless nodes: Fix GFP_THISNODE behavior
    
    GFP_THISNODE checks that the zone selected is within the pgdat (node) of the
    first zone of a nodelist.  That only works if the node has memory.  A
    memoryless node will have its first node on another pgdat (node).
    
    GFP_THISNODE currently will return simply memory on the first pgdat.  Thus it
    is returning memory on other nodes.  GFP_THISNODE should fail if there is no
    local memory on a node.
    
    Add a new set of zonelists for each node that only contain the nodes that
    belong to the zones itself so that no fallback is possible.
    
    Then modify gfp_type to pickup the right zone based on the presence of
    __GFP_THISNODE.
    
    Drop the existing GFP_THISNODE checks from the page_allocators hot path.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Tested-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Bob Picco <bob.picco@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@skynet.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f21e5951038b..f6167f2fd7fb 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -324,6 +324,17 @@ struct zone {
 #define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)
 
 #ifdef CONFIG_NUMA
+
+/*
+ * The NUMA zonelists are doubled becausse we need zonelists that restrict the
+ * allocations to a single node for GFP_THISNODE.
+ *
+ * [0 .. MAX_NR_ZONES -1] 		: Zonelists with fallback
+ * [MAZ_NR_ZONES ... MAZ_ZONELISTS -1]  : No fallback (GFP_THISNODE)
+ */
+#define MAX_ZONELISTS (2 * MAX_NR_ZONES)
+
+
 /*
  * We cache key information from each zonelist for smaller cache
  * footprint when scanning for free pages in get_page_from_freelist().
@@ -389,6 +400,7 @@ struct zonelist_cache {
 	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
 };
 #else
+#define MAX_ZONELISTS MAX_NR_ZONES
 struct zonelist_cache;
 #endif
 
@@ -455,7 +467,7 @@ extern struct page *mem_map;
 struct bootmem_data;
 typedef struct pglist_data {
 	struct zone node_zones[MAX_NR_ZONES];
-	struct zonelist node_zonelists[MAX_NR_ZONES];
+	struct zonelist node_zonelists[MAX_ZONELISTS];
 	int nr_zones;
 #ifdef CONFIG_FLAT_NODE_MEM_MAP
 	struct page *node_mem_map;

commit 540557b9439ec19668553830c90222f9fb0c2e95
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Tue Oct 16 01:24:11 2007 -0700

    sparsemem: record when a section has a valid mem_map
    
    We have flags to indicate whether a section actually has a valid mem_map
    associated with it.  This is never set and we rely solely on the present bit
    to indicate a section is valid.  By definition a section is not valid if it
    has no mem_map and there is a window during init where the present bit is set
    but there is no mem_map, during which pfn_valid() will return true
    incorrectly.
    
    Use the existing SECTION_HAS_MEM_MAP flag to indicate the presence of a valid
    mem_map.  Switch valid_section{,_nr} and pfn_valid() to this bit.  Add a new
    present_section{,_nr} and pfn_present() interfaces for those users who care to
    know that a section is going to be valid.
    
    [akpm@linux-foundation.org: coding-syle fixes]
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4e5627379b09..f21e5951038b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -771,12 +771,17 @@ static inline struct page *__section_mem_map_addr(struct mem_section *section)
 	return (struct page *)map;
 }
 
-static inline int valid_section(struct mem_section *section)
+static inline int present_section(struct mem_section *section)
 {
 	return (section && (section->section_mem_map & SECTION_MARKED_PRESENT));
 }
 
-static inline int section_has_mem_map(struct mem_section *section)
+static inline int present_section_nr(unsigned long nr)
+{
+	return present_section(__nr_to_section(nr));
+}
+
+static inline int valid_section(struct mem_section *section)
 {
 	return (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));
 }
@@ -798,6 +803,13 @@ static inline int pfn_valid(unsigned long pfn)
 	return valid_section(__nr_to_section(pfn_to_section_nr(pfn)));
 }
 
+static inline int pfn_present(unsigned long pfn)
+{
+	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
+		return 0;
+	return present_section(__nr_to_section(pfn_to_section_nr(pfn)));
+}
+
 /*
  * These are _only_ used during initialisation, therefore they
  * can use __initdata ...  They could have names to indicate

commit b377fd3982ad957c796758a90e2988401a884241
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Aug 22 14:02:05 2007 -0700

    Apply memory policies to top two highest zones when highest zone is ZONE_MOVABLE
    
    The NUMA layer only supports NUMA policies for the highest zone.  When
    ZONE_MOVABLE is configured with kernelcore=, the the highest zone becomes
    ZONE_MOVABLE.  The result is that policies are only applied to allocations
    like anonymous pages and page cache allocated from ZONE_MOVABLE when the
    zone is used.
    
    This patch applies policies to the two highest zones when the highest zone
    is ZONE_MOVABLE.  As ZONE_MOVABLE consists of pages from the highest "real"
    zone, it's always functionally equivalent.
    
    The patch has been tested on a variety of machines both NUMA and non-NUMA
    covering x86, x86_64 and ppc64.  No abnormal results were seen in
    kernbench, tbench, dbench or hackbench.  It passes regression tests from
    the numactl package with and without kernelcore= once numactl tests are
    patched to wait for vmstat counters to update.
    
    akpm: this is the nasty hack to fix NUMA mempolicies in the presence of
    ZONE_MOVABLE and kernelcore= in 2.6.23.  Christoph says "For .24 either merge
    the mobility or get the other solution that Mel is working on.  That solution
    would only use a single zonelist per node and filter on the fly.  That may
    help performance and also help to make memory policies work better."
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Tested-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3ea68cd3b61f..4e5627379b09 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -410,6 +410,24 @@ struct zonelist {
 #endif
 };
 
+#ifdef CONFIG_NUMA
+/*
+ * Only custom zonelists like MPOL_BIND need to be filtered as part of
+ * policies. As described in the comment for struct zonelist_cache, these
+ * zonelists will not have a zlcache so zlcache_ptr will not be set. Use
+ * that to determine if the zonelists needs to be filtered or not.
+ */
+static inline int alloc_should_filter_zonelist(struct zonelist *zonelist)
+{
+	return !zonelist->zlcache_ptr;
+}
+#else
+static inline int alloc_should_filter_zonelist(struct zonelist *zonelist)
+{
+	return 0;
+}
+#endif /* CONFIG_NUMA */
+
 #ifdef CONFIG_ARCH_POPULATES_NODE_MAP
 struct node_active_region {
 	unsigned long start_pfn;

commit 99eb8a550dbccc0e1f6c7e866fe421810e0585f6
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Jul 31 00:38:19 2007 -0700

    Remove the arm26 port
    
    The arm26 port has been in a state where it was far from even compiling
    for quite some time.
    
    Ian Molton agreed with the removal.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Cc: Ian Molton <spyro@f2s.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index da8eb8ad9e9b..3ea68cd3b61f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -120,7 +120,6 @@ enum zone_type {
 	 * ---------------------------
 	 * parisc, ia64, sparc	<4G
 	 * s390			<2G
-	 * arm26		<48M
 	 * arm			Various
 	 * alpha		Unlimited or 0-16MB.
 	 *

commit 5ad333eb66ff1e52a87639822ae088577669dcf9
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Tue Jul 17 04:03:16 2007 -0700

    Lumpy Reclaim V4
    
    When we are out of memory of a suitable size we enter reclaim.  The current
    reclaim algorithm targets pages in LRU order, which is great for fairness at
    order-0 but highly unsuitable if you desire pages at higher orders.  To get
    pages of higher order we must shoot down a very high proportion of memory;
    >95% in a lot of cases.
    
    This patch set adds a lumpy reclaim algorithm to the allocator.  It targets
    groups of pages at the specified order anchored at the end of the active and
    inactive lists.  This encourages groups of pages at the requested orders to
    move from active to inactive, and active to free lists.  This behaviour is
    only triggered out of direct reclaim when higher order pages have been
    requested.
    
    This patch set is particularly effective when utilised with an
    anti-fragmentation scheme which groups pages of similar reclaimability
    together.
    
    This patch set is based on Peter Zijlstra's lumpy reclaim V2 patch which forms
    the foundation.  Credit to Mel Gorman for sanitity checking.
    
    Mel said:
    
      The patches have an application with hugepage pool resizing.
    
      When lumpy-reclaim is used used with ZONE_MOVABLE, the hugepages pool can
      be resized with greater reliability.  Testing on a desktop machine with 2GB
      of RAM showed that growing the hugepage pool with ZONE_MOVABLE on it's own
      was very slow as the success rate was quite low.  Without lumpy-reclaim,
      each attempt to grow the pool by 100 pages would yield 1 or 2 hugepages.
      With lumpy-reclaim, getting 40 to 70 hugepages on each attempt was typical.
    
    [akpm@osdl.org: ia64 pfn_to_nid fixes and loop cleanup]
    [bunk@stusta.de: static declarations for internal functions]
    [a.p.zijlstra@chello.nl: initial lumpy V2 implementation]
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d71ff763c9df..da8eb8ad9e9b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -24,6 +24,14 @@
 #endif
 #define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))
 
+/*
+ * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
+ * costly to service.  That is between allocation orders which should
+ * coelesce naturally under reasonable reclaim pressure and those which
+ * will not.
+ */
+#define PAGE_ALLOC_COSTLY_ORDER 3
+
 struct free_area {
 	struct list_head	free_list;
 	unsigned long		nr_free;

commit 2a1e274acf0b1c192face19a4be7c12d4503eaaf
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jul 17 04:03:12 2007 -0700

    Create the ZONE_MOVABLE zone
    
    The following 8 patches against 2.6.20-mm2 create a zone called ZONE_MOVABLE
    that is only usable by allocations that specify both __GFP_HIGHMEM and
    __GFP_MOVABLE.  This has the effect of keeping all non-movable pages within a
    single memory partition while allowing movable allocations to be satisfied
    from either partition.  The patches may be applied with the list-based
    anti-fragmentation patches that groups pages together based on mobility.
    
    The size of the zone is determined by a kernelcore= parameter specified at
    boot-time.  This specifies how much memory is usable by non-movable
    allocations and the remainder is used for ZONE_MOVABLE.  Any range of pages
    within ZONE_MOVABLE can be released by migrating the pages or by reclaiming.
    
    When selecting a zone to take pages from for ZONE_MOVABLE, there are two
    things to consider.  First, only memory from the highest populated zone is
    used for ZONE_MOVABLE.  On the x86, this is probably going to be ZONE_HIGHMEM
    but it would be ZONE_DMA on ppc64 or possibly ZONE_DMA32 on x86_64.  Second,
    the amount of memory usable by the kernel will be spread evenly throughout
    NUMA nodes where possible.  If the nodes are not of equal size, the amount of
    memory usable by the kernel on some nodes may be greater than others.
    
    By default, the zone is not as useful for hugetlb allocations because they are
    pinned and non-migratable (currently at least).  A sysctl is provided that
    allows huge pages to be allocated from that zone.  This means that the huge
    page pool can be resized to the size of ZONE_MOVABLE during the lifetime of
    the system assuming that pages are not mlocked.  Despite huge pages being
    non-movable, we do not introduce additional external fragmentation of note as
    huge pages are always the largest contiguous block we care about.
    
    Credit goes to Andy Whitcroft for catching a large variety of problems during
    review of the patches.
    
    This patch creates an additional zone, ZONE_MOVABLE.  This zone is only usable
    by allocations which specify both __GFP_HIGHMEM and __GFP_MOVABLE.  Hot-added
    memory continues to be placed in their existing destination as there is no
    mechanism to redirect them to a specific zone.
    
    [y-goto@jp.fujitsu.com: Fix section mismatch of memory hotplug related code]
    [akpm@linux-foundation.org: various fixes]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 04b1636a970b..d71ff763c9df 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -146,6 +146,7 @@ enum zone_type {
 	 */
 	ZONE_HIGHMEM,
 #endif
+	ZONE_MOVABLE,
 	MAX_NR_ZONES
 };
 
@@ -167,6 +168,7 @@ enum zone_type {
 	+ defined(CONFIG_ZONE_DMA32)	\
 	+ 1				\
 	+ defined(CONFIG_HIGHMEM)	\
+	+ 1				\
 )
 #if __ZONE_COUNT < 2
 #define ZONES_SHIFT 0
@@ -499,10 +501,22 @@ static inline int populated_zone(struct zone *zone)
 	return (!!zone->present_pages);
 }
 
+extern int movable_zone;
+
+static inline int zone_movable_is_highmem(void)
+{
+#if defined(CONFIG_HIGHMEM) && defined(CONFIG_ARCH_POPULATES_NODE_MAP)
+	return movable_zone == ZONE_HIGHMEM;
+#else
+	return 0;
+#endif
+}
+
 static inline int is_highmem_idx(enum zone_type idx)
 {
 #ifdef CONFIG_HIGHMEM
-	return (idx == ZONE_HIGHMEM);
+	return (idx == ZONE_HIGHMEM ||
+		(idx == ZONE_MOVABLE && zone_movable_is_highmem()));
 #else
 	return 0;
 #endif
@@ -522,7 +536,9 @@ static inline int is_normal_idx(enum zone_type idx)
 static inline int is_highmem(struct zone *zone)
 {
 #ifdef CONFIG_HIGHMEM
-	return zone == zone->zone_pgdat->node_zones + ZONE_HIGHMEM;
+	int zone_idx = zone - zone->zone_pgdat->node_zones;
+	return zone_idx == ZONE_HIGHMEM ||
+		(zone_idx == ZONE_MOVABLE && zone_movable_is_highmem());
 #else
 	return 0;
 #endif

commit f0c0b2b808f232741eadac272bd4bc51f18df0f4
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Sun Jul 15 23:38:01 2007 -0700

    change zonelist order: zonelist order selection logic
    
    Make zonelist creation policy selectable from sysctl/boot option v6.
    
    This patch makes NUMA's zonelist (of pgdat) order selectable.
    Available order are Default(automatic)/ Node-based / Zone-based.
    
    [Default Order]
    The kernel selects Node-based or Zone-based order automatically.
    
    [Node-based Order]
    This policy treats the locality of memory as the most important parameter.
    Zonelist order is created by each zone's locality. This means lower zones
    (ex. ZONE_DMA) can be used before higher zone (ex. ZONE_NORMAL) exhausion.
    IOW. ZONE_DMA will be in the middle of zonelist.
    current 2.6.21 kernel uses this.
    
    Pros.
     * A user can expect local memory as much as possible.
    Cons.
     * lower zone will be exhansted before higher zone. This may cause OOM_KILL.
    
    Maybe suitable if ZONE_DMA is relatively big and you never see OOM_KILL
    because of ZONE_DMA exhaution and you need the best locality.
    
    (example)
    assume 2 node NUMA. node(0) has ZONE_DMA/ZONE_NORMAL, node(1) has ZONE_NORMAL.
    
    *node(0)'s memory allocation order:
    
     node(0)'s NORMAL -> node(0)'s DMA -> node(1)'s NORMAL.
    
    *node(1)'s memory allocation order:
    
     node(1)'s NORMAL -> node(0)'s NORMAL -> node(0)'s DMA.
    
    [Zone-based order]
    This policy treats the zone type as the most important parameter.
    Zonelist order is created by zone-type order. This means lower zone
    never be used bofere higher zone exhaustion.
    IOW. ZONE_DMA will be always at the tail of zonelist.
    
    Pros.
     * OOM_KILL(bacause of lower zone) occurs only if the whole zones are exhausted.
    Cons.
     * memory locality may not be best.
    
    (example)
    assume 2 node NUMA. node(0) has ZONE_DMA/ZONE_NORMAL, node(1) has ZONE_NORMAL.
    
    *node(0)'s memory allocation order:
    
     node(0)'s NORMAL -> node(1)'s NORMAL -> node(0)'s DMA.
    
    *node(1)'s memory allocation order:
    
     node(1)'s NORMAL -> node(0)'s NORMAL -> node(0)'s DMA.
    
    bootoption "numa_zonelist_order=" and proc/sysctl is supporetd.
    
    command:
    %echo N > /proc/sys/vm/numa_zonelist_order
    
    Will rebuild zonelist in Node-based order.
    
    command:
    %echo Z > /proc/sys/vm/numa_zonelist_order
    
    Will rebuild zonelist in Zone-based order.
    
    Thanks to Lee Schermerhorn, he gives me much help and codes.
    
    [Lee.Schermerhorn@hp.com: add check_highest_zone to build_zonelists_in_zone_order]
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "jesse.barnes@intel.com" <jesse.barnes@intel.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d09b1345a3a1..04b1636a970b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -566,6 +566,11 @@ int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
 int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
 			struct file *, void __user *, size_t *, loff_t *);
 
+extern int numa_zonelist_order_handler(struct ctl_table *, int,
+			struct file *, void __user *, size_t *, loff_t *);
+extern char numa_zonelist_order[];
+#define NUMA_ZONELIST_ORDER_LEN 16	/* string buffer size */
+
 #include <linux/topology.h>
 /* Returns the number of the current Node. */
 #ifndef numa_node_id

commit 4037d452202e34214e8a939fa5621b2b3bbb45b7
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 9 02:35:14 2007 -0700

    Move remote node draining out of slab allocators
    
    Currently the slab allocators contain callbacks into the page allocator to
    perform the draining of pagesets on remote nodes.  This requires SLUB to have
    a whole subsystem in order to be compatible with SLAB.  Moving node draining
    out of the slab allocators avoids a section of code in SLUB.
    
    Move the node draining so that is is done when the vm statistics are updated.
    At that point we are already touching all the cachelines with the pagesets of
    a processor.
    
    Add a expire counter there.  If we have to update per zone or global vm
    statistics then assume that the pageset will require subsequent draining.
    
    The expire counter will be decremented on each vm stats update pass until it
    reaches zero.  Then we will drain one batch from the pageset.  The draining
    will cause vm counter updates which will then cause another expiration until
    the pcp is empty.  So we will drain a batch every 3 seconds.
    
    Note that remote node draining is a somewhat esoteric feature that is required
    on large NUMA systems because otherwise significant portions of system memory
    can become trapped in pcp queues.  The number of pcp is determined by the
    number of processors and nodes in a system.  A system with 4 processors and 2
    nodes has 8 pcps which is okay.  But a system with 1024 processors and 512
    nodes has 512k pcps with a high potential for large amount of memory being
    caught in them.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2f1544e83042..d09b1345a3a1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -83,6 +83,9 @@ struct per_cpu_pages {
 
 struct per_cpu_pageset {
 	struct per_cpu_pages pcp[2];	/* 0: hot.  1: cold */
+#ifdef CONFIG_NUMA
+	s8 expire;
+#endif
 #ifdef CONFIG_SMP
 	s8 stat_threshold;
 	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];

commit 14e072984179d3d421bf9ab75cc67e0961742841
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Sun May 6 14:49:14 2007 -0700

    add pfn_valid_within helper for sub-MAX_ORDER hole detection
    
    Generally we work under the assumption that memory the mem_map array is
    contigious and valid out to MAX_ORDER_NR_PAGES block of pages, ie.  that if we
    have validated any page within this MAX_ORDER_NR_PAGES block we need not check
    any other.  This is not true when CONFIG_HOLES_IN_ZONE is set and we must
    check each and every reference we make from a pfn.
    
    Add a pfn_valid_within() helper which should be used when scanning pages
    within a MAX_ORDER_NR_PAGES block when we have already checked the validility
    of the block normally with pfn_valid().  This can then be optimised away when
    we do not have holes within a MAX_ORDER_NR_PAGES block of pages.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ee9e3143df4f..2f1544e83042 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -784,6 +784,18 @@ void sparse_init(void);
 void memory_present(int nid, unsigned long start, unsigned long end);
 unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
 
+/*
+ * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we
+ * need to check pfn validility within that MAX_ORDER_NR_PAGES block.
+ * pfn_valid_within() should be used in this case; we optimise this away
+ * when we have no holes within a MAX_ORDER_NR_PAGES block.
+ */
+#ifdef CONFIG_HOLES_IN_ZONE
+#define pfn_valid_within(pfn) pfn_valid(pfn)
+#else
+#define pfn_valid_within(pfn) (1)
+#endif
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MMZONE_H */

commit 4b51d66989218aad731a721b5b28c79bf5388c09
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:10 2007 -0800

    [PATCH] optional ZONE_DMA: optional ZONE_DMA in the VM
    
    Make ZONE_DMA optional in core code.
    
    - ifdef all code for ZONE_DMA and related definitions following the example
      for ZONE_DMA32 and ZONE_HIGHMEM.
    
    - Without ZONE_DMA, ZONE_HIGHMEM and ZONE_DMA32 we get to a ZONES_SHIFT of
      0.
    
    - Modify the VM statistics to work correctly without a DMA zone.
    
    - Modify slab to not create DMA slabs if there is no ZONE_DMA.
    
    [akpm@osdl.org: cleanup]
    [jdike@addtoit.com: build fix]
    [apw@shadowen.org: Simplify calculation of the number of bits we need for ZONES_SHIFT]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Matthew Wilcox <willy@debian.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 398f2ec55f54..ee9e3143df4f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -96,6 +96,7 @@ struct per_cpu_pageset {
 #endif
 
 enum zone_type {
+#ifdef CONFIG_ZONE_DMA
 	/*
 	 * ZONE_DMA is used when there are devices that are not able
 	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
@@ -116,6 +117,7 @@ enum zone_type {
 	 * 			<16M.
 	 */
 	ZONE_DMA,
+#endif
 #ifdef CONFIG_ZONE_DMA32
 	/*
 	 * x86_64 needs two ZONE_DMAs because it supports devices that are
@@ -152,11 +154,27 @@ enum zone_type {
  * match the requested limits. See gfp_zone() in include/linux/gfp.h
  */
 
-#if !defined(CONFIG_ZONE_DMA32) && !defined(CONFIG_HIGHMEM)
+/*
+ * Count the active zones.  Note that the use of defined(X) outside
+ * #if and family is not necessarily defined so ensure we cannot use
+ * it later.  Use __ZONE_COUNT to work out how many shift bits we need.
+ */
+#define __ZONE_COUNT (			\
+	  defined(CONFIG_ZONE_DMA)	\
+	+ defined(CONFIG_ZONE_DMA32)	\
+	+ 1				\
+	+ defined(CONFIG_HIGHMEM)	\
+)
+#if __ZONE_COUNT < 2
+#define ZONES_SHIFT 0
+#elif __ZONE_COUNT <= 2
 #define ZONES_SHIFT 1
-#else
+#elif __ZONE_COUNT <= 4
 #define ZONES_SHIFT 2
+#else
+#error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
+#undef __ZONE_COUNT
 
 struct zone {
 	/* Fields commonly accessed by the page allocator */
@@ -523,7 +541,11 @@ static inline int is_dma32(struct zone *zone)
 
 static inline int is_dma(struct zone *zone)
 {
+#ifdef CONFIG_ZONE_DMA
 	return zone == zone->zone_pgdat->node_zones + ZONE_DMA;
+#else
+	return 0;
+#endif
 }
 
 /* These two functions are used to setup the per zone pages min values */

commit 05a0416be2b88d859efcbc4a4290555a04d169a1
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:05 2007 -0800

    [PATCH] Drop __get_zone_counts()
    
    Values are readily available via ZVC per node and global sums.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d15b1f68aef9..398f2ec55f54 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -444,8 +444,6 @@ typedef struct pglist_data {
 
 #include <linux/memory_hotplug.h>
 
-void __get_zone_counts(unsigned long *active, unsigned long *inactive,
-			unsigned long *free, struct pglist_data *pgdat);
 void get_zone_counts(unsigned long *active, unsigned long *inactive,
 			unsigned long *free);
 void build_all_zonelists(void);

commit 51ed4491271be8c56bdb2a03481ed34ea4984bc2
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:02 2007 -0800

    [PATCH] Reorder ZVCs according to cacheline
    
    The global and per zone counter sums are in arrays of longs.  Reorder the ZVCs
    so that the most frequently used ZVCs are put into the same cacheline.  That
    way calculations of the global, node and per zone vm state touches only a
    single cacheline.  This is mostly important for 64 bit systems were one 128
    byte cacheline takes only 8 longs.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 824279c7884d..d15b1f68aef9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -47,6 +47,7 @@ struct zone_padding {
 #endif
 
 enum zone_stat_item {
+	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
 	NR_INACTIVE,
 	NR_ACTIVE,
@@ -54,11 +55,12 @@ enum zone_stat_item {
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
-	NR_SLAB_RECLAIMABLE,
-	NR_SLAB_UNRECLAIMABLE,
-	NR_PAGETABLE,	/* used for pagetables */
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
+	/* Second 128 byte cacheline */
+	NR_SLAB_RECLAIMABLE,
+	NR_SLAB_UNRECLAIMABLE,
+	NR_PAGETABLE,		/* used for pagetables */
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,

commit d23ad42324cc4378132e51f2fc5c9ba6cbe75182
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:02 2007 -0800

    [PATCH] Use ZVC for free_pages
    
    This is again simplifies some of the VM counter calculations through the use
    of the ZVC consolidated counters.
    
    [michal.k.k.piotrowski@gmail.com: build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Michal Piotrowski <michal.k.k.piotrowski@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9137d1b9735c..824279c7884d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -47,6 +47,7 @@ struct zone_padding {
 #endif
 
 enum zone_stat_item {
+	NR_FREE_PAGES,
 	NR_INACTIVE,
 	NR_ACTIVE,
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
@@ -157,7 +158,6 @@ enum zone_type {
 
 struct zone {
 	/* Fields commonly accessed by the page allocator */
-	unsigned long		free_pages;
 	unsigned long		pages_min, pages_low, pages_high;
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable

commit c878538598d1e7ab41ecc0de8894e34e2fdef630
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:01 2007 -0800

    [PATCH] Use ZVC for inactive and active counts
    
    The determination of the dirty ratio to determine writeback behavior is
    currently based on the number of total pages on the system.
    
    However, not all pages in the system may be dirtied.  Thus the ratio is always
    too low and can never reach 100%.  The ratio may be particularly skewed if
    large hugepage allocations, slab allocations or device driver buffers make
    large sections of memory not available anymore.  In that case we may get into
    a situation in which f.e.  the background writeback ratio of 40% cannot be
    reached anymore which leads to undesired writeback behavior.
    
    This patchset fixes that issue by determining the ratio based on the actual
    pages that may potentially be dirty.  These are the pages on the active and
    the inactive list plus free pages.
    
    The problem with those counts has so far been that it is expensive to
    calculate these because counts from multiple nodes and multiple zones will
    have to be summed up.  This patchset makes these counters ZVC counters.  This
    means that a current sum per zone, per node and for the whole system is always
    available via global variables and not expensive anymore to calculate.
    
    The patchset results in some other good side effects:
    
    - Removal of the various functions that sum up free, active and inactive
      page counts
    
    - Cleanup of the functions that display information via the proc filesystem.
    
    This patch:
    
    The use of a ZVC for nr_inactive and nr_active allows a simplification of some
    counter operations.  More ZVC functionality is used for sums etc in the
    following patches.
    
    [akpm@osdl.org: UP build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b262f47961fb..9137d1b9735c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -47,6 +47,8 @@ struct zone_padding {
 #endif
 
 enum zone_stat_item {
+	NR_INACTIVE,
+	NR_ACTIVE,
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
@@ -197,8 +199,6 @@ struct zone {
 	struct list_head	inactive_list;
 	unsigned long		nr_scan_active;
 	unsigned long		nr_scan_inactive;
-	unsigned long		nr_active;
-	unsigned long		nr_inactive;
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	int			all_unreclaimable; /* All pages pinned */
 

commit a2f3aa02576632cdb60bd3de1f4bf55e9ac65604
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Wed Jan 10 23:15:30 2007 -0800

    [PATCH] Fix sparsemem on Cell
    
    Fix an oops experienced on the Cell architecture when init-time functions,
    early_*(), are called at runtime.  It alters the call paths to make sure
    that the callers explicitly say whether the call is being made on behalf of
    a hotplug even, or happening at boot-time.
    
    It has been compile tested on ppc64, ia64, s390, i386 and x86_64.
    
    Acked-by: Arnd Bergmann <arndb@de.ibm.com>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e339a7345f25..b262f47961fb 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -450,9 +450,13 @@ void build_all_zonelists(void);
 void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);
-
+enum memmap_context {
+	MEMMAP_EARLY,
+	MEMMAP_HOTPLUG,
+};
 extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
-				     unsigned long size);
+				     unsigned long size,
+				     enum memmap_context context);
 
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);

commit 15ad7cdcfd76450d4beebc789ec646664238184d
Author: Helge Deller <deller@gmx.de>
Date:   Wed Dec 6 20:40:36 2006 -0800

    [PATCH] struct seq_operations and struct file_operations constification
    
     - move some file_operations structs into the .rodata section
    
     - move static strings from policy_types[] array into the .rodata section
    
     - fix generic seq_operations usages, so that those structs may be defined
       as "const" as well
    
    [akpm@osdl.org: couple of fixes]
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index da6002dec205..e339a7345f25 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -278,7 +278,7 @@ struct zone {
 	/*
 	 * rarely used fields:
 	 */
-	char			*name;
+	const char		*name;
 } ____cacheline_internodealigned_in_smp;
 
 /*

commit 7253f4ef04b1cd138baf2b29a95473743ac0a307
Author: Paul Jackson <pj@sgi.com>
Date:   Wed Dec 6 20:31:49 2006 -0800

    [PATCH] memory page_alloc zonelist caching reorder structure
    
    Rearrange the struct members in the 'struct zonelist_cache' structure, so
    as to put the readonly (once initialized) z_to_n[] array first, where it
    will come right after the zones[] array in struct zonelist.
    
    This pretty much eliminates the chance that the two frequently written
    elements of 'struct zonelist_cache', the fullzones bitmap and last_full_zap
    times, will end up on the same cache line as the performance sensitive,
    frequently read, never (after init) written zones[] array.
    
    Keeping frequently written data off frequently read cache lines is good for
    performance.
    
    Thanks to Rohit Seth for the suggestion.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Rohit Seth <rohitseth@google.com>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 09bf9d8d7b72..da6002dec205 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -352,8 +352,8 @@ struct zone {
 
 
 struct zonelist_cache {
-	DECLARE_BITMAP(fullzones, MAX_ZONES_PER_ZONELIST);	/* zone full? */
 	unsigned short z_to_n[MAX_ZONES_PER_ZONELIST];		/* zone->nid */
+	DECLARE_BITMAP(fullzones, MAX_ZONES_PER_ZONELIST);	/* zone full? */
 	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
 };
 #else

commit 9276b1bc96a132f4068fdee00983c532f43d3a26
Author: Paul Jackson <pj@sgi.com>
Date:   Wed Dec 6 20:31:48 2006 -0800

    [PATCH] memory page_alloc zonelist caching speedup
    
    Optimize the critical zonelist scanning for free pages in the kernel memory
    allocator by caching the zones that were found to be full recently, and
    skipping them.
    
    Remembers the zones in a zonelist that were short of free memory in the
    last second.  And it stashes a zone-to-node table in the zonelist struct,
    to optimize that conversion (minimize its cache footprint.)
    
    Recent changes:
    
        This differs in a significant way from a similar patch that I
        posted a week ago.  Now, instead of having a nodemask_t of
        recently full nodes, I have a bitmask of recently full zones.
        This solves a problem that last weeks patch had, which on
        systems with multiple zones per node (such as DMA zone) would
        take seeing any of these zones full as meaning that all zones
        on that node were full.
    
        Also I changed names - from "zonelist faster" to "zonelist cache",
        as that seemed to better convey what we're doing here - caching
        some of the key zonelist state (for faster access.)
    
        See below for some performance benchmark results.  After all that
        discussion with David on why I didn't need them, I went and got
        some ;).  I wanted to verify that I had not hurt the normal case
        of memory allocation noticeably.  At least for my one little
        microbenchmark, I found (1) the normal case wasn't affected, and
        (2) workloads that forced scanning across multiple nodes for
        memory improved up to 10% fewer System CPU cycles and lower
        elapsed clock time ('sys' and 'real').  Good.  See details, below.
    
        I didn't have the logic in get_page_from_freelist() for various
        full nodes and zone reclaim failures correct.  That should be
        fixed up now - notice the new goto labels zonelist_scan,
        this_zone_full, and try_next_zone, in get_page_from_freelist().
    
    There are two reasons I persued this alternative, over some earlier
    proposals that would have focused on optimizing the fake numa
    emulation case by caching the last useful zone:
    
     1) Contrary to what I said before, we (SGI, on large ia64 sn2 systems)
        have seen real customer loads where the cost to scan the zonelist
        was a problem, due to many nodes being full of memory before
        we got to a node we could use.  Or at least, I think we have.
        This was related to me by another engineer, based on experiences
        from some time past.  So this is not guaranteed.  Most likely, though.
    
        The following approach should help such real numa systems just as
        much as it helps fake numa systems, or any combination thereof.
    
     2) The effort to distinguish fake from real numa, using node_distance,
        so that we could cache a fake numa node and optimize choosing
        it over equivalent distance fake nodes, while continuing to
        properly scan all real nodes in distance order, was going to
        require a nasty blob of zonelist and node distance munging.
    
        The following approach has no new dependency on node distances or
        zone sorting.
    
    See comment in the patch below for a description of what it actually does.
    
    Technical details of note (or controversy):
    
     - See the use of "zlc_active" and "did_zlc_setup" below, to delay
       adding any work for this new mechanism until we've looked at the
       first zone in zonelist.  I figured the odds of the first zone
       having the memory we needed were high enough that we should just
       look there, first, then get fancy only if we need to keep looking.
    
     - Some odd hackery was needed to add items to struct zonelist, while
       not tripping up the custom zonelists built by the mm/mempolicy.c
       code for MPOL_BIND.  My usual wordy comments below explain this.
       Search for "MPOL_BIND".
    
     - Some per-node data in the struct zonelist is now modified frequently,
       with no locking.  Multiple CPU cores on a node could hit and mangle
       this data.  The theory is that this is just performance hint data,
       and the memory allocator will work just fine despite any such mangling.
       The fields at risk are the struct 'zonelist_cache' fields 'fullzones'
       (a bitmask) and 'last_full_zap' (unsigned long jiffies).  It should
       all be self correcting after at most a one second delay.
    
     - This still does a linear scan of the same lengths as before.  All
       I've optimized is making the scan faster, not algorithmically
       shorter.  It is now able to scan a compact array of 'unsigned
       short' in the case of many full nodes, so one cache line should
       cover quite a few nodes, rather than each node hitting another
       one or two new and distinct cache lines.
    
     - If both Andi and Nick don't find this too complicated, I will be
       (pleasantly) flabbergasted.
    
     - I removed the comment claiming we only use one cachline's worth of
       zonelist.  We seem, at least in the fake numa case, to have put the
       lie to that claim.
    
     - I pay no attention to the various watermarks and such in this performance
       hint.  A node could be marked full for one watermark, and then skipped
       over when searching for a page using a different watermark.  I think
       that's actually quite ok, as it will tend to slightly increase the
       spreading of memory over other nodes, away from a memory stressed node.
    
    ===============
    
    Performance - some benchmark results and analysis:
    
    This benchmark runs a memory hog program that uses multiple
    threads to touch alot of memory as quickly as it can.
    
    Multiple runs were made, touching 12, 38, 64 or 90 GBytes out of
    the total 96 GBytes on the system, and using 1, 19, 37, or 55
    threads (on a 56 CPU system.)  System, user and real (elapsed)
    timings were recorded for each run, shown in units of seconds,
    in the table below.
    
    Two kernels were tested - 2.6.18-mm3 and the same kernel with
    this zonelist caching patch added.  The table also shows the
    percentage improvement the zonelist caching sys time is over
    (lower than) the stock *-mm kernel.
    
          number     2.6.18-mm3        zonelist-cache    delta (< 0 good)   percent
     GBs    N       ------------       --------------    ----------------   systime
     mem threads   sys user  real     sys  user  real     sys  user  real    better
      12     1     153   24   177     151    24   176      -2     0    -1      1%
      12    19      99   22     8      99    22     8       0     0     0      0%
      12    37     111   25     6     112    25     6       1     0     0     -0%
      12    55     115   25     5     110    23     5      -5    -2     0      4%
      38     1     502   74   576     497    73   570      -5    -1    -6      0%
      38    19     426   78    48     373    76    39     -53    -2    -9     12%
      38    37     544   83    36     547    82    36       3    -1     0     -0%
      38    55     501   77    23     511    80    24      10     3     1     -1%
      64     1     917  125  1042     890   124  1014     -27    -1   -28      2%
      64    19    1118  138   119     965   141   103    -153     3   -16     13%
      64    37    1202  151    94    1136   150    81     -66    -1   -13      5%
      64    55    1118  141    61    1072   140    58     -46    -1    -3      4%
      90     1    1342  177  1519    1275   174  1450     -67    -3   -69      4%
      90    19    2392  199   192    2116   189   176    -276   -10   -16     11%
      90    37    3313  238   175    2972   225   145    -341   -13   -30     10%
      90    55    1948  210   104    1843   213   100    -105     3    -4      5%
    
    Notes:
     1) This test ran a memory hog program that started a specified number N of
        threads, and had each thread allocate and touch 1/N'th of
        the total memory to be used in the test run in a single loop,
        writing a constant word to memory, one store every 4096 bytes.
        Watching this test during some earlier trial runs, I would see
        each of these threads sit down on one CPU and stay there, for
        the remainder of the pass, a different CPU for each thread.
    
     2) The 'real' column is not comparable to the 'sys' or 'user' columns.
        The 'real' column is seconds wall clock time elapsed, from beginning
        to end of that test pass.  The 'sys' and 'user' columns are total
        CPU seconds spent on that test pass.  For a 19 thread test run,
        for example, the sum of 'sys' and 'user' could be up to 19 times the
        number of 'real' elapsed wall clock seconds.
    
     3) Tests were run on a fresh, single-user boot, to minimize the amount
        of memory already in use at the start of the test, and to minimize
        the amount of background activity that might interfere.
    
     4) Tests were done on a 56 CPU, 28 Node system with 96 GBytes of RAM.
    
     5) Notice that the 'real' time gets large for the single thread runs, even
        though the measured 'sys' and 'user' times are modest.  I'm not sure what
        that means - probably something to do with it being slow for one thread to
        be accessing memory along ways away.  Perhaps the fake numa system, running
        ostensibly the same workload, would not show this substantial degradation
        of 'real' time for one thread on many nodes -- lets hope not.
    
     6) The high thread count passes (one thread per CPU - on 55 of 56 CPUs)
        ran quite efficiently, as one might expect.  Each pair of threads needed
        to allocate and touch the memory on the node the two threads shared, a
        pleasantly parallizable workload.
    
     7) The intermediate thread count passes, when asking for alot of memory forcing
        them to go to a few neighboring nodes, improved the most with this zonelist
        caching patch.
    
    Conclusions:
     * This zonelist cache patch probably makes little difference one way or the
       other for most workloads on real numa hardware, if those workloads avoid
       heavy off node allocations.
     * For memory intensive workloads requiring substantial off-node allocations
       on real numa hardware, this patch improves both kernel and elapsed timings
       up to ten per-cent.
     * For fake numa systems, I'm optimistic, but will have to leave that up to
       Rohit Seth to actually test (once I get him a 2.6.18 backport.)
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Rohit Seth <rohitseth@google.com>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: David Rientjes <rientjes@cs.washington.edu>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e06683e2bea3..09bf9d8d7b72 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -288,19 +288,94 @@ struct zone {
  */
 #define DEF_PRIORITY 12
 
+/* Maximum number of zones on a zonelist */
+#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)
+
+#ifdef CONFIG_NUMA
+/*
+ * We cache key information from each zonelist for smaller cache
+ * footprint when scanning for free pages in get_page_from_freelist().
+ *
+ * 1) The BITMAP fullzones tracks which zones in a zonelist have come
+ *    up short of free memory since the last time (last_fullzone_zap)
+ *    we zero'd fullzones.
+ * 2) The array z_to_n[] maps each zone in the zonelist to its node
+ *    id, so that we can efficiently evaluate whether that node is
+ *    set in the current tasks mems_allowed.
+ *
+ * Both fullzones and z_to_n[] are one-to-one with the zonelist,
+ * indexed by a zones offset in the zonelist zones[] array.
+ *
+ * The get_page_from_freelist() routine does two scans.  During the
+ * first scan, we skip zones whose corresponding bit in 'fullzones'
+ * is set or whose corresponding node in current->mems_allowed (which
+ * comes from cpusets) is not set.  During the second scan, we bypass
+ * this zonelist_cache, to ensure we look methodically at each zone.
+ *
+ * Once per second, we zero out (zap) fullzones, forcing us to
+ * reconsider nodes that might have regained more free memory.
+ * The field last_full_zap is the time we last zapped fullzones.
+ *
+ * This mechanism reduces the amount of time we waste repeatedly
+ * reexaming zones for free memory when they just came up low on
+ * memory momentarilly ago.
+ *
+ * The zonelist_cache struct members logically belong in struct
+ * zonelist.  However, the mempolicy zonelists constructed for
+ * MPOL_BIND are intentionally variable length (and usually much
+ * shorter).  A general purpose mechanism for handling structs with
+ * multiple variable length members is more mechanism than we want
+ * here.  We resort to some special case hackery instead.
+ *
+ * The MPOL_BIND zonelists don't need this zonelist_cache (in good
+ * part because they are shorter), so we put the fixed length stuff
+ * at the front of the zonelist struct, ending in a variable length
+ * zones[], as is needed by MPOL_BIND.
+ *
+ * Then we put the optional zonelist cache on the end of the zonelist
+ * struct.  This optional stuff is found by a 'zlcache_ptr' pointer in
+ * the fixed length portion at the front of the struct.  This pointer
+ * both enables us to find the zonelist cache, and in the case of
+ * MPOL_BIND zonelists, (which will just set the zlcache_ptr to NULL)
+ * to know that the zonelist cache is not there.
+ *
+ * The end result is that struct zonelists come in two flavors:
+ *  1) The full, fixed length version, shown below, and
+ *  2) The custom zonelists for MPOL_BIND.
+ * The custom MPOL_BIND zonelists have a NULL zlcache_ptr and no zlcache.
+ *
+ * Even though there may be multiple CPU cores on a node modifying
+ * fullzones or last_full_zap in the same zonelist_cache at the same
+ * time, we don't lock it.  This is just hint data - if it is wrong now
+ * and then, the allocator will still function, perhaps a bit slower.
+ */
+
+
+struct zonelist_cache {
+	DECLARE_BITMAP(fullzones, MAX_ZONES_PER_ZONELIST);	/* zone full? */
+	unsigned short z_to_n[MAX_ZONES_PER_ZONELIST];		/* zone->nid */
+	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
+};
+#else
+struct zonelist_cache;
+#endif
+
 /*
  * One allocation request operates on a zonelist. A zonelist
  * is a list of zones, the first one is the 'goal' of the
  * allocation, the other zones are fallback zones, in decreasing
  * priority.
  *
- * Right now a zonelist takes up less than a cacheline. We never
- * modify it apart from boot-up, and only a few indices are used,
- * so despite the zonelist table being relatively big, the cache
- * footprint of this construct is very small.
+ * If zlcache_ptr is not NULL, then it is just the address of zlcache,
+ * as explained above.  If zlcache_ptr is NULL, there is no zlcache.
  */
+
 struct zonelist {
-	struct zone *zones[MAX_NUMNODES * MAX_NR_ZONES + 1]; // NULL delimited
+	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
+	struct zone *zones[MAX_ZONES_PER_ZONELIST + 1];      // NULL delimited
+#ifdef CONFIG_NUMA
+	struct zonelist_cache zlcache;			     // optional ...
+#endif
 };
 
 #ifdef CONFIG_ARCH_POPULATES_NODE_MAP

commit 3bb1a852ab6c9cdf211a2f4a2f502340c8c38eca
Author: Martin Bligh <mbligh@mbligh.org>
Date:   Sat Oct 28 10:38:24 2006 -0700

    [PATCH] vmscan: Fix temp_priority race
    
    The temp_priority field in zone is racy, as we can walk through a reclaim
    path, and just before we copy it into prev_priority, it can be overwritten
    (say with DEF_PRIORITY) by another reclaimer.
    
    The same bug is contained in both try_to_free_pages and balance_pgdat, but
    it is fixed slightly differently.  In balance_pgdat, we keep a separate
    priority record per zone in a local array.  In try_to_free_pages there is
    no need to do this, as the priority level is the same for all zones that we
    reclaim from.
    
    Impact of this bug is that temp_priority is copied into prev_priority, and
    setting this artificially high causes reclaimers to set distress
    artificially low.  They then fail to reclaim mapped pages, when they are,
    in fact, under severe memory pressure (their priority may be as low as 0).
    This causes the OOM killer to fire incorrectly.
    
    From: Andrew Morton <akpm@osdl.org>
    
    __zone_reclaim() isn't modifying zone->prev_priority.  But zone->prev_priority
    is used in the decision whether or not to bring mapped pages onto the inactive
    list.  Hence there's a risk here that __zone_reclaim() will fail because
    zone->prev_priority ir large (ie: low urgency) and lots of mapped pages end up
    stuck on the active list.
    
    Fix that up by decreasing (ie making more urgent) zone->prev_priority as
    __zone_reclaim() scans the zone's pages.
    
    This bug perhaps explains why ZONE_RECLAIM_PRIORITY was created.  It should be
    possible to remove that now, and to just start out at DEF_PRIORITY?
    
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ed0762b283a9..e06683e2bea3 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -218,13 +218,9 @@ struct zone {
 	 * under - it drives the swappiness decision: whether to unmap mapped
 	 * pages.
 	 *
-	 * temp_priority is used to remember the scanning priority at which
-	 * this zone was successfully refilled to free_pages == pages_high.
-	 *
-	 * Access to both these fields is quite racy even on uniprocessor.  But
+	 * Access to both this field is quite racy even on uniprocessor.  But
 	 * it is expected to average out OK.
 	 */
-	int temp_priority;
 	int prev_priority;
 
 

commit 7516795739bd53175629b90fab0ad488d7a6a9f7
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Sat Oct 21 10:24:14 2006 -0700

    [PATCH] Reintroduce NODES_SPAN_OTHER_NODES for powerpc
    
    Reintroduce NODES_SPAN_OTHER_NODES for powerpc
    
    Revert "[PATCH] Remove SPAN_OTHER_NODES config definition"
        This reverts commit f62859bb6871c5e4a8e591c60befc8caaf54db8c.
    Revert "[PATCH] mm: remove arch independent NODES_SPAN_OTHER_NODES"
        This reverts commit a94b3ab7eab4edcc9b2cb474b188f774c331adf7.
    
    Also update the comments to indicate that this is still required
    and where its used.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Will Schmidt <will_schmidt@vnet.ibm.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 59855b8718a0..ed0762b283a9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -674,6 +674,12 @@ void sparse_init(void);
 #define sparse_index_init(_sec, _nid)  do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
+#ifdef CONFIG_NODES_SPAN_OTHER_NODES
+#define early_pfn_in_nid(pfn, nid)	(early_pfn_to_nid(pfn) == (nid))
+#else
+#define early_pfn_in_nid(pfn, nid)	(1)
+#endif
+
 #ifndef early_pfn_valid
 #define early_pfn_valid(pfn)	(1)
 #endif

commit d5f541ed6e31518508c688912e7464facf253c87
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Sep 27 01:50:08 2006 -0700

    [PATCH] Add node to zone for the NUMA case
    
    Add the node in order to optimize zone_to_nid.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 562cf7a8f3ee..59855b8718a0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -168,6 +168,7 @@ struct zone {
 	unsigned long		lowmem_reserve[MAX_NR_ZONES];
 
 #ifdef CONFIG_NUMA
+	int node;
 	/*
 	 * zone reclaim becomes active if more unmapped pages exist.
 	 */

commit 5b99cd0effaf846240a15441aec459a592577eaf
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Sep 27 01:50:01 2006 -0700

    [PATCH] own header file for struct page
    
    This moves the definition of struct page from mm.h to its own header file
    page-struct.h.  This is a prereq to fix SetPageUptodate which is broken on
    s390:
    
    #define SetPageUptodate(_page)
           do {
                   struct page *__page = (_page);
                   if (!test_and_set_bit(PG_uptodate, &__page->flags))
                           page_test_and_clear_dirty(_page);
           } while (0)
    
    _page gets used twice in this macro which can cause subtle bugs.  Using
    __page for the page_test_and_clear_dirty call doesn't work since it causes
    yet another problem with the page_test_and_clear_dirty macro as well.
    
    In order to avoid all these problems caused by macros it seems to be a good
    idea to get rid of them and convert them to static inline functions.
    Because of header file include order it's necessary to have a seperate
    header file for the struct page definition.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1b0680cd84d2..562cf7a8f3ee 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -314,6 +314,11 @@ struct node_active_region {
 };
 #endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
 
+#ifndef CONFIG_DISCONTIGMEM
+/* The array of struct pages - for discontigmem use pgdat->lmem_map */
+extern struct page *mem_map;
+#endif
+
 /*
  * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
  * (mostly NUMA machines?) to denote a higher-level memory zone than the

commit e129b5c23c2b471d47f1c5d2b8b193fc2034af43
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Sep 27 01:50:00 2006 -0700

    [PATCH] vm: add per-zone writeout counter
    
    The VM is supposed to minimise the number of pages which get written off the
    LRU (for IO scheduling efficiency, and for high reclaim-success rates).  But
    we don't actually have a clear way of showing how true this is.
    
    So add `nr_vmscan_write' to /proc/vmstat and /proc/zoneinfo - the number of
    pages which have been written by the vm scanner in this zone and globally.
    
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7fa1cbe9fa7a..1b0680cd84d2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -58,6 +58,7 @@ enum zone_stat_item {
 	NR_WRITEBACK,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
+	NR_VMSCAN_WRITE,
 #ifdef CONFIG_NUMA
 	NUMA_HIT,		/* allocated in intended node */
 	NUMA_MISS,		/* allocated in non intended node */

commit c713216deebd95d2b0ab38fef8bb2361c0180c2d
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Sep 27 01:49:43 2006 -0700

    [PATCH] Introduce mechanism for registering active regions of memory
    
    At a basic level, architectures define structures to record where active
    ranges of page frames are located.  Once located, the code to calculate zone
    sizes and holes in each architecture is very similar.  Some of this zone and
    hole sizing code is difficult to read for no good reason.  This set of patches
    eliminates the similar-looking architecture-specific code.
    
    The patches introduce a mechanism where architectures register where the
    active ranges of page frames are with add_active_range().  When all areas have
    been discovered, free_area_init_nodes() is called to initialise the pgdat and
    zones.  The zone sizes and holes are then calculated in an architecture
    independent manner.
    
    Patch 1 introduces the mechanism for registering and initialising PFN ranges
    Patch 2 changes ppc to use the mechanism - 139 arch-specific LOC removed
    Patch 3 changes x86 to use the mechanism - 136 arch-specific LOC removed
    Patch 4 changes x86_64 to use the mechanism - 74 arch-specific LOC removed
    Patch 5 changes ia64 to use the mechanism - 52 arch-specific LOC removed
    Patch 6 accounts for mem_map as a memory hole as the pages are not reclaimable.
            It adjusts the watermarks slightly
    
    Tony Luck has successfully tested for ia64 on Itanium with tiger_defconfig,
    gensparse_defconfig and defconfig.  Bob Picco has also tested and debugged on
    IA64.  Jack Steiner successfully boot tested on a mammoth SGI IA64-based
    machine.  These were on patches against 2.6.17-rc1 and release 3 of these
    patches but there have been no ia64-changes since release 3.
    
    There are differences in the zone sizes for x86_64 as the arch-specific code
    for x86_64 accounts the kernel image and the starting mem_maps as memory holes
    but the architecture-independent code accounts the memory as present.
    
    The big benefit of this set of patches is a sizable reduction of
    architecture-specific code, some of which is very hairy.  There should be a
    greater reduction when other architectures use the same mechanisms for zone
    and hole sizing but I lack the hardware to test on.
    
    Additional credit;
            Dave Hansen for the initial suggestion and comments on early patches
            Andy Whitcroft for reviewing early versions and catching numerous
                    errors
            Tony Luck for testing and debugging on IA64
            Bob Picco for fixing bugs related to pfn registration, reviewing a
                    number of patch revisions, providing a number of suggestions
                    on future direction and testing heavily
            Jack Steiner and Robin Holt for testing on IA64 and clarifying
                    issues related to memory holes
            Yasunori for testing on IA64
            Andi Kleen for reviewing and feeding back about x86_64
            Christian Kujau for providing valuable information related to ACPI
                    problems on x86_64 and testing potential fixes
    
    This patch:
    
    Define the structure to represent an active range of page frames within a node
    in an architecture independent manner.  Architectures are expected to register
    active ranges of PFNs using add_active_range(nid, start_pfn, end_pfn) and call
    free_area_init_nodes() passing the PFNs of the end of each zone.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Keith Mannthey" <kmannth@gmail.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3693f1a52788..7fa1cbe9fa7a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -305,6 +305,13 @@ struct zonelist {
 	struct zone *zones[MAX_NUMNODES * MAX_NR_ZONES + 1]; // NULL delimited
 };
 
+#ifdef CONFIG_ARCH_POPULATES_NODE_MAP
+struct node_active_region {
+	unsigned long start_pfn;
+	unsigned long end_pfn;
+	int nid;
+};
+#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
 
 /*
  * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
@@ -518,7 +525,8 @@ extern struct zone *next_zone(struct zone *zone);
 
 #endif
 
-#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID
+#if !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) && \
+	!defined(CONFIG_ARCH_POPULATES_NODE_MAP)
 #define early_pfn_to_nid(nid)  (0UL)
 #endif
 

commit 0ff38490c836dc379ff7ec45b10a15a662f4e5f6
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:52 2006 -0700

    [PATCH] zone_reclaim: dynamic slab reclaim
    
    Currently one can enable slab reclaim by setting an explicit option in
    /proc/sys/vm/zone_reclaim_mode.  Slab reclaim is then used as a final
    option if the freeing of unmapped file backed pages is not enough to free
    enough pages to allow a local allocation.
    
    However, that means that the slab can grow excessively and that most memory
    of a node may be used by slabs.  We have had a case where a machine with
    46GB of memory was using 40-42GB for slab.  Zone reclaim was effective in
    dealing with pagecache pages.  However, slab reclaim was only done during
    global reclaim (which is a bit rare on NUMA systems).
    
    This patch implements slab reclaim during zone reclaim.  Zone reclaim
    occurs if there is a danger of an off node allocation.  At that point we
    
    1. Shrink the per node page cache if the number of pagecache
       pages is more than min_unmapped_ratio percent of pages in a zone.
    
    2. Shrink the slab cache if the number of the nodes reclaimable slab pages
       (patch depends on earlier one that implements that counter)
       are more than min_slab_ratio (a new /proc/sys/vm tunable).
    
    The shrinking of the slab cache is a bit problematic since it is not node
    specific.  So we simply calculate what point in the slab we want to reach
    (current per node slab use minus the number of pages that neeed to be
    allocated) and then repeately run the global reclaim until that is
    unsuccessful or we have reached the limit.  I hope we will have zone based
    slab reclaim at some point which will make that easier.
    
    The default for the min_slab_ratio is 5%
    
    Also remove the slab option from /proc/sys/vm/zone_reclaim_mode.
    
    [akpm@osdl.org: cleanups]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 08c41b9f92e0..3693f1a52788 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -171,6 +171,7 @@ struct zone {
 	 * zone reclaim becomes active if more unmapped pages exist.
 	 */
 	unsigned long		min_unmapped_pages;
+	unsigned long		min_slab_pages;
 	struct per_cpu_pageset	*pageset[NR_CPUS];
 #else
 	struct per_cpu_pageset	pageset[NR_CPUS];
@@ -448,6 +449,8 @@ int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int, struct file
 					void __user *, size_t *, loff_t *);
 int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
 			struct file *, void __user *, size_t *, loff_t *);
+int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
+			struct file *, void __user *, size_t *, loff_t *);
 
 #include <linux/topology.h>
 /* Returns the number of the current Node. */

commit 972d1a7b140569084439a81265a0f15b74e924e0
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:51 2006 -0700

    [PATCH] ZVC: Support NR_SLAB_RECLAIMABLE / NR_SLAB_UNRECLAIMABLE
    
    Remove the atomic counter for slab_reclaim_pages and replace the counter
    and NR_SLAB with two ZVC counter that account for unreclaimable and
    reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE.
    
    Change the check in vmscan.c to refer to to NR_SLAB_RECLAIMABLE.  The
    intend seems to be to check for slab pages that could be freed.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a703527e2b45..08c41b9f92e0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -51,7 +51,8 @@ enum zone_stat_item {
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
-	NR_SLAB,	/* Pages used by slab allocator */
+	NR_SLAB_RECLAIMABLE,
+	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,	/* used for pagetables */
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,

commit 8417bba4b151346ed475fcc923693c9e3be89063
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:51 2006 -0700

    [PATCH] Replace min_unmapped_ratio by min_unmapped_pages in struct zone
    
    *_pages is a better description of the role of the variable.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7fe317164b73..a703527e2b45 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -169,7 +169,7 @@ struct zone {
 	/*
 	 * zone reclaim becomes active if more unmapped pages exist.
 	 */
-	unsigned long		min_unmapped_ratio;
+	unsigned long		min_unmapped_pages;
 	struct per_cpu_pageset	*pageset[NR_CPUS];
 #else
 	struct per_cpu_pageset	pageset[NR_CPUS];

commit 19655d3487001d7df0e10e9cbfc27c758b77c2b5
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:19 2006 -0700

    [PATCH] linearly index zone->node_zonelists[]
    
    I wonder why we need this bitmask indexing into zone->node_zonelists[]?
    
    We always start with the highest zone and then include all lower zones
    if we build zonelists.
    
    Are there really cases where we need allocation from ZONE_DMA or
    ZONE_HIGHMEM but not ZONE_NORMAL? It seems that the current implementation
    of highest_zone() makes that already impossible.
    
    If we go linear on the index then gfp_zone() == highest_zone() and a lot
    of definitions fall by the wayside.
    
    We can now revert back to the use of gfp_zone() in mempolicy.c ;-)
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 76d33e688593..7fe317164b73 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -137,60 +137,18 @@ enum zone_type {
 	MAX_NR_ZONES
 };
 
-
 /*
  * When a memory allocation must conform to specific limitations (such
  * as being suitable for DMA) the caller will pass in hints to the
  * allocator in the gfp_mask, in the zone modifier bits.  These bits
  * are used to select a priority ordered list of memory zones which
- * match the requested limits.  GFP_ZONEMASK defines which bits within
- * the gfp_mask should be considered as zone modifiers.  Each valid
- * combination of the zone modifier bits has a corresponding list
- * of zones (in node_zonelists).  Thus for two zone modifiers there
- * will be a maximum of 4 (2 ** 2) zonelists, for 3 modifiers there will
- * be 8 (2 ** 3) zonelists.  GFP_ZONETYPES defines the number of possible
- * combinations of zone modifiers in "zone modifier space".
- *
- * As an optimisation any zone modifier bits which are only valid when
- * no other zone modifier bits are set (loners) should be placed in
- * the highest order bits of this field.  This allows us to reduce the
- * extent of the zonelists thus saving space.  For example in the case
- * of three zone modifier bits, we could require up to eight zonelists.
- * If the left most zone modifier is a "loner" then the highest valid
- * zonelist would be four allowing us to allocate only five zonelists.
- * Use the first form for GFP_ZONETYPES when the left most bit is not
- * a "loner", otherwise use the second.
- *
- * NOTE! Make sure this matches the zones in <linux/gfp.h>
+ * match the requested limits. See gfp_zone() in include/linux/gfp.h
  */
 
-#ifdef CONFIG_ZONE_DMA32
-
-#ifdef CONFIG_HIGHMEM
-#define GFP_ZONETYPES		((GFP_ZONEMASK + 1) / 2 + 1)    /* Loner */
-#define GFP_ZONEMASK		0x07
-#define ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
-#else
-#define GFP_ZONETYPES		((0x07 + 1) / 2 + 1)    /* Loner */
-/* Mask __GFP_HIGHMEM */
-#define GFP_ZONEMASK		0x05
-#define ZONES_SHIFT		2
-#endif
-
-#else
-#ifdef CONFIG_HIGHMEM
-
-#define GFP_ZONEMASK		0x03
-#define ZONES_SHIFT		2
-#define GFP_ZONETYPES		3
-
+#if !defined(CONFIG_ZONE_DMA32) && !defined(CONFIG_HIGHMEM)
+#define ZONES_SHIFT 1
 #else
-
-#define GFP_ZONEMASK		0x01
-#define ZONES_SHIFT		1
-#define GFP_ZONETYPES		2
-
-#endif
+#define ZONES_SHIFT 2
 #endif
 
 struct zone {
@@ -360,7 +318,7 @@ struct zonelist {
 struct bootmem_data;
 typedef struct pglist_data {
 	struct zone node_zones[MAX_NR_ZONES];
-	struct zonelist node_zonelists[GFP_ZONETYPES];
+	struct zonelist node_zonelists[MAX_NR_ZONES];
 	int nr_zones;
 #ifdef CONFIG_FLAT_NODE_MEM_MAP
 	struct page *node_mem_map;

commit e53ef38d05dd59ed281a35590e4a5b64d8ff4c52
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:14 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: make ZONE_HIGHMEM optional
    
    Make ZONE_HIGHMEM optional
    
    - ifdef out code and definitions related to CONFIG_HIGHMEM
    
    - __GFP_HIGHMEM falls back to normal allocations if there is no
      ZONE_HIGHMEM
    
    - GFP_ZONEMASK becomes 0x01 if there is no DMA32 and no HIGHMEM
      zone.
    
    [jdike@addtoit.com: build fix]
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index adae3c915938..76d33e688593 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -123,6 +123,7 @@ enum zone_type {
 	 * transfers to all addressable memory.
 	 */
 	ZONE_NORMAL,
+#ifdef CONFIG_HIGHMEM
 	/*
 	 * A memory area that is only addressable by the kernel through
 	 * mapping portions into its own address space. This is for example
@@ -132,11 +133,10 @@ enum zone_type {
 	 * access.
 	 */
 	ZONE_HIGHMEM,
-
+#endif
 	MAX_NR_ZONES
 };
 
-#define	ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
 
 /*
  * When a memory allocation must conform to specific limitations (such
@@ -163,12 +163,34 @@ enum zone_type {
  *
  * NOTE! Make sure this matches the zones in <linux/gfp.h>
  */
-#define GFP_ZONETYPES		((GFP_ZONEMASK + 1) / 2 + 1)    /* Loner */
 
 #ifdef CONFIG_ZONE_DMA32
+
+#ifdef CONFIG_HIGHMEM
+#define GFP_ZONETYPES		((GFP_ZONEMASK + 1) / 2 + 1)    /* Loner */
 #define GFP_ZONEMASK		0x07
+#define ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
 #else
+#define GFP_ZONETYPES		((0x07 + 1) / 2 + 1)    /* Loner */
+/* Mask __GFP_HIGHMEM */
+#define GFP_ZONEMASK		0x05
+#define ZONES_SHIFT		2
+#endif
+
+#else
+#ifdef CONFIG_HIGHMEM
+
 #define GFP_ZONEMASK		0x03
+#define ZONES_SHIFT		2
+#define GFP_ZONETYPES		3
+
+#else
+
+#define GFP_ZONEMASK		0x01
+#define ZONES_SHIFT		1
+#define GFP_ZONETYPES		2
+
+#endif
 #endif
 
 struct zone {
@@ -409,7 +431,11 @@ static inline int populated_zone(struct zone *zone)
 
 static inline int is_highmem_idx(enum zone_type idx)
 {
+#ifdef CONFIG_HIGHMEM
 	return (idx == ZONE_HIGHMEM);
+#else
+	return 0;
+#endif
 }
 
 static inline int is_normal_idx(enum zone_type idx)
@@ -425,7 +451,11 @@ static inline int is_normal_idx(enum zone_type idx)
  */
 static inline int is_highmem(struct zone *zone)
 {
+#ifdef CONFIG_HIGHMEM
 	return zone == zone->zone_pgdat->node_zones + ZONE_HIGHMEM;
+#else
+	return 0;
+#endif
 }
 
 static inline int is_normal(struct zone *zone)

commit fb0e7942bdcbbd2f90e61cb4cfa4fa892a873f8a
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:13 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: make ZONE_DMA32 optional
    
    Make ZONE_DMA32 optional
    
    - Add #ifdefs around ZONE_DMA32 specific code and definitions.
    
    - Add CONFIG_ZONE_DMA32 config option and use that for x86_64
      that alone needs this zone.
    
    - Remove the use of CONFIG_DMA_IS_DMA32 and CONFIG_DMA_IS_NORMAL
      for ia64 and fix up the way per node ZVCs are calculated.
    
    - Fall back to prior GFP_ZONEMASK of 0x03 if there is no
      DMA32 zone.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 03a5a6eb0ffa..adae3c915938 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -109,12 +109,14 @@ enum zone_type {
 	 * 			<16M.
 	 */
 	ZONE_DMA,
+#ifdef CONFIG_ZONE_DMA32
 	/*
 	 * x86_64 needs two ZONE_DMAs because it supports devices that are
 	 * only able to do DMA to the lower 16M but also 32 bit devices that
 	 * can only do DMA areas below 4G.
 	 */
 	ZONE_DMA32,
+#endif
 	/*
 	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
 	 * performed on pages in ZONE_NORMAL if the DMA devices support
@@ -161,9 +163,13 @@ enum zone_type {
  *
  * NOTE! Make sure this matches the zones in <linux/gfp.h>
  */
-#define GFP_ZONEMASK	0x07
-/* #define GFP_ZONETYPES       (GFP_ZONEMASK + 1) */           /* Non-loner */
-#define GFP_ZONETYPES  ((GFP_ZONEMASK + 1) / 2 + 1)            /* Loner */
+#define GFP_ZONETYPES		((GFP_ZONEMASK + 1) / 2 + 1)    /* Loner */
+
+#ifdef CONFIG_ZONE_DMA32
+#define GFP_ZONEMASK		0x07
+#else
+#define GFP_ZONEMASK		0x03
+#endif
 
 struct zone {
 	/* Fields commonly accessed by the page allocator */
@@ -429,7 +435,11 @@ static inline int is_normal(struct zone *zone)
 
 static inline int is_dma32(struct zone *zone)
 {
+#ifdef CONFIG_ZONE_DMA32
 	return zone == zone->zone_pgdat->node_zones + ZONE_DMA32;
+#else
+	return 0;
+#endif
 }
 
 static inline int is_dma(struct zone *zone)

commit 2f1b6248682f8b39ca3c7e549dfc216d26c4109b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:13 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: use enum to define zones, reformat and comment
    
    Use enum for zones and reformat zones dependent information
    
    Add comments explaning the use of zones and add a zones_t type for zone
    numbers.
    
    Line up information that will be #ifdefd by the following patches.
    
    [akpm@osdl.org: comment cleanups]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f45163c528e8..03a5a6eb0ffa 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -88,14 +88,53 @@ struct per_cpu_pageset {
 #define zone_pcp(__z, __cpu) (&(__z)->pageset[(__cpu)])
 #endif
 
-#define ZONE_DMA		0
-#define ZONE_DMA32		1
-#define ZONE_NORMAL		2
-#define ZONE_HIGHMEM		3
+enum zone_type {
+	/*
+	 * ZONE_DMA is used when there are devices that are not able
+	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
+	 * carve out the portion of memory that is needed for these devices.
+	 * The range is arch specific.
+	 *
+	 * Some examples
+	 *
+	 * Architecture		Limit
+	 * ---------------------------
+	 * parisc, ia64, sparc	<4G
+	 * s390			<2G
+	 * arm26		<48M
+	 * arm			Various
+	 * alpha		Unlimited or 0-16MB.
+	 *
+	 * i386, x86_64 and multiple other arches
+	 * 			<16M.
+	 */
+	ZONE_DMA,
+	/*
+	 * x86_64 needs two ZONE_DMAs because it supports devices that are
+	 * only able to do DMA to the lower 16M but also 32 bit devices that
+	 * can only do DMA areas below 4G.
+	 */
+	ZONE_DMA32,
+	/*
+	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
+	 * performed on pages in ZONE_NORMAL if the DMA devices support
+	 * transfers to all addressable memory.
+	 */
+	ZONE_NORMAL,
+	/*
+	 * A memory area that is only addressable by the kernel through
+	 * mapping portions into its own address space. This is for example
+	 * used by i386 to allow the kernel to address the memory beyond
+	 * 900MB. The kernel will set up special mappings (page
+	 * table entries on i386) for each page that the kernel needs to
+	 * access.
+	 */
+	ZONE_HIGHMEM,
 
-#define MAX_NR_ZONES		4	/* Sync this with ZONES_SHIFT */
-#define ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
+	MAX_NR_ZONES
+};
 
+#define	ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
 
 /*
  * When a memory allocation must conform to specific limitations (such
@@ -126,16 +165,6 @@ struct per_cpu_pageset {
 /* #define GFP_ZONETYPES       (GFP_ZONEMASK + 1) */           /* Non-loner */
 #define GFP_ZONETYPES  ((GFP_ZONEMASK + 1) / 2 + 1)            /* Loner */
 
-/*
- * On machines where it is needed (eg PCs) we divide physical memory
- * into multiple physical zones. On a 32bit PC we have 4 zones:
- *
- * ZONE_DMA	  < 16 MB	ISA DMA capable memory
- * ZONE_DMA32	     0 MB 	Empty
- * ZONE_NORMAL	16-896 MB	direct mapped by the kernel
- * ZONE_HIGHMEM	 > 896 MB	only page cache and user processes
- */
-
 struct zone {
 	/* Fields commonly accessed by the page allocator */
 	unsigned long		free_pages;
@@ -266,7 +295,6 @@ struct zone {
 	char			*name;
 } ____cacheline_internodealigned_in_smp;
 
-
 /*
  * The "priority" of VM scanning is how much of the queues we will scan in one
  * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
@@ -373,12 +401,12 @@ static inline int populated_zone(struct zone *zone)
 	return (!!zone->present_pages);
 }
 
-static inline int is_highmem_idx(int idx)
+static inline int is_highmem_idx(enum zone_type idx)
 {
 	return (idx == ZONE_HIGHMEM);
 }
 
-static inline int is_normal_idx(int idx)
+static inline int is_normal_idx(enum zone_type idx)
 {
 	return (idx == ZONE_NORMAL);
 }

commit df9ecaba3f152d1ea79f2a5e0b87505e03f47590
Author: Christoph Lameter <clameter@sgi.com>
Date:   Thu Aug 31 21:27:35 2006 -0700

    [PATCH] ZVC: Scale thresholds depending on the size of the system
    
    The ZVC counter update threshold is currently set to a fixed value of 32.
    This patch sets up the threshold depending on the number of processors and
    the sizes of the zones in the system.
    
    With the current threshold of 32, I was able to observe slight contention
    when more than 130-140 processors concurrently updated the counters.  The
    contention vanished when I either increased the threshold to 64 or used
    Andrew's idea of overstepping the interval (see ZVC overstep patch).
    
    However, we saw contention again at 220-230 processors.  So we need higher
    values for larger systems.
    
    But the current default is already a bit of an overkill for smaller
    systems.  Some systems have tiny zones where precision matters.  For
    example i386 and x86_64 have 16M DMA zones and either 900M ZONE_NORMAL or
    ZONE_DMA32.  These are even present on SMP and NUMA systems.
    
    The patch here sets up a threshold based on the number of processors in the
    system and the size of the zone that these counters are used for.  The
    threshold should grow logarithmically, so we use fls() as an easy
    approximation.
    
    Results of tests on a system with 1024 processors (4TB RAM)
    
    The following output is from a test allocating 1GB of memory concurrently
    on each processor (Forking the process.  So contention on mmap_sem and the
    pte locks is not a factor):
    
                           X                   MIN
    TYPE:               CPUS       WALL       WALL        SYS     USER     TOTCPU
    fork                   1      0.552      0.552      0.540    0.012      0.552
    fork                   4      0.552      0.548      2.164    0.036      2.200
    fork                  16      0.564      0.548      8.812    0.164      8.976
    fork                 128      0.580      0.572     72.204    1.208     73.412
    fork                 256      1.300      0.660    310.400    2.160    312.560
    fork                 512      3.512      0.696   1526.836    4.816   1531.652
    fork                1020     20.024      0.700  17243.176    6.688  17249.863
    
    So a threshold of 32 is fine up to 128 processors. At 256 processors contention
    becomes a factor.
    
    Overstepping the counter (earlier patch) improves the numbers a bit:
    
    fork                   4      0.552      0.548      2.164    0.040      2.204
    fork                  16      0.552      0.548      8.640    0.148      8.788
    fork                 128      0.556      0.548     69.676    0.956     70.632
    fork                 256      0.876      0.636    212.468    2.108    214.576
    fork                 512      2.276      0.672    997.324    4.260   1001.584
    fork                1020     13.564      0.680  11586.436    6.088  11592.523
    
    Still contention at 512 and 1020. Contention at 1020 is down by a third.
    256 still has a slight bit of contention.
    
    After this patch the counter threshold will be set to 125 which reduces
    contention significantly:
    
    fork                 128      0.560      0.548     69.776    0.932     70.708
    fork                 256      0.636      0.556    143.460    2.036    145.496
    fork                 512      0.640      0.548    284.244    4.236    288.480
    fork                1020      1.500      0.588   1326.152    8.892   1335.044
    
    [akpm@osdl.org: !SMP build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 656b588a9f96..f45163c528e8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -77,6 +77,7 @@ struct per_cpu_pages {
 struct per_cpu_pageset {
 	struct per_cpu_pages pcp[2];	/* 0: hot.  1: cold */
 #ifdef CONFIG_SMP
+	s8 stat_threshold;
 	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
 #endif
 } ____cacheline_aligned_in_smp;

commit 9614634fe6a138fd8ae044950700d2af8d203f97
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Jul 3 00:24:13 2006 -0700

    [PATCH] ZVC/zone_reclaim: Leave 1% of unmapped pagecache pages for file I/O
    
    It turns out that it is advantageous to leave a small portion of unmapped file
    backed pages if all of a zone's pages (or almost all pages) are allocated and
    so the page allocator has to go off-node.
    
    This allows recently used file I/O buffers to stay on the node and
    reduces the times that zone reclaim is invoked if file I/O occurs
    when we run out of memory in a zone.
    
    The problem is that zone reclaim runs too frequently when the page cache is
    used for file I/O (read write and therefore unmapped pages!) alone and we have
    almost all pages of the zone allocated.  Zone reclaim may remove 32 unmapped
    pages.  File I/O will use these pages for the next read/write requests and the
    unmapped pages increase.  After the zone has filled up again zone reclaim will
    remove it again after only 32 pages.  This cycle is too inefficient and there
    are potentially too many zone reclaim cycles.
    
    With the 1% boundary we may still remove all unmapped pages for file I/O in
    zone reclaim pass.  However.  it will take a large number of read and writes
    to get back to 1% again where we trigger zone reclaim again.
    
    The zone reclaim 2.6.16/17 does not show this behavior because we have a 30
    second timeout.
    
    [akpm@osdl.org: rename the /proc file and the variable]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 27e748eb72b0..656b588a9f96 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -150,6 +150,10 @@ struct zone {
 	unsigned long		lowmem_reserve[MAX_NR_ZONES];
 
 #ifdef CONFIG_NUMA
+	/*
+	 * zone reclaim becomes active if more unmapped pages exist.
+	 */
+	unsigned long		min_unmapped_ratio;
 	struct per_cpu_pageset	*pageset[NR_CPUS];
 #else
 	struct per_cpu_pageset	pageset[NR_CPUS];
@@ -414,6 +418,8 @@ int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
 					void __user *, size_t *, loff_t *);
 int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int, struct file *,
 					void __user *, size_t *, loff_t *);
+int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
+			struct file *, void __user *, size_t *, loff_t *);
 
 #include <linux/topology.h>
 /* Returns the number of the current Node. */

commit ca889e6c45e0b112cb2ca9d35afc66297519b5d5
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:44 2006 -0700

    [PATCH] Use Zoned VM Counters for NUMA statistics
    
    The numa statistics are really event counters.  But they are per node and
    so we have had special treatment for these counters through additional
    fields on the pcp structure.  We can now use the per zone nature of the
    zoned VM counters to realize these.
    
    This will shrink the size of the pcp structure on NUMA systems.  We will
    have some room to add additional per zone counters that will all still fit
    in the same cacheline.
    
     Bits   Prior pcp size          Size after patch        We can add
     ------------------------------------------------------------------
     64     128 bytes (16 words)    80 bytes (10 words)     48
     32      76 bytes (19 words)    56 bytes (14 words)     8 (64 byte cacheline)
                                                            72 (128 byte)
    
    Remove the special statistics for numa and replace them with zoned vm
    counters.  This has the side effect that global sums of these events now
    show up in /proc/vmstat.
    
    Also take the opportunity to move the zone_statistics() function from
    page_alloc.c into vmstat.c.
    
    Discussions:
    V2 http://marc.theaimsgroup.com/?t=115048227000002&r=1&w=2
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2dbeec1d2874..27e748eb72b0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -57,6 +57,14 @@ enum zone_stat_item {
 	NR_WRITEBACK,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
+#ifdef CONFIG_NUMA
+	NUMA_HIT,		/* allocated in intended node */
+	NUMA_MISS,		/* allocated in non intended node */
+	NUMA_FOREIGN,		/* was intended here, hit elsewhere */
+	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
+	NUMA_LOCAL,		/* allocation from local node */
+	NUMA_OTHER,		/* allocation from other node */
+#endif
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {
@@ -71,15 +79,6 @@ struct per_cpu_pageset {
 #ifdef CONFIG_SMP
 	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
 #endif
-
-#ifdef CONFIG_NUMA
-	unsigned long numa_hit;		/* allocated in intended node */
-	unsigned long numa_miss;	/* allocated in non intended node */
-	unsigned long numa_foreign;	/* was intended here, hit elsewhere */
-	unsigned long interleave_hit; 	/* interleaver prefered this zone */
-	unsigned long local_node;	/* allocation from local node */
-	unsigned long other_node;	/* allocation from other node */
-#endif
 } ____cacheline_aligned_in_smp;
 
 #ifdef CONFIG_NUMA

commit d2c5e30c9a1420902262aa923794d2ae4e0bc391
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:41 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_bounce to per zone counter
    
    Conversion of nr_bounce to a per zone counter
    
    nr_bounce is only used for proc output.  So it could be left as an event
    counter.  However, the event counters may not be accurate and nr_bounce is
    categorizing types of pages in a zone.  So we really need this to also be a
    per zone counter.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e9d80697f555..2dbeec1d2874 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -56,6 +56,7 @@ enum zone_stat_item {
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
+	NR_BOUNCE,
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit fd39fc8561be33065306bdac0e30414e1e8ac8e1
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:40 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_unstable to per zone counter
    
    Conversion of nr_unstable to a per zone counter
    
    We need to do some special modifications to the nfs code since there are
    multiple cases of disposition and we need to have a page ref for proper
    accounting.
    
    This converts the last critical page state of the VM and therefore we need to
    remove several functions that were depending on GET_PAGE_STATE_LAST in order
    to make the kernel compile again.  We are only left with event type counters
    in page state.
    
    [akpm@osdl.org: bugfixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 885cc9727001..e9d80697f555 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -55,6 +55,7 @@ enum zone_stat_item {
 	NR_PAGETABLE,	/* used for pagetables */
 	NR_FILE_DIRTY,
 	NR_WRITEBACK,
+	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit ce866b34ae1b7f1ce60234cf65855886ac7e7d30
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:40 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_writeback to per zone counter
    
    Conversion of nr_writeback to per zone counter.
    
    This removes the last page_state counter from arch/i386/mm/pgtable.c so we
    drop the page_state from there.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 1cc8412ac264..885cc9727001 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -54,6 +54,7 @@ enum zone_stat_item {
 	NR_SLAB,	/* Pages used by slab allocator */
 	NR_PAGETABLE,	/* used for pagetables */
 	NR_FILE_DIRTY,
+	NR_WRITEBACK,
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit b1e7a8fd854d2f895730e82137400012b509650e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:39 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_dirty to per zone counter
    
    This makes nr_dirty a per zone counter.  Looping over all processors is
    avoided during writeback state determination.
    
    The counter aggregation for nr_dirty had to be undone in the NFS layer since
    we summed up the page counts from multiple zones.  Someone more familiar with
    NFS should probably review what I have done.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 15adb435f240..1cc8412ac264 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -53,6 +53,7 @@ enum zone_stat_item {
 	NR_FILE_PAGES,
 	NR_SLAB,	/* Pages used by slab allocator */
 	NR_PAGETABLE,	/* used for pagetables */
+	NR_FILE_DIRTY,
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit df849a1529c106f7460e51479ca78fe07b07dc8c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:38 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_pagetables to per zone counter
    
    Conversion of nr_page_table_pages to a per zone counter
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 67e03fc8533e..15adb435f240 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -52,6 +52,7 @@ enum zone_stat_item {
 			   only modified from process context */
 	NR_FILE_PAGES,
 	NR_SLAB,	/* Pages used by slab allocator */
+	NR_PAGETABLE,	/* used for pagetables */
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit 9a865ffa34b6117a5e0b67640a084d8c2e198c93
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:38 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_slab to per zone counter
    
    - Allows reclaim to access counter without looping over processor counts.
    
    - Allows accurate statistics on how many pages are used in a zone by
      the slab. This may become useful to balance slab allocations over
      various zones.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 839e9a04fd49..67e03fc8533e 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -51,6 +51,7 @@ enum zone_stat_item {
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
+	NR_SLAB,	/* Pages used by slab allocator */
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit 34aa1330f9b3c5783d269851d467326525207422
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:37 2006 -0700

    [PATCH] zoned vm counters: zone_reclaim: remove /proc/sys/vm/zone_reclaim_interval
    
    The zone_reclaim_interval was necessary because we were not able to determine
    how many unmapped pages exist in a zone.  Therefore we had to scan in
    intervals to figure out if any pages were unmapped.
    
    With the zoned counters and NR_ANON_PAGES we now know the number of pagecache
    pages and the number of mapped pages in a zone.  So we can simply skip the
    reclaim if there is an insufficient number of unmapped pages.  We use
    SWAP_CLUSTER_MAX as the boundary.
    
    Drop all support for /proc/sys/vm/zone_reclaim_interval.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4833abd4458b..839e9a04fd49 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -178,12 +178,6 @@ struct zone {
 
 	/* Zone statistics */
 	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
-	/*
-	 * timestamp (in jiffies) of the last zone reclaim that did not
-	 * result in freeing of pages. This is used to avoid repeated scans
-	 * if all memory in the zone is in use.
-	 */
-	unsigned long		last_unsuccessful_zone_reclaim;
 
 	/*
 	 * prev_priority holds the scanning priority for this zone.  It is

commit f3dbd34460ff54962d3e3244b6bcb7f5295356e6
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:36 2006 -0700

    [PATCH] zoned vm counters: split NR_ANON_PAGES off from NR_FILE_MAPPED
    
    The current NR_FILE_MAPPED is used by zone reclaim and the dirty load
    calculation as the number of mapped pagecache pages.  However, that is not
    true.  NR_FILE_MAPPED includes the mapped anonymous pages.  This patch
    separates those and therefore allows an accurate tracking of the anonymous
    pages per zone.
    
    It then becomes possible to determine the number of unmapped pages per zone
    and we can avoid scanning for unmapped pages if there are none.
    
    Also it may now be possible to determine the mapped/unmapped ratio in
    get_dirty_limit.  Isnt the number of anonymous pages irrelevant in that
    calculation?
    
    Note that this will change the meaning of the number of mapped pages reported
    in /proc/vmstat /proc/meminfo and in the per node statistics.  This may affect
    user space tools that monitor these counters!  NR_FILE_MAPPED works like
    NR_FILE_DIRTY.  It is only valid for pagecache pages.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 08be91e6cecf..4833abd4458b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -47,7 +47,8 @@ struct zone_padding {
 #endif
 
 enum zone_stat_item {
-	NR_FILE_MAPPED,	/* mapped into pagetables.
+	NR_ANON_PAGES,	/* Mapped anonymous pages */
+	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
 			   only modified from process context */
 	NR_FILE_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };

commit 347ce434d57da80fd5809c0c836f206a50999c26
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:35 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_pagecache to per zone counter
    
    Currently a single atomic variable is used to establish the size of the page
    cache in the whole machine.  The zoned VM counters have the same method of
    implementation as the nr_pagecache code but also allow the determination of
    the pagecache size per zone.
    
    Remove the special implementation for nr_pagecache and make it a zoned counter
    named NR_FILE_PAGES.
    
    Updates of the page cache counters are always performed with interrupts off.
    We can therefore use the __ variant here.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index eb42c1277023..08be91e6cecf 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -49,7 +49,7 @@ struct zone_padding {
 enum zone_stat_item {
 	NR_FILE_MAPPED,	/* mapped into pagetables.
 			   only modified from process context */
-
+	NR_FILE_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit 65ba55f500a37272985d071c9bbb35256a2f7c14
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:34 2006 -0700

    [PATCH] zoned vm counters: convert nr_mapped to per zone counter
    
    nr_mapped is important because it allows a determination of how many pages of
    a zone are not mapped, which would allow a more efficient means of determining
    when we need to reclaim memory in a zone.
    
    We take the nr_mapped field out of the page state structure and define a new
    per zone counter named NR_FILE_MAPPED (the anonymous pages will be split off
    from NR_MAPPED in the next patch).
    
    We replace the use of nr_mapped in various kernel locations.  This avoids the
    looping over all processors in try_to_free_pages(), writeback, reclaim (swap +
    zone reclaim).
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 543f9e411563..eb42c1277023 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -47,6 +47,9 @@ struct zone_padding {
 #endif
 
 enum zone_stat_item {
+	NR_FILE_MAPPED,	/* mapped into pagetables.
+			   only modified from process context */
+
 	NR_VM_ZONE_STAT_ITEMS };
 
 struct per_cpu_pages {

commit 2244b95a7bcf8d24196f8a3a44187ba5dfff754c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:33 2006 -0700

    [PATCH] zoned vm counters: basic ZVC (zoned vm counter) implementation
    
    Per zone counter infrastructure
    
    The counters that we currently have for the VM are split per processor.  The
    processor however has not much to do with the zone these pages belong to.  We
    cannot tell f.e.  how many ZONE_DMA pages are dirty.
    
    So we are blind to potentially inbalances in the usage of memory in various
    zones.  F.e.  in a NUMA system we cannot tell how many pages are dirty on a
    particular node.  If we knew then we could put measures into the VM to balance
    the use of memory between different zones and different nodes in a NUMA
    system.  For example it would be possible to limit the dirty pages per node so
    that fast local memory is kept available even if a process is dirtying huge
    amounts of pages.
    
    Another example is zone reclaim.  We do not know how many unmapped pages exist
    per zone.  So we just have to try to reclaim.  If it is not working then we
    pause and try again later.  It would be better if we knew when it makes sense
    to reclaim unmapped pages from a zone.  This patchset allows the determination
    of the number of unmapped pages per zone.  We can remove the zone reclaim
    interval with the counters introduced here.
    
    Futhermore the ability to have various usage statistics available will allow
    the development of new NUMA balancing algorithms that may be able to improve
    the decision making in the scheduler of when to move a process to another node
    and hopefully will also enable automatic page migration through a user space
    program that can analyse the memory load distribution and then rebalance
    memory use in order to increase performance.
    
    The counter framework here implements differential counters for each processor
    in struct zone.  The differential counters are consolidated when a threshold
    is exceeded (like done in the current implementation for nr_pageache), when
    slab reaping occurs or when a consolidation function is called.
    
    Consolidation uses atomic operations and accumulates counters per zone in the
    zone structure and also globally in the vm_stat array.  VM functions can
    access the counts by simply indexing a global or zone specific array.
    
    The arrangement of counters in an array also simplifies processing when output
    has to be generated for /proc/*.
    
    Counters can be updated by calling inc/dec_zone_page_state or
    _inc/dec_zone_page_state analogous to *_page_state.  The second group of
    functions can be called if it is known that interrupts are disabled.
    
    Special optimized increment and decrement functions are provided.  These can
    avoid certain checks and use increment or decrement instructions that an
    architecture may provide.
    
    We also add a new CONFIG_DMA_IS_NORMAL that signifies that an architecture can
    do DMA to all memory and therefore ZONE_NORMAL will not be populated.  This is
    only currently set for IA64 SGI SN2 and currently only affects
    node_page_state().  In the best case node_page_state can be reduced to
    retrieving a single counter for the one zone on the node.
    
    [akpm@osdl.org: cleanups]
    [akpm@osdl.org: export vm_stat[] for filesystems]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d6120fa69116..543f9e411563 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -46,6 +46,9 @@ struct zone_padding {
 #define ZONE_PADDING(name)
 #endif
 
+enum zone_stat_item {
+	NR_VM_ZONE_STAT_ITEMS };
+
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
 	int high;		/* high watermark, emptying needed */
@@ -55,6 +58,10 @@ struct per_cpu_pages {
 
 struct per_cpu_pageset {
 	struct per_cpu_pages pcp[2];	/* 0: hot.  1: cold */
+#ifdef CONFIG_SMP
+	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
+#endif
+
 #ifdef CONFIG_NUMA
 	unsigned long numa_hit;		/* allocated in intended node */
 	unsigned long numa_miss;	/* allocated in non intended node */
@@ -165,6 +172,8 @@ struct zone {
 	/* A count of how many reclaimers are scanning this zone */
 	atomic_t		reclaim_in_progress;
 
+	/* Zone statistics */
+	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
 	/*
 	 * timestamp (in jiffies) of the last zone reclaim that did not
 	 * result in freeing of pages. This is used to avoid repeated scans

commit 30c253e6da655d73eb8bfe2adca9b8f4d82fb81e
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jun 23 02:03:41 2006 -0700

    [PATCH] sparsemem: record nid during memory present
    
    Record the node id as we mark sections for instantiation.  Use this nid
    during instantiation to direct allocations.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Cc: Mike Kravetz <kravetz@us.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Bob Picco <bob.picco@hp.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Martin Bligh <mbligh@google.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e82fc1a52cd0..d6120fa69116 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -509,6 +509,10 @@ struct mem_section {
 	 * pages.  However, it is stored with some other magic.
 	 * (see sparse.c::sparse_init_one_section())
 	 *
+	 * Additionally during early boot we encode node id of
+	 * the location of the section here to guide allocation.
+	 * (see sparse.c::memory_present())
+	 *
 	 * Making it a UL at least makes someone do a cast
 	 * before using it wrong.
 	 */
@@ -548,6 +552,7 @@ extern int __section_nr(struct mem_section* ms);
 #define SECTION_HAS_MEM_MAP	(1UL<<1)
 #define SECTION_MAP_LAST_BIT	(1UL<<2)
 #define SECTION_MAP_MASK	(~(SECTION_MAP_LAST_BIT-1))
+#define SECTION_NID_SHIFT	2
 
 static inline struct page *__section_mem_map_addr(struct mem_section *section)
 {

commit 718127cc3170454f4aa274fdd2f1e01574fecd66
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Fri Jun 23 02:03:10 2006 -0700

    [PATCH] wait_table and zonelist initializing for memory hotadd: add return code for init_current_empty_zone
    
    When add_zone() is called against empty zone (not populated zone), we have to
    initialize the zone which didn't initialize at boot time.  But,
    init_currently_empty_zone() may fail due to allocation of wait table.  So,
    this patch is to catch its error code.
    
    Changes against wait_table is in the next patch.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 652673ea92f1..e82fc1a52cd0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -333,6 +333,9 @@ void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		int classzone_idx, int alloc_flags);
 
+extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
+				     unsigned long size);
+
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);
 #else

commit 02b694dea473ad3db1e2d1b14c1fef8fbd92e5e6
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Fri Jun 23 02:03:08 2006 -0700

    [PATCH] wait_table and zonelist initializing for memory hotadd: change name of wait_table_size()
    
    This is just to rename from wait_table_size() to wait_table_hash_nr_entries().
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9742e3c16222..652673ea92f1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -197,7 +197,7 @@ struct zone {
 
 	/*
 	 * wait_table		-- the array holding the hash table
-	 * wait_table_size	-- the size of the hash table array
+	 * wait_table_hash_nr_entries	-- the size of the hash table array
 	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
 	 *
 	 * The purpose of all these is to keep track of the people
@@ -220,7 +220,7 @@ struct zone {
 	 * free_area_init_core() performs the initialization of them.
 	 */
 	wait_queue_head_t	* wait_table;
-	unsigned long		wait_table_size;
+	unsigned long		wait_table_hash_nr_entries;
 	unsigned long		wait_table_bits;
 
 	/*

commit cee4cca740d209bcb4b9857baa2253d5ba4e3fbe
Merge: 2edc322d420a 9348f0de2d2b
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:10:08 2006 -0700

    Merge git://git.infradead.org/hdrcleanup-2.6
    
    * git://git.infradead.org/hdrcleanup-2.6: (63 commits)
      [S390] __FD_foo definitions.
      Switch to __s32 types in joystick.h instead of C99 types for consistency.
      Add <sys/types.h> to headers included for userspace in <linux/input.h>
      Move inclusion of <linux/compat.h> out of user scope in asm-x86_64/mtrr.h
      Remove struct fddi_statistics from user view in <linux/if_fddi.h>
      Move user-visible parts of drivers/s390/crypto/z90crypt.h to include/asm-s390
      Revert include/media changes: Mauro says those ioctls are only used in-kernel(!)
      Include <linux/types.h> and use __uXX types in <linux/cramfs_fs.h>
      Use __uXX types in <linux/i2o_dev.h>, include <linux/ioctl.h> too
      Remove private struct dx_hash_info from public view in <linux/ext3_fs.h>
      Include <linux/types.h> and use __uXX types in <linux/affs_hardblocks.h>
      Use __uXX types in <linux/divert.h> for struct divert_blk et al.
      Use __u32 for elf_addr_t in <asm-powerpc/elf.h>, not u32. It's user-visible.
      Remove PPP_FCS from user view in <linux/ppp_defs.h>, remove __P mess entirely
      Use __uXX types in user-visible structures in <linux/nbd.h>
      Don't use 'u32' in user-visible struct ip_conntrack_old_tuple.
      Use __uXX types for S390 DASD volume label definitions which are user-visible
      S390 BIODASDREADCMB ioctl should use __u64 not u64 type.
      Remove unneeded inclusion of <linux/time.h> from <linux/ufs_fs.h>
      Fix private integer types used in V4L2 ioctls.
      ...
    
    Manually resolve conflict in include/linux/mtd/physmap.h

commit 93ff66bf1ef29881dffd6fdc344555dab03cdb42
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sun Jun 4 02:51:29 2006 -0700

    [PATCH] Sparsemem build fix
    
    From: Ralf Baechle <ralf@linux-mips.org>
    
    <linux/mmzone.h> uses PAGE_SIZE, PAGE_SHIFT from <asm/page.h> without
    including that header itself.  For some sparsemem configurations this may
    result in build errors like:
    
      CC      init/initramfs.o
    In file included from include/linux/gfp.h:4,
                     from include/linux/slab.h:15,
                     from include/linux/percpu.h:4,
                     from include/linux/rcupdate.h:41,
                     from include/linux/dcache.h:10,
                     from include/linux/fs.h:226,
                     from init/initramfs.c:2:
    include/linux/mmzone.h:498:22: warning: "PAGE_SHIFT" is not defined
    In file included from include/linux/gfp.h:4,
                     from include/linux/slab.h:15,
                     from include/linux/percpu.h:4,
                     from include/linux/rcupdate.h:41,
                     from include/linux/dcache.h:10,
                     from include/linux/fs.h:226,
                     from init/initramfs.c:2:
    include/linux/mmzone.h:526: error: `PAGE_SIZE' undeclared here (not in a function)
    include/linux/mmzone.h: In function `__pfn_to_section':
    include/linux/mmzone.h:573: error: `PAGE_SHIFT' undeclared (first use in this function)
    include/linux/mmzone.h:573: error: (Each undeclared identifier is reported only once
    include/linux/mmzone.h:573: error: for each function it appears in.)
    include/linux/mmzone.h: In function `pfn_valid':
    include/linux/mmzone.h:578: error: `PAGE_SHIFT' undeclared (first use in this function)
    make[1]: *** [init/initramfs.o] Error 1
    make: *** [init] Error 2
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Seems-reasonable-to: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 36740354d4db..2d8337150493 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -15,6 +15,7 @@
 #include <linux/seqlock.h>
 #include <linux/nodemask.h>
 #include <asm/atomic.h>
+#include <asm/page.h>
 
 /* Free memory management - zoned buddy allocator.  */
 #ifndef CONFIG_FORCE_MAX_ZONEORDER

commit 66643de455c27973ac31ad6de9f859d399916842
Merge: 2c23d62abb82 387e2b043902
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed May 24 09:22:21 2006 +0100

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6
    
    Conflicts:
    
            include/asm-powerpc/unistd.h
            include/asm-sparc/unistd.h
            include/asm-sparc64/unistd.h
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

commit e984bb43f7450312ba66fe0e67a99efa6be3b246
Author: Bob Picco <bob.picco@hp.com>
Date:   Sat May 20 15:00:31 2006 -0700

    [PATCH] Align the node_mem_map endpoints to a MAX_ORDER boundary
    
    Andy added code to buddy allocator which does not require the zone's
    endpoints to be aligned to MAX_ORDER.  An issue is that the buddy allocator
    requires the node_mem_map's endpoints to be MAX_ORDER aligned.  Otherwise
    __page_find_buddy could compute a buddy not in node_mem_map for partial
    MAX_ORDER regions at zone's endpoints.  page_is_buddy will detect that
    these pages at endpoints are not PG_buddy (they were zeroed out by bootmem
    allocator and not part of zone).  Of course the negative here is we could
    waste a little memory but the positive is eliminating all the old checks
    for zone boundary conditions.
    
    SPARSEMEM won't encounter this issue because of MAX_ORDER size constraint
    when SPARSEMEM is configured.  ia64 VIRTUAL_MEM_MAP doesn't need the logic
    either because the holes and endpoints are handled differently.  This
    leaves checking alloc_remap and other arches which privately allocate for
    node_mem_map.
    
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b5c21122c299..36740354d4db 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -22,6 +22,7 @@
 #else
 #define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
 #endif
+#define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))
 
 struct free_area {
 	struct list_head	free_list;

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b5c21122c299..6be91fb2deb1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -4,7 +4,6 @@
 #ifdef __KERNEL__
 #ifndef __ASSEMBLY__
 
-#include <linux/config.h>
 #include <linux/spinlock.h>
 #include <linux/list.h>
 #include <linux/wait.h>

commit 95144c788dc01b6a0ff2c9c2222e37ffdab358b8
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:16:02 2006 -0800

    [PATCH] uninline zone helpers
    
    Helper functions for for_each_online_pgdat/for_each_zone look too big to be
    inlined.  Speed of these helper macro itself is not very important.  (inner
    loops are tend to do more work than this)
    
    This patch make helper function to be out-of-lined.
    
            inline          out-of-line
    .text   005c0680        005bf6a0
    
    005c0680 - 005bf6a0 = FE0 = 4Kbytes.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0d12c3cf1f86..b5c21122c299 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -418,20 +418,9 @@ extern struct pglist_data contig_page_data;
 
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
-static inline struct pglist_data *first_online_pgdat(void)
-{
-	return NODE_DATA(first_online_node);
-}
-
-static inline struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
-{
-	int nid = next_online_node(pgdat->node_id);
-
-	if (nid == MAX_NUMNODES)
-		return NULL;
-	return NODE_DATA(nid);
-}
-
+extern struct pglist_data *first_online_pgdat(void);
+extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
+extern struct zone *next_zone(struct zone *zone);
 
 /**
  * for_each_pgdat - helper macro to iterate over all nodes
@@ -441,27 +430,6 @@ static inline struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
 	for (pgdat = first_online_pgdat();		\
 	     pgdat;					\
 	     pgdat = next_online_pgdat(pgdat))
-
-/*
- * next_zone - helper magic for for_each_zone()
- * Thanks to William Lee Irwin III for this piece of ingenuity.
- */
-static inline struct zone *next_zone(struct zone *zone)
-{
-	pg_data_t *pgdat = zone->zone_pgdat;
-
-	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
-		zone++;
-	else {
-		pgdat = next_online_pgdat(pgdat);
-		if (pgdat)
-			zone = pgdat->node_zones;
-		else
-			zone = NULL;
-	}
-	return zone;
-}
-
 /**
  * for_each_zone - helper macro to iterate over all memory zones
  * @zone - pointer to struct zone variable

commit ae0f15fb91274e67d78836d38c99ec363df33073
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:16:01 2006 -0800

    [PATCH] for_each_online_pgdat: remove pgdat_list
    
    By using for_each_online_pgdat(), pgdat_list is not necessary now.  This patch
    removes it.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 96eb08025092..0d12c3cf1f86 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -307,7 +307,6 @@ typedef struct pglist_data {
 	unsigned long node_spanned_pages; /* total size of physical page
 					     range, including holes */
 	int node_id;
-	struct pglist_data *pgdat_next;
 	wait_queue_head_t kswapd_wait;
 	struct task_struct *kswapd;
 	int kswapd_max_order;
@@ -324,8 +323,6 @@ typedef struct pglist_data {
 
 #include <linux/memory_hotplug.h>
 
-extern struct pglist_data *pgdat_list;
-
 void __get_zone_counts(unsigned long *active, unsigned long *inactive,
 			unsigned long *free, struct pglist_data *pgdat);
 void get_zone_counts(unsigned long *active, unsigned long *inactive,

commit 8357f8695d58b50fbf2bd507b4b0fc2cd1e43bd6
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:15:57 2006 -0800

    [PATCH] define for_each_online_pgdat
    
    This patch defines for_each_online_pgdat() as a replacement of
    for_each_pgdat()
    
    Now, online nodes are managed by node_online_map.  But for_each_pgdat()
    uses pgdat_link to iterate over all nodes(pgdat).  This means management
    structure for online pgdat is duplicated.
    
    I think using node_online_map for for_each_pgdat() is simple and sane
    rather ather than pgdat_link.  New macro is named as
    for_each_online_pgdat().  Following patch will fix callers of
    for_each_pgdat().
    
    The bootmem allocater uses for_each_pgdat() before pgdat initialization.  I
    don't think it's sane.  Following patch will fix it.
    
    Signed-off-by: Yasunori Goto     <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ace31c515a8c..96eb08025092 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -13,6 +13,7 @@
 #include <linux/numa.h>
 #include <linux/init.h>
 #include <linux/seqlock.h>
+#include <linux/nodemask.h>
 #include <asm/atomic.h>
 
 /* Free memory management - zoned buddy allocator.  */
@@ -349,57 +350,6 @@ unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
  */
 #define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
 
-/**
- * for_each_pgdat - helper macro to iterate over all nodes
- * @pgdat - pointer to a pg_data_t variable
- *
- * Meant to help with common loops of the form
- * pgdat = pgdat_list;
- * while(pgdat) {
- * 	...
- * 	pgdat = pgdat->pgdat_next;
- * }
- */
-#define for_each_pgdat(pgdat) \
-	for (pgdat = pgdat_list; pgdat; pgdat = pgdat->pgdat_next)
-
-/*
- * next_zone - helper magic for for_each_zone()
- * Thanks to William Lee Irwin III for this piece of ingenuity.
- */
-static inline struct zone *next_zone(struct zone *zone)
-{
-	pg_data_t *pgdat = zone->zone_pgdat;
-
-	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
-		zone++;
-	else if (pgdat->pgdat_next) {
-		pgdat = pgdat->pgdat_next;
-		zone = pgdat->node_zones;
-	} else
-		zone = NULL;
-
-	return zone;
-}
-
-/**
- * for_each_zone - helper macro to iterate over all memory zones
- * @zone - pointer to struct zone variable
- *
- * The user only needs to declare the zone variable, for_each_zone
- * fills it in. This basically means for_each_zone() is an
- * easier to read version of this piece of code:
- *
- * for (pgdat = pgdat_list; pgdat; pgdat = pgdat->node_next)
- * 	for (i = 0; i < MAX_NR_ZONES; ++i) {
- * 		struct zone * z = pgdat->node_zones + i;
- * 		...
- * 	}
- * }
- */
-#define for_each_zone(zone) \
-	for (zone = pgdat_list->node_zones; zone; zone = next_zone(zone))
-
 static inline int populated_zone(struct zone *zone)
 {
 	return (!!zone->present_pages);
@@ -471,6 +421,62 @@ extern struct pglist_data contig_page_data;
 
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
+static inline struct pglist_data *first_online_pgdat(void)
+{
+	return NODE_DATA(first_online_node);
+}
+
+static inline struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
+{
+	int nid = next_online_node(pgdat->node_id);
+
+	if (nid == MAX_NUMNODES)
+		return NULL;
+	return NODE_DATA(nid);
+}
+
+
+/**
+ * for_each_pgdat - helper macro to iterate over all nodes
+ * @pgdat - pointer to a pg_data_t variable
+ */
+#define for_each_online_pgdat(pgdat)			\
+	for (pgdat = first_online_pgdat();		\
+	     pgdat;					\
+	     pgdat = next_online_pgdat(pgdat))
+
+/*
+ * next_zone - helper magic for for_each_zone()
+ * Thanks to William Lee Irwin III for this piece of ingenuity.
+ */
+static inline struct zone *next_zone(struct zone *zone)
+{
+	pg_data_t *pgdat = zone->zone_pgdat;
+
+	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
+		zone++;
+	else {
+		pgdat = next_online_pgdat(pgdat);
+		if (pgdat)
+			zone = pgdat->node_zones;
+		else
+			zone = NULL;
+	}
+	return zone;
+}
+
+/**
+ * for_each_zone - helper macro to iterate over all memory zones
+ * @zone - pointer to struct zone variable
+ *
+ * The user only needs to declare the zone variable, for_each_zone
+ * fills it in.
+ */
+#define for_each_zone(zone)			        \
+	for (zone = (first_online_pgdat())->node_zones; \
+	     zone;					\
+	     zone = next_zone(zone))
+
 #ifdef CONFIG_SPARSEMEM
 #include <asm/sparsemem.h>
 #endif

commit a0140c1d85637ee5f4ea7c78f066e3611a6a79dc
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:15:55 2006 -0800

    [PATCH] remove zone_mem_map
    
    This patch removes zone_mem_map.
    
    pfn_to_page uses pgdat, page_to_pfn uses zone.  page_to_pfn can use pgdat
    instead of zone, which is only one user of zone_mem_map.  By modifing it,
    we can remove zone_mem_map.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0c1c0c0cce65..ace31c515a8c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -225,7 +225,6 @@ struct zone {
 	 * Discontig memory support fields.
 	 */
 	struct pglist_data	*zone_pgdat;
-	struct page		*zone_mem_map;
 	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 	unsigned long		zone_start_pfn;
 

commit a117e66ed45ac0569c039ea60bd7a9a61e031858
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:15:25 2006 -0800

    [PATCH] unify pfn_to_page: generic functions
    
    There are 3 memory models, FLATMEM, DISCONTIGMEM, SPARSEMEM.
    Each arch has its own page_to_pfn(), pfn_to_page() for each models.
    But most of them can use the same arithmetic.
    
    This patch adds asm-generic/memory_model.h, which includes generic
    page_to_pfn(), pfn_to_page() definitions for each memory model.
    
    When CONFIG_OUT_OF_LINE_PFN_TO_PAGE=y, out-of-line functions are
    used instead of macro. This is enabled by some archs and  reduces
    text size.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Hirokazu Takata <takata.hirokazu@renesas.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ebfc238cc243..0c1c0c0cce65 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -602,17 +602,6 @@ static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 	return __nr_to_section(pfn_to_section_nr(pfn));
 }
 
-#define pfn_to_page(pfn) 						\
-({ 									\
-	unsigned long __pfn = (pfn);					\
-	__section_mem_map_addr(__pfn_to_section(__pfn)) + __pfn;	\
-})
-#define page_to_pfn(page)						\
-({									\
-	page - __section_mem_map_addr(__nr_to_section(			\
-		page_to_section(page)));				\
-})
-
 static inline int pfn_valid(unsigned long pfn)
 {
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)

commit ce2ea89ba101d976907128441ba3aca72a8804b9
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Wed Feb 1 03:05:27 2006 -0800

    [PATCH] GFP_ZONETYPES: calculate from GFP_ZONEMASK
    
    GFP_ZONETYPES calculate from GFP_ZONEMASK
    
    GFP_ZONETYPES's value is directly related to the value of GFP_ZONEMASK.  It
    takes one of two forms depending whether the top bit of GFP_ZONEMASK is a
    'loner'.  Supply both forms, enabling the loner.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 88c30f844abf..ebfc238cc243 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -98,11 +98,14 @@ struct per_cpu_pageset {
  * of three zone modifier bits, we could require up to eight zonelists.
  * If the left most zone modifier is a "loner" then the highest valid
  * zonelist would be four allowing us to allocate only five zonelists.
+ * Use the first form for GFP_ZONETYPES when the left most bit is not
+ * a "loner", otherwise use the second.
  *
  * NOTE! Make sure this matches the zones in <linux/gfp.h>
  */
 #define GFP_ZONEMASK	0x07
-#define GFP_ZONETYPES	5
+/* #define GFP_ZONETYPES       (GFP_ZONEMASK + 1) */           /* Non-loner */
+#define GFP_ZONETYPES  ((GFP_ZONEMASK + 1) / 2 + 1)            /* Loner */
 
 /*
  * On machines where it is needed (eg PCs) we divide physical memory

commit 79046ae07ae21245520ca0aab985ee6678a879f8
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Wed Feb 1 03:05:26 2006 -0800

    [PATCH] GFP_ZONETYPES: add commentry on how to calculate
    
    GFP_ZONETYPES define using GFP_ZONEMASK and add commentry
    
    Add commentry explaining the optimisation that we can apply to GFP_ZONETYPES
    when the leftmost bit is a 'loaner', it can only be set in isolation.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 93a849f742db..88c30f844abf 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -91,6 +91,14 @@ struct per_cpu_pageset {
  * be 8 (2 ** 3) zonelists.  GFP_ZONETYPES defines the number of possible
  * combinations of zone modifiers in "zone modifier space".
  *
+ * As an optimisation any zone modifier bits which are only valid when
+ * no other zone modifier bits are set (loners) should be placed in
+ * the highest order bits of this field.  This allows us to reduce the
+ * extent of the zonelists thus saving space.  For example in the case
+ * of three zone modifier bits, we could require up to eight zonelists.
+ * If the left most zone modifier is a "loner" then the highest valid
+ * zonelist would be four allowing us to allocate only five zonelists.
+ *
  * NOTE! Make sure this matches the zones in <linux/gfp.h>
  */
 #define GFP_ZONEMASK	0x07

commit 9eeff2395e3cfd05c9b2e6074ff943a34b0c5c21
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Jan 18 17:42:31 2006 -0800

    [PATCH] Zone reclaim: Reclaim logic
    
    Some bits for zone reclaim exists in 2.6.15 but they are not usable.  This
    patch fixes them up, removes unused code and makes zone reclaim usable.
    
    Zone reclaim allows the reclaiming of pages from a zone if the number of
    free pages falls below the watermarks even if other zones still have enough
    pages available.  Zone reclaim is of particular importance for NUMA
    machines.  It can be more beneficial to reclaim a page than taking the
    performance penalties that come with allocating a page on a remote zone.
    
    Zone reclaim is enabled if the maximum distance to another node is higher
    than RECLAIM_DISTANCE, which may be defined by an arch.  By default
    RECLAIM_DISTANCE is 20.  20 is the distance to another node in the same
    component (enclosure or motherboard) on IA64.  The meaning of the NUMA
    distance information seems to vary by arch.
    
    If zone reclaim is not successful then no further reclaim attempts will
    occur for a certain time period (ZONE_RECLAIM_INTERVAL).
    
    This patch was discussed before. See
    
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113519961504207&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113408418232531&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113389027420032&w=2
    http://marc.theaimsgroup.com/?l=linux-kernel&m=113380938612205&w=2
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 34cbefd2ebde..93a849f742db 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -149,14 +149,16 @@ struct zone {
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	int			all_unreclaimable; /* All pages pinned */
 
-	/*
-	 * Does the allocator try to reclaim pages from the zone as soon
-	 * as it fails a watermark_ok() in __alloc_pages?
-	 */
-	int			reclaim_pages;
 	/* A count of how many reclaimers are scanning this zone */
 	atomic_t		reclaim_in_progress;
 
+	/*
+	 * timestamp (in jiffies) of the last zone reclaim that did not
+	 * result in freeing of pages. This is used to avoid repeated scans
+	 * if all memory in the zone is in use.
+	 */
+	unsigned long		last_unsuccessful_zone_reclaim;
+
 	/*
 	 * prev_priority holds the scanning priority for this zone.  It is
 	 * defined as the scanning priority at which we achieved our reclaim

commit 1f6818b90dbb887261c616a318733703ed526f0a
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 11 22:42:26 2006 +0100

    [PATCH] x86_64: Minor GFP_DMA32 comment fix
    
    Pretty obvious
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7e4ae6ab1977..34cbefd2ebde 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -98,7 +98,7 @@ struct per_cpu_pageset {
 
 /*
  * On machines where it is needed (eg PCs) we divide physical memory
- * into multiple physical zones. On a PC we have 4 zones:
+ * into multiple physical zones. On a 32bit PC we have 4 zones:
  *
  * ZONE_DMA	  < 16 MB	ISA DMA capable memory
  * ZONE_DMA32	     0 MB 	Empty

commit 22fc6eccbf4ce4eb6265e6ada7b50a7b9cc57d05
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Sun Jan 8 01:01:27 2006 -0800

    [PATCH] Change maxaligned_in_smp alignemnt macros to internodealigned_in_smp macros
    
    ____cacheline_maxaligned_in_smp is currently used to align critical structures
    and avoid false sharing.  It uses per-arch L1_CACHE_SHIFT_MAX and people find
    L1_CACHE_SHIFT_MAX useless.
    
    However, we have been using ____cacheline_maxaligned_in_smp to align
    structures on the internode cacheline size.  As per Andi's suggestion,
    following patch kills ____cacheline_maxaligned_in_smp and introduces
    INTERNODE_CACHE_SHIFT, which defaults to L1_CACHE_SHIFT for all arches.
    Arches needing L3/Internode cacheline alignment can define
    INTERNODE_CACHE_SHIFT in the arch asm/cache.h.  Patch replaces
    ____cacheline_maxaligned_in_smp with ____cacheline_internodealigned_in_smp
    
    With this patch, L1_CACHE_SHIFT_MAX can be killed
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Shai Fultheim <shai@scalex86.org>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2a89c132ba9c..7e4ae6ab1977 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -38,7 +38,7 @@ struct pglist_data;
 #if defined(CONFIG_SMP)
 struct zone_padding {
 	char x[0];
-} ____cacheline_maxaligned_in_smp;
+} ____cacheline_internodealigned_in_smp;
 #define ZONE_PADDING(name)	struct zone_padding name;
 #else
 #define ZONE_PADDING(name)
@@ -233,7 +233,7 @@ struct zone {
 	 * rarely used fields:
 	 */
 	char			*name;
-} ____cacheline_maxaligned_in_smp;
+} ____cacheline_internodealigned_in_smp;
 
 
 /*

commit 8ad4b1fb8205340dba16b63467bb23efc27264d6
Author: Rohit Seth <rohit.seth@intel.com>
Date:   Sun Jan 8 01:00:40 2006 -0800

    [PATCH] Make high and batch sizes of per_cpu_pagelists configurable
    
    As recently there has been lot of traffic on the right values for batch and
    high water marks for per_cpu_pagelists.  This patch makes these two
    variables configurable through /proc interface.
    
    A new tunable /proc/sys/vm/percpu_pagelist_fraction is added.  This entry
    controls the fraction of pages at most in each zone that are allocated for
    each per cpu page list.  The min value for this is 8.  It means that we
    don't allow more than 1/8th of pages in each zone to be allocated in any
    single per_cpu_pagelist.
    
    The batch value of each per cpu pagelist is also updated as a result.  It
    is set to pcp->high/4.  The upper limit of batch is (PAGE_SHIFT * 8)
    
    Signed-off-by: Rohit Seth <rohit.seth@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c34f4a2c62f8..2a89c132ba9c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -437,6 +437,8 @@ int min_free_kbytes_sysctl_handler(struct ctl_table *, int, struct file *,
 extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1];
 int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
 					void __user *, size_t *, loff_t *);
+int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int, struct file *,
+					void __user *, size_t *, loff_t *);
 
 #include <linux/topology.h>
 /* Returns the number of the current Node. */

commit f3fe65122da05e1cd4c9140340d96ea2f95d0c49
Author: Con Kolivas <kernel@kolivas.org>
Date:   Fri Jan 6 00:11:15 2006 -0800

    [PATCH] mm: add populated_zone() helper
    
    There are numerous places we check whether a zone is populated or not.
    
    Provide a helper function to check for populated zones and convert all
    checks for zone->present_pages.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8d6caa414c4c..c34f4a2c62f8 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -388,6 +388,11 @@ static inline struct zone *next_zone(struct zone *zone)
 #define for_each_zone(zone) \
 	for (zone = pgdat_list->node_zones; zone; zone = next_zone(zone))
 
+static inline int populated_zone(struct zone *zone)
+{
+	return (!!zone->present_pages);
+}
+
 static inline int is_highmem_idx(int idx)
 {
 	return (idx == ZONE_HIGHMEM);

commit 9328b8faae922e52073785ed6c1eaa8565648a0e
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Fri Jan 6 00:11:10 2006 -0800

    [PATCH] mm: dma32 zone statistics
    
    Add dma32 to zone statistics.  Also attempt to arrange struct page_state a
    bit better (visually).
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 0d1a5981bb94..8d6caa414c4c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -397,6 +397,7 @@ static inline int is_normal_idx(int idx)
 {
 	return (idx == ZONE_NORMAL);
 }
+
 /**
  * is_highmem - helper function to quickly check if a struct zone is a 
  *              highmem zone or not.  This is an attempt to keep references
@@ -413,6 +414,16 @@ static inline int is_normal(struct zone *zone)
 	return zone == zone->zone_pgdat->node_zones + ZONE_NORMAL;
 }
 
+static inline int is_dma32(struct zone *zone)
+{
+	return zone == zone->zone_pgdat->node_zones + ZONE_DMA32;
+}
+
+static inline int is_dma(struct zone *zone)
+{
+	return zone == zone->zone_pgdat->node_zones + ZONE_DMA;
+}
+
 /* These two functions are used to setup the per zone pages min values */
 struct ctl_table;
 struct file;

commit 2d92c5c9150a2a9ca3dc25da58d5042e17a96b6a
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Fri Jan 6 00:10:59 2006 -0800

    [PATCH] mm: remove pcp low
    
    struct per_cpu_pages.low is useless.  Remove it.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 8cba76c6a28c..0d1a5981bb94 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -46,7 +46,6 @@ struct zone_padding {
 
 struct per_cpu_pages {
 	int count;		/* number of pages in the list */
-	int low;		/* low watermark, refill needed */
 	int high;		/* high watermark, emptying needed */
 	int batch;		/* chunk size for buddy add/remove */
 	struct list_head list;	/* the list of pages */

commit 161599ff39a3c3cdea0a1be05ac53accd2c45cdd
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jan 6 00:10:54 2006 -0800

    [PATCH] sparsemem: provide pfn_to_nid
    
    Before SPARSEMEM is initialised we cannot provide an efficient pfn_to_nid()
    implmentation; before initialisation is complete we use early_pfn_to_nid()
    to provide location information.  Until recently there was no non-init user
    of this functionality.  Provide a post init pfn_to_nid() implementation.
    
    Note that this implmentation assumes that the pfn passed has been validated
    with pfn_valid().  The current single user of this function already has
    this check.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ee9f7b74e613..8cba76c6a28c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -596,7 +596,11 @@ static inline int pfn_valid(unsigned long pfn)
  * this restriction.
  */
 #ifdef CONFIG_NUMA
-#define pfn_to_nid		early_pfn_to_nid
+#define pfn_to_nid(pfn)							\
+({									\
+	unsigned long __pfn_to_nid_pfn = (pfn);				\
+	page_to_nid(pfn_to_page(__pfn_to_nid_pfn));			\
+})
 #else
 #define pfn_to_nid(pfn)		(0)
 #endif

commit 2bdaf115b1c364d89484b59d5b937973f1c5a5c3
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jan 6 00:10:53 2006 -0800

    [PATCH] flatmem split out memory model
    
    There are three places we define pfn_to_nid().  Two in linux/mmzone.h and one
    in asm/mmzone.h.  These in essence represent the three memory models.  The
    definition in linux/mmzone.h under !NEED_MULTIPLE_NODES is both the FLATMEM
    definition and the optimisation for single NUMA nodes; the one under SPARSEMEM
    is the NUMA sparsemem one; the one in asm/mmzone.h under DISCONTIGMEM is the
    discontigmem one.  This is not in the least bit obvious, particularly the
    connection between the non-NUMA optimisations and the memory models.
    
    Two patches:
    
    flatmem-split-out-memory-model: simplifies the selection of pfn_to_nid()
    implementations.  The selection is based primarily off the memory model
    selected.  Optimisations for non-NUMA are applied where needed.
    
    sparse-provide-pfn_to_nid: implement pfn_to_nid() for SPARSEMEM
    
    This patch:
    
    pfn_to_nid is memory model specific
    
    The pfn_to_nid() call is memory model specific.  It represents the locality
    identifier for the memory passed.  Classically this would be a NUMA node,
    but not a chunk of memory under DISCONTIGMEM.
    
    The SPARSEMEM and FLATMEM memory model non-NUMA versions of pfn_to_nid()
    are folded together under NEED_MULTIPLE_NODES, while DISCONTIGMEM has its
    own optimisation.  This is all very confusing.
    
    This patch splits out each implementation of pfn_to_nid() so that we can
    see them and the optimisations to each.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index d294b57a4016..ee9f7b74e613 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -435,7 +435,6 @@ extern struct pglist_data contig_page_data;
 #define NODE_DATA(nid)		(&contig_page_data)
 #define NODE_MEM_MAP(nid)	mem_map
 #define MAX_NODES_SHIFT		1
-#define pfn_to_nid(pfn)		(0)
 
 #else /* CONFIG_NEED_MULTIPLE_NODES */
 
@@ -470,6 +469,10 @@ extern struct pglist_data contig_page_data;
 #define early_pfn_to_nid(nid)  (0UL)
 #endif
 
+#ifdef CONFIG_FLATMEM
+#define pfn_to_nid(pfn)		(0)
+#endif
+
 #define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
 #define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
 
@@ -594,6 +597,8 @@ static inline int pfn_valid(unsigned long pfn)
  */
 #ifdef CONFIG_NUMA
 #define pfn_to_nid		early_pfn_to_nid
+#else
+#define pfn_to_nid(pfn)		(0)
 #endif
 
 #define early_pfn_valid(pfn)	pfn_valid(pfn)

commit a94b3ab7eab4edcc9b2cb474b188f774c331adf7
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Fri Jan 6 00:10:51 2006 -0800

    [PATCH] mm: remove arch independent NODES_SPAN_OTHER_NODES
    
    The NODES_SPAN_OTHER_NODES config option was created so that DISCONTIGMEM
    could handle pSeries numa layouts.  However, support for DISCONTIGMEM has
    been replaced by SPARSEMEM on powerpc.  As a result, this config option and
    supporting code is no longer needed.
    
    I have already sent a patch to Paul that removes the option from powerpc
    specific code.  This removes the arch independent piece.  Doesn't really
    matter which is applied first.
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 28f8496abcb9..d294b57a4016 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -603,12 +603,6 @@ void sparse_init(void);
 #define sparse_index_init(_sec, _nid)  do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
-#ifdef CONFIG_NODES_SPAN_OTHER_NODES
-#define early_pfn_in_nid(pfn, nid)	(early_pfn_to_nid(pfn) == (nid))
-#else
-#define early_pfn_in_nid(pfn, nid)	(1)
-#endif
-
 #ifndef early_pfn_valid
 #define early_pfn_valid(pfn)	(1)
 #endif

commit d5afa6dcf74c0efb60ce07c63d0a727be93c67c5
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jan 6 00:10:50 2006 -0800

    [PATCH] mm: pfn_to_pgdat not used in common code
    
    pfn_to_pgdat() isn't used in common code.  Remove definition.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3c49f786f90c..28f8496abcb9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -596,11 +596,6 @@ static inline int pfn_valid(unsigned long pfn)
 #define pfn_to_nid		early_pfn_to_nid
 #endif
 
-#define pfn_to_pgdat(pfn)						\
-({									\
-	NODE_DATA(pfn_to_nid(pfn));					\
-})
-
 #define early_pfn_valid(pfn)	pfn_valid(pfn)
 void sparse_init(void);
 #else

commit 9f3fd602aef96c2a490e3bfd669d06475aeba8d8
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jan 6 00:10:50 2006 -0800

    [PATCH] mm: kvaddr_to_nid not used in common code
    
    kvaddr_to_nid() isn't used in common code nor in i386 code.  Remove these
    definitions.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9f22090df7dd..3c49f786f90c 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -564,11 +564,6 @@ static inline int valid_section_nr(unsigned long nr)
 	return valid_section(__nr_to_section(nr));
 }
 
-/*
- * Given a kernel address, find the home node of the underlying memory.
- */
-#define kvaddr_to_nid(kaddr)	pfn_to_nid(__pa(kaddr) >> PAGE_SHIFT)
-
 static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 {
 	return __nr_to_section(pfn_to_section_nr(pfn));

commit ac3461ad632e86e7debd871776683c05ef3ba4c6
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Nov 22 19:39:30 2005 -0800

    Fix up GFP_ZONEMASK for GFP_DMA32 usage
    
    There was some confusion about the different zone usage, this should fix
    up the resulting mess in the GFP zonemask handling.
    
    The different zone usage is still confusing (it's very easy to mix up
    the individual zone numbers with the GFP zone _list_ numbers), so we
    might want to clean up some of this in the future, but in the meantime
    this should fix the actual problems.
    
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2c8edad5dccf..9f22090df7dd 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -91,21 +91,11 @@ struct per_cpu_pageset {
  * will be a maximum of 4 (2 ** 2) zonelists, for 3 modifiers there will
  * be 8 (2 ** 3) zonelists.  GFP_ZONETYPES defines the number of possible
  * combinations of zone modifiers in "zone modifier space".
+ *
+ * NOTE! Make sure this matches the zones in <linux/gfp.h>
  */
-#define GFP_ZONEMASK	0x03
-/*
- * As an optimisation any zone modifier bits which are only valid when
- * no other zone modifier bits are set (loners) should be placed in
- * the highest order bits of this field.  This allows us to reduce the
- * extent of the zonelists thus saving space.  For example in the case
- * of three zone modifier bits, we could require up to eight zonelists.
- * If the left most zone modifier is a "loner" then the highest valid
- * zonelist would be four allowing us to allocate only five zonelists.
- * Use the first form when the left most bit is not a "loner", otherwise
- * use the second.
- */
-/* #define GFP_ZONETYPES	(GFP_ZONEMASK + 1) */		/* Non-loner */
-#define GFP_ZONETYPES	((GFP_ZONEMASK + 1) / 2 + 1)		/* Loner */
+#define GFP_ZONEMASK	0x07
+#define GFP_ZONETYPES	5
 
 /*
  * On machines where it is needed (eg PCs) we divide physical memory

commit 4060994c3e337b40e0f6fa8ce2cc178e021baf3d
Merge: 0174f72f848d d3ee871e63d0
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Mon Nov 14 19:56:02 2005 -0800

    Merge x86-64 update from Andi

commit 69d81fcde7797342417591ba7affb372b9c86eae
Author: Andi Kleen <ak@suse.de>
Date:   Sat Nov 5 17:25:53 2005 +0100

    [PATCH] x86_64: Speed up numa_node_id by putting it directly into the PDA
    
    Not go from the CPU number to an mapping array.
    Mode number is often used now in fast paths.
    
    This also adds a generic numa_node_id to all the topology includes
    
    Suggested by Eric Dumazet
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 57fc99c67c31..f3cffc354dea 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -435,7 +435,9 @@ int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
 
 #include <linux/topology.h>
 /* Returns the number of the current Node. */
+#ifndef numa_node_id
 #define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
+#endif
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 

commit 07808b74e7dab1aa385e698795875337d72daf7d
Author: Andi Kleen <ak@suse.de>
Date:   Sat Nov 5 17:25:53 2005 +0100

    [PATCH] x86_64: Remove obsolete ARCH_HAS_ATOMIC_UNSIGNED and page_flags_t
    
    Has been introduced for x86-64 at some point to save memory
    in struct page, but has been obsolete for some time. Just
    remove it.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index da7a829f8561..57fc99c67c31 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -455,7 +455,7 @@ extern struct pglist_data contig_page_data;
 #include <asm/sparsemem.h>
 #endif
 
-#if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
+#if BITS_PER_LONG == 32
 /*
  * with 32 bit page->flags field, we reserve 9 bits for node/zone info.
  * there are 4 zones (3 bits) and this leaves 9-3=6 bits for nodes.

commit a2f1b424900715ed9d1699c3bb88a434a2b42bc0
Author: Andi Kleen <ak@suse.de>
Date:   Sat Nov 5 17:25:53 2005 +0100

    [PATCH] x86_64: Add 4GB DMA32 zone
    
    Add a new 4GB GFP_DMA32 zone between the GFP_DMA and GFP_NORMAL zones.
    
    As a bit of historical background: when the x86-64 port
    was originally designed we had some discussion if we should
    use a 16MB DMA zone like i386 or a 4GB DMA zone like IA64 or
    both. Both was ruled out at this point because it was in early
    2.4 when VM is still quite shakey and had bad troubles even
    dealing with one DMA zone.  We settled on the 16MB DMA zone mainly
    because we worried about older soundcards and the floppy.
    
    But this has always caused problems since then because
    device drivers had trouble getting enough DMA able memory. These days
    the VM works much better and the wide use of NUMA has proven
    it can deal with many zones successfully.
    
    So this patch adds both zones.
    
    This helps drivers who need a lot of memory below 4GB because
    their hardware is not accessing more (graphic drivers - proprietary
    and free ones, video frame buffer drivers, sound drivers etc.).
    Previously they could only use IOMMU+16MB GFP_DMA, which
    was not enough memory.
    
    Another common problem is that hardware who has full memory
    addressing for >4GB misses it for some control structures in memory
    (like transmit rings or other metadata).  They tended to allocate memory
    in the 16MB GFP_DMA or the IOMMU/swiotlb then using pci_alloc_consistent,
    but that can tie up a lot of precious 16MB GFPDMA/IOMMU/swiotlb memory
    (even on AMD systems the IOMMU tends to be quite small) especially if you have
    many devices.  With the new zone pci_alloc_consistent can just put
    this stuff into memory below 4GB which works better.
    
    One argument was still if the zone should be 4GB or 2GB. The main
    motivation for 2GB would be an unnamed not so unpopular hardware
    raid controller (mostly found in older machines from a particular four letter
    company) who has a strange 2GB restriction in firmware. But
    that one works ok with swiotlb/IOMMU anyways, so it doesn't really
    need GFP_DMA32. I chose 4GB to be compatible with IA64 and because
    it seems to be the most common restriction.
    
    The new zone is so far added only for x86-64.
    
    For other architectures who don't set up this
    new zone nothing changes. Architectures can set a compatibility
    define in Kconfig CONFIG_DMA_IS_DMA32 that will define GFP_DMA32
    as GFP_DMA. Otherwise it's a nop because on 32bit architectures
    it's normally not needed because GFP_NORMAL (=0) is DMA able
    enough.
    
    One problem is still that GFP_DMA means different things on different
    architectures. e.g. some drivers used to have #ifdef ia64  use GFP_DMA
    (trusting it to be 4GB) #elif __x86_64__ (use other hacks like
    the swiotlb because 16MB is not enough) ... . This was quite
    ugly and is now obsolete.
    
    These should be now converted to use GFP_DMA32 unconditionally. I haven't done
    this yet. Or best only use pci_alloc_consistent/dma_alloc_coherent
    which will use GFP_DMA32 transparently.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f5fa3082fd6a..da7a829f8561 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -71,10 +71,11 @@ struct per_cpu_pageset {
 #endif
 
 #define ZONE_DMA		0
-#define ZONE_NORMAL		1
-#define ZONE_HIGHMEM		2
+#define ZONE_DMA32		1
+#define ZONE_NORMAL		2
+#define ZONE_HIGHMEM		3
 
-#define MAX_NR_ZONES		3	/* Sync this with ZONES_SHIFT */
+#define MAX_NR_ZONES		4	/* Sync this with ZONES_SHIFT */
 #define ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
 
 
@@ -108,9 +109,10 @@ struct per_cpu_pageset {
 
 /*
  * On machines where it is needed (eg PCs) we divide physical memory
- * into multiple physical zones. On a PC we have 3 zones:
+ * into multiple physical zones. On a PC we have 4 zones:
  *
  * ZONE_DMA	  < 16 MB	ISA DMA capable memory
+ * ZONE_DMA32	     0 MB 	Empty
  * ZONE_NORMAL	16-896 MB	direct mapped by the kernel
  * ZONE_HIGHMEM	 > 896 MB	only page cache and user processes
  */
@@ -455,10 +457,10 @@ extern struct pglist_data contig_page_data;
 
 #if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
 /*
- * with 32 bit page->flags field, we reserve 8 bits for node/zone info.
- * there are 3 zones (2 bits) and this leaves 8-2=6 bits for nodes.
+ * with 32 bit page->flags field, we reserve 9 bits for node/zone info.
+ * there are 4 zones (3 bits) and this leaves 9-3=6 bits for nodes.
  */
-#define FLAGS_RESERVED		8
+#define FLAGS_RESERVED		9
 
 #elif BITS_PER_LONG == 64
 /*

commit 7fb1d9fca5c6e3b06773b69165a73f3fb786b8ee
Author: Rohit Seth <rohit.seth@intel.com>
Date:   Sun Nov 13 16:06:43 2005 -0800

    [PATCH] mm: __alloc_pages cleanup
    
    Clean up of __alloc_pages.
    
    Restoration of previous behaviour, plus further cleanups by introducing an
    'alloc_flags', removing the last of should_reclaim_zone.
    
    Signed-off-by: Rohit Seth <rohit.seth@intel.com>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index f5fa3082fd6a..6cfb114a0c34 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -329,7 +329,7 @@ void get_zone_counts(unsigned long *active, unsigned long *inactive,
 void build_all_zonelists(void);
 void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
-		int alloc_type, int can_try_harder, gfp_t gfp_high);
+		int classzone_idx, int alloc_flags);
 
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);

commit bdc8cb984576ab5b550c8b24c6fa111a873503e3
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:53 2005 -0700

    [PATCH] memory hotplug locking: zone span seqlock
    
    See the "fixup bad_range()" patch for more information, but this actually
    creates a the lock to protect things making assumptions about a zone's size
    staying constant at runtime.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e050d68963a1..f5fa3082fd6a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -12,6 +12,7 @@
 #include <linux/threads.h>
 #include <linux/numa.h>
 #include <linux/init.h>
+#include <linux/seqlock.h>
 #include <asm/atomic.h>
 
 /* Free memory management - zoned buddy allocator.  */
@@ -137,6 +138,10 @@ struct zone {
 	 * free areas of different sizes
 	 */
 	spinlock_t		lock;
+#ifdef CONFIG_MEMORY_HOTPLUG
+	/* see spanned/present_pages for more description */
+	seqlock_t		span_seqlock;
+#endif
 	struct free_area	free_area[MAX_ORDER];
 
 
@@ -220,6 +225,16 @@ struct zone {
 	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 	unsigned long		zone_start_pfn;
 
+	/*
+	 * zone_start_pfn, spanned_pages and present_pages are all
+	 * protected by span_seqlock.  It is a seqlock because it has
+	 * to be read outside of zone->lock, and it is done in the main
+	 * allocator path.  But, it is written quite infrequently.
+	 *
+	 * The lock is declared along with zone->lock because it is
+	 * frequently read in proximity to zone->lock.  It's good to
+	 * give them a chance of being in the same cacheline.
+	 */
 	unsigned long		spanned_pages;	/* total size, including holes */
 	unsigned long		present_pages;	/* amount of memory (excluding holes) */
 

commit 208d54e5513c0c02d85af0990901354c74364d5c
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:52 2005 -0700

    [PATCH] memory hotplug locking: node_size_lock
    
    pgdat->node_size_lock is basically only neeeded in one place in the normal
    code: show_mem(), which is the arch-specific sysrq-m printing function.
    
    Strictly speaking, the architectures not doing memory hotplug do no need this
    locking in show_mem().  However, they are all included for completeness.  This
    should also make any future consolidation of all of the implementations a
    little more straightforward.
    
    This lock is also held in the sparsemem code during a memory removal, as
    sections are invalidated.  This is the place there pfn_valid() is made false
    for a memory area that's being removed.  The lock is only required when doing
    pfn_valid() operations on memory which the user does not already have a
    reference on the page, such as in show_mem().
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4674145bb63d..e050d68963a1 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -273,6 +273,16 @@ typedef struct pglist_data {
 	struct page *node_mem_map;
 #endif
 	struct bootmem_data *bdata;
+#ifdef CONFIG_MEMORY_HOTPLUG
+	/*
+	 * Must be held any time you expect node_start_pfn, node_present_pages
+	 * or node_spanned_pages stay constant.  Holding this will also
+	 * guarantee that any pfn_valid() stays that way.
+	 *
+	 * Nests above zone->lock and zone->size_seqlock.
+	 */
+	spinlock_t node_size_lock;
+#endif
 	unsigned long node_start_pfn;
 	unsigned long node_present_pages; /* total number of physical pages */
 	unsigned long node_spanned_pages; /* total size of physical page
@@ -293,6 +303,8 @@ typedef struct pglist_data {
 #endif
 #define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))
 
+#include <linux/memory_hotplug.h>
+
 extern struct pglist_data *pgdat_list;
 
 void __get_zone_counts(unsigned long *active, unsigned long *inactive,

commit 4ca644d970bf2542623228a4624af356d20ca267
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:51 2005 -0700

    [PATCH] memory hotplug prep: __section_nr helper
    
    A little helper that we use in the hotplug code.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 7519eb4191e7..4674145bb63d 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -509,6 +509,7 @@ static inline struct mem_section *__nr_to_section(unsigned long nr)
 		return NULL;
 	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
 }
+extern int __section_nr(struct mem_section* ms);
 
 /*
  * We use the lower bits of the mem_map pointer to store

commit 260b23674fdb570f3235ce55892246bef1c24c2a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 21 03:22:44 2005 -0400

    [PATCH] gfp_t: the rest
    
    zone handling, mapping->flags handling
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5ed471b58f4f..7519eb4191e7 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -302,7 +302,7 @@ void get_zone_counts(unsigned long *active, unsigned long *inactive,
 void build_all_zonelists(void);
 void wakeup_kswapd(struct zone *zone, int order);
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
-		int alloc_type, int can_try_harder, int gfp_high);
+		int alloc_type, int can_try_harder, gfp_t gfp_high);
 
 #ifdef CONFIG_HAVE_MEMORY_PRESENT
 void memory_present(int nid, unsigned long start, unsigned long end);

commit 28ae55c98e4d16eac9a05a8a259d7763ef3aeb18
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Sep 3 15:54:29 2005 -0700

    [PATCH] sparsemem extreme: hotplug preparation
    
    This splits up sparse_index_alloc() into two pieces.  This is needed
    because we'll allocate the memory for the second level in a different place
    from where we actually consume it to keep the allocation from happening
    underneath a lock
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 79cf578e21b9..5ed471b58f4f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -588,6 +588,7 @@ static inline int pfn_valid(unsigned long pfn)
 void sparse_init(void);
 #else
 #define sparse_init()	do {} while (0)
+#define sparse_index_init(_sec, _nid)  do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
 #ifdef CONFIG_NODES_SPAN_OTHER_NODES

commit 3e347261a80b57df792ab9464b5f0ed59add53a8
Author: Bob Picco <bob.picco@hp.com>
Date:   Sat Sep 3 15:54:28 2005 -0700

    [PATCH] sparsemem extreme implementation
    
    With cleanups from Dave Hansen <haveblue@us.ibm.com>
    
    SPARSEMEM_EXTREME makes mem_section a one dimensional array of pointers to
    mem_sections.  This two level layout scheme is able to achieve smaller
    memory requirements for SPARSEMEM with the tradeoff of an additional shift
    and load when fetching the memory section.  The current SPARSEMEM
    implementation is a one dimensional array of mem_sections which is the
    default SPARSEMEM configuration.  The patch attempts isolates the
    implementation details of the physical layout of the sparsemem section
    array.
    
    SPARSEMEM_EXTREME requires bootmem to be functioning at the time of
    memory_present() calls.  This is not always feasible, so architectures
    which do not need it may allocate everything statically by using
    SPARSEMEM_STATIC.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b97054bbc394..79cf578e21b9 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -487,39 +487,29 @@ struct mem_section {
 	unsigned long section_mem_map;
 };
 
-#ifdef CONFIG_ARCH_SPARSEMEM_EXTREME
-/*
- * Should we ever require GCC 4 or later then the flat array scheme
- * can be eliminated and a uniform solution for EXTREME and !EXTREME can
- * be arrived at.
- */
-#define SECTION_ROOT_SHIFT	(PAGE_SHIFT-3)
-#define SECTION_ROOT_MASK	((1UL<<SECTION_ROOT_SHIFT) - 1)
-#define SECTION_TO_ROOT(_sec)	((_sec) >> SECTION_ROOT_SHIFT)
-#define NR_SECTION_ROOTS	(NR_MEM_SECTIONS >>  SECTION_ROOT_SHIFT)
+#ifdef CONFIG_SPARSEMEM_EXTREME
+#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))
+#else
+#define SECTIONS_PER_ROOT	1
+#endif
 
-extern struct mem_section *mem_section[NR_SECTION_ROOTS];
-
-static inline struct mem_section *__nr_to_section(unsigned long nr)
-{
-	if (!mem_section[SECTION_TO_ROOT(nr)])
-		return NULL;
-	return &mem_section[SECTION_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
-}
+#define SECTION_NR_TO_ROOT(sec)	((sec) / SECTIONS_PER_ROOT)
+#define NR_SECTION_ROOTS	(NR_MEM_SECTIONS / SECTIONS_PER_ROOT)
+#define SECTION_ROOT_MASK	(SECTIONS_PER_ROOT - 1)
 
+#ifdef CONFIG_SPARSEMEM_EXTREME
+extern struct mem_section *mem_section[NR_SECTION_ROOTS];
 #else
-
-extern struct mem_section mem_section[NR_MEM_SECTIONS];
+extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
+#endif
 
 static inline struct mem_section *__nr_to_section(unsigned long nr)
 {
-	return &mem_section[nr];
+	if (!mem_section[SECTION_NR_TO_ROOT(nr)])
+		return NULL;
+	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
 }
 
-#define sparse_index_init(_sec, _nid)  do {} while (0)
-
-#endif
-
 /*
  * We use the lower bits of the mem_map pointer to store
  * a little bit of information.  There should be at least

commit 802f192e4a600f7ef84ca25c8b818c8830acef5a
Author: Bob Picco <bob.picco@hp.com>
Date:   Sat Sep 3 15:54:26 2005 -0700

    [PATCH] SPARSEMEM EXTREME
    
    A new option for SPARSEMEM is ARCH_SPARSEMEM_EXTREME.  Architecture
    platforms with a very sparse physical address space would likely want to
    select this option.  For those architecture platforms that don't select the
    option, the code generated is equivalent to SPARSEMEM currently in -mm.
    I'll be posting a patch on ia64 ml which uses this new SPARSEMEM feature.
    
    ARCH_SPARSEMEM_EXTREME makes mem_section a one dimensional array of
    pointers to mem_sections.  This two level layout scheme is able to achieve
    smaller memory requirements for SPARSEMEM with the tradeoff of an
    additional shift and load when fetching the memory section.  The current
    SPARSEMEM -mm implementation is a one dimensional array of mem_sections
    which is the default SPARSEMEM configuration.  The patch attempts isolates
    the implementation details of the physical layout of the sparsemem section
    array.
    
    ARCH_SPARSEMEM_EXTREME depends on 64BIT and is by default boolean false.
    
    I've boot tested under aim load ia64 configured for ARCH_SPARSEMEM_EXTREME.
     I've also boot tested a 4 way Opteron machine with !ARCH_SPARSEMEM_EXTREME
    and tested with aim.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6c90461ed99f..b97054bbc394 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -487,6 +487,28 @@ struct mem_section {
 	unsigned long section_mem_map;
 };
 
+#ifdef CONFIG_ARCH_SPARSEMEM_EXTREME
+/*
+ * Should we ever require GCC 4 or later then the flat array scheme
+ * can be eliminated and a uniform solution for EXTREME and !EXTREME can
+ * be arrived at.
+ */
+#define SECTION_ROOT_SHIFT	(PAGE_SHIFT-3)
+#define SECTION_ROOT_MASK	((1UL<<SECTION_ROOT_SHIFT) - 1)
+#define SECTION_TO_ROOT(_sec)	((_sec) >> SECTION_ROOT_SHIFT)
+#define NR_SECTION_ROOTS	(NR_MEM_SECTIONS >>  SECTION_ROOT_SHIFT)
+
+extern struct mem_section *mem_section[NR_SECTION_ROOTS];
+
+static inline struct mem_section *__nr_to_section(unsigned long nr)
+{
+	if (!mem_section[SECTION_TO_ROOT(nr)])
+		return NULL;
+	return &mem_section[SECTION_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
+}
+
+#else
+
 extern struct mem_section mem_section[NR_MEM_SECTIONS];
 
 static inline struct mem_section *__nr_to_section(unsigned long nr)
@@ -494,6 +516,10 @@ static inline struct mem_section *__nr_to_section(unsigned long nr)
 	return &mem_section[nr];
 }
 
+#define sparse_index_init(_sec, _nid)  do {} while (0)
+
+#endif
+
 /*
  * We use the lower bits of the mem_map pointer to store
  * a little bit of information.  There should be at least
@@ -513,12 +539,12 @@ static inline struct page *__section_mem_map_addr(struct mem_section *section)
 
 static inline int valid_section(struct mem_section *section)
 {
-	return (section->section_mem_map & SECTION_MARKED_PRESENT);
+	return (section && (section->section_mem_map & SECTION_MARKED_PRESENT));
 }
 
 static inline int section_has_mem_map(struct mem_section *section)
 {
-	return (section->section_mem_map & SECTION_HAS_MEM_MAP);
+	return (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));
 }
 
 static inline int valid_section_nr(unsigned long nr)

commit 29751f6991e845f7d002a6ae520bf996b38c8dcd
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Thu Jun 23 00:08:00 2005 -0700

    [PATCH] sparsemem hotplug base
    
    Make sparse's initalization be accessible at runtime.  This allows sparse
    mappings to be created after boot in a hotplug situation.
    
    This patch is separated from the previous one just to give an indication how
    much of the sparse infrastructure is *just* for hotplug memory.
    
    The section_mem_map doesn't really store a pointer.  It stores something that
    is convenient to do some math against to get a pointer.  It isn't valid to
    just do *section_mem_map, so I don't think it should be stored as a pointer.
    
    There are a couple of things I'd like to store about a section.  First of all,
    the fact that it is !NULL does not mean that it is present.  There could be
    such a combination where section_mem_map *is* NULL, but the math gets you
    properly to a real mem_map.  So, I don't think that check is safe.
    
    Since we're storing 32-bit-aligned structures, we have a few bits in the
    bottom of the pointer to play with.  Use one bit to encode whether there's
    really a mem_map there, and the other one to tell whether there's a valid
    section there.  We need to distinguish between the two because sometimes
    there's a gap between when a section is discovered to be present and when we
    can get the mem_map for it.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 746b57e3d370..6c90461ed99f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -476,11 +476,56 @@ extern struct pglist_data contig_page_data;
 
 struct page;
 struct mem_section {
-	struct page *section_mem_map;
+	/*
+	 * This is, logically, a pointer to an array of struct
+	 * pages.  However, it is stored with some other magic.
+	 * (see sparse.c::sparse_init_one_section())
+	 *
+	 * Making it a UL at least makes someone do a cast
+	 * before using it wrong.
+	 */
+	unsigned long section_mem_map;
 };
 
 extern struct mem_section mem_section[NR_MEM_SECTIONS];
 
+static inline struct mem_section *__nr_to_section(unsigned long nr)
+{
+	return &mem_section[nr];
+}
+
+/*
+ * We use the lower bits of the mem_map pointer to store
+ * a little bit of information.  There should be at least
+ * 3 bits here due to 32-bit alignment.
+ */
+#define	SECTION_MARKED_PRESENT	(1UL<<0)
+#define SECTION_HAS_MEM_MAP	(1UL<<1)
+#define SECTION_MAP_LAST_BIT	(1UL<<2)
+#define SECTION_MAP_MASK	(~(SECTION_MAP_LAST_BIT-1))
+
+static inline struct page *__section_mem_map_addr(struct mem_section *section)
+{
+	unsigned long map = section->section_mem_map;
+	map &= SECTION_MAP_MASK;
+	return (struct page *)map;
+}
+
+static inline int valid_section(struct mem_section *section)
+{
+	return (section->section_mem_map & SECTION_MARKED_PRESENT);
+}
+
+static inline int section_has_mem_map(struct mem_section *section)
+{
+	return (section->section_mem_map & SECTION_HAS_MEM_MAP);
+}
+
+static inline int valid_section_nr(unsigned long nr)
+{
+	return valid_section(__nr_to_section(nr));
+}
+
 /*
  * Given a kernel address, find the home node of the underlying memory.
  */
@@ -488,24 +533,25 @@ extern struct mem_section mem_section[NR_MEM_SECTIONS];
 
 static inline struct mem_section *__pfn_to_section(unsigned long pfn)
 {
-	return &mem_section[pfn_to_section_nr(pfn)];
+	return __nr_to_section(pfn_to_section_nr(pfn));
 }
 
 #define pfn_to_page(pfn) 						\
 ({ 									\
 	unsigned long __pfn = (pfn);					\
-	__pfn_to_section(__pfn)->section_mem_map + __pfn;		\
+	__section_mem_map_addr(__pfn_to_section(__pfn)) + __pfn;	\
 })
 #define page_to_pfn(page)						\
 ({									\
-	page - mem_section[page_to_section(page)].section_mem_map;	\
+	page - __section_mem_map_addr(__nr_to_section(			\
+		page_to_section(page)));				\
 })
 
 static inline int pfn_valid(unsigned long pfn)
 {
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
 		return 0;
-	return mem_section[pfn_to_section_nr(pfn)].section_mem_map != 0;
+	return valid_section(__nr_to_section(pfn_to_section_nr(pfn)));
 }
 
 /*

commit 641c767389b19859a45e6de46d8e18cd935bdb60
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Thu Jun 23 00:07:59 2005 -0700

    [PATCH] sparsemem swiss cheese numa layouts
    
    The part of the sparsemem patch which modifies memmap_init_zone() has recently
    become a problem.  It changes behavior so that there is a call to
    pfn_to_page() for each individual page inside of a node's range:
    node_start_pfn through node_end_pfn.  It used to simply do this once, at the
    beginning of the node, but having sparsemem's non-contiguous mem_map[]s inside
    of a node made it necessary to change.
    
    Mike Kravetz recently wrote a patch which made the NUMA code accept some new
    kinds of layouts.  The system's memory was laid out like this, with node 0's
    memory in two pieces: one before and one after node 1's memory:
    
            Node 0: +++++     +++++
            Node 1:      +++++
    
    Previous behavior before Mike's patch was to assign nodes like this:
    
            Node 0: 00000     XXXXX
            Node 1:      11111
    
    Where the 'X' areas were simply thrown away.  The new behavior was to make the
    pg_data_t span node 0 across all of its areas, including areas that are really
    node 1's: Node 0: 000000000000000 Node 1: 11111
    
    This wastes a little bit of mem_map space, but ends up being OK, and more
    fully utilizes the system's memory.  memmap_init_zone() initializes all of the
    "struct page"s for node 0, even for the "hole", but those never get used,
    because there is no pfn_to_page() that resolves to those pages.  However, only
    calling pfn_to_page() once, memmap_init_zone() always uses the pages that were
    allocated for node0->node_mem_map because:
    
            struct page *start = pfn_to_page(start_pfn);
            // effectively start = &node->node_mem_map[0]
            for (page = start; page < (start + size); page++) {
                    init_page_here();...
                    page++;
            }
    
    Slow, and wasteful, but generally harmless.
    
    But, modify that to call pfn_to_page() for each loop iteration (like sparsemem
    does):
    
            for (pfn = start_pfn; pfn < < (start_pfn + size); pfn++++) {
                    page = pfn_to_page(pfn);
            }
    
    And you end up trying to initialize node 1's pages too early, along with bogus
    data from node 0.  This patch checks for those weird layouts and declines to
    touch the pages, making the more frequent pfn_to_page() calls OK to do.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 19860d317ec2..746b57e3d370 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -528,6 +528,12 @@ void sparse_init(void);
 #define sparse_init()	do {} while (0)
 #endif /* CONFIG_SPARSEMEM */
 
+#ifdef CONFIG_NODES_SPAN_OTHER_NODES
+#define early_pfn_in_nid(pfn, nid)	(early_pfn_to_nid(pfn) == (nid))
+#else
+#define early_pfn_in_nid(pfn, nid)	(1)
+#endif
+
 #ifndef early_pfn_valid
 #define early_pfn_valid(pfn)	(1)
 #endif

commit d41dee369bff3b9dcb6328d4d822926c28cc2594
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Thu Jun 23 00:07:54 2005 -0700

    [PATCH] sparsemem memory model
    
    Sparsemem abstracts the use of discontiguous mem_maps[].  This kind of
    mem_map[] is needed by discontiguous memory machines (like in the old
    CONFIG_DISCONTIGMEM case) as well as memory hotplug systems.  Sparsemem
    replaces DISCONTIGMEM when enabled, and it is hoped that it can eventually
    become a complete replacement.
    
    A significant advantage over DISCONTIGMEM is that it's completely separated
    from CONFIG_NUMA.  When producing this patch, it became apparent in that NUMA
    and DISCONTIG are often confused.
    
    Another advantage is that sparse doesn't require each NUMA node's ranges to be
    contiguous.  It can handle overlapping ranges between nodes with no problems,
    where DISCONTIGMEM currently throws away that memory.
    
    Sparsemem uses an array to provide different pfn_to_page() translations for
    each SECTION_SIZE area of physical memory.  This is what allows the mem_map[]
    to be chopped up.
    
    In order to do quick pfn_to_page() operations, the section number of the page
    is encoded in page->flags.  Part of the sparsemem infrastructure enables
    sharing of these bits more dynamically (at compile-time) between the
    page_zone() and sparsemem operations.  However, on 32-bit architectures, the
    number of bits is quite limited, and may require growing the size of the
    page->flags type in certain conditions.  Several things might force this to
    occur: a decrease in the SECTION_SIZE (if you want to hotplug smaller areas of
    memory), an increase in the physical address space, or an increase in the
    number of used page->flags.
    
    One thing to note is that, once sparsemem is present, the NUMA node
    information no longer needs to be stored in the page->flags.  It might provide
    speed increases on certain platforms and will be stored there if there is
    room.  But, if out of room, an alternate (theoretically slower) mechanism is
    used.
    
    This patch introduces CONFIG_FLATMEM.  It is used in almost all cases where
    there used to be an #ifndef DISCONTIG, because SPARSEMEM and DISCONTIGMEM
    often have to compile out the same areas of code.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Martin Bligh <mbligh@aracnet.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 6ef07de98d69..19860d317ec2 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -269,7 +269,9 @@ typedef struct pglist_data {
 	struct zone node_zones[MAX_NR_ZONES];
 	struct zonelist node_zonelists[GFP_ZONETYPES];
 	int nr_zones;
+#ifdef CONFIG_FLAT_NODE_MEM_MAP
 	struct page *node_mem_map;
+#endif
 	struct bootmem_data *bdata;
 	unsigned long node_start_pfn;
 	unsigned long node_present_pages; /* total number of physical pages */
@@ -284,7 +286,11 @@ typedef struct pglist_data {
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
 #define node_spanned_pages(nid)	(NODE_DATA(nid)->node_spanned_pages)
+#ifdef CONFIG_FLAT_NODE_MEM_MAP
 #define pgdat_page_nr(pgdat, pagenr)	((pgdat)->node_mem_map + (pagenr))
+#else
+#define pgdat_page_nr(pgdat, pagenr)	pfn_to_page((pgdat)->node_start_pfn + (pagenr))
+#endif
 #define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))
 
 extern struct pglist_data *pgdat_list;
@@ -416,6 +422,10 @@ extern struct pglist_data contig_page_data;
 
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
+#ifdef CONFIG_SPARSEMEM
+#include <asm/sparsemem.h>
+#endif
+
 #if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
 /*
  * with 32 bit page->flags field, we reserve 8 bits for node/zone info.
@@ -439,6 +449,92 @@ extern struct pglist_data contig_page_data;
 #define early_pfn_to_nid(nid)  (0UL)
 #endif
 
+#define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
+#define section_nr_to_pfn(sec) ((sec) << PFN_SECTION_SHIFT)
+
+#ifdef CONFIG_SPARSEMEM
+
+/*
+ * SECTION_SHIFT    		#bits space required to store a section #
+ *
+ * PA_SECTION_SHIFT		physical address to/from section number
+ * PFN_SECTION_SHIFT		pfn to/from section number
+ */
+#define SECTIONS_SHIFT		(MAX_PHYSMEM_BITS - SECTION_SIZE_BITS)
+
+#define PA_SECTION_SHIFT	(SECTION_SIZE_BITS)
+#define PFN_SECTION_SHIFT	(SECTION_SIZE_BITS - PAGE_SHIFT)
+
+#define NR_MEM_SECTIONS		(1UL << SECTIONS_SHIFT)
+
+#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)
+#define PAGE_SECTION_MASK	(~(PAGES_PER_SECTION-1))
+
+#if (MAX_ORDER - 1 + PAGE_SHIFT) > SECTION_SIZE_BITS
+#error Allocator MAX_ORDER exceeds SECTION_SIZE
+#endif
+
+struct page;
+struct mem_section {
+	struct page *section_mem_map;
+};
+
+extern struct mem_section mem_section[NR_MEM_SECTIONS];
+
+/*
+ * Given a kernel address, find the home node of the underlying memory.
+ */
+#define kvaddr_to_nid(kaddr)	pfn_to_nid(__pa(kaddr) >> PAGE_SHIFT)
+
+static inline struct mem_section *__pfn_to_section(unsigned long pfn)
+{
+	return &mem_section[pfn_to_section_nr(pfn)];
+}
+
+#define pfn_to_page(pfn) 						\
+({ 									\
+	unsigned long __pfn = (pfn);					\
+	__pfn_to_section(__pfn)->section_mem_map + __pfn;		\
+})
+#define page_to_pfn(page)						\
+({									\
+	page - mem_section[page_to_section(page)].section_mem_map;	\
+})
+
+static inline int pfn_valid(unsigned long pfn)
+{
+	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
+		return 0;
+	return mem_section[pfn_to_section_nr(pfn)].section_mem_map != 0;
+}
+
+/*
+ * These are _only_ used during initialisation, therefore they
+ * can use __initdata ...  They could have names to indicate
+ * this restriction.
+ */
+#ifdef CONFIG_NUMA
+#define pfn_to_nid		early_pfn_to_nid
+#endif
+
+#define pfn_to_pgdat(pfn)						\
+({									\
+	NODE_DATA(pfn_to_nid(pfn));					\
+})
+
+#define early_pfn_valid(pfn)	pfn_valid(pfn)
+void sparse_init(void);
+#else
+#define sparse_init()	do {} while (0)
+#endif /* CONFIG_SPARSEMEM */
+
+#ifndef early_pfn_valid
+#define early_pfn_valid(pfn)	(1)
+#endif
+
+void memory_present(int nid, unsigned long start, unsigned long end);
+unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MMZONE_H */

commit b159d43fbf7eaaac6ecc647f51cf4257332db47b
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Thu Jun 23 00:07:52 2005 -0700

    [PATCH] generify early_pfn_to_nid
    
    Provide a default implementation for early_pfn_to_nid returning node 0.  Allow
    architectures to override this with their own implementation out of
    asm/mmzone.h.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Martin Bligh <mbligh@aracnet.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 95f4a780ea66..6ef07de98d69 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -435,6 +435,10 @@ extern struct pglist_data contig_page_data;
 
 #endif
 
+#ifndef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID
+#define early_pfn_to_nid(nid)  (0UL)
+#endif
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MMZONE_H */

commit 93b7504e3e6c1d98586854806e51bea329ea3aa9
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Thu Jun 23 00:07:47 2005 -0700

    [PATCH] Introduce new Kconfig option for NUMA or DISCONTIG
    
    There is some confusion that arose when working on SPARSEMEM patch between
    what is needed for DISCONTIG vs. NUMA.
    
    Multiple pg_data_t's are needed for DISCONTIGMEM or NUMA, independently.
    All of the current NUMA implementations require an implementation of
    DISCONTIG.  Because of this, quite a lot of code which is really needed for
    NUMA is actually under DISCONTIG #ifdefs.  For SPARSEMEM, we changed some
    of these #ifdefs to CONFIG_NUMA, but that broke the DISCONTIG=y and NUMA=n
    case.
    
    Introducing this new NEED_MULTIPLE_NODES config option allows code that is
    needed for both NUMA or DISCONTIG to be separated out from code that is
    specific to DISCONTIG.
    
    One great advantage of this approach is that it doesn't require every
    architecture to be converted over.  All of the current implementations
    should "just work", only the ones implementing SPARSEMEM will have to be
    fixed up.
    
    The change to free_area_init() makes it work inside, or out of the new
    config option.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 39e912708e2a..95f4a780ea66 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -402,7 +402,7 @@ int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
 /* Returns the number of the current Node. */
 #define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
 
-#ifndef CONFIG_DISCONTIGMEM
+#ifndef CONFIG_NEED_MULTIPLE_NODES
 
 extern struct pglist_data contig_page_data;
 #define NODE_DATA(nid)		(&contig_page_data)
@@ -410,11 +410,11 @@ extern struct pglist_data contig_page_data;
 #define MAX_NODES_SHIFT		1
 #define pfn_to_nid(pfn)		(0)
 
-#else /* CONFIG_DISCONTIGMEM */
+#else /* CONFIG_NEED_MULTIPLE_NODES */
 
 #include <asm/mmzone.h>
 
-#endif /* !CONFIG_DISCONTIGMEM */
+#endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
 #if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
 /*

commit 348f8b6c4837a07304d2f72b11ce8d96588065e0
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Thu Jun 23 00:07:40 2005 -0700

    [PATCH] sparsemem base: reorganize page->flags bit operations
    
    Generify the value fields in the page_flags.  The aim is to allow the location
    and size of these fields to be varied.  Additionally we want to move away from
    fixed allocations per field whilst still enforcing the overall bit utilisation
    limits.  We rely on the compiler to spot and optimise the accessor functions.
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index b79633d3a97b..39e912708e2a 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -414,30 +414,25 @@ extern struct pglist_data contig_page_data;
 
 #include <asm/mmzone.h>
 
+#endif /* !CONFIG_DISCONTIGMEM */
+
 #if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
 /*
  * with 32 bit page->flags field, we reserve 8 bits for node/zone info.
  * there are 3 zones (2 bits) and this leaves 8-2=6 bits for nodes.
  */
-#define MAX_NODES_SHIFT		6
+#define FLAGS_RESERVED		8
+
 #elif BITS_PER_LONG == 64
 /*
  * with 64 bit flags field, there's plenty of room.
  */
-#define MAX_NODES_SHIFT		10
-#endif
+#define FLAGS_RESERVED		32
 
-#endif /* !CONFIG_DISCONTIGMEM */
-
-#if NODES_SHIFT > MAX_NODES_SHIFT
-#error NODES_SHIFT > MAX_NODES_SHIFT
-#endif
+#else
 
-/* There are currently 3 zones: DMA, Normal & Highmem, thus we need 2 bits */
-#define MAX_ZONES_SHIFT		2
+#error BITS_PER_LONG not defined
 
-#if ZONES_SHIFT > MAX_ZONES_SHIFT
-#error ZONES_SHIFT > MAX_ZONES_SHIFT
 #endif
 
 #endif /* !__ASSEMBLY__ */

commit 408fde81c1bff15c875a3618481e93a01dcc79ea
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Thu Jun 23 00:07:37 2005 -0700

    [PATCH] remove non-DISCONTIG use of pgdat->node_mem_map
    
    This patch effectively eliminates direct use of pgdat->node_mem_map outside
    of the DISCONTIG code.  On a flat memory system, these fields aren't
    currently used, neither are they on a sparsemem system.
    
    There was also a node_mem_map(nid) macro on many architectures.  Its use
    along with the use of ->node_mem_map itself was not consistent.  It has
    been removed in favor of two new, more explicit, arch-independent macros:
    
            pgdat_page_nr(pgdat, pagenr)
            nid_page_nr(nid, pagenr)
    
    I called them "pgdat" and "nid" because we overload the term "node" to mean
    "NUMA node", "DISCONTIG node" or "pg_data_t" in very confusing ways.  I
    believe the newer names are much clearer.
    
    These macros can be overridden in the sparsemem case with a theoretically
    slower operation using node_start_pfn and pfn_to_page(), instead.  We could
    make this the only behavior if people want, but I don't want to change too
    much at once.  One thing at a time.
    
    This patch removes more code than it adds.
    
    Compile tested on alpha, alpha discontig, arm, arm-discontig, i386, i386
    generic, NUMAQ, Summit, ppc64, ppc64 discontig, and x86_64.  Full list
    here: http://sr71.net/patches/2.6.12/2.6.12-rc1-mhp2/configs/
    
    Boot tested on NUMAQ, x86 SMP and ppc64 power4/5 LPARs.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Martin J. Bligh <mbligh@aracnet.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 4733d35d8223..b79633d3a97b 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -284,6 +284,8 @@ typedef struct pglist_data {
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
 #define node_spanned_pages(nid)	(NODE_DATA(nid)->node_spanned_pages)
+#define pgdat_page_nr(pgdat, pagenr)	((pgdat)->node_mem_map + (pagenr))
+#define nid_page_nr(nid, pagenr) 	pgdat_page_nr(NODE_DATA(nid),(pagenr))
 
 extern struct pglist_data *pgdat_list;
 

commit e7c8d5c9955a4d2e88e36b640563f5d6d5aba48a
Author: Christoph Lameter <christoph@lameter.com>
Date:   Tue Jun 21 17:14:47 2005 -0700

    [PATCH] node local per-cpu-pages
    
    This patch modifies the way pagesets in struct zone are managed.
    
    Each zone has a per-cpu array of pagesets.  So any particular CPU has some
    memory in each zone structure which belongs to itself.  Even if that CPU is
    not local to that zone.
    
    So the patch relocates the pagesets for each cpu to the node that is nearest
    to the cpu instead of allocating the pagesets in the (possibly remote) target
    zone.  This means that the operations to manage pages on remote zone can be
    done with information available locally.
    
    We play a macro trick so that non-NUMA pmachines avoid the additional
    pointer chase on the page allocator fastpath.
    
    AIM7 benchmark on a 32 CPU SGI Altix
    
    w/o patches:
    Tasks    jobs/min  jti  jobs/min/task      real       cpu
        1      484.68  100       484.6769     12.01      1.97   Fri Mar 25 11:01:42 2005
      100    27140.46   89       271.4046     21.44    148.71   Fri Mar 25 11:02:04 2005
      200    30792.02   82       153.9601     37.80    296.72   Fri Mar 25 11:02:42 2005
      300    32209.27   81       107.3642     54.21    451.34   Fri Mar 25 11:03:37 2005
      400    34962.83   78        87.4071     66.59    588.97   Fri Mar 25 11:04:44 2005
      500    31676.92   75        63.3538     91.87    742.71   Fri Mar 25 11:06:16 2005
      600    36032.69   73        60.0545     96.91    885.44   Fri Mar 25 11:07:54 2005
      700    35540.43   77        50.7720    114.63   1024.28   Fri Mar 25 11:09:49 2005
      800    33906.70   74        42.3834    137.32   1181.65   Fri Mar 25 11:12:06 2005
      900    34120.67   73        37.9119    153.51   1325.26   Fri Mar 25 11:14:41 2005
     1000    34802.37   74        34.8024    167.23   1465.26   Fri Mar 25 11:17:28 2005
    
    with slab API changes and pageset patch:
    
    Tasks    jobs/min  jti  jobs/min/task      real       cpu
        1      485.00  100       485.0000     12.00      1.96   Fri Mar 25 11:46:18 2005
      100    28000.96   89       280.0096     20.79    150.45   Fri Mar 25 11:46:39 2005
      200    32285.80   79       161.4290     36.05    293.37   Fri Mar 25 11:47:16 2005
      300    40424.15   84       134.7472     43.19    438.42   Fri Mar 25 11:47:59 2005
      400    39155.01   79        97.8875     59.46    590.05   Fri Mar 25 11:48:59 2005
      500    37881.25   82        75.7625     76.82    730.19   Fri Mar 25 11:50:16 2005
      600    39083.14   78        65.1386     89.35    872.79   Fri Mar 25 11:51:46 2005
      700    38627.83   77        55.1826    105.47   1022.46   Fri Mar 25 11:53:32 2005
      800    39631.94   78        49.5399    117.48   1169.94   Fri Mar 25 11:55:30 2005
      900    36903.70   79        41.0041    141.94   1310.78   Fri Mar 25 11:57:53 2005
     1000    36201.23   77        36.2012    160.77   1458.31   Fri Mar 25 12:00:34 2005
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Shobhit Dayal <shobhit@calsoftinc.com>
    Signed-off-by: Shai Fultheim <Shai@Scalex86.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 18fed8b67943..4733d35d8223 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -63,6 +63,12 @@ struct per_cpu_pageset {
 #endif
 } ____cacheline_aligned_in_smp;
 
+#ifdef CONFIG_NUMA
+#define zone_pcp(__z, __cpu) ((__z)->pageset[(__cpu)])
+#else
+#define zone_pcp(__z, __cpu) (&(__z)->pageset[(__cpu)])
+#endif
+
 #define ZONE_DMA		0
 #define ZONE_NORMAL		1
 #define ZONE_HIGHMEM		2
@@ -122,8 +128,11 @@ struct zone {
 	 */
 	unsigned long		lowmem_reserve[MAX_NR_ZONES];
 
+#ifdef CONFIG_NUMA
+	struct per_cpu_pageset	*pageset[NR_CPUS];
+#else
 	struct per_cpu_pageset	pageset[NR_CPUS];
-
+#endif
 	/*
 	 * free areas of different sizes
 	 */

commit 1e7e5a9048b30c57ba1ddaa6cdf59b21b65cde99
Author: Martin Hicks <mort@sgi.com>
Date:   Tue Jun 21 17:14:43 2005 -0700

    [PATCH] VM: rate limit early reclaim
    
    When early zone reclaim is turned on the LRU is scanned more frequently when a
    zone is low on memory.  This limits when the zone reclaim can be called by
    skipping the scan if another thread (either via kswapd or sync reclaim) is
    already reclaiming from the zone.
    
    Signed-off-by: Martin Hicks <mort@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dfc2452ccb10..18fed8b67943 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -149,6 +149,8 @@ struct zone {
 	 * as it fails a watermark_ok() in __alloc_pages?
 	 */
 	int			reclaim_pages;
+	/* A count of how many reclaimers are scanning this zone */
+	atomic_t		reclaim_in_progress;
 
 	/*
 	 * prev_priority holds the scanning priority for this zone.  It is

commit 753ee728964e5afb80c17659cc6c3a6fd0a42fe0
Author: Martin Hicks <mort@sgi.com>
Date:   Tue Jun 21 17:14:41 2005 -0700

    [PATCH] VM: early zone reclaim
    
    This is the core of the (much simplified) early reclaim.  The goal of this
    patch is to reclaim some easily-freed pages from a zone before falling back
    onto another zone.
    
    One of the major uses of this is NUMA machines.  With the default allocator
    behavior the allocator would look for memory in another zone, which might be
    off-node, before trying to reclaim from the current zone.
    
    This adds a zone tuneable to enable early zone reclaim.  It is selected on a
    per-zone basis and is turned on/off via syscall.
    
    Adding some extra throttling on the reclaim was also required (patch
    4/4).  Without the machine would grind to a crawl when doing a "make -j"
    kernel build.  Even with this patch the System Time is higher on
    average, but it seems tolerable.  Here are some numbers for kernbench
    runs on a 2-node, 4cpu, 8Gig RAM Altix in the "make -j" run:
    
                            wall  user   sys   %cpu  ctx sw.  sleeps
                            ----  ----   ---   ----   ------  ------
    No patch                1009  1384   847   258   298170   504402
    w/patch, no reclaim     880   1376   667   288   254064   396745
    w/patch & reclaim       1079  1385   926   252   291625   548873
    
    These numbers are the average of 2 runs of 3 "make -j" runs done right
    after system boot.  Run-to-run variability for "make -j" is huge, so
    these numbers aren't terribly useful except to seee that with reclaim
    the benchmark still finishes in a reasonable amount of time.
    
    I also looked at the NUMA hit/miss stats for the "make -j" runs and the
    reclaim doesn't make any difference when the machine is thrashing away.
    
    Doing a "make -j8" on a single node that is filled with page cache pages
    takes 700 seconds with reclaim turned on and 735 seconds without reclaim
    (due to remote memory accesses).
    
    The simple zone_reclaim syscall program is at
    http://www.bork.org/~mort/sgi/zone_reclaim.c
    
    Signed-off-by: Martin Hicks <mort@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index beacd931b606..dfc2452ccb10 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -144,6 +144,12 @@ struct zone {
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	int			all_unreclaimable; /* All pages pinned */
 
+	/*
+	 * Does the allocator try to reclaim pages from the zone as soon
+	 * as it fails a watermark_ok() in __alloc_pages?
+	 */
+	int			reclaim_pages;
+
 	/*
 	 * prev_priority holds the scanning priority for this zone.  It is
 	 * defined as the scanning priority at which we achieved our reclaim

commit 39c715b71740c4a78ba4769fb54826929bac03cb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 21 17:14:34 2005 -0700

    [PATCH] smp_processor_id() cleanup
    
    This patch implements a number of smp_processor_id() cleanup ideas that
    Arjan van de Ven and I came up with.
    
    The previous __smp_processor_id/_smp_processor_id/smp_processor_id API
    spaghetti was hard to follow both on the implementational and on the
    usage side.
    
    Some of the complexity arose from picking wrong names, some of the
    complexity comes from the fact that not all architectures defined
    __smp_processor_id.
    
    In the new code, there are two externally visible symbols:
    
     - smp_processor_id(): debug variant.
    
     - raw_smp_processor_id(): nondebug variant. Replaces all existing
       uses of _smp_processor_id() and __smp_processor_id(). Defined
       by every SMP architecture in include/asm-*/smp.h.
    
    There is one new internal symbol, dependent on DEBUG_PREEMPT:
    
     - debug_smp_processor_id(): internal debug variant, mapped to
                                 smp_processor_id().
    
    Also, i moved debug_smp_processor_id() from lib/kernel_lock.c into a new
    lib/smp_processor_id.c file.  All related comments got updated and/or
    clarified.
    
    I have build/boot tested the following 8 .config combinations on x86:
    
     {SMP,UP} x {PREEMPT,!PREEMPT} x {DEBUG_PREEMPT,!DEBUG_PREEMPT}
    
    I have also build/boot tested x64 on UP/PREEMPT/DEBUG_PREEMPT.  (Other
    architectures are untested, but should work just fine.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index e530c6c092f1..beacd931b606 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -381,7 +381,7 @@ int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
 
 #include <linux/topology.h>
 /* Returns the number of the current Node. */
-#define numa_node_id()		(cpu_to_node(_smp_processor_id()))
+#define numa_node_id()		(cpu_to_node(raw_smp_processor_id()))
 
 #ifndef CONFIG_DISCONTIGMEM
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
new file mode 100644
index 000000000000..e530c6c092f1
--- /dev/null
+++ b/include/linux/mmzone.h
@@ -0,0 +1,426 @@
+#ifndef _LINUX_MMZONE_H
+#define _LINUX_MMZONE_H
+
+#ifdef __KERNEL__
+#ifndef __ASSEMBLY__
+
+#include <linux/config.h>
+#include <linux/spinlock.h>
+#include <linux/list.h>
+#include <linux/wait.h>
+#include <linux/cache.h>
+#include <linux/threads.h>
+#include <linux/numa.h>
+#include <linux/init.h>
+#include <asm/atomic.h>
+
+/* Free memory management - zoned buddy allocator.  */
+#ifndef CONFIG_FORCE_MAX_ZONEORDER
+#define MAX_ORDER 11
+#else
+#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
+#endif
+
+struct free_area {
+	struct list_head	free_list;
+	unsigned long		nr_free;
+};
+
+struct pglist_data;
+
+/*
+ * zone->lock and zone->lru_lock are two of the hottest locks in the kernel.
+ * So add a wild amount of padding here to ensure that they fall into separate
+ * cachelines.  There are very few zone structures in the machine, so space
+ * consumption is not a concern here.
+ */
+#if defined(CONFIG_SMP)
+struct zone_padding {
+	char x[0];
+} ____cacheline_maxaligned_in_smp;
+#define ZONE_PADDING(name)	struct zone_padding name;
+#else
+#define ZONE_PADDING(name)
+#endif
+
+struct per_cpu_pages {
+	int count;		/* number of pages in the list */
+	int low;		/* low watermark, refill needed */
+	int high;		/* high watermark, emptying needed */
+	int batch;		/* chunk size for buddy add/remove */
+	struct list_head list;	/* the list of pages */
+};
+
+struct per_cpu_pageset {
+	struct per_cpu_pages pcp[2];	/* 0: hot.  1: cold */
+#ifdef CONFIG_NUMA
+	unsigned long numa_hit;		/* allocated in intended node */
+	unsigned long numa_miss;	/* allocated in non intended node */
+	unsigned long numa_foreign;	/* was intended here, hit elsewhere */
+	unsigned long interleave_hit; 	/* interleaver prefered this zone */
+	unsigned long local_node;	/* allocation from local node */
+	unsigned long other_node;	/* allocation from other node */
+#endif
+} ____cacheline_aligned_in_smp;
+
+#define ZONE_DMA		0
+#define ZONE_NORMAL		1
+#define ZONE_HIGHMEM		2
+
+#define MAX_NR_ZONES		3	/* Sync this with ZONES_SHIFT */
+#define ZONES_SHIFT		2	/* ceil(log2(MAX_NR_ZONES)) */
+
+
+/*
+ * When a memory allocation must conform to specific limitations (such
+ * as being suitable for DMA) the caller will pass in hints to the
+ * allocator in the gfp_mask, in the zone modifier bits.  These bits
+ * are used to select a priority ordered list of memory zones which
+ * match the requested limits.  GFP_ZONEMASK defines which bits within
+ * the gfp_mask should be considered as zone modifiers.  Each valid
+ * combination of the zone modifier bits has a corresponding list
+ * of zones (in node_zonelists).  Thus for two zone modifiers there
+ * will be a maximum of 4 (2 ** 2) zonelists, for 3 modifiers there will
+ * be 8 (2 ** 3) zonelists.  GFP_ZONETYPES defines the number of possible
+ * combinations of zone modifiers in "zone modifier space".
+ */
+#define GFP_ZONEMASK	0x03
+/*
+ * As an optimisation any zone modifier bits which are only valid when
+ * no other zone modifier bits are set (loners) should be placed in
+ * the highest order bits of this field.  This allows us to reduce the
+ * extent of the zonelists thus saving space.  For example in the case
+ * of three zone modifier bits, we could require up to eight zonelists.
+ * If the left most zone modifier is a "loner" then the highest valid
+ * zonelist would be four allowing us to allocate only five zonelists.
+ * Use the first form when the left most bit is not a "loner", otherwise
+ * use the second.
+ */
+/* #define GFP_ZONETYPES	(GFP_ZONEMASK + 1) */		/* Non-loner */
+#define GFP_ZONETYPES	((GFP_ZONEMASK + 1) / 2 + 1)		/* Loner */
+
+/*
+ * On machines where it is needed (eg PCs) we divide physical memory
+ * into multiple physical zones. On a PC we have 3 zones:
+ *
+ * ZONE_DMA	  < 16 MB	ISA DMA capable memory
+ * ZONE_NORMAL	16-896 MB	direct mapped by the kernel
+ * ZONE_HIGHMEM	 > 896 MB	only page cache and user processes
+ */
+
+struct zone {
+	/* Fields commonly accessed by the page allocator */
+	unsigned long		free_pages;
+	unsigned long		pages_min, pages_low, pages_high;
+	/*
+	 * We don't know if the memory that we're going to allocate will be freeable
+	 * or/and it will be released eventually, so to avoid totally wasting several
+	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
+	 * to run OOM on the lower zones despite there's tons of freeable ram
+	 * on the higher zones). This array is recalculated at runtime if the
+	 * sysctl_lowmem_reserve_ratio sysctl changes.
+	 */
+	unsigned long		lowmem_reserve[MAX_NR_ZONES];
+
+	struct per_cpu_pageset	pageset[NR_CPUS];
+
+	/*
+	 * free areas of different sizes
+	 */
+	spinlock_t		lock;
+	struct free_area	free_area[MAX_ORDER];
+
+
+	ZONE_PADDING(_pad1_)
+
+	/* Fields commonly accessed by the page reclaim scanner */
+	spinlock_t		lru_lock;	
+	struct list_head	active_list;
+	struct list_head	inactive_list;
+	unsigned long		nr_scan_active;
+	unsigned long		nr_scan_inactive;
+	unsigned long		nr_active;
+	unsigned long		nr_inactive;
+	unsigned long		pages_scanned;	   /* since last reclaim */
+	int			all_unreclaimable; /* All pages pinned */
+
+	/*
+	 * prev_priority holds the scanning priority for this zone.  It is
+	 * defined as the scanning priority at which we achieved our reclaim
+	 * target at the previous try_to_free_pages() or balance_pgdat()
+	 * invokation.
+	 *
+	 * We use prev_priority as a measure of how much stress page reclaim is
+	 * under - it drives the swappiness decision: whether to unmap mapped
+	 * pages.
+	 *
+	 * temp_priority is used to remember the scanning priority at which
+	 * this zone was successfully refilled to free_pages == pages_high.
+	 *
+	 * Access to both these fields is quite racy even on uniprocessor.  But
+	 * it is expected to average out OK.
+	 */
+	int temp_priority;
+	int prev_priority;
+
+
+	ZONE_PADDING(_pad2_)
+	/* Rarely used or read-mostly fields */
+
+	/*
+	 * wait_table		-- the array holding the hash table
+	 * wait_table_size	-- the size of the hash table array
+	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
+	 *
+	 * The purpose of all these is to keep track of the people
+	 * waiting for a page to become available and make them
+	 * runnable again when possible. The trouble is that this
+	 * consumes a lot of space, especially when so few things
+	 * wait on pages at a given time. So instead of using
+	 * per-page waitqueues, we use a waitqueue hash table.
+	 *
+	 * The bucket discipline is to sleep on the same queue when
+	 * colliding and wake all in that wait queue when removing.
+	 * When something wakes, it must check to be sure its page is
+	 * truly available, a la thundering herd. The cost of a
+	 * collision is great, but given the expected load of the
+	 * table, they should be so rare as to be outweighed by the
+	 * benefits from the saved space.
+	 *
+	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
+	 * primary users of these fields, and in mm/page_alloc.c
+	 * free_area_init_core() performs the initialization of them.
+	 */
+	wait_queue_head_t	* wait_table;
+	unsigned long		wait_table_size;
+	unsigned long		wait_table_bits;
+
+	/*
+	 * Discontig memory support fields.
+	 */
+	struct pglist_data	*zone_pgdat;
+	struct page		*zone_mem_map;
+	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
+	unsigned long		zone_start_pfn;
+
+	unsigned long		spanned_pages;	/* total size, including holes */
+	unsigned long		present_pages;	/* amount of memory (excluding holes) */
+
+	/*
+	 * rarely used fields:
+	 */
+	char			*name;
+} ____cacheline_maxaligned_in_smp;
+
+
+/*
+ * The "priority" of VM scanning is how much of the queues we will scan in one
+ * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
+ * queues ("queue_length >> 12") during an aging round.
+ */
+#define DEF_PRIORITY 12
+
+/*
+ * One allocation request operates on a zonelist. A zonelist
+ * is a list of zones, the first one is the 'goal' of the
+ * allocation, the other zones are fallback zones, in decreasing
+ * priority.
+ *
+ * Right now a zonelist takes up less than a cacheline. We never
+ * modify it apart from boot-up, and only a few indices are used,
+ * so despite the zonelist table being relatively big, the cache
+ * footprint of this construct is very small.
+ */
+struct zonelist {
+	struct zone *zones[MAX_NUMNODES * MAX_NR_ZONES + 1]; // NULL delimited
+};
+
+
+/*
+ * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
+ * (mostly NUMA machines?) to denote a higher-level memory zone than the
+ * zone denotes.
+ *
+ * On NUMA machines, each NUMA node would have a pg_data_t to describe
+ * it's memory layout.
+ *
+ * Memory statistics and page replacement data structures are maintained on a
+ * per-zone basis.
+ */
+struct bootmem_data;
+typedef struct pglist_data {
+	struct zone node_zones[MAX_NR_ZONES];
+	struct zonelist node_zonelists[GFP_ZONETYPES];
+	int nr_zones;
+	struct page *node_mem_map;
+	struct bootmem_data *bdata;
+	unsigned long node_start_pfn;
+	unsigned long node_present_pages; /* total number of physical pages */
+	unsigned long node_spanned_pages; /* total size of physical page
+					     range, including holes */
+	int node_id;
+	struct pglist_data *pgdat_next;
+	wait_queue_head_t kswapd_wait;
+	struct task_struct *kswapd;
+	int kswapd_max_order;
+} pg_data_t;
+
+#define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
+#define node_spanned_pages(nid)	(NODE_DATA(nid)->node_spanned_pages)
+
+extern struct pglist_data *pgdat_list;
+
+void __get_zone_counts(unsigned long *active, unsigned long *inactive,
+			unsigned long *free, struct pglist_data *pgdat);
+void get_zone_counts(unsigned long *active, unsigned long *inactive,
+			unsigned long *free);
+void build_all_zonelists(void);
+void wakeup_kswapd(struct zone *zone, int order);
+int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
+		int alloc_type, int can_try_harder, int gfp_high);
+
+#ifdef CONFIG_HAVE_MEMORY_PRESENT
+void memory_present(int nid, unsigned long start, unsigned long end);
+#else
+static inline void memory_present(int nid, unsigned long start, unsigned long end) {}
+#endif
+
+#ifdef CONFIG_NEED_NODE_MEMMAP_SIZE
+unsigned long __init node_memmap_size_bytes(int, unsigned long, unsigned long);
+#endif
+
+/*
+ * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
+ */
+#define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
+
+/**
+ * for_each_pgdat - helper macro to iterate over all nodes
+ * @pgdat - pointer to a pg_data_t variable
+ *
+ * Meant to help with common loops of the form
+ * pgdat = pgdat_list;
+ * while(pgdat) {
+ * 	...
+ * 	pgdat = pgdat->pgdat_next;
+ * }
+ */
+#define for_each_pgdat(pgdat) \
+	for (pgdat = pgdat_list; pgdat; pgdat = pgdat->pgdat_next)
+
+/*
+ * next_zone - helper magic for for_each_zone()
+ * Thanks to William Lee Irwin III for this piece of ingenuity.
+ */
+static inline struct zone *next_zone(struct zone *zone)
+{
+	pg_data_t *pgdat = zone->zone_pgdat;
+
+	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
+		zone++;
+	else if (pgdat->pgdat_next) {
+		pgdat = pgdat->pgdat_next;
+		zone = pgdat->node_zones;
+	} else
+		zone = NULL;
+
+	return zone;
+}
+
+/**
+ * for_each_zone - helper macro to iterate over all memory zones
+ * @zone - pointer to struct zone variable
+ *
+ * The user only needs to declare the zone variable, for_each_zone
+ * fills it in. This basically means for_each_zone() is an
+ * easier to read version of this piece of code:
+ *
+ * for (pgdat = pgdat_list; pgdat; pgdat = pgdat->node_next)
+ * 	for (i = 0; i < MAX_NR_ZONES; ++i) {
+ * 		struct zone * z = pgdat->node_zones + i;
+ * 		...
+ * 	}
+ * }
+ */
+#define for_each_zone(zone) \
+	for (zone = pgdat_list->node_zones; zone; zone = next_zone(zone))
+
+static inline int is_highmem_idx(int idx)
+{
+	return (idx == ZONE_HIGHMEM);
+}
+
+static inline int is_normal_idx(int idx)
+{
+	return (idx == ZONE_NORMAL);
+}
+/**
+ * is_highmem - helper function to quickly check if a struct zone is a 
+ *              highmem zone or not.  This is an attempt to keep references
+ *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
+ * @zone - pointer to struct zone variable
+ */
+static inline int is_highmem(struct zone *zone)
+{
+	return zone == zone->zone_pgdat->node_zones + ZONE_HIGHMEM;
+}
+
+static inline int is_normal(struct zone *zone)
+{
+	return zone == zone->zone_pgdat->node_zones + ZONE_NORMAL;
+}
+
+/* These two functions are used to setup the per zone pages min values */
+struct ctl_table;
+struct file;
+int min_free_kbytes_sysctl_handler(struct ctl_table *, int, struct file *, 
+					void __user *, size_t *, loff_t *);
+extern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1];
+int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, struct file *,
+					void __user *, size_t *, loff_t *);
+
+#include <linux/topology.h>
+/* Returns the number of the current Node. */
+#define numa_node_id()		(cpu_to_node(_smp_processor_id()))
+
+#ifndef CONFIG_DISCONTIGMEM
+
+extern struct pglist_data contig_page_data;
+#define NODE_DATA(nid)		(&contig_page_data)
+#define NODE_MEM_MAP(nid)	mem_map
+#define MAX_NODES_SHIFT		1
+#define pfn_to_nid(pfn)		(0)
+
+#else /* CONFIG_DISCONTIGMEM */
+
+#include <asm/mmzone.h>
+
+#if BITS_PER_LONG == 32 || defined(ARCH_HAS_ATOMIC_UNSIGNED)
+/*
+ * with 32 bit page->flags field, we reserve 8 bits for node/zone info.
+ * there are 3 zones (2 bits) and this leaves 8-2=6 bits for nodes.
+ */
+#define MAX_NODES_SHIFT		6
+#elif BITS_PER_LONG == 64
+/*
+ * with 64 bit flags field, there's plenty of room.
+ */
+#define MAX_NODES_SHIFT		10
+#endif
+
+#endif /* !CONFIG_DISCONTIGMEM */
+
+#if NODES_SHIFT > MAX_NODES_SHIFT
+#error NODES_SHIFT > MAX_NODES_SHIFT
+#endif
+
+/* There are currently 3 zones: DMA, Normal & Highmem, thus we need 2 bits */
+#define MAX_ZONES_SHIFT		2
+
+#if ZONES_SHIFT > MAX_ZONES_SHIFT
+#error ZONES_SHIFT > MAX_ZONES_SHIFT
+#endif
+
+#endif /* !__ASSEMBLY__ */
+#endif /* __KERNEL__ */
+#endif /* _LINUX_MMZONE_H */
