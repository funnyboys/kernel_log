commit 4505153954fdb1465d2b178288a9bf646f2a2166
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 16:57:47 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 333
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation inc 59 temple place suite 330 boston ma 02111
      1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 136 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190530000436.384967451@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 85abc2915e8d..2e9c7215882b 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 #ifndef LLIST_H
 #define LLIST_H
 /*
@@ -45,19 +46,6 @@
  *
  * Copyright 2010,2011 Intel Corp.
  *   Author: Huang Ying <ying.huang@intel.com>
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License version
- * 2 as published by the Free Software Foundation;
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
 
 #include <linux/atomic.h>

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 1957635e6d5f..85abc2915e8d 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -198,7 +198,7 @@ static inline void init_llist_head(struct llist_head *list)
  */
 static inline bool llist_empty(const struct llist_head *head)
 {
-	return ACCESS_ONCE(head->first) == NULL;
+	return READ_ONCE(head->first) == NULL;
 }
 
 static inline struct llist_node *llist_next(struct llist_node *node)

commit beaec533fc2701a28a4d667f67c9f59c6e4e0d13
Author: Alexander Potapenko <glider@google.com>
Date:   Wed Jul 19 20:27:30 2017 +0200

    llist: clang: introduce member_address_is_nonnull()
    
    Currently llist_for_each_entry() and llist_for_each_entry_safe() iterate
    until &pos->member != NULL.  But when building the kernel with Clang,
    the compiler assumes &pos->member cannot be NULL if the member's offset
    is greater than 0 (which would be equivalent to the object being
    non-contiguous in memory).  Therefore the loop condition is always true,
    and the loops become infinite.
    
    To work around this, introduce the member_address_is_nonnull() macro,
    which casts object pointer to uintptr_t, thus letting the member pointer
    to be NULL.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Tested-by: Sodagudi Prasad <psodagud@codeaurora.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index d11738110a7a..1957635e6d5f 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -92,6 +92,23 @@ static inline void init_llist_head(struct llist_head *list)
 #define llist_entry(ptr, type, member)		\
 	container_of(ptr, type, member)
 
+/**
+ * member_address_is_nonnull - check whether the member address is not NULL
+ * @ptr:	the object pointer (struct type * that contains the llist_node)
+ * @member:	the name of the llist_node within the struct.
+ *
+ * This macro is conceptually the same as
+ *	&ptr->member != NULL
+ * but it works around the fact that compilers can decide that taking a member
+ * address is never a NULL pointer.
+ *
+ * Real objects that start at a high address and have a member at NULL are
+ * unlikely to exist, but such pointers may be returned e.g. by the
+ * container_of() macro.
+ */
+#define member_address_is_nonnull(ptr, member)	\
+	((uintptr_t)(ptr) + offsetof(typeof(*(ptr)), member) != 0)
+
 /**
  * llist_for_each - iterate over some deleted entries of a lock-less list
  * @pos:	the &struct llist_node to use as a loop cursor
@@ -145,7 +162,7 @@ static inline void init_llist_head(struct llist_head *list)
  */
 #define llist_for_each_entry(pos, node, member)				\
 	for ((pos) = llist_entry((node), typeof(*(pos)), member);	\
-	     &(pos)->member != NULL;					\
+	     member_address_is_nonnull(pos, member);			\
 	     (pos) = llist_entry((pos)->member.next, typeof(*(pos)), member))
 
 /**
@@ -167,7 +184,7 @@ static inline void init_llist_head(struct llist_head *list)
  */
 #define llist_for_each_entry_safe(pos, n, node, member)			       \
 	for (pos = llist_entry((node), typeof(*pos), member);		       \
-	     &pos->member != NULL &&					       \
+	     member_address_is_nonnull(pos, member) &&			       \
 	        (n = llist_entry(pos->member.next, typeof(*n), member), true); \
 	     pos = n)
 

commit d714893e61cd8c6e5c7e095f7dd615aa434bca95
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Fri May 12 09:36:56 2017 +0900

    llist: Provide a safe version for llist_for_each()
    
    Sometimes we have to dereference next field of llist node before entering
    loop becasue the node might be deleted or the next field might be
    modified within the loop. So this adds the safe version of llist_for_each(),
    that is, llist_for_each_safe().
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Huang, Ying <ying.huang@intel.com>
    Cc: <kernel-team@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1494549416-10539-1-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 171baa90f6f6..d11738110a7a 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -109,6 +109,25 @@ static inline void init_llist_head(struct llist_head *list)
 #define llist_for_each(pos, node)			\
 	for ((pos) = (node); pos; (pos) = (pos)->next)
 
+/**
+ * llist_for_each_safe - iterate over some deleted entries of a lock-less list
+ *			 safe against removal of list entry
+ * @pos:	the &struct llist_node to use as a loop cursor
+ * @n:		another &struct llist_node to use as temporary storage
+ * @node:	the first entry of deleted list entries
+ *
+ * In general, some entries of the lock-less list can be traversed
+ * safely only after being deleted from list, so start with an entry
+ * instead of list head.
+ *
+ * If being used on entries deleted from lock-less list directly, the
+ * traverse order is from the newest to the oldest added entry.  If
+ * you want to traverse from the oldest to the newest, you must
+ * reverse the order by yourself before traversing.
+ */
+#define llist_for_each_safe(pos, n, node)			\
+	for ((pos) = (node); (pos) && ((n) = (pos)->next, true); (pos) = (n))
+
 /**
  * llist_for_each_entry - iterate over some deleted entries of lock-less list of given type
  * @pos:	the type * to use as a loop cursor.

commit d78973c32a210b0057b51880f7119bf2b61d5a65
Author: Joel Fernandes <joelaf@google.com>
Date:   Mon Dec 12 18:01:45 2016 -0800

    llist: Clarify comments about when locking is needed
    
    llist.h comments are confusing about when locking is needed versus when it
    isn't. Clarify these comments by being more descriptive about why locking is
    needed for llist_del_first.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index fd4ca0b4fe0f..171baa90f6f6 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -3,28 +3,33 @@
 /*
  * Lock-less NULL terminated single linked list
  *
- * If there are multiple producers and multiple consumers, llist_add
- * can be used in producers and llist_del_all can be used in
- * consumers.  They can work simultaneously without lock.  But
- * llist_del_first can not be used here.  Because llist_del_first
- * depends on list->first->next does not changed if list->first is not
- * changed during its operation, but llist_del_first, llist_add,
- * llist_add (or llist_del_all, llist_add, llist_add) sequence in
- * another consumer may violate that.
- *
- * If there are multiple producers and one consumer, llist_add can be
- * used in producers and llist_del_all or llist_del_first can be used
- * in the consumer.
- *
- * This can be summarized as follow:
+ * Cases where locking is not needed:
+ * If there are multiple producers and multiple consumers, llist_add can be
+ * used in producers and llist_del_all can be used in consumers simultaneously
+ * without locking. Also a single consumer can use llist_del_first while
+ * multiple producers simultaneously use llist_add, without any locking.
+ *
+ * Cases where locking is needed:
+ * If we have multiple consumers with llist_del_first used in one consumer, and
+ * llist_del_first or llist_del_all used in other consumers, then a lock is
+ * needed.  This is because llist_del_first depends on list->first->next not
+ * changing, but without lock protection, there's no way to be sure about that
+ * if a preemption happens in the middle of the delete operation and on being
+ * preempted back, the list->first is the same as before causing the cmpxchg in
+ * llist_del_first to succeed. For example, while a llist_del_first operation
+ * is in progress in one consumer, then a llist_del_first, llist_add,
+ * llist_add (or llist_del_all, llist_add, llist_add) sequence in another
+ * consumer may cause violations.
+ *
+ * This can be summarized as follows:
  *
  *           |   add    | del_first |  del_all
  * add       |    -     |     -     |     -
  * del_first |          |     L     |     L
  * del_all   |          |           |     -
  *
- * Where "-" stands for no lock is needed, while "L" stands for lock
- * is needed.
+ * Where, a particular row's operation can happen concurrently with a column's
+ * operation, with "-" being no lock needed, while "L" being lock is needed.
  *
  * The list entries deleted via llist_del_all can be traversed with
  * traversing function such as llist_for_each etc.  But the list

commit cd074aea9261784e44f292e1132830ec221802c6
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 6 17:54:43 2015 +0100

    locking, include/llist: Use linux/atomic.h instead of asm/cmpxchg.h
    
    Including an asm/ header directly is best avoided, so use linux/atomic.h
    instead of asm/cmpxchg.h in linux/llist.h.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman.Long@hp.com
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1438880084-18856-8-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index fbf10a0bc095..fd4ca0b4fe0f 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -55,8 +55,8 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
 
+#include <linux/atomic.h>
 #include <linux/kernel.h>
-#include <asm/cmpxchg.h>
 
 struct llist_head {
 	struct llist_node *first;

commit b89241e8cdb8321c20546d47645a9b65b58113b5
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Nov 14 14:32:11 2013 -0800

    llists: move llist_reverse_order from raid5 to llist.c
    
    Make this useful helper available for other users.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Neil Brown <neilb@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 8828a78dec9a..fbf10a0bc095 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -195,4 +195,6 @@ static inline struct llist_node *llist_del_all(struct llist_head *head)
 
 extern struct llist_node *llist_del_first(struct llist_head *head);
 
+struct llist_node *llist_reverse_order(struct llist_node *head);
+
 #endif /* LLIST_H */

commit 809850b7a5fcc0a96d023e1171a7944c60fd5a71
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat Jun 15 09:36:06 2013 -0400

    tty: Use lockless flip buffer free list
    
    In preparation for lockless flip buffers, make the flip buffer
    free list lockless.
    
    NB: using llist is not the optimal solution, as the driver and
    buffer work may contend over the llist head unnecessarily. However,
    test measurements indicate this contention is low.
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index cdaa7f023899..8828a78dec9a 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -124,6 +124,29 @@ static inline void init_llist_head(struct llist_head *list)
 	     &(pos)->member != NULL;					\
 	     (pos) = llist_entry((pos)->member.next, typeof(*(pos)), member))
 
+/**
+ * llist_for_each_entry_safe - iterate over some deleted entries of lock-less list of given type
+ *			       safe against removal of list entry
+ * @pos:	the type * to use as a loop cursor.
+ * @n:		another type * to use as temporary storage
+ * @node:	the first entry of deleted list entries.
+ * @member:	the name of the llist_node with the struct.
+ *
+ * In general, some entries of the lock-less list can be traversed
+ * safely only after being removed from list, so start with an entry
+ * instead of list head.
+ *
+ * If being used on entries deleted from lock-less list directly, the
+ * traverse order is from the newest to the oldest added entry.  If
+ * you want to traverse from the oldest to the newest, you must
+ * reverse the order by yourself before traversing.
+ */
+#define llist_for_each_entry_safe(pos, n, node, member)			       \
+	for (pos = llist_entry((node), typeof(*pos), member);		       \
+	     &pos->member != NULL &&					       \
+	        (n = llist_entry(pos->member.next, typeof(*n), member), true); \
+	     pos = n)
+
 /**
  * llist_empty - tests whether a lock-less list is empty
  * @head:	the list to test

commit e9a17bd73a29e5323c37ec5ffe50fc0e825d3d03
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 8 14:24:19 2013 -0700

    llist: llist_add() can use llist_add_batch()
    
    llist_add(new, head) can simply use llist_add_batch(new, new, head),
    no need to duplicate the code.
    
    This obviously uninlines llist_add() and to me this is a win. But we
    can make llist_add_batch() inline if this is desirable, in this case
    gcc can notice that new_first == new_last if the caller is llist_add().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Vagin <avagin@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 3e2b969d68f6..cdaa7f023899 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -142,6 +142,9 @@ static inline struct llist_node *llist_next(struct llist_node *node)
 	return node->next;
 }
 
+extern bool llist_add_batch(struct llist_node *new_first,
+			    struct llist_node *new_last,
+			    struct llist_head *head);
 /**
  * llist_add - add a new entry
  * @new:	new entry to be added
@@ -151,13 +154,7 @@ static inline struct llist_node *llist_next(struct llist_node *node)
  */
 static inline bool llist_add(struct llist_node *new, struct llist_head *head)
 {
-	struct llist_node *first;
-
-	do {
-		new->next = first = ACCESS_ONCE(head->first);
-	} while (cmpxchg(&head->first, first, new) != first);
-
-	return !first;
+	return llist_add_batch(new, new, head);
 }
 
 /**
@@ -173,9 +170,6 @@ static inline struct llist_node *llist_del_all(struct llist_head *head)
 	return xchg(&head->first, NULL);
 }
 
-extern bool llist_add_batch(struct llist_node *new_first,
-			    struct llist_node *new_last,
-			    struct llist_head *head);
 extern struct llist_node *llist_del_first(struct llist_head *head);
 
 #endif /* LLIST_H */

commit fb4214db50b00558cc6e274c88b3f7325068e942
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 8 14:24:18 2013 -0700

    llist: fix/simplify llist_add() and llist_add_batch()
    
    1. This is mostly theoretical, but llist_add*() need ACCESS_ONCE().
    
       Otherwise it is not guaranteed that the first cmpxchg() uses the
       same value for old_entry and new_last->next.
    
    2. These helpers cache the result of cmpxchg() and read the initial
       value of head->first before the main loop. I do not think this
       makes sense. In the likely case cmpxchg() succeeds, otherwise
       it doesn't hurt to reload head->first.
    
       I think it would be better to simplify the code and simply read
       ->first before cmpxchg().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Vagin <avagin@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index a5199f6d0e82..3e2b969d68f6 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -151,18 +151,13 @@ static inline struct llist_node *llist_next(struct llist_node *node)
  */
 static inline bool llist_add(struct llist_node *new, struct llist_head *head)
 {
-	struct llist_node *entry, *old_entry;
-
-	entry = head->first;
-	for (;;) {
-		old_entry = entry;
-		new->next = entry;
-		entry = cmpxchg(&head->first, old_entry, new);
-		if (entry == old_entry)
-			break;
-	}
-
-	return old_entry == NULL;
+	struct llist_node *first;
+
+	do {
+		new->next = first = ACCESS_ONCE(head->first);
+	} while (cmpxchg(&head->first, first, new) != first);
+
+	return !first;
 }
 
 /**

commit 96f951edb1f1bdbbc99b0cd458f9808bb83d58ae
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Add #includes needed to permit the removal of asm/system.h
    
    asm/system.h is a cause of circular dependency problems because it contains
    commonly used primitive stuff like barrier definitions and uncommonly used
    stuff like switch_to() that might require MMU definitions.
    
    asm/system.h has been disintegrated by this point on all arches into the
    following common segments:
    
     (1) asm/barrier.h
    
         Moved memory barrier definitions here.
    
     (2) asm/cmpxchg.h
    
         Moved xchg() and cmpxchg() here.  #included in asm/atomic.h.
    
     (3) asm/bug.h
    
         Moved die() and similar here.
    
     (4) asm/exec.h
    
         Moved arch_align_stack() here.
    
     (5) asm/elf.h
    
         Moved AT_VECTOR_SIZE_ARCH here.
    
     (6) asm/switch_to.h
    
         Moved switch_to() here.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 801b44b07aac..a5199f6d0e82 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -56,8 +56,7 @@
  */
 
 #include <linux/kernel.h>
-#include <asm/system.h>
-#include <asm/processor.h>
+#include <asm/cmpxchg.h>
 
 struct llist_head {
 	struct llist_node *first;

commit fc23af34b00ef444eec088f744983b9ca6c7f5d1
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Oct 31 17:13:08 2011 -0700

    llist-return-whether-list-is-empty-before-adding-in-llist_add-fix
    
    clarify comment
    
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 7287734e08d1..801b44b07aac 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -148,7 +148,7 @@ static inline struct llist_node *llist_next(struct llist_node *node)
  * @new:	new entry to be added
  * @head:	the head for your lock-less list
  *
- * Return whether list is empty before adding.
+ * Returns true if the list was empty prior to adding this entry.
  */
 static inline bool llist_add(struct llist_node *new, struct llist_head *head)
 {

commit 540f41edc15473ca3b2876de72646546ae101374
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Oct 5 17:25:28 2011 +1100

    llist: Add back llist_add_batch() and llist_del_first() prototypes
    
    Commit 1230db8e1543 ("llist: Make some llist functions inline")
    has deleted the definitions, causing problems for (not upstream yet)
    code that tries to make use of them.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/20111005172528.0d0a8afc65acef7ace22a24e@canb.auug.org.au
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 837fb4ae66fb..7287734e08d1 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -178,4 +178,10 @@ static inline struct llist_node *llist_del_all(struct llist_head *head)
 {
 	return xchg(&head->first, NULL);
 }
+
+extern bool llist_add_batch(struct llist_node *new_first,
+			    struct llist_node *new_last,
+			    struct llist_head *head);
+extern struct llist_node *llist_del_first(struct llist_head *head);
+
 #endif /* LLIST_H */

commit f0f1d32f931b705c4ee5dd374074d34edf3eae14
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 12 15:50:49 2011 +0200

    llist: Remove cpu_relax() usage in cmpxchg loops
    
    Initial benchmarks show they're a net loss:
    
     $ for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ; do echo performance > $i; done
     $ echo 4096 32000 64 128 > /proc/sys/kernel/sem
     $ ./sembench -t 2048 -w 1900 -o 0
    
    Pre:
    
     run time 30 seconds 778936 worker burns per second
     run time 30 seconds 912190 worker burns per second
     run time 30 seconds 817506 worker burns per second
     run time 30 seconds 830870 worker burns per second
     run time 30 seconds 845056 worker burns per second
    
    Post:
    
     run time 30 seconds 905920 worker burns per second
     run time 30 seconds 849046 worker burns per second
     run time 30 seconds 886286 worker burns per second
     run time 30 seconds 822320 worker burns per second
     run time 30 seconds 900283 worker burns per second
    
    So about 4% faster. (!)
    
    cpu_relax() stalls the pipeline, therefore, when used in a tight loop
    it has the following benefits:
    
     - allows SMT siblings to have a go;
     - reduces pressure on the CPU interconnect.
    
    However, cmpxchg loops are unfair and thus have unbounded completion
    time, therefore we should avoid getting in such heavily contended
    situations where the above benefits make any difference.
    
    A typical cmpxchg loop should not go round more than a handfull of
    times at worst, therefore adding extra delays just slows things down.
    
    Since the llist primitives are new, there aren't any bad users yet,
    and we should avoid growing them. Heavily contended sites should
    generally be better off using the ticket locks for serialization since
    they provide bounded completion times (fifo-fair over the cpus).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1315836358.26517.43.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index e2e96d04ee48..837fb4ae66fb 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -161,7 +161,6 @@ static inline bool llist_add(struct llist_node *new, struct llist_head *head)
 		entry = cmpxchg(&head->first, old_entry, new);
 		if (entry == old_entry)
 			break;
-		cpu_relax();
 	}
 
 	return old_entry == NULL;

commit 924f8f5af31423529cc3940cb2ae9fee736b7517
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 12 13:12:28 2011 +0200

    llist: Add llist_next()
    
    So we don't have to expose the struct list_node member.
    
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315836348.26517.41.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 27bbdf5ddf82..e2e96d04ee48 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -138,6 +138,11 @@ static inline bool llist_empty(const struct llist_head *head)
 	return ACCESS_ONCE(head->first) == NULL;
 }
 
+static inline struct llist_node *llist_next(struct llist_node *node)
+{
+	return node->next;
+}
+
 /**
  * llist_add - add a new entry
  * @new:	new entry to be added

commit 781f7fd916fc77a862e20063ed3aeedf173234f9
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:45 2011 +0800

    llist: Return whether list is empty before adding in llist_add()
    
    Extend the llist_add*() functions to return a success indicator, this
    allows us in the scheduler code to send an IPI if the queue was empty.
    
    ( There's no effect on existing users, because the list_add_xxx() functions
      are inline, thus this will be optimized out by the compiler if not used
      by callers. )
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-5-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index ca91875286bf..27bbdf5ddf82 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -142,8 +142,10 @@ static inline bool llist_empty(const struct llist_head *head)
  * llist_add - add a new entry
  * @new:	new entry to be added
  * @head:	the head for your lock-less list
+ *
+ * Return whether list is empty before adding.
  */
-static inline void llist_add(struct llist_node *new, struct llist_head *head)
+static inline bool llist_add(struct llist_node *new, struct llist_head *head)
 {
 	struct llist_node *entry, *old_entry;
 
@@ -156,6 +158,8 @@ static inline void llist_add(struct llist_node *new, struct llist_head *head)
 			break;
 		cpu_relax();
 	}
+
+	return old_entry == NULL;
 }
 
 /**

commit a3127336b71f6833d1483c856dce91fe558dc3a9
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:44 2011 +0800

    llist: Move cpu_relax() to after the cmpxchg()
    
    If in llist_add()/etc. functions the first cmpxchg() call succeeds, it is
    not necessary to use cpu_relax() before the cmpxchg(). So cpu_relax() in
    a busy loop involving cmpxchg() should go after cmpxchg() instead of before
    that.
    
    This patch fixes this for all involved llist functions.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-4-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 65fca1cbf514..ca91875286bf 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -148,11 +148,14 @@ static inline void llist_add(struct llist_node *new, struct llist_head *head)
 	struct llist_node *entry, *old_entry;
 
 	entry = head->first;
-	do {
+	for (;;) {
 		old_entry = entry;
 		new->next = entry;
+		entry = cmpxchg(&head->first, old_entry, new);
+		if (entry == old_entry)
+			break;
 		cpu_relax();
-	} while ((entry = cmpxchg(&head->first, old_entry, new)) != old_entry);
+	}
 }
 
 /**

commit 2c30245c65e8ebc3080b75ce65572ab8140bad0b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Oct 4 12:43:11 2011 +0200

    llist: Remove the platform-dependent NMI checks
    
    Remove the nmi() checks spread around the code. in_nmi() is not available
    on every architecture and it's a pretty obscure and ugly check in any case.
    
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-3-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index 3eccdfd66096..65fca1cbf514 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -35,8 +35,8 @@
  *
  * The basic atomic operation of this list is cmpxchg on long.  On
  * architectures that don't have NMI-safe cmpxchg implementation, the
- * list can NOT be used in NMI handler.  So code uses the list in NMI
- * handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
+ * list can NOT be used in NMI handlers.  So code that uses the list in
+ * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
  *
  * Copyright 2010,2011 Intel Corp.
  *   Author: Huang Ying <ying.huang@intel.com>
@@ -147,10 +147,6 @@ static inline void llist_add(struct llist_node *new, struct llist_head *head)
 {
 	struct llist_node *entry, *old_entry;
 
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
 	entry = head->first;
 	do {
 		old_entry = entry;
@@ -169,10 +165,6 @@ static inline void llist_add(struct llist_node *new, struct llist_head *head)
  */
 static inline struct llist_node *llist_del_all(struct llist_head *head)
 {
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
 	return xchg(&head->first, NULL);
 }
 #endif /* LLIST_H */

commit 1230db8e1543c0471dd165727d34647ab098cc1e
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:42 2011 +0800

    llist: Make some llist functions inline
    
    Because llist code will be used in performance critical scheduler
    code path, make llist_add() and llist_del_all() inline to avoid
    function calling overhead and related 'glue' overhead.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-2-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/llist.h b/include/linux/llist.h
index aa0c8b5b3cd0..3eccdfd66096 100644
--- a/include/linux/llist.h
+++ b/include/linux/llist.h
@@ -37,8 +37,28 @@
  * architectures that don't have NMI-safe cmpxchg implementation, the
  * list can NOT be used in NMI handler.  So code uses the list in NMI
  * handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
+ *
+ * Copyright 2010,2011 Intel Corp.
+ *   Author: Huang Ying <ying.huang@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License version
+ * 2 as published by the Free Software Foundation;
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
 
+#include <linux/kernel.h>
+#include <asm/system.h>
+#include <asm/processor.h>
+
 struct llist_head {
 	struct llist_node *first;
 };
@@ -113,14 +133,46 @@ static inline void init_llist_head(struct llist_head *list)
  * test whether the list is empty without deleting something from the
  * list.
  */
-static inline int llist_empty(const struct llist_head *head)
+static inline bool llist_empty(const struct llist_head *head)
 {
 	return ACCESS_ONCE(head->first) == NULL;
 }
 
-void llist_add(struct llist_node *new, struct llist_head *head);
-void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
-		     struct llist_head *head);
-struct llist_node *llist_del_first(struct llist_head *head);
-struct llist_node *llist_del_all(struct llist_head *head);
+/**
+ * llist_add - add a new entry
+ * @new:	new entry to be added
+ * @head:	the head for your lock-less list
+ */
+static inline void llist_add(struct llist_node *new, struct llist_head *head)
+{
+	struct llist_node *entry, *old_entry;
+
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	entry = head->first;
+	do {
+		old_entry = entry;
+		new->next = entry;
+		cpu_relax();
+	} while ((entry = cmpxchg(&head->first, old_entry, new)) != old_entry);
+}
+
+/**
+ * llist_del_all - delete all entries from lock-less list
+ * @head:	the head of lock-less list to delete all entries
+ *
+ * If list is empty, return NULL, otherwise, delete all entries and
+ * return the pointer to the first entry.  The order of entries
+ * deleted is from the newest to the oldest added one.
+ */
+static inline struct llist_node *llist_del_all(struct llist_head *head)
+{
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	return xchg(&head->first, NULL);
+}
 #endif /* LLIST_H */

commit f49f23abf3dd786ddcac1c1e7db3c2013b07413f
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Jul 13 13:14:23 2011 +0800

    lib, Add lock-less NULL terminated single list
    
    Cmpxchg is used to implement adding new entry to the list, deleting
    all entries from the list, deleting first entry of the list and some
    other operations.
    
    Because this is a single list, so the tail can not be accessed in O(1).
    
    If there are multiple producers and multiple consumers, llist_add can
    be used in producers and llist_del_all can be used in consumers.  They
    can work simultaneously without lock.  But llist_del_first can not be
    used here.  Because llist_del_first depends on list->first->next does
    not changed if list->first is not changed during its operation, but
    llist_del_first, llist_add, llist_add (or llist_del_all, llist_add,
    llist_add) sequence in another consumer may violate that.
    
    If there are multiple producers and one consumer, llist_add can be
    used in producers and llist_del_all or llist_del_first can be used in
    the consumer.
    
    This can be summarized as follow:
    
               |   add    | del_first |  del_all
     add       |    -     |     -     |     -
     del_first |          |     L     |     L
     del_all   |          |           |     -
    
    Where "-" stands for no lock is needed, while "L" stands for lock is
    needed.
    
    The list entries deleted via llist_del_all can be traversed with
    traversing function such as llist_for_each etc.  But the list entries
    can not be traversed safely before deleted from the list.  The order
    of deleted entries is from the newest to the oldest added one.  If you
    want to traverse from the oldest to the newest, you must reverse the
    order by yourself before traversing.
    
    The basic atomic operation of this list is cmpxchg on long.  On
    architectures that don't have NMI-safe cmpxchg implementation, the
    list can NOT be used in NMI handler.  So code uses the list in NMI
    handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/include/linux/llist.h b/include/linux/llist.h
new file mode 100644
index 000000000000..aa0c8b5b3cd0
--- /dev/null
+++ b/include/linux/llist.h
@@ -0,0 +1,126 @@
+#ifndef LLIST_H
+#define LLIST_H
+/*
+ * Lock-less NULL terminated single linked list
+ *
+ * If there are multiple producers and multiple consumers, llist_add
+ * can be used in producers and llist_del_all can be used in
+ * consumers.  They can work simultaneously without lock.  But
+ * llist_del_first can not be used here.  Because llist_del_first
+ * depends on list->first->next does not changed if list->first is not
+ * changed during its operation, but llist_del_first, llist_add,
+ * llist_add (or llist_del_all, llist_add, llist_add) sequence in
+ * another consumer may violate that.
+ *
+ * If there are multiple producers and one consumer, llist_add can be
+ * used in producers and llist_del_all or llist_del_first can be used
+ * in the consumer.
+ *
+ * This can be summarized as follow:
+ *
+ *           |   add    | del_first |  del_all
+ * add       |    -     |     -     |     -
+ * del_first |          |     L     |     L
+ * del_all   |          |           |     -
+ *
+ * Where "-" stands for no lock is needed, while "L" stands for lock
+ * is needed.
+ *
+ * The list entries deleted via llist_del_all can be traversed with
+ * traversing function such as llist_for_each etc.  But the list
+ * entries can not be traversed safely before deleted from the list.
+ * The order of deleted entries is from the newest to the oldest added
+ * one.  If you want to traverse from the oldest to the newest, you
+ * must reverse the order by yourself before traversing.
+ *
+ * The basic atomic operation of this list is cmpxchg on long.  On
+ * architectures that don't have NMI-safe cmpxchg implementation, the
+ * list can NOT be used in NMI handler.  So code uses the list in NMI
+ * handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
+ */
+
+struct llist_head {
+	struct llist_node *first;
+};
+
+struct llist_node {
+	struct llist_node *next;
+};
+
+#define LLIST_HEAD_INIT(name)	{ NULL }
+#define LLIST_HEAD(name)	struct llist_head name = LLIST_HEAD_INIT(name)
+
+/**
+ * init_llist_head - initialize lock-less list head
+ * @head:	the head for your lock-less list
+ */
+static inline void init_llist_head(struct llist_head *list)
+{
+	list->first = NULL;
+}
+
+/**
+ * llist_entry - get the struct of this entry
+ * @ptr:	the &struct llist_node pointer.
+ * @type:	the type of the struct this is embedded in.
+ * @member:	the name of the llist_node within the struct.
+ */
+#define llist_entry(ptr, type, member)		\
+	container_of(ptr, type, member)
+
+/**
+ * llist_for_each - iterate over some deleted entries of a lock-less list
+ * @pos:	the &struct llist_node to use as a loop cursor
+ * @node:	the first entry of deleted list entries
+ *
+ * In general, some entries of the lock-less list can be traversed
+ * safely only after being deleted from list, so start with an entry
+ * instead of list head.
+ *
+ * If being used on entries deleted from lock-less list directly, the
+ * traverse order is from the newest to the oldest added entry.  If
+ * you want to traverse from the oldest to the newest, you must
+ * reverse the order by yourself before traversing.
+ */
+#define llist_for_each(pos, node)			\
+	for ((pos) = (node); pos; (pos) = (pos)->next)
+
+/**
+ * llist_for_each_entry - iterate over some deleted entries of lock-less list of given type
+ * @pos:	the type * to use as a loop cursor.
+ * @node:	the fist entry of deleted list entries.
+ * @member:	the name of the llist_node with the struct.
+ *
+ * In general, some entries of the lock-less list can be traversed
+ * safely only after being removed from list, so start with an entry
+ * instead of list head.
+ *
+ * If being used on entries deleted from lock-less list directly, the
+ * traverse order is from the newest to the oldest added entry.  If
+ * you want to traverse from the oldest to the newest, you must
+ * reverse the order by yourself before traversing.
+ */
+#define llist_for_each_entry(pos, node, member)				\
+	for ((pos) = llist_entry((node), typeof(*(pos)), member);	\
+	     &(pos)->member != NULL;					\
+	     (pos) = llist_entry((pos)->member.next, typeof(*(pos)), member))
+
+/**
+ * llist_empty - tests whether a lock-less list is empty
+ * @head:	the list to test
+ *
+ * Not guaranteed to be accurate or up to date.  Just a quick way to
+ * test whether the list is empty without deleting something from the
+ * list.
+ */
+static inline int llist_empty(const struct llist_head *head)
+{
+	return ACCESS_ONCE(head->first) == NULL;
+}
+
+void llist_add(struct llist_node *new, struct llist_head *head);
+void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
+		     struct llist_head *head);
+struct llist_node *llist_del_first(struct llist_head *head);
+struct llist_node *llist_del_all(struct llist_head *head);
+#endif /* LLIST_H */
