commit dbfb089d360b1cc623c51a2c7cf9b99eff78e0e7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 3 12:40:33 2020 +0200

    sched: Fix loadavg accounting race
    
    The recent commit:
    
      c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
    
    moved these lines in ttwu():
    
            p->sched_contributes_to_load = !!task_contributes_to_load(p);
            p->state = TASK_WAKING;
    
    up before:
    
            smp_cond_load_acquire(&p->on_cpu, !VAL);
    
    into the 'p->on_rq == 0' block, with the thinking that once we hit
    schedule() the current task cannot change it's ->state anymore. And
    while this is true, it is both incorrect and flawed.
    
    It is incorrect in that we need at least an ACQUIRE on 'p->on_rq == 0'
    to avoid weak hardware from re-ordering things for us. This can fairly
    easily be achieved by relying on the control-dependency already in
    place.
    
    The second problem, which makes the flaw in the original argument, is
    that while schedule() will not change prev->state, it will read it a
    number of times (arguably too many times since it's marked volatile).
    The previous condition 'p->on_cpu == 0' was sufficient because that
    indicates schedule() has completed, and will no longer read
    prev->state. So now the trick is to make this same true for the (much)
    earlier 'prev->on_rq == 0' case.
    
    Furthermore, in order to make the ordering stick, the 'prev->on_rq = 0'
    assignment needs to he a RELEASE, but adding additional ordering to
    schedule() is an unwelcome proposition at the best of times, doubly so
    for mere accounting.
    
    Luckily we can push the prev->state load up before rq->lock, with the
    only caveat that we then have to re-read the state after. However, we
    know that if it changed, we no longer have to worry about the blocking
    path. This gives us the required ordering, if we block, we did the
    prev->state load before an (effective) smp_mb() and the p->on_rq store
    needs not change.
    
    With this we end up with the effective ordering:
    
            LOAD p->state           LOAD-ACQUIRE p->on_rq == 0
            MB
            STORE p->on_rq, 0       STORE p->state, TASK_WAKING
    
    which ensures the TASK_WAKING store happens after the prev->state
    load, and all is well again.
    
    Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
    Reported-by: Dave Jones <davej@codemonkey.org.uk>
    Reported-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Dave Jones <davej@codemonkey.org.uk>
    Tested-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Link: https://lkml.kernel.org/r/20200707102957.GN117543@hirez.programming.kicks-ass.net

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 692e327d7455..683372943093 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -114,10 +114,6 @@ struct task_group;
 
 #define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 
-#define task_contributes_to_load(task)	((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
-					 (task->flags & PF_FROZEN) == 0 && \
-					 (task->state & TASK_NOLOAD) == 0)
-
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
 /*

commit 8c4890d1c3358fb8023d46e1e554c41d54f02878
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 22 12:01:25 2020 +0200

    smp, irq_work: Continue smp_call_function*() and irq_work*() integration
    
    Instead of relying on BUG_ON() to ensure the various data structures
    line up, use a bunch of horrible unions to make it all automatic.
    
    Much of the union magic is to ensure irq_work and smp_call_function do
    not (yet) see the members of their respective data structures change
    name.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20200622100825.844455025@infradead.org

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 224b5de568e7..692e327d7455 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -654,11 +654,8 @@ struct task_struct {
 	unsigned int			ptrace;
 
 #ifdef CONFIG_SMP
-	struct {
-		struct llist_node		wake_entry;
-		unsigned int			wake_entry_type;
-	};
 	int				on_cpu;
+	struct __call_single_node	wake_entry;
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* Current CPU: */
 	unsigned int			cpu;

commit 4f311afc2035f7b33d97e69ffaa2e6cd67fca16d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 10 12:14:09 2020 +0200

    sched/core: Fix CONFIG_GCC_PLUGIN_RANDSTRUCT build fail
    
    As a temporary build fix, the proper cleanup needs more work.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Reported-by: Eric Biggers <ebiggers@kernel.org>
    Suggested-by: Eric Biggers <ebiggers@kernel.org>
    Suggested-by: Kees Cook <keescook@chromium.org>
    Fixes: a148866489fb ("sched: Replace rq::wake_list")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b62e6aaf28f0..224b5de568e7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -654,8 +654,10 @@ struct task_struct {
 	unsigned int			ptrace;
 
 #ifdef CONFIG_SMP
-	struct llist_node		wake_entry;
-	unsigned int			wake_entry_type;
+	struct {
+		struct llist_node		wake_entry;
+		unsigned int			wake_entry_type;
+	};
 	int				on_cpu;
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* Current CPU: */

commit a9429089d3e822d45be01a9635f0685174508fd3
Merge: 076f14be7fc9 7ccddc4613db
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 13 10:21:00 2020 -0700

    Merge tag 'ras-core-2020-06-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 RAS updates from Thomas Gleixner:
     "RAS updates from Borislav Petkov:
    
       - Unmap a whole guest page if an MCE is encountered in it to avoid
         follow-on MCEs leading to the guest crashing, by Tony Luck.
    
         This change collided with the entry changes and the merge
         resolution would have been rather unpleasant. To avoid that the
         entry branch was merged in before applying this. The resulting code
         did not change over the rebase.
    
       - AMD MCE error thresholding machinery cleanup and hotplug
         sanitization, by Thomas Gleixner.
    
       - Change the MCE notifiers to denote whether they have handled the
         error and not break the chain early by returning NOTIFY_STOP, thus
         giving the opportunity for the later handlers in the chain to see
         it. By Tony Luck.
    
       - Add AMD family 0x17, models 0x60-6f support, by Alexander Monakov.
    
       - Last but not least, the usual round of fixes and improvements"
    
    * tag 'ras-core-2020-06-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/mce/dev-mcelog: Fix -Wstringop-truncation warning about strncpy()
      x86/{mce,mm}: Unmap the entire page if the whole page is affected and poisoned
      EDAC/amd64: Add AMD family 17h model 60h PCI IDs
      hwmon: (k10temp) Add AMD family 17h model 60h PCI match
      x86/amd_nb: Add AMD family 17h model 60h PCI IDs
      x86/mcelog: Add compat_ioctl for 32-bit mcelog support
      x86/mce: Drop bogus comment about mce.kflags
      x86/mce: Fixup exception only for the correct MCEs
      EDAC: Drop the EDAC report status checks
      x86/mce: Add mce=print_all option
      x86/mce: Change default MCE logger to check mce->kflags
      x86/mce: Fix all mce notifiers to update the mce->kflags bitmask
      x86/mce: Add a struct mce.kflags field
      x86/mce: Convert the CEC to use the MCE notifier
      x86/mce: Rename "first" function as "early"
      x86/mce/amd, edac: Remove report_gart_errors
      x86/mce/amd: Make threshold bank setting hotplug robust
      x86/mce/amd: Cleanup threshold device remove path
      x86/mce/amd: Straighten CPU hotplug path
      x86/mce/amd: Sanitize thresholding device creation hotplug path
      ...

commit 37d1a04b13a6d2fec91a6813fc034947a27db034
Merge: 37f8173dd849 97a9474aeb78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 11 20:02:46 2020 +0200

    Rebase locking/kcsan to locking/urgent
    
    Merge the state of the locking kcsan branch before the read/write_once()
    and the atomics modifications got merged.
    
    Squash the fallout of the rebase on top of the read/write once and atomic
    fallback work into the merge. The history of the original branch is
    preserved in tag locking-kcsan-2020-06-02.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 17fae1294ad9d711b2c3dd0edef479d40c76a5e8
Author: Tony Luck <tony.luck@intel.com>
Date:   Wed May 20 09:35:46 2020 -0700

    x86/{mce,mm}: Unmap the entire page if the whole page is affected and poisoned
    
    An interesting thing happened when a guest Linux instance took a machine
    check. The VMM unmapped the bad page from guest physical space and
    passed the machine check to the guest.
    
    Linux took all the normal actions to offline the page from the process
    that was using it. But then guest Linux crashed because it said there
    was a second machine check inside the kernel with this stack trace:
    
    do_memory_failure
        set_mce_nospec
             set_memory_uc
                  _set_memory_uc
                       change_page_attr_set_clr
                            cpa_flush
                                 clflush_cache_range_opt
    
    This was odd, because a CLFLUSH instruction shouldn't raise a machine
    check (it isn't consuming the data). Further investigation showed that
    the VMM had passed in another machine check because is appeared that the
    guest was accessing the bad page.
    
    Fix is to check the scope of the poison by checking the MCi_MISC register.
    If the entire page is affected, then unmap the page. If only part of the
    page is affected, then mark the page as uncacheable.
    
    This assumes that VMMs will do the logical thing and pass in the "whole
    page scope" via the MCi_MISC register (since they unmapped the entire
    page).
    
      [ bp: Adjust to x86/entry changes. ]
    
    Fixes: 284ce4011ba6 ("x86/memory_failure: Introduce {set, clear}_mce_nospec()")
    Reported-by: Jue Wang <juew@google.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jue Wang <juew@google.com>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200520163546.GA7977@agluck-desk2.amr.corp.intel.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c5d96e3e7fff..62c1de522fc5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1304,7 +1304,9 @@ struct task_struct {
 
 #ifdef CONFIG_X86_MCE
 	u64				mce_addr;
-	u64				mce_status;
+	__u64				mce_ripv : 1,
+					mce_whole_page : 1,
+					__mce_reserved : 62;
 	struct callback_head		mce_kill_me;
 #endif
 

commit 5ff3b30ab57da82d8db4f14662a2858cabfbc2c0
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Thu Jun 4 16:46:04 2020 -0700

    kcov: collect coverage from interrupts
    
    This change extends kcov remote coverage support to allow collecting
    coverage from soft interrupts in addition to kernel background threads.
    
    To collect coverage from code that is executed in softirq context, a part
    of that code has to be annotated with kcov_remote_start/stop() in a
    similar way as how it is done for global kernel background threads.  Then
    the handle used for the annotations has to be passed to the
    KCOV_REMOTE_ENABLE ioctl.
    
    Internally this patch adjusts the __sanitizer_cov_trace_pc() compiler
    inserted callback to not bail out when called from softirq context.
    kcov_remote_start/stop() are updated to save/restore the current per task
    kcov state in a per-cpu area (in case the softirq came when the kernel was
    already collecting coverage in task context).  Coverage from softirqs is
    collected into pre-allocated per-cpu areas, whose size is controlled by
    the new CONFIG_KCOV_IRQ_AREA_SIZE.
    
    [andreyknvl@google.com: turn current->kcov_softirq into unsigned int to fix objtool warning]
      Link: http://lkml.kernel.org/r/841c778aa3849c5cb8c3761f56b87ce653a88671.1585233617.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Konovalov <andreyknvl@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Marco Elver <elver@google.com>
    Link: http://lkml.kernel.org/r/469bd385c431d050bc38a593296eff4baae50666.1584655448.git.andreyknvl@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 57a5ce9f33c5..c5d96e3e7fff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1247,6 +1247,9 @@ struct task_struct {
 
 	/* KCOV sequence number: */
 	int				kcov_sequence;
+
+	/* Collect coverage from softirq context: */
+	unsigned int			kcov_softirq;
 #endif
 
 #ifdef CONFIG_MEMCG

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit d479c5a1919b4e569dcd3ae9c84ed74a675d0b94
Merge: f6aee505c71b 25de110d1486
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 13:06:42 2020 -0700

    Merge tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The changes in this cycle are:
    
       - Optimize the task wakeup CPU selection logic, to improve
         scalability and reduce wakeup latency spikes
    
       - PELT enhancements
    
       - CFS bandwidth handling fixes
    
       - Optimize the wakeup path by remove rq->wake_list and replacing it
         with ->ttwu_pending
    
       - Optimize IPI cross-calls by making flush_smp_call_function_queue()
         process sync callbacks first.
    
       - Misc fixes and enhancements"
    
    * tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      irq_work: Define irq_work_single() on !CONFIG_IRQ_WORK too
      sched/headers: Split out open-coded prototypes into kernel/sched/smp.h
      sched: Replace rq::wake_list
      sched: Add rq::ttwu_pending
      irq_work, smp: Allow irq_work on call_single_queue
      smp: Optimize send_call_function_single_ipi()
      smp: Move irq_work_run() out of flush_smp_call_function_queue()
      smp: Optimize flush_smp_call_function_queue()
      sched: Fix smp_call_function_single_async() usage for ILB
      sched/core: Offload wakee task activation if it the wakee is descheduling
      sched/core: Optimize ttwu() spinning on p->on_cpu
      sched: Defend cfs and rt bandwidth quota against overflow
      sched/cpuacct: Fix charge cpuacct.usage_sys
      sched/fair: Replace zero-length array with flexible-array
      sched/pelt: Sync util/runnable_sum with PELT window when propagating
      sched/cpuacct: Use __this_cpu_add() instead of this_cpu_ptr()
      sched/fair: Optimize enqueue_task_fair()
      sched: Make scheduler_ipi inline
      sched: Clean up scheduler_ipi()
      sched/core: Simplify sched_init()
      ...

commit 94709049fb8442fb2f7b91fbec3c2897a75e18df
Merge: 17839856fd58 4fba37586e4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 12:21:36 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
     "A few little subsystems and a start of a lot of MM patches.
    
      Subsystems affected by this patch series: squashfs, ocfs2, parisc,
      vfs. With mm subsystems: slab-generic, slub, debug, pagecache, gup,
      swap, memcg, pagemap, memory-failure, vmalloc, kasan"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (128 commits)
      kasan: move kasan_report() into report.c
      mm/mm_init.c: report kasan-tag information stored in page->flags
      ubsan: entirely disable alignment checks under UBSAN_TRAP
      kasan: fix clang compilation warning due to stack protector
      x86/mm: remove vmalloc faulting
      mm: remove vmalloc_sync_(un)mappings()
      x86/mm/32: implement arch_sync_kernel_mappings()
      x86/mm/64: implement arch_sync_kernel_mappings()
      mm/ioremap: track which page-table levels were modified
      mm/vmalloc: track which page-table levels were modified
      mm: add functions to track page directory modifications
      s390: use __vmalloc_node in stack_alloc
      powerpc: use __vmalloc_node in alloc_vm_stack
      arm64: use __vmalloc_node in arch_alloc_vmap_stack
      mm: remove vmalloc_user_node_flags
      mm: switch the test_vmalloc module to use __vmalloc_node
      mm: remove __vmalloc_node_flags_caller
      mm: remove both instances of __vmalloc_node_flags
      mm: remove the prot argument to __vmalloc_node
      mm: remove the pgprot argument to __vmalloc
      ...

commit a37b0715ddf3007734c4e2424c14bc7efcdd1190
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jun 1 21:48:18 2020 -0700

    mm/writeback: replace PF_LESS_THROTTLE with PF_LOCAL_THROTTLE
    
    PF_LESS_THROTTLE exists for loop-back nfsd (and a similar need in the
    loop block driver and callers of prctl(PR_SET_IO_FLUSHER)), where a
    daemon needs to write to one bdi (the final bdi) in order to free up
    writes queued to another bdi (the client bdi).
    
    The daemon sets PF_LESS_THROTTLE and gets a larger allowance of dirty
    pages, so that it can still dirty pages after other processses have been
    throttled.  The purpose of this is to avoid deadlock that happen when
    the PF_LESS_THROTTLE process must write for any dirty pages to be freed,
    but it is being thottled and cannot write.
    
    This approach was designed when all threads were blocked equally,
    independently on which device they were writing to, or how fast it was.
    Since that time the writeback algorithm has changed substantially with
    different threads getting different allowances based on non-trivial
    heuristics.  This means the simple "add 25%" heuristic is no longer
    reliable.
    
    The important issue is not that the daemon needs a *larger* dirty page
    allowance, but that it needs a *private* dirty page allowance, so that
    dirty pages for the "client" bdi that it is helping to clear (the bdi
    for an NFS filesystem or loop block device etc) do not affect the
    throttling of the daemon writing to the "final" bdi.
    
    This patch changes the heuristic so that the task is not throttled when
    the bdi it is writing to has a dirty page count below below (or equal
    to) the free-run threshold for that bdi.  This ensures it will always be
    able to have some pages in flight, and so will not deadlock.
    
    In a steady-state, it is expected that PF_LOCAL_THROTTLE tasks might
    still be throttled by global threshold, but that is acceptable as it is
    only the deadlock state that is interesting for this flag.
    
    This approach of "only throttle when target bdi is busy" is consistent
    with the other use of PF_LESS_THROTTLE in current_may_throttle(), were
    it causes attention to be focussed only on the target bdi.
    
    So this patch
     - renames PF_LESS_THROTTLE to PF_LOCAL_THROTTLE,
     - removes the 25% bonus that that flag gives, and
     - If PF_LOCAL_THROTTLE is set, don't delay at all unless the
       global and the local free-run thresholds are exceeded.
    
    Note that previously realtime threads were treated the same as
    PF_LESS_THROTTLE threads.  This patch does *not* change the behvaiour
    for real-time threads, so it is now different from the behaviour of nfsd
    and loop tasks.  I don't know what is wanted for realtime.
    
    [akpm@linux-foundation.org: coding style fixes]
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Acked-by: Chuck Lever <chuck.lever@oracle.com>  [nfsd]
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Trond Myklebust <trond.myklebust@hammerspace.com>
    Link: http://lkml.kernel.org/r/87ftbf7gs3.fsf@notabene.neil.brown.name
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4418f5cb8324..12ef0c753284 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1481,7 +1481,8 @@ extern struct pid *cad_pid;
 #define PF_KSWAPD		0x00020000	/* I am kswapd */
 #define PF_MEMALLOC_NOFS	0x00040000	/* All allocation requests will inherit GFP_NOFS */
 #define PF_MEMALLOC_NOIO	0x00080000	/* All allocation requests will inherit GFP_NOIO */
-#define PF_LESS_THROTTLE	0x00100000	/* Throttle me less: I clean memory */
+#define PF_LOCAL_THROTTLE	0x00100000	/* Throttle writes only against the bdi I write to,
+						 * I am cleaning dirty pages from some other bdi. */
 #define PF_KTHREAD		0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
 #define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */

commit a148866489fbe243c936fe43e4525d8dbfa0318f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:04 2020 +0200

    sched: Replace rq::wake_list
    
    The recent commit: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    got smp_call_function_single_async() subtly wrong. Even though it will
    return -EBUSY when trying to re-use a csd, that condition is not
    atomic and still requires external serialization.
    
    The change in ttwu_queue_remote() got this wrong.
    
    While on first reading ttwu_queue_remote() has an atomic test-and-set
    that appears to serialize the use, the matching 'release' is not in
    the right place to actually guarantee this serialization.
    
    The actual race is vs the sched_ttwu_pending() call in the idle loop;
    that can run the wakeup-list without consuming the CSD.
    
    Instead of trying to chain the lists, merge them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.129371594@infradead.org

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ebc68706152a..e0f5f41cf2ee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -654,6 +654,7 @@ struct task_struct {
 
 #ifdef CONFIG_SMP
 	struct llist_node		wake_entry;
+	unsigned int			wake_entry_type;
 	int				on_cpu;
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* Current CPU: */

commit 58ef57b16d9e91cce1c640a6fe8a21d53a85181d
Merge: 498bdcdb949e 806f04e9fd2c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 28 10:52:53 2020 +0200

    Merge branch 'core/rcu' into sched/core, to pick up dependency
    
    We are going to rely on the loosening of RCU callback semantics,
    introduced by this commit:
    
      806f04e9fd2c: ("rcu: Allow for smp_call_function() running callbacks from idle")
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5567d11c21a1d508a91a8cb64a819783a0835d9f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 19 10:22:06 2020 +0100

    x86/mce: Send #MC singal from task work
    
    Convert #MC over to using task_work_add(); it will run the same code
    slightly later, on the return to user path of the same exception.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134100.957390899@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9437b53cc603..57d0ed061ae4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1297,6 +1297,12 @@ struct task_struct {
 	unsigned long			prev_lowest_stack;
 #endif
 
+#ifdef CONFIG_X86_MCE
+	u64				mce_addr;
+	u64				mce_status;
+	struct callback_head		mce_kill_me;
+#endif
+
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.

commit c86e9b987cea3dd0209203e714553a47f5d7c6dd
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 18 14:22:03 2020 +0100

    lockdep: Prepare for noinstr sections
    
    Force inlining and prevent instrumentation of all sorts by marking the
    functions which are invoked from low level entry code with 'noinstr'.
    
    Split the irqflags tracking into two parts. One which does the heavy
    lifting while RCU is watching and the final one which can be invoked after
    RCU is turned off.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134100.484532537@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4418f5cb8324..658de6164853 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -983,6 +983,7 @@ struct task_struct {
 	unsigned int			hardirq_disable_event;
 	int				hardirqs_enabled;
 	int				hardirq_context;
+	u64				hardirq_chain_key;
 	unsigned long			softirq_disable_ip;
 	unsigned long			softirq_enable_ip;
 	unsigned int			softirq_disable_event;

commit 2a0a24ebb499c9d499eea948d3fc108f936e36d4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 27 12:42:00 2020 +0100

    sched: Make scheduler_ipi inline
    
    Now that the scheduler IPI is trivial and simple again there is no point to
    have the little function out of line. This simplifies the effort of
    constraining the instrumentation nicely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134058.453581595@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4418f5cb8324..d4ea4407cd6d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1715,7 +1715,15 @@ extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);
 })
 
 #ifdef CONFIG_SMP
-void scheduler_ipi(void);
+static __always_inline void scheduler_ipi(void)
+{
+	/*
+	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting
+	 * TIF_NEED_RESCHED remotely (for the first time) will also send
+	 * this IPI.
+	 */
+	preempt_fold_need_resched();
+}
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
 static inline void scheduler_ipi(void) { }

commit 276c410448dbca357a2bc3539acfe04862e5f172
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Tue Mar 17 16:02:06 2020 -0700

    rcu-tasks: Split ->trc_reader_need_end
    
    This commit splits ->trc_reader_need_end by using the rcu_special union.
    This change permits readers to check to see if a memory barrier is
    required without any added overhead in the common case where no such
    barrier is required.  This commit also adds the read-side checking.
    Later commits will add the machinery to properly set the new
    ->trc_reader_special.b.need_mb field.
    
    This commit also makes rcu_read_unlock_trace_special() tolerate nested
    read-side critical sections within interrupt and NMI handlers.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 864f60e51c41..9437b53cc603 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -613,7 +613,7 @@ union rcu_special {
 		u8			blocked;
 		u8			need_qs;
 		u8			exp_hint; /* Hint for performance. */
-		u8			pad; /* No garbage from compiler! */
+		u8			need_mb; /* Readers need smp_mb(). */
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };
@@ -727,7 +727,7 @@ struct task_struct {
 #ifdef CONFIG_TASKS_TRACE_RCU
 	int				trc_reader_nesting;
 	int				trc_ipi_to_cpu;
-	bool				trc_reader_need_end;
+	union rcu_special		trc_reader_special;
 	bool				trc_reader_checked;
 	struct list_head		trc_holdout_list;
 #endif /* #ifdef CONFIG_TASKS_TRACE_RCU */

commit d5f177d35c24429c87db2567d20563fc16f7e8f6
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Mar 9 19:56:53 2020 -0700

    rcu-tasks: Add an RCU Tasks Trace to simplify protection of tracing hooks
    
    Because RCU does not watch exception early-entry/late-exit, idle-loop,
    or CPU-hotplug execution, protection of tracing and BPF operations is
    needlessly complicated.  This commit therefore adds a variant of
    Tasks RCU that:
    
    o       Has explicit read-side markers to allow finite grace periods in
            the face of in-kernel loops for PREEMPT=n builds.  These markers
            are rcu_read_lock_trace() and rcu_read_unlock_trace().
    
    o       Protects code in the idle loop, exception entry/exit, and
            CPU-hotplug code paths.  In this respect, RCU-tasks trace is
            similar to SRCU, but with lighter-weight readers.
    
    o       Avoids expensive read-side instruction, having overhead similar
            to that of Preemptible RCU.
    
    There are of course downsides:
    
    o       The grace-period code can send IPIs to CPUs, even when those
            CPUs are in the idle loop or in nohz_full userspace.  This is
            mitigated by later commits.
    
    o       It is necessary to scan the full tasklist, much as for Tasks RCU.
    
    o       There is a single callback queue guarded by a single lock,
            again, much as for Tasks RCU.  However, those early use cases
            that request multiple grace periods in quick succession are
            expected to do so from a single task, which makes the single
            lock almost irrelevant.  If needed, multiple callback queues
            can be provided using any number of schemes.
    
    Perhaps most important, this variant of RCU does not affect the vanilla
    flavors, rcu_preempt and rcu_sched.  The fact that RCU Tasks Trace
    readers can operate from idle, offline, and exception entry/exit in no
    way enables rcu_preempt and rcu_sched readers to do so.
    
    The memory ordering was outlined here:
    https://lore.kernel.org/lkml/20200319034030.GX3199@paulmck-ThinkPad-P72/
    
    This effort benefited greatly from off-list discussions of BPF
    requirements with Alexei Starovoitov and Andrii Nakryiko.  At least
    some of the on-list discussions are captured in the Link: tags below.
    In addition, KCSAN was quite helpful in finding some early bugs.
    
    Link: https://lore.kernel.org/lkml/20200219150744.428764577@infradead.org/
    Link: https://lore.kernel.org/lkml/87mu8p797b.fsf@nanos.tec.linutronix.de/
    Link: https://lore.kernel.org/lkml/20200225221305.605144982@linutronix.de/
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Andrii Nakryiko <andriin@fb.com>
    [ paulmck: Apply feedback from Steve Rostedt and Joel Fernandes. ]
    [ paulmck: Decrement trc_n_readers_need_end upon IPI failure. ]
    [ paulmck: Fix locking issue reported by rcutorture. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a4b727f57095..864f60e51c41 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -724,6 +724,14 @@ struct task_struct {
 	struct list_head		rcu_tasks_holdout_list;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
+#ifdef CONFIG_TASKS_TRACE_RCU
+	int				trc_reader_nesting;
+	int				trc_ipi_to_cpu;
+	bool				trc_reader_need_end;
+	bool				trc_reader_checked;
+	struct list_head		trc_holdout_list;
+#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */
+
 	struct sched_info		sched_info;
 
 	struct list_head		tasks;

commit f0bdf6d473cf12a488a78422e15aafdfe77cf853
Author: Lai Jiangshan <laijs@linux.alibaba.com>
Date:   Sat Feb 15 14:52:32 2020 -0800

    rcu: Remove unused ->rcu_read_unlock_special.b.deferred_qs field
    
    The ->rcu_read_unlock_special.b.deferred_qs field is set to true in
    rcu_read_unlock_special() but never set to false.  This is not
    particularly useful, so this commit removes this field.
    
    The only possible justification for this field is to ease debugging
    of RCU deferred quiscent states, but the combination of the other
    ->rcu_read_unlock_special fields plus ->rcu_blocked_node and of course
    ->rcu_read_lock_nesting should cover debugging needs.  And if this last
    proves incorrect, this patch can always be reverted, along with the
    required setting of ->rcu_read_unlock_special.b.deferred_qs to false
    in rcu_preempt_deferred_qs_irqrestore().
    
    Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4418f5cb8324..a4b727f57095 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -613,7 +613,7 @@ union rcu_special {
 		u8			blocked;
 		u8			need_qs;
 		u8			exp_hint; /* Hint for performance. */
-		u8			deferred_qs;
+		u8			pad; /* No garbage from compiler! */
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };

commit 3b02a051d25d9600e9d403ad3043aed7de00160e
Merge: f5d2313bd3c5 8f3d9f354286
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 13 09:44:39 2020 +0200

    Merge tag 'v5.7-rc1' into locking/kcsan, to resolve conflicts and refresh
    
    Resolve these conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/Makefile
    
    Do a minor "evil merge" to move the KCSAN entry up a bit by a few lines
    in the Kconfig to reduce the probability of future conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d987ca1c6b7e22fbd30664111e85cec7aa66000d
Merge: 919dce24701f d1e7fd6462ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 11:22:17 2020 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull exec/proc updates from Eric Biederman:
     "This contains two significant pieces of work: the work to sort out
      proc_flush_task, and the work to solve a deadlock between strace and
      exec.
    
      Fixing proc_flush_task so that it no longer requires a persistent
      mount makes improvements to proc possible. The removal of the
      persistent mount solves an old regression that that caused the hidepid
      mount option to only work on remount not on mount. The regression was
      found and reported by the Android folks. This further allows Alexey
      Gladkov's work making proc mount options specific to an individual
      mount of proc to move forward.
    
      The work on exec starts solving a long standing issue with exec that
      it takes mutexes of blocking userspace applications, which makes exec
      extremely deadlock prone. For the moment this adds a second mutex with
      a narrower scope that handles all of the easy cases. Which makes the
      tricky cases easy to spot. With a little luck the code to solve those
      deadlocks will be ready by next merge window"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (25 commits)
      signal: Extend exec_id to 64bits
      pidfd: Use new infrastructure to fix deadlocks in execve
      perf: Use new infrastructure to fix deadlocks in execve
      proc: io_accounting: Use new infrastructure to fix deadlocks in execve
      proc: Use new infrastructure to fix deadlocks in execve
      kernel/kcmp.c: Use new infrastructure to fix deadlocks in execve
      kernel: doc: remove outdated comment cred.c
      mm: docs: Fix a comment in process_vm_rw_core
      selftests/ptrace: add test cases for dead-locks
      exec: Fix a deadlock in strace
      exec: Add exec_update_mutex to replace cred_guard_mutex
      exec: Move exec_mmap right after de_thread in flush_old_exec
      exec: Move cleanup of posix timers on exec out of de_thread
      exec: Factor unshare_sighand out of de_thread and call it separately
      exec: Only compute current once in flush_old_exec
      pid: Improve the comment about waiting in zap_pid_ns_processes
      proc: Remove the now unnecessary internal mount of proc
      uml: Create a private mount of proc for mconsole
      uml: Don't consult current to find the proc_mnt in mconsole_proc
      proc: Use a list of inodes to flush from proc
      ...

commit d1e7fd6462ca9fc76650fbe6ca800e35b24267da
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Mar 30 19:01:04 2020 -0500

    signal: Extend exec_id to 64bits
    
    Replace the 32bit exec_id with a 64bit exec_id to make it impossible
    to wrap the exec_id counter.  With care an attacker can cause exec_id
    wrap and send arbitrary signals to a newly exec'd parent.  This
    bypasses the signal sending checks if the parent changes their
    credentials during exec.
    
    The severity of this problem can been seen that in my limited testing
    of a 32bit exec_id it can take as little as 19s to exec 65536 times.
    Which means that it can take as little as 14 days to wrap a 32bit
    exec_id.  Adam Zabrocki has succeeded wrapping the self_exe_id in 7
    days.  Even my slower timing is in the uptime of a typical server.
    Which means self_exec_id is simply a speed bump today, and if exec
    gets noticably faster self_exec_id won't even be a speed bump.
    
    Extending self_exec_id to 64bits introduces a problem on 32bit
    architectures where reading self_exec_id is no longer atomic and can
    take two read instructions.  Which means that is is possible to hit
    a window where the read value of exec_id does not match the written
    value.  So with very lucky timing after this change this still
    remains expoiltable.
    
    I have updated the update of exec_id on exec to use WRITE_ONCE
    and the read of exec_id in do_notify_parent to use READ_ONCE
    to make it clear that there is no locking between these two
    locations.
    
    Link: https://lore.kernel.org/kernel-hardening/20200324215049.GA3710@pi3.com.pl
    Fixes: 2.3.23pre2
    Cc: stable@vger.kernel.org
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04278493bf15..0323e4f0982a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -939,8 +939,8 @@ struct task_struct {
 	struct seccomp			seccomp;
 
 	/* Thread group tracking: */
-	u32				parent_exec_id;
-	u32				self_exec_id;
+	u64				parent_exec_id;
+	u64				self_exec_id;
 
 	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
 	spinlock_t			alloc_lock;

commit 642e53ead6aea8740a219ede509a5d138fd4f780
Merge: 9b82f05f869a 313f16e2e35a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 17:01:51 2020 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - Various NUMA scheduling updates: harmonize the load-balancer and
         NUMA placement logic to not work against each other. The intended
         result is better locality, better utilization and fewer migrations.
    
       - Introduce Thermal Pressure tracking and optimizations, to improve
         task placement on thermally overloaded systems.
    
       - Implement frequency invariant scheduler accounting on (some) x86
         CPUs. This is done by observing and sampling the 'recent' CPU
         frequency average at ~tick boundaries. The CPU provides this data
         via the APERF/MPERF MSRs. This hopefully makes our capacity
         estimates more precise and keeps tasks on the same CPU better even
         if it might seem overloaded at a lower momentary frequency. (As
         usual, turbo mode is a complication that we resolve by observing
         the maximum frequency and renormalizing to it.)
    
       - Add asymmetric CPU capacity wakeup scan to improve capacity
         utilization on asymmetric topologies. (big.LITTLE systems)
    
       - PSI fixes and optimizations.
    
       - RT scheduling capacity awareness fixes & improvements.
    
       - Optimize the CONFIG_RT_GROUP_SCHED constraints code.
    
       - Misc fixes, cleanups and optimizations - see the changelog for
         details"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (62 commits)
      threads: Update PID limit comment according to futex UAPI change
      sched/fair: Fix condition of avg_load calculation
      sched/rt: cpupri_find: Trigger a full search as fallback
      kthread: Do not preempt current task if it is going to call schedule()
      sched/fair: Improve spreading of utilization
      sched: Avoid scale real weight down to zero
      psi: Move PF_MEMSTALL out of task->flags
      MAINTAINERS: Add maintenance information for psi
      psi: Optimize switching tasks inside shared cgroups
      psi: Fix cpu.pressure for cpu.max and competing cgroups
      sched/core: Distribute tasks within affinity masks
      sched/fair: Fix enqueue_task_fair warning
      thermal/cpu-cooling, sched/core: Move the arch_set_thermal_pressure() API to generic scheduler code
      sched/rt: Remove unnecessary push for unfit tasks
      sched/rt: Allow pulling unfitting task
      sched/rt: Optimize cpupri_find() on non-heterogenous systems
      sched/rt: Re-instate old behavior in select_task_rq_rt()
      sched/rt: cpupri_find: Implement fallback mechanism for !fit case
      sched/fair: Fix reordering of enqueue/dequeue_task_fair()
      sched/fair: Fix runnable_avg for throttled cfs
      ...

commit 40db173965c05a1d803451240ed41707d5bd978d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Mar 21 12:26:02 2020 +0100

    lockdep: Add hrtimer context tracing bits
    
    Set current->irq_config = 1 for hrtimers which are not marked to expire in
    hard interrupt context during hrtimer_init(). These timers will expire in
    softirq context on PREEMPT_RT.
    
    Setting this allows lockdep to differentiate these timers. If a timer is
    marked to expire in hard interrupt context then the timer callback is not
    supposed to acquire a regular spinlock instead of a raw_spinlock in the
    expiry callback.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.534508206@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4d3b9ecce074..933914cdf219 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -983,6 +983,7 @@ struct task_struct {
 	unsigned int			softirq_enable_event;
 	int				softirqs_enabled;
 	int				softirq_context;
+	int				irq_config;
 #endif
 
 #ifdef CONFIG_LOCKDEP

commit de8f5e4f2dc1f032b46afda0a78cab5456974f89
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Mar 21 12:26:01 2020 +0100

    lockdep: Introduce wait-type checks
    
    Extend lockdep to validate lock wait-type context.
    
    The current wait-types are:
    
            LD_WAIT_FREE,           /* wait free, rcu etc.. */
            LD_WAIT_SPIN,           /* spin loops, raw_spinlock_t etc.. */
            LD_WAIT_CONFIG,         /* CONFIG_PREEMPT_LOCK, spinlock_t etc.. */
            LD_WAIT_SLEEP,          /* sleeping locks, mutex_t etc.. */
    
    Where lockdep validates that the current lock (the one being acquired)
    fits in the current wait-context (as generated by the held stack).
    
    This ensures that there is no attempt to acquire mutexes while holding
    spinlocks, to acquire spinlocks while holding raw_spinlocks and so on. In
    other words, its a more fancy might_sleep().
    
    Obviously RCU made the entire ordeal more complex than a simple single
    value test because RCU can be acquired in (pretty much) any context and
    while it presents a context to nested locks it is not the same as it
    got acquired in.
    
    Therefore its necessary to split the wait_type into two values, one
    representing the acquire (outer) and one representing the nested context
    (inner). For most 'normal' locks these two are the same.
    
    [ To make static initialization easier we have the rule that:
      .outer == INV means .outer == .inner; because INV == 0. ]
    
    It further means that its required to find the minimal .inner of the held
    stack to compare against the outer of the new lock; because while 'normal'
    RCU presents a CONFIG type to nested locks, if it is taken while already
    holding a SPIN type it obviously doesn't relax the rules.
    
    Below is an example output generated by the trivial test code:
    
      raw_spin_lock(&foo);
      spin_lock(&bar);
      spin_unlock(&bar);
      raw_spin_unlock(&foo);
    
     [ BUG: Invalid wait context ]
     -----------------------------
     swapper/0/1 is trying to lock:
     ffffc90000013f20 (&bar){....}-{3:3}, at: kernel_init+0xdb/0x187
     other info that might help us debug this:
     1 lock held by swapper/0/1:
      #0: ffffc90000013ee0 (&foo){+.+.}-{2:2}, at: kernel_init+0xd1/0x187
    
    The way to read it is to look at the new -{n,m} part in the lock
    description; -{3:3} for the attempted lock, and try and match that up to
    the held locks, which in this case is the one: -{2,2}.
    
    This tells that the acquiring lock requires a more relaxed environment than
    presented by the lock stack.
    
    Currently only the normal locks and RCU are converted, the rest of the
    lockdep users defaults to .inner = INV which is ignored. More conversions
    can be done when desired.
    
    The check for spinlock_t nesting is not enabled by default. It's a separate
    config option for now as there are known problems which are currently
    addressed. The config option allows to identify these problems and to
    verify that the solutions found are indeed solving them.
    
    The config switch will be removed and the checks will permanently enabled
    once the vast majority of issues has been addressed.
    
    [ bigeasy: Move LD_WAIT_FREE,â€¦ out of CONFIG_LOCKDEP to avoid compile
               failure with CONFIG_DEBUG_SPINLOCK + !CONFIG_LOCKDEP]
    [ tglx: Add the config option ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.427089655@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04278493bf15..4d3b9ecce074 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -970,6 +970,7 @@ struct task_struct {
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int			irq_events;
+	unsigned int			hardirq_threaded;
 	unsigned long			hardirq_enable_ip;
 	unsigned long			hardirq_disable_ip;
 	unsigned int			hardirq_enable_event;

commit a4654e9bde4ecedb4921e6c8fe2088114bdff1b3
Merge: 7add7875a8eb e4160b2e4b02
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 21 09:23:40 2020 +0100

    Merge branch 'x86/kdump' into locking/kcsan, to resolve conflicts
    
    Conflicts:
            arch/x86/purgatory/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1066d1b6974e095d5a6c472ad9180a957b496cd6
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Mon Mar 16 21:28:05 2020 -0400

    psi: Move PF_MEMSTALL out of task->flags
    
    The task->flags is a 32-bits flag, in which 31 bits have already been
    consumed. So it is hardly to introduce other new per process flag.
    Currently there're still enough spaces in the bit-field section of
    task_struct, so we can define the memstall state as a single bit in
    task_struct instead.
    This patch also removes an out-of-date comment pointed by Matthew.
    
    Suggested-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Link: https://lkml.kernel.org/r/1584408485-1921-1-git-send-email-laoar.shao@gmail.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e9199bf947b..09bddd9e69a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -785,9 +785,12 @@ struct task_struct {
 	unsigned			frozen:1;
 #endif
 #ifdef CONFIG_BLK_CGROUP
-	/* to be used once the psi infrastructure lands upstream. */
 	unsigned			use_memdelay:1;
 #endif
+#ifdef CONFIG_PSI
+	/* Stalled due to lack of memory */
+	unsigned			in_memstall:1;
+#endif
 
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
@@ -1480,7 +1483,6 @@ extern struct pid *cad_pid;
 #define PF_KTHREAD		0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
 #define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */
-#define PF_MEMSTALL		0x01000000	/* Stalled due to lack of memory */
 #define PF_UMH			0x02000000	/* I'm an Usermodehelper process */
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_mask */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */

commit 9f68395333ad7f5bfe2f83473fed363d4229f11c
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:18 2020 +0000

    sched/pelt: Add a new runnable average signal
    
    Now that runnable_load_avg has been removed, we can replace it by a new
    signal that will highlight the runnable pressure on a cfs_rq. This signal
    track the waiting time of tasks on rq and can help to better define the
    state of rqs.
    
    At now, only util_avg is used to define the state of a rq:
      A rq with more that around 80% of utilization and more than 1 tasks is
      considered as overloaded.
    
    But the util_avg signal of a rq can become temporaly low after that a task
    migrated onto another rq which can bias the classification of the rq.
    
    When tasks compete for the same rq, their runnable average signal will be
    higher than util_avg as it will include the waiting time and we can use
    this signal to better classify cfs_rqs.
    
    The new runnable_avg will track the runnable time of a task which simply
    adds the waiting time to the running time. The runnable _avg of cfs_rq
    will be the /Sum of se's runnable_avg and the runnable_avg of group entity
    will follow the one of the rq similarly to util_avg.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-9-mgorman@techsingularity.net

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 037eaffabc24..2e9199bf947b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -356,28 +356,30 @@ struct util_est {
 } __attribute__((__aligned__(sizeof(u64))));
 
 /*
- * The load_avg/util_avg accumulates an infinite geometric series
+ * The load/runnable/util_avg accumulates an infinite geometric series
  * (see __update_load_avg_cfs_rq() in kernel/sched/pelt.c).
  *
  * [load_avg definition]
  *
  *   load_avg = runnable% * scale_load_down(load)
  *
- * where runnable% is the time ratio that a sched_entity is runnable.
- * For cfs_rq, it is the aggregated load_avg of all runnable and
- * blocked sched_entities.
+ * [runnable_avg definition]
+ *
+ *   runnable_avg = runnable% * SCHED_CAPACITY_SCALE
  *
  * [util_avg definition]
  *
  *   util_avg = running% * SCHED_CAPACITY_SCALE
  *
- * where running% is the time ratio that a sched_entity is running on
- * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable
- * and blocked sched_entities.
+ * where runnable% is the time ratio that a sched_entity is runnable and
+ * running% the time ratio that a sched_entity is running.
+ *
+ * For cfs_rq, they are the aggregated values of all runnable and blocked
+ * sched_entities.
  *
- * load_avg and util_avg don't direcly factor frequency scaling and CPU
- * capacity scaling. The scaling is done through the rq_clock_pelt that
- * is used for computing those signals (see update_rq_clock_pelt())
+ * The load/runnable/util_avg doesn't direcly factor frequency scaling and CPU
+ * capacity scaling. The scaling is done through the rq_clock_pelt that is used
+ * for computing those signals (see update_rq_clock_pelt())
  *
  * N.B., the above ratios (runnable% and running%) themselves are in the
  * range of [0, 1]. To do fixed point arithmetics, we therefore scale them
@@ -401,9 +403,11 @@ struct util_est {
 struct sched_avg {
 	u64				last_update_time;
 	u64				load_sum;
+	u64				runnable_sum;
 	u32				util_sum;
 	u32				period_contrib;
 	unsigned long			load_avg;
+	unsigned long			runnable_avg;
 	unsigned long			util_avg;
 	struct util_est			util_est;
 } ____cacheline_aligned;
@@ -467,6 +471,8 @@ struct sched_entity {
 	struct cfs_rq			*cfs_rq;
 	/* rq "owned" by this entity/group: */
 	struct cfs_rq			*my_q;
+	/* cached value of my_q->h_nr_running */
+	unsigned long			runnable_weight;
 #endif
 
 #ifdef CONFIG_SMP

commit 0dacee1bfa70e171be3a12a30414c228453048d2
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:17 2020 +0000

    sched/pelt: Remove unused runnable load average
    
    Now that runnable_load_avg is no more used, we can remove it to make
    space for a new signal.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-8-mgorman@techsingularity.net

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04278493bf15..037eaffabc24 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -357,7 +357,7 @@ struct util_est {
 
 /*
  * The load_avg/util_avg accumulates an infinite geometric series
- * (see __update_load_avg() in kernel/sched/fair.c).
+ * (see __update_load_avg_cfs_rq() in kernel/sched/pelt.c).
  *
  * [load_avg definition]
  *
@@ -401,11 +401,9 @@ struct util_est {
 struct sched_avg {
 	u64				last_update_time;
 	u64				load_sum;
-	u64				runnable_load_sum;
 	u32				util_sum;
 	u32				period_contrib;
 	unsigned long			load_avg;
-	unsigned long			runnable_load_avg;
 	unsigned long			util_avg;
 	struct util_est			util_est;
 } ____cacheline_aligned;
@@ -449,7 +447,6 @@ struct sched_statistics {
 struct sched_entity {
 	/* For load-balancing: */
 	struct load_weight		load;
-	unsigned long			runnable_weight;
 	struct rb_node			run_node;
 	struct list_head		group_node;
 	unsigned int			on_rq;

commit 83fa805bcbfc53ae82eedd65132794ae324798e5
Merge: 896f8d23d0cb 8d19f1c8e193
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 29 19:38:34 2020 -0800

    Merge tag 'threads-v5.6' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull thread management updates from Christian Brauner:
     "Sargun Dhillon over the last cycle has worked on the pidfd_getfd()
      syscall.
    
      This syscall allows for the retrieval of file descriptors of a process
      based on its pidfd. A task needs to have ptrace_may_access()
      permissions with PTRACE_MODE_ATTACH_REALCREDS (suggested by Oleg and
      Andy) on the target.
    
      One of the main use-cases is in combination with seccomp's user
      notification feature. As a reminder, seccomp's user notification
      feature was made available in v5.0. It allows a task to retrieve a
      file descriptor for its seccomp filter. The file descriptor is usually
      handed of to a more privileged supervising process. The supervisor can
      then listen for syscall events caught by the seccomp filter of the
      supervisee and perform actions in lieu of the supervisee, usually
      emulating syscalls. pidfd_getfd() is needed to expand its uses.
    
      There are currently two major users that wait on pidfd_getfd() and one
      future user:
    
       - Netflix, Sargun said, is working on a service mesh where users
         should be able to connect to a dns-based VIP. When a user connects
         to e.g. 1.2.3.4:80 that runs e.g. service "foo" they will be
         redirected to an envoy process. This service mesh uses seccomp user
         notifications and pidfd to intercept all connect calls and instead
         of connecting them to 1.2.3.4:80 connects them to e.g.
         127.0.0.1:8080.
    
       - LXD uses the seccomp notifier heavily to intercept and emulate
         mknod() and mount() syscalls for unprivileged containers/processes.
         With pidfd_getfd() more uses-cases e.g. bridging socket connections
         will be possible.
    
       - The patchset has also seen some interest from the browser corner.
         Right now, Firefox is using a SECCOMP_RET_TRAP sandbox managed by a
         broker process. In the future glibc will start blocking all signals
         during dlopen() rendering this type of sandbox impossible. Hence,
         in the future Firefox will switch to a seccomp-user-nofication
         based sandbox which also makes use of file descriptor retrieval.
         The thread for this can be found at
         https://sourceware.org/ml/libc-alpha/2019-12/msg00079.html
    
      With pidfd_getfd() it is e.g. possible to bridge socket connections
      for the supervisee (binding to a privileged port) and taking actions
      on file descriptors on behalf of the supervisee in general.
    
      Sargun's first version was using an ioctl on pidfds but various people
      pushed for it to be a proper syscall which he duely implemented as
      well over various review cycles. Selftests are of course included.
      I've also added instructions how to deal with merge conflicts below.
    
      There's also a small fix coming from the kernel mentee project to
      correctly annotate struct sighand_struct with __rcu to fix various
      sparse warnings. We've received a few more such fixes and even though
      they are mostly trivial I've decided to postpone them until after -rc1
      since they came in rather late and I don't want to risk introducing
      build warnings.
    
      Finally, there's a new prctl() command PR_{G,S}ET_IO_FLUSHER which is
      needed to avoid allocation recursions triggerable by storage drivers
      that have userspace parts that run in the IO path (e.g. dm-multipath,
      iscsi, etc). These allocation recursions deadlock the device.
    
      The new prctl() allows such privileged userspace components to avoid
      allocation recursions by setting the PF_MEMALLOC_NOIO and
      PF_LESS_THROTTLE flags. The patch carries the necessary acks from the
      relevant maintainers and is routed here as part of prctl()
      thread-management."
    
    * tag 'threads-v5.6' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      prctl: PR_{G,S}ET_IO_FLUSHER to support controlling memory reclaim
      sched.h: Annotate sighand_struct with __rcu
      test: Add test for pidfd getfd
      arch: wire up pidfd_getfd syscall
      pid: Implement pidfd_getfd syscall
      vfs, fdtable: Add fget_task helper

commit 913292c97d750fe4188b4f5aa770e5e0ca1e5a91
Author: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
Date:   Fri Jan 24 10:29:08 2020 +0530

    sched.h: Annotate sighand_struct with __rcu
    
    This patch fixes the following sparse errors by annotating the
    sighand_struct with __rcu
    
    kernel/fork.c:1511:9: error: incompatible types in comparison expression
    kernel/exit.c:100:19: error: incompatible types in comparison expression
    kernel/signal.c:1370:27: error: incompatible types in comparison expression
    
    This fix introduces the following sparse error in signal.c due to
    checking the sighand pointer without rcu primitives:
    
    kernel/signal.c:1386:21: error: incompatible types in comparison expression
    
    This new sparse error is also fixed in this patch.
    
    Signed-off-by: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@kernel.org>
    Link: https://lore.kernel.org/r/20200124045908.26389-1-madhuparnabhowmik10@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 467d26046416..ef60aa15097a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -917,7 +917,7 @@ struct task_struct {
 
 	/* Signal handlers: */
 	struct signal_struct		*signal;
-	struct sighand_struct		*sighand;
+	struct sighand_struct __rcu		*sighand;
 	sigset_t			blocked;
 	sigset_t			real_blocked;
 	/* Restored if set_restore_sigmask() was used: */

commit 837171fe77d700222bb75ef5fe26f4785fcd9e99
Merge: c29a59e43829 def9d2780727
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 20 08:42:47 2020 +0100

    Merge tag 'v5.5-rc7' into locking/kcsan, to refresh the tree
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 28336be568bb473d16ba80db0801276fb4f1bbe5
Merge: 5cbaefe9743b fd6988496e79
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 30 08:10:51 2019 +0100

    Merge tag 'v5.5-rc4' into locking/kcsan, to resolve conflicts
    
    Conflicts:
            init/main.c
            lib/Kconfig.debug
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 463f550fb47bede3a5d7d5177f363a6c3b45d50b
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Wed Dec 11 11:17:12 2019 -0500

    rseq: Unregister rseq for clone CLONE_VM
    
    It has been reported by Google that rseq is not behaving properly
    with respect to clone when CLONE_VM is used without CLONE_THREAD.
    
    It keeps the prior thread's rseq TLS registered when the TLS of the
    thread has moved, so the kernel can corrupt the TLS of the parent.
    
    The approach of clearing the per task-struct rseq registration
    on clone with CLONE_THREAD flag is incomplete. It does not cover
    the use-case of clone with CLONE_VM set, but without CLONE_THREAD.
    
    Here is the rationale for unregistering rseq on clone with CLONE_VM
    flag set:
    
    1) CLONE_THREAD requires CLONE_SIGHAND, which requires CLONE_VM to be
       set. Therefore, just checking for CLONE_VM covers all CLONE_THREAD
       uses. There is no point in checking for both CLONE_THREAD and
       CLONE_VM,
    
    2) There is the possibility of an unlikely scenario where CLONE_SETTLS
       is used without CLONE_VM. In order to be an issue, it would require
       that the rseq TLS is in a shared memory area.
    
       I do not plan on adding CLONE_SETTLS to the set of clone flags which
       unregister RSEQ, because it would require that we also unregister RSEQ
       on set_thread_area(2) and arch_prctl(2) ARCH_SET_FS for completeness.
       So rather than doing a partial solution, it appears better to let
       user-space explicitly perform rseq unregistration across clone if
       needed in scenarios where CLONE_VM is not set.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191211161713.4490-3-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 467d26046416..716ad1d8d95e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1929,11 +1929,11 @@ static inline void rseq_migrate(struct task_struct *t)
 
 /*
  * If parent process has a registered restartable sequences area, the
- * child inherits. Only applies when forking a process, not a thread.
+ * child inherits. Unregister rseq for a clone with CLONE_VM set.
  */
 static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
 {
-	if (clone_flags & CLONE_THREAD) {
+	if (clone_flags & CLONE_VM) {
 		t->rseq = NULL;
 		t->rseq_sig = 0;
 		t->rseq_event_mask = 0;

commit eec028c9386ed1a692aa01a85b55952202b41619
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Wed Dec 4 16:52:43 2019 -0800

    kcov: remote coverage support
    
    Patch series " kcov: collect coverage from usb and vhost", v3.
    
    This patchset extends kcov to allow collecting coverage from backgound
    kernel threads.  This extension requires custom annotations for each of
    the places where coverage collection is desired.  This patchset
    implements this for hub events in the USB subsystem and for vhost
    workers.  See the first patch description for details about the kcov
    extension.  The other two patches apply this kcov extension to USB and
    vhost.
    
    Examples of other subsystems that might potentially benefit from this
    when custom annotations are added (the list is based on
    process_one_work() callers for bugs recently reported by syzbot):
    
    1. fs: writeback wb_workfn() worker,
    2. net: addrconf_dad_work()/addrconf_verify_work() workers,
    3. net: neigh_periodic_work() worker,
    4. net/p9: p9_write_work()/p9_read_work() workers,
    5. block: blk_mq_run_work_fn() worker.
    
    These patches have been used to enable coverage-guided USB fuzzing with
    syzkaller for the last few years, see the details here:
    
      https://github.com/google/syzkaller/blob/master/docs/linux/external_fuzzing_usb.md
    
    This patchset has been pushed to the public Linux kernel Gerrit
    instance:
    
      https://linux-review.googlesource.com/c/linux/kernel/git/torvalds/linux/+/1524
    
    This patch (of 3):
    
    Add background thread coverage collection ability to kcov.
    
    With KCOV_ENABLE coverage is collected only for syscalls that are issued
    from the current process.  With KCOV_REMOTE_ENABLE it's possible to
    collect coverage for arbitrary parts of the kernel code, provided that
    those parts are annotated with kcov_remote_start()/kcov_remote_stop().
    
    This allows to collect coverage from two types of kernel background
    threads: the global ones, that are spawned during kernel boot in a
    limited number of instances (e.g.  one USB hub_event() worker thread is
    spawned per USB HCD); and the local ones, that are spawned when a user
    interacts with some kernel interface (e.g.  vhost workers).
    
    To enable collecting coverage from a global background thread, a unique
    global handle must be assigned and passed to the corresponding
    kcov_remote_start() call.  Then a userspace process can pass a list of
    such handles to the KCOV_REMOTE_ENABLE ioctl in the handles array field
    of the kcov_remote_arg struct.  This will attach the used kcov device to
    the code sections, that are referenced by those handles.
    
    Since there might be many local background threads spawned from
    different userspace processes, we can't use a single global handle per
    annotation.  Instead, the userspace process passes a non-zero handle
    through the common_handle field of the kcov_remote_arg struct.  This
    common handle gets saved to the kcov_handle field in the current
    task_struct and needs to be passed to the newly spawned threads via
    custom annotations.  Those threads should in turn be annotated with
    kcov_remote_start()/kcov_remote_stop().
    
    Internally kcov stores handles as u64 integers.  The top byte of a
    handle is used to denote the id of a subsystem that this handle belongs
    to, and the lower 4 bytes are used to denote the id of a thread instance
    within that subsystem.  A reserved value 0 is used as a subsystem id for
    common handles as they don't belong to a particular subsystem.  The
    bytes 4-7 are currently reserved and must be zero.  In the future the
    number of bytes used for the subsystem or handle ids might be increased.
    
    When a particular userspace process collects coverage by via a common
    handle, kcov will collect coverage for each code section that is
    annotated to use the common handle obtained as kcov_handle from the
    current task_struct.  However non common handles allow to collect
    coverage selectively from different subsystems.
    
    Link: http://lkml.kernel.org/r/e90e315426a384207edbec1d6aa89e43008e4caf.1572366574.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: David Windsor <dwindsor@gmail.com>
    Cc: Elena Reshetova <elena.reshetova@intel.com>
    Cc: Anders Roxell <anders.roxell@linaro.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Marco Elver <elver@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0cd97d9dd021..467d26046416 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1210,6 +1210,8 @@ struct task_struct {
 #endif /* CONFIG_TRACING */
 
 #ifdef CONFIG_KCOV
+	/* See kernel/kcov.c for more details. */
+
 	/* Coverage collection mode enabled for this task (0 if disabled): */
 	unsigned int			kcov_mode;
 
@@ -1221,6 +1223,12 @@ struct task_struct {
 
 	/* KCOV descriptor wired with this task or NULL: */
 	struct kcov			*kcov;
+
+	/* KCOV common handle for remote coverage collection: */
+	u64				kcov_handle;
+
+	/* KCOV sequence number: */
+	int				kcov_sequence;
 #endif
 
 #ifdef CONFIG_MEMCG

commit 043cf46825c102683b1027762c09c7e2b749e5a3
Merge: b22bfea7f16c 83bae01182ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 3 12:20:25 2019 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "The main changes in the timer code in this cycle were:
    
       - Clockevent updates:
    
          - timer-of framework cleanups. (Geert Uytterhoeven)
    
          - Use timer-of for the renesas-ostm and the device name to prevent
            name collision in case of multiple timers. (Geert Uytterhoeven)
    
          - Check if there is an error after calling of_clk_get in asm9260
            (Chuhong Yuan)
    
       - ABI fix: Zero out high order bits of nanoseconds on compat
         syscalls. This got broken a year ago, with apparently no side
         effects so far.
    
         Since the kernel would use random data otherwise I don't think we'd
         have other options but to fix the bug, even if there was a side
         effect to applications (Dmitry Safonov)
    
       - Optimize ns_to_timespec64() on 32-bit systems: move away from
         div_s64_rem() which can be slow, to div_u64_rem() which is faster
         (Arnd Bergmann)
    
       - Annotate KCSAN-reported false positive data races in
         hrtimer_is_queued() users by moving timer->state handling over to
         the READ_ONCE()/WRITE_ONCE() APIs. This documents these accesses
         (Eric Dumazet)
    
       - Misc cleanups and small fixes"
    
    [ I undid the "ABI fix" and updated the comments instead. The reason
      there were apparently no side effects is that the fix was a no-op.
    
      The updated comment is to say _why_ it was a no-op.    - Linus ]
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      time: Zero the upper 32-bits in __kernel_timespec on 32-bit
      time: Rename tsk->real_start_time to ->start_boottime
      hrtimer: Remove the comment about not used HRTIMER_SOFTIRQ
      time: Fix spelling mistake in comment
      time: Optimize ns_to_timespec64()
      hrtimer: Annotate lockless access to timer->state
      clocksource/drivers/asm9260: Add a check for of_clk_get
      clocksource/drivers/renesas-ostm: Use unique device name instead of ostm
      clocksource/drivers/renesas-ostm: Convert to timer_of
      clocksource/drivers/timer-of: Use unique device name instead of timer
      clocksource/drivers/timer-of: Convert last full_name to %pOF

commit 168829ad09ca9cdfdc664b2110d0e3569932c12d
Merge: 1ae78780eda5 500543c53a54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 16:02:40 2019 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - A comprehensive rewrite of the robust/PI futex code's exit handling
         to fix various exit races. (Thomas Gleixner et al)
    
       - Rework the generic REFCOUNT_FULL implementation using
         atomic_fetch_* operations so that the performance impact of the
         cmpxchg() loops is mitigated for common refcount operations.
    
         With these performance improvements the generic implementation of
         refcount_t should be good enough for everybody - and this got
         confirmed by performance testing, so remove ARCH_HAS_REFCOUNT and
         REFCOUNT_FULL entirely, leaving the generic implementation enabled
         unconditionally. (Will Deacon)
    
       - Other misc changes, fixes, cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      lkdtm: Remove references to CONFIG_REFCOUNT_FULL
      locking/refcount: Remove unused 'refcount_error_report()' function
      locking/refcount: Consolidate implementations of refcount_t
      locking/refcount: Consolidate REFCOUNT_{MAX,SATURATED} definitions
      locking/refcount: Move saturation warnings out of line
      locking/refcount: Improve performance of generic REFCOUNT_FULL code
      locking/refcount: Move the bulk of the REFCOUNT_FULL implementation into the <linux/refcount.h> header
      locking/refcount: Remove unused refcount_*_checked() variants
      locking/refcount: Ensure integer operands are treated as signed
      locking/refcount: Define constants for saturation and max refcount values
      futex: Prevent exit livelock
      futex: Provide distinct return value when owner is exiting
      futex: Add mutex around futex exit
      futex: Provide state handling for exec() as well
      futex: Sanitize exit state handling
      futex: Mark the begin of futex exit explicitly
      futex: Set task::futex_state to DEAD right after handling futex exit
      futex: Split futex_mm_release() for exit/exec
      exit/exec: Seperate mm_release()
      futex: Replace PF_EXITPIDONE with a state
      ...

commit 77a05940eee7e9891cd6add7a690a3e762ee21b0
Merge: 3f59dbcace56 de881a341c41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 15:23:14 2019 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest changes in this cycle were:
    
       - Make kcpustat vtime aware (Frederic Weisbecker)
    
       - Rework the CFS load_balance() logic (Vincent Guittot)
    
       - Misc cleanups, smaller enhancements, fixes.
    
      The load-balancing rework is the most intrusive change: it replaces
      the old heuristics that have become less meaningful after the
      introduction of the PELT metrics, with a grounds-up load-balancing
      algorithm.
    
      As such it's not really an iterative series, but replaces the old
      load-balancing logic with the new one. We hope there are no
      performance regressions left - but statistically it's highly probable
      that there *is* going to be some workload that is hurting from these
      chnages. If so then we'd prefer to have a look at that workload and
      fix its scheduling, instead of reverting the changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      rackmeter: Use vtime aware kcpustat accessor
      leds: Use all-in-one vtime aware kcpustat accessor
      cpufreq: Use vtime aware kcpustat accessors for user time
      procfs: Use all-in-one vtime aware kcpustat accessor
      sched/vtime: Bring up complete kcpustat accessor
      sched/cputime: Support other fields on kcpustat_field()
      sched/cpufreq: Move the cfs_rq_util_change() call to cpufreq_update_util()
      sched/fair: Add comments for group_type and balancing at SD_NUMA level
      sched/fair: Fix rework of find_idlest_group()
      sched/uclamp: Fix overzealous type replacement
      sched/Kconfig: Fix spelling mistake in user-visible help text
      sched/core: Further clarify sched_class::set_next_task()
      sched/fair: Use mul_u32_u32()
      sched/core: Simplify sched_class::pick_next_task()
      sched/core: Optimize pick_next_task()
      sched/core: Make pick_next_task_idle() more consistent
      sched/fair: Better document newidle_balance()
      leds: Use vtime aware kcpustat accessor to fetch CPUTIME_SYSTEM
      cpufreq: Use vtime aware kcpustat accessor to fetch CPUTIME_SYSTEM
      procfs: Use vtime aware kcpustat accessor to fetch CPUTIME_SYSTEM
      ...

commit 3f186d974826847a07bc7964d79ec4eded475ad9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:44 2019 +0100

    futex: Add mutex around futex exit
    
    The mutex will be used in subsequent changes to replace the busy looping of
    a waiter when the futex owner is currently executing the exit cleanup to
    prevent a potential live lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.845798895@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 85dab2f721c9..1ebe540f8a08 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1053,6 +1053,7 @@ struct task_struct {
 #endif
 	struct list_head		pi_state_list;
 	struct futex_pi_state		*pi_state_cache;
+	struct mutex			futex_exit_mutex;
 	unsigned int			futex_state;
 #endif
 #ifdef CONFIG_PERF_EVENTS

commit 3d4775df0a89240f671861c6ab6e8d59af8e9e41
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:37 2019 +0100

    futex: Replace PF_EXITPIDONE with a state
    
    The futex exit handling relies on PF_ flags. That's suboptimal as it
    requires a smp_mb() and an ugly lock/unlock of the exiting tasks pi_lock in
    the middle of do_exit() to enforce the observability of PF_EXITING in the
    futex code.
    
    Add a futex_state member to task_struct and convert the PF_EXITPIDONE logic
    over to the new state. The PF_EXITING dependency will be cleaned up in a
    later step.
    
    This prepares for handling various futex exit issues later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.149449274@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c2e56bd8913..85dab2f721c9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1053,6 +1053,7 @@ struct task_struct {
 #endif
 	struct list_head		pi_state_list;
 	struct futex_pi_state		*pi_state_cache;
+	unsigned int			futex_state;
 #endif
 #ifdef CONFIG_PERF_EVENTS
 	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
@@ -1441,7 +1442,6 @@ extern struct pid *cad_pid;
  */
 #define PF_IDLE			0x00000002	/* I am an IDLE thread */
 #define PF_EXITING		0x00000004	/* Getting shut down */
-#define PF_EXITPIDONE		0x00000008	/* PI exit done on shut down */
 #define PF_VCPU			0x00000010	/* I'm a virtual CPU */
 #define PF_WQ_WORKER		0x00000020	/* I'm a workqueue worker */
 #define PF_FORKNOEXEC		0x00000040	/* Forked but didn't exec */

commit dfd402a4c4baae42398ce9180ff424d589b8bffc
Author: Marco Elver <elver@google.com>
Date:   Thu Nov 14 19:02:54 2019 +0100

    kcsan: Add Kernel Concurrency Sanitizer infrastructure
    
    Kernel Concurrency Sanitizer (KCSAN) is a dynamic data-race detector for
    kernel space. KCSAN is a sampling watchpoint-based data-race detector.
    See the included Documentation/dev-tools/kcsan.rst for more details.
    
    This patch adds basic infrastructure, but does not yet enable KCSAN for
    any architecture.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Acked-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67a1d86981a9..ae4f341c1db4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -31,6 +31,7 @@
 #include <linux/task_io_accounting.h>
 #include <linux/posix-timers.h>
 #include <linux/rseq.h>
+#include <linux/kcsan.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -1172,6 +1173,9 @@ struct task_struct {
 #ifdef CONFIG_KASAN
 	unsigned int			kasan_depth;
 #endif
+#ifdef CONFIG_KCSAN
+	struct kcsan_ctx		kcsan_ctx;
+#endif
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored address in ret_stack: */

commit cf25e24db61cc9df42c47485a2ec2bff4e9a3692
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 7 11:07:58 2019 +0100

    time: Rename tsk->real_start_time to ->start_boottime
    
    Since it stores CLOCK_BOOTTIME, not, as the name suggests,
    CLOCK_REALTIME, let's rename ->real_start_time to ->start_bootime.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67a1d86981a9..254128952eab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -857,7 +857,7 @@ struct task_struct {
 	u64				start_time;
 
 	/* Boot based time in nsecs: */
-	u64				real_start_time;
+	u64				start_boottime;
 
 	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */
 	unsigned long			min_flt;

commit 6d5a763c303bc9d78b17361d30b692ba2facf9b4
Merge: e79b3ddad679 31f4f5b495a6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Nov 11 08:34:59 2019 +0100

    Merge tag 'v5.4-rc7' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 771b53d033e8663abdf59704806aa856b236dcdb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 22 10:25:58 2019 -0600

    io-wq: small threadpool implementation for io_uring
    
    This adds support for io-wq, a smaller and specialized thread pool
    implementation. This is meant to replace workqueues for io_uring. Among
    the reasons for this addition are:
    
    - We can assign memory context smarter and more persistently if we
      manage the life time of threads.
    
    - We can drop various work-arounds we have in io_uring, like the
      async_list.
    
    - We can implement hashed work insertion, to manage concurrency of
      buffered writes without needing a) an extra workqueue, or b)
      needlessly making the concurrency of said workqueue very low
      which hurts performance of multiple buffered file writers.
    
    - We can implement cancel through signals, for cancelling
      interruptible work like read/write (or send/recv) to/from sockets.
    
    - We need the above cancel for being able to assign and use file tables
      from a process.
    
    - We can implement a more thorough cancel operation in general.
    
    - We need it to move towards a syslet/threadlet model for even faster
      async execution. For that we need to take ownership of the used
      threads.
    
    This list is just off the top of my head. Performance should be the
    same, or better, at least that's what I've seen in my testing. io-wq
    supports basic NUMA functionality, setting up a pool per node.
    
    io-wq hooks up to the scheduler schedule in/out just like workqueue
    and uses that to drive the need for more/less workers.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67a1d86981a9..6666e25606b7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1468,6 +1468,7 @@ extern struct pid *cad_pid;
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_mask */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
 #define PF_MEMALLOC_NOCMA	0x10000000	/* All allocation request will have _GFP_MOVABLE cleared */
+#define PF_IO_WORKER		0x20000000	/* Task is an IO worker */
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */
 

commit e6d5bf3e321ca664d12eb00ceb40bd58987ce8a1
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:49 2019 +0200

    sched/cputime: Add vtime guest task state
    
    Record guest as a VTIME state instead of guessing it from VTIME_SYS and
    PF_VCPU. This is going to simplify the cputime read side especially as
    its state machine is going to further expand in order to fully support
    kcpustat on nohz_full.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-4-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ae19be2c126..988c4da00c31 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -255,6 +255,8 @@ enum vtime_state {
 	VTIME_SYS,
 	/* Task runs in userspace in a CPU with VTIME active: */
 	VTIME_USER,
+	/* Task runs as guests in a CPU with VTIME active: */
+	VTIME_GUEST,
 };
 
 struct vtime {

commit 14faf6fcac4ba33f8fd8d9b2d0278010a9eb1742
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:48 2019 +0200

    sched/cputime: Add vtime idle task state
    
    Record idle as a VTIME state instead of guessing it from VTIME_SYS and
    is_idle_task(). This is going to simplify the cputime read side
    especially as its state machine is going to further expand in order to
    fully support kcpustat on nohz_full.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-3-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d5d07335a97b..4ae19be2c126 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -249,10 +249,12 @@ struct prev_cputime {
 enum vtime_state {
 	/* Task is sleeping or running in a CPU with VTIME inactive: */
 	VTIME_INACTIVE = 0,
-	/* Task runs in userspace in a CPU with VTIME active: */
-	VTIME_USER,
+	/* Task is idle */
+	VTIME_IDLE,
 	/* Task runs in kernelspace in a CPU with VTIME active: */
 	VTIME_SYS,
+	/* Task runs in userspace in a CPU with VTIME active: */
+	VTIME_USER,
 };
 
 struct vtime {

commit 802f4a827f139f2581b3c50c69d20f8bf4c24af1
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:47 2019 +0200

    sched/vtime: Record CPU under seqcount for kcpustat needs
    
    In order to compute the kcpustat delta on a nohz CPU, we'll need to
    fetch the task running on that target. Checking that its vtime
    state snapshot actually refers to the relevant target involves recording
    that CPU under the seqcount locked on task switch.
    
    This is a step toward making kcpustat moving forward on full nohz CPUs.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-2-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c2e56bd8913..d5d07335a97b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -259,6 +259,7 @@ struct vtime {
 	seqcount_t		seqcount;
 	unsigned long long	starttime;
 	enum vtime_state	state;
+	unsigned int		cpu;
 	u64			utime;
 	u64			stime;
 	u64			gtime;

commit 19c95f261c6558d4c2cbbfacd2d8bb6501384601
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Oct 15 18:25:44 2019 +0100

    arm64: entry.S: Do not preempt from IRQ before all cpufeatures are enabled
    
    Preempting from IRQ-return means that the task has its PSTATE saved
    on the stack, which will get restored when the task is resumed and does
    the actual IRQ return.
    
    However, enabling some CPU features requires modifying the PSTATE. This
    means that, if a task was scheduled out during an IRQ-return before all
    CPU features are enabled, the task might restore a PSTATE that does not
    include the feature enablement changes once scheduled back in.
    
    * Task 1:
    
    PAN == 0 ---|                          |---------------
                |                          |<- return from IRQ, PSTATE.PAN = 0
                | <- IRQ                   |
                +--------+ <- preempt()  +--
                                         ^
                                         |
                                         reschedule Task 1, PSTATE.PAN == 1
    * Init:
            --------------------+------------------------
                                ^
                                |
                                enable_cpu_features
                                set PSTATE.PAN on all CPUs
    
    Worse than this, since PSTATE is untouched when task switching is done,
    a task missing the new bits in PSTATE might affect another task, if both
    do direct calls to schedule() (outside of IRQ/exception contexts).
    
    Fix this by preventing preemption on IRQ-return until features are
    enabled on all CPUs.
    
    This way the only PSTATE values that are saved on the stack are from
    synchronous exceptions. These are expected to be fatal this early, the
    exception is BRK for WARN_ON(), but as this uses do_debug_exception()
    which keeps IRQs masked, it shouldn't call schedule().
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    [james: Replaced a really cool hack, with an even simpler static key in C.
     expanded commit message with Julien's cover-letter ascii art]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c2e56bd8913..67a1d86981a9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -223,6 +223,7 @@ extern long schedule_timeout_uninterruptible(long timeout);
 extern long schedule_timeout_idle(long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
+asmlinkage void preempt_schedule_irq(void);
 
 extern int __must_check io_schedule_prepare(void);
 extern void io_schedule_finish(int token);

commit 9c5efe9ae7df78600c0ee7bcce27516eb687fa6e
Merge: aefcf2f4b581 4892f51ad54d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 28 12:39:07 2019 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
    
     - Apply a number of membarrier related fixes and cleanups, which fixes
       a use-after-free race in the membarrier code
    
     - Introduce proper RCU protection for tasks on the runqueue - to get
       rid of the subtle task_rcu_dereference() interface that was easy to
       get wrong
    
     - Misc fixes, but also an EAS speedup
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/fair: Avoid redundant EAS calculation
      sched/core: Remove double update_max_interval() call on CPU startup
      sched/core: Fix preempt_schedule() interrupt return comment
      sched/fair: Fix -Wunused-but-set-variable warnings
      sched/core: Fix migration to invalid CPU in __set_cpus_allowed_ptr()
      sched/membarrier: Return -ENOMEM to userspace on memory allocation failure
      sched/membarrier: Skip IPIs when mm->mm_users == 1
      selftests, sched/membarrier: Add multi-threaded test
      sched/membarrier: Fix p->mm->membarrier_state racy load
      sched/membarrier: Call sync_core only before usermode for same mm
      sched/membarrier: Remove redundant check
      sched/membarrier: Fix private expedited registration check
      tasks, sched/core: RCUify the assignment of rq->curr
      tasks, sched/core: With a grace period after finish_task_switch(), remove unnecessary code
      tasks, sched/core: Ensure tasks are available for a grace period after leaving the runqueue
      tasks: Add a count of task RCU users
      sched/core: Convert vcpu_is_preempted() from macro to an inline function
      sched/fair: Remove unused cfs_rq_clock_task() function

commit 3fbd7ee285b2bbc6eebd15a3c8786d9776a402a8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 14 07:33:34 2019 -0500

    tasks: Add a count of task RCU users
    
    Add a count of the number of RCU users (currently 1) of the task
    struct so that we can later add the scheduler case and get rid of the
    very subtle task_rcu_dereference(), and just use rcu_dereference().
    
    As suggested by Oleg have the count overlap rcu_head so that no
    additional space in task_struct is required.
    
    Inspired-by: Linus Torvalds <torvalds@linux-foundation.org>
    Inspired-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87woebdplt.fsf_-_@x220.int.ebiederm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e2e91960d79f..8e43e54a02c7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1147,7 +1147,10 @@ struct task_struct {
 
 	struct tlbflush_unmap_batch	tlb_ubc;
 
-	struct rcu_head			rcu;
+	union {
+		refcount_t		rcu_users;
+		struct rcu_head		rcu;
+	};
 
 	/* Cache last used pipe for splice(): */
 	struct pipe_inode_info		*splice_pipe;

commit 84da111de0b4be15bd500deff773f5116f39f7be
Merge: 227c3e9eb5cf 62974fc389b3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 21 10:07:42 2019 -0700

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull hmm updates from Jason Gunthorpe:
     "This is more cleanup and consolidation of the hmm APIs and the very
      strongly related mmu_notifier interfaces. Many places across the tree
      using these interfaces are touched in the process. Beyond that a
      cleanup to the page walker API and a few memremap related changes
      round out the series:
    
       - General improvement of hmm_range_fault() and related APIs, more
         documentation, bug fixes from testing, API simplification &
         consolidation, and unused API removal
    
       - Simplify the hmm related kconfigs to HMM_MIRROR and DEVICE_PRIVATE,
         and make them internal kconfig selects
    
       - Hoist a lot of code related to mmu notifier attachment out of
         drivers by using a refcount get/put attachment idiom and remove the
         convoluted mmu_notifier_unregister_no_release() and related APIs.
    
       - General API improvement for the migrate_vma API and revision of its
         only user in nouveau
    
       - Annotate mmu_notifiers with lockdep and sleeping region debugging
    
      Two series unrelated to HMM or mmu_notifiers came along due to
      dependencies:
    
       - Allow pagemap's memremap_pages family of APIs to work without
         providing a struct device
    
       - Make walk_page_range() and related use a constant structure for
         function pointers"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (75 commits)
      libnvdimm: Enable unit test infrastructure compile checks
      mm, notifier: Catch sleeping/blocking for !blockable
      kernel.h: Add non_block_start/end()
      drm/radeon: guard against calling an unpaired radeon_mn_unregister()
      csky: add missing brackets in a macro for tlb.h
      pagewalk: use lockdep_assert_held for locking validation
      pagewalk: separate function pointers from iterator data
      mm: split out a new pagewalk.h header from mm.h
      mm/mmu_notifiers: annotate with might_sleep()
      mm/mmu_notifiers: prime lockdep
      mm/mmu_notifiers: add a lockdep map for invalidate_range_start/end
      mm/mmu_notifiers: remove the __mmu_notifier_invalidate_range_start/end exports
      mm/hmm: hmm_range_fault() infinite loop
      mm/hmm: hmm_range_fault() NULL pointer bug
      mm/hmm: fix hmm_range_fault()'s handling of swapped out pages
      mm/mmu_notifiers: remove unregister_no_release
      RDMA/odp: remove ib_ucontext from ib_umem
      RDMA/odp: use mmu_notifier_get/put for 'struct ib_ucontext_per_mm'
      RDMA/mlx5: Use odp instead of mr->umem in pagefault_mr
      RDMA/mlx5: Use ib_umem_start instead of umem.address
      ...

commit 42fd8baab31f53bed2952485fcf0e92f244c5e55
Author: Qian Cai <cai@lca.pw>
Date:   Tue Sep 17 10:34:54 2019 -0400

    sched/core: Convert vcpu_is_preempted() from macro to an inline function
    
    Clang reports this warning:
    
      kernel/locking/osq_lock.c:25:19: warning: unused function 'node_cpu' [-Wunused-function]
    
    due to osq_lock() calling vcpu_is_preempted(node_cpu(node->prev))), but
    vcpu_is_preempted() is compiled away. Fix it by converting the dummy
    vcpu_is_preempted() from a macro to a proper static inline function.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: rostedt@goodmis.org
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/1568730894-10483-1-git-send-email-cai@lca.pw
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f0edee94834a..e2e91960d79f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1856,7 +1856,10 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
  * running or not.
  */
 #ifndef vcpu_is_preempted
-# define vcpu_is_preempted(cpu)	false
+static inline bool vcpu_is_preempted(int cpu)
+{
+	return false;
+}
 #endif
 
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);

commit 7f2444d38f6bbfa12bc15e2533d8f9daa85ca02b
Merge: c5f12fdb8bd8 77b4b5420422
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:35:15 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core timer updates from Thomas Gleixner:
     "Timers and timekeeping updates:
    
       - A large overhaul of the posix CPU timer code which is a preparation
         for moving the CPU timer expiry out into task work so it can be
         properly accounted on the task/process.
    
         An update to the bogus permission checks will come later during the
         merge window as feedback was not complete before heading of for
         travel.
    
       - Switch the timerqueue code to use cached rbtrees and get rid of the
         homebrewn caching of the leftmost node.
    
       - Consolidate hrtimer_init() + hrtimer_init_sleeper() calls into a
         single function
    
       - Implement the separation of hrtimers to be forced to expire in hard
         interrupt context even when PREEMPT_RT is enabled and mark the
         affected timers accordingly.
    
       - Implement a mechanism for hrtimers and the timer wheel to protect
         RT against priority inversion and live lock issues when a (hr)timer
         which should be canceled is currently executing the callback.
         Instead of infinitely spinning, the task which tries to cancel the
         timer blocks on a per cpu base expiry lock which is held and
         released by the (hr)timer expiry code.
    
       - Enable the Hyper-V TSC page based sched_clock for Hyper-V guests
         resulting in faster access to timekeeping functions.
    
       - Updates to various clocksource/clockevent drivers and their device
         tree bindings.
    
       - The usual small improvements all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      posix-cpu-timers: Fix permission check regression
      posix-cpu-timers: Always clear head pointer on dequeue
      hrtimer: Add a missing bracket and hide `migration_base' on !SMP
      posix-cpu-timers: Make expiry_active check actually work correctly
      posix-timers: Unbreak CONFIG_POSIX_TIMERS=n build
      tick: Mark sched_timer to expire in hard interrupt context
      hrtimer: Add kernel doc annotation for HRTIMER_MODE_HARD
      x86/hyperv: Hide pv_ops access for CONFIG_PARAVIRT=n
      posix-cpu-timers: Utilize timerqueue for storage
      posix-cpu-timers: Move state tracking to struct posix_cputimers
      posix-cpu-timers: Deduplicate rlimit handling
      posix-cpu-timers: Remove pointless comparisons
      posix-cpu-timers: Get rid of 64bit divisions
      posix-cpu-timers: Consolidate timer expiry further
      posix-cpu-timers: Get rid of zero checks
      rlimit: Rewrite non-sensical RLIMIT_CPU comment
      posix-cpu-timers: Respect INFINITY for hard RTTIME limit
      posix-cpu-timers: Switch thread group sampling to array
      posix-cpu-timers: Restructure expiry array
      posix-cpu-timers: Remove cputime_expires
      ...

commit 563c4f85f9f0d63b712081d5b4522152cdcb8b6b
Merge: 4adcdcea717c 09c7e8b21d67
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Sep 16 14:04:28 2019 +0200

    Merge branch 'sched/rt' into sched/core, to pick up -rt changes
    
    Pick up the first couple of patches working towards PREEMPT_RT.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 312364f3534cc974b79a96d062bde2386315201f
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Mon Aug 26 22:14:23 2019 +0200

    kernel.h: Add non_block_start/end()
    
    In some special cases we must not block, but there's not a spinlock,
    preempt-off, irqs-off or similar critical section already that arms the
    might_sleep() debug checks. Add a non_block_start/end() pair to annotate
    these.
    
    This will be used in the oom paths of mmu-notifiers, where blocking is not
    allowed to make sure there's forward progress. Quoting Michal:
    
    "The notifier is called from quite a restricted context - oom_reaper -
    which shouldn't depend on any locks or sleepable conditionals. The code
    should be swift as well but we mostly do care about it to make a forward
    progress. Checking for sleepable context is the best thing we could come
    up with that would describe these demands at least partially."
    
    Peter also asked whether we want to catch spinlocks on top, but Michal
    said those are less of a problem because spinlocks can't have an indirect
    dependency upon the page allocator and hence close the loop with the oom
    reaper.
    
    Suggested by Michal Hocko.
    
    Link: https://lore.kernel.org/r/20190826201425.17547-4-daniel.vetter@ffwll.ch
    Acked-by: Christian KÃ¶nig <christian.koenig@amd.com> (v1)
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9f51932bd543..c5630f3dca1f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -974,6 +974,10 @@ struct task_struct {
 	struct mutex_waiter		*blocked_on;
 #endif
 
+#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
+	int				non_block_count;
+#endif
+
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int			irq_events;
 	unsigned long			hardirq_enable_ip;

commit 3a245c0f110e2bfcf7f2cd2248a29005c78999e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:06 2019 +0200

    posix-cpu-timers: Move expiry cache into struct posix_cputimers
    
    The expiry cache belongs into the posix_cputimers container where the other
    cpu timers information is.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20190821192921.014444012@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 37c39df9b186..8cc8e323093f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -246,11 +246,6 @@ struct prev_cputime {
 #endif
 };
 
-/* Alternate field names when used on cache expirations: */
-#define virt_exp			utime
-#define prof_exp			stime
-#define sched_exp			sum_exec_runtime
-
 enum vtime_state {
 	/* Task is sleeping or running in a CPU with VTIME inactive: */
 	VTIME_INACTIVE = 0,
@@ -862,9 +857,6 @@ struct task_struct {
 	unsigned long			min_flt;
 	unsigned long			maj_flt;
 
-#ifdef CONFIG_POSIX_TIMERS
-	struct task_cputime		cputime_expires;
-#endif
 	/* Empty if CONFIG_POSIX_CPUTIMERS=n */
 	struct posix_cputimers		posix_cputimers;
 

commit 9eacb5c7e6607aba00a7322b21cad83fc8b101c8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:05 2019 +0200

    sched: Move struct task_cputime to types.h
    
    For upcoming posix-timer changes to avoid include recursion hell.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190821192920.909530418@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fde844a3b86e..37c39df9b186 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -25,6 +25,7 @@
 #include <linux/resource.h>
 #include <linux/latencytop.h>
 #include <linux/sched/prio.h>
+#include <linux/sched/types.h>
 #include <linux/signal_types.h>
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
@@ -245,22 +246,6 @@ struct prev_cputime {
 #endif
 };
 
-/**
- * struct task_cputime - collected CPU time counts
- * @utime:		time spent in user mode, in nanoseconds
- * @stime:		time spent in kernel mode, in nanoseconds
- * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
- *
- * This structure groups together three kinds of CPU time that are tracked for
- * threads and thread groups.  Most things considering CPU time want to group
- * these counts together and treat all three of them in parallel.
- */
-struct task_cputime {
-	u64				utime;
-	u64				stime;
-	unsigned long long		sum_exec_runtime;
-};
-
 /* Alternate field names when used on cache expirations: */
 #define virt_exp			utime
 #define prof_exp			stime

commit 2b69942f9021bf75bd1b001f53bd2578361fadf3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:04 2019 +0200

    posix-cpu-timers: Create a container struct
    
    Per task/process data of posix CPU timers is all over the place which
    makes the code hard to follow and requires ifdeffery.
    
    Create a container to hold all this information in one place, so data is
    consolidated and the ifdeffery can be confined to the posix timer header
    file and removed from places like fork.
    
    As a first step, move the cpu_timers list head array into the new struct
    and clean up the initializers and simplify fork. The remaining #ifdef in
    fork will be removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20190821192920.819418976@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8dc1811487f5..fde844a3b86e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -28,6 +28,7 @@
 #include <linux/signal_types.h>
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
+#include <linux/posix-timers.h>
 #include <linux/rseq.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
@@ -878,8 +879,9 @@ struct task_struct {
 
 #ifdef CONFIG_POSIX_TIMERS
 	struct task_cputime		cputime_expires;
-	struct list_head		cpu_timers[3];
 #endif
+	/* Empty if CONFIG_POSIX_CPUTIMERS=n */
+	struct posix_cputimers		posix_cputimers;
 
 	/* Process credentials: */
 

commit c1a280b68d4e6b6db4a65aa7865c22d8789ddf09
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:37 2019 +0200

    sched/preempt: Use CONFIG_PREEMPTION where appropriate
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the preemption code, scheduler and init task over to use
    CONFIG_PREEMPTION.
    
    That's the first step towards RT in that area. The more complex changes are
    coming separately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.117528401@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9f51932bd543..6947516a2d3e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1767,7 +1767,7 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
  * value indicates whether a reschedule was done in fact.
  * cond_resched_lock() will drop the spinlock before scheduling,
  */
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 extern int _cond_resched(void);
 #else
 static inline int _cond_resched(void) { return 0; }
@@ -1796,12 +1796,12 @@ static inline void cond_resched_rcu(void)
 
 /*
  * Does a critical section need to be broken due to another
- * task waiting?: (technically does not depend on CONFIG_PREEMPT,
+ * task waiting?: (technically does not depend on CONFIG_PREEMPTION,
  * but a general need for low latency)
  */
 static inline int spin_needbreak(spinlock_t *lock)
 {
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	return spin_is_contended(lock);
 #else
 	return 0;

commit f9a25f776d780bfa3279f0b6e5f5cf3224997976
Author: Mathieu Poirier <mathieu.poirier@linaro.org>
Date:   Fri Jul 19 15:59:55 2019 +0200

    cpusets: Rebuild root domain deadline accounting information
    
    When the topology of root domains is modified by CPUset or CPUhotplug
    operations information about the current deadline bandwidth held in the
    root domain is lost.
    
    This patch addresses the issue by recalculating the lost deadline
    bandwidth information by circling through the deadline tasks held in
    CPUsets and adding their current load to the root domain they are
    associated with.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
    [ Various additional modifications. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: lizefan@huawei.com
    Cc: longman@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: rostedt@goodmis.org
    Cc: tj@kernel.org
    Cc: tommaso.cucinotta@santannapisa.it
    Link: https://lkml.kernel.org/r/20190719140000.31694-4-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9f51932bd543..b94ad92dfbe6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -295,6 +295,11 @@ enum uclamp_id {
 	UCLAMP_CNT
 };
 
+#ifdef CONFIG_SMP
+extern struct root_domain def_root_domain;
+extern struct mutex sched_domains_mutex;
+#endif
+
 struct sched_info {
 #ifdef CONFIG_SCHED_INFO
 	/* Cumulative counters: */

commit cb361d8cdef69990f6b4504dc1fd9a594d983c97
Author: Jann Horn <jannh@google.com>
Date:   Tue Jul 16 17:20:47 2019 +0200

    sched/fair: Use RCU accessors consistently for ->numa_group
    
    The old code used RCU annotations and accessors inconsistently for
    ->numa_group, which can lead to use-after-frees and NULL dereferences.
    
    Let all accesses to ->numa_group use proper RCU helpers to prevent such
    issues.
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Fixes: 8c8a743c5087 ("sched/numa: Use {cpu, pid} to create task groups for shared faults")
    Link: https://lkml.kernel.org/r/20190716152047.14424-3-jannh@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8dc1811487f5..9f51932bd543 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1092,7 +1092,15 @@ struct task_struct {
 	u64				last_sum_exec_runtime;
 	struct callback_head		numa_work;
 
-	struct numa_group		*numa_group;
+	/*
+	 * This pointer is only modified for current in syscall and
+	 * pagefault context (and for tasks being destroyed), so it can be read
+	 * from any of the following contexts:
+	 *  - RCU read-side critical section
+	 *  - current->numa_group from everywhere
+	 *  - task's runqueue locked, task not running
+	 */
+	struct numa_group __rcu		*numa_group;
 
 	/*
 	 * numa_faults is an array split into four regions:

commit c236b6dd48dcf2ae6ed14b9068830eccc3e181e6
Merge: d44a62742dec 3b8c4a08a471
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 19:19:37 2019 -0700

    Merge tag 'keys-request-20190626' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull request_key improvements from David Howells:
     "These are all request_key()-related, including a fix and some improvements:
    
       - Fix the lack of a Link permission check on a key found by
         request_key(), thereby enabling request_key() to link keys that
         don't grant this permission to the target keyring (which must still
         grant Write permission).
    
         Note that the key must be in the caller's keyrings already to be
         found.
    
       - Invalidate used request_key authentication keys rather than
         revoking them, so that they get cleaned up immediately rather than
         hanging around till the expiry time is passed.
    
       - Move the RCU locks outwards from the keyring search functions so
         that a request_key_rcu() can be provided. This can be called in RCU
         mode, so it can't sleep and can't upcall - but it can be called
         from LOOKUP_RCU pathwalk mode.
    
       - Cache the latest positive result of request_key*() temporarily in
         task_struct so that filesystems that make a lot of request_key()
         calls during pathwalk can take advantage of it to avoid having to
         redo the searching. This requires CONFIG_KEYS_REQUEST_CACHE=y.
    
         It is assumed that the key just found is likely to be used multiple
         times in each step in an RCU pathwalk, and is likely to be reused
         for the next step too.
    
         Note that the cleanup of the cache is done on TIF_NOTIFY_RESUME,
         just before userspace resumes, and on exit"
    
    * tag 'keys-request-20190626' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      keys: Kill off request_key_async{,_with_auxdata}
      keys: Cache result of request_key*() temporarily in task_struct
      keys: Provide request_key_rcu()
      keys: Move the RCU locks outwards from the keyring search functions
      keys: Invalidate used request_key authentication keys
      keys: Fix request_key() lack of Link perm check on found key

commit dad1c12ed831a7a89cc01e5582cd0b81a4be7f19
Merge: 090bc5a2a914 af24bde8df20
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:39:53 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Remove the unused per rq load array and all its infrastructure, by
       Dietmar Eggemann.
    
     - Add utilization clamping support by Patrick Bellasi. This is a
       refinement of the energy aware scheduling framework with support for
       boosting of interactive and capping of background workloads: to make
       sure critical GUI threads get maximum frequency ASAP, and to make
       sure background processing doesn't unnecessarily move to cpufreq
       governor to higher frequencies and less energy efficient CPU modes.
    
     - Add the bare minimum of tracepoints required for LISA EAS regression
       testing, by Qais Yousef - which allows automated testing of various
       power management features, including energy aware scheduling.
    
     - Restructure the former tsk_nr_cpus_allowed() facility that the -rt
       kernel used to modify the scheduler's CPU affinity logic such as
       migrate_disable() - introduce the task->cpus_ptr value instead of
       taking the address of &task->cpus_allowed directly - by Sebastian
       Andrzej Siewior.
    
     - Misc optimizations, fixes, cleanups and small enhancements - see the
       Git log for details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      sched/uclamp: Add uclamp support to energy_compute()
      sched/uclamp: Add uclamp_util_with()
      sched/cpufreq, sched/uclamp: Add clamps for FAIR and RT tasks
      sched/uclamp: Set default clamps for RT tasks
      sched/uclamp: Reset uclamp values on RESET_ON_FORK
      sched/uclamp: Extend sched_setattr() to support utilization clamping
      sched/core: Allow sched_setattr() to use the current policy
      sched/uclamp: Add system default clamps
      sched/uclamp: Enforce last task's UCLAMP_MAX
      sched/uclamp: Add bucket local max tracking
      sched/uclamp: Add CPU's clamp buckets refcounting
      sched/fair: Rename weighted_cpuload() to cpu_runnable_load()
      sched/debug: Export the newly added tracepoints
      sched/debug: Add sched_overutilized tracepoint
      sched/debug: Add new tracepoint to track PELT at se level
      sched/debug: Add new tracepoints to track PELT at rq level
      sched/debug: Add a new sched_trace_*() helper functions
      sched/autogroup: Make autogroup_path() always available
      sched/wait: Deduplicate code with do-while
      sched/topology: Remove unused 'sd' parameter from arch_scale_cpu_capacity()
      ...

commit 46f1ec23a46940846f86a91c46f7119d8a8b5de1
Merge: 223cea6a4f05 83086d654dd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 15:45:14 2019 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The changes in this cycle are:
    
       - RCU flavor consolidation cleanups and optmizations
    
       - Documentation updates
    
       - Miscellaneous fixes
    
       - SRCU updates
    
       - RCU-sync flavor consolidation
    
       - Torture-test updates
    
       - Linux-kernel memory-consistency-model updates, most notably the
         addition of plain C-language accesses"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (61 commits)
      tools/memory-model: Improve data-race detection
      tools/memory-model: Change definition of rcu-fence
      tools/memory-model: Expand definition of barrier
      tools/memory-model: Do not use "herd" to refer to "herd7"
      tools/memory-model: Fix comment in MP+poonceonces.litmus
      Documentation: atomic_t.txt: Explain ordering provided by smp_mb__{before,after}_atomic()
      rcu: Don't return a value from rcu_assign_pointer()
      rcu: Force inlining of rcu_read_lock()
      rcu: Fix irritating whitespace error in rcu_assign_pointer()
      rcu: Upgrade sync_exp_work_done() to smp_mb()
      rcutorture: Upper case solves the case of the vanishing NULL pointer
      torture: Suppress propagating trace_printk() warning
      rcutorture: Dump trace buffer for callback pipe drain failures
      torture: Add --trust-make to suppress "make clean"
      torture: Make --cpus override idleness calculations
      torture: Run kernel build in source directory
      torture: Add function graph-tracing cheat sheet
      torture: Capture qemu output
      rcutorture: Tweak kvm options
      rcutorture: Add trivial RCU implementation
      ...

commit a509a7cd79747074a2c018a45bbbc52d1f4aed44
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:07 2019 +0100

    sched/uclamp: Extend sched_setattr() to support utilization clamping
    
    The SCHED_DEADLINE scheduling class provides an advanced and formal
    model to define tasks requirements that can translate into proper
    decisions for both task placements and frequencies selections. Other
    classes have a more simplified model based on the POSIX concept of
    priorities.
    
    Such a simple priority based model however does not allow to exploit
    most advanced features of the Linux scheduler like, for example, driving
    frequencies selection via the schedutil cpufreq governor. However, also
    for non SCHED_DEADLINE tasks, it's still interesting to define tasks
    properties to support scheduler decisions.
    
    Utilization clamping exposes to user-space a new set of per-task
    attributes the scheduler can use as hints about the expected/required
    utilization for a task. This allows to implement a "proactive" per-task
    frequency control policy, a more advanced policy than the current one
    based just on "passive" measured task utilization. For example, it's
    possible to boost interactive tasks (e.g. to get better performance) or
    cap background tasks (e.g. to be more energy/thermal efficient).
    
    Introduce a new API to set utilization clamping values for a specified
    task by extending sched_setattr(), a syscall which already allows to
    define task specific properties for different scheduling classes. A new
    pair of attributes allows to specify a minimum and maximum utilization
    the scheduler can consider for a task.
    
    Do that by validating the required clamp values before and then applying
    the required changes using _the_ same pattern already in use for
    __setscheduler(). This ensures that the task is re-enqueued with the new
    clamp values.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-7-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5485f411e8e1..1113dd4706ae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -587,6 +587,7 @@ struct sched_dl_entity {
  * @value:		clamp value "assigned" to a se
  * @bucket_id:		bucket index corresponding to the "assigned" value
  * @active:		the se is currently refcounted in a rq's bucket
+ * @user_defined:	the requested clamp value comes from user-space
  *
  * The bucket_id is the index of the clamp bucket matching the clamp value
  * which is pre-computed and stored to avoid expensive integer divisions from
@@ -596,11 +597,19 @@ struct sched_dl_entity {
  * which can be different from the clamp value "requested" from user-space.
  * This allows to know a task is refcounted in the rq's bucket corresponding
  * to the "effective" bucket_id.
+ *
+ * The user_defined bit is set whenever a task has got a task-specific clamp
+ * value requested from userspace, i.e. the system defaults apply to this task
+ * just as a restriction. This allows to relax default clamps when a less
+ * restrictive task-specific value has been requested, thus allowing to
+ * implement a "nice" semantic. For example, a task running with a 20%
+ * default boost can still drop its own boosting to 0%.
  */
 struct uclamp_se {
 	unsigned int value		: bits_per(SCHED_CAPACITY_SCALE);
 	unsigned int bucket_id		: bits_per(UCLAMP_BUCKETS);
 	unsigned int active		: 1;
+	unsigned int user_defined	: 1;
 };
 #endif /* CONFIG_UCLAMP_TASK */
 

commit e8f14172c6b11e9a86c65532497087f8eb0f91b1
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:05 2019 +0100

    sched/uclamp: Add system default clamps
    
    Tasks without a user-defined clamp value are considered not clamped
    and by default their utilization can have any value in the
    [0..SCHED_CAPACITY_SCALE] range.
    
    Tasks with a user-defined clamp value are allowed to request any value
    in that range, and the required clamp is unconditionally enforced.
    However, a "System Management Software" could be interested in limiting
    the range of clamp values allowed for all tasks.
    
    Add a privileged interface to define a system default configuration via:
    
      /proc/sys/kernel/sched_uclamp_util_{min,max}
    
    which works as an unconditional clamp range restriction for all tasks.
    
    With the default configuration, the full SCHED_CAPACITY_SCALE range of
    values is allowed for each clamp index. Otherwise, the task-specific
    clamp is capped by the corresponding system default value.
    
    Do that by tracking, for each task, the "effective" clamp value and
    bucket the task has been refcounted in at enqueue time. This
    allows to lazy aggregate "requested" and "system default" values at
    enqueue time and simplifies refcounting updates at dequeue time.
    
    The cached bucket ids are used to avoid (relatively) more expensive
    integer divisions every time a task is enqueued.
    
    An active flag is used to report when the "effective" value is valid and
    thus the task is actually refcounted in the corresponding rq's bucket.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-5-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 80235bcd05f2..5485f411e8e1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -586,14 +586,21 @@ struct sched_dl_entity {
  * Utilization clamp for a scheduling entity
  * @value:		clamp value "assigned" to a se
  * @bucket_id:		bucket index corresponding to the "assigned" value
+ * @active:		the se is currently refcounted in a rq's bucket
  *
  * The bucket_id is the index of the clamp bucket matching the clamp value
  * which is pre-computed and stored to avoid expensive integer divisions from
  * the fast path.
+ *
+ * The active bit is set whenever a task has got an "effective" value assigned,
+ * which can be different from the clamp value "requested" from user-space.
+ * This allows to know a task is refcounted in the rq's bucket corresponding
+ * to the "effective" bucket_id.
  */
 struct uclamp_se {
 	unsigned int value		: bits_per(SCHED_CAPACITY_SCALE);
 	unsigned int bucket_id		: bits_per(UCLAMP_BUCKETS);
+	unsigned int active		: 1;
 };
 #endif /* CONFIG_UCLAMP_TASK */
 
@@ -678,6 +685,9 @@ struct task_struct {
 	struct sched_dl_entity		dl;
 
 #ifdef CONFIG_UCLAMP_TASK
+	/* Clamp values requested for a scheduling entity */
+	struct uclamp_se		uclamp_req[UCLAMP_CNT];
+	/* Effective clamp values used for a scheduling entity */
 	struct uclamp_se		uclamp[UCLAMP_CNT];
 #endif
 

commit 69842cba9ace84849bb9b8edcdf2cefccd97901c
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:02 2019 +0100

    sched/uclamp: Add CPU's clamp buckets refcounting
    
    Utilization clamping allows to clamp the CPU's utilization within a
    [util_min, util_max] range, depending on the set of RUNNABLE tasks on
    that CPU. Each task references two "clamp buckets" defining its minimum
    and maximum (util_{min,max}) utilization "clamp values". A CPU's clamp
    bucket is active if there is at least one RUNNABLE tasks enqueued on
    that CPU and refcounting that bucket.
    
    When a task is {en,de}queued {on,from} a rq, the set of active clamp
    buckets on that CPU can change. If the set of active clamp buckets
    changes for a CPU a new "aggregated" clamp value is computed for that
    CPU. This is because each clamp bucket enforces a different utilization
    clamp value.
    
    Clamp values are always MAX aggregated for both util_min and util_max.
    This ensures that no task can affect the performance of other
    co-scheduled tasks which are more boosted (i.e. with higher util_min
    clamp) or less capped (i.e. with higher util_max clamp).
    
    A task has:
       task_struct::uclamp[clamp_id]::bucket_id
    to track the "bucket index" of the CPU's clamp bucket it refcounts while
    enqueued, for each clamp index (clamp_id).
    
    A runqueue has:
       rq::uclamp[clamp_id]::bucket[bucket_id].tasks
    to track how many RUNNABLE tasks on that CPU refcount each
    clamp bucket (bucket_id) of a clamp index (clamp_id).
    It also has a:
       rq::uclamp[clamp_id]::bucket[bucket_id].value
    to track the clamp value of each clamp bucket (bucket_id) of a clamp
    index (clamp_id).
    
    The rq::uclamp::bucket[clamp_id][] array is scanned every time it's
    needed to find a new MAX aggregated clamp value for a clamp_id. This
    operation is required only when it's dequeued the last task of a clamp
    bucket tracking the current MAX aggregated clamp value. In this case,
    the CPU is either entering IDLE or going to schedule a less boosted or
    more clamped task.
    The expected number of different clamp values configured at build time
    is small enough to fit the full unordered array into a single cache
    line, for configurations of up to 7 buckets.
    
    Add to struct rq the basic data structures required to refcount the
    number of RUNNABLE tasks for each clamp bucket. Add also the max
    aggregation required to update the rq's clamp value at each
    enqueue/dequeue event.
    
    Use a simple linear mapping of clamp values into clamp buckets.
    Pre-compute and cache bucket_id to avoid integer divisions at
    enqueue/dequeue time.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-2-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 044c023875e8..80235bcd05f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -283,6 +283,18 @@ struct vtime {
 	u64			gtime;
 };
 
+/*
+ * Utilization clamp constraints.
+ * @UCLAMP_MIN:	Minimum utilization
+ * @UCLAMP_MAX:	Maximum utilization
+ * @UCLAMP_CNT:	Utilization clamp constraints count
+ */
+enum uclamp_id {
+	UCLAMP_MIN = 0,
+	UCLAMP_MAX,
+	UCLAMP_CNT
+};
+
 struct sched_info {
 #ifdef CONFIG_SCHED_INFO
 	/* Cumulative counters: */
@@ -314,6 +326,10 @@ struct sched_info {
 # define SCHED_FIXEDPOINT_SHIFT		10
 # define SCHED_FIXEDPOINT_SCALE		(1L << SCHED_FIXEDPOINT_SHIFT)
 
+/* Increase resolution of cpu_capacity calculations */
+# define SCHED_CAPACITY_SHIFT		SCHED_FIXEDPOINT_SHIFT
+# define SCHED_CAPACITY_SCALE		(1L << SCHED_CAPACITY_SHIFT)
+
 struct load_weight {
 	unsigned long			weight;
 	u32				inv_weight;
@@ -562,6 +578,25 @@ struct sched_dl_entity {
 	struct hrtimer inactive_timer;
 };
 
+#ifdef CONFIG_UCLAMP_TASK
+/* Number of utilization clamp buckets (shorter alias) */
+#define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT
+
+/*
+ * Utilization clamp for a scheduling entity
+ * @value:		clamp value "assigned" to a se
+ * @bucket_id:		bucket index corresponding to the "assigned" value
+ *
+ * The bucket_id is the index of the clamp bucket matching the clamp value
+ * which is pre-computed and stored to avoid expensive integer divisions from
+ * the fast path.
+ */
+struct uclamp_se {
+	unsigned int value		: bits_per(SCHED_CAPACITY_SCALE);
+	unsigned int bucket_id		: bits_per(UCLAMP_BUCKETS);
+};
+#endif /* CONFIG_UCLAMP_TASK */
+
 union rcu_special {
 	struct {
 		u8			blocked;
@@ -642,6 +677,10 @@ struct task_struct {
 #endif
 	struct sched_dl_entity		dl;
 
+#ifdef CONFIG_UCLAMP_TASK
+	struct uclamp_se		uclamp[UCLAMP_CNT];
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* List of struct preempt_notifier: */
 	struct hlist_head		preempt_notifiers;

commit 3c93a0c04dfdcba199982b53b97488b1b1d90eff
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Tue Jun 4 12:14:55 2019 +0100

    sched/debug: Add a new sched_trace_*() helper functions
    
    The new functions allow modules to access internal data structures of
    unexported struct cfs_rq and struct rq to extract important information
    from the tracepoints to be introduced in later patches.
    
    While at it fix alphabetical order of struct declarations in sched.h
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavankumar Kondeti <pkondeti@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Uwe Kleine-Konig <u.kleine-koenig@pengutronix.de>
    Link: https://lkml.kernel.org/r/20190604111459.2862-3-qais.yousef@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1b2590a8d038..044c023875e8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -35,6 +35,7 @@ struct audit_context;
 struct backing_dev_info;
 struct bio_list;
 struct blk_plug;
+struct capture_control;
 struct cfs_rq;
 struct fs_struct;
 struct futex_pi_state;
@@ -47,8 +48,9 @@ struct pid_namespace;
 struct pipe_inode_info;
 struct rcu_node;
 struct reclaim_state;
-struct capture_control;
 struct robust_list_head;
+struct root_domain;
+struct rq;
 struct sched_attr;
 struct sched_param;
 struct seq_file;
@@ -1920,4 +1922,16 @@ static inline void rseq_syscall(struct pt_regs *regs)
 
 #endif
 
+const struct sched_avg *sched_trace_cfs_rq_avg(struct cfs_rq *cfs_rq);
+char *sched_trace_cfs_rq_path(struct cfs_rq *cfs_rq, char *str, int len);
+int sched_trace_cfs_rq_cpu(struct cfs_rq *cfs_rq);
+
+const struct sched_avg *sched_trace_rq_avg_rt(struct rq *rq);
+const struct sched_avg *sched_trace_rq_avg_dl(struct rq *rq);
+const struct sched_avg *sched_trace_rq_avg_irq(struct rq *rq);
+
+int sched_trace_rq_cpu(struct rq *rq);
+
+const struct cpumask *sched_trace_rd_span(struct root_domain *rd);
+
 #endif

commit 7743c48e54ee9be9c799cbf3b8e3e9f2b8d19e72
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 19 16:10:15 2019 +0100

    keys: Cache result of request_key*() temporarily in task_struct
    
    If a filesystem uses keys to hold authentication tokens, then it needs a
    token for each VFS operation that might perform an authentication check -
    either by passing it to the server, or using to perform a check based on
    authentication data cached locally.
    
    For open files this isn't a problem, since the key should be cached in the
    file struct since it represents the subject performing operations on that
    file descriptor.
    
    During pathwalk, however, there isn't anywhere to cache the key, except
    perhaps in the nameidata struct - but that isn't exposed to the
    filesystems.  Further, a pathwalk can incur a lot of operations, calling
    one or more of the following, for instance:
    
            ->lookup()
            ->permission()
            ->d_revalidate()
            ->d_automount()
            ->get_acl()
            ->getxattr()
    
    on each dentry/inode it encounters - and each one may need to call
    request_key().  And then, at the end of pathwalk, it will call the actual
    operation:
    
            ->mkdir()
            ->mknod()
            ->getattr()
            ->open()
            ...
    
    which may need to go and get the token again.
    
    However, it is very likely that all of the operations on a single
    dentry/inode - and quite possibly a sequence of them - will all want to use
    the same authentication token, which suggests that caching it would be a
    good idea.
    
    To this end:
    
     (1) Make it so that a positive result of request_key() and co. that didn't
         require upcalling to userspace is cached temporarily in task_struct.
    
     (2) The cache is 1 deep, so a new result displaces the old one.
    
     (3) The key is released by exit and by notify-resume.
    
     (4) The cache is cleared in a newly forked process.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11837410690f..e5f18857dd53 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -831,6 +831,11 @@ struct task_struct {
 	/* Effective (overridable) subjective task credentials (COW): */
 	const struct cred __rcu		*cred;
 
+#ifdef CONFIG_KEYS
+	/* Cached requested key. */
+	struct key			*cached_requested_key;
+#endif
+
 	/*
 	 * executable name, excluding path.
 	 *

commit 4ecf0a43e729a7e641d800c294faabe87378fc05
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sat Jun 8 12:13:57 2019 +0200

    processor: get rid of cpu_relax_yield
    
    stop_machine is the only user left of cpu_relax_yield. Given that it
    now has special semantics which are tied to stop_machine introduce a
    weak stop_machine_yield function which architectures can override, and
    get rid of the generic cpu_relax_yield implementation.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1f9f3160da7e..911675416b05 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1518,10 +1518,6 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpuma
 }
 #endif
 
-#ifndef cpu_relax_yield
-#define cpu_relax_yield(cpumask) cpu_relax()
-#endif
-
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);

commit 38f2c691a4b3e89d476f8e8350d1ca299974b89d
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri May 17 12:50:42 2019 +0200

    s390: improve wait logic of stop_machine
    
    The stop_machine loop to advance the state machine and to wait for all
    affected CPUs to check-in calls cpu_relax_yield in a tight loop until
    the last missing CPUs acknowledged the state transition.
    
    On a virtual system where not all logical CPUs are backed by real CPUs
    all the time it can take a while for all CPUs to check-in. With the
    current definition of cpu_relax_yield a diagnose 0x44 is done which
    tells the hypervisor to schedule *some* other CPU. That can be any
    CPU and not necessarily one of the CPUs that need to run in order to
    advance the state machine. This can lead to a pretty bad diagnose 0x44
    storm until the last missing CPU finally checked-in.
    
    Replace the undirected cpu_relax_yield based on diagnose 0x44 with a
    directed yield. Each CPU in the wait loop will pick up the next CPU
    in the cpumask of stop_machine. The diagnose 0x9c is used to tell the
    hypervisor to run this next CPU instead of the current one. If there
    is only a limited number of real CPUs backing the virtual CPUs we
    end up with the real CPUs passed around in a round-robin fashion.
    
    [heiko.carstens@de.ibm.com]:
        Use cpumask_next_wrap as suggested by Peter Zijlstra.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11837410690f..1f9f3160da7e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1519,7 +1519,7 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpuma
 #endif
 
 #ifndef cpu_relax_yield
-#define cpu_relax_yield() cpu_relax()
+#define cpu_relax_yield(cpumask) cpu_relax()
 #endif
 
 extern int yield_to(struct task_struct *p, bool preempt);

commit 3bd3706251ee8ab67e69d9340ac2abdca217e733
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Apr 23 16:26:36 2019 +0200

    sched/core: Provide a pointer to the valid CPU mask
    
    In commit:
    
      4b53a3412d66 ("sched/core: Remove the tsk_nr_cpus_allowed() wrapper")
    
    the tsk_nr_cpus_allowed() wrapper was removed. There was not
    much difference in !RT but in RT we used this to implement
    migrate_disable(). Within a migrate_disable() section the CPU mask is
    restricted to single CPU while the "normal" CPU mask remains untouched.
    
    As an alternative implementation Ingo suggested to use:
    
            struct task_struct {
                    const cpumask_t         *cpus_ptr;
                    cpumask_t               cpus_mask;
            };
    with
            t->cpus_ptr = &t->cpus_mask;
    
    In -RT we then can switch the cpus_ptr to:
    
            t->cpus_ptr = &cpumask_of(task_cpu(p));
    
    in a migration disabled region. The rules are simple:
    
     - Code that 'uses' ->cpus_allowed would use the pointer.
     - Code that 'modifies' ->cpus_allowed would use the direct mask.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190423142636.14347-1-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11837410690f..1b2590a8d038 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -651,7 +651,8 @@ struct task_struct {
 
 	unsigned int			policy;
 	int				nr_cpus_allowed;
-	cpumask_t			cpus_allowed;
+	const cpumask_t			*cpus_ptr;
+	cpumask_t			cpus_mask;
 
 #ifdef CONFIG_PREEMPT_RCU
 	int				rcu_read_lock_nesting;
@@ -1399,7 +1400,7 @@ extern struct pid *cad_pid;
 #define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */
 #define PF_MEMSTALL		0x01000000	/* Stalled due to lack of memory */
 #define PF_UMH			0x02000000	/* I'm an Usermodehelper process */
-#define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
+#define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_mask */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
 #define PF_MEMALLOC_NOCMA	0x10000000	/* All allocation request will have _GFP_MOVABLE cleared */
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */

commit 23634ebc1d946f19eb112d4455c1d84948875e31
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sun Mar 24 15:25:51 2019 -0700

    rcu: Check for wakeup-safe conditions in rcu_read_unlock_special()
    
    When RCU core processing is offloaded from RCU_SOFTIRQ to the rcuc
    kthreads, a full and unconditional wakeup is required to initiate RCU
    core processing.  In contrast, when RCU core processing is carried
    out by RCU_SOFTIRQ, a raise_softirq() suffices.  Of course, there are
    situations where raise_softirq() does a full wakeup, but these do not
    occur with normal usage of rcu_read_unlock().
    
    The reason that full wakeups can be problematic is that the scheduler
    sometimes invokes rcu_read_unlock() with its pi or rq locks held,
    which can of course result in deadlock in CONFIG_PREEMPT=y kernels when
    rcu_read_unlock() invokes the scheduler.  Scheduler invocations can happen
    in the following situations: (1) The just-ended reader has been subjected
    to RCU priority boosting, in which case rcu_read_unlock() must deboost,
    (2) Interrupts were disabled across the call to rcu_read_unlock(), so
    the quiescent state must be deferred, requiring a wakeup of the rcuc
    kthread corresponding to the current CPU.
    
    Now, the scheduler may hold one of its locks across rcu_read_unlock()
    only if preemption has been disabled across the entire RCU read-side
    critical section, which in the days prior to RCU flavor consolidation
    meant that rcu_read_unlock() never needed to do wakeups.  However, this
    is no longer the case for any but the first rcu_read_unlock() following a
    condition (e.g., preempted RCU reader) requiring special rcu_read_unlock()
    attention.  For example, an RCU read-side critical section might be
    preempted, but preemption might be disabled across the rcu_read_unlock().
    The rcu_read_unlock() must defer the quiescent state, and therefore
    leaves the task queued on its leaf rcu_node structure.  If a scheduler
    interrupt occurs, the scheduler might well invoke rcu_read_unlock() with
    one of its locks held.  However, the preempted task is still queued, so
    rcu_read_unlock() will attempt to defer the quiescent state once more.
    When RCU core processing is carried out by RCU_SOFTIRQ, this works just
    fine: The raise_softirq() function simply sets a bit in a per-CPU mask
    and the RCU core processing will be undertaken upon return from interrupt.
    
    Not so when RCU core processing is carried out by the rcuc kthread: In this
    case, the required wakeup can result in deadlock.
    
    The initial solution to this problem was to use set_tsk_need_resched() and
    set_preempt_need_resched() to force a future context switch, which allows
    rcu_preempt_note_context_switch() to report the deferred quiescent state
    to RCU's core processing.  Unfortunately for expedited grace periods,
    there can be a significant delay between the call for a context switch
    and the actual context switch.
    
    This commit therefore introduces a ->deferred_qs flag to the task_struct
    structure's rcu_special structure.  This flag is initially false, and
    is set to true by the first call to rcu_read_unlock() requiring special
    attention, then finally reset back to false when the quiescent state is
    finally reported.  Then rcu_read_unlock() attempts full wakeups only when
    ->deferred_qs is false, that is, on the first rcu_read_unlock() requiring
    special attention.  Note that a chain of RCU readers linked by some other
    sort of reader may find that a later rcu_read_unlock() is once again able
    to do a full wakeup, courtesy of an intervening preemption:
    
            rcu_read_lock();
            /* preempted */
            local_irq_disable();
            rcu_read_unlock(); /* Can do full wakeup, sets ->deferred_qs. */
            rcu_read_lock();
            local_irq_enable();
            preempt_disable()
            rcu_read_unlock(); /* Cannot do full wakeup, ->deferred_qs set. */
            rcu_read_lock();
            preempt_enable();
            /* preempted, >deferred_qs reset. */
            local_irq_disable();
            rcu_read_unlock(); /* Can again do full wakeup, sets ->deferred_qs. */
    
    Such linked RCU readers do not yet seem to appear in the Linux kernel, and
    it is probably best if they don't.  However, RCU needs to handle them, and
    some variations on this theme could make even raise_softirq() unsafe due to
    the possibility of its doing a full wakeup.  This commit therefore also
    avoids invoking raise_softirq() when the ->deferred_qs set flag is set.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11837410690f..942a44c1b8eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -565,7 +565,7 @@ union rcu_special {
 		u8			blocked;
 		u8			need_qs;
 		u8			exp_hint; /* Hint for performance. */
-		u8			pad; /* No garbage from compiler! */
+		u8			deferred_qs;
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };

commit 8af0c18af1425fc70686c0fdcfc0072cd8431aa0
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Tue May 14 15:41:12 2019 -0700

    include/: refactor headers to allow kthread.h inclusion in psi_types.h
    
    kthread.h can't be included in psi_types.h because it creates a circular
    inclusion with kthread.h eventually including psi_types.h and
    complaining on kthread structures not being defined because they are
    defined further in the kthread.h.  Resolve this by removing psi_types.h
    inclusion from the headers included from kthread.h.
    
    Link: http://lkml.kernel.org/r/20190319235619.260832-7-surenb@google.com
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a2cd15855bad..11837410690f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -26,7 +26,6 @@
 #include <linux/latencytop.h>
 #include <linux/sched/prio.h>
 #include <linux/signal_types.h>
-#include <linux/psi_types.h>
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
 #include <linux/rseq.h>

commit abde77eb5c66b2f98539c4644b54f34b7e179e6b
Merge: 23c970608a09 f2b31bb59824
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 9 13:52:12 2019 -0700

    Merge branch 'for-5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "This includes Roman's cgroup2 freezer implementation.
    
      It's a separate machanism from cgroup1 freezer. Instead of blocking
      user tasks in arbitrary uninterruptible sleeps, the new implementation
      extends jobctl stop - frozen tasks are trapped in jobctl stop until
      thawed and can be killed and ptraced. Lots of thanks to Oleg for
      sheperding the effort.
    
      Other than that, there are a few trivial changes"
    
    * 'for-5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: never call do_group_exit() with task->frozen bit set
      kernel: cgroup: fix misuse of %x
      cgroup: get rid of cgroup_freezer_frozen_exit()
      cgroup: prevent spurious transition into non-frozen state
      cgroup: Remove unused cgrp variable
      cgroup: document cgroup v2 freezer interface
      cgroup: add tracing points for cgroup v2 freezer
      cgroup: make TRACE_CGROUP_PATH irq-safe
      kselftests: cgroup: add freezer controller self-tests
      kselftests: cgroup: don't fail on cg_kill_all() error in cg_destroy()
      cgroup: cgroup v2 freezer
      cgroup: protect cgroup->nr_(dying_)descendants by css_set_lock
      cgroup: implement __cgroup_task_count() helper
      cgroup: rename freezer.c into legacy_freezer.c
      cgroup: remove extra cgroup_migrate_finish() call

commit 76f969e8948d82e78e1bc4beb6b9465908e74873
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 19 10:03:04 2019 -0700

    cgroup: cgroup v2 freezer
    
    Cgroup v1 implements the freezer controller, which provides an ability
    to stop the workload in a cgroup and temporarily free up some
    resources (cpu, io, network bandwidth and, potentially, memory)
    for some other tasks. Cgroup v2 lacks this functionality.
    
    This patch implements freezer for cgroup v2.
    
    Cgroup v2 freezer tries to put tasks into a state similar to jobctl
    stop. This means that tasks can be killed, ptraced (using
    PTRACE_SEIZE*), and interrupted. It is possible to attach to
    a frozen task, get some information (e.g. read registers) and detach.
    It's also possible to migrate a frozen tasks to another cgroup.
    
    This differs cgroup v2 freezer from cgroup v1 freezer, which mostly
    tried to imitate the system-wide freezer. However uninterruptible
    sleep is fine when all tasks are going to be frozen (hibernation case),
    it's not the acceptable state for some subset of the system.
    
    Cgroup v2 freezer is not supporting freezing kthreads.
    If a non-root cgroup contains kthread, the cgroup still can be frozen,
    but the kthread will remain running, the cgroup will be shown
    as non-frozen, and the notification will not be delivered.
    
    * PTRACE_ATTACH is not working because non-fatal signal delivery
    is blocked in frozen state.
    
    There are some interface differences between cgroup v1 and cgroup v2
    freezer too, which are required to conform the cgroup v2 interface
    design principles:
    1) There is no separate controller, which has to be turned on:
    the functionality is always available and is represented by
    cgroup.freeze and cgroup.events cgroup control files.
    2) The desired state is defined by the cgroup.freeze control file.
    Any hierarchical configuration is allowed.
    3) The interface is asynchronous. The actual state is available
    using cgroup.events control file ("frozen" field). There are no
    dedicated transitional states.
    4) It's allowed to make any changes with the cgroup hierarchy
    (create new cgroups, remove old cgroups, move tasks between cgroups)
    no matter if some cgroups are frozen.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    No-objection-from-me-by: Oleg Nesterov <oleg@redhat.com>
    Cc: kernel-team@fb.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1549584a1538..45b2199bd683 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -726,6 +726,8 @@ struct task_struct {
 #ifdef CONFIG_CGROUPS
 	/* disallow userland-initiated cgroup migration */
 	unsigned			no_cgroup_migration:1;
+	/* task is frozen/stopped (used by the cgroup freezer) */
+	unsigned			frozen:1;
 #endif
 #ifdef CONFIG_BLK_CGROUP
 	/* to be used once the psi infrastructure lands upstream. */

commit 83b0b15bcb0f700e7c1d070aae2e7841170a4c33
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Tue Mar 5 14:47:54 2019 -0500

    rseq: Remove superfluous rseq_len from task_struct
    
    The rseq system call, when invoked with flags of "0" or
    "RSEQ_FLAG_UNREGISTER" values, expects the rseq_len parameter to
    be equal to sizeof(struct rseq), which is fixed-size and fixed-layout,
    specified in uapi linux/rseq.h.
    
    Expecting a fixed size for rseq_len is a design choice that ensures
    multiple libraries and application defining __rseq_abi in the same
    process agree on its exact size.
    
    Considering that this size is and will always be the same value, there
    is no point in saving this value within task_struct rseq_len. Remove
    this field from task_struct.
    
    No change in functionality intended.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ben Maurer <bmaurer@fb.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Lameter <cl@linux.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190305194755.2602-3-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1549584a1538..50606a6e73d6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1057,7 +1057,6 @@ struct task_struct {
 
 #ifdef CONFIG_RSEQ
 	struct rseq __user *rseq;
-	u32 rseq_len;
 	u32 rseq_sig;
 	/*
 	 * RmW on rseq_event_mask must be performed atomically
@@ -1855,12 +1854,10 @@ static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
 {
 	if (clone_flags & CLONE_THREAD) {
 		t->rseq = NULL;
-		t->rseq_len = 0;
 		t->rseq_sig = 0;
 		t->rseq_event_mask = 0;
 	} else {
 		t->rseq = current->rseq;
-		t->rseq_len = current->rseq_len;
 		t->rseq_sig = current->rseq_sig;
 		t->rseq_event_mask = current->rseq_event_mask;
 	}
@@ -1869,7 +1866,6 @@ static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
 static inline void rseq_execve(struct task_struct *t)
 {
 	t->rseq = NULL;
-	t->rseq_len = 0;
 	t->rseq_sig = 0;
 	t->rseq_event_mask = 0;
 }

commit be37f21a08ce65c7632c7f45e1755a4b07f278a0
Merge: 3ac96c30ccfa 131d34cb0795
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 12:20:11 2019 -0800

    Merge tag 'audit-pr-20190305' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit
    
    Pull audit updates from Paul Moore:
     "A lucky 13 audit patches for v5.1.
    
      Despite the rather large diffstat, most of the changes are from two
      bug fix patches that move code from one Kconfig option to another.
    
      Beyond that bit of churn, the remaining changes are largely cleanups
      and bug-fixes as we slowly march towards container auditing. It isn't
      all boring though, we do have a couple of new things: file
      capabilities v3 support, and expanded support for filtering on
      filesystems to solve problems with remote filesystems.
    
      All changes pass the audit-testsuite.  Please merge for v5.1"
    
    * tag 'audit-pr-20190305' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit:
      audit: mark expected switch fall-through
      audit: hide auditsc_get_stamp and audit_serial prototypes
      audit: join tty records to their syscall
      audit: remove audit_context when CONFIG_ AUDIT and not AUDITSYSCALL
      audit: remove unused actx param from audit_rule_match
      audit: ignore fcaps on umount
      audit: clean up AUDITSYSCALL prototypes and stubs
      audit: more filter PATH records keyed on filesystem magic
      audit: add support for fcaps v3
      audit: move loginuid and sessionid from CONFIG_AUDITSYSCALL to CONFIG_AUDIT
      audit: add syscall information to CONFIG_CHANGE records
      audit: hand taken context to audit_kill_trees for syscall logging
      audit: give a clue what CONFIG_CHANGE op was involved

commit 8dcd175bc3d50b78413c56d5b17d4bddd77412ef
Merge: afe6fe7036c6 fff04900ea79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 10:31:36 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - a few misc things
    
     - ocfs2 updates
    
     - most of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (159 commits)
      tools/testing/selftests/proc/proc-self-syscall.c: remove duplicate include
      proc: more robust bulk read test
      proc: test /proc/*/maps, smaps, smaps_rollup, statm
      proc: use seq_puts() everywhere
      proc: read kernel cpu stat pointer once
      proc: remove unused argument in proc_pid_lookup()
      fs/proc/thread_self.c: code cleanup for proc_setup_thread_self()
      fs/proc/self.c: code cleanup for proc_setup_self()
      proc: return exit code 4 for skipped tests
      mm,mremap: bail out earlier in mremap_to under map pressure
      mm/sparse: fix a bad comparison
      mm/memory.c: do_fault: avoid usage of stale vm_area_struct
      writeback: fix inode cgroup switching comment
      mm/huge_memory.c: fix "orig_pud" set but not used
      mm/hotplug: fix an imbalance with DEBUG_PAGEALLOC
      mm/memcontrol.c: fix bad line in comment
      mm/cma.c: cma_declare_contiguous: correct err handling
      mm/page_ext.c: fix an imbalance with kmemleak
      mm/compaction: pass pgdat to too_many_isolated() instead of zone
      mm: remove zone_lru_lock() function, access ->lru_lock directly
      ...

commit 45802da05e666a81b421422d3e302930c0e24e77
Merge: 203b6609e0ed ad01423aedaa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 08:14:05 2019 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - refcount conversions
    
       - Solve the rq->leaf_cfs_rq_list can of worms for real.
    
       - improve power-aware scheduling
    
       - add sysctl knob for Energy Aware Scheduling
    
       - documentation updates
    
       - misc other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      kthread: Do not use TIMER_IRQSAFE
      kthread: Convert worker lock to raw spinlock
      sched/fair: Use non-atomic cpumask_{set,clear}_cpu()
      sched/fair: Remove unused 'sd' parameter from select_idle_smt()
      sched/wait: Use freezable_schedule() when possible
      sched/fair: Prune, fix and simplify the nohz_balancer_kick() comment block
      sched/fair: Explain LLC nohz kick condition
      sched/fair: Simplify nohz_balancer_kick()
      sched/topology: Fix percpu data types in struct sd_data & struct s_data
      sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument
      sched/fair: Fix O(nr_cgroups) in the load balancing path
      sched/fair: Optimize update_blocked_averages()
      sched/fair: Fix insertion in rq->leaf_cfs_rq_list
      sched/fair: Add tmp_alone_branch assertion
      sched/core: Use READ_ONCE()/WRITE_ONCE() in move_queued_task()/task_rq_lock()
      sched/debug: Initialize sd_sysctl_cpus if !CONFIG_CPUMASK_OFFSTACK
      sched/pelt: Skip updating util_est when utilization is higher than CPU's capacity
      sched/fair: Update scale invariance of PELT
      sched/fair: Move the rq_of() helper function
      sched/core: Convert task_struct.stack_refcount to refcount_t
      ...

commit d7fefcc8de9147cc37d0c00df12e7ea4f77999b5
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Mar 5 15:47:40 2019 -0800

    mm/cma: add PF flag to force non cma alloc
    
    Patch series "mm/kvm/vfio/ppc64: Migrate compound pages out of CMA
    region", v8.
    
    ppc64 uses the CMA area for the allocation of guest page table (hash
    page table).  We won't be able to start guest if we fail to allocate
    hash page table.  We have observed hash table allocation failure because
    we failed to migrate pages out of CMA region because they were pinned.
    This happen when we are using VFIO.  VFIO on ppc64 pins the entire guest
    RAM.  If the guest RAM pages get allocated out of CMA region, we won't
    be able to migrate those pages.  The pages are also pinned for the
    lifetime of the guest.
    
    Currently we support migration of non-compound pages.  With THP and with
    the addition of hugetlb migration we can end up allocating compound
    pages from CMA region.  This patch series add support for migrating
    compound pages.
    
    This patch (of 4):
    
    Add PF_MEMALLOC_NOCMA which make sure any allocation in that context is
    marked non-movable and hence cannot be satisfied by CMA region.
    
    This is useful with get_user_pages_longterm where we want to take a page
    pin by migrating pages from CMA region.  Marking the section
    PF_MEMALLOC_NOCMA ensures that we avoid unnecessary page migration
    later.
    
    Link: http://lkml.kernel.org/r/20190114095438.32470-2-aneesh.kumar@linux.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ebfb34fb9b30..36ec6e7e8291 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1407,6 +1407,7 @@ extern struct pid *cad_pid;
 #define PF_UMH			0x02000000	/* I'm an Usermodehelper process */
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
+#define PF_MEMALLOC_NOCMA	0x10000000	/* All allocation request will have _GFP_MOVABLE cleared */
 #define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */

commit 5e1f0f098b4649fad53011246bcaeff011ffdf5d
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Tue Mar 5 15:45:41 2019 -0800

    mm, compaction: capture a page under direct compaction
    
    Compaction is inherently race-prone as a suitable page freed during
    compaction can be allocated by any parallel task.  This patch uses a
    capture_control structure to isolate a page immediately when it is freed
    by a direct compactor in the slow path of the page allocator.  The
    intent is to avoid redundant scanning.
    
                                         5.0.0-rc1              5.0.0-rc1
                                   selective-v3r17          capture-v3r19
    Amean     fault-both-1         0.00 (   0.00%)        0.00 *   0.00%*
    Amean     fault-both-3      2582.11 (   0.00%)     2563.68 (   0.71%)
    Amean     fault-both-5      4500.26 (   0.00%)     4233.52 (   5.93%)
    Amean     fault-both-7      5819.53 (   0.00%)     6333.65 (  -8.83%)
    Amean     fault-both-12     9321.18 (   0.00%)     9759.38 (  -4.70%)
    Amean     fault-both-18     9782.76 (   0.00%)    10338.76 (  -5.68%)
    Amean     fault-both-24    15272.81 (   0.00%)    13379.55 *  12.40%*
    Amean     fault-both-30    15121.34 (   0.00%)    16158.25 (  -6.86%)
    Amean     fault-both-32    18466.67 (   0.00%)    18971.21 (  -2.73%)
    
    Latency is only moderately affected but the devil is in the details.  A
    closer examination indicates that base page fault latency is reduced but
    latency of huge pages is increased as it takes creater care to succeed.
    Part of the "problem" is that allocation success rates are close to 100%
    even when under pressure and compaction gets harder
    
                                    5.0.0-rc1              5.0.0-rc1
                              selective-v3r17          capture-v3r19
    Percentage huge-3        96.70 (   0.00%)       98.23 (   1.58%)
    Percentage huge-5        96.99 (   0.00%)       95.30 (  -1.75%)
    Percentage huge-7        94.19 (   0.00%)       97.24 (   3.24%)
    Percentage huge-12       94.95 (   0.00%)       97.35 (   2.53%)
    Percentage huge-18       96.74 (   0.00%)       97.30 (   0.58%)
    Percentage huge-24       97.07 (   0.00%)       97.55 (   0.50%)
    Percentage huge-30       95.69 (   0.00%)       98.50 (   2.95%)
    Percentage huge-32       96.70 (   0.00%)       99.27 (   2.65%)
    
    And scan rates are reduced as expected by 6% for the migration scanner
    and 29% for the free scanner indicating that there is less redundant
    work.
    
    Compaction migrate scanned    20815362    19573286
    Compaction free scanned       16352612    11510663
    
    [mgorman@techsingularity.net: remove redundant check]
      Link: http://lkml.kernel.org/r/20190201143853.GH9565@techsingularity.net
    Link: http://lkml.kernel.org/r/20190118175136.31341-23-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f9b43c989577..ebfb34fb9b30 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -47,6 +47,7 @@ struct pid_namespace;
 struct pipe_inode_info;
 struct rcu_node;
 struct reclaim_state;
+struct capture_control;
 struct robust_list_head;
 struct sched_attr;
 struct sched_param;
@@ -958,6 +959,9 @@ struct task_struct {
 
 	struct io_context		*io_context;
 
+#ifdef CONFIG_COMPACTION
+	struct capture_control		*capture_control;
+#endif
 	/* Ptrace state: */
 	unsigned long			ptrace_message;
 	kernel_siginfo_t		*last_siginfo;

commit edaed168e135f8ec87b27b567a367cbb041f2243
Merge: 78f860135433 71368af9027f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 5 12:50:34 2019 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/pti update from Thomas Gleixner:
     "Just a single change from the anti-performance departement:
    
       - Add a new PR_SPEC_DISABLE_NOEXEC option which allows to apply the
         speculation protections on a process without inheriting the state
         on exec.
    
         This remedies a situation where a Java-launcher has speculation
         protections enabled because that's the default for JVMs which
         causes the launched regular harmless processes to inherit the
         protection state which results in unintended performance
         degradation"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/speculation: Add PR_SPEC_DISABLE_NOEXEC

commit 53a41cb7ed381edee91029cdcabe9b3250f43f4d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 25 09:10:51 2019 -0800

    Revert "x86/fault: BUG() when uaccess helpers fault on kernel addresses"
    
    This reverts commit 9da3f2b74054406f87dff7101a569217ffceb29b.
    
    It was well-intentioned, but wrong.  Overriding the exception tables for
    instructions for random reasons is just wrong, and that is what the new
    code did.
    
    It caused problems for tracing, and it caused problems for strncpy_from_user(),
    because the new checks made perfectly valid use cases break, rather than
    catch things that did bad things.
    
    Unchecked user space accesses are a problem, but that's not a reason to
    add invalid checks that then people have to work around with silly flags
    (in this case, that 'kernel_uaccess_faults_ok' flag, which is just an
    odd way to say "this commit was wrong" and was sprinked into random
    places to hide the wrongness).
    
    The real fix to unchecked user space accesses is to get rid of the
    special "let's not check __get_user() and __put_user() at all" logic.
    Make __{get|put}_user() be just aliases to the regular {get|put}_user()
    functions, and make it impossible to access user space without having
    the proper checks in places.
    
    The raison d'Ãªtre of the special double-underscore versions used to be
    that the range check was expensive, and if you did multiple user
    accesses, you'd do the range check up front (like the signal frame
    handling code, for example).  But SMAP (on x86) and PAN (on ARM) have
    made that optimization pointless, because the _real_ expense is the "set
    CPU flag to allow user space access".
    
    Do let's not break the valid cases to catch invalid cases that shouldn't
    even exist.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Tobin C. Harding <tobin@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Jann Horn <jannh@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bba3afb4e9bf..f9b43c989577 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -739,12 +739,6 @@ struct task_struct {
 	unsigned			use_memdelay:1;
 #endif
 
-	/*
-	 * May usercopy functions fault on kernel addresses?
-	 * This is not just a single bit because this can potentially nest.
-	 */
-	unsigned int			kernel_uaccess_faults_ok;
-
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
 	struct restart_block		restart_block;

commit c9ba7560c550fe6c1f4a8f0666bea41d2a349d1d
Merge: f6783319737f d13937116f1e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 11 08:01:50 2019 +0100

    Merge tag 'v5.0-rc6' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c546951d9c9300065bad253ecdf1ac59ce9d06c8
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon Jan 21 16:52:40 2019 +0100

    sched/core: Use READ_ONCE()/WRITE_ONCE() in move_queued_task()/task_rq_lock()
    
    move_queued_task() synchronizes with task_rq_lock() as follows:
    
            move_queued_task()              task_rq_lock()
    
            [S] ->on_rq = MIGRATING         [L] rq = task_rq()
            WMB (__set_task_cpu())          ACQUIRE (rq->lock);
            [S] ->cpu = new_cpu             [L] ->on_rq
    
    where "[L] rq = task_rq()" is ordered before "ACQUIRE (rq->lock)" by an
    address dependency and, in turn, "ACQUIRE (rq->lock)" is ordered before
    "[L] ->on_rq" by the ACQUIRE itself.
    
    Use READ_ONCE() to load ->cpu in task_rq() (c.f., task_cpu()) to honor
    this address dependency.  Also, mark the accesses to ->cpu and ->on_rq
    with READ_ONCE()/WRITE_ONCE() to comply with the LKMM.
    
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://lkml.kernel.org/r/20190121155240.27173-1-andrea.parri@amarulasolutions.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 351c0fe64c85..4112639c2a85 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1745,9 +1745,9 @@ static __always_inline bool need_resched(void)
 static inline unsigned int task_cpu(const struct task_struct *p)
 {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-	return p->cpu;
+	return READ_ONCE(p->cpu);
 #else
-	return task_thread_info(p)->cpu;
+	return READ_ONCE(task_thread_info(p)->cpu);
 #endif
 }
 

commit 23127296889fe84b0762b191b5d041e8ba6f2599
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jan 23 16:26:53 2019 +0100

    sched/fair: Update scale invariance of PELT
    
    The current implementation of load tracking invariance scales the
    contribution with current frequency and uarch performance (only for
    utilization) of the CPU. One main result of this formula is that the
    figures are capped by current capacity of CPU. Another one is that the
    load_avg is not invariant because not scaled with uarch.
    
    The util_avg of a periodic task that runs r time slots every p time slots
    varies in the range :
    
        U * (1-y^r)/(1-y^p) * y^i < Utilization < U * (1-y^r)/(1-y^p)
    
    with U is the max util_avg value = SCHED_CAPACITY_SCALE
    
    At a lower capacity, the range becomes:
    
        U * C * (1-y^r')/(1-y^p) * y^i' < Utilization <  U * C * (1-y^r')/(1-y^p)
    
    with C reflecting the compute capacity ratio between current capacity and
    max capacity.
    
    so C tries to compensate changes in (1-y^r') but it can't be accurate.
    
    Instead of scaling the contribution value of PELT algo, we should scale the
    running time. The PELT signal aims to track the amount of computation of
    tasks and/or rq so it seems more correct to scale the running time to
    reflect the effective amount of computation done since the last update.
    
    In order to be fully invariant, we need to apply the same amount of
    running time and idle time whatever the current capacity. Because running
    at lower capacity implies that the task will run longer, we have to ensure
    that the same amount of idle time will be applied when system becomes idle
    and no idle time has been "stolen". But reaching the maximum utilization
    value (SCHED_CAPACITY_SCALE) means that the task is seen as an
    always-running task whatever the capacity of the CPU (even at max compute
    capacity). In this case, we can discard this "stolen" idle times which
    becomes meaningless.
    
    In order to achieve this time scaling, a new clock_pelt is created per rq.
    The increase of this clock scales with current capacity when something
    is running on rq and synchronizes with clock_task when rq is idle. With
    this mechanism, we ensure the same running and idle time whatever the
    current capacity. This also enables to simplify the pelt algorithm by
    removing all references of uarch and frequency and applying the same
    contribution to utilization and loads. Furthermore, the scaling is done
    only once per update of clock (update_rq_clock_task()) instead of during
    each update of sched_entities and cfs/rt/dl_rq of the rq like the current
    implementation. This is interesting when cgroup are involved as shown in
    the results below:
    
    On a hikey (octo Arm64 platform).
    Performance cpufreq governor and only shallowest c-state to remove variance
    generated by those power features so we only track the impact of pelt algo.
    
    each test runs 16 times:
    
            ./perf bench sched pipe
            (higher is better)
            kernel  tip/sched/core     + patch
                    ops/seconds        ops/seconds         diff
            cgroup
            root    59652(+/- 0.18%)   59876(+/- 0.24%)    +0.38%
            level1  55608(+/- 0.27%)   55923(+/- 0.24%)    +0.57%
            level2  52115(+/- 0.29%)   52564(+/- 0.22%)    +0.86%
    
            hackbench -l 1000
            (lower is better)
            kernel  tip/sched/core     + patch
                    duration(sec)      duration(sec)        diff
            cgroup
            root    4.453(+/- 2.37%)   4.383(+/- 2.88%)     -1.57%
            level1  4.859(+/- 8.50%)   4.830(+/- 7.07%)     -0.60%
            level2  5.063(+/- 9.83%)   4.928(+/- 9.66%)     -2.66%
    
    Then, the responsiveness of PELT is improved when CPU is not running at max
    capacity with this new algorithm. I have put below some examples of
    duration to reach some typical load values according to the capacity of the
    CPU with current implementation and with this patch. These values has been
    computed based on the geometric series and the half period value:
    
      Util (%)     max capacity  half capacity(mainline)  half capacity(w/ patch)
      972 (95%)    138ms         not reachable            276ms
      486 (47.5%)  30ms          138ms                     60ms
      256 (25%)    13ms           32ms                     26ms
    
    On my hikey (octo Arm64 platform) with schedutil governor, the time to
    reach max OPP when starting from a null utilization, decreases from 223ms
    with current scale invariance down to 121ms with the new algorithm.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pjt@google.com
    Cc: pkondeti@codeaurora.org
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Link: https://lkml.kernel.org/r/1548257214-13745-3-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 628bf13cb5a5..351c0fe64c85 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -357,12 +357,6 @@ struct util_est {
  * For cfs_rq, it is the aggregated load_avg of all runnable and
  * blocked sched_entities.
  *
- * load_avg may also take frequency scaling into account:
- *
- *   load_avg = runnable% * scale_load_down(load) * freq%
- *
- * where freq% is the CPU frequency normalized to the highest frequency.
- *
  * [util_avg definition]
  *
  *   util_avg = running% * SCHED_CAPACITY_SCALE
@@ -371,17 +365,14 @@ struct util_est {
  * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable
  * and blocked sched_entities.
  *
- * util_avg may also factor frequency scaling and CPU capacity scaling:
- *
- *   util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%
- *
- * where freq% is the same as above, and capacity% is the CPU capacity
- * normalized to the greatest capacity (due to uarch differences, etc).
+ * load_avg and util_avg don't direcly factor frequency scaling and CPU
+ * capacity scaling. The scaling is done through the rq_clock_pelt that
+ * is used for computing those signals (see update_rq_clock_pelt())
  *
- * N.B., the above ratios (runnable%, running%, freq%, and capacity%)
- * themselves are in the range of [0, 1]. To do fixed point arithmetics,
- * we therefore scale them to as large a range as necessary. This is for
- * example reflected by util_avg's SCHED_CAPACITY_SCALE.
+ * N.B., the above ratios (runnable% and running%) themselves are in the
+ * range of [0, 1]. To do fixed point arithmetics, we therefore scale them
+ * to as large a range as necessary. This is for example reflected by
+ * util_avg's SCHED_CAPACITY_SCALE.
  *
  * [Overflow issue]
  *

commit f0b89d3958d73cd0785ec381f0ddf8efb6f183d8
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:30 2019 +0200

    sched/core: Convert task_struct.stack_refcount to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.stack_refcount is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.stack_refcount it might make a difference
    in following places:
    
     - try_get_task_stack(): increment in refcount_inc_not_zero() only
       guarantees control dependency on success vs. fully ordered
       atomic counterpart
     - put_task_stack(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-6-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9d14d6864ca6..628bf13cb5a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1194,7 +1194,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* A live task holds one reference: */
-	atomic_t			stack_refcount;
+	refcount_t			stack_refcount;
 #endif
 #ifdef CONFIG_LIVEPATCH
 	int patch_state;

commit ec1d281923cf81cc660343d0cb8ffc837ffb991d
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:29 2019 +0200

    sched/core: Convert task_struct.usage to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.usage is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.usage it might make a difference
    in following places:
    
     - put_task_struct(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-5-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e2bba022827d..9d14d6864ca6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -21,6 +21,7 @@
 #include <linux/seccomp.h>
 #include <linux/nodemask.h>
 #include <linux/rcupdate.h>
+#include <linux/refcount.h>
 #include <linux/resource.h>
 #include <linux/latencytop.h>
 #include <linux/sched/prio.h>
@@ -607,7 +608,7 @@ struct task_struct {
 	randomized_struct_fields_start
 
 	void				*stack;
-	atomic_t			usage;
+	refcount_t			usage;
 	/* Per task flags (PF_*), defined further below: */
 	unsigned int			flags;
 	unsigned int			ptrace;

commit 5f3d544f1671d214cd26e45bda326f921455256e
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Fri Feb 1 22:45:17 2019 -0500

    audit: remove audit_context when CONFIG_ AUDIT and not AUDITSYSCALL
    
    Remove audit_context from struct task_struct and struct audit_buffer
    when CONFIG_AUDIT is enabled but CONFIG_AUDITSYSCALL is not.
    
    Also, audit_log_name() (and supporting inode and fcaps functions) should
    have been put back in auditsc.c when soft and hard link logging was
    normalized since it is only used by syscall auditing.
    
    See github issue https://github.com/linux-audit/audit-kernel/issues/105
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f9788bb122c5..765119df759a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -885,8 +885,10 @@ struct task_struct {
 
 	struct callback_head		*task_works;
 
-	struct audit_context		*audit_context;
 #ifdef CONFIG_AUDIT
+#ifdef CONFIG_AUDITSYSCALL
+	struct audit_context		*audit_context;
+#endif
 	kuid_t				loginuid;
 	unsigned int			sessionid;
 #endif

commit 24b888d8d59847871387aa3b241b524661070a6e
Merge: cc6810e36bd8 e6d429313ea5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 3 09:08:12 2019 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A few updates for x86:
    
       - Fix an unintended sign extension issue in the fault handling code
    
       - Rename the new resource control config switch so it's less
         confusing
    
       - Avoid setting up EFI info in kexec when the EFI runtime is
         disabled.
    
       - Fix the microcode version check in the AMD microcode loader so it
         only loads higher version numbers and never downgrades
    
       - Set EFER.LME in the 32bit trampoline before returning to long mode
         to handle older AMD/KVM behaviour properly.
    
       - Add Darren and Andy as x86/platform reviewers"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/resctrl: Avoid confusion over the new X86_RESCTRL config
      x86/kexec: Don't setup EFI info if EFI runtime is not enabled
      x86/microcode/amd: Don't falsely trick the late loading mechanism
      MAINTAINERS: Add Andy and Darren as arch/x86/platform/ reviewers
      x86/fault: Fix sign-extend unintended sign extension
      x86/boot/compressed/64: Set EFER.LME=1 in 32-bit trampoline before returning to long mode
      x86/cpu: Add Atom Tremont (Jacobsville)

commit e6d429313ea5c776d2e76b4494df69102e6b7115
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Tue Jan 29 17:44:36 2019 -0500

    x86/resctrl: Avoid confusion over the new X86_RESCTRL config
    
    "Resource Control" is a very broad term for this CPU feature, and a term
    that is also associated with containers, cgroups etc. This can easily
    cause confusion.
    
    Make the user prompt more specific. Match the config symbol name.
    
     [ bp: In the future, the corresponding ARM arch-specific code will be
       under ARM_CPU_RESCTRL and the arch-agnostic bits will be carved out
       under the CPU_RESCTRL umbrella symbol. ]
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Babu Moger <Babu.Moger@amd.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: linux-doc@vger.kernel.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Reinette Chatre <reinette.chatre@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190130195621.GA30653@cmpxchg.org

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 224666226e87..8c328b14c424 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -995,7 +995,7 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
 	struct list_head		cg_list;
 #endif
-#ifdef CONFIG_X86_RESCTRL
+#ifdef CONFIG_X86_CPU_RESCTRL
 	u32				closid;
 	u32				rmid;
 #endif

commit 71368af9027f18fe5d1c6f372cfdff7e4bde8b48
Author: Waiman Long <longman@redhat.com>
Date:   Wed Jan 16 17:01:36 2019 -0500

    x86/speculation: Add PR_SPEC_DISABLE_NOEXEC
    
    With the default SPEC_STORE_BYPASS_SECCOMP/SPEC_STORE_BYPASS_PRCTL mode,
    the TIF_SSBD bit will be inherited when a new task is fork'ed or cloned.
    It will also remain when a new program is execve'ed.
    
    Only certain class of applications (like Java) that can run on behalf of
    multiple users on a single thread will require disabling speculative store
    bypass for security purposes. Those applications will call prctl(2) at
    startup time to disable SSB. They won't rely on the fact the SSB might have
    been disabled. Other applications that don't need SSBD will just move on
    without checking if SSBD has been turned on or not.
    
    The fact that the TIF_SSBD is inherited across execve(2) boundary will
    cause performance of applications that don't need SSBD but their
    predecessors have SSBD on to be unwittingly impacted especially if they
    write to memory a lot.
    
    To remedy this problem, a new PR_SPEC_DISABLE_NOEXEC argument for the
    PR_SET_SPECULATION_CTRL option of prctl(2) is added to allow applications
    to specify that the SSBD feature bit on the task structure should be
    cleared whenever a new program is being execve'ed.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: linux-doc@vger.kernel.org
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: KarimAllah Ahmed <karahmed@amazon.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Link: https://lkml.kernel.org/r/1547676096-3281-1-git-send-email-longman@redhat.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2f90fa92468..fc836dc71bba 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1459,6 +1459,7 @@ static inline bool is_percpu_thread(void)
 #define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/
 #define PFA_SPEC_IB_DISABLE		5	/* Indirect branch speculation restricted */
 #define PFA_SPEC_IB_FORCE_DISABLE	6	/* Indirect branch speculation permanently restricted */
+#define PFA_SPEC_SSB_NOEXEC		7	/* Speculative Store Bypass clear on execve() */
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
@@ -1487,6 +1488,10 @@ TASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)
 TASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)
 TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)
 
+TASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)
+TASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)
+TASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)
+
 TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 

commit 15917dc02841862840efcbfe1da0830f88078b5c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 19 13:04:41 2018 +0100

    sched: Remove stale PF_MUTEX_TESTER bit
    
    The RTMUTEX tester was removed long ago but the PF bit stayed
    around. Remove it and free up the space.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2f90fa92468..e2bba022827d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1409,7 +1409,6 @@ extern struct pid *cad_pid;
 #define PF_UMH			0x02000000	/* I'm an Usermodehelper process */
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
-#define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */
 

commit 4b7d248b3a1de483ffe9d05c1debbf32a544164d
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Tue Jan 22 17:06:39 2019 -0500

    audit: move loginuid and sessionid from CONFIG_AUDITSYSCALL to CONFIG_AUDIT
    
    loginuid and sessionid (and audit_log_session_info) should be part of
    CONFIG_AUDIT scope and not CONFIG_AUDITSYSCALL since it is used in
    CONFIG_CHANGE, ANOM_LINK, FEATURE_CHANGE (and INTEGRITY_RULE), none of
    which are otherwise dependent on AUDITSYSCALL.
    
    Please see github issue
    https://github.com/linux-audit/audit-kernel/issues/104
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    [PM: tweaked subject line for better grep'ing]
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89541d248893..f9788bb122c5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -886,7 +886,7 @@ struct task_struct {
 	struct callback_head		*task_works;
 
 	struct audit_context		*audit_context;
-#ifdef CONFIG_AUDITSYSCALL
+#ifdef CONFIG_AUDIT
 	kuid_t				loginuid;
 	unsigned int			sessionid;
 #endif

commit e8746440bf68212f19688f1454dad593c74abee1
Merge: fe76fc6aaf53 2f960bd05640
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 16 05:13:36 2019 +1200

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Pull networking fixes from David Miller:
    
     1) Fix regression in multi-SKB responses to RTM_GETADDR, from Arthur
        Gautier.
    
     2) Fix ipv6 frag parsing in openvswitch, from Yi-Hung Wei.
    
     3) Unbounded recursion in ipv4 and ipv6 GUE tunnels, from Stefano
        Brivio.
    
     4) Use after free in hns driver, from Yonglong Liu.
    
     5) icmp6_send() needs to handle the case of NULL skb, from Eric
        Dumazet.
    
     6) Missing rcu read lock in __inet6_bind() when operating on mapped
        addresses, from David Ahern.
    
     7) Memory leak in tipc-nl_compat_publ_dump(), from Gustavo A. R. Silva.
    
     8) Fix PHY vs r8169 module loading ordering issues, from Heiner
        Kallweit.
    
     9) Fix bridge vlan memory leak, from Ido Schimmel.
    
    10) Dev refcount leak in AF_PACKET, from Jason Gunthorpe.
    
    11) Infoleak in ipv6_local_error(), flow label isn't completely
        initialized. From Eric Dumazet.
    
    12) Handle mv88e6390 errata, from Andrew Lunn.
    
    13) Making vhost/vsock CID hashing consistent, from Zha Bin.
    
    14) Fix lack of UMH cleanup when it unexpectedly exits, from Taehee Yoo.
    
    15) Bridge forwarding must clear skb->tstamp, from Paolo Abeni.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net: (87 commits)
      bnxt_en: Fix context memory allocation.
      bnxt_en: Fix ring checking logic on 57500 chips.
      mISDN: hfcsusb: Use struct_size() in kzalloc()
      net: clear skb->tstamp in bridge forwarding path
      net: bpfilter: disallow to remove bpfilter module while being used
      net: bpfilter: restart bpfilter_umh when error occurred
      net: bpfilter: use cleanup callback to release umh_info
      umh: add exit routine for UMH process
      isdn: i4l: isdn_tty: Fix some concurrency double-free bugs
      vhost/vsock: fix vhost vsock cid hashing inconsistent
      net: stmmac: Prevent RX starvation in stmmac_napi_poll()
      net: stmmac: Fix the logic of checking if RX Watchdog must be enabled
      net: stmmac: Check if CBS is supported before configuring
      net: stmmac: dwxgmac2: Only clear interrupts that are active
      net: stmmac: Fix PCI module removal leak
      tools/bpf: fix bpftool map dump with bitfields
      tools/bpf: test btf bitfield with >=256 struct member offset
      bpf: fix bpffs bitfield pretty print
      net: ethernet: mediatek: fix warning in phy_start_aneg
      tcp: change txhash on SYN-data timeout
      ...

commit 73ab1cb2de9e3efe7f818d5453de271e5371df1d
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Wed Jan 9 02:23:56 2019 +0900

    umh: add exit routine for UMH process
    
    A UMH process which is created by the fork_usermode_blob() such as
    bpfilter needs to release members of the umh_info when process is
    terminated.
    But the do_exit() does not release members of the umh_info. hence module
    which uses UMH needs own code to detect whether UMH process is
    terminated or not.
    But this implementation needs extra code for checking the status of
    UMH process. it eventually makes the code more complex.
    
    The new PF_UMH flag is added and it is used to identify UMH processes.
    The exit_umh() does not release members of the umh_info.
    Hence umh_info->cleanup callback should release both members of the
    umh_info and the private data.
    
    Suggested-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89541d248893..e35e35b9fc48 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1406,6 +1406,7 @@ extern struct pid *cad_pid;
 #define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
 #define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */
 #define PF_MEMSTALL		0x01000000	/* Stalled due to lack of memory */
+#define PF_UMH			0x02000000	/* I'm an Usermodehelper process */
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
 #define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */
@@ -1904,6 +1905,14 @@ static inline void rseq_execve(struct task_struct *t)
 
 #endif
 
+void __exit_umh(struct task_struct *tsk);
+
+static inline void exit_umh(struct task_struct *tsk)
+{
+	if (unlikely(tsk->flags & PF_UMH))
+		__exit_umh(tsk);
+}
+
 #ifdef CONFIG_DEBUG_RSEQ
 
 void rseq_syscall(struct pt_regs *regs);

commit 90802938f7e88045ace123e105e22e8c3e7f9c7e
Author: Borislav Petkov <bp@suse.de>
Date:   Tue Jan 8 17:38:29 2019 +0100

    x86/cache: Rename config option to CONFIG_X86_RESCTRL
    
    CONFIG_RESCTRL is too generic. The final goal is to have a generic
    option called like this which is selected by the arch-specific ones
    CONFIG_X86_RESCTRL and CONFIG_ARM64_RESCTRL. The generic one will
    cover the resctrl filesystem and other generic and shared bits of
    functionality.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Requested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Babu Moger <babu.moger@amd.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Reinette Chatre <reinette.chatre@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20190108171401.GC12235@zn.tnic

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89541d248893..224666226e87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -995,7 +995,7 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
 	struct list_head		cg_list;
 #endif
-#ifdef CONFIG_RESCTRL
+#ifdef CONFIG_X86_RESCTRL
 	u32				closid;
 	u32				rmid;
 #endif

commit 17bf423a1f2d134187191f0ceb4b395173cc98a7
Merge: 116b081c285d 732cd75b8c92
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 14:56:10 2018 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Introduce "Energy Aware Scheduling" - by Quentin Perret.
    
         This is a coherent topology description of CPUs in cooperation with
         the PM subsystem, with the goal to schedule more energy-efficiently
         on asymetric SMP platform - such as waking up tasks to the more
         energy-efficient CPUs first, as long as the system isn't
         oversubscribed.
    
         For details of the design, see:
    
            https://lore.kernel.org/lkml/20180724122521.22109-1-quentin.perret@arm.com/
    
       - Misc cleanups and smaller enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      sched/fair: Select an energy-efficient CPU on task wake-up
      sched/fair: Introduce an energy estimation helper function
      sched/fair: Add over-utilization/tipping point indicator
      sched/fair: Clean-up update_sg_lb_stats parameters
      sched/toplogy: Introduce the 'sched_energy_present' static key
      sched/topology: Make Energy Aware Scheduling depend on schedutil
      sched/topology: Disable EAS on inappropriate platforms
      sched/topology: Add lowest CPU asymmetry sched_domain level pointer
      sched/topology: Reference the Energy Model of CPUs when available
      PM: Introduce an Energy Model management framework
      sched/cpufreq: Prepare schedutil for Energy Aware Scheduling
      sched/topology: Relocate arch_scale_cpu_capacity() to the internal header
      sched/core: Remove unnecessary unlikely() in push_*_task()
      sched/topology: Remove the ::smt_gain field from 'struct sched_domain'
      sched: Fix various typos in comments
      sched/core: Clean up the #ifdef block in add_nr_running()
      sched/fair: Make some variables static
      sched/core: Create task_has_idle_policy() helper
      sched/fair: Add lsub_positive() and use it consistently
      sched/fair: Mask UTIL_AVG_UNCHANGED usages
      ...

commit 792bf4d871dea8b69be2aaabdd320d7c6ed15985
Merge: eed9688f8513 4bbfd7467cfc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 13:07:19 2018 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The biggest RCU changes in this cycle were:
    
       - Convert RCU's BUG_ON() and similar calls to WARN_ON() and similar.
    
       - Replace calls of RCU-bh and RCU-sched update-side functions to
         their vanilla RCU counterparts. This series is a step towards
         complete removal of the RCU-bh and RCU-sched update-side functions.
    
         ( Note that some of these conversions are going upstream via their
           respective maintainers. )
    
       - Documentation updates, including a number of flavor-consolidation
         updates from Joel Fernandes.
    
       - Miscellaneous fixes.
    
       - Automate generation of the initrd filesystem used for rcutorture
         testing.
    
       - Convert spin_is_locked() assertions to instead use lockdep.
    
         ( Note that some of these conversions are going upstream via their
           respective maintainers. )
    
       - SRCU updates, especially including a fix from Dennis Krein for a
         bag-on-head-class bug.
    
       - RCU torture-test updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (112 commits)
      rcutorture: Don't do busted forward-progress testing
      rcutorture: Use 100ms buckets for forward-progress callback histograms
      rcutorture: Recover from OOM during forward-progress tests
      rcutorture: Print forward-progress test age upon failure
      rcutorture: Print time since GP end upon forward-progress failure
      rcutorture: Print histogram of CB invocation at OOM time
      rcutorture: Print GP age upon forward-progress failure
      rcu: Print per-CPU callback counts for forward-progress failures
      rcu: Account for nocb-CPU callback counts in RCU CPU stall warnings
      rcutorture: Dump grace-period diagnostics upon forward-progress OOM
      rcutorture: Prepare for asynchronous access to rcu_fwd_startat
      torture: Remove unnecessary "ret" variables
      rcutorture: Affinity forward-progress test to avoid housekeeping CPUs
      rcutorture: Break up too-long rcu_torture_fwd_prog() function
      rcutorture: Remove cbflood facility
      torture: Bring any extra CPUs online during kernel startup
      rcutorture: Add call_rcu() flooding forward-progress tests
      rcutorture/formal: Replace synchronize_sched() with synchronize_rcu()
      tools/kernel.h: Replace synchronize_sched() with synchronize_rcu()
      net/decnet: Replace rcu_barrier_bh() with rcu_barrier()
      ...

commit a52fb43a5faa40507cb164a793a7fa08da863ac7
Merge: 42b00f122cfb 52eb74339a62
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 12:17:43 2018 -0800

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache control updates from Borislav Petkov:
    
     - The generalization of the RDT code to accommodate the addition of
       AMD's very similar implementation of the cache monitoring feature.
    
       This entails a subsystem move into a separate and generic
       arch/x86/kernel/cpu/resctrl/ directory along with adding
       vendor-specific initialization and feature detection helpers.
    
       Ontop of that is the unification of user-visible strings, both in the
       resctrl filesystem error handling and Kconfig.
    
       Provided by Babu Moger and Sherry Hurwitz.
    
     - Code simplifications and error handling improvements by Reinette
       Chatre.
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/resctrl: Fix rdt_find_domain() return value and checks
      x86/resctrl: Remove unnecessary check for cbm_validate()
      x86/resctrl: Use rdt_last_cmd_puts() where possible
      MAINTAINERS: Update resctrl filename patterns
      Documentation: Rename and update intel_rdt_ui.txt to resctrl_ui.txt
      x86/resctrl: Introduce AMD QOS feature
      x86/resctrl: Fixup the user-visible strings
      x86/resctrl: Add AMD's X86_FEATURE_MBA to the scattered CPUID features
      x86/resctrl: Rename the config option INTEL_RDT to RESCTRL
      x86/resctrl: Add vendor check for the MBA software controller
      x86/resctrl: Bring cbm_validate() into the resource structure
      x86/resctrl: Initialize the vendor-specific resource functions
      x86/resctrl: Move all the macros to resctrl/internal.h
      x86/resctrl: Re-arrange the RDT init code
      x86/resctrl: Rename the RDT functions and definitions
      x86/resctrl: Rename and move rdt files to a separate directory

commit 4bbfd7467cfc7d42e18d3008fa6a28ffd56e901a
Merge: 2595646791c3 5ac7cdc29897
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 4 07:52:30 2018 +0100

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU changes from Paul E. McKenney:
    
    - Convert RCU's BUG_ON() and similar calls to WARN_ON() and similar.
    
    - Replace calls of RCU-bh and RCU-sched update-side functions
      to their vanilla RCU counterparts.  This series is a step
      towards complete removal of the RCU-bh and RCU-sched update-side
      functions.
    
      ( Note that some of these conversions are going upstream via their
        respective maintainers. )
    
    - Documentation updates, including a number of flavor-consolidation
      updates from Joel Fernandes.
    
    - Miscellaneous fixes.
    
    - Automate generation of the initrd filesystem used for
      rcutorture testing.
    
    - Convert spin_is_locked() assertions to instead use lockdep.
    
      ( Note that some of these conversions are going upstream via their
        respective maintainers. )
    
    - SRCU updates, especially including a fix from Dennis Krein
      for a bag-on-head-class bug.
    
    - RCU torture-test updates.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit dfcb245e28481256a10a9133441baf2a93d26642
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:05:56 2018 +0100

    sched: Fix various typos in comments
    
    Go over the scheduler source code and fix common typos
    in comments - and a typo in an actual variable name.
    
    No change in functionality intended.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 291a9bd5b97f..b8c7ba0e3796 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -176,7 +176,7 @@ struct task_group;
  * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).
  *
  * However, with slightly different timing the wakeup TASK_RUNNING store can
- * also collide with the TASK_UNINTERRUPTIBLE store. Loosing that store is not
+ * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not
  * a problem either because that will result in one extra go around the loop
  * and our @cond test will save the day.
  *
@@ -515,7 +515,7 @@ struct sched_dl_entity {
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,
-	 * they are continously updated during task execution. Note that
+	 * they are continuously updated during task execution. Note that
 	 * the remaining runtime could be < 0 in case we are in overrun.
 	 */
 	s64				runtime;	/* Remaining runtime for this instance	*/

commit 4b78317679c4f3782a3cff0ddb269c1fcfde7621
Merge: 880584176ed7 55a974021ec9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 1 12:35:48 2018 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull STIBP fallout fixes from Thomas Gleixner:
     "The performance destruction department finally got it's act together
      and came up with a cure for the STIPB regression:
    
       - Provide a command line option to control the spectre v2 user space
         mitigations. Default is either seccomp or prctl (if seccomp is
         disabled in Kconfig). prctl allows mitigation opt-in, seccomp
         enables the migitation for sandboxed processes.
    
       - Rework the code to handle the conditional STIBP/IBPB control and
         remove the now unused ptrace_may_access_sched() optimization
         attempt
    
       - Disable STIBP automatically when SMT is disabled
    
       - Optimize the switch_to() logic to avoid MSR writes and invocations
         of __switch_to_xtra().
    
       - Make the asynchronous speculation TIF updates synchronous to
         prevent stale mitigation state.
    
      As a general cleanup this also makes retpoline directly depend on
      compiler support and removes the 'minimal retpoline' option which just
      pretended to provide some form of security while providing none"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (31 commits)
      x86/speculation: Provide IBPB always command line options
      x86/speculation: Add seccomp Spectre v2 user space protection mode
      x86/speculation: Enable prctl mode for spectre_v2_user
      x86/speculation: Add prctl() control for indirect branch speculation
      x86/speculation: Prepare arch_smt_update() for PRCTL mode
      x86/speculation: Prevent stale SPEC_CTRL msr content
      x86/speculation: Split out TIF update
      ptrace: Remove unused ptrace_may_access_sched() and MODE_IBRS
      x86/speculation: Prepare for conditional IBPB in switch_mm()
      x86/speculation: Avoid __switch_to_xtra() calls
      x86/process: Consolidate and simplify switch_to_xtra() code
      x86/speculation: Prepare for per task indirect branch speculation control
      x86/speculation: Add command line control for indirect branch speculation
      x86/speculation: Unify conditional spectre v2 print functions
      x86/speculataion: Mark command line parser data __initdata
      x86/speculation: Mark string arrays const correctly
      x86/speculation: Reorder the spec_v2 code
      x86/l1tf: Show actual SMT state
      x86/speculation: Rework SMT state change
      sched/smt: Expose sched_smt_present static key
      ...

commit 9137bb27e60e554dab694eafa4cca241fa3a694f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:53 2018 +0100

    x86/speculation: Add prctl() control for indirect branch speculation
    
    Add the PR_SPEC_INDIRECT_BRANCH option for the PR_GET_SPECULATION_CTRL and
    PR_SET_SPECULATION_CTRL prctls to allow fine grained per task control of
    indirect branch speculation via STIBP and IBPB.
    
    Invocations:
     Check indirect branch speculation status with
     - prctl(PR_GET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, 0, 0, 0);
    
     Enable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_ENABLE, 0, 0);
    
     Disable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_DISABLE, 0, 0);
    
     Force disable indirect branch speculation with
     - prctl(PR_SET_SPECULATION_CTRL, PR_SPEC_INDIRECT_BRANCH, PR_SPEC_FORCE_DISABLE, 0, 0);
    
    See Documentation/userspace-api/spec_ctrl.rst.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185005.866780996@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a51c13c2b1a0..d607db5fcc6a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1453,6 +1453,8 @@ static inline bool is_percpu_thread(void)
 #define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
 #define PFA_SPEC_SSB_DISABLE		3	/* Speculative Store Bypass disabled */
 #define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/
+#define PFA_SPEC_IB_DISABLE		5	/* Indirect branch speculation restricted */
+#define PFA_SPEC_IB_FORCE_DISABLE	6	/* Indirect branch speculation permanently restricted */
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
@@ -1484,6 +1486,13 @@ TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)
 TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
 
+TASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)
+TASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)
+TASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)
+
+TASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)
+TASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)
+
 static inline void
 current_restore_flags(unsigned long orig_flags, unsigned long flags)
 {

commit 39eb456dacb543de90d3bc6a8e0ac5cf51ac475e
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Nov 19 08:07:12 2018 -0500

    function_graph: Use new curr_ret_depth to manage depth instead of curr_ret_stack
    
    Currently, the depth of the ret_stack is determined by curr_ret_stack index.
    The issue is that there's a race between setting of the curr_ret_stack and
    calling of the callback attached to the return of the function.
    
    Commit 03274a3ffb44 ("tracing/fgraph: Adjust fgraph depth before calling
    trace return callback") moved the calling of the callback to after the
    setting of the curr_ret_stack, even stating that it was safe to do so, when
    in fact, it was the reason there was a barrier() there (yes, I should have
    commented that barrier()).
    
    Not only does the curr_ret_stack keep track of the current call graph depth,
    it also keeps the ret_stack content from being overwritten by new data.
    
    The function profiler, uses the "subtime" variable of ret_stack structure
    and by moving the curr_ret_stack, it allows for interrupts to use the same
    structure it was using, corrupting the data, and breaking the profiler.
    
    To fix this, there needs to be two variables to handle the call stack depth
    and the pointer to where the ret_stack is being used, as they need to change
    at two different locations.
    
    Cc: stable@kernel.org
    Fixes: 03274a3ffb449 ("tracing/fgraph: Adjust fgraph depth before calling trace return callback")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a51c13c2b1a0..d6183a55e8eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1116,6 +1116,7 @@ struct task_struct {
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored address in ret_stack: */
 	int				curr_ret_stack;
+	int				curr_ret_depth;
 
 	/* Stack of return addresses for return function tracing: */
 	struct ftrace_ret_stack		*ret_stack;

commit 6fe07ce35e8ad870ba1cf82e0481e0fc0f526eff
Author: Babu Moger <Babu.Moger@amd.com>
Date:   Wed Nov 21 20:28:39 2018 +0000

    x86/resctrl: Rename the config option INTEL_RDT to RESCTRL
    
    The resource control feature is supported by both Intel and AMD. So,
    rename CONFIG_INTEL_RDT to the vendor-neutral CONFIG_RESCTRL.
    
    Now CONFIG_RESCTRL will be used for both Intel and AMD to enable
    Resource Control support. Update the texts in config and condition
    accordingly.
    
     [ bp: Simplify Kconfig text. ]
    
    Signed-off-by: Babu Moger <babu.moger@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: <linux-doc@vger.kernel.org>
    Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: <qianyue.zj@alibaba-inc.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Reinette Chatre <reinette.chatre@intel.com>
    Cc: Rian Hunter <rian@alum.mit.edu>
    Cc: Sherry Hurwitz <sherry.hurwitz@amd.com>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Lendacky <Thomas.Lendacky@amd.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: <xiaochen.shen@intel.com>
    Link: https://lkml.kernel.org/r/20181121202811.4492-9-babu.moger@amd.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a51c13c2b1a0..7952dfba2c76 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -993,7 +993,7 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
 	struct list_head		cg_list;
 #endif
-#ifdef CONFIG_INTEL_RDT
+#ifdef CONFIG_RESCTRL
 	u32				closid;
 	u32				rmid;
 #endif

commit 05f415715ce45da07a0b1a5eac842765b733157f
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Oct 16 04:12:58 2018 -0700

    rcu: Speed up expedited GPs when interrupting RCU reader
    
    In PREEMPT kernels, an expedited grace period might send an IPI to a
    CPU that is executing an RCU read-side critical section.  In that case,
    it would be nice if the rcu_read_unlock() directly interacted with the
    RCU core code to immediately report the quiescent state.  And this does
    happen in the case where the reader has been preempted.  But it would
    also be a nice performance optimization if immediate reporting also
    happened in the preemption-free case.
    
    This commit therefore adds an ->exp_hint field to the task_struct structure's
    ->rcu_read_unlock_special field.  The IPI handler sets this hint when
    it has interrupted an RCU read-side critical section, and this causes
    the outermost rcu_read_unlock() call to invoke rcu_read_unlock_special(),
    which, if preemption is enabled, reports the quiescent state immediately.
    If preemption is disabled, then the report is required to be deferred
    until preemption (or bottom halves or interrupts or whatever) is re-enabled.
    
    Because this is a hint, it does nothing for more complicated cases.  For
    example, if the IPI interrupts an RCU reader, but interrupts are disabled
    across the rcu_read_unlock(), but another rcu_read_lock() is executed
    before interrupts are re-enabled, the hint will already have been cleared.
    If you do crazy things like this, reporting will be deferred until some
    later RCU_SOFTIRQ handler, context switch, cond_resched(), or similar.
    
    Reported-by: Joel Fernandes <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Acked-by: Joel Fernandes (Google) <joel@joelfernandes.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a51c13c2b1a0..e4c7b6241088 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -572,8 +572,10 @@ union rcu_special {
 	struct {
 		u8			blocked;
 		u8			need_qs;
+		u8			exp_hint; /* Hint for performance. */
+		u8			pad; /* No garbage from compiler! */
 	} b; /* Bits. */
-	u16 s; /* Set of bits. */
+	u32 s; /* Set of bits. */
 };
 
 enum perf_event_task_context {

commit 2d6bb6adb714b133db92ccd4bfc9c20f75f71f3f
Merge: 7c6c54b505b8 6fcde9046673
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 1 11:46:27 2018 -0700

    Merge tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull stackleak gcc plugin from Kees Cook:
     "Please pull this new GCC plugin, stackleak, for v4.20-rc1. This plugin
      was ported from grsecurity by Alexander Popov. It provides efficient
      stack content poisoning at syscall exit. This creates a defense
      against at least two classes of flaws:
    
       - Uninitialized stack usage. (We continue to work on improving the
         compiler to do this in other ways: e.g. unconditional zero init was
         proposed to GCC and Clang, and more plugin work has started too).
    
       - Stack content exposure. By greatly reducing the lifetime of valid
         stack contents, exposures via either direct read bugs or unknown
         cache side-channels become much more difficult to exploit. This
         complements the existing buddy and heap poisoning options, but
         provides the coverage for stacks.
    
      The x86 hooks are included in this series (which have been reviewed by
      Ingo, Dave Hansen, and Thomas Gleixner). The arm64 hooks have already
      been merged through the arm64 tree (written by Laura Abbott and
      reviewed by Mark Rutland and Will Deacon).
    
      With VLAs having been removed this release, there is no need for
      alloca() protection, so it has been removed from the plugin"
    
    * tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      arm64: Drop unneeded stackleak_check_alloca()
      stackleak: Allow runtime disabling of kernel stack erasing
      doc: self-protection: Add information about STACKLEAK feature
      fs/proc: Show STACKLEAK metrics in the /proc file system
      lkdtm: Add a test for STACKLEAK
      gcc-plugins: Add STACKLEAK plugin for tracking the kernel stack
      x86/entry: Add STACKLEAK erasing the kernel stack at the end of syscalls

commit 85cfb245060e45640fa3447f8b0bad5e8bd3bdaf
Author: Shakeel Butt <shakeelb@google.com>
Date:   Fri Oct 26 15:07:41 2018 -0700

    memcg: remove memcg_kmem_skip_account
    
    The flag memcg_kmem_skip_account was added during the era of opt-out kmem
    accounting.  There is no need for such flag in the opt-in world as there
    aren't any __GFP_ACCOUNT allocations within memcg_create_cache_enqueue().
    
    Link: http://lkml.kernel.org/r/20180919004501.178023-1-shakeelb@google.com
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Greg Thelen <gthelen@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b8fcc6b3080c..8f8a5418b627 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -724,9 +724,6 @@ struct task_struct {
 #endif
 #ifdef CONFIG_MEMCG
 	unsigned			in_user_fault:1;
-#ifdef CONFIG_MEMCG_KMEM
-	unsigned			memcg_kmem_skip_account:1;
-#endif
 #endif
 #ifdef CONFIG_COMPAT_BRK
 	unsigned			brk_randomized:1;

commit eb414681d5a07d28d2ff90dc05f69ec6b232ebd2
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:27 2018 -0700

    psi: pressure stall information for CPU, memory, and IO
    
    When systems are overcommitted and resources become contended, it's hard
    to tell exactly the impact this has on workload productivity, or how close
    the system is to lockups and OOM kills.  In particular, when machines work
    multiple jobs concurrently, the impact of overcommit in terms of latency
    and throughput on the individual job can be enormous.
    
    In order to maximize hardware utilization without sacrificing individual
    job health or risk complete machine lockups, this patch implements a way
    to quantify resource pressure in the system.
    
    A kernel built with CONFIG_PSI=y creates files in /proc/pressure/ that
    expose the percentage of time the system is stalled on CPU, memory, or IO,
    respectively.  Stall states are aggregate versions of the per-task delay
    accounting delays:
    
           cpu: some tasks are runnable but not executing on a CPU
           memory: tasks are reclaiming, or waiting for swapin or thrashing cache
           io: tasks are waiting for io completions
    
    These percentages of walltime can be thought of as pressure percentages,
    and they give a general sense of system health and productivity loss
    incurred by resource overcommit.  They can also indicate when the system
    is approaching lockup scenarios and OOMs.
    
    To do this, psi keeps track of the task states associated with each CPU
    and samples the time they spend in stall states.  Every 2 seconds, the
    samples are averaged across CPUs - weighted by the CPUs' non-idle time to
    eliminate artifacts from unused CPUs - and translated into percentages of
    walltime.  A running average of those percentages is maintained over 10s,
    1m, and 5m periods (similar to the loadaverage).
    
    [hannes@cmpxchg.org: doc fixlet, per Randy]
      Link: http://lkml.kernel.org/r/20180828205625.GA14030@cmpxchg.org
    [hannes@cmpxchg.org: code optimization]
      Link: http://lkml.kernel.org/r/20180907175015.GA8479@cmpxchg.org
    [hannes@cmpxchg.org: rename psi_clock() to psi_update_work(), per Peter]
      Link: http://lkml.kernel.org/r/20180907145404.GB11088@cmpxchg.org
    [hannes@cmpxchg.org: fix build]
      Link: http://lkml.kernel.org/r/20180913014222.GA2370@cmpxchg.org
    Link: http://lkml.kernel.org/r/20180828172258.3185-9-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index adfb3f9a7597..b8fcc6b3080c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -25,6 +25,7 @@
 #include <linux/latencytop.h>
 #include <linux/sched/prio.h>
 #include <linux/signal_types.h>
+#include <linux/psi_types.h>
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
 #include <linux/rseq.h>
@@ -706,6 +707,10 @@ struct task_struct {
 	unsigned			sched_contributes_to_load:1;
 	unsigned			sched_migrated:1;
 	unsigned			sched_remote_wakeup:1;
+#ifdef CONFIG_PSI
+	unsigned			sched_psi_wake_requeue:1;
+#endif
+
 	/* Force alignment to the next boundary: */
 	unsigned			:0;
 
@@ -965,6 +970,10 @@ struct task_struct {
 	kernel_siginfo_t		*last_siginfo;
 
 	struct task_io_accounting	ioac;
+#ifdef CONFIG_PSI
+	/* Pressure stall state */
+	unsigned int			psi_flags;
+#endif
 #ifdef CONFIG_TASK_XACCT
 	/* Accumulated RSS usage: */
 	u64				acct_rss_mem1;
@@ -1391,6 +1400,7 @@ extern struct pid *cad_pid;
 #define PF_KTHREAD		0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
 #define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */
+#define PF_MEMSTALL		0x01000000	/* Stalled due to lack of memory */
 #define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
 #define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit 0200fbdd431519d730b5d399a12840ec832b27cc
Merge: de3fbb2aa802 01a14bda11ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 13:08:53 2018 +0100

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and misc x86 updates from Ingo Molnar:
     "Lots of changes in this cycle - in part because locking/core attracted
      a number of related x86 low level work which was easier to handle in a
      single tree:
    
       - Linux Kernel Memory Consistency Model updates (Alan Stern, Paul E.
         McKenney, Andrea Parri)
    
       - lockdep scalability improvements and micro-optimizations (Waiman
         Long)
    
       - rwsem improvements (Waiman Long)
    
       - spinlock micro-optimization (Matthew Wilcox)
    
       - qspinlocks: Provide a liveness guarantee (more fairness) on x86.
         (Peter Zijlstra)
    
       - Add support for relative references in jump tables on arm64, x86
         and s390 to optimize jump labels (Ard Biesheuvel, Heiko Carstens)
    
       - Be a lot less permissive on weird (kernel address) uaccess faults
         on x86: BUG() when uaccess helpers fault on kernel addresses (Jann
         Horn)
    
       - macrofy x86 asm statements to un-confuse the GCC inliner. (Nadav
         Amit)
    
       - ... and a handful of other smaller changes as well"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      locking/lockdep: Make global debug_locks* variables read-mostly
      locking/lockdep: Fix debug_locks off performance problem
      locking/pvqspinlock: Extend node size when pvqspinlock is configured
      locking/qspinlock_stat: Count instances of nested lock slowpaths
      locking/qspinlock, x86: Provide liveness guarantee
      x86/asm: 'Simplify' GEN_*_RMWcc() macros
      locking/qspinlock: Rework some comments
      locking/qspinlock: Re-order code
      locking/lockdep: Remove duplicated 'lock_class_ops' percpu array
      x86/defconfig: Enable CONFIG_USB_XHCI_HCD=y
      futex: Replace spin_is_locked() with lockdep
      locking/lockdep: Make class->ops a percpu counter and move it under CONFIG_DEBUG_LOCKDEP=y
      x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs
      x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs
      x86/extable: Macrofy inline assembly code to work around GCC inlining bugs
      x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops
      x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs
      x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs
      x86/refcount: Work around GCC inlining bug
      x86/objtool: Use asm macros to work around GCC inlining bugs
      ...

commit ae7795bc6187a15ec51cf258abae656a625f9980
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 25 11:27:20 2018 +0200

    signal: Distinguish between kernel_siginfo and siginfo
    
    Linus recently observed that if we did not worry about the padding
    member in struct siginfo it is only about 48 bytes, and 48 bytes is
    much nicer than 128 bytes for allocating on the stack and copying
    around in the kernel.
    
    The obvious thing of only adding the padding when userspace is
    including siginfo.h won't work as there are sigframe definitions in
    the kernel that embed struct siginfo.
    
    So split siginfo in two; kernel_siginfo and siginfo.  Keeping the
    traditional name for the userspace definition.  While the version that
    is used internally to the kernel and ultimately will not be padded to
    128 bytes is called kernel_siginfo.
    
    The definition of struct kernel_siginfo I have put in include/signal_types.h
    
    A set of buildtime checks has been added to verify the two structures have
    the same field offsets.
    
    To make it easy to verify the change kernel_siginfo retains the same
    size as siginfo.  The reduction in size comes in a following change.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 977cb57d7bc9..2ba88082e1ef 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -960,7 +960,7 @@ struct task_struct {
 
 	/* Ptrace state: */
 	unsigned long			ptrace_message;
-	siginfo_t			*last_siginfo;
+	kernel_siginfo_t		*last_siginfo;
 
 	struct task_io_accounting	ioac;
 #ifdef CONFIG_TASK_XACCT

commit c8d126275a5fa59394fe17109bdb9812fed296b8
Author: Alexander Popov <alex.popov@linux.com>
Date:   Fri Aug 17 01:17:01 2018 +0300

    fs/proc: Show STACKLEAK metrics in the /proc file system
    
    Introduce CONFIG_STACKLEAK_METRICS providing STACKLEAK information about
    tasks via the /proc file system. In particular, /proc/<pid>/stack_depth
    shows the maximum kernel stack consumption for the current and previous
    syscalls. Although this information is not precise, it can be useful for
    estimating the STACKLEAK performance impact for your workloads.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Alexander Popov <alex.popov@linux.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1a23acd24e7..ae9d10e14b82 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1194,6 +1194,7 @@ struct task_struct {
 
 #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
 	unsigned long			lowest_stack;
+	unsigned long			prev_lowest_stack;
 #endif
 
 	/*

commit afaef01c001537fa97a25092d7f54d764dc7d8c1
Author: Alexander Popov <alex.popov@linux.com>
Date:   Fri Aug 17 01:16:58 2018 +0300

    x86/entry: Add STACKLEAK erasing the kernel stack at the end of syscalls
    
    The STACKLEAK feature (initially developed by PaX Team) has the following
    benefits:
    
    1. Reduces the information that can be revealed through kernel stack leak
       bugs. The idea of erasing the thread stack at the end of syscalls is
       similar to CONFIG_PAGE_POISONING and memzero_explicit() in kernel
       crypto, which all comply with FDP_RIP.2 (Full Residual Information
       Protection) of the Common Criteria standard.
    
    2. Blocks some uninitialized stack variable attacks (e.g. CVE-2017-17712,
       CVE-2010-2963). That kind of bugs should be killed by improving C
       compilers in future, which might take a long time.
    
    This commit introduces the code filling the used part of the kernel
    stack with a poison value before returning to userspace. Full
    STACKLEAK feature also contains the gcc plugin which comes in a
    separate commit.
    
    The STACKLEAK feature is ported from grsecurity/PaX. More information at:
      https://grsecurity.net/
      https://pax.grsecurity.net/
    
    This code is modified from Brad Spengler/PaX Team's code in the last
    public patch of grsecurity/PaX based on our understanding of the code.
    Changes or omissions from the original code are ours and don't reflect
    the original grsecurity/PaX code.
    
    Performance impact:
    
    Hardware: Intel Core i7-4770, 16 GB RAM
    
    Test #1: building the Linux kernel on a single core
            0.91% slowdown
    
    Test #2: hackbench -s 4096 -l 2000 -g 15 -f 25 -P
            4.2% slowdown
    
    So the STACKLEAK description in Kconfig includes: "The tradeoff is the
    performance impact: on a single CPU system kernel compilation sees a 1%
    slowdown, other systems and workloads may vary and you are advised to
    test this feature on your expected workload before deploying it".
    
    Signed-off-by: Alexander Popov <alex.popov@linux.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 977cb57d7bc9..c1a23acd24e7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1192,6 +1192,10 @@ struct task_struct {
 	void				*security;
 #endif
 
+#ifdef CONFIG_GCC_PLUGIN_STACKLEAK
+	unsigned long			lowest_stack;
+#endif
+
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.

commit 9da3f2b74054406f87dff7101a569217ffceb29b
Author: Jann Horn <jannh@google.com>
Date:   Tue Aug 28 22:14:20 2018 +0200

    x86/fault: BUG() when uaccess helpers fault on kernel addresses
    
    There have been multiple kernel vulnerabilities that permitted userspace to
    pass completely unchecked pointers through to userspace accessors:
    
     - the waitid() bug - commit 96ca579a1ecc ("waitid(): Add missing
       access_ok() checks")
     - the sg/bsg read/write APIs
     - the infiniband read/write APIs
    
    These don't happen all that often, but when they do happen, it is hard to
    test for them properly; and it is probably also hard to discover them with
    fuzzing. Even when an unmapped kernel address is supplied to such buggy
    code, it just returns -EFAULT instead of doing a proper BUG() or at least
    WARN().
    
    Try to make such misbehaving code a bit more visible by refusing to do a
    fixup in the pagefault handler code when a userspace accessor causes a #PF
    on a kernel address and the current context isn't whitelisted.
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: kernel-hardening@lists.openwall.com
    Cc: dvyukov@google.com
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: Borislav Petkov <bp@alien8.de>
    Link: https://lkml.kernel.org/r/20180828201421.157735-7-jannh@google.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 977cb57d7bc9..56dd65f1be4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -739,6 +739,12 @@ struct task_struct {
 	unsigned			use_memdelay:1;
 #endif
 
+	/*
+	 * May usercopy functions fault on kernel addresses?
+	 * This is not just a single bit because this can potentially nest.
+	 */
+	unsigned int			kernel_uaccess_faults_ok;
+
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
 	struct restart_block		restart_block;

commit fcc878e4dfb70128a73857c609d70570629b0d9e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 07:39:59 2018 -0700

    rcu: Remove now-unused ->b.exp_need_qs field from the rcu_special union
    
    The ->b.exp_need_qs field is now set only to false, so this commit
    removes it.  The job this field used to do is now done by the rcu_data
    structure's ->deferred_qs field, which is a consequence of a better
    split between task-based (the rcu_node structure's ->exp_tasks field) and
    CPU-based (the aforementioned rcu_data structure's ->deferred_qs field)
    tracking of quiescent states for RCU-preempt expedited grace periods.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 977cb57d7bc9..004ca21f7e80 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -571,12 +571,8 @@ union rcu_special {
 	struct {
 		u8			blocked;
 		u8			need_qs;
-		u8			exp_need_qs;
-
-		/* Otherwise the compiler can store garbage here: */
-		u8			pad;
 	} b; /* Bits. */
-	u32 s; /* Set of bits. */
+	u16 s; /* Set of bits. */
 };
 
 enum perf_event_task_context {

commit cd9b44f90763c3367e8dd0601849ffb028e8ba52
Merge: df2def49c57b 2a9d64810042
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 22 12:34:08 2018 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - procfs updates
    
     - various misc things
    
     - more y2038 fixes
    
     - get_maintainer updates
    
     - lib/ updates
    
     - checkpatch updates
    
     - various epoll updates
    
     - autofs updates
    
     - hfsplus
    
     - some reiserfs work
    
     - fatfs updates
    
     - signal.c cleanups
    
     - ipc/ updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (166 commits)
      ipc/util.c: update return value of ipc_getref from int to bool
      ipc/util.c: further variable name cleanups
      ipc: simplify ipc initialization
      ipc: get rid of ids->tables_initialized hack
      lib/rhashtable: guarantee initial hashtable allocation
      lib/rhashtable: simplify bucket_table_alloc()
      ipc: drop ipc_lock()
      ipc/util.c: correct comment in ipc_obtain_object_check
      ipc: rename ipcctl_pre_down_nolock()
      ipc/util.c: use ipc_rcu_putref() for failues in ipc_addid()
      ipc: reorganize initialization of kern_ipc_perm.seq
      ipc: compute kern_ipc_perm.id under the ipc lock
      init/Kconfig: remove EXPERT from CHECKPOINT_RESTORE
      fs/sysv/inode.c: use ktime_get_real_seconds() for superblock stamp
      adfs: use timespec64 for time conversion
      kernel/sysctl.c: fix typos in comments
      drivers/rapidio/devices/rio_mport_cdev.c: remove redundant pointer md
      fork: don't copy inconsistent signal handler state to child
      signal: make get_signal() return bool
      signal: make sigkill_pending() return bool
      ...

commit a2e514453861dd39b53b7a50b6771bd3f9852078
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Aug 21 21:55:52 2018 -0700

    kernel/hung_task.c: allow to set checking interval separately from timeout
    
    Currently task hung checking interval is equal to timeout, as the result
    hung is detected anywhere between timeout and 2*timeout.  This is fine for
    most interactive environments, but this hurts automated testing setups
    (syzbot).  In an automated setup we need to strictly order CPU lockup <
    RCU stall < workqueue lockup < task hung < silent loss, so that RCU stall
    is not detected as task hung and task hung is not detected as silent
    machine loss.  The large variance in task hung detection timeout requires
    setting silent machine loss timeout to a very large value (e.g.  if task
    hung is 3 mins, then silent loss need to be set to ~7 mins).  The
    additional 3 minutes significantly reduce testing efficiency because
    usually we crash kernel within a minute, and this can add hours to bug
    localization process as it needs to do dozens of tests.
    
    Allow setting checking interval separately from timeout.  This allows to
    set timeout to, say, 3 minutes, but checking interval to 10 secs.
    
    The interval is controlled via a new hung_task_check_interval_secs sysctl,
    similar to the existing hung_task_timeout_secs sysctl.  The default value
    of 0 results in the current behavior: checking interval is equal to
    timeout.
    
    [akpm@linux-foundation.org: update hung_task_timeout_max's comment]
    Link: http://lkml.kernel.org/r/20180611111004.203513-1-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 789923fbee3a..58eb3a2bc695 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -853,6 +853,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_DETECT_HUNG_TASK
 	unsigned long			last_switch_count;
+	unsigned long			last_switch_time;
 #endif
 	/* Filesystem information: */
 	struct fs_struct		*fs;

commit 0214f46b3a0383d6e33c297e7706216b6a550e4b
Merge: 40fafdcbcd7a 84fe4cc09abc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 21 13:47:29 2018 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull core signal handling updates from Eric Biederman:
     "It was observed that a periodic timer in combination with a
      sufficiently expensive fork could prevent fork from every completing.
      This contains the changes to remove the need for that restart.
    
      This set of changes is split into several parts:
    
       - The first part makes PIDTYPE_TGID a proper pid type instead
         something only for very special cases. The part starts using
         PIDTYPE_TGID enough so that in __send_signal where signals are
         actually delivered we know if the signal is being sent to a a group
         of processes or just a single process.
    
       - With that prep work out of the way the logic in fork is modified so
         that fork logically makes signals received while it is running
         appear to be received after the fork completes"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (22 commits)
      signal: Don't send signals to tasks that don't exist
      signal: Don't restart fork when signals come in.
      fork: Have new threads join on-going signal group stops
      fork: Skip setting TIF_SIGPENDING in ptrace_init_task
      signal: Add calculate_sigpending()
      fork: Unconditionally exit if a fatal signal is pending
      fork: Move and describe why the code examines PIDNS_ADDING
      signal: Push pid type down into complete_signal.
      signal: Push pid type down into __send_signal
      signal: Push pid type down into send_signal
      signal: Pass pid type into do_send_sig_info
      signal: Pass pid type into send_sigio_to_task & send_sigurg_to_task
      signal: Pass pid type into group_send_sig_info
      signal: Pass pid and pid type into send_sigqueue
      posix-timers: Noralize good_sigevent
      signal: Use PIDTYPE_TGID to clearly store where file signals will be sent
      pid: Implement PIDTYPE_TGID
      pids: Move the pgrp and session pid pointers from task_struct to signal_struct
      kvm: Don't open code task_pid in kvm_vcpu_ioctl
      pids: Compute task_tgid using signal->leader_pid
      ...

commit 84c07d11aa619c6d24c682f469b10f344f0c02aa
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri Aug 17 15:47:25 2018 -0700

    mm: introduce CONFIG_MEMCG_KMEM as combination of CONFIG_MEMCG && !CONFIG_SLOB
    
    Introduce new config option, which is used to replace repeating
    CONFIG_MEMCG && !CONFIG_SLOB pattern.  Next patches add a little more
    memcg+kmem related code, so let's keep the defines more clearly.
    
    Link: http://lkml.kernel.org/r/153063053670.1818.15013136946600481138.stgit@localhost.localdomain
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Tested-by: Shakeel Butt <shakeelb@google.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: "Huang, Ying" <ying.huang@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: Li RongQing <lirongqing@baidu.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matthias Kaehlcke <mka@chromium.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Sahitya Tummala <stummala@codeaurora.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 066a2c328653..789923fbee3a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -723,7 +723,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_MEMCG
 	unsigned			in_user_fault:1;
-#ifndef CONFIG_SLOB
+#ifdef CONFIG_MEMCG_KMEM
 	unsigned			memcg_kmem_skip_account:1;
 #endif
 #endif

commit 29ef680ae7c21110af8e6416d84d8a72fc147b14
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Aug 17 15:47:11 2018 -0700

    memcg, oom: move out_of_memory back to the charge path
    
    Commit 3812c8c8f395 ("mm: memcg: do not trap chargers with full
    callstack on OOM") has changed the ENOMEM semantic of memcg charges.
    Rather than invoking the oom killer from the charging context it delays
    the oom killer to the page fault path (pagefault_out_of_memory).  This
    in turn means that many users (e.g.  slab or g-u-p) will get ENOMEM when
    the corresponding memcg hits the hard limit and the memcg is is OOM.
    This is behavior is inconsistent with !memcg case where the oom killer
    is invoked from the allocation context and the allocator keeps retrying
    until it succeeds.
    
    The difference in the behavior is user visible.  mmap(MAP_POPULATE)
    might result in not fully populated ranges while the mmap return code
    doesn't tell that to the userspace.  Random syscalls might fail with
    ENOMEM etc.
    
    The primary motivation of the different memcg oom semantic was the
    deadlock avoidance.  Things have changed since then, though.  We have an
    async oom teardown by the oom reaper now and so we do not have to rely
    on the victim to tear down its memory anymore.  Therefore we can return
    to the original semantic as long as the memcg oom killer is not handed
    over to the users space.
    
    There is still one thing to be careful about here though.  If the oom
    killer is not able to make any forward progress - e.g.  because there is
    no eligible task to kill - then we have to bail out of the charge path
    to prevent from same class of deadlocks.  We have basically two options
    here.  Either we fail the charge with ENOMEM or force the charge and
    allow overcharge.  The first option has been considered more harmful
    than useful because rare inconsistencies in the ENOMEM behavior is hard
    to test for and error prone.  Basically the same reason why the page
    allocator doesn't fail allocations under such conditions.  The later
    might allow runaways but those should be really unlikely unless somebody
    misconfigures the system.  E.g.  allowing to migrate tasks away from the
    memcg to a different unlimited memcg with move_charge_at_immigrate
    disabled.
    
    Link: http://lkml.kernel.org/r/20180628151101.25307-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1827f4a7a6de..066a2c328653 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -722,7 +722,7 @@ struct task_struct {
 	unsigned			restore_sigmask:1;
 #endif
 #ifdef CONFIG_MEMCG
-	unsigned			memcg_may_oom:1;
+	unsigned			in_user_fault:1;
 #ifndef CONFIG_SLOB
 	unsigned			memcg_kmem_skip_account:1;
 #endif

commit d46eb14b735b11927d4bdc2d1854c311af19de6d
Author: Shakeel Butt <shakeelb@google.com>
Date:   Fri Aug 17 15:46:39 2018 -0700

    fs: fsnotify: account fsnotify metadata to kmemcg
    
    Patch series "Directed kmem charging", v8.
    
    The Linux kernel's memory cgroup allows limiting the memory usage of the
    jobs running on the system to provide isolation between the jobs.  All
    the kernel memory allocated in the context of the job and marked with
    __GFP_ACCOUNT will also be included in the memory usage and be limited
    by the job's limit.
    
    The kernel memory can only be charged to the memcg of the process in
    whose context kernel memory was allocated.  However there are cases
    where the allocated kernel memory should be charged to the memcg
    different from the current processes's memcg.  This patch series
    contains two such concrete use-cases i.e.  fsnotify and buffer_head.
    
    The fsnotify event objects can consume a lot of system memory for large
    or unlimited queues if there is either no or slow listener.  The events
    are allocated in the context of the event producer.  However they should
    be charged to the event consumer.  Similarly the buffer_head objects can
    be allocated in a memcg different from the memcg of the page for which
    buffer_head objects are being allocated.
    
    To solve this issue, this patch series introduces mechanism to charge
    kernel memory to a given memcg.  In case of fsnotify events, the memcg
    of the consumer can be used for charging and for buffer_head, the memcg
    of the page can be charged.  For directed charging, the caller can use
    the scope API memalloc_[un]use_memcg() to specify the memcg to charge
    for all the __GFP_ACCOUNT allocations within the scope.
    
    This patch (of 2):
    
    A lot of memory can be consumed by the events generated for the huge or
    unlimited queues if there is either no or slow listener.  This can cause
    system level memory pressure or OOMs.  So, it's better to account the
    fsnotify kmem caches to the memcg of the listener.
    
    However the listener can be in a different memcg than the memcg of the
    producer and these allocations happen in the context of the event
    producer.  This patch introduces remote memcg charging API which the
    producer can use to charge the allocations to the memcg of the listener.
    
    There are seven fsnotify kmem caches and among them allocations from
    dnotify_struct_cache, dnotify_mark_cache, fanotify_mark_cache and
    inotify_inode_mark_cachep happens in the context of syscall from the
    listener.  So, SLAB_ACCOUNT is enough for these caches.
    
    The objects from fsnotify_mark_connector_cachep are not accounted as
    they are small compared to the notification mark or events and it is
    unclear whom to account connector to since it is shared by all events
    attached to the inode.
    
    The allocations from the event caches happen in the context of the event
    producer.  For such caches we will need to remote charge the allocations
    to the listener's memcg.  Thus we save the memcg reference in the
    fsnotify_group structure of the listener.
    
    This patch has also moved the members of fsnotify_group to keep the size
    same, at least for 64 bit build, even with additional member by filling
    the holes.
    
    [shakeelb@google.com: use GFP_KERNEL_ACCOUNT rather than open-coding it]
      Link: http://lkml.kernel.org/r/20180702215439.211597-1-shakeelb@google.com
    Link: http://lkml.kernel.org/r/20180627191250.209150-2-shakeelb@google.com
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Amir Goldstein <amir73il@gmail.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 95a5018c338e..1827f4a7a6de 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1152,6 +1152,9 @@ struct task_struct {
 
 	/* Number of pages to reclaim on returning to userland: */
 	unsigned int			memcg_nr_pages_over_high;
+
+	/* Used by memcontrol for targeted memcg charge: */
+	struct mem_cgroup		*active_memcg;
 #endif
 
 #ifdef CONFIG_BLK_CGROUP

commit 73ba2fb33c492916853dfe63e3b3163da0be661d
Merge: 958f338e96f8 b86d865cb1ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 10:23:25 2018 -0700

    Merge tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "First pull request for this merge window, there will also be a
      followup request with some stragglers.
    
      This pull request contains:
    
       - Fix for a thundering heard issue in the wbt block code (Anchal
         Agarwal)
    
       - A few NVMe pull requests:
          * Improved tracepoints (Keith)
          * Larger inline data support for RDMA (Steve Wise)
          * RDMA setup/teardown fixes (Sagi)
          * Effects log suppor for NVMe target (Chaitanya Kulkarni)
          * Buffered IO suppor for NVMe target (Chaitanya Kulkarni)
          * TP4004 (ANA) support (Christoph)
          * Various NVMe fixes
    
       - Block io-latency controller support. Much needed support for
         properly containing block devices. (Josef)
    
       - Series improving how we handle sense information on the stack
         (Kees)
    
       - Lightnvm fixes and updates/improvements (Mathias/Javier et al)
    
       - Zoned device support for null_blk (Matias)
    
       - AIX partition fixes (Mauricio Faria de Oliveira)
    
       - DIF checksum code made generic (Max Gurtovoy)
    
       - Add support for discard in iostats (Michael Callahan / Tejun)
    
       - Set of updates for BFQ (Paolo)
    
       - Removal of async write support for bsg (Christoph)
    
       - Bio page dirtying and clone fixups (Christoph)
    
       - Set of bcache fix/changes (via Coly)
    
       - Series improving blk-mq queue setup/teardown speed (Ming)
    
       - Series improving merging performance on blk-mq (Ming)
    
       - Lots of other fixes and cleanups from a slew of folks"
    
    * tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block: (190 commits)
      blkcg: Make blkg_root_lookup() work for queues in bypass mode
      bcache: fix error setting writeback_rate through sysfs interface
      null_blk: add lock drop/acquire annotation
      Blk-throttle: reduce tail io latency when iops limit is enforced
      block: paride: pd: mark expected switch fall-throughs
      block: Ensure that a request queue is dissociated from the cgroup controller
      block: Introduce blk_exit_queue()
      blkcg: Introduce blkg_root_lookup()
      block: Remove two superfluous #include directives
      blk-mq: count the hctx as active before allocating tag
      block: bvec_nr_vecs() returns value for wrong slab
      bcache: trivial - remove tailing backslash in macro BTREE_FLAG
      bcache: make the pr_err statement used for ENOENT only in sysfs_attatch section
      bcache: set max writeback rate when I/O request is idle
      bcache: add code comments for bset.c
      bcache: fix mistaken comments in request.c
      bcache: fix mistaken code comments in bcache.h
      bcache: add a comment in super.c
      bcache: avoid unncessary cache prefetch bch_btree_node_get()
      bcache: display rate debug parameters to 0 when writeback is not running
      ...

commit de5d1b39ea0b38a9f4dfb08966042b7b91e2df30
Merge: 1c594774283a fd2efaa4eb53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 12:23:39 2018 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking/atomics update from Thomas Gleixner:
     "The locking, atomics and memory model brains delivered:
    
       - A larger update to the atomics code which reworks the ordering
         barriers, consolidates the atomic primitives, provides the new
         atomic64_fetch_add_unless() primitive and cleans up the include
         hell.
    
       - Simplify cmpxchg() instrumentation and add instrumentation for
         xchg() and cmpxchg_double().
    
       - Updates to the memory model and documentation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      locking/atomics: Rework ordering barriers
      locking/atomics: Instrument cmpxchg_double*()
      locking/atomics: Instrument xchg()
      locking/atomics: Simplify cmpxchg() instrumentation
      locking/atomics/x86: Reduce arch_cmpxchg64*() instrumentation
      tools/memory-model: Rename litmus tests to comply to norm7
      tools/memory-model/Documentation: Fix typo, smb->smp
      sched/Documentation: Update wake_up() & co. memory-barrier guarantees
      locking/spinlock, sched/core: Clarify requirements for smp_mb__after_spinlock()
      sched/core: Use smp_mb() in wake_woken_function()
      tools/memory-model: Add informal LKMM documentation to MAINTAINERS
      locking/atomics/Documentation: Describe atomic_set() as a write operation
      tools/memory-model: Make scripts executable
      tools/memory-model: Remove ACCESS_ONCE() from model
      tools/memory-model: Remove ACCESS_ONCE() from recipes
      locking/memory-barriers.txt/kokr: Update Korean translation to fix broken DMA vs. MMIO ordering example
      MAINTAINERS: Add Daniel Lustig as an LKMM reviewer
      tools/memory-model: Fix ISA2+pooncelock+pooncelock+pombonce name
      tools/memory-model: Add litmus test for full multicopy atomicity
      locking/refcount: Always allow checked forms
      ...

commit 6e30396767508101eacec8b93b068e8905e660dc
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Jun 20 22:32:42 2018 +0530

    sched/numa: Remove redundant field
    
    'numa_entry' is a struct list_head defined in task_struct, but never used.
    
    No functional change.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1529514181-9842-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43731fe51c97..e0f4f56c9310 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1017,7 +1017,6 @@ struct task_struct {
 	u64				last_sum_exec_runtime;
 	struct callback_head		numa_work;
 
-	struct list_head		numa_entry;
 	struct numa_group		*numa_group;
 
 	/*

commit 6883f81aac6f44e7df70a6af189b3689ff52cbfb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jun 4 04:32:13 2017 -0500

    pid: Implement PIDTYPE_TGID
    
    Everywhere except in the pid array we distinguish between a tasks pid and
    a tasks tgid (thread group id).  Even in the enumeration we want that
    distinction sometimes so we have added __PIDTYPE_TGID.  With leader_pid
    we almost have an implementation of PIDTYPE_TGID in struct signal_struct.
    
    Add PIDTYPE_TGID as a first class member of the pid_type enumeration and
    into the pids array.  Then remove the __PIDTYPE_TGID special case and the
    leader_pid in signal_struct.
    
    The net size increase is just an extra pointer added to struct pid and
    an extra pair of pointers of an hlist_node added to task_struct.
    
    The effect on code maintenance is the removal of a number of special
    cases today and the potential to remove many more special cases as
    PIDTYPE_TGID gets used to it's fullest.  The long term potential
    is allowing zombie thread group leaders to exit, which will remove
    a lot more special cases in the code.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 445bdf5b1f64..06b4e3bda93a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1275,12 +1275,12 @@ static inline pid_t task_session_vnr(struct task_struct *tsk)
 
 static inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
 {
-	return __task_pid_nr_ns(tsk, __PIDTYPE_TGID, ns);
+	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);
 }
 
 static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 {
-	return __task_pid_nr_ns(tsk, __PIDTYPE_TGID, NULL);
+	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, NULL);
 }
 
 static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)

commit 2c4704756cab7cfa031ada4dab361562f0e357c0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 26 13:06:43 2017 -0500

    pids: Move the pgrp and session pid pointers from task_struct to signal_struct
    
    To access these fields the code always has to go to group leader so
    going to signal struct is no loss and is actually a fundamental simplification.
    
    This saves a little bit of memory by only allocating the pid pointer array
    once instead of once for every thread, and even better this removes a
    few potential races caused by the fact that group_leader can be changed
    by de_thread, while signal_struct can not.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a461ff89a3af..445bdf5b1f64 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -775,7 +775,8 @@ struct task_struct {
 	struct list_head		ptrace_entry;
 
 	/* PID/PID hash table linkage. */
-	struct pid_link			pids[PIDTYPE_MAX];
+	struct pid			*thread_pid;
+	struct hlist_node		pid_links[PIDTYPE_MAX];
 	struct list_head		thread_group;
 	struct list_head		thread_node;
 
@@ -1199,22 +1200,7 @@ struct task_struct {
 
 static inline struct pid *task_pid(struct task_struct *task)
 {
-	return task->pids[PIDTYPE_PID].pid;
-}
-
-/*
- * Without tasklist or RCU lock it is not safe to dereference
- * the result of task_pgrp/task_session even if task == current,
- * we can race with another thread doing sys_setsid/sys_setpgid.
- */
-static inline struct pid *task_pgrp(struct task_struct *task)
-{
-	return task->group_leader->pids[PIDTYPE_PGID].pid;
-}
-
-static inline struct pid *task_session(struct task_struct *task)
-{
-	return task->group_leader->pids[PIDTYPE_SID].pid;
+	return task->thread_pid;
 }
 
 /*
@@ -1263,7 +1249,7 @@ static inline pid_t task_tgid_nr(struct task_struct *tsk)
  */
 static inline int pid_alive(const struct task_struct *p)
 {
-	return p->pids[PIDTYPE_PID].pid != NULL;
+	return p->thread_pid != NULL;
 }
 
 static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)

commit 7a36094d61bfe9843de5484ff0140227983ac5d5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 26 12:45:33 2017 -0500

    pids: Compute task_tgid using signal->leader_pid
    
    The cost is the the same and this removes the need
    to worry about complications that come from de_thread
    and group_leader changing.
    
    __task_pid_nr_ns has been updated to take advantage of this change.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 87bf02d93a27..a461ff89a3af 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1202,11 +1202,6 @@ static inline struct pid *task_pid(struct task_struct *task)
 	return task->pids[PIDTYPE_PID].pid;
 }
 
-static inline struct pid *task_tgid(struct task_struct *task)
-{
-	return task->group_leader->pids[PIDTYPE_PID].pid;
-}
-
 /*
  * Without tasklist or RCU lock it is not safe to dereference
  * the result of task_pgrp/task_session even if task == current,

commit 7696f9910a9a40b8a952f57d3428515fabd2d889
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon Jul 16 11:06:03 2018 -0700

    sched/Documentation: Update wake_up() & co. memory-barrier guarantees
    
    Both the implementation and the users' expectation [1] for the various
    wakeup primitives have evolved over time, but the documentation has not
    kept up with these changes: brings it into 2018.
    
    [1] http://lkml.kernel.org/r/20180424091510.GB4064@hirez.programming.kicks-ass.net
    
    Also applied feedback from Alan Stern.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Akira Yokosawa <akiyks@gmail.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Daniel Lustig <dlustig@nvidia.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jade Alglave <j.alglave@ucl.ac.uk>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Maranget <luc.maranget@inria.fr>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: parri.andrea@gmail.com
    Link: http://lkml.kernel.org/r/20180716180605.16115-12-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43731fe51c97..05cd419f962d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -167,8 +167,8 @@ struct task_group;
  *   need_sleep = false;
  *   wake_up_state(p, TASK_UNINTERRUPTIBLE);
  *
- * Where wake_up_state() (and all other wakeup primitives) imply enough
- * barriers to order the store of the variable against wakeup.
+ * where wake_up_state() executes a full memory barrier before accessing the
+ * task state.
  *
  * Wakeup will do: if (@state & p->state) p->state = TASK_RUNNING, that is,
  * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a

commit d09d8df3a29403693d9d20cc34ed101f2c558e2b
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Jul 3 11:14:55 2018 -0400

    blkcg: add generic throttling mechanism
    
    Since IO can be issued from literally anywhere it's almost impossible to
    do throttling without having some sort of adverse effect somewhere else
    in the system because of locking or other dependencies.  The best way to
    solve this is to do the throttling when we know we aren't holding any
    other kernel resources.  Do this by tracking throttling in a per-blkg
    basis, and if we require throttling flag the task that it needs to check
    before it returns to user space and possibly sleep there.
    
    This is to address the case where a process is doing work that is
    generating IO that can't be throttled, whether that is directly with a
    lot of REQ_META IO, or indirectly by allocating so much memory that it
    is swamping the disk with REQ_SWAP.  We can't use task_add_work as we
    don't want to induce a memory allocation in the IO path, so simply
    saving the request queue in the task and flagging it to do the
    notify_resume thing achieves the same result without the overhead of a
    memory allocation.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43731fe51c97..c2e993de67ec 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -734,6 +734,10 @@ struct task_struct {
 	/* disallow userland-initiated cgroup migration */
 	unsigned			no_cgroup_migration:1;
 #endif
+#ifdef CONFIG_BLK_CGROUP
+	/* to be used once the psi infrastructure lands upstream. */
+	unsigned			use_memdelay:1;
+#endif
 
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
@@ -1151,6 +1155,10 @@ struct task_struct {
 	unsigned int			memcg_nr_pages_over_high;
 #endif
 
+#ifdef CONFIG_BLK_CGROUP
+	struct request_queue		*throttle_queue;
+#endif
+
 #ifdef CONFIG_UPROBES
 	struct uprobe_task		*utask;
 #endif

commit 1cef1150ef40ec52f507436a14230cbc2623299c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 7 11:45:49 2018 +0200

    kthread, sched/core: Fix kthread_parkme() (again...)
    
    Gaurav reports that commit:
    
      85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    
    isn't working for him. Because of the following race:
    
    > controller Thread                               CPUHP Thread
    > takedown_cpu
    > kthread_park
    > kthread_parkme
    > Set KTHREAD_SHOULD_PARK
    >                                                 smpboot_thread_fn
    >                                                 set Task interruptible
    >
    >
    > wake_up_process
    >  if (!(p->state & state))
    >                 goto out;
    >
    >                                                 Kthread_parkme
    >                                                 SET TASK_PARKED
    >                                                 schedule
    >                                                 raw_spin_lock(&rq->lock)
    > ttwu_remote
    > waiting for __task_rq_lock
    >                                                 context_switch
    >
    >                                                 finish_lock_switch
    >
    >
    >
    >                                                 Case TASK_PARKED
    >                                                 kthread_park_complete
    >
    >
    > SET Running
    
    Furthermore, Oleg noticed that the whole scheduler TASK_PARKED
    handling is buggered because the TASK_DEAD thing is done with
    preemption disabled, the current code can still complete early on
    preemption :/
    
    So basically revert that earlier fix and go with a variant of the
    alternative mentioned in the commit. Promote TASK_PARKED to special
    state to avoid the store-store issue on task->state leading to the
    WARN in kthread_unpark() -> __kthread_bind().
    
    But in addition, add wait_task_inactive() to kthread_park() to ensure
    the task really is PARKED when we return from kthread_park(). This
    avoids the whole kthread still gets migrated nonsense -- although it
    would be really good to get this done differently.
    
    Reported-by: Gaurav Kohli <gkohli@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9256118bd40c..43731fe51c97 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -118,7 +118,7 @@ struct task_group;
  * the comment with set_special_state().
  */
 #define is_special_task_state(state)				\
-	((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_DEAD))
+	((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_PARKED | TASK_DEAD))
 
 #define __set_current_state(state_value)			\
 	do {							\

commit 784e0300fe9fe4aa81bd7df9d59e138f56bb605b
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 22 11:45:07 2018 +0100

    rseq: Avoid infinite recursion when delivering SIGSEGV
    
    When delivering a signal to a task that is using rseq, we call into
    __rseq_handle_notify_resume() so that the registers pushed in the
    sigframe are updated to reflect the state of the restartable sequence
    (for example, ensuring that the signal returns to the abort handler if
    necessary).
    
    However, if the rseq management fails due to an unrecoverable fault when
    accessing userspace or certain combinations of RSEQ_CS_* flags, then we
    will attempt to deliver a SIGSEGV. This has the potential for infinite
    recursion if the rseq code continuously fails on signal delivery.
    
    Avoid this problem by using force_sigsegv() instead of force_sig(), which
    is explicitly designed to reset the SEGV handler to SIG_DFL in the case
    of a recursive fault. In doing so, remove rseq_signal_deliver() from the
    internal rseq API and have an optional struct ksignal * parameter to
    rseq_handle_notify_resume() instead.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: peterz@infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Cc: boqun.feng@gmail.com
    Link: https://lkml.kernel.org/r/1529664307-983-1-git-send-email-will.deacon@arm.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1882643d455..9256118bd40c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1799,20 +1799,22 @@ static inline void rseq_set_notify_resume(struct task_struct *t)
 		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
 }
 
-void __rseq_handle_notify_resume(struct pt_regs *regs);
+void __rseq_handle_notify_resume(struct ksignal *sig, struct pt_regs *regs);
 
-static inline void rseq_handle_notify_resume(struct pt_regs *regs)
+static inline void rseq_handle_notify_resume(struct ksignal *ksig,
+					     struct pt_regs *regs)
 {
 	if (current->rseq)
-		__rseq_handle_notify_resume(regs);
+		__rseq_handle_notify_resume(ksig, regs);
 }
 
-static inline void rseq_signal_deliver(struct pt_regs *regs)
+static inline void rseq_signal_deliver(struct ksignal *ksig,
+				       struct pt_regs *regs)
 {
 	preempt_disable();
 	__set_bit(RSEQ_EVENT_SIGNAL_BIT, &current->rseq_event_mask);
 	preempt_enable();
-	rseq_handle_notify_resume(regs);
+	rseq_handle_notify_resume(ksig, regs);
 }
 
 /* rseq_preempt() requires preemption to be disabled. */
@@ -1861,10 +1863,12 @@ static inline void rseq_execve(struct task_struct *t)
 static inline void rseq_set_notify_resume(struct task_struct *t)
 {
 }
-static inline void rseq_handle_notify_resume(struct pt_regs *regs)
+static inline void rseq_handle_notify_resume(struct ksignal *ksig,
+					     struct pt_regs *regs)
 {
 }
-static inline void rseq_signal_deliver(struct pt_regs *regs)
+static inline void rseq_signal_deliver(struct ksignal *ksig,
+				       struct pt_regs *regs)
 {
 }
 static inline void rseq_preempt(struct task_struct *t)

commit 9a789fcfe8605417f7a1a970355f5efa4fe88c64
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Tue Jun 19 09:32:30 2018 -0400

    rseq/cleanup: Do not abort rseq c.s. in child on fork()
    
    Considering that we explicitly forbid system calls in rseq critical
    sections, it is not valid to issue a fork or clone system call within a
    rseq critical section, so rseq_fork() is not required to restart an
    active rseq c.s. in the child process.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ben Maurer <bmaurer@fb.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Lameter <cl@linux.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-api@vger.kernel.org
    Cc: linux-kselftest@vger.kernel.org
    Link: https://lore.kernel.org/lkml/20180619133230.4087-4-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 87bf02d93a27..c1882643d455 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1831,9 +1831,7 @@ static inline void rseq_migrate(struct task_struct *t)
 
 /*
  * If parent process has a registered restartable sequences area, the
- * child inherits. Only applies when forking a process, not a thread. In
- * case a parent fork() in the middle of a restartable sequence, set the
- * resume notifier to force the child to retry.
+ * child inherits. Only applies when forking a process, not a thread.
  */
 static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
 {
@@ -1847,7 +1845,6 @@ static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
 		t->rseq_len = current->rseq_len;
 		t->rseq_sig = current->rseq_sig;
 		t->rseq_event_mask = current->rseq_event_mask;
-		rseq_preempt(t);
 	}
 }
 

commit 0ed557aa813922f6f32adec69e266532091c895b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jun 14 15:27:41 2018 -0700

    sched/core / kcov: avoid kcov_area during task switch
    
    During a context switch, we first switch_mm() to the next task's mm,
    then switch_to() that new task.  This means that vmalloc'd regions which
    had previously been faulted in can transiently disappear in the context
    of the prev task.
    
    Functions instrumented by KCOV may try to access a vmalloc'd kcov_area
    during this window, and as the fault handling code is instrumented, this
    results in a recursive fault.
    
    We must avoid accessing any kcov_area during this window.  We can do so
    with a new flag in kcov_mode, set prior to switching the mm, and cleared
    once the new task is live.  Since task_struct::kcov_mode isn't always a
    specific enum kcov_mode value, this is made an unsigned int.
    
    The manipulation is hidden behind kcov_{prepare,finish}_switch() helpers,
    which are empty for !CONFIG_KCOV kernels.
    
    The code uses macros because I can't use static inline functions without a
    circular include dependency between <linux/sched.h> and <linux/kcov.h>,
    since the definition of task_struct uses things defined in <linux/kcov.h>
    
    Link: http://lkml.kernel.org/r/20180504135535.53744-4-mark.rutland@arm.com
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cfb7da88c217..87bf02d93a27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1130,7 +1130,7 @@ struct task_struct {
 
 #ifdef CONFIG_KCOV
 	/* Coverage collection mode enabled for this task (0 if disabled): */
-	enum kcov_mode			kcov_mode;
+	unsigned int			kcov_mode;
 
 	/* Size of the kcov_area: */
 	unsigned int			kcov_size;

commit 050e9baa9dc9fbd9ce2b27f0056990fc9e0a08a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 14 12:21:18 2018 +0900

    Kbuild: rename CC_STACKPROTECTOR[_STRONG] config variables
    
    The changes to automatically test for working stack protector compiler
    support in the Kconfig files removed the special STACKPROTECTOR_AUTO
    option that picked the strongest stack protector that the compiler
    supported.
    
    That was all a nice cleanup - it makes no sense to have the AUTO case
    now that the Kconfig phase can just determine the compiler support
    directly.
    
    HOWEVER.
    
    It also meant that doing "make oldconfig" would now _disable_ the strong
    stackprotector if you had AUTO enabled, because in a legacy config file,
    the sane stack protector configuration would look like
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_NONE is not set
      # CONFIG_CC_STACKPROTECTOR_REGULAR is not set
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_STACKPROTECTOR_AUTO=y
    
    and when you ran this through "make oldconfig" with the Kbuild changes,
    it would ask you about the regular CONFIG_CC_STACKPROTECTOR (that had
    been renamed from CONFIG_CC_STACKPROTECTOR_REGULAR to just
    CONFIG_CC_STACKPROTECTOR), but it would think that the STRONG version
    used to be disabled (because it was really enabled by AUTO), and would
    disable it in the new config, resulting in:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    That's dangerously subtle - people could suddenly find themselves with
    the weaker stack protector setup without even realizing.
    
    The solution here is to just rename not just the old RECULAR stack
    protector option, but also the strong one.  This does that by just
    removing the CC_ prefix entirely for the user choices, because it really
    is not about the compiler support (the compiler support now instead
    automatially impacts _visibility_ of the options to users).
    
    This results in "make oldconfig" actually asking the user for their
    choice, so that we don't have any silent subtle security model changes.
    The end result would generally look like this:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_STACKPROTECTOR=y
      CONFIG_STACKPROTECTOR_STRONG=y
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    where the "CC_" versions really are about internal compiler
    infrastructure, not the user selections.
    
    Acked-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 16e4d984fe51..cfb7da88c217 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -742,7 +742,7 @@ struct task_struct {
 	pid_t				pid;
 	pid_t				tgid;
 
-#ifdef CONFIG_CC_STACKPROTECTOR
+#ifdef CONFIG_STACKPROTECTOR
 	/* Canary value for the -fstack-protector GCC feature: */
 	unsigned long			stack_canary;
 #endif

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit d7822b1e24f2df5df98c76f0e94a5416349ff759
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Sat Jun 2 08:43:54 2018 -0400

    rseq: Introduce restartable sequences system call
    
    Expose a new system call allowing each thread to register one userspace
    memory area to be used as an ABI between kernel and user-space for two
    purposes: user-space restartable sequences and quick access to read the
    current CPU number value from user-space.
    
    * Restartable sequences (per-cpu atomics)
    
    Restartables sequences allow user-space to perform update operations on
    per-cpu data without requiring heavy-weight atomic operations.
    
    The restartable critical sections (percpu atomics) work has been started
    by Paul Turner and Andrew Hunter. It lets the kernel handle restart of
    critical sections. [1] [2] The re-implementation proposed here brings a
    few simplifications to the ABI which facilitates porting to other
    architectures and speeds up the user-space fast path.
    
    Here are benchmarks of various rseq use-cases.
    
    Test hardware:
    
    arm32: ARMv7 Processor rev 4 (v7l) "Cubietruck", 2-core
    x86-64: Intel E5-2630 v3@2.40GHz, 16-core, hyperthreading
    
    The following benchmarks were all performed on a single thread.
    
    * Per-CPU statistic counter increment
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:                344.0                 31.4          11.0
    x86-64:                15.3                  2.0           7.7
    
    * LTTng-UST: write event 32-bit header, 32-bit payload into tracer
                 per-cpu buffer
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:               2502.0                 2250.0         1.1
    x86-64:               117.4                   98.0         1.2
    
    * liburcu percpu: lock-unlock pair, dereference, read/compare word
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:                751.0                 128.5          5.8
    x86-64:                53.4                  28.6          1.9
    
    * jemalloc memory allocator adapted to use rseq
    
    Using rseq with per-cpu memory pools in jemalloc at Facebook (based on
    rseq 2016 implementation):
    
    The production workload response-time has 1-2% gain avg. latency, and
    the P99 overall latency drops by 2-3%.
    
    * Reading the current CPU number
    
    Speeding up reading the current CPU number on which the caller thread is
    running is done by keeping the current CPU number up do date within the
    cpu_id field of the memory area registered by the thread. This is done
    by making scheduler preemption set the TIF_NOTIFY_RESUME flag on the
    current thread. Upon return to user-space, a notify-resume handler
    updates the current CPU value within the registered user-space memory
    area. User-space can then read the current CPU number directly from
    memory.
    
    Keeping the current cpu id in a memory area shared between kernel and
    user-space is an improvement over current mechanisms available to read
    the current CPU number, which has the following benefits over
    alternative approaches:
    
    - 35x speedup on ARM vs system call through glibc
    - 20x speedup on x86 compared to calling glibc, which calls vdso
      executing a "lsl" instruction,
    - 14x speedup on x86 compared to inlined "lsl" instruction,
    - Unlike vdso approaches, this cpu_id value can be read from an inline
      assembly, which makes it a useful building block for restartable
      sequences.
    - The approach of reading the cpu id through memory mapping shared
      between kernel and user-space is portable (e.g. ARM), which is not the
      case for the lsl-based x86 vdso.
    
    On x86, yet another possible approach would be to use the gs segment
    selector to point to user-space per-cpu data. This approach performs
    similarly to the cpu id cache, but it has two disadvantages: it is
    not portable, and it is incompatible with existing applications already
    using the gs segment selector for other purposes.
    
    Benchmarking various approaches for reading the current CPU number:
    
    ARMv7 Processor rev 4 (v7l)
    Machine model: Cubietruck
    - Baseline (empty loop):                                    8.4 ns
    - Read CPU from rseq cpu_id:                               16.7 ns
    - Read CPU from rseq cpu_id (lazy register):               19.8 ns
    - glibc 2.19-0ubuntu6.6 getcpu:                           301.8 ns
    - getcpu system call:                                     234.9 ns
    
    x86-64 Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz:
    - Baseline (empty loop):                                    0.8 ns
    - Read CPU from rseq cpu_id:                                0.8 ns
    - Read CPU from rseq cpu_id (lazy register):                0.8 ns
    - Read using gs segment selector:                           0.8 ns
    - "lsl" inline assembly:                                   13.0 ns
    - glibc 2.19-0ubuntu6 getcpu:                              16.6 ns
    - getcpu system call:                                      53.9 ns
    
    - Speed (benchmark taken on v8 of patchset)
    
    Running 10 runs of hackbench -l 100000 seems to indicate, contrary to
    expectations, that enabling CONFIG_RSEQ slightly accelerates the
    scheduler:
    
    Configuration: 2 sockets * 8-core Intel(R) Xeon(R) CPU E5-2630 v3 @
    2.40GHz (directly on hardware, hyperthreading disabled in BIOS, energy
    saving disabled in BIOS, turboboost disabled in BIOS, cpuidle.off=1
    kernel parameter), with a Linux v4.6 defconfig+localyesconfig,
    restartable sequences series applied.
    
    * CONFIG_RSEQ=n
    
    avg.:      41.37 s
    std.dev.:   0.36 s
    
    * CONFIG_RSEQ=y
    
    avg.:      40.46 s
    std.dev.:   0.33 s
    
    - Size
    
    On x86-64, between CONFIG_RSEQ=n/y, the text size increase of vmlinux is
    567 bytes, and the data size increase of vmlinux is 5696 bytes.
    
    [1] https://lwn.net/Articles/650333/
    [2] http://www.linuxplumbersconf.org/2013/ocw/system/presentations/1695/original/LPC%20-%20PerCpu%20Atomics.pdf
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Chris Lameter <cl@linux.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ben Maurer <bmaurer@fb.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-api@vger.kernel.org
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20151027235635.16059.11630.stgit@pjt-glaptop.roam.corp.google.com
    Link: http://lkml.kernel.org/r/20150624222609.6116.86035.stgit@kitami.mtv.corp.google.com
    Link: https://lkml.kernel.org/r/20180602124408.8430-3-mathieu.desnoyers@efficios.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 14e4f9c12337..3aa4fcb74e76 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -27,6 +27,7 @@
 #include <linux/signal_types.h>
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
+#include <linux/rseq.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -1047,6 +1048,17 @@ struct task_struct {
 	unsigned long			numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_RSEQ
+	struct rseq __user *rseq;
+	u32 rseq_len;
+	u32 rseq_sig;
+	/*
+	 * RmW on rseq_event_mask must be performed atomically
+	 * with respect to preemption.
+	 */
+	unsigned long rseq_event_mask;
+#endif
+
 	struct tlbflush_unmap_batch	tlb_ubc;
 
 	struct rcu_head			rcu;
@@ -1757,4 +1769,126 @@ extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
+#ifdef CONFIG_RSEQ
+
+/*
+ * Map the event mask on the user-space ABI enum rseq_cs_flags
+ * for direct mask checks.
+ */
+enum rseq_event_mask_bits {
+	RSEQ_EVENT_PREEMPT_BIT	= RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT,
+	RSEQ_EVENT_SIGNAL_BIT	= RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT,
+	RSEQ_EVENT_MIGRATE_BIT	= RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT,
+};
+
+enum rseq_event_mask {
+	RSEQ_EVENT_PREEMPT	= (1U << RSEQ_EVENT_PREEMPT_BIT),
+	RSEQ_EVENT_SIGNAL	= (1U << RSEQ_EVENT_SIGNAL_BIT),
+	RSEQ_EVENT_MIGRATE	= (1U << RSEQ_EVENT_MIGRATE_BIT),
+};
+
+static inline void rseq_set_notify_resume(struct task_struct *t)
+{
+	if (t->rseq)
+		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
+}
+
+void __rseq_handle_notify_resume(struct pt_regs *regs);
+
+static inline void rseq_handle_notify_resume(struct pt_regs *regs)
+{
+	if (current->rseq)
+		__rseq_handle_notify_resume(regs);
+}
+
+static inline void rseq_signal_deliver(struct pt_regs *regs)
+{
+	preempt_disable();
+	__set_bit(RSEQ_EVENT_SIGNAL_BIT, &current->rseq_event_mask);
+	preempt_enable();
+	rseq_handle_notify_resume(regs);
+}
+
+/* rseq_preempt() requires preemption to be disabled. */
+static inline void rseq_preempt(struct task_struct *t)
+{
+	__set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask);
+	rseq_set_notify_resume(t);
+}
+
+/* rseq_migrate() requires preemption to be disabled. */
+static inline void rseq_migrate(struct task_struct *t)
+{
+	__set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask);
+	rseq_set_notify_resume(t);
+}
+
+/*
+ * If parent process has a registered restartable sequences area, the
+ * child inherits. Only applies when forking a process, not a thread. In
+ * case a parent fork() in the middle of a restartable sequence, set the
+ * resume notifier to force the child to retry.
+ */
+static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
+{
+	if (clone_flags & CLONE_THREAD) {
+		t->rseq = NULL;
+		t->rseq_len = 0;
+		t->rseq_sig = 0;
+		t->rseq_event_mask = 0;
+	} else {
+		t->rseq = current->rseq;
+		t->rseq_len = current->rseq_len;
+		t->rseq_sig = current->rseq_sig;
+		t->rseq_event_mask = current->rseq_event_mask;
+		rseq_preempt(t);
+	}
+}
+
+static inline void rseq_execve(struct task_struct *t)
+{
+	t->rseq = NULL;
+	t->rseq_len = 0;
+	t->rseq_sig = 0;
+	t->rseq_event_mask = 0;
+}
+
+#else
+
+static inline void rseq_set_notify_resume(struct task_struct *t)
+{
+}
+static inline void rseq_handle_notify_resume(struct pt_regs *regs)
+{
+}
+static inline void rseq_signal_deliver(struct pt_regs *regs)
+{
+}
+static inline void rseq_preempt(struct task_struct *t)
+{
+}
+static inline void rseq_migrate(struct task_struct *t)
+{
+}
+static inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)
+{
+}
+static inline void rseq_execve(struct task_struct *t)
+{
+}
+
+#endif
+
+#ifdef CONFIG_DEBUG_RSEQ
+
+void rseq_syscall(struct pt_regs *regs);
+
+#else
+
+static inline void rseq_syscall(struct pt_regs *regs)
+{
+}
+
+#endif
+
 #endif

commit f7f4e7fc6c517708738d1d1984b170e9475a130f
Merge: d9b446e294f2 2539fc82aa9b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 17:45:38 2018 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - power-aware scheduling improvements (Patrick Bellasi)
    
     - NUMA balancing improvements (Mel Gorman)
    
     - vCPU scheduling fixes (Rohit Jain)
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/fair: Update util_est before updating schedutil
      sched/cpufreq: Modify aggregate utilization to always include blocked FAIR utilization
      sched/deadline/Documentation: Add overrun signal and GRUB-PA documentation
      sched/core: Distinguish between idle_cpu() calls based on desired effect, introduce available_idle_cpu()
      sched/wait: Include <linux/wait.h> in <linux/swait.h>
      sched/numa: Stagger NUMA balancing scan periods for new threads
      sched/core: Don't schedule threads on pre-empted vCPUs
      sched/fair: Avoid calling sync_entity_load_avg() unnecessarily
      sched/fair: Rearrange select_task_rq_fair() to optimize it

commit 4057adafb395204af4ff93f3669ecb49eb45b3cf
Merge: 137f5ae4dae8 52f2b34f4622
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 15:54:04 2018 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
    
     - updates to the handling of expedited grace periods
    
     - updates to reduce lock contention in the rcu_node combining tree
    
       [ These are in preparation for the consolidation of RCU-bh,
         RCU-preempt, and RCU-sched into a single flavor, which was
         requested by Linus in response to a security flaw whose root cause
         included confusion between the multiple flavors of RCU ]
    
     - torture-test updates that save their users some time and effort
    
     - miscellaneous fixes
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      rcu/x86: Provide early rcu_cpu_starting() callback
      torture: Make kvm-find-errors.sh find build warnings
      rcutorture: Abbreviate kvm.sh summary lines
      rcutorture: Print end-of-test state in kvm.sh summary
      rcutorture: Print end-of-test state
      torture: Fold parse-torture.sh into parse-console.sh
      torture: Add a script to edit output from failed runs
      rcu: Update list of rcu_future_grace_period() trace events
      rcu: Drop early GP request check from rcu_gp_kthread()
      rcu: Simplify and inline cpu_needs_another_gp()
      rcu: The rcu_gp_cleanup() function does not need cpu_needs_another_gp()
      rcu: Make rcu_start_this_gp() check for out-of-range requests
      rcu: Add funnel locking to rcu_start_this_gp()
      rcu: Make rcu_start_future_gp() caller select grace period
      rcu: Inline rcu_start_gp_advanced() into rcu_start_future_gp()
      rcu: Clear request other than RCU_GP_FLAG_INIT at GP end
      rcu: Cleanup, don't put ->completed into an int
      rcu: Switch __rcu_process_callbacks() to rcu_accelerate_cbs()
      rcu: Avoid __call_rcu_core() root rcu_node ->lock acquisition
      rcu: Make rcu_migrate_callbacks wake GP kthread when needed
      ...

commit 5eec43a1fa2a7ec5225411c97538fa582d36f579
Merge: 75025cc9d13f e25028c8ded0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 1 19:17:22 2018 +0200

    Merge tag 'kvmarm-for-v4.18' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.18
    
    - Lazy context-switching of FPSIMD registers on arm64
    - Allow virtual redistributors to be part of two or more MMIO ranges

commit 93ee37c2a6a97e5d358af2d8f0510817b6e75679
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Apr 11 17:54:20 2018 +0100

    thread_info: Add update_thread_flag() helpers
    
    There are a number of bits of code sprinkled around the kernel to
    set a thread flag if a certain condition is true, and clear it
    otherwise.
    
    To help make those call sites terser and less cumbersome, this
    patch adds a new family of thread flag manipulators
    
            update*_thread_flag([...,] flag, cond)
    
    which do the equivalent of:
    
            if (cond)
                    set*_thread_flag([...,] flag);
            else
                    clear*_thread_flag([...,] flag);
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex BennÃ©e <alex.bennee@linaro.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b3d697f3b573..c2c305199721 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1578,6 +1578,12 @@ static inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)
 	clear_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
+static inline void update_tsk_thread_flag(struct task_struct *tsk, int flag,
+					  bool value)
+{
+	update_ti_thread_flag(task_thread_info(tsk), flag, value);
+}
+
 static inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
 	return test_and_set_ti_thread_flag(task_thread_info(tsk), flag);

commit 0548dc5cde19e88b8495cb74e3893d8c8713392a
Merge: bb4e30a48045 4ff648decf47
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri May 25 08:04:39 2018 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3b78ce4a34b761c7fe13520de822984019ff1a8f
Merge: 6741c4bb389d af86ca4e3088
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 21 11:23:26 2018 -0700

    Merge branch 'speck-v20' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Merge speculative store buffer bypass fixes from Thomas Gleixner:
    
     - rework of the SPEC_CTRL MSR management to accomodate the new fancy
       SSBD (Speculative Store Bypass Disable) bit handling.
    
     - the CPU bug and sysfs infrastructure for the exciting new Speculative
       Store Bypass 'feature'.
    
     - support for disabling SSB via LS_CFG MSR on AMD CPUs including
       Hyperthread synchronization on ZEN.
    
     - PRCTL support for dynamic runtime control of SSB
    
     - SECCOMP integration to automatically disable SSB for sandboxed
       processes with a filter flag for opt-out.
    
     - KVM integration to allow guests fiddling with SSBD including the new
       software MSR VIRT_SPEC_CTRL to handle the LS_CFG based oddities on
       AMD.
    
     - BPF protection against SSB
    
    .. this is just the core and x86 side, other architecture support will
    come separately.
    
    * 'speck-v20' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (49 commits)
      bpf: Prevent memory disambiguation attack
      x86/bugs: Rename SSBD_NO to SSB_NO
      KVM: SVM: Implement VIRT_SPEC_CTRL support for SSBD
      x86/speculation, KVM: Implement support for VIRT_SPEC_CTRL/LS_CFG
      x86/bugs: Rework spec_ctrl base and mask logic
      x86/bugs: Remove x86_spec_ctrl_set()
      x86/bugs: Expose x86_spec_ctrl_base directly
      x86/bugs: Unify x86_spec_ctrl_{set_guest,restore_host}
      x86/speculation: Rework speculative_store_bypass_update()
      x86/speculation: Add virtualized speculative store bypass disable support
      x86/bugs, KVM: Extend speculation control for VIRT_SPEC_CTRL
      x86/speculation: Handle HT correctly on AMD
      x86/cpufeatures: Add FEATURE_ZEN
      x86/cpufeatures: Disentangle SSBD enumeration
      x86/cpufeatures: Disentangle MSR_SPEC_CTRL enumeration from IBRS
      x86/speculation: Use synthetic bits for IBRS/IBPB/STIBP
      KVM: SVM: Move spec control call after restore of GS
      x86/cpu: Make alternative_msr_write work for 32-bit code
      x86/bugs: Fix the parameters alignment and missing void
      x86/bugs: Make cpu_show_common() static
      ...

commit 13a553199fa9327f957ba5e8462705ed250a9e6a
Merge: 67b8d5c70812 22df7316ac71
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 16 09:34:51 2018 +0200

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
     - Updates to the handling of expedited grace periods, perhaps most
       notably parallelizing their initialization.  Other changes
       include fixes from Boqun Feng.
    
     - Miscellaneous fixes.  These include an nvme fix from Nitzan Carmi
       that I am carrying because it depends on a new SRCU function
       cleanup_srcu_struct_quiesced().  This branch also includes fixes
       from Byungchul Park and Yury Norov.
    
     - Updates to reduce lock contention in the rcu_node combining tree.
       These are in preparation for the consolidation of RCU-bh,
       RCU-preempt, and RCU-sched into a single flavor, which was
       requested by Linus Torvalds in response to a security flaw
       whose root cause included confusion between the multiple flavors
       of RCU.
    
     - Torture-test updates that save their users some time and effort.
    
    Conflicts:
            drivers/nvme/host/core.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c3442697c2d73d3cdb9d4135cf630ad36ba8552f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Mar 5 11:29:40 2018 -0800

    softirq: Eliminate unused cond_resched_softirq() macro
    
    The cond_resched_softirq() macro is not used anywhere in mainline, so
    this commit simplifies the kernel by eliminating it.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b3d697f3b573..6fc99045658a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1613,7 +1613,6 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
  * explicit rescheduling in places that are safe. The return
  * value indicates whether a reschedule was done in fact.
  * cond_resched_lock() will drop the spinlock before scheduling,
- * cond_resched_softirq() will enable bhs before scheduling.
  */
 #ifndef CONFIG_PREEMPT
 extern int _cond_resched(void);
@@ -1633,13 +1632,6 @@ extern int __cond_resched_lock(spinlock_t *lock);
 	__cond_resched_lock(lock);				\
 })
 
-extern int __cond_resched_softirq(void);
-
-#define cond_resched_softirq() ({					\
-	___might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
-	__cond_resched_softirq();					\
-})
-
 static inline void cond_resched_rcu(void)
 {
 #if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)

commit 943d355d7feef380e15a95892be3dff1095ef54b
Author: Rohit Jain <rohit.k.jain@oracle.com>
Date:   Wed May 9 09:39:48 2018 -0700

    sched/core: Distinguish between idle_cpu() calls based on desired effect, introduce available_idle_cpu()
    
    In the following commit:
    
      247f2f6f3c70 ("sched/core: Don't schedule threads on pre-empted vCPUs")
    
    ... we distinguish between idle_cpu() when the vCPU is not running for
    scheduling threads.
    
    However, the idle_cpu() function is used in other places for
    actually checking whether the state of the CPU is idle or not.
    
    Hence split the use of that function based on the desired return value,
    by introducing the available_idle_cpu() function.
    
    This fixes a (slight) regression in that initial vCPU commit, because
    some code paths (like the load-balancer) don't care and shouldn't care
    if the vCPU is preempted or not, they just want to know if there's any
    tasks on the CPU.
    
    Signed-off-by: Rohit Jain <rohit.k.jain@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dhaval.giani@oracle.com
    Cc: linux-kernel@vger.kernel.org
    Cc: matt@codeblueprint.co.uk
    Cc: steven.sistare@oracle.com
    Cc: subhra.mazumdar@oracle.com
    Link: http://lkml.kernel.org/r/1525883988-10356-1-git-send-email-rohit.k.jain@oracle.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c2413703f45d..959a8588e365 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1504,6 +1504,7 @@ static inline int task_nice(const struct task_struct *p)
 extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
+extern int available_idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
 extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
 extern int sched_setattr(struct task_struct *, const struct sched_attr *);

commit 356e4bfff2c5489e016fdb925adbf12a1e3950ee
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 3 22:09:15 2018 +0200

    prctl: Add force disable speculation
    
    For certain use cases it is desired to enforce mitigations so they cannot
    be undone afterwards. That's important for loader stubs which want to
    prevent a child from disabling the mitigation again. Will also be used for
    seccomp(). The extra state preserving of the prctl state for SSB is a
    preparatory step for EBPF dymanic speculation control.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b3d697f3b573..e4218d4deba0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1393,7 +1393,8 @@ static inline bool is_percpu_thread(void)
 #define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
 #define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */
 #define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
-
+#define PFA_SPEC_SSB_DISABLE		3	/* Speculative Store Bypass disabled */
+#define PFA_SPEC_SSB_FORCE_DISABLE	4	/* Speculative Store Bypass force disabled*/
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
@@ -1418,6 +1419,13 @@ TASK_PFA_TEST(SPREAD_SLAB, spread_slab)
 TASK_PFA_SET(SPREAD_SLAB, spread_slab)
 TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 
+TASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)
+TASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)
+TASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)
+
+TASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
+TASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)
+
 static inline void
 current_restore_flags(unsigned long orig_flags, unsigned long flags)
 {

commit b5bf9a90bbebffba888c9144c5a8a10317b04064
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 30 14:51:01 2018 +0200

    sched/core: Introduce set_special_state()
    
    Gaurav reported a perceived problem with TASK_PARKED, which turned out
    to be a broken wait-loop pattern in __kthread_parkme(), but the
    reported issue can (and does) in fact happen for states that do not do
    condition based sleeps.
    
    When the 'current->state = TASK_RUNNING' store of a previous
    (concurrent) try_to_wake_up() collides with the setting of a 'special'
    sleep state, we can loose the sleep state.
    
    Normal condition based wait-loops are immune to this problem, but for
    sleep states that are not condition based are subject to this problem.
    
    There already is a fix for TASK_DEAD. Abstract that and also apply it
    to TASK_STOPPED and TASK_TRACED, both of which are also without
    condition based wait-loop.
    
    Reported-by: Gaurav Kohli <gkohli@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b3d697f3b573..c2413703f45d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -112,17 +112,36 @@ struct task_group;
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
+/*
+ * Special states are those that do not use the normal wait-loop pattern. See
+ * the comment with set_special_state().
+ */
+#define is_special_task_state(state)				\
+	((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_DEAD))
+
 #define __set_current_state(state_value)			\
 	do {							\
+		WARN_ON_ONCE(is_special_task_state(state_value));\
 		current->task_state_change = _THIS_IP_;		\
 		current->state = (state_value);			\
 	} while (0)
+
 #define set_current_state(state_value)				\
 	do {							\
+		WARN_ON_ONCE(is_special_task_state(state_value));\
 		current->task_state_change = _THIS_IP_;		\
 		smp_store_mb(current->state, (state_value));	\
 	} while (0)
 
+#define set_special_state(state_value)					\
+	do {								\
+		unsigned long flags; /* may shadow */			\
+		WARN_ON_ONCE(!is_special_task_state(state_value));	\
+		raw_spin_lock_irqsave(&current->pi_lock, flags);	\
+		current->task_state_change = _THIS_IP_;			\
+		current->state = (state_value);				\
+		raw_spin_unlock_irqrestore(&current->pi_lock, flags);	\
+	} while (0)
 #else
 /*
  * set_current_state() includes a barrier so that the write of current->state
@@ -144,8 +163,8 @@ struct task_group;
  *
  * The above is typically ordered against the wakeup, which does:
  *
- *	need_sleep = false;
- *	wake_up_state(p, TASK_UNINTERRUPTIBLE);
+ *   need_sleep = false;
+ *   wake_up_state(p, TASK_UNINTERRUPTIBLE);
  *
  * Where wake_up_state() (and all other wakeup primitives) imply enough
  * barriers to order the store of the variable against wakeup.
@@ -154,12 +173,33 @@ struct task_group;
  * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a
  * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).
  *
- * This is obviously fine, since they both store the exact same value.
+ * However, with slightly different timing the wakeup TASK_RUNNING store can
+ * also collide with the TASK_UNINTERRUPTIBLE store. Loosing that store is not
+ * a problem either because that will result in one extra go around the loop
+ * and our @cond test will save the day.
  *
  * Also see the comments of try_to_wake_up().
  */
-#define __set_current_state(state_value) do { current->state = (state_value); } while (0)
-#define set_current_state(state_value)	 smp_store_mb(current->state, (state_value))
+#define __set_current_state(state_value)				\
+	current->state = (state_value)
+
+#define set_current_state(state_value)					\
+	smp_store_mb(current->state, (state_value))
+
+/*
+ * set_special_state() should be used for those states when the blocking task
+ * can not use the regular condition based wait-loop. In that case we must
+ * serialize against wakeups such that any possible in-flight TASK_RUNNING stores
+ * will not collide with our state change.
+ */
+#define set_special_state(state_value)					\
+	do {								\
+		unsigned long flags; /* may shadow */			\
+		raw_spin_lock_irqsave(&current->pi_lock, flags);	\
+		current->state = (state_value);				\
+		raw_spin_unlock_irqrestore(&current->pi_lock, flags);	\
+	} while (0)
+
 #endif
 
 /* Task command name length: */

commit 317d359df95dd0cb7653d09b7fc513770590cf85
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 5 10:05:21 2018 +0200

    sched/core: Force proper alignment of 'struct util_est'
    
    For some as yet not understood reason, Tony gets unaligned access
    traps on IA64 because of:
    
      struct util_est ue = READ_ONCE(p->se.avg.util_est);
    
    and:
    
      WRITE_ONCE(p->se.avg.util_est, ue);
    
    introduced by commit:
    
      d519329f72a6 ("sched/fair: Update util_est only on util_avg updates")
    
    Normally those two fields should end up on an 8-byte aligned location,
    but UP and RANDSTRUCT can mess that up so enforce the alignment
    explicitly.
    
    Also make the alignment on sched_avg unconditional, as it is really
    about data locality, not false-sharing.
    
    With or without this patch the layout for sched_avg on a
    ia64-defconfig build looks like:
    
            $ pahole -EC sched_avg ia64-defconfig/kernel/sched/core.o
            die__process_function: tag not supported (INVALID)!
            struct sched_avg {
                    /* typedef u64 */ long long unsigned int     last_update_time;                   /*     0     8 */
                    /* typedef u64 */ long long unsigned int     load_sum;                           /*     8     8 */
                    /* typedef u64 */ long long unsigned int     runnable_load_sum;                  /*    16     8 */
                    /* typedef u32 */ unsigned int               util_sum;                           /*    24     4 */
                    /* typedef u32 */ unsigned int               period_contrib;                     /*    28     4 */
                    long unsigned int          load_avg;                                             /*    32     8 */
                    long unsigned int          runnable_load_avg;                                    /*    40     8 */
                    long unsigned int          util_avg;                                             /*    48     8 */
                    struct util_est {
                            unsigned int       enqueued;                                             /*    56     4 */
                            unsigned int       ewma;                                                 /*    60     4 */
                    } util_est; /*    56     8 */
                    /* --- cacheline 1 boundary (64 bytes) --- */
    
                    /* size: 64, cachelines: 1, members: 9 */
            };
    
    Reported-and-Tested-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Norbert Manthey <nmanthey@amazon.de>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony <tony.luck@intel.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Fixes: d519329f72a6 ("sched/fair: Update util_est only on util_avg updates")
    Link: http://lkml.kernel.org/r/20180405080521.GG4129@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f228c6033832..b3d697f3b573 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -300,7 +300,7 @@ struct util_est {
 	unsigned int			enqueued;
 	unsigned int			ewma;
 #define UTIL_EST_WEIGHT_SHIFT		2
-};
+} __attribute__((__aligned__(sizeof(u64))));
 
 /*
  * The load_avg/util_avg accumulates an infinite geometric series
@@ -364,7 +364,7 @@ struct sched_avg {
 	unsigned long			runnable_load_avg;
 	unsigned long			util_avg;
 	struct util_est			util_est;
-};
+} ____cacheline_aligned;
 
 struct sched_statistics {
 #ifdef CONFIG_SCHEDSTATS
@@ -435,7 +435,7 @@ struct sched_entity {
 	 * Put into separate cache line so it does not
 	 * collide with read-mostly values above.
 	 */
-	struct sched_avg		avg ____cacheline_aligned_in_smp;
+	struct sched_avg		avg;
 #endif
 };
 

commit 7f65ea42eb00bc902f1c37a71e984e4f4064cfa9
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Mar 9 09:52:42 2018 +0000

    sched/fair: Add util_est on top of PELT
    
    The util_avg signal computed by PELT is too variable for some use-cases.
    For example, a big task waking up after a long sleep period will have its
    utilization almost completely decayed. This introduces some latency before
    schedutil will be able to pick the best frequency to run a task.
    
    The same issue can affect task placement. Indeed, since the task
    utilization is already decayed at wakeup, when the task is enqueued in a
    CPU, this can result in a CPU running a big task as being temporarily
    represented as being almost empty. This leads to a race condition where
    other tasks can be potentially allocated on a CPU which just started to run
    a big task which slept for a relatively long period.
    
    Moreover, the PELT utilization of a task can be updated every [ms], thus
    making it a continuously changing value for certain longer running
    tasks. This means that the instantaneous PELT utilization of a RUNNING
    task is not really meaningful to properly support scheduler decisions.
    
    For all these reasons, a more stable signal can do a better job of
    representing the expected/estimated utilization of a task/cfs_rq.
    Such a signal can be easily created on top of PELT by still using it as
    an estimator which produces values to be aggregated on meaningful
    events.
    
    This patch adds a simple implementation of util_est, a new signal built on
    top of PELT's util_avg where:
    
        util_est(task) = max(task::util_avg, f(task::util_avg@dequeue))
    
    This allows to remember how big a task has been reported by PELT in its
    previous activations via f(task::util_avg@dequeue), which is the new
    _task_util_est(struct task_struct*) function added by this patch.
    
    If a task should change its behavior and it runs longer in a new
    activation, after a certain time its util_est will just track the
    original PELT signal (i.e. task::util_avg).
    
    The estimated utilization of cfs_rq is defined only for root ones.
    That's because the only sensible consumer of this signal are the
    scheduler and schedutil when looking for the overall CPU utilization
    due to FAIR tasks.
    
    For this reason, the estimated utilization of a root cfs_rq is simply
    defined as:
    
        util_est(cfs_rq) = max(cfs_rq::util_avg, cfs_rq::util_est::enqueued)
    
    where:
    
        cfs_rq::util_est::enqueued = sum(_task_util_est(task))
                                     for each RUNNABLE task on that root cfs_rq
    
    It's worth noting that the estimated utilization is tracked only for
    objects of interests, specifically:
    
     - Tasks: to better support tasks placement decisions
     - root cfs_rqs: to better support both tasks placement decisions as
                     well as frequencies selection
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@android.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: http://lkml.kernel.org/r/20180309095245.11071-2-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 21b1168da951..f228c6033832 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -274,6 +274,34 @@ struct load_weight {
 	u32				inv_weight;
 };
 
+/**
+ * struct util_est - Estimation utilization of FAIR tasks
+ * @enqueued: instantaneous estimated utilization of a task/cpu
+ * @ewma:     the Exponential Weighted Moving Average (EWMA)
+ *            utilization of a task
+ *
+ * Support data structure to track an Exponential Weighted Moving Average
+ * (EWMA) of a FAIR task's utilization. New samples are added to the moving
+ * average each time a task completes an activation. Sample's weight is chosen
+ * so that the EWMA will be relatively insensitive to transient changes to the
+ * task's workload.
+ *
+ * The enqueued attribute has a slightly different meaning for tasks and cpus:
+ * - task:   the task's util_avg at last task dequeue time
+ * - cfs_rq: the sum of util_est.enqueued for each RUNNABLE task on that CPU
+ * Thus, the util_est.enqueued of a task represents the contribution on the
+ * estimated utilization of the CPU where that task is currently enqueued.
+ *
+ * Only for tasks we track a moving average of the past instantaneous
+ * estimated utilization. This allows to absorb sporadic drops in utilization
+ * of an otherwise almost periodic task.
+ */
+struct util_est {
+	unsigned int			enqueued;
+	unsigned int			ewma;
+#define UTIL_EST_WEIGHT_SHIFT		2
+};
+
 /*
  * The load_avg/util_avg accumulates an infinite geometric series
  * (see __update_load_avg() in kernel/sched/fair.c).
@@ -335,6 +363,7 @@ struct sched_avg {
 	unsigned long			load_avg;
 	unsigned long			runnable_load_avg;
 	unsigned long			util_avg;
+	struct util_est			util_est;
 };
 
 struct sched_statistics {

commit 9e49e2447c6385e45c6fddd70d6c0e917e21b669
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Feb 27 17:05:10 2018 +0100

    sched/core: Remove TASK_ALL
    
    It's unused:
    
      $ git grep "\<TASK_ALL\>" | wc -l
      1
    
    ... and it is also dangerous, kill the bugger.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180227160510.10829-1-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b161ef8a902e..21b1168da951 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -93,7 +93,6 @@ struct task_group;
 
 /* Convenience macros for the sake of wake_up(): */
 #define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
-#define TASK_ALL			(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
 
 /* get_task_state(): */
 #define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \

commit a2e5790d841658485d642196dbb0927303d6c22f
Merge: ab2d92ad881d 60c3e026d73c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 6 22:15:42 2018 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - kasan updates
    
     - procfs
    
     - lib/bitmap updates
    
     - other lib/ updates
    
     - checkpatch tweaks
    
     - rapidio
    
     - ubsan
    
     - pipe fixes and cleanups
    
     - lots of other misc bits
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (114 commits)
      Documentation/sysctl/user.txt: fix typo
      MAINTAINERS: update ARM/QUALCOMM SUPPORT patterns
      MAINTAINERS: update various PALM patterns
      MAINTAINERS: update "ARM/OXNAS platform support" patterns
      MAINTAINERS: update Cortina/Gemini patterns
      MAINTAINERS: remove ARM/CLKDEV SUPPORT file pattern
      MAINTAINERS: remove ANDROID ION pattern
      mm: docs: add blank lines to silence sphinx "Unexpected indentation" errors
      mm: docs: fix parameter names mismatch
      mm: docs: fixup punctuation
      pipe: read buffer limits atomically
      pipe: simplify round_pipe_size()
      pipe: reject F_SETPIPE_SZ with size over UINT_MAX
      pipe: fix off-by-one error when checking buffer limits
      pipe: actually allow root to exceed the pipe buffer limits
      pipe, sysctl: remove pipe_proc_fn()
      pipe, sysctl: drop 'min' parameter from pipe-max-size converter
      kasan: rework Kconfig settings
      crash_dump: is_kdump_kernel can be boolean
      kernel/mutex: mutex_is_locked can be boolean
      ...

commit 2ee0826085d1c0281cb60c1f4bc3e0c27efeedc3
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Feb 6 15:40:17 2018 -0800

    pids: introduce find_get_task_by_vpid() helper
    
    There are several functions that do find_task_by_vpid() followed by
    get_task_struct().  We can use a helper function instead.
    
    Link: http://lkml.kernel.org/r/1509602027-11337-1-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 166144c04ef6..ce5a27304b03 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1489,6 +1489,11 @@ static inline struct thread_info *task_thread_info(struct task_struct *task)
 extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
 
+/*
+ * find a task by its virtual pid and get the task struct
+ */
+extern struct task_struct *find_get_task_by_vpid(pid_t nr);
+
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);

commit 32e839dda3ba576943365f0f5817ce5c843137dc
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Tue Jan 30 10:45:55 2018 +0000

    sched/fair: Use a recently used CPU as an idle candidate and the basis for SIS
    
    The select_idle_sibling() (SIS) rewrite in commit:
    
      10e2f1acd010 ("sched/core: Rewrite and improve select_idle_siblings()")
    
    ... replaced a domain iteration with a search that broadly speaking
    does a wrapped walk of the scheduler domain sharing a last-level-cache.
    
    While this had a number of improvements, one consequence is that two tasks
    that share a waker/wakee relationship push each other around a socket. Even
    though two tasks may be active, all cores are evenly used. This is great from
    a search perspective and spreads a load across individual cores, but it has
    adverse consequences for cpufreq. As each CPU has relatively low utilisation,
    cpufreq may decide the utilisation is too low to used a higher P-state and
    overall computation throughput suffers.
    
    While individual cpufreq and cpuidle drivers may compensate by artifically
    boosting P-state (at c0) or avoiding lower C-states (during idle), it does
    not help if hardware-based cpufreq (e.g. HWP) is used.
    
    This patch tracks a recently used CPU based on what CPU a task was running
    on when it last was a waker a CPU it was recently using when a task is a
    wakee. During SIS, the recently used CPU is used as a target if it's still
    allowed by the task and is idle.
    
    The benefit may be non-obvious so consider an example of two tasks
    communicating back and forth. Task A may be an application doing IO where
    task B is a kworker or kthread like journald. Task A may issue IO, wake
    B and B wakes up A on completion.  With the existing scheme this may look
    like the following (potentially different IDs if SMT is in use but similar
    principal applies).
    
     A (cpu 0)      wake    B (wakes on cpu 1)
     B (cpu 1)      wake    A (wakes on cpu 2)
     A (cpu 2)      wake    B (wakes on cpu 3)
     etc.
    
    A careful reader may wonder why CPU 0 was not idle when B wakes A the
    first time and it's simply due to the fact that A can be rescheduled to
    another CPU and the pattern is that prev == target when B tries to wakeup A
    and the information about CPU 0 has been lost.
    
    With this patch, the pattern is more likely to be:
    
     A (cpu 0)      wake    B (wakes on cpu 1)
     B (cpu 1)      wake    A (wakes on cpu 0)
     A (cpu 0)      wake    B (wakes on cpu 1)
     etc
    
    i.e. two communicating casts are more likely to use just two cores instead
    of all available cores sharing a LLC.
    
    The most dramatic speedup was noticed on dbench using the XFS filesystem on
    UMA as clients interact heavily with workqueues in that configuration. Note
    that a similar speedup is not observed on ext4 as the wakeup pattern
    is different:
    
                              4.15.0-rc9             4.15.0-rc9
                               waprev-v1        biasancestor-v1
     Hmean      1      287.54 (   0.00%)      817.01 ( 184.14%)
     Hmean      2     1268.12 (   0.00%)     1781.24 (  40.46%)
     Hmean      4     1739.68 (   0.00%)     1594.47 (  -8.35%)
     Hmean      8     2464.12 (   0.00%)     2479.56 (   0.63%)
     Hmean     64     1455.57 (   0.00%)     1434.68 (  -1.44%)
    
    The results can be less dramatic on NUMA where automatic balancing interferes
    with the test. It's also known that network benchmarks running on localhost
    also benefit quite a bit from this patch (roughly 10% on netperf RR for UDP
    and TCP depending on the machine). Hackbench also seens small improvements
    (6-11% depending on machine and thread count). The facebook schbench was also
    tested but in most cases showed little or no different to wakeup latencies.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180130104555.4125-5-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 166144c04ef6..92744e3f1556 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -555,6 +555,14 @@ struct task_struct {
 	unsigned long			wakee_flip_decay_ts;
 	struct task_struct		*last_wakee;
 
+	/*
+	 * recent_used_cpu is initially set as the last CPU used by a task
+	 * that wakes affine another task. Waker/wakee relationships can
+	 * push tasks around a CPU where each wakeup moves to the next one.
+	 * Tracking a recently used CPU allows a quick search for a recently
+	 * used CPU that may be idle.
+	 */
+	int				recent_used_cpu;
 	int				wake_cpu;
 #endif
 	int				on_rq;

commit af8c5e2d6071c71d228788d1ebb0b9676829001a
Merge: a1c75e17e7d1 07881166a892
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 11:55:56 2018 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Implement frequency/CPU invariance and OPP selection for
         SCHED_DEADLINE (Juri Lelli)
    
       - Tweak the task migration logic for better multi-tasking
         workload scalability (Mel Gorman)
    
       - Misc cleanups, fixes and improvements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/deadline: Make bandwidth enforcement scale-invariant
      sched/cpufreq: Move arch_scale_{freq,cpu}_capacity() outside of #ifdef CONFIG_SMP
      sched/cpufreq: Remove arch_scale_freq_capacity()'s 'sd' parameter
      sched/cpufreq: Always consider all CPUs when deciding next freq
      sched/cpufreq: Split utilization signals
      sched/cpufreq: Change the worker kthread to SCHED_DEADLINE
      sched/deadline: Move CPU frequency selection triggering points
      sched/cpufreq: Use the DEADLINE utilization signal
      sched/deadline: Implement "runtime overrun signal" support
      sched/fair: Only immediately migrate tasks due to interrupts if prev and target CPUs share cache
      sched/fair: Correct obsolete comment about cpufreq_update_util()
      sched/fair: Remove impossible condition from find_idlest_group_cpu()
      sched/cpufreq: Don't pass flags to sugov_set_iowait_boost()
      sched/cpufreq: Initialize sg_cpu->flags to 0
      sched/fair: Consider RT/IRQ pressure in capacity_spare_wake()
      sched/fair: Use 'unsigned long' for utilization, consistently
      sched/core: Rework and clarify prepare_lock_switch()
      sched/fair: Remove unused 'curr' parameter from wakeup_gran
      sched/headers: Constify object_is_on_stack()

commit 794a56ebd9a57db12abaec63f038c6eb073461f7
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:20 2017 +0100

    sched/cpufreq: Change the worker kthread to SCHED_DEADLINE
    
    Worker kthread needs to be able to change frequency for all other
    threads.
    
    Make it special, just under STOP class.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-4-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 274a449c805a..f7506712825c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1431,6 +1431,7 @@ extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
 extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
 extern int sched_setattr(struct task_struct *, const struct sched_attr *);
+extern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);
 extern struct task_struct *idle_task(int cpu);
 
 /**

commit 34be39305a77b8b1ec9f279163c7cdb6cc719b91
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Tue Dec 12 12:10:24 2017 +0100

    sched/deadline: Implement "runtime overrun signal" support
    
    This patch adds the possibility of getting the delivery of a SIGXCPU
    signal whenever there is a runtime overrun. The request is done through
    the sched_flags field within the sched_attr structure.
    
    Forward port of https://lkml.org/lkml/2009/10/16/170
    
    Tested-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Claudio Scordino <claudio@evidence.eu.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1513077024-25461-1-git-send-email-claudio@evidence.eu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2588263a989..274a449c805a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -472,11 +472,15 @@ struct sched_dl_entity {
 	 * has not been executed yet. This flag is useful to avoid race
 	 * conditions between the inactive timer handler and the wakeup
 	 * code.
+	 *
+	 * @dl_overrun tells if the task asked to be informed about runtime
+	 * overruns.
 	 */
 	unsigned int			dl_throttled      : 1;
 	unsigned int			dl_boosted        : 1;
 	unsigned int			dl_yielded        : 1;
 	unsigned int			dl_non_contending : 1;
+	unsigned int			dl_overrun	  : 1;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2588263a989..68a504f6e474 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1446,12 +1446,21 @@ extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 void yield(void);
 
 union thread_union {
+#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK
+	struct task_struct task;
+#endif
 #ifndef CONFIG_THREAD_INFO_IN_TASK
 	struct thread_info thread_info;
 #endif
 	unsigned long stack[THREAD_SIZE/sizeof(long)];
 };
 
+#ifndef CONFIG_THREAD_INFO_IN_TASK
+extern struct thread_info init_thread_info;
+#endif
+
+extern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];
+
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 static inline struct thread_info *task_thread_info(struct task_struct *task)
 {

commit 1f76a75561a006fc03559f665c835e0e69c9014d
Merge: a58653cc1e8b 92ccc262e485
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 15 11:44:59 2017 -0800

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "Misc fixes:
    
       - Fix a S390 boot hang that was caused by the lock-break logic.
         Remove lock-break to begin with, as review suggested it was
         unreasonably fragile and our confidence in its continued good
         health is lower than our confidence in its removal.
    
       - Remove the lockdep cross-release checking code for now, because of
         unresolved false positive warnings. This should make lockdep work
         well everywhere again.
    
       - Get rid of the final (and single) ACCESS_ONCE() straggler and
         remove the API from v4.15.
    
       - Fix a liblockdep build warning"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tools/lib/lockdep: Add missing declaration of 'pr_cont()'
      checkpatch: Remove ACCESS_ONCE() warning
      compiler.h: Remove ACCESS_ONCE()
      tools/include: Remove ACCESS_ONCE()
      tools/perf: Convert ACCESS_ONCE() to READ_ONCE()
      locking/lockdep: Remove the cross-release locking checks
      locking/core: Remove break_lock field when CONFIG_GENERIC_LOCKBREAK=y
      locking/core: Fix deadlock during boot on systems with GENERIC_LOCKBREAK

commit 3756f6401c302617c5e091081ca4d26ab604bec5
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Dec 14 15:32:41 2017 -0800

    exec: avoid gcc-8 warning for get_task_comm
    
    gcc-8 warns about using strncpy() with the source size as the limit:
    
      fs/exec.c:1223:32: error: argument to 'sizeof' in 'strncpy' call is the same expression as the source; did you mean to use the size of the destination? [-Werror=sizeof-pointer-memaccess]
    
    This is indeed slightly suspicious, as it protects us from source
    arguments without NUL-termination, but does not guarantee that the
    destination is terminated.
    
    This keeps the strncpy() to ensure we have properly padded target
    buffer, but ensures that we use the correct length, by passing the
    actual length of the destination buffer as well as adding a build-time
    check to ensure it is exactly TASK_COMM_LEN.
    
    There are only 23 callsites which I all reviewed to ensure this is
    currently the case.  We could get away with doing only the check or
    passing the right length, but it doesn't hurt to do both.
    
    Link: http://lkml.kernel.org/r/20171205151724.1764896-1-arnd@arndb.de
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Suggested-by: Kees Cook <keescook@chromium.org>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Serge Hallyn <serge@hallyn.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Aleksa Sarai <asarai@suse.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 21991d668d35..5124ba709830 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1503,7 +1503,11 @@ static inline void set_task_comm(struct task_struct *tsk, const char *from)
 	__set_task_comm(tsk, from, false);
 }
 
-extern char *get_task_comm(char *to, struct task_struct *tsk);
+extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);
+#define get_task_comm(buf, tsk) ({			\
+	BUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);	\
+	__get_task_comm(buf, sizeof(buf), tsk);		\
+})
 
 #ifdef CONFIG_SMP
 void scheduler_ipi(void);

commit e966eaeeb623f09975ef362c2866fae6f86844f9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 12 12:31:16 2017 +0100

    locking/lockdep: Remove the cross-release locking checks
    
    This code (CONFIG_LOCKDEP_CROSSRELEASE=y and CONFIG_LOCKDEP_COMPLETIONS=y),
    while it found a number of old bugs initially, was also causing too many
    false positives that caused people to disable lockdep - which is arguably
    a worse overall outcome.
    
    If we disable cross-release by default but keep the code upstream then
    in practice the most likely outcome is that we'll allow the situation
    to degrade gradually, by allowing entropy to introduce more and more
    false positives, until it overwhelms maintenance capacity.
    
    Another bad side effect was that people were trying to work around
    the false positives by uglifying/complicating unrelated code. There's
    a marked difference between annotating locking operations and
    uglifying good code just due to bad lock debugging code ...
    
    This gradual decrease in quality happened to a number of debugging
    facilities in the kernel, and lockdep is pretty complex already,
    so we cannot risk this outcome.
    
    Either cross-release checking can be done right with no false positives,
    or it should not be included in the upstream kernel.
    
    ( Note that it might make sense to maintain it out of tree and go through
      the false positives every now and then and see whether new bugs were
      introduced. )
    
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 21991d668d35..9ce6c3001e9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -849,17 +849,6 @@ struct task_struct {
 	struct held_lock		held_locks[MAX_LOCK_DEPTH];
 #endif
 
-#ifdef CONFIG_LOCKDEP_CROSSRELEASE
-#define MAX_XHLOCKS_NR 64UL
-	struct hist_lock *xhlocks; /* Crossrelease history locks */
-	unsigned int xhlock_idx;
-	/* For restoring at history boundaries */
-	unsigned int xhlock_idx_hist[XHLOCK_CTX_NR];
-	unsigned int hist_id;
-	/* For overwrite check at each context exit */
-	unsigned int hist_id_save[XHLOCK_CTX_NR];
-#endif
-
 #ifdef CONFIG_UBSAN
 	unsigned int			in_ubsan;
 #endif

commit aa5222e92f8000ed3c1c38dddf11c83222aadfb3
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri Oct 13 10:01:22 2017 +0300

    sched/deadline: Don't use dubious signed bitfields
    
    It doesn't cause a run-time bug, but these bitfields should be unsigned.
    When it's signed ->dl_throttled is set to either 0 or -1, instead of
    0 and 1 as expected.
    
    The sched.h file is included into tons of places so Sparse generates
    a flood of warnings like this:
    
      ./include/linux/sched.h:477:54: error: dubious one-bit signed bitfield
    
    Reported-by: Matthew Wilcox <willy@infradead.org>
    Reported-by: Xin Long <lucien.xin@gmail.com>
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-janitors@vger.kernel.org
    Cc: luca abeni <luca.abeni@santannapisa.it>
    Link: http://lkml.kernel.org/r/20171013070121.dzcncojuj2f4utij@mwanda
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a5dc7c98b0a2..21991d668d35 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -473,10 +473,10 @@ struct sched_dl_entity {
 	 * conditions between the inactive timer handler and the wakeup
 	 * code.
 	 */
-	int				dl_throttled      : 1;
-	int				dl_boosted        : 1;
-	int				dl_yielded        : 1;
-	int				dl_non_contending : 1;
+	unsigned int			dl_throttled      : 1;
+	unsigned int			dl_boosted        : 1;
+	unsigned int			dl_yielded        : 1;
+	unsigned int			dl_non_contending : 1;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its

commit 8a103df440afea30c91ebd42e61dc644e647f4bd
Merge: a9903f04e0a4 fbc3edf7d773
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 8 10:17:15 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a7df4e558c..fdf74f27acf1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 

commit edb9382175c3ebdced8ffdb3e0f20052ad9fdbe9
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Fri Oct 27 04:42:37 2017 +0200

    sched/isolation: Move isolcpus= handling to the housekeeping code
    
    We want to centralize the isolation features, to be done by the housekeeping
    subsystem and scheduler domain isolation is a significant part of it.
    
    No intended behaviour change, we just reuse the housekeeping cpumask
    and core code.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1509072159-31808-11-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0f897dfc195e..1b0cc0d6df8d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -165,8 +165,6 @@ struct task_group;
 /* Task command name length: */
 #define TASK_COMM_LEN			16
 
-extern cpumask_var_t			cpu_isolated_map;
-
 extern void scheduler_tick(void);
 
 #define	MAX_SCHEDULE_TIMEOUT		LONG_MAX

commit 799ba82de01e7543f6b2042e1a739f3a20255f23
Author: luca abeni <luca.abeni@santannapisa.it>
Date:   Thu Sep 7 12:09:31 2017 +0200

    sched/deadline: Use C bitfields for the state flags
    
    Ask the compiler to use a single bit for storing true / false values,
    instead of wasting the size of a whole int value.
    Tested with gcc 5.4.0 on x86_64, and the compiler produces the expected
    Assembly (similar to the Assembly code generated when explicitly accessing
    the bits with bitmasks, "&" and "|").
    
    Signed-off-by: luca abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1504778971-13573-5-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 33a01f4deb00..0f897dfc195e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -474,10 +474,10 @@ struct sched_dl_entity {
 	 * conditions between the inactive timer handler and the wakeup
 	 * code.
 	 */
-	int				dl_throttled;
-	int				dl_boosted;
-	int				dl_yielded;
-	int				dl_non_contending;
+	int				dl_throttled      : 1;
+	int				dl_boosted        : 1;
+	int				dl_yielded        : 1;
+	int				dl_non_contending : 1;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its

commit 1d48b080bcce0a5e7d7aa2dbcdb35deefc188c3f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 29 13:50:16 2017 +0200

    sched/debug: Rename task-state printing helpers
    
    Steve requested better names for the new task-state helper functions.
    
    So introduce the concept of task-state index for the printing and
    rename __get_task_state() to task_state_index() and
    __task_state_to_char() to task_index_to_char().
    
    Requested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170929115016.pzlqc7ss3ccystyg@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bdd6ad6fcce1..33a01f4deb00 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1248,7 +1248,7 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 #define TASK_REPORT_IDLE	(TASK_REPORT + 1)
 #define TASK_REPORT_MAX		(TASK_REPORT_IDLE << 1)
 
-static inline unsigned int __get_task_state(struct task_struct *tsk)
+static inline unsigned int task_state_index(struct task_struct *tsk)
 {
 	unsigned int tsk_state = READ_ONCE(tsk->state);
 	unsigned int state = (tsk_state | tsk->exit_state) & TASK_REPORT;
@@ -1261,7 +1261,7 @@ static inline unsigned int __get_task_state(struct task_struct *tsk)
 	return fls(state);
 }
 
-static inline char __task_state_to_char(unsigned int state)
+static inline char task_index_to_char(unsigned int state)
 {
 	static const char state_char[] = "RSDTtXZPI";
 
@@ -1272,7 +1272,7 @@ static inline char __task_state_to_char(unsigned int state)
 
 static inline char task_state_to_char(struct task_struct *tsk)
 {
-	return __task_state_to_char(__get_task_state(tsk));
+	return task_index_to_char(task_state_index(tsk));
 }
 
 /**

commit 1ea6c46a23f1213d1972bfae220db5c165e27bba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat May 6 15:59:54 2017 +0200

    sched/fair: Propagate an effective runnable_load_avg
    
    The load balancer uses runnable_load_avg as load indicator. For
    !cgroup this is:
    
      runnable_load_avg = \Sum se->avg.load_avg ; where se->on_rq
    
    That is, a direct sum of all runnable tasks on that runqueue. As
    opposed to load_avg, which is a sum of all tasks on the runqueue,
    which includes a blocked component.
    
    However, in the cgroup case, this comes apart since the group entities
    are always runnable, even if most of their constituent entities are
    blocked.
    
    Therefore introduce a runnable_weight which for task entities is the
    same as the regular weight, but for group entities is a fraction of
    the entity weight and represents the runnable part of the group
    runqueue.
    
    Then propagate this load through the PELT hierarchy to arrive at an
    effective runnable load avgerage -- which we should not confuse with
    the canonical runnable load average.
    
    Suggested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a7df4e558c..bdd6ad6fcce1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -331,9 +331,11 @@ struct load_weight {
 struct sched_avg {
 	u64				last_update_time;
 	u64				load_sum;
+	u64				runnable_load_sum;
 	u32				util_sum;
 	u32				period_contrib;
 	unsigned long			load_avg;
+	unsigned long			runnable_load_avg;
 	unsigned long			util_avg;
 };
 
@@ -376,6 +378,7 @@ struct sched_statistics {
 struct sched_entity {
 	/* For load-balancing: */
 	struct load_weight		load;
+	unsigned long			runnable_weight;
 	struct rb_node			run_node;
 	struct list_head		group_node;
 	unsigned int			on_rq;

commit 8ef9925b02c23e3838d5e593c5cf37984141150f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:37:28 2017 +0200

    sched/debug: Add explicit TASK_PARKED printing
    
    Currently TASK_PARKED is masqueraded as TASK_INTERRUPTIBLE, give it
    its own print state because it will not in fact get woken by regular
    wakeups and is a long-term state.
    
    This requires moving TASK_PARKED into the TASK_REPORT mask, and since
    that latter needs to be a contiguous bitmask, we need to shuffle the
    bits around a bit.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 286fc1117046..26a7df4e558c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -75,10 +75,10 @@ struct task_group;
 #define EXIT_ZOMBIE			0x0020
 #define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
 /* Used in tsk->state again: */
-#define TASK_DEAD			0x0040
-#define TASK_WAKEKILL			0x0080
-#define TASK_WAKING			0x0100
-#define TASK_PARKED			0x0200
+#define TASK_PARKED			0x0040
+#define TASK_DEAD			0x0080
+#define TASK_WAKEKILL			0x0100
+#define TASK_WAKING			0x0200
 #define TASK_NOLOAD			0x0400
 #define TASK_NEW			0x0800
 #define TASK_STATE_MAX			0x1000
@@ -97,7 +97,8 @@ struct task_group;
 /* get_task_state(): */
 #define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
 					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
-					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE)
+					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
+					 TASK_PARKED)
 
 #define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
 
@@ -1251,9 +1252,6 @@ static inline unsigned int __get_task_state(struct task_struct *tsk)
 
 	BUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);
 
-	if (tsk_state == TASK_PARKED)
-		state = TASK_INTERRUPTIBLE;
-
 	if (tsk_state == TASK_IDLE)
 		state = TASK_REPORT_IDLE;
 
@@ -1262,7 +1260,7 @@ static inline unsigned int __get_task_state(struct task_struct *tsk)
 
 static inline char __task_state_to_char(unsigned int state)
 {
-	static const char state_char[] = "RSDTtXZI";
+	static const char state_char[] = "RSDTtXZPI";
 
 	BUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1);
 

commit 06eb61844d841d0032a9950ce7f8e783ee49c0d0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:30:40 2017 +0200

    sched/debug: Add explicit TASK_IDLE printing
    
    Markus reported that kthreads that idle using TASK_IDLE instead of
    TASK_INTERRUPTIBLE are reported in as TASK_UNINTERRUPTIBLE and things
    like htop mark those red.
    
    This is undesirable, so add an explicit state for TASK_IDLE.
    
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bc7807933415..286fc1117046 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1241,22 +1241,30 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 	return task_pgrp_nr_ns(tsk, &init_pid_ns);
 }
 
+#define TASK_REPORT_IDLE	(TASK_REPORT + 1)
+#define TASK_REPORT_MAX		(TASK_REPORT_IDLE << 1)
+
 static inline unsigned int __get_task_state(struct task_struct *tsk)
 {
 	unsigned int tsk_state = READ_ONCE(tsk->state);
 	unsigned int state = (tsk_state | tsk->exit_state) & TASK_REPORT;
 
+	BUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);
+
 	if (tsk_state == TASK_PARKED)
 		state = TASK_INTERRUPTIBLE;
 
+	if (tsk_state == TASK_IDLE)
+		state = TASK_REPORT_IDLE;
+
 	return fls(state);
 }
 
 static inline char __task_state_to_char(unsigned int state)
 {
-	static const char state_char[] = "RSDTtXZ";
+	static const char state_char[] = "RSDTtXZI";
 
-	BUILD_BUG_ON(1 + ilog2(TASK_REPORT) != sizeof(state_char) - 2);
+	BUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1);
 
 	return state_char[state];
 }

commit 5f6ad26ea353fdf3dad2328052cbee49e0b9c5b4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:23:31 2017 +0200

    sched/tracing: Use common task-state helpers
    
    Remove yet another task-state char instance.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a2fe636b6825..bc7807933415 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -83,8 +83,6 @@ struct task_group;
 #define TASK_NEW			0x0800
 #define TASK_STATE_MAX			0x1000
 
-#define TASK_STATE_TO_CHAR_STR		"RSDTtXZxKWPNn"
-
 /* Convenience macros for the sake of set_current_state: */
 #define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
 #define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)

commit efb40f588b4370ffaeffafbd50f6ff213d954254
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:19:53 2017 +0200

    sched/tracing: Fix trace_sched_switch task-state printing
    
    Convert trace_sched_switch to use the common task-state helpers and
    fix the "X" and "Z" order, possibly they ended up in the wrong order
    because TASK_REPORT has them in the wrong order too.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 69bed5339ffa..a2fe636b6825 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -99,7 +99,7 @@ struct task_group;
 /* get_task_state(): */
 #define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
 					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
-					 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
+					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE)
 
 #define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
 

commit 92c4bc9f9cd92a8581e36bc5105f03b569f37e36
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:13:36 2017 +0200

    sched/debug: Convert TASK_state to hex
    
    Bit patterns are easier in hex.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 163a0b738908..69bed5339ffa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -65,23 +65,23 @@ struct task_group;
  */
 
 /* Used in tsk->state: */
-#define TASK_RUNNING			0
-#define TASK_INTERRUPTIBLE		1
-#define TASK_UNINTERRUPTIBLE		2
-#define __TASK_STOPPED			4
-#define __TASK_TRACED			8
+#define TASK_RUNNING			0x0000
+#define TASK_INTERRUPTIBLE		0x0001
+#define TASK_UNINTERRUPTIBLE		0x0002
+#define __TASK_STOPPED			0x0004
+#define __TASK_TRACED			0x0008
 /* Used in tsk->exit_state: */
-#define EXIT_DEAD			16
-#define EXIT_ZOMBIE			32
+#define EXIT_DEAD			0x0010
+#define EXIT_ZOMBIE			0x0020
 #define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
 /* Used in tsk->state again: */
-#define TASK_DEAD			64
-#define TASK_WAKEKILL			128
-#define TASK_WAKING			256
-#define TASK_PARKED			512
-#define TASK_NOLOAD			1024
-#define TASK_NEW			2048
-#define TASK_STATE_MAX			4096
+#define TASK_DEAD			0x0040
+#define TASK_WAKEKILL			0x0080
+#define TASK_WAKING			0x0100
+#define TASK_PARKED			0x0200
+#define TASK_NOLOAD			0x0400
+#define TASK_NEW			0x0800
+#define TASK_STATE_MAX			0x1000
 
 #define TASK_STATE_TO_CHAR_STR		"RSDTtXZxKWPNn"
 

commit 1593baab910da72480d651ea7bf2ce6e3a25a484
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:09:26 2017 +0200

    sched/debug: Implement consistent task-state printing
    
    Currently get_task_state() and task_state_to_char() report different
    states, create a number of common helpers and unify the reported state
    space.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 92fb8dd5a9e4..163a0b738908 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1243,17 +1243,29 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 	return task_pgrp_nr_ns(tsk, &init_pid_ns);
 }
 
-static inline char task_state_to_char(struct task_struct *task)
+static inline unsigned int __get_task_state(struct task_struct *tsk)
 {
-	const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
-	unsigned long state = task->state;
+	unsigned int tsk_state = READ_ONCE(tsk->state);
+	unsigned int state = (tsk_state | tsk->exit_state) & TASK_REPORT;
 
-	state = state ? __ffs(state) + 1 : 0;
+	if (tsk_state == TASK_PARKED)
+		state = TASK_INTERRUPTIBLE;
 
-	/* Make sure the string lines up properly with the number of task states: */
-	BUILD_BUG_ON(sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1);
+	return fls(state);
+}
+
+static inline char __task_state_to_char(unsigned int state)
+{
+	static const char state_char[] = "RSDTtXZ";
+
+	BUILD_BUG_ON(1 + ilog2(TASK_REPORT) != sizeof(state_char) - 2);
 
-	return state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?';
+	return state_char[state];
+}
+
+static inline char task_state_to_char(struct task_struct *tsk)
+{
+	return __task_state_to_char(__get_task_state(tsk));
 }
 
 /**

commit a23ba907d5e65d6aeea3e59c82fda9cd206a7aad
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:15:01 2017 -0700

    locking/rtmutex: replace top-waiter and pi_waiters leftmost caching
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-10-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68b38335d33c..92fb8dd5a9e4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -812,8 +812,7 @@ struct task_struct {
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task: */
-	struct rb_root			pi_waiters;
-	struct rb_node			*pi_waiters_leftmost;
+	struct rb_root_cached		pi_waiters;
 	/* Updated under owner's pi_lock and rq lock */
 	struct task_struct		*pi_top_task;
 	/* Deadlock detection and priority inheritance handling: */

commit f57091767add2b79d76aac41b83b192d8ba1dce7
Merge: d725c7ac8b96 d56593eb5eda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 13:56:37 2017 -0700

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache quality monitoring update from Thomas Gleixner:
     "This update provides a complete rewrite of the Cache Quality
      Monitoring (CQM) facility.
    
      The existing CQM support was duct taped into perf with a lot of issues
      and the attempts to fix those turned out to be incomplete and
      horrible.
    
      After lengthy discussions it was decided to integrate the CQM support
      into the Resource Director Technology (RDT) facility, which is the
      obvious choise as in hardware CQM is part of RDT. This allowed to add
      Memory Bandwidth Monitoring support on top.
    
      As a result the mechanisms for allocating cache/memory bandwidth and
      the corresponding monitoring mechanisms are integrated into a single
      management facility with a consistent user interface"
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/intel_rdt: Turn off most RDT features on Skylake
      x86/intel_rdt: Add command line options for resource director technology
      x86/intel_rdt: Move special case code for Haswell to a quirk function
      x86/intel_rdt: Remove redundant ternary operator on return
      x86/intel_rdt/cqm: Improve limbo list processing
      x86/intel_rdt/mbm: Fix MBM overflow handler during CPU hotplug
      x86/intel_rdt: Modify the intel_pqr_state for better performance
      x86/intel_rdt/cqm: Clear the default RMID during hotcpu
      x86/intel_rdt: Show bitmask of shareable resource with other executing units
      x86/intel_rdt/mbm: Handle counter overflow
      x86/intel_rdt/mbm: Add mbm counter initialization
      x86/intel_rdt/mbm: Basic counting of MBM events (total and local)
      x86/intel_rdt/cqm: Add CPU hotplug support
      x86/intel_rdt/cqm: Add sched_in support
      x86/intel_rdt: Introduce rdt_enable_key for scheduling
      x86/intel_rdt/cqm: Add mount,umount support
      x86/intel_rdt/cqm: Add rmdir support
      x86/intel_rdt: Separate the ctrl bits from rmdir
      x86/intel_rdt/cqm: Add mon_data
      x86/intel_rdt: Prepare for RDT monitor data support
      ...

commit 5f82e71a001d14824a7728ad9e49f6aea420f161
Merge: 6c51e67b64d1 edc2988c548d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 11:52:29 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Add 'cross-release' support to lockdep, which allows APIs like
       completions, where it's not the 'owner' who releases the lock, to be
       tracked. It's all activated automatically under
       CONFIG_PROVE_LOCKING=y.
    
     - Clean up (restructure) the x86 atomics op implementation to be more
       readable, in preparation of KASAN annotations. (Dmitry Vyukov)
    
     - Fix static keys (Paolo Bonzini)
    
     - Add killable versions of down_read() et al (Kirill Tkhai)
    
     - Rework and fix jump_label locking (Marc Zyngier, Paolo Bonzini)
    
     - Rework (and fix) tlb_flush_pending() barriers (Peter Zijlstra)
    
     - Remove smp_mb__before_spinlock() and convert its usages, introduce
       smp_mb__after_spinlock() (Peter Zijlstra)
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      locking/lockdep/selftests: Fix mixed read-write ABBA tests
      sched/completion: Avoid unnecessary stack allocation for COMPLETION_INITIALIZER_ONSTACK()
      acpi/nfit: Fix COMPLETION_INITIALIZER_ONSTACK() abuse
      locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
      smp: Avoid using two cache lines for struct call_single_data
      locking/lockdep: Untangle xhlock history save/restore from task independence
      locking/refcounts, x86/asm: Disable CONFIG_ARCH_HAS_REFCOUNT for the time being
      futex: Remove duplicated code and fix undefined behaviour
      Documentation/locking/atomic: Finish the document...
      locking/lockdep: Fix workqueue crossrelease annotation
      workqueue/lockdep: 'Fix' flush_work() annotation
      locking/lockdep/selftests: Add mixed read-write ABBA tests
      mm, locking/barriers: Clarify tlb_flush_pending() barriers
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE and CONFIG_LOCKDEP_COMPLETIONS truly non-interactive
      locking/lockdep: Explicitly initialize wq_barrier::done::map
      locking/lockdep: Rename CONFIG_LOCKDEP_COMPLETE to CONFIG_LOCKDEP_COMPLETIONS
      locking/lockdep: Reword title of LOCKDEP_CROSSRELEASE config
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE part of CONFIG_PROVE_LOCKING
      locking/refcounts, x86/asm: Implement fast refcount overflow protection
      locking/lockdep: Fix the rollback and overwrite detection logic in crossrelease
      ...

commit f213a6c84c1b4b396a0713ee33cff0e02ba8235f
Merge: 621bee34f6ed bbdacdfed2f5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 09:10:24 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - fix affine wakeups (Peter Zijlstra)
    
       - improve CPU onlining (and general bootup) scalability on systems
         with ridiculous number (thousands) of CPUs (Peter Zijlstra)
    
       - sched/numa updates (Rik van Riel)
    
       - sched/deadline updates (Byungchul Park)
    
       - sched/cpufreq enhancements and related cleanups (Viresh Kumar)
    
       - sched/debug enhancements (Xie XiuQi)
    
       - various fixes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      sched/debug: Optimize sched_domain sysctl generation
      sched/topology: Avoid pointless rebuild
      sched/topology, cpuset: Avoid spurious/wrong domain rebuilds
      sched/topology: Improve comments
      sched/topology: Fix memory leak in __sdt_alloc()
      sched/completion: Document that reinit_completion() must be called after complete_all()
      sched/autogroup: Fix error reporting printk text in autogroup_create()
      sched/fair: Fix wake_affine() for !NUMA_BALANCING
      sched/debug: Intruduce task_state_to_char() helper function
      sched/debug: Show task state in /proc/sched_debug
      sched/debug: Use task_pid_nr_ns in /proc/$pid/sched
      sched/core: Remove unnecessary initialization init_idle_bootup_task()
      sched/deadline: Change return value of cpudl_find()
      sched/deadline: Make find_later_rq() choose a closer CPU in topology
      sched/numa: Scale scan period with tasks in group and shared/private
      sched/numa: Slow down scan rate if shared faults dominate
      sched/pelt: Fix false running accounting
      sched: Mark pick_next_task_dl() and build_sched_domain() as static
      sched/cpupri: Don't re-initialize 'struct cpupri'
      sched/deadline: Don't re-initialize 'struct cpudl'
      ...

commit 0081a0ce809b611c1f37da5d6ae5bc8027ffd1c4
Merge: fea154376035 94edf6f3c20c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 08:13:52 2017 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnad:
     "The main RCU related changes in this cycle were:
    
       - Removal of spin_unlock_wait()
       - SRCU updates
       - RCU torture-test updates
       - RCU Documentation updates
       - Extend the sys_membarrier() ABI with the MEMBARRIER_CMD_PRIVATE_EXPEDITED variant
       - Miscellaneous RCU fixes
       - CPU-hotplug fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (63 commits)
      arch: Remove spin_unlock_wait() arch-specific definitions
      locking: Remove spin_unlock_wait() generic definitions
      drivers/ata: Replace spin_unlock_wait() with lock/unlock pair
      ipc: Replace spin_unlock_wait() with lock/unlock pair
      exit: Replace spin_unlock_wait() with lock/unlock pair
      completion: Replace spin_unlock_wait() with lock/unlock pair
      doc: Set down RCU's scheduling-clock-interrupt needs
      doc: No longer allowed to use rcu_dereference on non-pointers
      doc: Add RCU files to docbook-generation files
      doc: Update memory-barriers.txt for read-to-write dependencies
      doc: Update RCU documentation
      membarrier: Provide expedited private command
      rcu: Remove exports from rcu_idle_exit() and rcu_idle_enter()
      rcu: Add warning to rcu_idle_enter() for irqs enabled
      rcu: Make rcu_idle_enter() rely on callers disabling irqs
      rcu: Add assertions verifying blocked-tasks list
      rcu/tracing: Set disable_rcu_irq_enter on rcu_eqs_exit()
      rcu: Add TPS() protection for _rcu_barrier_trace strings
      rcu: Use idle versions of swait to make idle-hack clear
      swait: Add idle variants which don't contribute to load average
      ...

commit 3a9ff4fd04cc6ad199419508c8ea6eb839e0262d
Merge: 9c8783201cb5 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Aug 25 11:07:13 2017 +0200

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 10c9850cb2ced2ce528e5b692c639974213a64ec
Merge: 0c2364791343 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Aug 25 11:04:51 2017 +0200

    Merge branch 'linus' into locking/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit dd1c1f2f2028a7b851f701fc6a8ebe39dcb95e7c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Aug 21 17:35:02 2017 +0200

    pids: make task_tgid_nr_ns() safe
    
    This was reported many times, and this was even mentioned in commit
    52ee2dfdd4f5 ("pids: refactor vnr/nr_ns helpers to make them safe") but
    somehow nobody bothered to fix the obvious problem: task_tgid_nr_ns() is
    not safe because task->group_leader points to nowhere after the exiting
    task passes exit_notify(), rcu_read_lock() can not help.
    
    We really need to change __unhash_process() to nullify group_leader,
    parent, and real_parent, but this needs some cleanups.  Until then we
    can turn task_tgid_nr_ns() into another user of __task_pid_nr_ns() and
    fix the problem.
    
    Reported-by: Troy Kensinger <tkensinger@google.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8337e2db0bb2..c05ac5f5aa03 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1163,13 +1163,6 @@ static inline pid_t task_tgid_nr(struct task_struct *tsk)
 	return tsk->tgid;
 }
 
-extern pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
-
-static inline pid_t task_tgid_vnr(struct task_struct *tsk)
-{
-	return pid_vnr(task_tgid(tsk));
-}
-
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.
@@ -1185,23 +1178,6 @@ static inline int pid_alive(const struct task_struct *p)
 	return p->pids[PIDTYPE_PID].pid != NULL;
 }
 
-static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
-{
-	pid_t pid = 0;
-
-	rcu_read_lock();
-	if (pid_alive(tsk))
-		pid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);
-	rcu_read_unlock();
-
-	return pid;
-}
-
-static inline pid_t task_ppid_nr(const struct task_struct *tsk)
-{
-	return task_ppid_nr_ns(tsk, &init_pid_ns);
-}
-
 static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
@@ -1223,6 +1199,33 @@ static inline pid_t task_session_vnr(struct task_struct *tsk)
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);
 }
 
+static inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
+{
+	return __task_pid_nr_ns(tsk, __PIDTYPE_TGID, ns);
+}
+
+static inline pid_t task_tgid_vnr(struct task_struct *tsk)
+{
+	return __task_pid_nr_ns(tsk, __PIDTYPE_TGID, NULL);
+}
+
+static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
+{
+	pid_t pid = 0;
+
+	rcu_read_lock();
+	if (pid_alive(tsk))
+		pid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);
+	rcu_read_unlock();
+
+	return pid;
+}
+
+static inline pid_t task_ppid_nr(const struct task_struct *tsk)
+{
+	return task_ppid_nr_ns(tsk, &init_pid_ns);
+}
+
 /* Obsolete, do not use: */
 static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {

commit ccdd29ffffa7246cb359b9408772858a15fc4ea5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 25 08:51:48 2017 -0700

    rcu: Create reasonable API for do_exit() TASKS_RCU processing
    
    Currently, the exit-time support for TASKS_RCU is open-coded in do_exit().
    This commit creates exit_tasks_rcu_start() and exit_tasks_rcu_finish()
    APIs for do_exit() use.  This has the benefit of confining the use of the
    tasks_rcu_exit_srcu variable to one file, allowing it to become static.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8337e2db0bb2..e4c38809a09e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -589,9 +589,10 @@ struct task_struct {
 
 #ifdef CONFIG_TASKS_RCU
 	unsigned long			rcu_tasks_nvcsw;
-	bool				rcu_tasks_holdout;
-	struct list_head		rcu_tasks_holdout_list;
+	u8				rcu_tasks_holdout;
+	u8				rcu_tasks_idx;
 	int				rcu_tasks_idle_cpu;
+	struct list_head		rcu_tasks_holdout_list;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
 	struct sched_info		sched_info;

commit 23f873d8f9526ed7e49a1a02a45f8afb9ae5fb84
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:53 2017 +0900

    locking/lockdep: Detect and handle hist_lock ring buffer overwrite
    
    The ring buffer can be overwritten by hardirq/softirq/work contexts.
    That cases must be considered on rollback or commit. For example,
    
              |<------ hist_lock ring buffer size ----->|
              ppppppppppppiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
    wrapped > iiiiiiiiiiiiiiiiiiiiiii....................
    
              where 'p' represents an acquisition in process context,
              'i' represents an acquisition in irq context.
    
    On irq exit, crossrelease tries to rollback idx to original position,
    but it should not because the entry already has been invalid by
    overwriting 'i'. Avoid rollback or commit for entries overwritten.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-7-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5235fba537fc..772c5f643764 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -854,6 +854,9 @@ struct task_struct {
 	unsigned int xhlock_idx;
 	/* For restoring at history boundaries */
 	unsigned int xhlock_idx_hist[XHLOCK_CTX_NR];
+	unsigned int hist_id;
+	/* For overwrite check at each context exit */
+	unsigned int hist_id_save[XHLOCK_CTX_NR];
 #endif
 
 #ifdef CONFIG_UBSAN

commit b09be676e0ff25bd6d2e7637e26d349f9109ad75
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:52 2017 +0900

    locking/lockdep: Implement the 'crossrelease' feature
    
    Lockdep is a runtime locking correctness validator that detects and
    reports a deadlock or its possibility by checking dependencies between
    locks. It's useful since it does not report just an actual deadlock but
    also the possibility of a deadlock that has not actually happened yet.
    That enables problems to be fixed before they affect real systems.
    
    However, this facility is only applicable to typical locks, such as
    spinlocks and mutexes, which are normally released within the context in
    which they were acquired. However, synchronization primitives like page
    locks or completions, which are allowed to be released in any context,
    also create dependencies and can cause a deadlock.
    
    So lockdep should track these locks to do a better job. The 'crossrelease'
    implementation makes these primitives also be tracked.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-6-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 57db70ec70d0..5235fba537fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -848,6 +848,14 @@ struct task_struct {
 	struct held_lock		held_locks[MAX_LOCK_DEPTH];
 #endif
 
+#ifdef CONFIG_LOCKDEP_CROSSRELEASE
+#define MAX_XHLOCKS_NR 64UL
+	struct hist_lock *xhlocks; /* Crossrelease history locks */
+	unsigned int xhlock_idx;
+	/* For restoring at history boundaries */
+	unsigned int xhlock_idx_hist[XHLOCK_CTX_NR];
+#endif
+
 #ifdef CONFIG_UBSAN
 	unsigned int			in_ubsan;
 #endif

commit d92a8cfcb37ecd1315269dab741f073b63b3a8b6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 3 10:13:38 2017 +0100

    locking/lockdep: Rework FS_RECLAIM annotation
    
    A while ago someone, and I cannot find the email just now, asked if we
    could not implement the RECLAIM_FS inversion stuff with a 'fake' lock
    like we use for other things like workqueues etc. I think this should
    be possible which allows reducing the 'irq' states and will reduce the
    amount of __bfs() lookups we do.
    
    Removing the 1 IRQ state results in 4 less __bfs() walks per
    dependency, improving lockdep performance. And by moving this
    annotation out of the lockdep code it becomes easier for the mm people
    to extend.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: iamjoonsoo.kim@lge.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8337e2db0bb2..57db70ec70d0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -846,7 +846,6 @@ struct task_struct {
 	int				lockdep_depth;
 	unsigned int			lockdep_recursion;
 	struct held_lock		held_locks[MAX_LOCK_DEPTH];
-	gfp_t				lockdep_reclaim_gfp;
 #endif
 
 #ifdef CONFIG_UBSAN

commit 20435d84e5f2041c64c792399ab6f2948a2c2252
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Mon Aug 7 16:44:23 2017 +0800

    sched/debug: Intruduce task_state_to_char() helper function
    
    Now that we have more than one place to get the task state,
    intruduce the task_state_to_char() helper function to save some code.
    
    No functionality changed.
    
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <cj.chengjian@huawei.com>
    Cc: <huawei.libin@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1502095463-160172-3-git-send-email-xiexiuqi@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8337e2db0bb2..c28b182c9833 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1229,6 +1229,19 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 	return task_pgrp_nr_ns(tsk, &init_pid_ns);
 }
 
+static inline char task_state_to_char(struct task_struct *task)
+{
+	const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
+	unsigned long state = task->state;
+
+	state = state ? __ffs(state) + 1 : 0;
+
+	/* Make sure the string lines up properly with the number of task states: */
+	BUILD_BUG_ON(sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1);
+
+	return state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?';
+}
+
 /**
  * is_global_init - check if a task structure is init. Since init
  * is free to have sub-threads we need to check tgid.

commit d6aaba615a482ce7d3ec218cf7b8d02d0d5753b8
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:34 2017 -0700

    x86/intel_rdt/cqm: Add tasks file support
    
    The root directory, ctrl_mon and monitor groups are populated
    with a read/write file named "tasks". When read, it shows all the task
    IDs assigned to the resource group.
    
    Tasks can be added to groups by writing the PID to the file. A task can
    be present in one "ctrl_mon" group "and" one "monitor" group. IOW a
    PID_x can be seen in a ctrl_mon group and a monitor group at the same
    time. When a task is added to a ctrl_mon group, it is automatically
    removed from the previous ctrl_mon group where it belonged. Similarly if
    a task is moved to a monitor group it is removed from the previous
    monitor group . Also since the monitor groups can only have subset of
    tasks of parent ctrl_mon group, a task can be moved to a monitor group
    only if its already present in the parent ctrl_mon group.
    
    Task membership is indicated by a new field in the task_struct "u32
    rmid" which holds the RMID for the task. RMID=0 is reserved for the
    default root group where the tasks belong to at mount.
    
    [tony: zero the rmid if rdtgroup was deleted when task was being moved]
    
    Signed-off-by: Tony Luck <tony.luck@linux.intel.com>
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-16-git-send-email-vikas.shivappa@linux.intel.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8839fd09540c..067a41ac5fd4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -900,6 +900,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_INTEL_RDT
 	u32				closid;
+	u32				rmid;
 #endif
 #ifdef CONFIG_FUTEX
 	struct robust_list_head __user	*robust_list;

commit 0734ded1abee9439b0c5d7b62af1ead78aab895b
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:33 2017 -0700

    x86/intel_rdt: Change closid type from int to u32
    
    OS associates a CLOSid(Class of service id) to a task by writing the
    high 32 bits of per CPU IA32_PQR_ASSOC MSR when a task is scheduled in.
    CPUID.(EAX=10H, ECX=1):EDX[15:0] enumerates the max CLOSID supported and
    it is zero indexed. Hence change the type to u32 from int.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-15-git-send-email-vikas.shivappa@linux.intel.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20b2ff2f4fde..8839fd09540c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -899,7 +899,7 @@ struct task_struct {
 	struct list_head		cg_list;
 #endif
 #ifdef CONFIG_INTEL_RDT
-	int				closid;
+	u32				closid;
 #endif
 #ifdef CONFIG_FUTEX
 	struct robust_list_head __user	*robust_list;

commit f01d7d51f577b5dc0fa5919ab8a9228e2bf49f3e
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:22 2017 -0700

    x86/intel_rdt: Introduce a common compile option for RDT
    
    We currently have a CONFIG_RDT_A which is for RDT(Resource directory
    technology) allocation based resctrl filesystem interface. As a
    preparation to add support for RDT monitoring as well into the same
    resctrl filesystem, change the config option to be CONFIG_RDT which
    would include both RDT allocation and monitoring code.
    
    No functional change.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-4-git-send-email-vikas.shivappa@linux.intel.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8337e2db0bb2..20b2ff2f4fde 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -898,7 +898,7 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
 	struct list_head		cg_list;
 #endif
-#ifdef CONFIG_INTEL_RDT_A
+#ifdef CONFIG_INTEL_RDT
 	int				closid;
 #endif
 #ifdef CONFIG_FUTEX

commit e06fdaf40a5c021dd4a2ec797e8b724f07360070
Merge: a90c6ac2b565 8acdf5055974
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 19 08:55:18 2017 -0700

    Merge tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull structure randomization updates from Kees Cook:
     "Now that IPC and other changes have landed, enable manual markings for
      randstruct plugin, including the task_struct.
    
      This is the rest of what was staged in -next for the gcc-plugins, and
      comes in three patches, largest first:
    
       - mark "easy" structs with __randomize_layout
    
       - mark task_struct with an optional anonymous struct to isolate the
         __randomize_layout section
    
       - mark structs to opt _out_ of automated marking (which will come
         later)
    
      And, FWIW, this continues to pass allmodconfig (normal and patched to
      enable gcc-plugins) builds of x86_64, i386, arm64, arm, powerpc, and
      s390 for me"
    
    * tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      randstruct: opt-out externally exposed function pointer structs
      task_struct: Allow randomized layout
      randstruct: Mark various structs for randomization

commit 9049f2f6e7bdfb5de0c63c2635bf3cdb70c4efb5
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Jul 14 14:49:52 2017 -0700

    fault-inject: parse as natural 1-based value for fail-nth write interface
    
    The value written to fail-nth file is parsed as 0-based.  Parsing as
    one-based is more natural to understand and it enables to cancel the
    previous setup by simply writing '0'.
    
    This change also converts task->fail_nth from signed to unsigned int.
    
    Link: http://lkml.kernel.org/r/1491490561-10485-3-git-send-email-akinobu.mita@gmail.com
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3822d749fc9e..2ba9ec93423f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -974,7 +974,7 @@ struct task_struct {
 
 #ifdef CONFIG_FAULT_INJECTION
 	int				make_it_fail;
-	int fail_nth;
+	unsigned int			fail_nth;
 #endif
 	/*
 	 * When (nr_dirtied >= nr_dirtied_pause), it's time to call

commit e41d58185f1444368873d4d7422f7664a68be61d
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Wed Jul 12 14:34:35 2017 -0700

    fault-inject: support systematic fault injection
    
    Add /proc/self/task/<current-tid>/fail-nth file that allows failing
    0-th, 1-st, 2-nd and so on calls systematically.
    Excerpt from the added documentation:
    
     "Write to this file of integer N makes N-th call in the current task
      fail (N is 0-based). Read from this file returns a single char 'Y' or
      'N' that says if the fault setup with a previous write to this file
      was injected or not, and disables the fault if it wasn't yet injected.
      Note that this file enables all types of faults (slab, futex, etc).
      This setting takes precedence over all other generic settings like
      probability, interval, times, etc. But per-capability settings (e.g.
      fail_futex/ignore-private) take precedence over it. This feature is
      intended for systematic testing of faults in a single system call. See
      an example below"
    
    Why add a new setting:
    1. Existing settings are global rather than per-task.
       So parallel testing is not possible.
    2. attr->interval is close but it depends on attr->count
       which is non reset to 0, so interval does not work as expected.
    3. Trying to model this with existing settings requires manipulations
       of all of probability, interval, times, space, task-filter and
       unexposed count and per-task make-it-fail files.
    4. Existing settings are per-failure-type, and the set of failure
       types is potentially expanding.
    5. make-it-fail can't be changed by unprivileged user and aggressive
       stress testing better be done from an unprivileged user.
       Similarly, this would require opening the debugfs files to the
       unprivileged user, as he would need to reopen at least times file
       (not possible to pre-open before dropping privs).
    
    The proposed interface solves all of the above (see the example).
    
    We want to integrate this into syzkaller fuzzer.  A prototype has found
    10 bugs in kernel in first day of usage:
    
      https://groups.google.com/forum/#!searchin/syzkaller/%22FAULT_INJECTION%22%7Csort:relevance
    
    I've made the current interface work with all types of our sandboxes.
    For setuid the secret sauce was prctl(PR_SET_DUMPABLE, 1, 0, 0, 0) to
    make /proc entries non-root owned.  So I am fine with the current
    version of the code.
    
    [akpm@linux-foundation.org: fix build]
    Link: http://lkml.kernel.org/r/20170328130128.101773-1-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20814b7d7d70..3822d749fc9e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -974,6 +974,7 @@ struct task_struct {
 
 #ifdef CONFIG_FAULT_INJECTION
 	int				make_it_fail;
+	int fail_nth;
 #endif
 	/*
 	 * When (nr_dirtied >= nr_dirtied_pause), it's time to call

commit 4fde846ac0f019b7c877da35e1c1517d79e17ffc
Merge: c3931a87db9e 242fc35290bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 9 10:52:16 2017 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Thomas Gleixner:
     "This scheduler update provides:
    
       - The (hopefully) final fix for the vtime accounting issues which
         were around for quite some time
    
       - Use types known to user space in UAPI headers to unbreak user space
         builds
    
       - Make load balancing respect the current scheduling domain again
         instead of evaluating unrelated CPUs"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/headers/uapi: Fix linux/sched/types.h userspace compilation errors
      sched/fair: Fix load_balance() affinity redo path
      sched/cputime: Accumulate vtime on top of nsec clocksource
      sched/cputime: Move the vtime task fields to their own struct
      sched/cputime: Rename vtime fields
      sched/cputime: Always set tsk->vtime_snap_whence after accounting vtime
      vtime, sched/cputime: Remove vtime_account_user()
      Revert "sched/cputime: Refactor the cputime_adjust() code"

commit 45816682b2cd6771cf63cb7dc7dbebdd827a0132
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Jul 6 15:39:59 2017 -0700

    mm, mempolicy: stop adjusting current->il_next in mpol_rebind_nodemask()
    
    The task->il_next variable stores the next allocation node id for task's
    MPOL_INTERLEAVE policy.  mpol_rebind_nodemask() updates interleave and
    bind mempolicies due to changing cpuset mems.  Currently it also tries
    to make sure that current->il_next is valid within the updated nodemask.
    This is bogus, because 1) we are updating potentially any task's
    mempolicy, not just current, and 2) we might be updating a per-vma
    mempolicy, not task one.
    
    The interleave_nodes() function that uses il_next can cope fine with the
    value not being within the currently allowed nodes, so this hasn't
    manifested as an actual issue.
    
    We can remove the need for updating il_next completely by changing it to
    il_prev and store the node id of the previous interleave allocation
    instead of the next id.  Then interleave_nodes() can calculate the next
    id using the current nodemask and also store it as il_prev, except when
    querying the next node via do_get_mempolicy().
    
    Link: http://lkml.kernel.org/r/20170517081140.30654-3-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c4ca7433d9d..5e8759b1b428 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -904,7 +904,7 @@ struct task_struct {
 #ifdef CONFIG_NUMA
 	/* Protected by alloc_lock: */
 	struct mempolicy		*mempolicy;
-	short				il_next;
+	short				il_prev;
 	short				pref_node_fork;
 #endif
 #ifdef CONFIG_NUMA_BALANCING

commit 2a42eb9594a1480b4ead9e036e06ee1290e5fa6d
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jun 29 19:15:11 2017 +0200

    sched/cputime: Accumulate vtime on top of nsec clocksource
    
    Currently the cputime source used by vtime is jiffies. When we cross
    a context boundary and jiffies have changed since the last snapshot, the
    pending cputime is accounted to the switching out context.
    
    This system works ok if the ticks are not aligned across CPUs. If they
    instead are aligned (ie: all fire at the same time) and the CPUs run in
    userspace, the jiffies change is only observed on tick exit and therefore
    the user cputime is accounted as system cputime. This is because the
    CPU that maintains timekeeping fires its tick at the same time as the
    others. It updates jiffies in the middle of the tick and the other CPUs
    see that update on IRQ exit:
    
        CPU 0 (timekeeper)                  CPU 1
        -------------------              -------------
                          jiffies = N
        ...                              run in userspace for a jiffy
        tick entry                       tick entry (sees jiffies = N)
        set jiffies = N + 1
        tick exit                        tick exit (sees jiffies = N + 1)
                                                    account 1 jiffy as stime
    
    Fix this with using a nanosec clock source instead of jiffies. The
    cputime is then accumulated and flushed everytime the pending delta
    reaches a jiffy in order to mitigate the accounting overhead.
    
    [ fweisbec: changelog, rebase on struct vtime, field renames, add delta
      on cputime readers, keep idle vtime as-is (low overhead accounting),
      harmonize clock sources. ]
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Luiz Capitulino <lcapitulino@redhat.com>
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-6-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eeff8a024f0c..4818126c5153 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -236,6 +236,9 @@ struct vtime {
 	seqcount_t		seqcount;
 	unsigned long long	starttime;
 	enum vtime_state	state;
+	u64			utime;
+	u64			stime;
+	u64			gtime;
 };
 
 struct sched_info {

commit bac5b6b6b11560f323e71d0ebac4061cfe5f56c0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:10 2017 +0200

    sched/cputime: Move the vtime task fields to their own struct
    
    We are about to add vtime accumulation fields to the task struct. Let's
    avoid more bloatification and gather vtime information to their own
    struct.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-5-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff001646549e..eeff8a024f0c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -223,6 +223,21 @@ struct task_cputime {
 #define prof_exp			stime
 #define sched_exp			sum_exec_runtime
 
+enum vtime_state {
+	/* Task is sleeping or running in a CPU with VTIME inactive: */
+	VTIME_INACTIVE = 0,
+	/* Task runs in userspace in a CPU with VTIME active: */
+	VTIME_USER,
+	/* Task runs in kernelspace in a CPU with VTIME active: */
+	VTIME_SYS,
+};
+
+struct vtime {
+	seqcount_t		seqcount;
+	unsigned long long	starttime;
+	enum vtime_state	state;
+};
+
 struct sched_info {
 #ifdef CONFIG_SCHED_INFO
 	/* Cumulative counters: */
@@ -688,16 +703,7 @@ struct task_struct {
 	u64				gtime;
 	struct prev_cputime		prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqcount_t			vtime_seqcount;
-	unsigned long long		vtime_starttime;
-	enum {
-		/* Task is sleeping or running in a CPU with VTIME inactive: */
-		VTIME_INACTIVE = 0,
-		/* Task runs in userspace in a CPU with VTIME active: */
-		VTIME_USER,
-		/* Task runs in kernelspace in a CPU with VTIME active: */
-		VTIME_SYS,
-	} vtime_state;
+	struct vtime			vtime;
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL

commit 60a9ce57e7c5ac1df3a39fb941022bbfa40c0862
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:09 2017 +0200

    sched/cputime: Rename vtime fields
    
    The current "snapshot" based naming on vtime fields suggests we record
    some past event but that's a low level picture of their actual purpose
    which comes out blurry. The real point of these fields is to run a basic
    state machine that tracks down cputime entry while switching between
    contexts.
    
    So lets reflect that with more meaningful names.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c4ca7433d9d..ff001646549e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -689,7 +689,7 @@ struct task_struct {
 	struct prev_cputime		prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqcount_t			vtime_seqcount;
-	unsigned long long		vtime_snap;
+	unsigned long long		vtime_starttime;
 	enum {
 		/* Task is sleeping or running in a CPU with VTIME inactive: */
 		VTIME_INACTIVE = 0,
@@ -697,7 +697,7 @@ struct task_struct {
 		VTIME_USER,
 		/* Task runs in kernelspace in a CPU with VTIME active: */
 		VTIME_SYS,
-	} vtime_snap_whence;
+	} vtime_state;
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL

commit 9a9594efe54324e9124add7e7b1e7bdb6d0b08a3
Merge: 3ad918e65d69 993647a29381
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 18:08:06 2017 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP hotplug updates from Thomas Gleixner:
     "This update is primarily a cleanup of the CPU hotplug locking code.
    
      The hotplug locking mechanism is an open coded RWSEM, which allows
      recursive locking. The main problem with that is the recursive nature
      as it evades the full lockdep coverage and hides potential deadlocks.
    
      The rework replaces the open coded RWSEM with a percpu RWSEM and
      establishes full lockdep coverage that way.
    
      The bulk of the changes fix up recursive locking issues and address
      the now fully reported potential deadlocks all over the place. Some of
      these deadlocks have been observed in the RT tree, but on mainline the
      probability was low enough to hide them away."
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      cpu/hotplug: Constify attribute_group structures
      powerpc: Only obtain cpu_hotplug_lock if called by rtasd
      ARM/hw_breakpoint: Fix possible recursive locking for arch_hw_breakpoint_init
      cpu/hotplug: Remove unused check_for_tasks() function
      perf/core: Don't release cred_guard_mutex if not taken
      cpuhotplug: Link lock stacks for hotplug callbacks
      acpi/processor: Prevent cpu hotplug deadlock
      sched: Provide is_percpu_thread() helper
      cpu/hotplug: Convert hotplug locking to percpu rwsem
      s390: Prevent hotplug rwsem recursion
      arm: Prevent hotplug rwsem recursion
      arm64: Prevent cpu hotplug rwsem recursion
      kprobes: Cure hotplug lock ordering issues
      jump_label: Reorder hotplug lock and jump_label_lock
      perf/tracing/cpuhotplug: Fix locking order
      ACPI/processor: Use cpu_hotplug_disable() instead of get_online_cpus()
      PCI: Replace the racy recursion prevention
      PCI: Use cpu_hotplug_disable() instead of get_online_cpus()
      perf/x86/intel: Drop get_online_cpus() in intel_snb_check_microcode()
      x86/perf: Drop EXPORT of perf_check_microcode
      ...

commit 29e48ce87f1eaaa4b1fe3d9af90c586ac2d1fb74
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 5 22:43:33 2017 -0700

    task_struct: Allow randomized layout
    
    This marks most of the layout of task_struct as randomizable, but leaves
    thread_info and scheduler state untouched at the start, and thread_struct
    untouched at the end.
    
    Other parts of the kernel use unnamed structures, but the 0-day builder
    using gcc-4.4 blows up on static initializers. Officially, it's documented
    as only working on gcc 4.6 and later, which further confuses me:
            https://gcc.gnu.org/wiki/C11Status
    The structure layout randomization already requires gcc 4.7, but instead
    of depending on the plugin being enabled, just check the gcc versions
    for wider build testing. At Linus's suggestion, the marking is hidden
    in a macro to reduce how ugly it looks. Additionally, indenting is left
    unchanged since it would make things harder to read.
    
    Randomization of task_struct is modified from Brad Spengler/PaX Team's
    code in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f833254fce00..e2ad3531e7fe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -490,6 +490,13 @@ struct task_struct {
 #endif
 	/* -1 unrunnable, 0 runnable, >0 stopped: */
 	volatile long			state;
+
+	/*
+	 * This begins the randomizable portion of task_struct. Only
+	 * scheduling-critical items should be added above here.
+	 */
+	randomized_struct_fields_start
+
 	void				*stack;
 	atomic_t			usage;
 	/* Per task flags (PF_*), defined further below: */
@@ -1051,6 +1058,13 @@ struct task_struct {
 	/* Used by LSM modules for access restriction: */
 	void				*security;
 #endif
+
+	/*
+	 * New fields for task_struct should be added above here, so that
+	 * they are included in the randomized portion of task_struct.
+	 */
+	randomized_struct_fields_end
+
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;
 

commit 3859a271a003aba01e45b85c9d8b355eb7bf25f9
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Oct 28 01:22:25 2016 -0700

    randstruct: Mark various structs for randomization
    
    This marks many critical kernel structures for randomization. These are
    structures that have been targeted in the past in security exploits, or
    contain functions pointers, pointers to function pointer tables, lists,
    workqueues, ref-counters, credentials, permissions, or are otherwise
    sensitive. This initial list was extracted from Brad Spengler/PaX Team's
    code in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    Left out of this list is task_struct, which requires special handling
    and will be covered in a subsequent patch.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2b69fc650201..f833254fce00 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -408,7 +408,7 @@ struct sched_rt_entity {
 	/* rq "owned" by this entity/group: */
 	struct rt_rq			*my_q;
 #endif
-};
+} __randomize_layout;
 
 struct sched_dl_entity {
 	struct rb_node			rb_node;

commit 3effcb4247e74a51f5d8b775a1ee4abf87cc089a
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Mon May 29 16:24:03 2017 +0200

    sched/deadline: Use the revised wakeup rule for suspending constrained dl tasks
    
    We have been facing some problems with self-suspending constrained
    deadline tasks. The main reason is that the original CBS was not
    designed for such sort of tasks.
    
    One problem reported by Xunlei Pang takes place when a task
    suspends, and then is awakened before the deadline, but so close
    to the deadline that its remaining runtime can cause the task
    to have an absolute density higher than allowed. In such situation,
    the original CBS assumes that the task is facing an early activation,
    and so it replenishes the task and set another deadline, one deadline
    in the future. This rule works fine for implicit deadline tasks.
    Moreover, it allows the system to adapt the period of a task in which
    the external event source suffered from a clock drift.
    
    However, this opens the window for bandwidth leakage for constrained
    deadline tasks. For instance, a task with the following parameters:
    
      runtime   = 5 ms
      deadline  = 7 ms
      [density] = 5 / 7 = 0.71
      period    = 1000 ms
    
    If the task runs for 1 ms, and then suspends for another 1ms,
    it will be awakened with the following parameters:
    
      remaining runtime = 4
      laxity = 5
    
    presenting a absolute density of 4 / 5 = 0.80.
    
    In this case, the original CBS would assume the task had an early
    wakeup. Then, CBS will reset the runtime, and the absolute deadline will
    be postponed by one relative deadline, allowing the task to run.
    
    The problem is that, if the task runs this pattern forever, it will keep
    receiving bandwidth, being able to run 1ms every 2ms. Following this
    behavior, the task would be able to run 500 ms in 1 sec. Thus running
    more than the 5 ms / 1 sec the admission control allowed it to run.
    
    Trying to address the self-suspending case, Luca Abeni, Giuseppe
    Lipari, and Juri Lelli [1] revisited the CBS in order to deal with
    self-suspending tasks. In the new approach, rather than
    replenishing/postponing the absolute deadline, the revised wakeup rule
    adjusts the remaining runtime, reducing it to fit into the allowed
    density.
    
    A revised version of the idea is:
    
    At a given time t, the maximum absolute density of a task cannot be
    higher than its relative density, that is:
    
      runtime / (deadline - t) <= dl_runtime / dl_deadline
    
    Knowing the laxity of a task (deadline - t), it is possible to move
    it to the other side of the equality, thus enabling to define max
    remaining runtime a task can use within the absolute deadline, without
    over-running the allowed density:
    
      runtime = (dl_runtime / dl_deadline) * (deadline - t)
    
    For instance, in our previous example, the task could still run:
    
      runtime = ( 5 / 7 ) * 5
      runtime = 3.57 ms
    
    Without causing damage for other deadline tasks. It is note worthy
    that the laxity cannot be negative because that would cause a negative
    runtime. Thus, this patch depends on the patch:
    
      df8eac8cafce ("sched/deadline: Throttle a constrained deadline task activated after the deadline")
    
    Which throttles a constrained deadline task activated after the
    deadline.
    
    Finally, it is also possible to use the revised wakeup rule for
    all other tasks, but that would require some more discussions
    about pros and cons.
    
    Reported-by: Xunlei Pang <xpang@redhat.com>
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    [peterz: replaced dl_is_constrained with dl_is_implicit]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Romulo Silva de Oliveira <romulo.deoliveira@ufsc.br>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/5c800ab3a74a168a84ee5f3f84d12a02e11383be.1495803804.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3113c828483b..1f0f427e0292 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -422,6 +422,7 @@ struct sched_dl_entity {
 	u64				dl_deadline;	/* Relative deadline of each instance	*/
 	u64				dl_period;	/* Separation of two instances (period) */
 	u64				dl_bw;		/* dl_runtime / dl_period		*/
+	u64				dl_density;	/* dl_runtime / dl_deadline		*/
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,

commit 54d6d3039e2d84b6fbfbe59ec57d856371edf0a2
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Mon May 29 16:24:02 2017 +0200

    sched/deadline: Fix dl_bw comment
    
    The sched_dl_entity's dl_bw variable stores the utilization (dl_runtime / dl_period)
    of a task, not its density (dl_runtime / dl_deadline), as the comment says.
    
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Romulo Silva de Oliveira <romulo.deoliveira@ufsc.br>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Cc: Xunlei Pang <xpang@redhat.com>
    Link: http://lkml.kernel.org/r/8d05f1ccfd02da1a11bda62494d98f5456c1469a.1495803804.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f1ead2e88d3d..3113c828483b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -421,7 +421,7 @@ struct sched_dl_entity {
 	u64				dl_runtime;	/* Maximum runtime for each instance	*/
 	u64				dl_deadline;	/* Relative deadline of each instance	*/
 	u64				dl_period;	/* Separation of two instances (period) */
-	u64				dl_bw;		/* dl_runtime / dl_deadline		*/
+	u64				dl_bw;		/* dl_runtime / dl_period		*/
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,

commit 209a0cbda7a01d2ea32a8b631d35e873bee498e9
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:29 2017 +0200

    sched/deadline: Improve the tracking of active utilization
    
    This patch implements a more theoretically sound algorithm for
    tracking active utilization: instead of decreasing it when a
    task blocks, use a timer (the "inactive timer", named after the
    "Inactive" task state of the GRUB algorithm) to decrease the
    active utilization at the so called "0-lag time".
    
    Tested-by: Claudio Scordino <claudio@evidence.eu.com>
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-3-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1abaa3728bf7..f1ead2e88d3d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -445,16 +445,33 @@ struct sched_dl_entity {
 	 *
 	 * @dl_yielded tells if task gave up the CPU before consuming
 	 * all its available runtime during the last job.
+	 *
+	 * @dl_non_contending tells if the task is inactive while still
+	 * contributing to the active utilization. In other words, it
+	 * indicates if the inactive timer has been armed and its handler
+	 * has not been executed yet. This flag is useful to avoid race
+	 * conditions between the inactive timer handler and the wakeup
+	 * code.
 	 */
 	int				dl_throttled;
 	int				dl_boosted;
 	int				dl_yielded;
+	int				dl_non_contending;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its
 	 * own bandwidth to be enforced, thus we need one timer per task.
 	 */
 	struct hrtimer			dl_timer;
+
+	/*
+	 * Inactive timer, responsible for decreasing the active utilization
+	 * at the "0-lag time". When a -deadline task blocks, it contributes
+	 * to GRUB's active utilization until the "0-lag time", hence a
+	 * timer is needed to decrease the active utilization at the correct
+	 * time.
+	 */
+	struct hrtimer inactive_timer;
 };
 
 union rcu_special {

commit 9b01d43170aa70a435105f6413759e2ab7e00219
Author: Perr Zhang <strongbox8@zoho.com>
Date:   Fri Jun 2 11:59:53 2017 +0800

    sched/header: Remove leftover, obsolete comment
    
    There is no more set_task_vxid() helper, remove its description.
    
    Signed-off-by: Perr Zhang <strongbox8@zoho.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170602035953.28949-1-strongbox8@zoho.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2b69fc650201..1abaa3728bf7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1096,8 +1096,6 @@ static inline struct pid *task_session(struct task_struct *task)
  *                     current.
  * task_xid_nr_ns()  : id seen from the ns specified;
  *
- * set_task_vxid()   : assigns a virtual id to a task;
- *
  * see also pid_nr() etc in include/linux/pid.h
  */
 pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);

commit 62ec05dd71b19f5be890a1992227cc7b2ac0adc4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:41 2017 +0200

    sched: Provide is_percpu_thread() helper
    
    Provide a helper function for checking whether current task is a per cpu
    thread.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170524081549.541649540@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2b69fc650201..3dfa5f99d6ee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1265,6 +1265,16 @@ extern struct pid *cad_pid;
 #define tsk_used_math(p)			((p)->flags & PF_USED_MATH)
 #define used_math()				tsk_used_math(current)
 
+static inline bool is_percpu_thread(void)
+{
+#ifdef CONFIG_SMP
+	return (current->flags & PF_NO_SETAFFINITY) &&
+		(current->nr_cpus_allowed  == 1);
+#else
+	return true;
+#endif
+}
+
 /* Per-process atomic flags. */
 #define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
 #define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */

commit c6a677c6f37bb7abc85ba7e3465e82b9f7eb1d91
Merge: e87d51ac61f8 11270059e8d0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 18:16:23 2017 -0700

    Merge tag 'staging-4.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging
    
    Pull staging/IIO updates from Greg KH:
     "Here is the big staging tree update for 4.12-rc1.
    
      It's a big one, adding about 350k new lines of crap^Wcode, mostly all
      in a big dump of media drivers from Intel. But there's other new
      drivers in here as well, yet-another-wifi driver, new IIO drivers, and
      a new crypto accelerator.
    
      We also deleted a bunch of stuff, mostly in patch cleanups, but also
      the Android ION code has shrunk a lot, and the Android low memory
      killer driver was finally deleted, much to the celebration of the -mm
      developers.
    
      All of these have been in linux-next with a few build issues that will
      show up when you merge to your tree"
    
    Merge conflicts in the new rtl8723bs driver (due to the wifi changes
    this merge window) handled as per linux-next, courtesy of Stephen
    Rothwell.
    
    * tag 'staging-4.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging: (1182 commits)
      staging: fsl-mc/dpio: add cpu <--> LE conversion for dpaa2_fd
      staging: ks7010: remove line continuations in quoted strings
      staging: vt6656: use tabs instead of spaces
      staging: android: ion: Fix unnecessary initialization of static variable
      staging: media: atomisp: fix range checking on clk_num
      staging: media: atomisp: fix misspelled word in comment
      staging: media: atomisp: kmap() can't fail
      staging: atomisp: remove #ifdef for runtime PM functions
      staging: atomisp: satm include directory is gone
      atomisp: remove some more unused files
      atomisp: remove hmm_load/store/clear indirections
      atomisp: kill off mmgr_free
      atomisp: clean up the hmm init/cleanup indirections
      atomisp: handle allocation calls before init in the hmm layer
      staging: fsl-dpaa2/eth: Add maintainer for Ethernet driver
      staging: fsl-dpaa2/eth: Add TODO file
      staging: fsl-dpaa2/eth: Add trace points
      staging: fsl-dpaa2/eth: Add driver specific stats
      staging: fsl-dpaa2/eth: Add ethtool support
      staging: fsl-dpaa2/eth: Add Freescale DPAA2 Ethernet driver
      ...

commit 7dea19f9ee636cb244109a4dba426bbb3e5304b7
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed May 3 14:53:15 2017 -0700

    mm: introduce memalloc_nofs_{save,restore} API
    
    GFP_NOFS context is used for the following 5 reasons currently:
    
     - to prevent from deadlocks when the lock held by the allocation
       context would be needed during the memory reclaim
    
     - to prevent from stack overflows during the reclaim because the
       allocation is performed from a deep context already
    
     - to prevent lockups when the allocation context depends on other
       reclaimers to make a forward progress indirectly
    
     - just in case because this would be safe from the fs POV
    
     - silence lockdep false positives
    
    Unfortunately overuse of this allocation context brings some problems to
    the MM.  Memory reclaim is much weaker (especially during heavy FS
    metadata workloads), OOM killer cannot be invoked because the MM layer
    doesn't have enough information about how much memory is freeable by the
    FS layer.
    
    In many cases it is far from clear why the weaker context is even used
    and so it might be used unnecessarily.  We would like to get rid of
    those as much as possible.  One way to do that is to use the flag in
    scopes rather than isolated cases.  Such a scope is declared when really
    necessary, tracked per task and all the allocation requests from within
    the context will simply inherit the GFP_NOFS semantic.
    
    Not only this is easier to understand and maintain because there are
    much less problematic contexts than specific allocation requests, this
    also helps code paths where FS layer interacts with other layers (e.g.
    crypto, security modules, MM etc...) and there is no easy way to convey
    the allocation context between the layers.
    
    Introduce memalloc_nofs_{save,restore} API to control the scope of
    GFP_NOFS allocation context.  This is basically copying
    memalloc_noio_{save,restore} API we have for other restricted allocation
    context GFP_NOIO.  The PF_MEMALLOC_NOFS flag already exists and it is
    just an alias for PF_FSTRANS which has been xfs specific until recently.
    There are no more PF_FSTRANS users anymore so let's just drop it.
    
    PF_MEMALLOC_NOFS is now checked in the MM layer and drops __GFP_FS
    implicitly same as PF_MEMALLOC_NOIO drops __GFP_IO.  memalloc_noio_flags
    is renamed to current_gfp_context because it now cares about both
    PF_MEMALLOC_NOFS and PF_MEMALLOC_NOIO contexts.  Xfs code paths preserve
    their semantic.  kmem_flags_convert() doesn't need to evaluate the flag
    anymore.
    
    This patch shouldn't introduce any functional changes.
    
    Let's hope that filesystems will drop direct GFP_NOFS (resp.  ~__GFP_FS)
    usage as much as possible and only use a properly documented
    memalloc_nofs_{save,restore} checkpoints where they are appropriate.
    
    [akpm@linux-foundation.org: fix comment typo, reflow comment]
    Link: http://lkml.kernel.org/r/20170306131408.9828-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <clm@fb.com>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Brian Foster <bfoster@redhat.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8ac11465ac5b..993e7e25a3a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1224,9 +1224,9 @@ extern struct pid *cad_pid;
 #define PF_USED_ASYNC		0x00004000	/* Used async_schedule*(), used by module init */
 #define PF_NOFREEZE		0x00008000	/* This thread should not be frozen */
 #define PF_FROZEN		0x00010000	/* Frozen for system suspend */
-#define PF_FSTRANS		0x00020000	/* Inside a filesystem transaction */
-#define PF_KSWAPD		0x00040000	/* I am kswapd */
-#define PF_MEMALLOC_NOIO	0x00080000	/* Allocating memory without IO involved */
+#define PF_KSWAPD		0x00020000	/* I am kswapd */
+#define PF_MEMALLOC_NOFS	0x00040000	/* All allocation requests will inherit GFP_NOFS */
+#define PF_MEMALLOC_NOIO	0x00080000	/* All allocation requests will inherit GFP_NOIO */
 #define PF_LESS_THROTTLE	0x00100000	/* Throttle me less: I clean memory */
 #define PF_KTHREAD		0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
@@ -1237,8 +1237,6 @@ extern struct pid *cad_pid;
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */
 
-#define PF_MEMALLOC_NOFS PF_FSTRANS	/* Transition to a more generic GFP_NOFS scope semantic */
-
 /*
  * Only the _current_ task can read/write to tsk->flags, but other
  * tasks can access tsk->flags in readonly mode for example

commit 9070733b4efac4bf17f299a81b01c15e206f9ff5
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed May 3 14:53:12 2017 -0700

    xfs: abstract PF_FSTRANS to PF_MEMALLOC_NOFS
    
    xfs has defined PF_FSTRANS to declare a scope GFP_NOFS semantic quite
    some time ago.  We would like to make this concept more generic and use
    it for other filesystems as well.  Let's start by giving the flag a more
    generic name PF_MEMALLOC_NOFS which is in line with an exiting
    PF_MEMALLOC_NOIO already used for the same purpose for GFP_NOIO
    contexts.  Replace all PF_FSTRANS usage from the xfs code in the first
    step before we introduce a full API for it as xfs uses the flag directly
    anyway.
    
    This patch doesn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170306131408.9828-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Brian Foster <bfoster@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <clm@fb.com>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d4fa448223f..8ac11465ac5b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1237,6 +1237,8 @@ extern struct pid *cad_pid;
 #define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */
 
+#define PF_MEMALLOC_NOFS PF_FSTRANS	/* Transition to a more generic GFP_NOFS scope semantic */
+
 /*
  * Only the _current_ task can read/write to tsk->flags, but other
  * tasks can access tsk->flags in readonly mode for example

commit 0302e28dee643932ee7b3c112ebccdbb9f8ec32c
Merge: 89c9fea3c803 8979b02aaf1d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 08:50:52 2017 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "Highlights:
    
      IMA:
       - provide ">" and "<" operators for fowner/uid/euid rules
    
      KEYS:
       - add a system blacklist keyring
    
       - add KEYCTL_RESTRICT_KEYRING, exposes keyring link restriction
         functionality to userland via keyctl()
    
      LSM:
       - harden LSM API with __ro_after_init
    
       - add prlmit security hook, implement for SELinux
    
       - revive security_task_alloc hook
    
      TPM:
       - implement contextual TPM command 'spaces'"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (98 commits)
      tpm: Fix reference count to main device
      tpm_tis: convert to using locality callbacks
      tpm: fix handling of the TPM 2.0 event logs
      tpm_crb: remove a cruft constant
      keys: select CONFIG_CRYPTO when selecting DH / KDF
      apparmor: Make path_max parameter readonly
      apparmor: fix parameters so that the permission test is bypassed at boot
      apparmor: fix invalid reference to index variable of iterator line 836
      apparmor: use SHASH_DESC_ON_STACK
      security/apparmor/lsm.c: set debug messages
      apparmor: fix boolreturn.cocci warnings
      Smack: Use GFP_KERNEL for smk_netlbl_mls().
      smack: fix double free in smack_parse_opts_str()
      KEYS: add SP800-56A KDF support for DH
      KEYS: Keyring asymmetric key restrict method with chaining
      KEYS: Restrict asymmetric key linkage using a specific keychain
      KEYS: Add a lookup_restriction function for the asymmetric key type
      KEYS: Add KEYCTL_RESTRICT_KEYRING
      KEYS: Consistent ordering for __key_link_begin and restrict check
      KEYS: Add an optional lookup_restriction hook to key_type
      ...

commit 89c9fea3c8034cdb2fd745f551cde0b507fd6893
Merge: 76f1948a79b2 6fbc8798d946
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 19:09:35 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree updates from Jiri Kosina.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial:
      tty: fix comment for __tty_alloc_driver()
      init/main: properly align the multi-line comment
      init/main: Fix double "the" in comment
      Fix dead URLs to ftp.kernel.org
      drivers: Clean up duplicated email address
      treewide: Fix typo in xml/driver-api/basics.xml
      tools/testing/selftests/powerpc: remove redundant CFLAGS in Makefile: "-Wall -O2 -Wall" -> "-O2 -Wall"
      selftests/timers: Spelling s/privledges/privileges/
      HID: picoLCD: Spelling s/REPORT_WRTIE_MEMORY/REPORT_WRITE_MEMORY/
      net: phy: dp83848: Fix Typo
      UBI: Fix typos
      Documentation: ftrace.txt: Correct nice value of 120 priority
      net: fec: Fix typo in error msg and comment
      treewide: Fix typos in printk

commit 76f1948a79b26d5f57a5ee9941876b745c6baaea
Merge: 7af4c727c7b6 a0841609f658
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 18:24:16 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatch updates from Jiri Kosina:
    
     - a per-task consistency model is being added for architectures that
       support reliable stack dumping (extending this, currently rather
       trivial set, is currently in the works).
    
       This extends the nature of the types of patches that can be applied
       by live patching infrastructure. The code stems from the design
       proposal made [1] back in November 2014. It's a hybrid of SUSE's
       kGraft and RH's kpatch, combining advantages of both: it uses
       kGraft's per-task consistency and syscall barrier switching combined
       with kpatch's stack trace switching. There are also a number of
       fallback options which make it quite flexible.
    
       Most of the heavy lifting done by Josh Poimboeuf with help from
       Miroslav Benes and Petr Mladek
    
       [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
     - module load time patch optimization from Zhou Chengming
    
     - a few assorted small fixes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: add missing printk newlines
      livepatch: Cancel transition a safe way for immediate patches
      livepatch: Reduce the time of finding module symbols
      livepatch: make klp_mutex proper part of API
      livepatch: allow removal of a disabled patch
      livepatch: add /proc/<pid>/patch_state
      livepatch: change to a per-task consistency model
      livepatch: store function sizes
      livepatch: use kstrtobool() in enabled_store()
      livepatch: move patching functions into patch.c
      livepatch: remove unnecessary object loaded check
      livepatch: separate enabled and patched states
      livepatch/s390: add TIF_PATCH_PENDING thread flag
      livepatch/s390: reorganize TIF thread flag bits
      livepatch/powerpc: add TIF_PATCH_PENDING thread flag
      livepatch/x86: add TIF_PATCH_PENDING thread flag
      livepatch: create temporary klp_update_patch_state() stub
      x86/entry: define _TIF_ALLWORK_MASK flags explicitly
      stacktrace/x86: add function for detecting reliable stack traces

commit 207fb8c3049cdc31de20ca9f91d0ae319125eb62
Merge: 3527d3e9514f 59cd42c29618
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 19:36:00 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - a big round of FUTEX_UNLOCK_PI improvements, fixes, cleanups and
         general restructuring
    
       - lockdep updates such as new checks for lock_downgrade()
    
       - introduce the new atomic_try_cmpxchg() locking API and use it to
         optimize refcount code generation
    
       - ... plus misc fixes, updates and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (38 commits)
      MAINTAINERS: Add FUTEX SUBSYSTEM
      futex: Clarify mark_wake_futex memory barrier usage
      futex: Fix small (and harmless looking) inconsistencies
      futex: Avoid freeing an active timer
      rtmutex: Plug preempt count leak in rt_mutex_futex_unlock()
      rtmutex: Fix more prio comparisons
      rtmutex: Fix PI chain order integrity
      sched,tracing: Update trace_sched_pi_setprio()
      sched/rtmutex: Refactor rt_mutex_setprio()
      rtmutex: Clean up
      sched/deadline/rtmutex: Dont miss the dl_runtime/dl_period update
      sched/rtmutex/deadline: Fix a PI crash for deadline tasks
      rtmutex: Deboost before waking up the top waiter
      locking/ww-mutex: Limit stress test to 2 seconds
      locking/atomic: Fix atomic_try_cmpxchg() semantics
      lockdep: Fix per-cpu static objects
      futex: Drop hb->lock before enqueueing on the rtmutex
      futex: Futex_unlock_pi() determinism
      futex: Rework futex_lock_pi() to use rt_mutex_*_proxy_lock()
      futex,rt_mutex: Restructure rt_mutex_finish_proxy_lock()
      ...

commit 3527d3e9514f013f361fba29fd71858d9361049d
Merge: 3711c94fd659 21173d0b4d2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 19:12:53 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - another round of rq-clock handling debugging, robustization and
         fixes
    
       - PELT accounting improvements
    
       - CPU hotplug related ->cpus_allowed affinity handling fixes all
         around the tree
    
       - ... plus misc fixes, cleanups and updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (35 commits)
      sched/x86: Update reschedule warning text
      crypto: N2 - Replace racy task affinity logic
      cpufreq/sparc-us2e: Replace racy task affinity logic
      cpufreq/sparc-us3: Replace racy task affinity logic
      cpufreq/sh: Replace racy task affinity logic
      cpufreq/ia64: Replace racy task affinity logic
      ACPI/processor: Replace racy task affinity logic
      ACPI/processor: Fix error handling in __acpi_processor_start()
      sparc/sysfs: Replace racy task affinity logic
      powerpc/smp: Replace open coded task affinity logic
      ia64/sn/hwperf: Replace racy task affinity logic
      ia64/salinfo: Replace racy task affinity logic
      workqueue: Provide work_on_cpu_safe()
      ia64/topology: Remove cpus_allowed manipulation
      sched/fair: Move the PELT constants into a generated header
      sched/fair: Increase PELT accuracy for small tasks
      sched/fair: Fix comments
      sched/Documentation: Add 'sched-pelt' tool
      sched/fair: Fix corner case in __accumulate_sum()
      sched/core: Remove 'task' parameter and rename tsk_restore_flags() to current_restore_flags()
      ...

commit 0ba78a95a6629975ff16545ae868fa1bb38f786a
Merge: def34eaae5ce a232591ba289
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Apr 14 10:29:40 2017 +0200

    Merge branch 'linus' into locking/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 717a94b5fc7092afebe9c93791f29b2d8e5d297a
Author: NeilBrown <neilb@suse.com>
Date:   Fri Apr 7 10:03:26 2017 +1000

    sched/core: Remove 'task' parameter and rename tsk_restore_flags() to current_restore_flags()
    
    It is not safe for one thread to modify the ->flags
    of another thread as there is no locking that can protect
    the update.
    
    So tsk_restore_flags(), which takes a task pointer and modifies
    the flags, is an invitation to do the wrong thing.
    
    All current users pass "current" as the task, so no developers have
    accepted that invitation.  It would be best to ensure it remains
    that way.
    
    So rename tsk_restore_flags() to current_restore_flags() and don't
    pass in a task_struct pointer.  Always operate on current->flags.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..0978fb74e45a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1286,10 +1286,10 @@ TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
 TASK_PFA_SET(LMK_WAITING, lmk_waiting)
 
 static inline void
-tsk_restore_flags(struct task_struct *task, unsigned long orig_flags, unsigned long flags)
+current_restore_flags(unsigned long orig_flags, unsigned long flags)
 {
-	task->flags &= ~flags;
-	task->flags |= orig_flags & flags;
+	current->flags &= ~flags;
+	current->flags |= orig_flags & flags;
 }
 
 extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);

commit e96a7705e7d3fef96aec9b590c63b2f6f7d2ba22
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Thu Mar 23 15:56:08 2017 +0100

    sched/rtmutex/deadline: Fix a PI crash for deadline tasks
    
    A crash happened while I was playing with deadline PI rtmutex.
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
        IP: [<ffffffff810eeb8f>] rt_mutex_get_top_task+0x1f/0x30
        PGD 232a75067 PUD 230947067 PMD 0
        Oops: 0000 [#1] SMP
        CPU: 1 PID: 10994 Comm: a.out Not tainted
    
        Call Trace:
        [<ffffffff810b658c>] enqueue_task+0x2c/0x80
        [<ffffffff810ba763>] activate_task+0x23/0x30
        [<ffffffff810d0ab5>] pull_dl_task+0x1d5/0x260
        [<ffffffff810d0be6>] pre_schedule_dl+0x16/0x20
        [<ffffffff8164e783>] __schedule+0xd3/0x900
        [<ffffffff8164efd9>] schedule+0x29/0x70
        [<ffffffff8165035b>] __rt_mutex_slowlock+0x4b/0xc0
        [<ffffffff81650501>] rt_mutex_slowlock+0xd1/0x190
        [<ffffffff810eeb33>] rt_mutex_timed_lock+0x53/0x60
        [<ffffffff810ecbfc>] futex_lock_pi.isra.18+0x28c/0x390
        [<ffffffff810ed8b0>] do_futex+0x190/0x5b0
        [<ffffffff810edd50>] SyS_futex+0x80/0x180
    
    This is because rt_mutex_enqueue_pi() and rt_mutex_dequeue_pi()
    are only protected by pi_lock when operating pi waiters, while
    rt_mutex_get_top_task(), will access them with rq lock held but
    not holding pi_lock.
    
    In order to tackle it, we introduce new "pi_top_task" pointer
    cached in task_struct, and add new rt_mutex_update_top_task()
    to update its value, it can be called by rt_mutex_setprio()
    which held both owner's pi_lock and rq lock. Thus "pi_top_task"
    can be safely accessed by enqueue_task_dl() under rq lock.
    
    Originally-From: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: juri.lelli@arm.com
    Cc: bigeasy@linutronix.de
    Cc: mathieu.desnoyers@efficios.com
    Cc: jdesfossez@efficios.com
    Cc: bristot@redhat.com
    Link: http://lkml.kernel.org/r/20170323150216.157682758@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..1ea2eee7bc4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -775,6 +775,8 @@ struct task_struct {
 	/* PI waiters blocked on a rt_mutex held by this task: */
 	struct rb_root			pi_waiters;
 	struct rb_node			*pi_waiters_leftmost;
+	/* Updated under owner's pi_lock and rq lock */
+	struct task_struct		*pi_top_task;
 	/* Deadlock detection and priority inheritance handling: */
 	struct rt_mutex_waiter		*pi_blocked_on;
 #endif

commit e4e55b47ed9ae2c05ff062601ff6dacbe9dc4775
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Mar 24 20:46:33 2017 +0900

    LSM: Revive security_task_alloc() hook and per "struct task_struct" security blob.
    
    We switched from "struct task_struct"->security to "struct cred"->security
    in Linux 2.6.29. But not all LSM modules were happy with that change.
    TOMOYO LSM module is an example which want to use per "struct task_struct"
    security blob, for TOMOYO's security context is defined based on "struct
    task_struct" rather than "struct cred". AppArmor LSM module is another
    example which want to use it, for AppArmor is currently abusing the cred
    a little bit to store the change_hat and setexeccon info. Although
    security_task_free() hook was revived in Linux 3.4 because Yama LSM module
    wanted to release per "struct task_struct" security blob,
    security_task_alloc() hook and "struct task_struct"->security field were
    not revived. Nowadays, we are getting proposals of lightweight LSM modules
    which want to use per "struct task_struct" security blob.
    
    We are already allowing multiple concurrent LSM modules (up to one fully
    armored module which uses "struct cred"->security field or exclusive hooks
    like security_xfrm_state_pol_flow_match(), plus unlimited number of
    lightweight modules which do not use "struct cred"->security nor exclusive
    hooks) as long as they are built into the kernel. But this patch does not
    implement variable length "struct task_struct"->security field which will
    become needed when multiple LSM modules want to use "struct task_struct"->
    security field. Although it won't be difficult to implement variable length
    "struct task_struct"->security field, let's think about it after we merged
    this patch.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: John Johansen <john.johansen@canonical.com>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Acked-by: Casey Schaufler <casey@schaufler-ca.com>
    Tested-by: Djalal Harouni <tixxdz@gmail.com>
    Acked-by: JosÃ© Bollo <jobol@nonadev.net>
    Cc: Paul Moore <paul@paul-moore.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Eric Paris <eparis@parisplace.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: JosÃ© Bollo <jobol@nonadev.net>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..71b8df306bb0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1037,6 +1037,10 @@ struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* A live task holds one reference: */
 	atomic_t			stack_refcount;
+#endif
+#ifdef CONFIG_SECURITY
+	/* Used by LSM modules for access restriction: */
+	void				*security;
 #endif
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;

commit 0ba42a599fbf59a55c1ffedb980be3726c734433
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Tue Mar 7 20:48:02 2017 +0900

    treewide: Fix typo in xml/driver-api/basics.xml
    
    This patch fix spelling typos found in
    Documentation/output/xml/driver-api/basics.xml.
    It is because the xml file was generated from comments in source,
    so I had to fix the comments.
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..5ce85e861901 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -186,7 +186,7 @@ extern long io_schedule_timeout(long timeout);
 extern void io_schedule(void);
 
 /**
- * struct prev_cputime - snaphsot of system and user cputime
+ * struct prev_cputime - snapshot of system and user cputime
  * @utime: time spent in user mode
  * @stime: time spent in system mode
  * @lock: protects the above two fields

commit 77f88796cee819b9c4562b0b6b44691b3b7755b1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 16 16:54:24 2017 -0400

    cgroup, kthread: close race window where new kthreads can be migrated to non-root cgroups
    
    Creation of a kthread goes through a couple interlocked stages between
    the kthread itself and its creator.  Once the new kthread starts
    running, it initializes itself and wakes up the creator.  The creator
    then can further configure the kthread and then let it start doing its
    job by waking it up.
    
    In this configuration-by-creator stage, the creator is the only one
    that can wake it up but the kthread is visible to userland.  When
    altering the kthread's attributes from userland is allowed, this is
    fine; however, for cases where CPU affinity is critical,
    kthread_bind() is used to first disable affinity changes from userland
    and then set the affinity.  This also prevents the kthread from being
    migrated into non-root cgroups as that can affect the CPU affinity and
    many other things.
    
    Unfortunately, the cgroup side of protection is racy.  While the
    PF_NO_SETAFFINITY flag prevents further migrations, userland can win
    the race before the creator sets the flag with kthread_bind() and put
    the kthread in a non-root cgroup, which can lead to all sorts of
    problems including incorrect CPU affinity and starvation.
    
    This bug got triggered by userland which periodically tries to migrate
    all processes in the root cpuset cgroup to a non-root one.  Per-cpu
    workqueue workers got caught while being created and ended up with
    incorrected CPU affinity breaking concurrency management and sometimes
    stalling workqueue execution.
    
    This patch adds task->no_cgroup_migration which disallows the task to
    be migrated by userland.  kthreadd starts with the flag set making
    every child kthread start in the root cgroup with migration
    disallowed.  The flag is cleared after the kthread finishes
    initialization by which time PF_NO_SETAFFINITY is set if the kthread
    should stay in the root cgroup.
    
    It'd be better to wait for the initialization instead of failing but I
    couldn't think of a way of implementing that without adding either a
    new PF flag, or sleeping and retrying from waiting side.  Even if
    userland depends on changing cgroup membership of a kthread, it either
    has to be synchronized with kthread_create() or periodically repeat,
    so it's unlikely that this would break anything.
    
    v2: Switch to a simpler implementation using a new task_struct bit
        field suggested by Oleg.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reported-and-debugged-by: Chris Mason <clm@fb.com>
    Cc: stable@vger.kernel.org # v4.3+ (we can't close the race on < v4.3)
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..4cf9a59a4d08 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -604,6 +604,10 @@ struct task_struct {
 #ifdef CONFIG_COMPAT_BRK
 	unsigned			brk_randomized:1;
 #endif
+#ifdef CONFIG_CGROUPS
+	/* disallow userland-initiated cgroup migration */
+	unsigned			no_cgroup_migration:1;
+#endif
 
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 

commit 915e70f9263d56fbf103742265025f7a492aa625
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Feb 22 13:01:21 2017 +0100

    staging, android: remove lowmemory killer from the tree
    
    Lowmemory killer is sitting in the staging tree since 2008 without any
    serious interest for fixing issues brought up by the MM folks. The main
    objection is that the implementation is basically broken by design:
            - it hooks into slab shrinker API which is not suitable for this
              purpose. lowmem_count implementation just shows this nicely.
              There is no scaling based on the memory pressure and no
              feedback to the generic shrinker infrastructure.
              Moreover lowmem_scan is called way too often for the heavy
              work it performs.
            - it is not reclaim context aware - no NUMA and/or memcg
              awareness.
    
    As the code stands right now it just adds a maintenance overhead when
    core MM changes have to update lowmemorykiller.c as well. It also seems
    that the alternative LMK implementation will be solely in the userspace
    so this code has no perspective it seems. The staging tree is supposed
    to be for a code which needs to be put in shape before it can be merged
    which is not the case here obviously.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..942c2250301b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1256,7 +1256,6 @@ extern struct pid *cad_pid;
 #define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
 #define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */
 #define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
-#define PFA_LMK_WAITING			3	/* Lowmemorykiller is waiting */
 
 
 #define TASK_PFA_TEST(name, func)					\
@@ -1282,9 +1281,6 @@ TASK_PFA_TEST(SPREAD_SLAB, spread_slab)
 TASK_PFA_SET(SPREAD_SLAB, spread_slab)
 TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 
-TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
-TASK_PFA_SET(LMK_WAITING, lmk_waiting)
-
 static inline void
 tsk_restore_flags(struct task_struct *task, unsigned long orig_flags, unsigned long flags)
 {

commit d83a7cb375eec21f04c83542395d08b2f6641da2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:40 2017 -0600

    livepatch: change to a per-task consistency model
    
    Change livepatch to use a basic per-task consistency model.  This is the
    foundation which will eventually enable us to patch those ~10% of
    security patches which change function or data semantics.  This is the
    biggest remaining piece needed to make livepatch more generally useful.
    
    This code stems from the design proposal made by Vojtech [1] in November
    2014.  It's a hybrid of kGraft and kpatch: it uses kGraft's per-task
    consistency and syscall barrier switching combined with kpatch's stack
    trace switching.  There are also a number of fallback options which make
    it quite flexible.
    
    Patches are applied on a per-task basis, when the task is deemed safe to
    switch over.  When a patch is enabled, livepatch enters into a
    transition state where tasks are converging to the patched state.
    Usually this transition state can complete in a few seconds.  The same
    sequence occurs when a patch is disabled, except the tasks converge from
    the patched state to the unpatched state.
    
    An interrupt handler inherits the patched state of the task it
    interrupts.  The same is true for forked tasks: the child inherits the
    patched state of the parent.
    
    Livepatch uses several complementary approaches to determine when it's
    safe to patch tasks:
    
    1. The first and most effective approach is stack checking of sleeping
       tasks.  If no affected functions are on the stack of a given task,
       the task is patched.  In most cases this will patch most or all of
       the tasks on the first try.  Otherwise it'll keep trying
       periodically.  This option is only available if the architecture has
       reliable stacks (HAVE_RELIABLE_STACKTRACE).
    
    2. The second approach, if needed, is kernel exit switching.  A
       task is switched when it returns to user space from a system call, a
       user space IRQ, or a signal.  It's useful in the following cases:
    
       a) Patching I/O-bound user tasks which are sleeping on an affected
          function.  In this case you have to send SIGSTOP and SIGCONT to
          force it to exit the kernel and be patched.
       b) Patching CPU-bound user tasks.  If the task is highly CPU-bound
          then it will get patched the next time it gets interrupted by an
          IRQ.
       c) In the future it could be useful for applying patches for
          architectures which don't yet have HAVE_RELIABLE_STACKTRACE.  In
          this case you would have to signal most of the tasks on the
          system.  However this isn't supported yet because there's
          currently no way to patch kthreads without
          HAVE_RELIABLE_STACKTRACE.
    
    3. For idle "swapper" tasks, since they don't ever exit the kernel, they
       instead have a klp_update_patch_state() call in the idle loop which
       allows them to be patched before the CPU enters the idle state.
    
       (Note there's not yet such an approach for kthreads.)
    
    All the above approaches may be skipped by setting the 'immediate' flag
    in the 'klp_patch' struct, which will disable per-task consistency and
    patch all tasks immediately.  This can be useful if the patch doesn't
    change any function or data semantics.  Note that, even with this flag
    set, it's possible that some tasks may still be running with an old
    version of the function, until that function returns.
    
    There's also an 'immediate' flag in the 'klp_func' struct which allows
    you to specify that certain functions in the patch can be applied
    without per-task consistency.  This might be useful if you want to patch
    a common function like schedule(), and the function change doesn't need
    consistency but the rest of the patch does.
    
    For architectures which don't have HAVE_RELIABLE_STACKTRACE, the user
    must set patch->immediate which causes all tasks to be patched
    immediately.  This option should be used with care, only when the patch
    doesn't change any function or data semantics.
    
    In the future, architectures which don't have HAVE_RELIABLE_STACKTRACE
    may be allowed to use per-task consistency if we can come up with
    another way to patch kthreads.
    
    The /sys/kernel/livepatch/<patch>/transition file shows whether a patch
    is in transition.  Only a single patch (the topmost patch on the stack)
    can be in transition at a given time.  A patch can remain in transition
    indefinitely, if any of the tasks are stuck in the initial patch state.
    
    A transition can be reversed and effectively canceled by writing the
    opposite value to the /sys/kernel/livepatch/<patch>/enabled file while
    the transition is in progress.  Then all the tasks will attempt to
    converge back to the original patch state.
    
    [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the scheduler changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d67eee84fd43..e11032010318 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1037,6 +1037,9 @@ struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/* A live task holds one reference: */
 	atomic_t			stack_refcount;
+#endif
+#ifdef CONFIG_LIVEPATCH
+	int patch_state;
 #endif
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;

commit 5eca1c10cbaa9c366c18ca79f81f21c731e3dcc7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 22:06:35 2017 +0100

    sched/headers: Clean up <linux/sched.h>
    
    Now that <linux/sched.h> dependencies have been sorted out,
    do various trivial cleanups:
    
     - remove unnecessary structure predeclarations
     - fix various typos
     - update comments where necessary
     - remove pointless comments
     - use consistent types
     - tabulate consistently
     - use a consistent comment style
     - clean up the header section a bit
     - use a consistent style of a single field per line
     - remove line-breaks where they make the code look worse
     - etc ...
    
    No change in functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 341f5792fbe0..d67eee84fd43 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,38 +1,38 @@
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
-#include <uapi/linux/sched.h>
+/*
+ * Define 'struct task_struct' and provide the main scheduler
+ * APIs (schedule(), wakeup variants, etc.)
+ */
 
-#include <linux/sched/prio.h>
-#include <linux/nodemask.h>
+#include <uapi/linux/sched.h>
 
-#include <linux/mutex.h>
-#include <linux/plist.h>
-#include <linux/mm_types_task.h>
+#include <asm/current.h>
 
+#include <linux/pid.h>
 #include <linux/sem.h>
 #include <linux/shm.h>
-#include <linux/signal_types.h>
-#include <linux/pid.h>
+#include <linux/kcov.h>
+#include <linux/mutex.h>
+#include <linux/plist.h>
+#include <linux/hrtimer.h>
 #include <linux/seccomp.h>
+#include <linux/nodemask.h>
 #include <linux/rcupdate.h>
-
 #include <linux/resource.h>
-#include <linux/hrtimer.h>
-#include <linux/kcov.h>
-#include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
+#include <linux/sched/prio.h>
+#include <linux/signal_types.h>
+#include <linux/mm_types_task.h>
+#include <linux/task_io_accounting.h>
 
-#include <asm/current.h>
-
-/* task_struct member predeclarations: */
+/* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
-struct autogroup;
 struct backing_dev_info;
 struct bio_list;
 struct blk_plug;
 struct cfs_rq;
-struct filename;
 struct fs_struct;
 struct futex_pi_state;
 struct io_context;
@@ -52,8 +52,6 @@ struct sighand_struct;
 struct signal_struct;
 struct task_delay_info;
 struct task_group;
-struct task_struct;
-struct uts_namespace;
 
 /*
  * Task state bitmask. NOTE! These bits are also
@@ -65,50 +63,53 @@ struct uts_namespace;
  * modifying one set can't modify the other one by
  * mistake.
  */
-#define TASK_RUNNING		0
-#define TASK_INTERRUPTIBLE	1
-#define TASK_UNINTERRUPTIBLE	2
-#define __TASK_STOPPED		4
-#define __TASK_TRACED		8
-/* in tsk->exit_state */
-#define EXIT_DEAD		16
-#define EXIT_ZOMBIE		32
-#define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)
-/* in tsk->state again */
-#define TASK_DEAD		64
-#define TASK_WAKEKILL		128
-#define TASK_WAKING		256
-#define TASK_PARKED		512
-#define TASK_NOLOAD		1024
-#define TASK_NEW		2048
-#define TASK_STATE_MAX		4096
-
-#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPNn"
-
-/* Convenience macros for the sake of set_current_state */
-#define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
-#define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
-#define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)
-
-#define TASK_IDLE		(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)
-
-/* Convenience macros for the sake of wake_up */
-#define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
-#define TASK_ALL		(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
-
-/* get_task_state() */
-#define TASK_REPORT		(TASK_RUNNING | TASK_INTERRUPTIBLE | \
-				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
-				 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
-
-#define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
-#define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
-#define task_is_stopped_or_traced(task)	\
-			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
-#define task_contributes_to_load(task)	\
-				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
-				 (task->flags & PF_FROZEN) == 0 && \
-				 (task->state & TASK_NOLOAD) == 0)
+
+/* Used in tsk->state: */
+#define TASK_RUNNING			0
+#define TASK_INTERRUPTIBLE		1
+#define TASK_UNINTERRUPTIBLE		2
+#define __TASK_STOPPED			4
+#define __TASK_TRACED			8
+/* Used in tsk->exit_state: */
+#define EXIT_DEAD			16
+#define EXIT_ZOMBIE			32
+#define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
+/* Used in tsk->state again: */
+#define TASK_DEAD			64
+#define TASK_WAKEKILL			128
+#define TASK_WAKING			256
+#define TASK_PARKED			512
+#define TASK_NOLOAD			1024
+#define TASK_NEW			2048
+#define TASK_STATE_MAX			4096
+
+#define TASK_STATE_TO_CHAR_STR		"RSDTtXZxKWPNn"
+
+/* Convenience macros for the sake of set_current_state: */
+#define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
+#define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)
+#define TASK_TRACED			(TASK_WAKEKILL | __TASK_TRACED)
+
+#define TASK_IDLE			(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)
+
+/* Convenience macros for the sake of wake_up(): */
+#define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
+#define TASK_ALL			(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
+
+/* get_task_state(): */
+#define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
+					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
+					 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
+
+#define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
+
+#define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
+
+#define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
+
+#define task_contributes_to_load(task)	((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
+					 (task->flags & PF_FROZEN) == 0 && \
+					 (task->state & TASK_NOLOAD) == 0)
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
@@ -158,26 +159,24 @@ struct uts_namespace;
  *
  * Also see the comments of try_to_wake_up().
  */
-#define __set_current_state(state_value)		\
-	do { current->state = (state_value); } while (0)
-#define set_current_state(state_value)			\
-	smp_store_mb(current->state, (state_value))
-
+#define __set_current_state(state_value) do { current->state = (state_value); } while (0)
+#define set_current_state(state_value)	 smp_store_mb(current->state, (state_value))
 #endif
 
-/* Task command name length */
-#define TASK_COMM_LEN 16
+/* Task command name length: */
+#define TASK_COMM_LEN			16
 
-extern cpumask_var_t cpu_isolated_map;
+extern cpumask_var_t			cpu_isolated_map;
 
 extern void scheduler_tick(void);
 
-#define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
-extern signed long schedule_timeout(signed long timeout);
-extern signed long schedule_timeout_interruptible(signed long timeout);
-extern signed long schedule_timeout_killable(signed long timeout);
-extern signed long schedule_timeout_uninterruptible(signed long timeout);
-extern signed long schedule_timeout_idle(signed long timeout);
+#define	MAX_SCHEDULE_TIMEOUT		LONG_MAX
+
+extern long schedule_timeout(long timeout);
+extern long schedule_timeout_interruptible(long timeout);
+extern long schedule_timeout_killable(long timeout);
+extern long schedule_timeout_uninterruptible(long timeout);
+extern long schedule_timeout_idle(long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
 
@@ -197,9 +196,9 @@ extern void io_schedule(void);
  */
 struct prev_cputime {
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	u64 utime;
-	u64 stime;
-	raw_spinlock_t lock;
+	u64				utime;
+	u64				stime;
+	raw_spinlock_t			lock;
 #endif
 };
 
@@ -214,25 +213,34 @@ struct prev_cputime {
  * these counts together and treat all three of them in parallel.
  */
 struct task_cputime {
-	u64 utime;
-	u64 stime;
-	unsigned long long sum_exec_runtime;
+	u64				utime;
+	u64				stime;
+	unsigned long long		sum_exec_runtime;
 };
 
-/* Alternate field names when used to cache expirations. */
-#define virt_exp	utime
-#define prof_exp	stime
-#define sched_exp	sum_exec_runtime
+/* Alternate field names when used on cache expirations: */
+#define virt_exp			utime
+#define prof_exp			stime
+#define sched_exp			sum_exec_runtime
 
 struct sched_info {
 #ifdef CONFIG_SCHED_INFO
-	/* cumulative counters */
-	unsigned long pcount;	      /* # of times run on this cpu */
-	unsigned long long run_delay; /* time spent waiting on a runqueue */
+	/* Cumulative counters: */
+
+	/* # of times we have run on this CPU: */
+	unsigned long			pcount;
+
+	/* Time spent waiting on a runqueue: */
+	unsigned long long		run_delay;
+
+	/* Timestamps: */
+
+	/* When did we last run on a CPU? */
+	unsigned long long		last_arrival;
+
+	/* When were we last queued to run? */
+	unsigned long long		last_queued;
 
-	/* timestamps */
-	unsigned long long last_arrival,/* when we last ran on a cpu */
-			   last_queued;	/* when we were last queued to run */
 #endif /* CONFIG_SCHED_INFO */
 };
 
@@ -243,12 +251,12 @@ struct sched_info {
  * We define a basic fixed point arithmetic range, and then formalize
  * all these metrics based on that basic range.
  */
-# define SCHED_FIXEDPOINT_SHIFT	10
-# define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
+# define SCHED_FIXEDPOINT_SHIFT		10
+# define SCHED_FIXEDPOINT_SCALE		(1L << SCHED_FIXEDPOINT_SHIFT)
 
 struct load_weight {
-	unsigned long weight;
-	u32 inv_weight;
+	unsigned long			weight;
+	u32				inv_weight;
 };
 
 /*
@@ -304,69 +312,73 @@ struct load_weight {
  * issues.
  */
 struct sched_avg {
-	u64 last_update_time, load_sum;
-	u32 util_sum, period_contrib;
-	unsigned long load_avg, util_avg;
+	u64				last_update_time;
+	u64				load_sum;
+	u32				util_sum;
+	u32				period_contrib;
+	unsigned long			load_avg;
+	unsigned long			util_avg;
 };
 
 struct sched_statistics {
 #ifdef CONFIG_SCHEDSTATS
-	u64			wait_start;
-	u64			wait_max;
-	u64			wait_count;
-	u64			wait_sum;
-	u64			iowait_count;
-	u64			iowait_sum;
-
-	u64			sleep_start;
-	u64			sleep_max;
-	s64			sum_sleep_runtime;
-
-	u64			block_start;
-	u64			block_max;
-	u64			exec_max;
-	u64			slice_max;
-
-	u64			nr_migrations_cold;
-	u64			nr_failed_migrations_affine;
-	u64			nr_failed_migrations_running;
-	u64			nr_failed_migrations_hot;
-	u64			nr_forced_migrations;
-
-	u64			nr_wakeups;
-	u64			nr_wakeups_sync;
-	u64			nr_wakeups_migrate;
-	u64			nr_wakeups_local;
-	u64			nr_wakeups_remote;
-	u64			nr_wakeups_affine;
-	u64			nr_wakeups_affine_attempts;
-	u64			nr_wakeups_passive;
-	u64			nr_wakeups_idle;
+	u64				wait_start;
+	u64				wait_max;
+	u64				wait_count;
+	u64				wait_sum;
+	u64				iowait_count;
+	u64				iowait_sum;
+
+	u64				sleep_start;
+	u64				sleep_max;
+	s64				sum_sleep_runtime;
+
+	u64				block_start;
+	u64				block_max;
+	u64				exec_max;
+	u64				slice_max;
+
+	u64				nr_migrations_cold;
+	u64				nr_failed_migrations_affine;
+	u64				nr_failed_migrations_running;
+	u64				nr_failed_migrations_hot;
+	u64				nr_forced_migrations;
+
+	u64				nr_wakeups;
+	u64				nr_wakeups_sync;
+	u64				nr_wakeups_migrate;
+	u64				nr_wakeups_local;
+	u64				nr_wakeups_remote;
+	u64				nr_wakeups_affine;
+	u64				nr_wakeups_affine_attempts;
+	u64				nr_wakeups_passive;
+	u64				nr_wakeups_idle;
 #endif
 };
 
 struct sched_entity {
-	struct load_weight	load;		/* for load-balancing */
-	struct rb_node		run_node;
-	struct list_head	group_node;
-	unsigned int		on_rq;
+	/* For load-balancing: */
+	struct load_weight		load;
+	struct rb_node			run_node;
+	struct list_head		group_node;
+	unsigned int			on_rq;
 
-	u64			exec_start;
-	u64			sum_exec_runtime;
-	u64			vruntime;
-	u64			prev_sum_exec_runtime;
+	u64				exec_start;
+	u64				sum_exec_runtime;
+	u64				vruntime;
+	u64				prev_sum_exec_runtime;
 
-	u64			nr_migrations;
+	u64				nr_migrations;
 
-	struct sched_statistics statistics;
+	struct sched_statistics		statistics;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	int			depth;
-	struct sched_entity	*parent;
+	int				depth;
+	struct sched_entity		*parent;
 	/* rq on which this entity is (to be) queued: */
-	struct cfs_rq		*cfs_rq;
+	struct cfs_rq			*cfs_rq;
 	/* rq "owned" by this entity/group: */
-	struct cfs_rq		*my_q;
+	struct cfs_rq			*my_q;
 #endif
 
 #ifdef CONFIG_SMP
@@ -376,49 +388,49 @@ struct sched_entity {
 	 * Put into separate cache line so it does not
 	 * collide with read-mostly values above.
 	 */
-	struct sched_avg	avg ____cacheline_aligned_in_smp;
+	struct sched_avg		avg ____cacheline_aligned_in_smp;
 #endif
 };
 
 struct sched_rt_entity {
-	struct list_head run_list;
-	unsigned long timeout;
-	unsigned long watchdog_stamp;
-	unsigned int time_slice;
-	unsigned short on_rq;
-	unsigned short on_list;
-
-	struct sched_rt_entity *back;
+	struct list_head		run_list;
+	unsigned long			timeout;
+	unsigned long			watchdog_stamp;
+	unsigned int			time_slice;
+	unsigned short			on_rq;
+	unsigned short			on_list;
+
+	struct sched_rt_entity		*back;
 #ifdef CONFIG_RT_GROUP_SCHED
-	struct sched_rt_entity	*parent;
+	struct sched_rt_entity		*parent;
 	/* rq on which this entity is (to be) queued: */
-	struct rt_rq		*rt_rq;
+	struct rt_rq			*rt_rq;
 	/* rq "owned" by this entity/group: */
-	struct rt_rq		*my_q;
+	struct rt_rq			*my_q;
 #endif
 };
 
 struct sched_dl_entity {
-	struct rb_node	rb_node;
+	struct rb_node			rb_node;
 
 	/*
 	 * Original scheduling parameters. Copied here from sched_attr
 	 * during sched_setattr(), they will remain the same until
 	 * the next sched_setattr().
 	 */
-	u64 dl_runtime;		/* maximum runtime for each instance	*/
-	u64 dl_deadline;	/* relative deadline of each instance	*/
-	u64 dl_period;		/* separation of two instances (period) */
-	u64 dl_bw;		/* dl_runtime / dl_deadline		*/
+	u64				dl_runtime;	/* Maximum runtime for each instance	*/
+	u64				dl_deadline;	/* Relative deadline of each instance	*/
+	u64				dl_period;	/* Separation of two instances (period) */
+	u64				dl_bw;		/* dl_runtime / dl_deadline		*/
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,
 	 * they are continously updated during task execution. Note that
 	 * the remaining runtime could be < 0 in case we are in overrun.
 	 */
-	s64 runtime;		/* remaining runtime for this instance	*/
-	u64 deadline;		/* absolute deadline for this instance	*/
-	unsigned int flags;	/* specifying the scheduler behaviour	*/
+	s64				runtime;	/* Remaining runtime for this instance	*/
+	u64				deadline;	/* Absolute deadline for this instance	*/
+	unsigned int			flags;		/* Specifying the scheduler behaviour	*/
 
 	/*
 	 * Some bool flags:
@@ -431,24 +443,28 @@ struct sched_dl_entity {
 	 * outside bandwidth enforcement mechanism (but only until we
 	 * exit the critical section);
 	 *
-	 * @dl_yielded tells if task gave up the cpu before consuming
+	 * @dl_yielded tells if task gave up the CPU before consuming
 	 * all its available runtime during the last job.
 	 */
-	int dl_throttled, dl_boosted, dl_yielded;
+	int				dl_throttled;
+	int				dl_boosted;
+	int				dl_yielded;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its
 	 * own bandwidth to be enforced, thus we need one timer per task.
 	 */
-	struct hrtimer dl_timer;
+	struct hrtimer			dl_timer;
 };
 
 union rcu_special {
 	struct {
-		u8 blocked;
-		u8 need_qs;
-		u8 exp_need_qs;
-		u8 pad;	/* Otherwise the compiler can store garbage here. */
+		u8			blocked;
+		u8			need_qs;
+		u8			exp_need_qs;
+
+		/* Otherwise the compiler can store garbage here: */
+		u8			pad;
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };
@@ -470,361 +486,417 @@ struct task_struct {
 	 * For reasons of header soup (see current_thread_info()), this
 	 * must be the first element of task_struct.
 	 */
-	struct thread_info thread_info;
+	struct thread_info		thread_info;
 #endif
-	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
-	void *stack;
-	atomic_t usage;
-	unsigned int flags;	/* per process flags, defined below */
-	unsigned int ptrace;
+	/* -1 unrunnable, 0 runnable, >0 stopped: */
+	volatile long			state;
+	void				*stack;
+	atomic_t			usage;
+	/* Per task flags (PF_*), defined further below: */
+	unsigned int			flags;
+	unsigned int			ptrace;
 
 #ifdef CONFIG_SMP
-	struct llist_node wake_entry;
-	int on_cpu;
+	struct llist_node		wake_entry;
+	int				on_cpu;
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-	unsigned int cpu;	/* current CPU */
+	/* Current CPU: */
+	unsigned int			cpu;
 #endif
-	unsigned int wakee_flips;
-	unsigned long wakee_flip_decay_ts;
-	struct task_struct *last_wakee;
+	unsigned int			wakee_flips;
+	unsigned long			wakee_flip_decay_ts;
+	struct task_struct		*last_wakee;
 
-	int wake_cpu;
+	int				wake_cpu;
 #endif
-	int on_rq;
+	int				on_rq;
+
+	int				prio;
+	int				static_prio;
+	int				normal_prio;
+	unsigned int			rt_priority;
 
-	int prio, static_prio, normal_prio;
-	unsigned int rt_priority;
-	const struct sched_class *sched_class;
-	struct sched_entity se;
-	struct sched_rt_entity rt;
+	const struct sched_class	*sched_class;
+	struct sched_entity		se;
+	struct sched_rt_entity		rt;
 #ifdef CONFIG_CGROUP_SCHED
-	struct task_group *sched_task_group;
+	struct task_group		*sched_task_group;
 #endif
-	struct sched_dl_entity dl;
+	struct sched_dl_entity		dl;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
-	/* list of struct preempt_notifier: */
-	struct hlist_head preempt_notifiers;
+	/* List of struct preempt_notifier: */
+	struct hlist_head		preempt_notifiers;
 #endif
 
 #ifdef CONFIG_BLK_DEV_IO_TRACE
-	unsigned int btrace_seq;
+	unsigned int			btrace_seq;
 #endif
 
-	unsigned int policy;
-	int nr_cpus_allowed;
-	cpumask_t cpus_allowed;
+	unsigned int			policy;
+	int				nr_cpus_allowed;
+	cpumask_t			cpus_allowed;
 
 #ifdef CONFIG_PREEMPT_RCU
-	int rcu_read_lock_nesting;
-	union rcu_special rcu_read_unlock_special;
-	struct list_head rcu_node_entry;
-	struct rcu_node *rcu_blocked_node;
+	int				rcu_read_lock_nesting;
+	union rcu_special		rcu_read_unlock_special;
+	struct list_head		rcu_node_entry;
+	struct rcu_node			*rcu_blocked_node;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
+
 #ifdef CONFIG_TASKS_RCU
-	unsigned long rcu_tasks_nvcsw;
-	bool rcu_tasks_holdout;
-	struct list_head rcu_tasks_holdout_list;
-	int rcu_tasks_idle_cpu;
+	unsigned long			rcu_tasks_nvcsw;
+	bool				rcu_tasks_holdout;
+	struct list_head		rcu_tasks_holdout_list;
+	int				rcu_tasks_idle_cpu;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
-	struct sched_info sched_info;
+	struct sched_info		sched_info;
 
-	struct list_head tasks;
+	struct list_head		tasks;
 #ifdef CONFIG_SMP
-	struct plist_node pushable_tasks;
-	struct rb_node pushable_dl_tasks;
+	struct plist_node		pushable_tasks;
+	struct rb_node			pushable_dl_tasks;
 #endif
 
-	struct mm_struct *mm, *active_mm;
+	struct mm_struct		*mm;
+	struct mm_struct		*active_mm;
 
 	/* Per-thread vma caching: */
-	struct vmacache vmacache;
-
-#if defined(SPLIT_RSS_COUNTING)
-	struct task_rss_stat	rss_stat;
-#endif
-/* task state */
-	int exit_state;
-	int exit_code, exit_signal;
-	int pdeath_signal;  /*  The signal sent when the parent dies  */
-	unsigned long jobctl;	/* JOBCTL_*, siglock protected */
-
-	/* Used for emulating ABI behavior of previous Linux versions */
-	unsigned int personality;
-
-	/* scheduler bits, serialized by scheduler locks */
-	unsigned sched_reset_on_fork:1;
-	unsigned sched_contributes_to_load:1;
-	unsigned sched_migrated:1;
-	unsigned sched_remote_wakeup:1;
-	unsigned :0; /* force alignment to the next boundary */
-
-	/* unserialized, strictly 'current' */
-	unsigned in_execve:1; /* bit to tell LSMs we're in execve */
-	unsigned in_iowait:1;
-#if !defined(TIF_RESTORE_SIGMASK)
-	unsigned restore_sigmask:1;
+	struct vmacache			vmacache;
+
+#ifdef SPLIT_RSS_COUNTING
+	struct task_rss_stat		rss_stat;
+#endif
+	int				exit_state;
+	int				exit_code;
+	int				exit_signal;
+	/* The signal sent when the parent dies: */
+	int				pdeath_signal;
+	/* JOBCTL_*, siglock protected: */
+	unsigned long			jobctl;
+
+	/* Used for emulating ABI behavior of previous Linux versions: */
+	unsigned int			personality;
+
+	/* Scheduler bits, serialized by scheduler locks: */
+	unsigned			sched_reset_on_fork:1;
+	unsigned			sched_contributes_to_load:1;
+	unsigned			sched_migrated:1;
+	unsigned			sched_remote_wakeup:1;
+	/* Force alignment to the next boundary: */
+	unsigned			:0;
+
+	/* Unserialized, strictly 'current' */
+
+	/* Bit to tell LSMs we're in execve(): */
+	unsigned			in_execve:1;
+	unsigned			in_iowait:1;
+#ifndef TIF_RESTORE_SIGMASK
+	unsigned			restore_sigmask:1;
 #endif
 #ifdef CONFIG_MEMCG
-	unsigned memcg_may_oom:1;
+	unsigned			memcg_may_oom:1;
 #ifndef CONFIG_SLOB
-	unsigned memcg_kmem_skip_account:1;
+	unsigned			memcg_kmem_skip_account:1;
 #endif
 #endif
 #ifdef CONFIG_COMPAT_BRK
-	unsigned brk_randomized:1;
+	unsigned			brk_randomized:1;
 #endif
 
-	unsigned long atomic_flags; /* Flags needing atomic access. */
+	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
-	struct restart_block restart_block;
+	struct restart_block		restart_block;
 
-	pid_t pid;
-	pid_t tgid;
+	pid_t				pid;
+	pid_t				tgid;
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-	/* Canary value for the -fstack-protector gcc feature */
-	unsigned long stack_canary;
+	/* Canary value for the -fstack-protector GCC feature: */
+	unsigned long			stack_canary;
 #endif
 	/*
-	 * pointers to (original) parent process, youngest child, younger sibling,
+	 * Pointers to the (original) parent process, youngest child, younger sibling,
 	 * older sibling, respectively.  (p->father can be replaced with
 	 * p->real_parent->pid)
 	 */
-	struct task_struct __rcu *real_parent; /* real parent process */
-	struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
+
+	/* Real parent process: */
+	struct task_struct __rcu	*real_parent;
+
+	/* Recipient of SIGCHLD, wait4() reports: */
+	struct task_struct __rcu	*parent;
+
 	/*
-	 * children/sibling forms the list of my natural children
+	 * Children/sibling form the list of natural children:
 	 */
-	struct list_head children;	/* list of my children */
-	struct list_head sibling;	/* linkage in my parent's children list */
-	struct task_struct *group_leader;	/* threadgroup leader */
+	struct list_head		children;
+	struct list_head		sibling;
+	struct task_struct		*group_leader;
 
 	/*
-	 * ptraced is the list of tasks this task is using ptrace on.
+	 * 'ptraced' is the list of tasks this task is using ptrace() on.
+	 *
 	 * This includes both natural children and PTRACE_ATTACH targets.
-	 * p->ptrace_entry is p's link on the p->parent->ptraced list.
+	 * 'ptrace_entry' is this task's link on the p->parent->ptraced list.
 	 */
-	struct list_head ptraced;
-	struct list_head ptrace_entry;
+	struct list_head		ptraced;
+	struct list_head		ptrace_entry;
 
 	/* PID/PID hash table linkage. */
-	struct pid_link pids[PIDTYPE_MAX];
-	struct list_head thread_group;
-	struct list_head thread_node;
+	struct pid_link			pids[PIDTYPE_MAX];
+	struct list_head		thread_group;
+	struct list_head		thread_node;
+
+	struct completion		*vfork_done;
 
-	struct completion *vfork_done;		/* for vfork() */
-	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
-	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
+	/* CLONE_CHILD_SETTID: */
+	int __user			*set_child_tid;
 
-	u64 utime, stime;
+	/* CLONE_CHILD_CLEARTID: */
+	int __user			*clear_child_tid;
+
+	u64				utime;
+	u64				stime;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
-	u64 utimescaled, stimescaled;
+	u64				utimescaled;
+	u64				stimescaled;
 #endif
-	u64 gtime;
-	struct prev_cputime prev_cputime;
+	u64				gtime;
+	struct prev_cputime		prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqcount_t vtime_seqcount;
-	unsigned long long vtime_snap;
+	seqcount_t			vtime_seqcount;
+	unsigned long long		vtime_snap;
 	enum {
-		/* Task is sleeping or running in a CPU with VTIME inactive */
+		/* Task is sleeping or running in a CPU with VTIME inactive: */
 		VTIME_INACTIVE = 0,
-		/* Task runs in userspace in a CPU with VTIME active */
+		/* Task runs in userspace in a CPU with VTIME active: */
 		VTIME_USER,
-		/* Task runs in kernelspace in a CPU with VTIME active */
+		/* Task runs in kernelspace in a CPU with VTIME active: */
 		VTIME_SYS,
 	} vtime_snap_whence;
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL
-	atomic_t tick_dep_mask;
+	atomic_t			tick_dep_mask;
 #endif
-	unsigned long nvcsw, nivcsw; /* context switch counts */
-	u64 start_time;		/* monotonic time in nsec */
-	u64 real_start_time;	/* boot based time in nsec */
-/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
-	unsigned long min_flt, maj_flt;
+	/* Context switch counts: */
+	unsigned long			nvcsw;
+	unsigned long			nivcsw;
+
+	/* Monotonic time in nsecs: */
+	u64				start_time;
+
+	/* Boot based time in nsecs: */
+	u64				real_start_time;
+
+	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */
+	unsigned long			min_flt;
+	unsigned long			maj_flt;
 
 #ifdef CONFIG_POSIX_TIMERS
-	struct task_cputime cputime_expires;
-	struct list_head cpu_timers[3];
-#endif
-
-/* process credentials */
-	const struct cred __rcu *ptracer_cred; /* Tracer's credentials at attach */
-	const struct cred __rcu *real_cred; /* objective and real subjective task
-					 * credentials (COW) */
-	const struct cred __rcu *cred;	/* effective (overridable) subjective task
-					 * credentials (COW) */
-	char comm[TASK_COMM_LEN]; /* executable name excluding path
-				     - access with [gs]et_task_comm (which lock
-				       it with task_lock())
-				     - initialized normally by setup_new_exec */
-/* file system info */
-	struct nameidata *nameidata;
+	struct task_cputime		cputime_expires;
+	struct list_head		cpu_timers[3];
+#endif
+
+	/* Process credentials: */
+
+	/* Tracer's credentials at attach: */
+	const struct cred __rcu		*ptracer_cred;
+
+	/* Objective and real subjective task credentials (COW): */
+	const struct cred __rcu		*real_cred;
+
+	/* Effective (overridable) subjective task credentials (COW): */
+	const struct cred __rcu		*cred;
+
+	/*
+	 * executable name, excluding path.
+	 *
+	 * - normally initialized setup_new_exec()
+	 * - access it with [gs]et_task_comm()
+	 * - lock it with task_lock()
+	 */
+	char				comm[TASK_COMM_LEN];
+
+	struct nameidata		*nameidata;
+
 #ifdef CONFIG_SYSVIPC
-/* ipc stuff */
-	struct sysv_sem sysvsem;
-	struct sysv_shm sysvshm;
+	struct sysv_sem			sysvsem;
+	struct sysv_shm			sysvshm;
 #endif
 #ifdef CONFIG_DETECT_HUNG_TASK
-/* hung task detection */
-	unsigned long last_switch_count;
-#endif
-/* filesystem information */
-	struct fs_struct *fs;
-/* open file information */
-	struct files_struct *files;
-/* namespaces */
-	struct nsproxy *nsproxy;
-/* signal handlers */
-	struct signal_struct *signal;
-	struct sighand_struct *sighand;
-
-	sigset_t blocked, real_blocked;
-	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
-	struct sigpending pending;
-
-	unsigned long sas_ss_sp;
-	size_t sas_ss_size;
-	unsigned sas_ss_flags;
-
-	struct callback_head *task_works;
-
-	struct audit_context *audit_context;
+	unsigned long			last_switch_count;
+#endif
+	/* Filesystem information: */
+	struct fs_struct		*fs;
+
+	/* Open file information: */
+	struct files_struct		*files;
+
+	/* Namespaces: */
+	struct nsproxy			*nsproxy;
+
+	/* Signal handlers: */
+	struct signal_struct		*signal;
+	struct sighand_struct		*sighand;
+	sigset_t			blocked;
+	sigset_t			real_blocked;
+	/* Restored if set_restore_sigmask() was used: */
+	sigset_t			saved_sigmask;
+	struct sigpending		pending;
+	unsigned long			sas_ss_sp;
+	size_t				sas_ss_size;
+	unsigned int			sas_ss_flags;
+
+	struct callback_head		*task_works;
+
+	struct audit_context		*audit_context;
 #ifdef CONFIG_AUDITSYSCALL
-	kuid_t loginuid;
-	unsigned int sessionid;
+	kuid_t				loginuid;
+	unsigned int			sessionid;
 #endif
-	struct seccomp seccomp;
+	struct seccomp			seccomp;
 
-/* Thread group tracking */
-   	u32 parent_exec_id;
-   	u32 self_exec_id;
-/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
- * mempolicy */
-	spinlock_t alloc_lock;
+	/* Thread group tracking: */
+	u32				parent_exec_id;
+	u32				self_exec_id;
+
+	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
+	spinlock_t			alloc_lock;
 
 	/* Protection of the PI data structures: */
-	raw_spinlock_t pi_lock;
+	raw_spinlock_t			pi_lock;
 
-	struct wake_q_node wake_q;
+	struct wake_q_node		wake_q;
 
 #ifdef CONFIG_RT_MUTEXES
-	/* PI waiters blocked on a rt_mutex held by this task */
-	struct rb_root pi_waiters;
-	struct rb_node *pi_waiters_leftmost;
-	/* Deadlock detection and priority inheritance handling */
-	struct rt_mutex_waiter *pi_blocked_on;
+	/* PI waiters blocked on a rt_mutex held by this task: */
+	struct rb_root			pi_waiters;
+	struct rb_node			*pi_waiters_leftmost;
+	/* Deadlock detection and priority inheritance handling: */
+	struct rt_mutex_waiter		*pi_blocked_on;
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES
-	/* mutex deadlock detection */
-	struct mutex_waiter *blocked_on;
+	/* Mutex deadlock detection: */
+	struct mutex_waiter		*blocked_on;
 #endif
+
 #ifdef CONFIG_TRACE_IRQFLAGS
-	unsigned int irq_events;
-	unsigned long hardirq_enable_ip;
-	unsigned long hardirq_disable_ip;
-	unsigned int hardirq_enable_event;
-	unsigned int hardirq_disable_event;
-	int hardirqs_enabled;
-	int hardirq_context;
-	unsigned long softirq_disable_ip;
-	unsigned long softirq_enable_ip;
-	unsigned int softirq_disable_event;
-	unsigned int softirq_enable_event;
-	int softirqs_enabled;
-	int softirq_context;
+	unsigned int			irq_events;
+	unsigned long			hardirq_enable_ip;
+	unsigned long			hardirq_disable_ip;
+	unsigned int			hardirq_enable_event;
+	unsigned int			hardirq_disable_event;
+	int				hardirqs_enabled;
+	int				hardirq_context;
+	unsigned long			softirq_disable_ip;
+	unsigned long			softirq_enable_ip;
+	unsigned int			softirq_disable_event;
+	unsigned int			softirq_enable_event;
+	int				softirqs_enabled;
+	int				softirq_context;
 #endif
+
 #ifdef CONFIG_LOCKDEP
-# define MAX_LOCK_DEPTH 48UL
-	u64 curr_chain_key;
-	int lockdep_depth;
-	unsigned int lockdep_recursion;
-	struct held_lock held_locks[MAX_LOCK_DEPTH];
-	gfp_t lockdep_reclaim_gfp;
+# define MAX_LOCK_DEPTH			48UL
+	u64				curr_chain_key;
+	int				lockdep_depth;
+	unsigned int			lockdep_recursion;
+	struct held_lock		held_locks[MAX_LOCK_DEPTH];
+	gfp_t				lockdep_reclaim_gfp;
 #endif
+
 #ifdef CONFIG_UBSAN
-	unsigned int in_ubsan;
+	unsigned int			in_ubsan;
 #endif
 
-/* journalling filesystem info */
-	void *journal_info;
+	/* Journalling filesystem info: */
+	void				*journal_info;
 
-/* stacked block device info */
-	struct bio_list *bio_list;
+	/* Stacked block device info: */
+	struct bio_list			*bio_list;
 
 #ifdef CONFIG_BLOCK
-/* stack plugging */
-	struct blk_plug *plug;
+	/* Stack plugging: */
+	struct blk_plug			*plug;
 #endif
 
-/* VM state */
-	struct reclaim_state *reclaim_state;
+	/* VM state: */
+	struct reclaim_state		*reclaim_state;
+
+	struct backing_dev_info		*backing_dev_info;
 
-	struct backing_dev_info *backing_dev_info;
+	struct io_context		*io_context;
 
-	struct io_context *io_context;
+	/* Ptrace state: */
+	unsigned long			ptrace_message;
+	siginfo_t			*last_siginfo;
 
-	unsigned long ptrace_message;
-	siginfo_t *last_siginfo; /* For ptrace use.  */
-	struct task_io_accounting ioac;
-#if defined(CONFIG_TASK_XACCT)
-	u64 acct_rss_mem1;	/* accumulated rss usage */
-	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
-	u64 acct_timexpd;	/* stime + utime since last update */
+	struct task_io_accounting	ioac;
+#ifdef CONFIG_TASK_XACCT
+	/* Accumulated RSS usage: */
+	u64				acct_rss_mem1;
+	/* Accumulated virtual memory usage: */
+	u64				acct_vm_mem1;
+	/* stime + utime since last update: */
+	u64				acct_timexpd;
 #endif
 #ifdef CONFIG_CPUSETS
-	nodemask_t mems_allowed;	/* Protected by alloc_lock */
-	seqcount_t mems_allowed_seq;	/* Seqence no to catch updates */
-	int cpuset_mem_spread_rotor;
-	int cpuset_slab_spread_rotor;
+	/* Protected by ->alloc_lock: */
+	nodemask_t			mems_allowed;
+	/* Seqence number to catch updates: */
+	seqcount_t			mems_allowed_seq;
+	int				cpuset_mem_spread_rotor;
+	int				cpuset_slab_spread_rotor;
 #endif
 #ifdef CONFIG_CGROUPS
-	/* Control Group info protected by css_set_lock */
-	struct css_set __rcu *cgroups;
-	/* cg_list protected by css_set_lock and tsk->alloc_lock */
-	struct list_head cg_list;
+	/* Control Group info protected by css_set_lock: */
+	struct css_set __rcu		*cgroups;
+	/* cg_list protected by css_set_lock and tsk->alloc_lock: */
+	struct list_head		cg_list;
 #endif
 #ifdef CONFIG_INTEL_RDT_A
-	int closid;
+	int				closid;
 #endif
 #ifdef CONFIG_FUTEX
-	struct robust_list_head __user *robust_list;
+	struct robust_list_head __user	*robust_list;
 #ifdef CONFIG_COMPAT
 	struct compat_robust_list_head __user *compat_robust_list;
 #endif
-	struct list_head pi_state_list;
-	struct futex_pi_state *pi_state_cache;
+	struct list_head		pi_state_list;
+	struct futex_pi_state		*pi_state_cache;
 #endif
 #ifdef CONFIG_PERF_EVENTS
-	struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];
-	struct mutex perf_event_mutex;
-	struct list_head perf_event_list;
+	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
+	struct mutex			perf_event_mutex;
+	struct list_head		perf_event_list;
 #endif
 #ifdef CONFIG_DEBUG_PREEMPT
-	unsigned long preempt_disable_ip;
+	unsigned long			preempt_disable_ip;
 #endif
 #ifdef CONFIG_NUMA
-	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
-	short il_next;
-	short pref_node_fork;
+	/* Protected by alloc_lock: */
+	struct mempolicy		*mempolicy;
+	short				il_next;
+	short				pref_node_fork;
 #endif
 #ifdef CONFIG_NUMA_BALANCING
-	int numa_scan_seq;
-	unsigned int numa_scan_period;
-	unsigned int numa_scan_period_max;
-	int numa_preferred_nid;
-	unsigned long numa_migrate_retry;
-	u64 node_stamp;			/* migration stamp  */
-	u64 last_task_numa_placement;
-	u64 last_sum_exec_runtime;
-	struct callback_head numa_work;
-
-	struct list_head numa_entry;
-	struct numa_group *numa_group;
+	int				numa_scan_seq;
+	unsigned int			numa_scan_period;
+	unsigned int			numa_scan_period_max;
+	int				numa_preferred_nid;
+	unsigned long			numa_migrate_retry;
+	/* Migration stamp: */
+	u64				node_stamp;
+	u64				last_task_numa_placement;
+	u64				last_sum_exec_runtime;
+	struct callback_head		numa_work;
+
+	struct list_head		numa_entry;
+	struct numa_group		*numa_group;
 
 	/*
 	 * numa_faults is an array split into four regions:
@@ -840,8 +912,8 @@ struct task_struct {
 	 * during the current scan window. When the scan completes, the counts
 	 * in faults_memory and faults_cpu decay and these values are copied.
 	 */
-	unsigned long *numa_faults;
-	unsigned long total_numa_faults;
+	unsigned long			*numa_faults;
+	unsigned long			total_numa_faults;
 
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last
@@ -849,119 +921,132 @@ struct task_struct {
 	 * period is adapted based on the locality of the faults with different
 	 * weights depending on whether they were shared or private faults
 	 */
-	unsigned long numa_faults_locality[3];
+	unsigned long			numa_faults_locality[3];
 
-	unsigned long numa_pages_migrated;
+	unsigned long			numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
-	struct tlbflush_unmap_batch tlb_ubc;
+	struct tlbflush_unmap_batch	tlb_ubc;
 
-	struct rcu_head rcu;
+	struct rcu_head			rcu;
 
-	/*
-	 * cache last used pipe for splice
-	 */
-	struct pipe_inode_info *splice_pipe;
+	/* Cache last used pipe for splice(): */
+	struct pipe_inode_info		*splice_pipe;
 
-	struct page_frag task_frag;
+	struct page_frag		task_frag;
 
 #ifdef CONFIG_TASK_DELAY_ACCT
 	struct task_delay_info		*delays;
 #endif
 
 #ifdef CONFIG_FAULT_INJECTION
-	int make_it_fail;
+	int				make_it_fail;
 #endif
 	/*
-	 * when (nr_dirtied >= nr_dirtied_pause), it's time to call
-	 * balance_dirty_pages() for some dirty throttling pause
+	 * When (nr_dirtied >= nr_dirtied_pause), it's time to call
+	 * balance_dirty_pages() for a dirty throttling pause:
 	 */
-	int nr_dirtied;
-	int nr_dirtied_pause;
-	unsigned long dirty_paused_when; /* start of a write-and-pause period */
+	int				nr_dirtied;
+	int				nr_dirtied_pause;
+	/* Start of a write-and-pause period: */
+	unsigned long			dirty_paused_when;
 
 #ifdef CONFIG_LATENCYTOP
-	int latency_record_count;
-	struct latency_record latency_record[LT_SAVECOUNT];
+	int				latency_record_count;
+	struct latency_record		latency_record[LT_SAVECOUNT];
 #endif
 	/*
-	 * time slack values; these are used to round up poll() and
+	 * Time slack values; these are used to round up poll() and
 	 * select() etc timeout values. These are in nanoseconds.
 	 */
-	u64 timer_slack_ns;
-	u64 default_timer_slack_ns;
+	u64				timer_slack_ns;
+	u64				default_timer_slack_ns;
 
 #ifdef CONFIG_KASAN
-	unsigned int kasan_depth;
+	unsigned int			kasan_depth;
 #endif
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	/* Index of current stored address in ret_stack */
-	int curr_ret_stack;
-	/* Stack of return addresses for return function tracing */
-	struct ftrace_ret_stack	*ret_stack;
-	/* time stamp for last schedule */
-	unsigned long long ftrace_timestamp;
+	/* Index of current stored address in ret_stack: */
+	int				curr_ret_stack;
+
+	/* Stack of return addresses for return function tracing: */
+	struct ftrace_ret_stack		*ret_stack;
+
+	/* Timestamp for last schedule: */
+	unsigned long long		ftrace_timestamp;
+
 	/*
 	 * Number of functions that haven't been traced
-	 * because of depth overrun.
+	 * because of depth overrun:
 	 */
-	atomic_t trace_overrun;
-	/* Pause for the tracing */
-	atomic_t tracing_graph_pause;
+	atomic_t			trace_overrun;
+
+	/* Pause tracing: */
+	atomic_t			tracing_graph_pause;
 #endif
+
 #ifdef CONFIG_TRACING
-	/* state flags for use by tracers */
-	unsigned long trace;
-	/* bitmask and counter of trace recursion */
-	unsigned long trace_recursion;
+	/* State flags for use by tracers: */
+	unsigned long			trace;
+
+	/* Bitmask and counter of trace recursion: */
+	unsigned long			trace_recursion;
 #endif /* CONFIG_TRACING */
+
 #ifdef CONFIG_KCOV
-	/* Coverage collection mode enabled for this task (0 if disabled). */
-	enum kcov_mode kcov_mode;
-	/* Size of the kcov_area. */
-	unsigned	kcov_size;
-	/* Buffer for coverage collection. */
-	void		*kcov_area;
-	/* kcov desciptor wired with this task or NULL. */
-	struct kcov	*kcov;
+	/* Coverage collection mode enabled for this task (0 if disabled): */
+	enum kcov_mode			kcov_mode;
+
+	/* Size of the kcov_area: */
+	unsigned int			kcov_size;
+
+	/* Buffer for coverage collection: */
+	void				*kcov_area;
+
+	/* KCOV descriptor wired with this task or NULL: */
+	struct kcov			*kcov;
 #endif
+
 #ifdef CONFIG_MEMCG
-	struct mem_cgroup *memcg_in_oom;
-	gfp_t memcg_oom_gfp_mask;
-	int memcg_oom_order;
+	struct mem_cgroup		*memcg_in_oom;
+	gfp_t				memcg_oom_gfp_mask;
+	int				memcg_oom_order;
 
-	/* number of pages to reclaim on returning to userland */
-	unsigned int memcg_nr_pages_over_high;
+	/* Number of pages to reclaim on returning to userland: */
+	unsigned int			memcg_nr_pages_over_high;
 #endif
+
 #ifdef CONFIG_UPROBES
-	struct uprobe_task *utask;
+	struct uprobe_task		*utask;
 #endif
 #if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
-	unsigned int	sequential_io;
-	unsigned int	sequential_io_avg;
+	unsigned int			sequential_io;
+	unsigned int			sequential_io_avg;
 #endif
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
-	unsigned long	task_state_change;
+	unsigned long			task_state_change;
 #endif
-	int pagefault_disabled;
+	int				pagefault_disabled;
 #ifdef CONFIG_MMU
-	struct task_struct *oom_reaper_list;
+	struct task_struct		*oom_reaper_list;
 #endif
 #ifdef CONFIG_VMAP_STACK
-	struct vm_struct *stack_vm_area;
+	struct vm_struct		*stack_vm_area;
 #endif
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-	/* A live task holds one reference. */
-	atomic_t stack_refcount;
+	/* A live task holds one reference: */
+	atomic_t			stack_refcount;
 #endif
-/* CPU-specific state of this task */
-	struct thread_struct thread;
-/*
- * WARNING: on x86, 'thread_struct' contains a variable-sized
- * structure.  It *MUST* be at the end of 'task_struct'.
- *
- * Do not put anything below here!
- */
+	/* CPU-specific state of this task: */
+	struct thread_struct		thread;
+
+	/*
+	 * WARNING: on x86, 'thread_struct' contains a variable-sized
+	 * structure.  It *MUST* be at the end of 'task_struct'.
+	 *
+	 * Do not put anything below here!
+	 */
 };
 
 static inline struct pid *task_pid(struct task_struct *task)
@@ -975,7 +1060,7 @@ static inline struct pid *task_tgid(struct task_struct *task)
 }
 
 /*
- * Without tasklist or rcu lock it is not safe to dereference
+ * Without tasklist or RCU lock it is not safe to dereference
  * the result of task_pgrp/task_session even if task == current,
  * we can race with another thread doing sys_setsid/sys_setpgid.
  */
@@ -1002,16 +1087,14 @@ static inline struct pid *task_session(struct task_struct *task)
  *
  * see also pid_nr() etc in include/linux/pid.h
  */
-pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
-			struct pid_namespace *ns);
+pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);
 
 static inline pid_t task_pid_nr(struct task_struct *tsk)
 {
 	return tsk->pid;
 }
 
-static inline pid_t task_pid_nr_ns(struct task_struct *tsk,
-					struct pid_namespace *ns)
+static inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);
 }
@@ -1027,15 +1110,28 @@ static inline pid_t task_tgid_nr(struct task_struct *tsk)
 	return tsk->tgid;
 }
 
-pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
+extern pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
 
 static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 {
 	return pid_vnr(task_tgid(tsk));
 }
 
+/**
+ * pid_alive - check that a task structure is not stale
+ * @p: Task structure to be checked.
+ *
+ * Test if a process is not yet dead (at most zombie state)
+ * If pid_alive fails, then pointers within the task structure
+ * can be stale and must not be dereferenced.
+ *
+ * Return: 1 if the process is alive. 0 otherwise.
+ */
+static inline int pid_alive(const struct task_struct *p)
+{
+	return p->pids[PIDTYPE_PID].pid != NULL;
+}
 
-static inline int pid_alive(const struct task_struct *p);
 static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
 {
 	pid_t pid = 0;
@@ -1053,8 +1149,7 @@ static inline pid_t task_ppid_nr(const struct task_struct *tsk)
 	return task_ppid_nr_ns(tsk, &init_pid_ns);
 }
 
-static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
-					struct pid_namespace *ns)
+static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
 }
@@ -1065,8 +1160,7 @@ static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_session_nr_ns(struct task_struct *tsk,
-					struct pid_namespace *ns)
+static inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);
 }
@@ -1076,27 +1170,12 @@ static inline pid_t task_session_vnr(struct task_struct *tsk)
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);
 }
 
-/* obsolete, do not use */
+/* Obsolete, do not use: */
 static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {
 	return task_pgrp_nr_ns(tsk, &init_pid_ns);
 }
 
-/**
- * pid_alive - check that a task structure is not stale
- * @p: Task structure to be checked.
- *
- * Test if a process is not yet dead (at most zombie state)
- * If pid_alive fails, then pointers within the task structure
- * can be stale and must not be dereferenced.
- *
- * Return: 1 if the process is alive. 0 otherwise.
- */
-static inline int pid_alive(const struct task_struct *p)
-{
-	return p->pids[PIDTYPE_PID].pid != NULL;
-}
-
 /**
  * is_global_init - check if a task structure is init. Since init
  * is free to have sub-threads we need to check tgid.
@@ -1116,34 +1195,34 @@ extern struct pid *cad_pid;
 /*
  * Per process flags
  */
-#define PF_IDLE		0x00000002	/* I am an IDLE thread */
-#define PF_EXITING	0x00000004	/* getting shut down */
-#define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
-#define PF_VCPU		0x00000010	/* I'm a virtual CPU */
-#define PF_WQ_WORKER	0x00000020	/* I'm a workqueue worker */
-#define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
-#define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */
-#define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
-#define PF_DUMPCORE	0x00000200	/* dumped core */
-#define PF_SIGNALED	0x00000400	/* killed by a signal */
-#define PF_MEMALLOC	0x00000800	/* Allocating memory */
-#define PF_NPROC_EXCEEDED 0x00001000	/* set_user noticed that RLIMIT_NPROC was exceeded */
-#define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
-#define PF_USED_ASYNC	0x00004000	/* used async_schedule*(), used by module init */
-#define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
-#define PF_FROZEN	0x00010000	/* frozen for system suspend */
-#define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
-#define PF_KSWAPD	0x00040000	/* I am kswapd */
-#define PF_MEMALLOC_NOIO 0x00080000	/* Allocating memory without IO involved */
-#define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
-#define PF_KTHREAD	0x00200000	/* I am a kernel thread */
-#define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */
-#define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
-#define PF_NO_SETAFFINITY 0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
-#define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
-#define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
-#define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
-#define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */
+#define PF_IDLE			0x00000002	/* I am an IDLE thread */
+#define PF_EXITING		0x00000004	/* Getting shut down */
+#define PF_EXITPIDONE		0x00000008	/* PI exit done on shut down */
+#define PF_VCPU			0x00000010	/* I'm a virtual CPU */
+#define PF_WQ_WORKER		0x00000020	/* I'm a workqueue worker */
+#define PF_FORKNOEXEC		0x00000040	/* Forked but didn't exec */
+#define PF_MCE_PROCESS		0x00000080      /* Process policy on mce errors */
+#define PF_SUPERPRIV		0x00000100	/* Used super-user privileges */
+#define PF_DUMPCORE		0x00000200	/* Dumped core */
+#define PF_SIGNALED		0x00000400	/* Killed by a signal */
+#define PF_MEMALLOC		0x00000800	/* Allocating memory */
+#define PF_NPROC_EXCEEDED	0x00001000	/* set_user() noticed that RLIMIT_NPROC was exceeded */
+#define PF_USED_MATH		0x00002000	/* If unset the fpu must be initialized before use */
+#define PF_USED_ASYNC		0x00004000	/* Used async_schedule*(), used by module init */
+#define PF_NOFREEZE		0x00008000	/* This thread should not be frozen */
+#define PF_FROZEN		0x00010000	/* Frozen for system suspend */
+#define PF_FSTRANS		0x00020000	/* Inside a filesystem transaction */
+#define PF_KSWAPD		0x00040000	/* I am kswapd */
+#define PF_MEMALLOC_NOIO	0x00080000	/* Allocating memory without IO involved */
+#define PF_LESS_THROTTLE	0x00100000	/* Throttle me less: I clean memory */
+#define PF_KTHREAD		0x00200000	/* I am a kernel thread */
+#define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */
+#define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */
+#define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
+#define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */
+#define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */
+#define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */
+#define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other
@@ -1156,33 +1235,38 @@ extern struct pid *cad_pid;
  * child is not running and in turn not changing child->flags
  * at the same time the parent does it.
  */
-#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)
-#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)
-#define clear_used_math() clear_stopped_child_used_math(current)
-#define set_used_math() set_stopped_child_used_math(current)
+#define clear_stopped_child_used_math(child)	do { (child)->flags &= ~PF_USED_MATH; } while (0)
+#define set_stopped_child_used_math(child)	do { (child)->flags |= PF_USED_MATH; } while (0)
+#define clear_used_math()			clear_stopped_child_used_math(current)
+#define set_used_math()				set_stopped_child_used_math(current)
+
 #define conditional_stopped_child_used_math(condition, child) \
 	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)
-#define conditional_used_math(condition) \
-	conditional_stopped_child_used_math(condition, current)
+
+#define conditional_used_math(condition)	conditional_stopped_child_used_math(condition, current)
+
 #define copy_to_stopped_child_used_math(child) \
 	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)
+
 /* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */
-#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
-#define used_math() tsk_used_math(current)
+#define tsk_used_math(p)			((p)->flags & PF_USED_MATH)
+#define used_math()				tsk_used_math(current)
 
 /* Per-process atomic flags. */
-#define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
-#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
-#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
-#define PFA_LMK_WAITING  3      /* Lowmemorykiller is waiting */
+#define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */
+#define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */
+#define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */
+#define PFA_LMK_WAITING			3	/* Lowmemorykiller is waiting */
 
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
 	{ return test_bit(PFA_##name, &p->atomic_flags); }
+
 #define TASK_PFA_SET(name, func)					\
 	static inline void task_set_##func(struct task_struct *p)	\
 	{ set_bit(PFA_##name, &p->atomic_flags); }
+
 #define TASK_PFA_CLEAR(name, func)					\
 	static inline void task_clear_##func(struct task_struct *p)	\
 	{ clear_bit(PFA_##name, &p->atomic_flags); }
@@ -1201,30 +1285,23 @@ TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
 TASK_PFA_SET(LMK_WAITING, lmk_waiting)
 
-static inline void tsk_restore_flags(struct task_struct *task,
-				unsigned long orig_flags, unsigned long flags)
+static inline void
+tsk_restore_flags(struct task_struct *task, unsigned long orig_flags, unsigned long flags)
 {
 	task->flags &= ~flags;
 	task->flags |= orig_flags & flags;
 }
 
-extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,
-				     const struct cpumask *trial);
-extern int task_can_attach(struct task_struct *p,
-			   const struct cpumask *cs_cpus_allowed);
+extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
+extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);
 #ifdef CONFIG_SMP
-extern void do_set_cpus_allowed(struct task_struct *p,
-			       const struct cpumask *new_mask);
-
-extern int set_cpus_allowed_ptr(struct task_struct *p,
-				const struct cpumask *new_mask);
+extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);
+extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);
 #else
-static inline void do_set_cpus_allowed(struct task_struct *p,
-				      const struct cpumask *new_mask)
+static inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
 }
-static inline int set_cpus_allowed_ptr(struct task_struct *p,
-				       const struct cpumask *new_mask)
+static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 {
 	if (!cpumask_test_cpu(0, new_mask))
 		return -EINVAL;
@@ -1239,6 +1316,7 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
+
 /**
  * task_nice - return the nice value of a given task.
  * @p: the task in question.
@@ -1249,16 +1327,15 @@ static inline int task_nice(const struct task_struct *p)
 {
 	return PRIO_TO_NICE((p)->static_prio);
 }
+
 extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
-extern int sched_setscheduler(struct task_struct *, int,
-			      const struct sched_param *);
-extern int sched_setscheduler_nocheck(struct task_struct *, int,
-				      const struct sched_param *);
-extern int sched_setattr(struct task_struct *,
-			 const struct sched_attr *);
+extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
+extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
+extern int sched_setattr(struct task_struct *, const struct sched_attr *);
 extern struct task_struct *idle_task(int cpu);
+
 /**
  * is_idle_task - is the specified task an idle task?
  * @p: the task in question.
@@ -1269,6 +1346,7 @@ static inline bool is_idle_task(const struct task_struct *p)
 {
 	return !!(p->flags & PF_IDLE);
 }
+
 extern struct task_struct *curr_task(int cpu);
 extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 
@@ -1302,23 +1380,25 @@ static inline struct thread_info *task_thread_info(struct task_struct *task)
  */
 
 extern struct task_struct *find_task_by_vpid(pid_t nr);
-extern struct task_struct *find_task_by_pid_ns(pid_t nr,
-		struct pid_namespace *ns);
+extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
+
 #ifdef CONFIG_SMP
- extern void kick_process(struct task_struct *tsk);
+extern void kick_process(struct task_struct *tsk);
 #else
- static inline void kick_process(struct task_struct *tsk) { }
+static inline void kick_process(struct task_struct *tsk) { }
 #endif
 
 extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
+
 static inline void set_task_comm(struct task_struct *tsk, const char *from)
 {
 	__set_task_comm(tsk, from, false);
 }
+
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
@@ -1326,15 +1406,15 @@ void scheduler_ipi(void);
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
 static inline void scheduler_ipi(void) { }
-static inline unsigned long wait_task_inactive(struct task_struct *p,
-					       long match_state)
+static inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 {
 	return 1;
 }
 #endif
 
-/* set thread flags in other task's structures
- * - see asm/thread_info.h for TIF_xxxx flags available
+/*
+ * Set thread flags in other task's structures.
+ * See asm/thread_info.h for TIF_xxxx flags available:
  */
 static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)
 {

commit 7f5f8e8d97d77edf33f2836259d1f19c6f4d94f5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 11:44:12 2017 +0100

    sched/headers: Remove #ifdefs from <linux/sched.h>
    
    We can remove two pairs of #ifdefs by defining structures in a smarter way.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f9dc9cfaf079..341f5792fbe0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -224,8 +224,8 @@ struct task_cputime {
 #define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
-#ifdef CONFIG_SCHED_INFO
 struct sched_info {
+#ifdef CONFIG_SCHED_INFO
 	/* cumulative counters */
 	unsigned long pcount;	      /* # of times run on this cpu */
 	unsigned long long run_delay; /* time spent waiting on a runqueue */
@@ -233,8 +233,8 @@ struct sched_info {
 	/* timestamps */
 	unsigned long long last_arrival,/* when we last ran on a cpu */
 			   last_queued;	/* when we were last queued to run */
-};
 #endif /* CONFIG_SCHED_INFO */
+};
 
 /*
  * Integer metrics need fixed point arithmetic, e.g., sched/fair
@@ -309,8 +309,8 @@ struct sched_avg {
 	unsigned long load_avg, util_avg;
 };
 
-#ifdef CONFIG_SCHEDSTATS
 struct sched_statistics {
+#ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;
 	u64			wait_count;
@@ -342,8 +342,8 @@ struct sched_statistics {
 	u64			nr_wakeups_affine_attempts;
 	u64			nr_wakeups_passive;
 	u64			nr_wakeups_idle;
-};
 #endif
+};
 
 struct sched_entity {
 	struct load_weight	load;		/* for load-balancing */
@@ -358,9 +358,7 @@ struct sched_entity {
 
 	u64			nr_migrations;
 
-#ifdef CONFIG_SCHEDSTATS
 	struct sched_statistics statistics;
-#endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	int			depth;
@@ -530,9 +528,7 @@ struct task_struct {
 	int rcu_tasks_idle_cpu;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
-#ifdef CONFIG_SCHED_INFO
 	struct sched_info sched_info;
-#endif
 
 	struct list_head tasks;
 #ifdef CONFIG_SMP

commit ee6a3d19f15b9b10075481088b8d4537f286d7b4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 10:01:09 2017 +0100

    sched/headers: Remove the <linux/topology.h> include from <linux/sched.h>
    
    It's used only by a single (rarely used) inline function (task_node(p)),
    which we can move to <linux/sched/topology.h>.
    
    ( Add <linux/nodemask.h>, because we rely on that. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 559be4f1aee5..f9dc9cfaf079 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -4,6 +4,7 @@
 #include <uapi/linux/sched.h>
 
 #include <linux/sched/prio.h>
+#include <linux/nodemask.h>
 
 #include <linux/mutex.h>
 #include <linux/plist.h>
@@ -21,7 +22,6 @@
 #include <linux/kcov.h>
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
-#include <linux/topology.h>
 
 #include <asm/current.h>
 
@@ -1454,11 +1454,6 @@ static inline unsigned int task_cpu(const struct task_struct *p)
 #endif
 }
 
-static inline int task_node(const struct task_struct *p)
-{
-	return cpu_to_node(task_cpu(p));
-}
-
 extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
 
 #else

commit 50ff9d130014f7b19541dbf607a615a72b75b778
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 16:03:58 2017 +0100

    sched/headers: Remove <linux/magic.h> from <linux/sched/task_stack.h>
    
    It's not used by any of the scheduler methods, but <linux/sched/task_stack.h>
    needs it to pick up STACK_END_MAGIC.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bd0111a06aa2..559be4f1aee5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -22,7 +22,6 @@
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
 #include <linux/topology.h>
-#include <linux/magic.h>
 
 #include <asm/current.h>
 

commit 2c873d55cd838deef8218b6d5fe9bd336cb3113a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 12:11:03 2017 +0100

    sched/core: Remove unused prefetch_stack()
    
    prefetch_stack() is defined by IA64, but not actually used anywhere anymore.
    
    Remove it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5720d11f3224..bd0111a06aa2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -247,12 +247,6 @@ struct sched_info {
 # define SCHED_FIXEDPOINT_SHIFT	10
 # define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
 
-#ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
-extern void prefetch_stack(struct task_struct *t);
-#else
-static inline void prefetch_stack(struct task_struct *t) { }
-#endif
-
 struct load_weight {
 	unsigned long weight;
 	u32 inv_weight;

commit b68070e146b9c2b4ece8d869a4fab9a4f14bbfb4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 01:27:20 2017 +0100

    sched/headers: Remove <linux/rculist.h> from <linux/sched.h>
    
    We don't actually need the full rculist.h header anymore, include
    the smaller rcupdate.h header instead.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 057b2a85ecc7..5720d11f3224 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -14,7 +14,7 @@
 #include <linux/signal_types.h>
 #include <linux/pid.h>
 #include <linux/seccomp.h>
-#include <linux/rculist.h>
+#include <linux/rcupdate.h>
 
 #include <linux/resource.h>
 #include <linux/hrtimer.h>

commit 1d1954e038435765a623884b626731784cde768f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 01:16:15 2017 +0100

    sched/headers: Remove the 'init_pid_ns' prototype from <linux/sched.h>
    
    pid.h already defines it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 905d4f148d65..057b2a85ecc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1301,8 +1301,6 @@ static inline struct thread_info *task_thread_info(struct task_struct *task)
 # define task_thread_info(task)	((struct thread_info *)(task)->stack)
 #endif
 
-extern struct pid_namespace init_pid_ns;
-
 /*
  * find a task by one of its numerical ids
  *

commit 192c9414516e7178a44f9ed48d47501c4c19771c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:47:37 2017 +0100

    sched/headers: Remove <linux/signal.h> from <linux/sched.h>
    
    Instead of including the full <linux/signal.h>, only include the types-only
    <linux/signal_types.h> header in <linux/sched.h>, to further decouple the
    scheduler header from the signal headers.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b361f881fe44..905d4f148d65 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -11,7 +11,6 @@
 
 #include <linux/sem.h>
 #include <linux/shm.h>
-#include <linux/signal.h>
 #include <linux/signal_types.h>
 #include <linux/pid.h>
 #include <linux/seccomp.h>

commit cd9c513be34ceaae8bf211474b91b6897574efdd
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:58 2017 +0100

    sched/headers: Remove <linux/rwsem.h> from <linux/sched.h>
    
    This is a stray header that is not needed by anything in sched.h,
    so remove it.
    
    Update files that relied on the stray inclusion.
    
    This reduces the size of the header dependency graph.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ac98255d00fb..b361f881fe44 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -226,8 +226,6 @@ struct task_cputime {
 #define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
-#include <linux/rwsem.h>
-
 #ifdef CONFIG_SCHED_INFO
 struct sched_info {
 	/* cumulative counters */

commit fae3c30c2d1ae3ff93877f64e5d55e2443d8f82f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 12:00:08 2017 +0100

    sched/headers: Remove the runqueue_is_locked() prototype
    
    Remove the runqueue_is_locked() prototype, the function does not exist anymore.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ab80596dddfd..ac98255d00fb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -172,8 +172,6 @@ struct uts_namespace;
 
 extern cpumask_var_t cpu_isolated_map;
 
-extern int runqueue_is_locked(int cpu);
-
 extern void scheduler_tick(void);
 
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX

commit f0a0eb699967f4f51567b563818bafe4017bef73
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 20:56:33 2017 +0100

    sched/headers: Remove the <linux/gfp.h> include from <linux/sched.h>
    
    This reduces sched.h header dependencies.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cc5c99a27e21..ab80596dddfd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -22,7 +22,6 @@
 #include <linux/kcov.h>
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
-#include <linux/gfp.h>
 #include <linux/topology.h>
 #include <linux/magic.h>
 

commit 9c6da18109d603a99915b257929c0370c9d3ae40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:08:30 2017 +0100

    sched/headers: Remove <linux/rtmutex.h> from <linux/sched.h>
    
    This reduces header dependencies.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9dfa9c113570..cc5c99a27e21 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -16,7 +16,6 @@
 #include <linux/pid.h>
 #include <linux/seccomp.h>
 #include <linux/rculist.h>
-#include <linux/rtmutex.h>
 
 #include <linux/resource.h>
 #include <linux/hrtimer.h>

commit f780d89a0e820a529cf91fb78b52565e1b37b774
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:03:42 2017 +0100

    sched/headers: Remove <asm/ptrace.h> from <linux/sched.h>
    
    This reduces header dependencies.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1be69735cef3..9dfa9c113570 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -8,7 +8,6 @@
 #include <linux/mutex.h>
 #include <linux/plist.h>
 #include <linux/mm_types_task.h>
-#include <asm/ptrace.h>
 
 #include <linux/sem.h>
 #include <linux/shm.h>

commit e26512fea5bcd6602dbf02a551ed073cd4529449
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 17:54:15 2017 +0100

    sched/headers: Remove <linux/cred.h> inclusion from <linux/sched.h>
    
    This reduces header dependencies and speeds up the build.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8991e4d7cd0f..1be69735cef3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -24,7 +24,6 @@
 #include <linux/kcov.h>
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
-#include <linux/cred.h>
 #include <linux/gfp.h>
 #include <linux/topology.h>
 #include <linux/magic.h>

commit aa829c7679a13fca667f772db1f0ea03adc29122
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:54:05 2017 +0100

    sched/headers: Remove <linux/cgroup-defs.h> from <linux/sched.h>
    
    It's not used by anything in <linux/sched.h> anymore.
    
    This reduces the preprocessed size of <linux/sched.h> and
    speeds up the build a bit.
    
    Also fix code that implicitly relied on headers included by <linux/cgroup-defs.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 54eefb2ad603..8991e4d7cd0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -28,7 +28,6 @@
 #include <linux/gfp.h>
 #include <linux/topology.h>
 #include <linux/magic.h>
-#include <linux/cgroup-defs.h>
 
 #include <asm/current.h>
 

commit ac4e620967bb72f86371154935aa8b67cf54e225
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:44:31 2017 +0100

    sched/headers: Remove #include <linux/capability.h> from <linux/sched.h>
    
    The <linux/sched.h> header does not actually make use of any
    types or APIs defined in <linux/capability.h>, so remove its inclusion.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43ed34ccb53a..54eefb2ad603 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -5,7 +5,6 @@
 
 #include <linux/sched/prio.h>
 
-#include <linux/capability.h>
 #include <linux/mutex.h>
 #include <linux/plist.h>
 #include <linux/mm_types_task.h>

commit 556725839e543bc2b45bd73121c0b1c1d6c23714
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 10:31:24 2017 +0100

    sched/headers: Remove unused 'task_can_switch_user()' prototype
    
    The function does not exist anymore.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bd89fc17767c..43ed34ccb53a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1509,9 +1509,6 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
-extern int task_can_switch_user(struct user_struct *up,
-					struct task_struct *tsk);
-
 #ifndef TASK_SIZE_OF
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif

commit 6f175fc9536355d8aa5c2d4854848a97c244a031
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 11:12:45 2017 +0100

    sched/headers: Move the sched_exec() prototype to <linux/sched/task.h>
    
    sched_exec() better fits into the task lifetime APIs than into the core scheduler
    APIs.
    
    This reduces the size of <linux/sched.h> a bit.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b68358c60560..bd89fc17767c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1258,13 +1258,6 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 #define cpu_relax_yield() cpu_relax()
 #endif
 
-/* sched_exec is called by processes performing an exec */
-#ifdef CONFIG_SMP
-extern void sched_exec(void);
-#else
-#define sched_exec()   {}
-#endif
-
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);

commit cda66725c1444db67127115d611c982b62b45d8c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:30:50 2017 +0100

    sched/headers: Move the get_task_struct()/put_task_struct() and related APIs from <linux/sched.h> to <linux/sched/task.h>
    
    These belong into the task lifetime API header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8d6b7aa7f3ac..b68358c60560 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1135,20 +1135,6 @@ static inline int is_global_init(struct task_struct *tsk)
 
 extern struct pid *cad_pid;
 
-extern void free_task(struct task_struct *tsk);
-#define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
-
-extern void __put_task_struct(struct task_struct *t);
-
-static inline void put_task_struct(struct task_struct *t)
-{
-	if (atomic_dec_and_test(&t->usage))
-		__put_task_struct(t);
-}
-
-struct task_struct *task_rcu_dereference(struct task_struct **ptask);
-struct task_struct *try_get_task_struct(struct task_struct **ptask);
-
 /*
  * Per process flags
  */

commit dcc2dc45f7cf267d37843059ff75b70260596c69
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:05:49 2017 +0100

    sched/headers, mm: Move 'struct tlbflush_unmap_batch' from <linux/sched.h> to <linux/mm_types_task.h>
    
    Unclutter <linux/sched.h> some more.
    
    Also move the CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH condition inside the
    structure body definition, to remove a pair of #ifdefs from sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc8d94dc140f..8d6b7aa7f3ac 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -484,25 +484,6 @@ struct wake_q_node {
 	struct wake_q_node *next;
 };
 
-/* Track pages that require TLB flushes */
-struct tlbflush_unmap_batch {
-	/*
-	 * Each bit set is a CPU that potentially has a TLB entry for one of
-	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().
-	 */
-	struct cpumask cpumask;
-
-	/* True if any bit in cpumask is set */
-	bool flush_required;
-
-	/*
-	 * If true then the PTE was dirty when unmapped. The entry must be
-	 * flushed before IO is initiated or a stale TLB entry potentially
-	 * allows an update without redirtying the page.
-	 */
-	bool writable;
-};
-
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -895,9 +876,7 @@ struct task_struct {
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
-#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 	struct tlbflush_unmap_batch tlb_ubc;
-#endif
 
 	struct rcu_head rcu;
 

commit 93b5a9a7051e51ce50109046af0235268a261ba0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:53:36 2017 +0100

    sched/headers, timekeeping: Move the timer tick function prototypes to <linux/timekeeping.h>
    
    Move the update_process_times() and xtime_update() prototypes to <linux/timekeeping.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0fb56ae0abc7..dc8d94dc140f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -180,7 +180,6 @@ extern cpumask_var_t cpu_isolated_map;
 
 extern int runqueue_is_locked(int cpu);
 
-extern void update_process_times(int user);
 extern void scheduler_tick(void);
 
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX

commit c5a21921d55a2aee9d1e031a78cef6cda3eab78b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:47:12 2017 +0100

    sched/headers: Move _init() prototypes from <linux/sched.h> to <linux/sched/init.h>
    
    trap_init() and cpu_init() belong into <linux/cpu.h>, sched_init*() into <linux/sched/init.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a03c49e3bcc..0fb56ae0abc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -176,15 +176,10 @@ struct uts_namespace;
 /* Task command name length */
 #define TASK_COMM_LEN 16
 
-extern void sched_init(void);
-extern void sched_init_smp(void);
-
 extern cpumask_var_t cpu_isolated_map;
 
 extern int runqueue_is_locked(int cpu);
 
-extern void cpu_init (void);
-extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 

commit 42011db0ed5a9c92b1281e1300eb3d026f3764a8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:35:41 2017 +0100

    sched/headers: Move exit_files() and exit_itimers() from <linux/sched.h> to <linux/sched/task.h>
    
    These two functions are task management related, not core scheduler APIs.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 309eb76b7eab..4a03c49e3bcc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1386,10 +1386,6 @@ extern void wake_up_new_task(struct task_struct *tsk);
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
 
-extern void exit_files(struct task_struct *);
-
-extern void exit_itimers(struct signal_struct *);
-
 extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
 static inline void set_task_comm(struct task_struct *tsk, const char *from)
 {

commit 9049863a32fad5d7158318e45f0a41a0f2192c0c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:31:22 2017 +0100

    sched/headers: Move kstack_end() from <linux/sched.h> to <linux/sched/task_stack.h>
    
    This is a task-stack related API, not core scheduler functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f2267e41fde..309eb76b7eab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1360,16 +1360,6 @@ static inline struct thread_info *task_thread_info(struct task_struct *task)
 # define task_thread_info(task)	((struct thread_info *)(task)->stack)
 #endif
 
-#ifndef __HAVE_ARCH_KSTACK_END
-static inline int kstack_end(void *addr)
-{
-	/* Reliable end of stack detection:
-	 * Some APM bios versions misalign the stack
-	 */
-	return !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));
-}
-#endif
-
 extern struct pid_namespace init_pid_ns;
 
 /*

commit 28851755990c832d7050d5e1e2c91ed9d9e6fe59
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:24:31 2017 +0100

    sched/headers, vfs/execve: Move the do_execve*() prototypes from <linux/sched.h> to <linux/binfmts.h>
    
    These are not scheduler related.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 707eee099332..5f2267e41fde 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1400,14 +1400,6 @@ extern void exit_files(struct task_struct *);
 
 extern void exit_itimers(struct signal_struct *);
 
-extern int do_execve(struct filename *,
-		     const char __user * const __user *,
-		     const char __user * const __user *);
-extern int do_execveat(int, struct filename *,
-		       const char __user * const __user *,
-		       const char __user * const __user *,
-		       int);
-
 extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
 static inline void set_task_comm(struct task_struct *tsk, const char *from)
 {

commit a2d7a7463c38dd14b845721aac3583a26a97c66d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 12:07:04 2017 +0100

    sched/headers: Move sched_info_on() and force_schedstat_enabled() from <linux/sched.h> to <linux/sched/stat.h>
    
    These APIs are not core scheduler but scheduler statistics related.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2b43f55e4956..707eee099332 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -254,22 +254,6 @@ struct sched_info {
 };
 #endif /* CONFIG_SCHED_INFO */
 
-static inline int sched_info_on(void)
-{
-#ifdef CONFIG_SCHEDSTATS
-	return 1;
-#elif defined(CONFIG_TASK_DELAY_ACCT)
-	extern int delayacct_on;
-	return delayacct_on;
-#else
-	return 0;
-#endif
-}
-
-#ifdef CONFIG_SCHEDSTATS
-void force_schedstat_enabled(void);
-#endif
-
 /*
  * Integer metrics need fixed point arithmetic, e.g., sched/fair
  * has a few: load, load_avg, util_avg, freq, and capacity.

commit 1050b27c52f615bc0e772b3119881e5304ccde6b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Move cputime functionality from <linux/sched.h> and <linux/cputime.h> into <linux/sched/cputime.h>
    
    Move cputime related functionality out of <linux/sched.h>, as most code
    that includes <linux/sched.h> does not use that functionality.
    
    Move data types that are not included in task_struct directly to
    the signal definitions, into <linux/sched/signal.h>.
    
    Also merge the (small) existing <linux/cputime.h> header into <linux/sched/cputime.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d481c129a822..2b43f55e4956 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -219,14 +219,6 @@ struct prev_cputime {
 #endif
 };
 
-static inline void prev_cputime_init(struct prev_cputime *prev)
-{
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	prev->utime = prev->stime = 0;
-	raw_spin_lock_init(&prev->lock);
-#endif
-}
-
 /**
  * struct task_cputime - collected CPU time counts
  * @utime:		time spent in user mode, in nanoseconds
@@ -248,40 +240,6 @@ struct task_cputime {
 #define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
-/*
- * This is the atomic variant of task_cputime, which can be used for
- * storing and updating task_cputime statistics without locking.
- */
-struct task_cputime_atomic {
-	atomic64_t utime;
-	atomic64_t stime;
-	atomic64_t sum_exec_runtime;
-};
-
-#define INIT_CPUTIME_ATOMIC \
-	(struct task_cputime_atomic) {				\
-		.utime = ATOMIC64_INIT(0),			\
-		.stime = ATOMIC64_INIT(0),			\
-		.sum_exec_runtime = ATOMIC64_INIT(0),		\
-	}
-
-/**
- * struct thread_group_cputimer - thread group interval timer counts
- * @cputime_atomic:	atomic thread group interval timers.
- * @running:		true when there are timers running and
- *			@cputime_atomic receives updates.
- * @checking_timer:	true when a thread in the group is in the
- *			process of checking for thread group timers.
- *
- * This structure contains the version of task_cputime, above, that is
- * used for thread group CPU timer calculations.
- */
-struct thread_group_cputimer {
-	struct task_cputime_atomic cputime_atomic;
-	bool running;
-	bool checking_timer;
-};
-
 #include <linux/rwsem.h>
 
 #ifdef CONFIG_SCHED_INFO
@@ -1234,44 +1192,6 @@ static inline void put_task_struct(struct task_struct *t)
 struct task_struct *task_rcu_dereference(struct task_struct **ptask);
 struct task_struct *try_get_task_struct(struct task_struct **ptask);
 
-#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-extern void task_cputime(struct task_struct *t,
-			 u64 *utime, u64 *stime);
-extern u64 task_gtime(struct task_struct *t);
-#else
-static inline void task_cputime(struct task_struct *t,
-				u64 *utime, u64 *stime)
-{
-	*utime = t->utime;
-	*stime = t->stime;
-}
-
-static inline u64 task_gtime(struct task_struct *t)
-{
-	return t->gtime;
-}
-#endif
-
-#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
-static inline void task_cputime_scaled(struct task_struct *t,
-				       u64 *utimescaled,
-				       u64 *stimescaled)
-{
-	*utimescaled = t->utimescaled;
-	*stimescaled = t->stimescaled;
-}
-#else
-static inline void task_cputime_scaled(struct task_struct *t,
-				       u64 *utimescaled,
-				       u64 *stimescaled)
-{
-	task_cputime(t, utimescaled, stimescaled);
-}
-#endif
-
-extern void task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
-extern void thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
-
 /*
  * Per process flags
  */
@@ -1395,9 +1315,6 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 #define cpu_relax_yield() cpu_relax()
 #endif
 
-extern unsigned long long
-task_sched_runtime(struct task_struct *task);
-
 /* sched_exec is called by processes performing an exec */
 #ifdef CONFIG_SMP
 extern void sched_exec(void);
@@ -1629,12 +1546,6 @@ static __always_inline bool need_resched(void)
 	return unlikely(tif_need_resched());
 }
 
-/*
- * Thread group CPU time accounting.
- */
-void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
-void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
-
 /*
  * Wrappers for p->thread_info->cpu access. No-op on UP.
  */

commit 56cd697366b6d6f67acb6c58ac7f3b185d11ef07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 10:57:33 2017 +0100

    sched/headers: Move the task_lock()/unlock() APIs to <linux/sched/task.h>
    
    The task_lock()/task_unlock() APIs are not realated to core scheduling,
    they are task lifetime APIs, i.e. they belong into <linux/sched/task.h>.
    
    Move them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ab105a2a8f9..d481c129a822 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1526,26 +1526,6 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 }
 #endif
 
-/*
- * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
- * subscriptions and synchronises with wait4().  Also used in procfs.  Also
- * pins the final release of task.io_context.  Also protects ->cpuset and
- * ->cgroup.subsys[]. And ->vfork_done.
- *
- * Nests both inside and outside of read_lock(&tasklist_lock).
- * It must not be nested with write_lock_irq(&tasklist_lock),
- * neither inside nor outside.
- */
-static inline void task_lock(struct task_struct *p)
-{
-	spin_lock(&p->alloc_lock);
-}
-
-static inline void task_unlock(struct task_struct *p)
-{
-	spin_unlock(&p->alloc_lock);
-}
-
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit cdc75e9f7b14f29efcf4b162a3c673733e96db79
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 01:20:53 2017 +0100

    sched/headers: Move 'init_task' and 'init_thread_union' from <linux/sched.h> to <linux/sched/task.h>
    
    'init_task' is really not part of core scheduler APIs but part of
    the fork() interface between the scheduler and process management.
    
    So move the declarations.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3eb284741790..4ab105a2a8f9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1469,9 +1469,6 @@ static inline int kstack_end(void *addr)
 }
 #endif
 
-extern union thread_union init_thread_union;
-extern struct task_struct init_task;
-
 extern struct pid_namespace init_pid_ns;
 
 /*

commit 77ba809e8b39b4e384df0433e2bd3dd0907dad29
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Use the freshly introduced, reduced size <linux/mm_types_task.h> header instead.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4934733bd6cb..3eb284741790 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -8,7 +8,7 @@
 #include <linux/capability.h>
 #include <linux/mutex.h>
 #include <linux/plist.h>
-#include <linux/mm_types.h>
+#include <linux/mm_types_task.h>
 #include <asm/ptrace.h>
 
 #include <linux/sem.h>

commit 70806b21e4d64f01193a2b64a053b75ff53a2b10
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:15:21 2017 +0100

    sched/headers: Move the 'root_task_group' declaration to <linux/sched/autogroup.h>
    
    Also remove the duplicate declaration from <linux/init_task.h>.
    
    ( That declaration was originally duplicated for dependency hell reasons,
      but there's no problem including the much smaller <linux/sched/autogroup.h>
      header now, to pick up the right prototype. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af9590c8bfb0..4934733bd6cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1707,10 +1707,6 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
-#ifdef CONFIG_CGROUP_SCHED
-extern struct task_group root_task_group;
-#endif /* CONFIG_CGROUP_SCHED */
-
 extern int task_can_switch_user(struct user_struct *up,
 					struct task_struct *tsk);
 

commit f3ac60671954c8d413532627b1be13a76f394c49
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 22:59:33 2017 +0100

    sched/headers: Move task-stack related APIs from <linux/sched.h> to <linux/sched/task_stack.h>
    
    Split out the task->stack related functionality, which is not really
    part of the core scheduler APIs.
    
    Only keep task_thread_info() because it's used by sched.h.
    
    Update the code that uses those facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b3e31da416e..af9590c8bfb0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1450,6 +1450,15 @@ union thread_union {
 	unsigned long stack[THREAD_SIZE/sizeof(long)];
 };
 
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+static inline struct thread_info *task_thread_info(struct task_struct *task)
+{
+	return &task->thread_info;
+}
+#elif !defined(__HAVE_THREAD_FUNCTIONS)
+# define task_thread_info(task)	((struct thread_info *)(task)->stack)
+#endif
+
 #ifndef __HAVE_ARCH_KSTACK_END
 static inline int kstack_end(void *addr)
 {
@@ -1540,112 +1549,6 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
-#ifdef CONFIG_THREAD_INFO_IN_TASK
-
-static inline struct thread_info *task_thread_info(struct task_struct *task)
-{
-	return &task->thread_info;
-}
-
-/*
- * When accessing the stack of a non-current task that might exit, use
- * try_get_task_stack() instead.  task_stack_page will return a pointer
- * that could get freed out from under you.
- */
-static inline void *task_stack_page(const struct task_struct *task)
-{
-	return task->stack;
-}
-
-#define setup_thread_stack(new,old)	do { } while(0)
-
-static inline unsigned long *end_of_stack(const struct task_struct *task)
-{
-	return task->stack;
-}
-
-#elif !defined(__HAVE_THREAD_FUNCTIONS)
-
-#define task_thread_info(task)	((struct thread_info *)(task)->stack)
-#define task_stack_page(task)	((void *)(task)->stack)
-
-static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
-{
-	*task_thread_info(p) = *task_thread_info(org);
-	task_thread_info(p)->task = p;
-}
-
-/*
- * Return the address of the last usable long on the stack.
- *
- * When the stack grows down, this is just above the thread
- * info struct. Going any lower will corrupt the threadinfo.
- *
- * When the stack grows up, this is the highest address.
- * Beyond that position, we corrupt data on the next page.
- */
-static inline unsigned long *end_of_stack(struct task_struct *p)
-{
-#ifdef CONFIG_STACK_GROWSUP
-	return (unsigned long *)((unsigned long)task_thread_info(p) + THREAD_SIZE) - 1;
-#else
-	return (unsigned long *)(task_thread_info(p) + 1);
-#endif
-}
-
-#endif
-
-#ifdef CONFIG_THREAD_INFO_IN_TASK
-static inline void *try_get_task_stack(struct task_struct *tsk)
-{
-	return atomic_inc_not_zero(&tsk->stack_refcount) ?
-		task_stack_page(tsk) : NULL;
-}
-
-extern void put_task_stack(struct task_struct *tsk);
-#else
-static inline void *try_get_task_stack(struct task_struct *tsk)
-{
-	return task_stack_page(tsk);
-}
-
-static inline void put_task_stack(struct task_struct *tsk) {}
-#endif
-
-#define task_stack_end_corrupted(task) \
-		(*(end_of_stack(task)) != STACK_END_MAGIC)
-
-static inline int object_is_on_stack(void *obj)
-{
-	void *stack = task_stack_page(current);
-
-	return (obj >= stack) && (obj < (stack + THREAD_SIZE));
-}
-
-extern void thread_stack_cache_init(void);
-
-#ifdef CONFIG_DEBUG_STACK_USAGE
-static inline unsigned long stack_not_used(struct task_struct *p)
-{
-	unsigned long *n = end_of_stack(p);
-
-	do { 	/* Skip over canary */
-# ifdef CONFIG_STACK_GROWSUP
-		n--;
-# else
-		n++;
-# endif
-	} while (!*n);
-
-# ifdef CONFIG_STACK_GROWSUP
-	return (unsigned long)end_of_stack(p) - (unsigned long)n;
-# else
-	return (unsigned long)n - (unsigned long)end_of_stack(p);
-# endif
-}
-#endif
-extern void set_task_stack_end_magic(struct task_struct *tsk);
-
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit d04b0ad37e4b6ac39a56c823ae76ab37cd044dc7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 22:07:57 2017 +0100

    sched/headers: Move the PREEMPT_COUNT defines from <linux/sched.h> to <linux/preempt.h>
    
    These defines are not really part of the scheduler's driver API, but are
    related to the preempt count - so move them to <linux/preempt.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5398356e33c7..3b3e31da416e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -265,27 +265,6 @@ struct task_cputime_atomic {
 		.sum_exec_runtime = ATOMIC64_INIT(0),		\
 	}
 
-#define PREEMPT_DISABLED	(PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)
-
-/*
- * Disable preemption until the scheduler is running -- use an unconditional
- * value so that it also works on !PREEMPT_COUNT kernels.
- *
- * Reset by start_kernel()->sched_init()->init_idle()->init_idle_preempt_count().
- */
-#define INIT_PREEMPT_COUNT	PREEMPT_OFFSET
-
-/*
- * Initial preempt_count value; reflects the preempt_count schedule invariant
- * which states that during context switches:
- *
- *    preempt_count() == 2*PREEMPT_DISABLE_OFFSET
- *
- * Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.
- * Note: See finish_task_switch().
- */
-#define FORK_PREEMPT_COUNT	(2*PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)
-
 /**
  * struct thread_group_cputimer - thread group interval timer counts
  * @cputime_atomic:	atomic thread group interval timers.

commit c7af7877eeacfeaaf6a1b6f54c481292ef116837
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 22:01:58 2017 +0100

    sched/core: Move, sort and clean up <linux/sched.h> structure predeclarations
    
    Most of the structure predeclarations were at the head of sched.h, but not
    all of them - there were a number of lines spread around sched.h, in
    random places.
    
    Move them to the head, and also sort them alphabetically.
    
    Remove unused entries.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b1677c8db03f..5398356e33c7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -33,24 +33,35 @@
 
 #include <asm/current.h>
 
-struct sched_attr;
-struct sched_param;
-
-struct futex_pi_state;
-struct robust_list_head;
+/* task_struct member predeclarations: */
+struct audit_context;
+struct autogroup;
+struct backing_dev_info;
 struct bio_list;
-struct fs_struct;
-struct perf_event_context;
 struct blk_plug;
+struct cfs_rq;
 struct filename;
+struct fs_struct;
+struct futex_pi_state;
+struct io_context;
+struct mempolicy;
 struct nameidata;
-
-struct signal_struct;
-struct sighand_struct;
-
+struct nsproxy;
+struct perf_event_context;
+struct pid_namespace;
+struct pipe_inode_info;
+struct rcu_node;
+struct reclaim_state;
+struct robust_list_head;
+struct sched_attr;
+struct sched_param;
 struct seq_file;
-struct cfs_rq;
+struct sighand_struct;
+struct signal_struct;
+struct task_delay_info;
 struct task_group;
+struct task_struct;
+struct uts_namespace;
 
 /*
  * Task state bitmask. NOTE! These bits are also
@@ -165,8 +176,6 @@ struct task_group;
 /* Task command name length */
 #define TASK_COMM_LEN 16
 
-struct task_struct;
-
 extern void sched_init(void);
 extern void sched_init_smp(void);
 
@@ -193,8 +202,6 @@ extern void io_schedule_finish(int token);
 extern long io_schedule_timeout(long timeout);
 extern void io_schedule(void);
 
-struct nsproxy;
-
 /**
  * struct prev_cputime - snaphsot of system and user cputime
  * @utime: time spent in user mode
@@ -297,10 +304,6 @@ struct thread_group_cputimer {
 };
 
 #include <linux/rwsem.h>
-struct autogroup;
-
-struct backing_dev_info;
-struct reclaim_state;
 
 #ifdef CONFIG_SCHED_INFO
 struct sched_info {
@@ -314,8 +317,6 @@ struct sched_info {
 };
 #endif /* CONFIG_SCHED_INFO */
 
-struct task_delay_info;
-
 static inline int sched_info_on(void)
 {
 #ifdef CONFIG_SCHEDSTATS
@@ -342,20 +343,12 @@ void force_schedstat_enabled(void);
 # define SCHED_FIXEDPOINT_SHIFT	10
 # define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
 
-struct io_context;			/* See blkdev.h */
-
-
 #ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
 extern void prefetch_stack(struct task_struct *t);
 #else
 static inline void prefetch_stack(struct task_struct *t) { }
 #endif
 
-struct audit_context;		/* See audit.c */
-struct mempolicy;
-struct pipe_inode_info;
-struct uts_namespace;
-
 struct load_weight {
 	unsigned long weight;
 	u32 inv_weight;
@@ -564,7 +557,6 @@ union rcu_special {
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };
-struct rcu_node;
 
 enum perf_event_task_context {
 	perf_invalid_context = -1,
@@ -1125,8 +1117,6 @@ static inline struct pid *task_session(struct task_struct *task)
 	return task->group_leader->pids[PIDTYPE_SID].pid;
 }
 
-struct pid_namespace;
-
 /*
  * the helpers to get the task's different pids as they are seen
  * from various namespaces

commit 901b14bd946a8b7ea211105b6207e082ddd36846
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 15:24:12 2017 +0100

    sched/headers: Move task lifetime APIs from <linux/sched.h> to <linux/sched/task.h>
    
    There's a fair amount of task lifetime management (a.k.a fork()/exit())
    related APIs in <linux/sched.h>, but only a small fraction of
    the users of the generic sched.h header make use of them.
    
    Move these functions to the <linux/sched/task.h> header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 25cc0adb3e08..b1677c8db03f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -165,28 +165,10 @@ struct task_group;
 /* Task command name length */
 #define TASK_COMM_LEN 16
 
-#include <linux/spinlock.h>
-
-/*
- * This serializes "schedule()" and also protects
- * the run-queue from deletions/modifications (but
- * _adding_ to the beginning of the run-queue has
- * a separate lock).
- */
-extern rwlock_t tasklist_lock;
-extern spinlock_t mmlist_lock;
-
 struct task_struct;
 
-#ifdef CONFIG_PROVE_RCU
-extern int lockdep_tasklist_lock_is_held(void);
-#endif /* #ifdef CONFIG_PROVE_RCU */
-
 extern void sched_init(void);
 extern void sched_init_smp(void);
-extern asmlinkage void schedule_tail(struct task_struct *prev);
-extern void init_idle(struct task_struct *idle, int cpu);
-extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern cpumask_var_t cpu_isolated_map;
 
@@ -211,8 +193,6 @@ extern void io_schedule_finish(int token);
 extern long io_schedule_timeout(long timeout);
 extern void io_schedule(void);
 
-void __noreturn do_task_dead(void);
-
 struct nsproxy;
 
 /**
@@ -1120,24 +1100,6 @@ struct task_struct {
  */
 };
 
-#ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT
-extern int arch_task_struct_size __read_mostly;
-#else
-# define arch_task_struct_size (sizeof(struct task_struct))
-#endif
-
-#ifdef CONFIG_VMAP_STACK
-static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
-{
-	return t->stack_vm_area;
-}
-#else
-static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
-{
-	return NULL;
-}
-#endif
-
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -1429,21 +1391,6 @@ TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
 TASK_PFA_SET(LMK_WAITING, lmk_waiting)
 
-static inline void rcu_copy_process(struct task_struct *p)
-{
-#ifdef CONFIG_PREEMPT_RCU
-	p->rcu_read_lock_nesting = 0;
-	p->rcu_read_unlock_special.s = 0;
-	p->rcu_blocked_node = NULL;
-	INIT_LIST_HEAD(&p->rcu_node_entry);
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
-#ifdef CONFIG_TASKS_RCU
-	p->rcu_tasks_holdout = false;
-	INIT_LIST_HEAD(&p->rcu_tasks_holdout_list);
-	p->rcu_tasks_idle_cpu = -1;
-#endif /* #ifdef CONFIG_TASKS_RCU */
-}
-
 static inline void tsk_restore_flags(struct task_struct *task,
 				unsigned long orig_flags, unsigned long flags)
 {
@@ -1572,45 +1519,11 @@ extern void wake_up_new_task(struct task_struct *tsk);
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
-extern void sched_dead(struct task_struct *p);
-
-extern void proc_caches_init(void);
-
-extern void release_task(struct task_struct * p);
-
-#ifdef CONFIG_HAVE_COPY_THREAD_TLS
-extern int copy_thread_tls(unsigned long, unsigned long, unsigned long,
-			struct task_struct *, unsigned long);
-#else
-extern int copy_thread(unsigned long, unsigned long, unsigned long,
-			struct task_struct *);
-
-/* Architectures that haven't opted into copy_thread_tls get the tls argument
- * via pt_regs, so ignore the tls argument passed via C. */
-static inline int copy_thread_tls(
-		unsigned long clone_flags, unsigned long sp, unsigned long arg,
-		struct task_struct *p, unsigned long tls)
-{
-	return copy_thread(clone_flags, sp, arg, p);
-}
-#endif
-extern void flush_thread(void);
-
-#ifdef CONFIG_HAVE_EXIT_THREAD
-extern void exit_thread(struct task_struct *tsk);
-#else
-static inline void exit_thread(struct task_struct *tsk)
-{
-}
-#endif
 
 extern void exit_files(struct task_struct *);
 
 extern void exit_itimers(struct signal_struct *);
 
-extern void do_group_exit(int);
-
 extern int do_execve(struct filename *,
 		     const char __user * const __user *,
 		     const char __user * const __user *);
@@ -1618,10 +1531,6 @@ extern int do_execveat(int, struct filename *,
 		       const char __user * const __user *,
 		       const char __user * const __user *,
 		       int);
-extern long _do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *, unsigned long);
-extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
-struct task_struct *fork_idle(int);
-extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 
 extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
 static inline void set_task_comm(struct task_struct *tsk, const char *from)

commit 0ca0156973a47e689f3bc817e26e15fff3f84eec
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 14:52:01 2017 +0100

    sched/headers: Split hotplug CPU interfaces out of <linux/sched.h> into <linux/sched/hotplug.h>
    
    Split the CPU hotplug scheduler APIs out of the common header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ac0fed4c3130..25cc0adb3e08 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -196,15 +196,6 @@ extern void cpu_init (void);
 extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
-extern int sched_cpu_starting(unsigned int cpu);
-extern int sched_cpu_activate(unsigned int cpu);
-extern int sched_cpu_deactivate(unsigned int cpu);
-
-#ifdef CONFIG_HOTPLUG_CPU
-extern int sched_cpu_dying(unsigned int cpu);
-#else
-# define sched_cpu_dying	NULL
-#endif
 
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long schedule_timeout(signed long timeout);
@@ -1498,12 +1489,6 @@ extern void sched_exec(void);
 #define sched_exec()   {}
 #endif
 
-#ifdef CONFIG_HOTPLUG_CPU
-extern void idle_task_exit(void);
-#else
-static inline void idle_task_exit(void) {}
-#endif
-
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);

commit 70b8157e61d0143fb44ae9482557d7aca365da3d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 12:11:00 2017 +0100

    sched/headers: Move <asm/current.h> include from the middle of <linux/sched.h> to the header portion
    
    Linux-0.01 already defined 'current' in the middle of sched.h, so this
    is an ancient historical precedent - but still in a modern kernel it
    looks a bit weird that we have:
    
            #include <asm/current.h>
    
    in the middle of the header.
    
    Move it further up. If this was done for some obscure dependency
    reasons then we'll trigger and document it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a4fbc76feb4..ac0fed4c3130 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -31,6 +31,8 @@
 #include <linux/magic.h>
 #include <linux/cgroup-defs.h>
 
+#include <asm/current.h>
+
 struct sched_attr;
 struct sched_param;
 
@@ -1577,8 +1579,6 @@ extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);
 
-#include <asm/current.h>
-
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);

commit 63cc9d6fca8c220c2406f1376100a9acb55197af
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 12:04:39 2017 +0100

    sched/headers, time/timekeeping: Move the xtime_update() prototype from <linux/sched.h> to <linux/time.h>
    
    This was in <linux/sched.h> only for hysterical raisins.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 76a2e522be29..5a4fbc76feb4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1579,8 +1579,6 @@ extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 
 #include <asm/current.h>
 
-extern void xtime_update(unsigned long ticks);
-
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);

commit d3d1e320d43a7bad9603acf0214406a1e8795c63
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 11:16:09 2017 +0100

    sched/headers: Move debugging functions from <linux/sched.h> to <linux/sched/debug.h>
    
    Collect the various scheduler and task state debugging APIs scattered
    around <linux/sched.h> into the new <linux/sched/debug.h> header.
    
    In particular the show_regs() and show_stack() prototype affects many files,
    update them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f9bfc603d19..76a2e522be29 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -46,15 +46,9 @@ struct nameidata;
 struct signal_struct;
 struct sighand_struct;
 
-extern void dump_cpu_task(int cpu);
-
 struct seq_file;
 struct cfs_rq;
 struct task_group;
-#ifdef CONFIG_SCHED_DEBUG
-extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
-extern void proc_sched_set_task(struct task_struct *p);
-#endif
 
 /*
  * Task state bitmask. NOTE! These bits are also
@@ -196,25 +190,6 @@ extern cpumask_var_t cpu_isolated_map;
 
 extern int runqueue_is_locked(int cpu);
 
-/*
- * Only dump TASK_* tasks. (0 for all tasks)
- */
-extern void show_state_filter(unsigned long state_filter);
-
-static inline void show_state(void)
-{
-	show_state_filter(0);
-}
-
-extern void show_regs(struct pt_regs *);
-
-/*
- * TASK is a pointer to the task whose backtrace we want to see (or NULL for current
- * task), SP is the stack pointer of the first frame that should be shown in the back
- * trace (or NULL if the entire call-chain of the task should be shown).
- */
-extern void show_stack(struct task_struct *task, unsigned long *sp);
-
 extern void cpu_init (void);
 extern void trap_init(void);
 extern void update_process_times(int user);
@@ -229,17 +204,6 @@ extern int sched_cpu_dying(unsigned int cpu);
 # define sched_cpu_dying	NULL
 #endif
 
-extern void sched_show_task(struct task_struct *p);
-
-/* Attach to any functions which should be ignored in wchan output. */
-#define __sched		__attribute__((__section__(".sched.text")))
-
-/* Linker adds these: start and end of __sched functions */
-extern char __sched_text_start[], __sched_text_end[];
-
-/* Is this address in the __sched functions? */
-extern int in_sched_functions(unsigned long addr);
-
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long schedule_timeout(signed long timeout);
 extern signed long schedule_timeout_interruptible(signed long timeout);

commit 752b3ca734cbc17c0456b846ae886c0ed39c5703
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 11:12:59 2017 +0100

    sched/headers: Move the NOHZ APIs from <linux/sched.h> to <linux/sched/nohz.h>
    
    There's a number of NOHZ/dyntics related functionality in <linux/sched.h>,
    but only a handful of timer files are making use of them.
    
    Move them into their own header. This better documents these APIs
    and unclutters <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 415baf253517..5f9bfc603d19 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -46,14 +46,6 @@ struct nameidata;
 struct signal_struct;
 struct sighand_struct;
 
-#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
-extern void cpu_load_update_nohz_start(void);
-extern void cpu_load_update_nohz_stop(void);
-#else
-static inline void cpu_load_update_nohz_start(void) { }
-static inline void cpu_load_update_nohz_stop(void) { }
-#endif
-
 extern void dump_cpu_task(int cpu);
 
 struct seq_file;
@@ -204,15 +196,6 @@ extern cpumask_var_t cpu_isolated_map;
 
 extern int runqueue_is_locked(int cpu);
 
-#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
-extern void nohz_balance_enter_idle(int cpu);
-extern void set_cpu_sd_state_idle(void);
-extern int get_nohz_timer_target(void);
-#else
-static inline void nohz_balance_enter_idle(int cpu) { }
-static inline void set_cpu_sd_state_idle(void) { }
-#endif
-
 /*
  * Only dump TASK_* tasks. (0 for all tasks)
  */
@@ -1535,14 +1518,6 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 }
 #endif
 
-#ifdef CONFIG_NO_HZ_COMMON
-void calc_load_enter_idle(void);
-void calc_load_exit_idle(void);
-#else
-static inline void calc_load_enter_idle(void) { }
-static inline void calc_load_exit_idle(void) { }
-#endif /* CONFIG_NO_HZ_COMMON */
-
 #ifndef cpu_relax_yield
 #define cpu_relax_yield() cpu_relax()
 #endif
@@ -1563,16 +1538,6 @@ extern void idle_task_exit(void);
 static inline void idle_task_exit(void) {}
 #endif
 
-#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)
-extern void wake_up_nohz_cpu(int cpu);
-#else
-static inline void wake_up_nohz_cpu(int cpu) { }
-#endif
-
-#ifdef CONFIG_NO_HZ_FULL
-extern u64 scheduler_tick_max_deferment(void);
-#endif
-
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);

commit 3605df49556ebb7641d1f8ec2e7eeecf4dd734bf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 11:03:12 2017 +0100

    sched/headers: Move task statistics APIs from <linux/sched.h> to <linux/sched/stat.h>
    
    There are a number of task statistics related variables and methods exported
    via sched.h - collect them into <linux/sched/stat.h> and include it from
    their usage sites.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dbc3ff04750a..415baf253517 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -46,16 +46,6 @@ struct nameidata;
 struct signal_struct;
 struct sighand_struct;
 
-extern unsigned long total_forks;
-extern int nr_threads;
-DECLARE_PER_CPU(unsigned long, process_counts);
-extern int nr_processes(void);
-extern unsigned long nr_running(void);
-extern bool single_task_running(void);
-extern unsigned long nr_iowait(void);
-extern unsigned long nr_iowait_cpu(int cpu);
-extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
-
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void cpu_load_update_nohz_start(void);
 extern void cpu_load_update_nohz_stop(void);

commit 74444edaa0b2ef946e846d5ddf1ba90efcd7c200
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 20:43:54 2017 +0100

    sched/headers: Move the memalloc_noio_*() APIs to <linux/sched/mm.h>
    
    In preparation to remove the <linux/gfp.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e0400069e95..dbc3ff04750a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1468,28 +1468,6 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *s
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
-/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags
- * __GFP_FS is also cleared as it implies __GFP_IO.
- */
-static inline gfp_t memalloc_noio_flags(gfp_t flags)
-{
-	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
-		flags &= ~(__GFP_IO | __GFP_FS);
-	return flags;
-}
-
-static inline unsigned int memalloc_noio_save(void)
-{
-	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
-	current->flags |= PF_MEMALLOC_NOIO;
-	return flags;
-}
-
-static inline void memalloc_noio_restore(unsigned int flags)
-{
-	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
-}
-
 /* Per-process atomic flags. */
 #define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 #define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */

commit 2a1f062a4acf0be50516ceece92a7182a173d55a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    This reduces the size of <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1201312e111e..1e0400069e95 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1931,37 +1931,6 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
-static inline int restart_syscall(void)
-{
-	set_tsk_thread_flag(current, TIF_SIGPENDING);
-	return -ERESTARTNOINTR;
-}
-
-static inline int signal_pending(struct task_struct *p)
-{
-	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
-}
-
-static inline int __fatal_signal_pending(struct task_struct *p)
-{
-	return unlikely(sigismember(&p->pending.signal, SIGKILL));
-}
-
-static inline int fatal_signal_pending(struct task_struct *p)
-{
-	return signal_pending(p) && __fatal_signal_pending(p);
-}
-
-static inline int signal_pending_state(long state, struct task_struct *p)
-{
-	if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
-		return 0;
-	if (!signal_pending(p))
-		return 0;
-
-	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
-}
-
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
@@ -2028,26 +1997,6 @@ static __always_inline bool need_resched(void)
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
-/*
- * Reevaluate whether the task has signals pending delivery.
- * Wake the task if so.
- * This is required every time the blocked sigset_t changes.
- * callers must hold sighand->siglock.
- */
-extern void recalc_sigpending_and_wake(struct task_struct *t);
-extern void recalc_sigpending(void);
-
-extern void signal_wake_up_state(struct task_struct *t, unsigned int state);
-
-static inline void signal_wake_up(struct task_struct *t, bool resume)
-{
-	signal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);
-}
-static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)
-{
-	signal_wake_up_state(t, resume ? __TASK_TRACED : 0);
-}
-
 /*
  * Wrappers for p->thread_info->cpu access. No-op on UP.
  */

commit 9a07000400c853c57477c2a5138ae9f556c40897
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:10:05 2017 +0100

    sched/headers: Move CONFIG_TASK_XACCT bits from <linux/sched.h> to <linux/sched/xacct.h>
    
    The CONFIG_TASK_XACCT=y accounting inline functions are only used by
    fs/read_write.c, so move them into their separate header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7752025679c0..1201312e111e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2104,44 +2104,6 @@ extern struct task_group root_task_group;
 extern int task_can_switch_user(struct user_struct *up,
 					struct task_struct *tsk);
 
-#ifdef CONFIG_TASK_XACCT
-static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
-{
-	tsk->ioac.rchar += amt;
-}
-
-static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
-{
-	tsk->ioac.wchar += amt;
-}
-
-static inline void inc_syscr(struct task_struct *tsk)
-{
-	tsk->ioac.syscr++;
-}
-
-static inline void inc_syscw(struct task_struct *tsk)
-{
-	tsk->ioac.syscw++;
-}
-#else
-static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
-{
-}
-
-static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
-{
-}
-
-static inline void inc_syscr(struct task_struct *tsk)
-{
-}
-
-static inline void inc_syscw(struct task_struct *tsk)
-{
-}
-#endif
-
 #ifndef TASK_SIZE_OF
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif

commit 30a1baab81189f01c427c4939610b2dde2dbec37
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:06:45 2017 +0100

    sched/headers: Remove various unrelated headers from <linux/sched.h>
    
    Remove the following header inclusions from <linux/sched.h>:
    
            #include <asm/param.h>
            #include <linux/threads.h>
            #include <linux/kernel.h>
            #include <linux/types.h>
            #include <linux/timex.h>
            #include <linux/jiffies.h>
            #include <linux/rbtree.h>
            #include <linux/thread_info.h>
            #include <linux/cpumask.h>
            #include <linux/errno.h>
            #include <linux/nodemask.h>
            #include <linux/preempt.h>
            #include <asm/page.h>
            #include <linux/smp.h>
            #include <linux/compiler.h>
            #include <linux/completion.h>
            #include <linux/percpu.h>
            #include <linux/topology.h>
            #include <linux/rcupdate.h>
            #include <linux/time.h>
            #include <linux/timer.h>
            #include <linux/llist.h>
            #include <linux/uidgid.h>
            #include <asm/processor.h>
    
    because they are either not required, or are already included
    naturally as part of the remaining headers.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 46ba8f1b5f1f..7752025679c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -5,60 +5,32 @@
 
 #include <linux/sched/prio.h>
 
-#include <asm/param.h>	/* for HZ */
-
 #include <linux/capability.h>
-#include <linux/threads.h>
-#include <linux/kernel.h>
-#include <linux/types.h>
-#include <linux/timex.h>
-#include <linux/jiffies.h>
 #include <linux/mutex.h>
 #include <linux/plist.h>
-#include <linux/rbtree.h>
-#include <linux/thread_info.h>
-#include <linux/cpumask.h>
-#include <linux/errno.h>
-#include <linux/nodemask.h>
 #include <linux/mm_types.h>
-#include <linux/preempt.h>
-
-#include <asm/page.h>
 #include <asm/ptrace.h>
 
-#include <linux/smp.h>
 #include <linux/sem.h>
 #include <linux/shm.h>
 #include <linux/signal.h>
-#include <linux/compiler.h>
-#include <linux/completion.h>
 #include <linux/signal_types.h>
 #include <linux/pid.h>
-#include <linux/percpu.h>
-#include <linux/topology.h>
 #include <linux/seccomp.h>
-#include <linux/rcupdate.h>
 #include <linux/rculist.h>
 #include <linux/rtmutex.h>
 
-#include <linux/time.h>
-#include <linux/param.h>
 #include <linux/resource.h>
-#include <linux/timer.h>
 #include <linux/hrtimer.h>
 #include <linux/kcov.h>
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
 #include <linux/cred.h>
-#include <linux/llist.h>
-#include <linux/uidgid.h>
 #include <linux/gfp.h>
 #include <linux/topology.h>
 #include <linux/magic.h>
 #include <linux/cgroup-defs.h>
 
-#include <asm/processor.h>
-
 struct sched_attr;
 struct sched_param;
 

commit c41cfc6c5ba46050b416c0b0b2621cbe68c4669c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:45:58 2017 +0100

    sched/headers: Move the JOBCTL_ defines and methods from <linux/sched.h> to <linux/sched/jobctl.h>
    
    Only a small fraction of sched.h users actually utilizes these defines,
    and they are not scheduler functionality in any case, so move them
    into their separate header.
    
    (Also make <linux/sched/jobctl.h> a self-contained header.)
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fcaea1e7b08a..46ba8f1b5f1f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1549,36 +1549,6 @@ TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
 TASK_PFA_SET(LMK_WAITING, lmk_waiting)
 
-/*
- * task->jobctl flags
- */
-#define JOBCTL_STOP_SIGMASK	0xffff	/* signr of the last group stop */
-
-#define JOBCTL_STOP_DEQUEUED_BIT 16	/* stop signal dequeued */
-#define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
-#define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
-#define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
-#define JOBCTL_TRAP_NOTIFY_BIT	20	/* trap for NOTIFY */
-#define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
-#define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
-
-#define JOBCTL_STOP_DEQUEUED	(1UL << JOBCTL_STOP_DEQUEUED_BIT)
-#define JOBCTL_STOP_PENDING	(1UL << JOBCTL_STOP_PENDING_BIT)
-#define JOBCTL_STOP_CONSUME	(1UL << JOBCTL_STOP_CONSUME_BIT)
-#define JOBCTL_TRAP_STOP	(1UL << JOBCTL_TRAP_STOP_BIT)
-#define JOBCTL_TRAP_NOTIFY	(1UL << JOBCTL_TRAP_NOTIFY_BIT)
-#define JOBCTL_TRAPPING		(1UL << JOBCTL_TRAPPING_BIT)
-#define JOBCTL_LISTENING	(1UL << JOBCTL_LISTENING_BIT)
-
-#define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
-#define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
-
-extern bool task_set_jobctl_pending(struct task_struct *task,
-				    unsigned long mask);
-extern void task_clear_jobctl_trapping(struct task_struct *task);
-extern void task_clear_jobctl_pending(struct task_struct *task,
-				      unsigned long mask);
-
 static inline void rcu_copy_process(struct task_struct *p)
 {
 #ifdef CONFIG_PREEMPT_RCU

commit 5647028d550c959577527cec9c0f29fbcea029ea
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:39:17 2017 +0100

    sched/headers: Move the NUMA balancing interfaces from <linux/sched.h> to <linux/sched/numa_balancing.h>
    
    Split out the interface between the scheduler and the MM which
    deals with page fault driven NUMA balancing, into the new
    <linux/sched/numa_balancing.h> header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aa60812b4b7a..fcaea1e7b08a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1236,41 +1236,6 @@ static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
 }
 #endif
 
-#define TNF_MIGRATED	0x01
-#define TNF_NO_GROUP	0x02
-#define TNF_SHARED	0x04
-#define TNF_FAULT_LOCAL	0x08
-#define TNF_MIGRATE_FAIL 0x10
-
-#ifdef CONFIG_NUMA_BALANCING
-extern void task_numa_fault(int last_node, int node, int pages, int flags);
-extern pid_t task_numa_group_id(struct task_struct *p);
-extern void set_numabalancing_state(bool enabled);
-extern void task_numa_free(struct task_struct *p);
-extern bool should_numa_migrate_memory(struct task_struct *p, struct page *page,
-					int src_nid, int dst_cpu);
-#else
-static inline void task_numa_fault(int last_node, int node, int pages,
-				   int flags)
-{
-}
-static inline pid_t task_numa_group_id(struct task_struct *p)
-{
-	return 0;
-}
-static inline void set_numabalancing_state(bool enabled)
-{
-}
-static inline void task_numa_free(struct task_struct *p)
-{
-}
-static inline bool should_numa_migrate_memory(struct task_struct *p,
-				struct page *page, int src_nid, int dst_cpu)
-{
-	return true;
-}
-#endif
-
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;

commit d026ce796cbca3c49678a68bb4a39fb4b9cf8192
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:32:21 2017 +0100

    sched/headers: Move in_vfork() from <linux/sched.h> to <linux/sched/mm.h>
    
    The in_vfork() function deals with task->mm, so it better belongs
    into <linux/sched/mm.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 253d8ab6256c..aa60812b4b7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1242,32 +1242,6 @@ static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
 #define TNF_FAULT_LOCAL	0x08
 #define TNF_MIGRATE_FAIL 0x10
 
-static inline bool in_vfork(struct task_struct *tsk)
-{
-	bool ret;
-
-	/*
-	 * need RCU to access ->real_parent if CLONE_VM was used along with
-	 * CLONE_PARENT.
-	 *
-	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
-	 * imply CLONE_VM
-	 *
-	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
-	 * ->real_parent is not necessarily the task doing vfork(), so in
-	 * theory we can't rely on task_lock() if we want to dereference it.
-	 *
-	 * And in this case we can't trust the real_parent->mm == tsk->mm
-	 * check, it can be false negative. But we do not care, if init or
-	 * another oom-unkillable task does this it should blame itself.
-	 */
-	rcu_read_lock();
-	ret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;
-	rcu_read_unlock();
-
-	return ret;
-}
-
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);

commit abe722a1c59728c6c0ea4e4d5efcfe397c8abebc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:27:56 2017 +0100

    sched/headers: Move the 'init_mm' declaration from <linux/sched.h> to <linux/mm_types.h>
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1ddb82be6b50..253d8ab6256c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1782,8 +1782,6 @@ static inline int kstack_end(void *addr)
 extern union thread_union init_thread_union;
 extern struct task_struct init_task;
 
-extern struct   mm_struct init_mm;
-
 extern struct pid_namespace init_pid_ns;
 
 /*

commit 4240c8bf877f1145571106a2934c5cea0b51b178
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:18:24 2017 +0100

    sched/headers: Move more mm_struct related functionality from <linux/sched.h> to <linux/sched/mm.h>
    
    Neither the mmap_layout nor the mm_update_next_owner() methods need to be
    in <linux/sched.h> - move them to the more appropriate <linux/sched/mm.h> header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aae79706ec4f..1ddb82be6b50 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -313,19 +313,6 @@ void __noreturn do_task_dead(void);
 
 struct nsproxy;
 
-#ifdef CONFIG_MMU
-extern void arch_pick_mmap_layout(struct mm_struct *mm);
-extern unsigned long
-arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
-		       unsigned long, unsigned long);
-extern unsigned long
-arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
-			  unsigned long len, unsigned long pgoff,
-			  unsigned long flags);
-#else
-static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
-#endif
-
 /**
  * struct prev_cputime - snaphsot of system and user cputime
  * @utime: time spent in user mode
@@ -2280,12 +2267,4 @@ static inline void inc_syscw(struct task_struct *tsk)
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
-#ifdef CONFIG_MEMCG
-extern void mm_update_next_owner(struct mm_struct *mm);
-#else
-static inline void mm_update_next_owner(struct mm_struct *mm)
-{
-}
-#endif /* CONFIG_MEMCG */
-
 #endif

commit 7284c6d419b5a6ae9806927f1fd4f0cfd19a16f5
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:14:19 2017 +0100

    sched/headers: Move the cpufreq interfaces to <linux/sched/cpufreq.h>
    
    No need to have this in the generic <linux/sched.h> header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8bf111efe98e..aae79706ec4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2288,21 +2288,4 @@ static inline void mm_update_next_owner(struct mm_struct *mm)
 }
 #endif /* CONFIG_MEMCG */
 
-#define SCHED_CPUFREQ_RT	(1U << 0)
-#define SCHED_CPUFREQ_DL	(1U << 1)
-#define SCHED_CPUFREQ_IOWAIT	(1U << 2)
-
-#define SCHED_CPUFREQ_RT_DL	(SCHED_CPUFREQ_RT | SCHED_CPUFREQ_DL)
-
-#ifdef CONFIG_CPU_FREQ
-struct update_util_data {
-       void (*func)(struct update_util_data *data, u64 time, unsigned int flags);
-};
-
-void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,
-                       void (*func)(struct update_util_data *data, u64 time,
-				    unsigned int flags));
-void cpufreq_remove_update_util_hook(int cpu);
-#endif /* CONFIG_CPU_FREQ */
-
 #endif

commit 8d88460edc05224b8d7ae3372173153876a02825
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 12:06:10 2017 +0100

    sched/headers: Move 'struct pacct_struct' and 'struct cpu_itimer' form <linux/sched.h> to <linux/sched/signal.h>
    
    These structures are actually part of 'struct signal', so move them to <linux/sched/signal.h>
    where they belong.
    
    This further decreases the size and complexity of <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8a1d296c53a0..8bf111efe98e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -326,19 +326,6 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-struct pacct_struct {
-	int			ac_flag;
-	long			ac_exitcode;
-	unsigned long		ac_mem;
-	u64			ac_utime, ac_stime;
-	unsigned long		ac_minflt, ac_majflt;
-};
-
-struct cpu_itimer {
-	u64 expires;
-	u64 incr;
-};
-
 /**
  * struct prev_cputime - snaphsot of system and user cputime
  * @utime: time spent in user mode

commit d151b27d3f86589308cd8c891fdfd2db5f8e80d6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:17:23 2017 +0100

    sched/headers: Move softlockup detector watchdog methods to <linux/nmi.h>
    
    These methods don't belong into <linux/sched.h>, they are neither directly
    related to task_struct or are scheduler functionality.
    
    Put them next to the other watchdog methods in <linux/nmi.h>.
    
    ( Arguably that header's name is a misnomer, and this patch makes it
      more so - but it should be renamed in another patch. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 71efbcafaa31..8a1d296c53a0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -286,43 +286,6 @@ extern int sched_cpu_dying(unsigned int cpu);
 
 extern void sched_show_task(struct task_struct *p);
 
-#ifdef CONFIG_LOCKUP_DETECTOR
-extern void touch_softlockup_watchdog_sched(void);
-extern void touch_softlockup_watchdog(void);
-extern void touch_softlockup_watchdog_sync(void);
-extern void touch_all_softlockup_watchdogs(void);
-extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
-				  void __user *buffer,
-				  size_t *lenp, loff_t *ppos);
-extern unsigned int  softlockup_panic;
-extern unsigned int  hardlockup_panic;
-void lockup_detector_init(void);
-#else
-static inline void touch_softlockup_watchdog_sched(void)
-{
-}
-static inline void touch_softlockup_watchdog(void)
-{
-}
-static inline void touch_softlockup_watchdog_sync(void)
-{
-}
-static inline void touch_all_softlockup_watchdogs(void)
-{
-}
-static inline void lockup_detector_init(void)
-{
-}
-#endif
-
-#ifdef CONFIG_DETECT_HUNG_TASK
-void reset_hung_task_detector(void);
-#else
-static inline void reset_hung_task_detector(void)
-{
-}
-#endif
-
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 

commit bcbb6a5bf7df6e37ba652d1f426eab042ec4f56b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 10:22:42 2017 +0100

    sched/headers: Move 'struct user_struct' definition and APIs to the new <linux/sched/user.h> header
    
    'struct user_struct' was added to sched.h historically, but it's actually
    entirely independent of task_struct and of scheduler details, so move
    it to its own header.
    
    Fix up .c files using those facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1586104d4c0..71efbcafaa31 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -349,7 +349,6 @@ extern void io_schedule(void);
 void __noreturn do_task_dead(void);
 
 struct nsproxy;
-struct user_namespace;
 
 #ifdef CONFIG_MMU
 extern void arch_pick_mmap_layout(struct mm_struct *mm);
@@ -481,49 +480,6 @@ struct thread_group_cputimer {
 #include <linux/rwsem.h>
 struct autogroup;
 
-/*
- * Some day this will be a full-fledged user tracking system..
- */
-struct user_struct {
-	atomic_t __count;	/* reference count */
-	atomic_t processes;	/* How many processes does this user have? */
-	atomic_t sigpending;	/* How many pending signals does this user have? */
-#ifdef CONFIG_FANOTIFY
-	atomic_t fanotify_listeners;
-#endif
-#ifdef CONFIG_EPOLL
-	atomic_long_t epoll_watches; /* The number of file descriptors currently watched */
-#endif
-#ifdef CONFIG_POSIX_MQUEUE
-	/* protected by mq_lock	*/
-	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
-#endif
-	unsigned long locked_shm; /* How many pages of mlocked shm ? */
-	unsigned long unix_inflight;	/* How many files in flight in unix sockets */
-	atomic_long_t pipe_bufs;  /* how many pages are allocated in pipe buffers */
-
-#ifdef CONFIG_KEYS
-	struct key *uid_keyring;	/* UID specific keyring */
-	struct key *session_keyring;	/* UID's default session keyring */
-#endif
-
-	/* Hash table maintenance information */
-	struct hlist_node uidhash_node;
-	kuid_t uid;
-
-#if defined(CONFIG_PERF_EVENTS) || defined(CONFIG_BPF_SYSCALL)
-	atomic_long_t locked_vm;
-#endif
-};
-
-extern int uids_sysfs_init(void);
-
-extern struct user_struct *find_user(kuid_t);
-
-extern struct user_struct root_user;
-#define INIT_USER (&root_user)
-
-
 struct backing_dev_info;
 struct reclaim_state;
 
@@ -1908,15 +1864,6 @@ extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);
 
-/* per-UID process charging. */
-extern struct user_struct * alloc_uid(kuid_t);
-static inline struct user_struct *get_uid(struct user_struct *u)
-{
-	atomic_inc(&u->__count);
-	return u;
-}
-extern void free_uid(struct user_struct *);
-
 #include <asm/current.h>
 
 extern void xtime_update(unsigned long ticks);

commit c3edc4010e9d102eb7b8f17d15c2ebc425fed63c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 08:35:14 2017 +0100

    sched/headers: Move task_struct::signal and task_struct::sighand types and accessors into <linux/sched/signal.h>
    
    task_struct::signal and task_struct::sighand are pointers, which would normally make it
    straightforward to not define those types in sched.h.
    
    That is not so, because the types are accompanied by a myriad of APIs (macros and inline
    functions) that dereference them.
    
    Split the types and the APIs out of sched.h and move them into a new header, <linux/sched/signal.h>.
    
    With this change sched.h does not know about 'struct signal' and 'struct sighand' anymore,
    trying to put accessors into sched.h as a test fails the following way:
    
      ./include/linux/sched.h: In function â€˜test_signal_typesâ€™:
      ./include/linux/sched.h:2461:18: error: dereferencing pointer to incomplete type â€˜struct signal_structâ€™
                        ^
    
    This reduces the size and complexity of sched.h significantly.
    
    Update all headers and .c code that relied on getting the signal handling
    functionality from <linux/sched.h> to include <linux/sched/signal.h>.
    
    The list of affected files in the preparatory patch was partly generated by
    grepping for the APIs, and partly by doing coverage build testing, both
    all[yes|mod|def|no]config builds on 64-bit and 32-bit x86, and an array of
    cross-architecture builds.
    
    Nevertheless some (trivial) build breakage is still expected related to rare
    Kconfig combinations and in-flight patches to various kernel code, but most
    of it should be handled by this patch.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7934cd0acbc7..c1586104d4c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -71,6 +71,9 @@ struct blk_plug;
 struct filename;
 struct nameidata;
 
+struct signal_struct;
+struct sighand_struct;
+
 extern unsigned long total_forks;
 extern int nr_threads;
 DECLARE_PER_CPU(unsigned long, process_counts);
@@ -361,13 +364,6 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-struct sighand_struct {
-	atomic_t		count;
-	struct k_sigaction	action[_NSIG];
-	spinlock_t		siglock;
-	wait_queue_head_t	signalfd_wqh;
-};
-
 struct pacct_struct {
 	int			ac_flag;
 	long			ac_exitcode;
@@ -485,195 +481,6 @@ struct thread_group_cputimer {
 #include <linux/rwsem.h>
 struct autogroup;
 
-/*
- * NOTE! "signal_struct" does not have its own
- * locking, because a shared signal_struct always
- * implies a shared sighand_struct, so locking
- * sighand_struct is always a proper superset of
- * the locking of signal_struct.
- */
-struct signal_struct {
-	atomic_t		sigcnt;
-	atomic_t		live;
-	int			nr_threads;
-	struct list_head	thread_head;
-
-	wait_queue_head_t	wait_chldexit;	/* for wait4() */
-
-	/* current thread group signal load-balancing target: */
-	struct task_struct	*curr_target;
-
-	/* shared signal handling: */
-	struct sigpending	shared_pending;
-
-	/* thread group exit support */
-	int			group_exit_code;
-	/* overloaded:
-	 * - notify group_exit_task when ->count is equal to notify_count
-	 * - everyone except group_exit_task is stopped during signal delivery
-	 *   of fatal signals, group_exit_task processes the signal.
-	 */
-	int			notify_count;
-	struct task_struct	*group_exit_task;
-
-	/* thread group stop support, overloads group_exit_code too */
-	int			group_stop_count;
-	unsigned int		flags; /* see SIGNAL_* flags below */
-
-	/*
-	 * PR_SET_CHILD_SUBREAPER marks a process, like a service
-	 * manager, to re-parent orphan (double-forking) child processes
-	 * to this process instead of 'init'. The service manager is
-	 * able to receive SIGCHLD signals and is able to investigate
-	 * the process until it calls wait(). All children of this
-	 * process will inherit a flag if they should look for a
-	 * child_subreaper process at exit.
-	 */
-	unsigned int		is_child_subreaper:1;
-	unsigned int		has_child_subreaper:1;
-
-#ifdef CONFIG_POSIX_TIMERS
-
-	/* POSIX.1b Interval Timers */
-	int			posix_timer_id;
-	struct list_head	posix_timers;
-
-	/* ITIMER_REAL timer for the process */
-	struct hrtimer real_timer;
-	ktime_t it_real_incr;
-
-	/*
-	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
-	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
-	 * values are defined to 0 and 1 respectively
-	 */
-	struct cpu_itimer it[2];
-
-	/*
-	 * Thread group totals for process CPU timers.
-	 * See thread_group_cputimer(), et al, for details.
-	 */
-	struct thread_group_cputimer cputimer;
-
-	/* Earliest-expiration cache. */
-	struct task_cputime cputime_expires;
-
-	struct list_head cpu_timers[3];
-
-#endif
-
-	struct pid *leader_pid;
-
-#ifdef CONFIG_NO_HZ_FULL
-	atomic_t tick_dep_mask;
-#endif
-
-	struct pid *tty_old_pgrp;
-
-	/* boolean value for session group leader */
-	int leader;
-
-	struct tty_struct *tty; /* NULL if no tty */
-
-#ifdef CONFIG_SCHED_AUTOGROUP
-	struct autogroup *autogroup;
-#endif
-	/*
-	 * Cumulative resource counters for dead threads in the group,
-	 * and for reaped dead child processes forked by this group.
-	 * Live threads maintain their own counters and add to these
-	 * in __exit_signal, except for the group leader.
-	 */
-	seqlock_t stats_lock;
-	u64 utime, stime, cutime, cstime;
-	u64 gtime;
-	u64 cgtime;
-	struct prev_cputime prev_cputime;
-	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
-	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
-	unsigned long inblock, oublock, cinblock, coublock;
-	unsigned long maxrss, cmaxrss;
-	struct task_io_accounting ioac;
-
-	/*
-	 * Cumulative ns of schedule CPU time fo dead threads in the
-	 * group, not including a zombie group leader, (This only differs
-	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
-	 * other than jiffies.)
-	 */
-	unsigned long long sum_sched_runtime;
-
-	/*
-	 * We don't bother to synchronize most readers of this at all,
-	 * because there is no reader checking a limit that actually needs
-	 * to get both rlim_cur and rlim_max atomically, and either one
-	 * alone is a single word that can safely be read normally.
-	 * getrlimit/setrlimit use task_lock(current->group_leader) to
-	 * protect this instead of the siglock, because they really
-	 * have no need to disable irqs.
-	 */
-	struct rlimit rlim[RLIM_NLIMITS];
-
-#ifdef CONFIG_BSD_PROCESS_ACCT
-	struct pacct_struct pacct;	/* per-process accounting information */
-#endif
-#ifdef CONFIG_TASKSTATS
-	struct taskstats *stats;
-#endif
-#ifdef CONFIG_AUDIT
-	unsigned audit_tty;
-	struct tty_audit_buf *tty_audit_buf;
-#endif
-
-	/*
-	 * Thread is the potential origin of an oom condition; kill first on
-	 * oom
-	 */
-	bool oom_flag_origin;
-	short oom_score_adj;		/* OOM kill score adjustment */
-	short oom_score_adj_min;	/* OOM kill score adjustment min value.
-					 * Only settable by CAP_SYS_RESOURCE. */
-	struct mm_struct *oom_mm;	/* recorded mm when the thread group got
-					 * killed by the oom killer */
-
-	struct mutex cred_guard_mutex;	/* guard against foreign influences on
-					 * credential calculations
-					 * (notably. ptrace) */
-};
-
-/*
- * Bits in flags field of signal_struct.
- */
-#define SIGNAL_STOP_STOPPED	0x00000001 /* job control stop in effect */
-#define SIGNAL_STOP_CONTINUED	0x00000002 /* SIGCONT since WCONTINUED reap */
-#define SIGNAL_GROUP_EXIT	0x00000004 /* group exit in progress */
-#define SIGNAL_GROUP_COREDUMP	0x00000008 /* coredump in progress */
-/*
- * Pending notifications to parent.
- */
-#define SIGNAL_CLD_STOPPED	0x00000010
-#define SIGNAL_CLD_CONTINUED	0x00000020
-#define SIGNAL_CLD_MASK		(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)
-
-#define SIGNAL_UNKILLABLE	0x00000040 /* for init: ignore fatal signals */
-
-#define SIGNAL_STOP_MASK (SIGNAL_CLD_MASK | SIGNAL_STOP_STOPPED | \
-			  SIGNAL_STOP_CONTINUED)
-
-static inline void signal_set_stop_flags(struct signal_struct *sig,
-					 unsigned int flags)
-{
-	WARN_ON(sig->flags & (SIGNAL_GROUP_EXIT|SIGNAL_GROUP_COREDUMP));
-	sig->flags = (sig->flags & ~SIGNAL_STOP_MASK) | flags;
-}
-
-/* If true, all threads except ->group_exit_task have pending SIGKILL */
-static inline int signal_group_exit(const struct signal_struct *sig)
-{
-	return	(sig->flags & SIGNAL_GROUP_EXIT) ||
-		(sig->group_exit_task != NULL);
-}
-
 /*
  * Some day this will be a full-fledged user tracking system..
  */
@@ -2126,190 +1933,8 @@ extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);
-extern void flush_signals(struct task_struct *);
-extern void ignore_signals(struct task_struct *);
-extern void flush_signal_handlers(struct task_struct *, int force_default);
-extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
-
-static inline int kernel_dequeue_signal(siginfo_t *info)
-{
-	struct task_struct *tsk = current;
-	siginfo_t __info;
-	int ret;
-
-	spin_lock_irq(&tsk->sighand->siglock);
-	ret = dequeue_signal(tsk, &tsk->blocked, info ?: &__info);
-	spin_unlock_irq(&tsk->sighand->siglock);
-
-	return ret;
-}
-
-static inline void kernel_signal_stop(void)
-{
-	spin_lock_irq(&current->sighand->siglock);
-	if (current->jobctl & JOBCTL_STOP_DEQUEUED)
-		__set_current_state(TASK_STOPPED);
-	spin_unlock_irq(&current->sighand->siglock);
-
-	schedule();
-}
 
 extern void release_task(struct task_struct * p);
-extern int send_sig_info(int, struct siginfo *, struct task_struct *);
-extern int force_sigsegv(int, struct task_struct *);
-extern int force_sig_info(int, struct siginfo *, struct task_struct *);
-extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
-extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
-extern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,
-				const struct cred *, u32);
-extern int kill_pgrp(struct pid *pid, int sig, int priv);
-extern int kill_pid(struct pid *pid, int sig, int priv);
-extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern __must_check bool do_notify_parent(struct task_struct *, int);
-extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
-extern void force_sig(int, struct task_struct *);
-extern int send_sig(int, struct task_struct *, int);
-extern int zap_other_threads(struct task_struct *p);
-extern struct sigqueue *sigqueue_alloc(void);
-extern void sigqueue_free(struct sigqueue *);
-extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
-extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
-
-#ifdef TIF_RESTORE_SIGMASK
-/*
- * Legacy restore_sigmask accessors.  These are inefficient on
- * SMP architectures because they require atomic operations.
- */
-
-/**
- * set_restore_sigmask() - make sure saved_sigmask processing gets done
- *
- * This sets TIF_RESTORE_SIGMASK and ensures that the arch signal code
- * will run before returning to user mode, to process the flag.  For
- * all callers, TIF_SIGPENDING is already set or it's no harm to set
- * it.  TIF_RESTORE_SIGMASK need not be in the set of bits that the
- * arch code will notice on return to user mode, in case those bits
- * are scarce.  We set TIF_SIGPENDING here to ensure that the arch
- * signal code always gets run when TIF_RESTORE_SIGMASK is set.
- */
-static inline void set_restore_sigmask(void)
-{
-	set_thread_flag(TIF_RESTORE_SIGMASK);
-	WARN_ON(!test_thread_flag(TIF_SIGPENDING));
-}
-static inline void clear_restore_sigmask(void)
-{
-	clear_thread_flag(TIF_RESTORE_SIGMASK);
-}
-static inline bool test_restore_sigmask(void)
-{
-	return test_thread_flag(TIF_RESTORE_SIGMASK);
-}
-static inline bool test_and_clear_restore_sigmask(void)
-{
-	return test_and_clear_thread_flag(TIF_RESTORE_SIGMASK);
-}
-
-#else	/* TIF_RESTORE_SIGMASK */
-
-/* Higher-quality implementation, used if TIF_RESTORE_SIGMASK doesn't exist. */
-static inline void set_restore_sigmask(void)
-{
-	current->restore_sigmask = true;
-	WARN_ON(!test_thread_flag(TIF_SIGPENDING));
-}
-static inline void clear_restore_sigmask(void)
-{
-	current->restore_sigmask = false;
-}
-static inline bool test_restore_sigmask(void)
-{
-	return current->restore_sigmask;
-}
-static inline bool test_and_clear_restore_sigmask(void)
-{
-	if (!current->restore_sigmask)
-		return false;
-	current->restore_sigmask = false;
-	return true;
-}
-#endif
-
-static inline void restore_saved_sigmask(void)
-{
-	if (test_and_clear_restore_sigmask())
-		__set_current_blocked(&current->saved_sigmask);
-}
-
-static inline sigset_t *sigmask_to_save(void)
-{
-	sigset_t *res = &current->blocked;
-	if (unlikely(test_restore_sigmask()))
-		res = &current->saved_sigmask;
-	return res;
-}
-
-static inline int kill_cad_pid(int sig, int priv)
-{
-	return kill_pid(cad_pid, sig, priv);
-}
-
-/* These can be the second arg to send_sig_info/send_group_sig_info.  */
-#define SEND_SIG_NOINFO ((struct siginfo *) 0)
-#define SEND_SIG_PRIV	((struct siginfo *) 1)
-#define SEND_SIG_FORCED	((struct siginfo *) 2)
-
-/*
- * True if we are on the alternate signal stack.
- */
-static inline int on_sig_stack(unsigned long sp)
-{
-	/*
-	 * If the signal stack is SS_AUTODISARM then, by construction, we
-	 * can't be on the signal stack unless user code deliberately set
-	 * SS_AUTODISARM when we were already on it.
-	 *
-	 * This improves reliability: if user state gets corrupted such that
-	 * the stack pointer points very close to the end of the signal stack,
-	 * then this check will enable the signal to be handled anyway.
-	 */
-	if (current->sas_ss_flags & SS_AUTODISARM)
-		return 0;
-
-#ifdef CONFIG_STACK_GROWSUP
-	return sp >= current->sas_ss_sp &&
-		sp - current->sas_ss_sp < current->sas_ss_size;
-#else
-	return sp > current->sas_ss_sp &&
-		sp - current->sas_ss_sp <= current->sas_ss_size;
-#endif
-}
-
-static inline int sas_ss_flags(unsigned long sp)
-{
-	if (!current->sas_ss_size)
-		return SS_DISABLE;
-
-	return on_sig_stack(sp) ? SS_ONSTACK : 0;
-}
-
-static inline void sas_ss_reset(struct task_struct *p)
-{
-	p->sas_ss_sp = 0;
-	p->sas_ss_size = 0;
-	p->sas_ss_flags = SS_DISABLE;
-}
-
-static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
-{
-	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))
-#ifdef CONFIG_STACK_GROWSUP
-		return current->sas_ss_sp;
-#else
-		return current->sas_ss_sp + current->sas_ss_size;
-#endif
-	return sp;
-}
 
 #ifdef CONFIG_HAVE_COPY_THREAD_TLS
 extern int copy_thread_tls(unsigned long, unsigned long, unsigned long,
@@ -2338,10 +1963,8 @@ static inline void exit_thread(struct task_struct *tsk)
 #endif
 
 extern void exit_files(struct task_struct *);
-extern void __cleanup_sighand(struct sighand_struct *);
 
 extern void exit_itimers(struct signal_struct *);
-extern void flush_itimer_signals(void);
 
 extern void do_group_exit(int);
 
@@ -2376,81 +1999,6 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 }
 #endif
 
-#define tasklist_empty() \
-	list_empty(&init_task.tasks)
-
-#define next_task(p) \
-	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
-
-#define for_each_process(p) \
-	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
-
-extern bool current_is_single_threaded(void);
-
-/*
- * Careful: do_each_thread/while_each_thread is a double loop so
- *          'break' will not work as expected - use goto instead.
- */
-#define do_each_thread(g, t) \
-	for (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do
-
-#define while_each_thread(g, t) \
-	while ((t = next_thread(t)) != g)
-
-#define __for_each_thread(signal, t)	\
-	list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)
-
-#define for_each_thread(p, t)		\
-	__for_each_thread((p)->signal, t)
-
-/* Careful: this is a double loop, 'break' won't work as expected. */
-#define for_each_process_thread(p, t)	\
-	for_each_process(p) for_each_thread(p, t)
-
-typedef int (*proc_visitor)(struct task_struct *p, void *data);
-void walk_process_tree(struct task_struct *top, proc_visitor, void *);
-
-static inline int get_nr_threads(struct task_struct *tsk)
-{
-	return tsk->signal->nr_threads;
-}
-
-static inline bool thread_group_leader(struct task_struct *p)
-{
-	return p->exit_signal >= 0;
-}
-
-/* Do to the insanities of de_thread it is possible for a process
- * to have the pid of the thread group leader without actually being
- * the thread group leader.  For iteration through the pids in proc
- * all we care about is that we have a task with the appropriate
- * pid, we don't actually care if we have the right task.
- */
-static inline bool has_group_leader_pid(struct task_struct *p)
-{
-	return task_pid(p) == p->signal->leader_pid;
-}
-
-static inline
-bool same_thread_group(struct task_struct *p1, struct task_struct *p2)
-{
-	return p1->signal == p2->signal;
-}
-
-static inline struct task_struct *next_thread(const struct task_struct *p)
-{
-	return list_entry_rcu(p->thread_group.next,
-			      struct task_struct, thread_group);
-}
-
-static inline int thread_group_empty(struct task_struct *p)
-{
-	return list_empty(&p->thread_group);
-}
-
-#define delay_group_leader(p) \
-		(thread_group_leader(p) && !thread_group_empty(p))
-
 /*
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
@@ -2471,25 +2019,6 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
-extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
-							unsigned long *flags);
-
-static inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
-						       unsigned long *flags)
-{
-	struct sighand_struct *ret;
-
-	ret = __lock_task_sighand(tsk, flags);
-	(void)__cond_lock(&tsk->sighand->siglock, ret);
-	return ret;
-}
-
-static inline void unlock_task_sighand(struct task_struct *tsk,
-						unsigned long *flags)
-{
-	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
-}
-
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 
 static inline struct thread_info *task_thread_info(struct task_struct *task)
@@ -2862,28 +2391,6 @@ static inline void mm_update_next_owner(struct mm_struct *mm)
 }
 #endif /* CONFIG_MEMCG */
 
-static inline unsigned long task_rlimit(const struct task_struct *tsk,
-		unsigned int limit)
-{
-	return READ_ONCE(tsk->signal->rlim[limit].rlim_cur);
-}
-
-static inline unsigned long task_rlimit_max(const struct task_struct *tsk,
-		unsigned int limit)
-{
-	return READ_ONCE(tsk->signal->rlim[limit].rlim_max);
-}
-
-static inline unsigned long rlimit(unsigned int limit)
-{
-	return task_rlimit(current, limit);
-}
-
-static inline unsigned long rlimit_max(unsigned int limit)
-{
-	return task_rlimit_max(current, limit);
-}
-
 #define SCHED_CPUFREQ_RT	(1U << 0)
 #define SCHED_CPUFREQ_DL	(1U << 1)
 #define SCHED_CPUFREQ_IOWAIT	(1U << 2)

commit 11701c6768367294c5086738d49196192aaf3d60
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 19:21:47 2017 +0100

    sched/headers: Move task->mm coredumping related defines and methods from <linux/sched.h> to <linux/sched/coredump.h>
    
    This further reduces the size and complexity of <linux/sched.h>.
    
    These are the definitions and APIs that are moved:
    
      # MMF_*:
      fs/binfmt_elf.c
      fs/binfmt_elf_fdpic.c
      fs/exec.c
      fs/proc/base.c
      include/linux/khugepaged.h
      include/linux/ksm.h
      include/linux/sched/coredump.h
      kernel/events/uprobes.c
      kernel/fork.c
      mm/huge_memory.c
      mm/khugepaged.c
      mm/ksm.c
      mm/memory.c
      mm/oom_kill.c
    
      # SUID_DUMP_*:
      arch/ia64/include/asm/processor.h
      fs/coredump.c
      fs/exec.c
      fs/proc/internal.h
      include/linux/sched/coredump.h
      kernel/ptrace.c
      kernel/sys.c
      kernel/sysctl.c
    
      # get_dumpable():
      arch/ia64/include/asm/processor.h
      fs/coredump.c
      fs/exec.c
      fs/proc/internal.h
      include/linux/sched/coredump.h
      kernel/ptrace.c
      kernel/sys.c
    
      # set_dumpable():
      fs/exec.c
      include/linux/sched/coredump.h
      kernel/cred.c
      kernel/sys.c
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d29bbe0ee41f..7934cd0acbc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -361,74 +361,6 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-#define SUID_DUMP_DISABLE	0	/* No setuid dumping */
-#define SUID_DUMP_USER		1	/* Dump as user of process */
-#define SUID_DUMP_ROOT		2	/* Dump as root */
-
-/* mm flags */
-
-/* for SUID_DUMP_* above */
-#define MMF_DUMPABLE_BITS 2
-#define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
-
-extern void set_dumpable(struct mm_struct *mm, int value);
-/*
- * This returns the actual value of the suid_dumpable flag. For things
- * that are using this for checking for privilege transitions, it must
- * test against SUID_DUMP_USER rather than treating it as a boolean
- * value.
- */
-static inline int __get_dumpable(unsigned long mm_flags)
-{
-	return mm_flags & MMF_DUMPABLE_MASK;
-}
-
-static inline int get_dumpable(struct mm_struct *mm)
-{
-	return __get_dumpable(mm->flags);
-}
-
-/* coredump filter bits */
-#define MMF_DUMP_ANON_PRIVATE	2
-#define MMF_DUMP_ANON_SHARED	3
-#define MMF_DUMP_MAPPED_PRIVATE	4
-#define MMF_DUMP_MAPPED_SHARED	5
-#define MMF_DUMP_ELF_HEADERS	6
-#define MMF_DUMP_HUGETLB_PRIVATE 7
-#define MMF_DUMP_HUGETLB_SHARED  8
-#define MMF_DUMP_DAX_PRIVATE	9
-#define MMF_DUMP_DAX_SHARED	10
-
-#define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
-#define MMF_DUMP_FILTER_BITS	9
-#define MMF_DUMP_FILTER_MASK \
-	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
-#define MMF_DUMP_FILTER_DEFAULT \
-	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED) |\
-	 (1 << MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)
-
-#ifdef CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS
-# define MMF_DUMP_MASK_DEFAULT_ELF	(1 << MMF_DUMP_ELF_HEADERS)
-#else
-# define MMF_DUMP_MASK_DEFAULT_ELF	0
-#endif
-					/* leave room for more dump flags */
-#define MMF_VM_MERGEABLE	16	/* KSM may merge identical pages */
-#define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
-/*
- * This one-shot flag is dropped due to necessity of changing exe once again
- * on NFS restore
- */
-//#define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
-
-#define MMF_HAS_UPROBES		19	/* has uprobes */
-#define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
-#define MMF_OOM_SKIP		21	/* mm is of no interest for the OOM killer */
-#define MMF_UNSTABLE		22	/* mm is unstable for copy_from_user */
-#define MMF_HUGE_ZERO_PAGE	23      /* mm has ever used the global huge zero page */
-
-#define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
-
 struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];

commit 68e21be2916b359fd8afb536c1911dc014cfd03e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 19:08:20 2017 +0100

    sched/headers: Move task->mm handling methods to <linux/sched/mm.h>
    
    Move the following task->mm helper APIs into a new header file,
    <linux/sched/mm.h>, to further reduce the size and complexity
    of <linux/sched.h>.
    
    Here are how the APIs are used in various kernel files:
    
      # mm_alloc():
      arch/arm/mach-rpc/ecard.c
      fs/exec.c
      include/linux/sched/mm.h
      kernel/fork.c
    
      # __mmdrop():
      arch/arc/include/asm/mmu_context.h
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmdrop():
      arch/arm/mach-rpc/ecard.c
      arch/m68k/sun3/mmu_emu.c
      arch/x86/mm/tlb.c
      drivers/gpu/drm/amd/amdkfd/kfd_process.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/hw/hfi1/file_ops.c
      drivers/vfio/vfio_iommu_spapr_tce.c
      fs/exec.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/mmu_notifier.h
      include/linux/sched/mm.h
      kernel/fork.c
      kernel/futex.c
      kernel/sched/core.c
      mm/khugepaged.c
      mm/ksm.c
      mm/mmu_context.c
      mm/mmu_notifier.c
      mm/oom_kill.c
      virt/kvm/kvm_main.c
    
      # mmdrop_async_fn():
      include/linux/sched/mm.h
    
      # mmdrop_async():
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmget_not_zero():
      fs/userfaultfd.c
      include/linux/sched/mm.h
      mm/oom_kill.c
    
      # mmput():
      arch/arc/include/asm/mmu_context.h
      arch/arc/kernel/troubleshoot.c
      arch/frv/mm/mmu-context.c
      arch/powerpc/platforms/cell/spufs/context.c
      arch/sparc/include/asm/mmu_context_32.h
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/core/uverbs_main.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/exec.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/events/uprobes.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/oom_kill.c
      mm/process_vm_access.c
      mm/rmap.c
      mm/swapfile.c
      mm/util.c
      virt/kvm/async_pf.c
    
      # mmput_async():
      include/linux/sched/mm.h
      kernel/fork.c
      mm/oom_kill.c
    
      # get_task_mm():
      arch/arc/kernel/troubleshoot.c
      arch/powerpc/platforms/cell/spufs/context.c
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/util.c
    
      # mm_access():
      fs/proc/base.c
      include/linux/sched/mm.h
      kernel/fork.c
      mm/process_vm_access.c
    
      # mm_release():
      arch/arc/include/asm/mmu_context.h
      fs/exec.c
      include/linux/sched/mm.h
      include/uapi/linux/sched.h
      kernel/exit.c
      kernel/fork.c
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 042620729230..d29bbe0ee41f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2379,101 +2379,6 @@ static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
 	return sp;
 }
 
-/*
- * Routines for handling mm_structs
- */
-extern struct mm_struct * mm_alloc(void);
-
-/**
- * mmgrab() - Pin a &struct mm_struct.
- * @mm: The &struct mm_struct to pin.
- *
- * Make sure that @mm will not get freed even after the owning task
- * exits. This doesn't guarantee that the associated address space
- * will still exist later on and mmget_not_zero() has to be used before
- * accessing it.
- *
- * This is a preferred way to to pin @mm for a longer/unbounded amount
- * of time.
- *
- * Use mmdrop() to release the reference acquired by mmgrab().
- *
- * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
- * of &mm_struct.mm_count vs &mm_struct.mm_users.
- */
-static inline void mmgrab(struct mm_struct *mm)
-{
-	atomic_inc(&mm->mm_count);
-}
-
-/* mmdrop drops the mm and the page tables */
-extern void __mmdrop(struct mm_struct *);
-static inline void mmdrop(struct mm_struct *mm)
-{
-	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
-		__mmdrop(mm);
-}
-
-static inline void mmdrop_async_fn(struct work_struct *work)
-{
-	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
-	__mmdrop(mm);
-}
-
-static inline void mmdrop_async(struct mm_struct *mm)
-{
-	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
-		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
-		schedule_work(&mm->async_put_work);
-	}
-}
-
-/**
- * mmget() - Pin the address space associated with a &struct mm_struct.
- * @mm: The address space to pin.
- *
- * Make sure that the address space of the given &struct mm_struct doesn't
- * go away. This does not protect against parts of the address space being
- * modified or freed, however.
- *
- * Never use this function to pin this address space for an
- * unbounded/indefinite amount of time.
- *
- * Use mmput() to release the reference acquired by mmget().
- *
- * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
- * of &mm_struct.mm_count vs &mm_struct.mm_users.
- */
-static inline void mmget(struct mm_struct *mm)
-{
-	atomic_inc(&mm->mm_users);
-}
-
-static inline bool mmget_not_zero(struct mm_struct *mm)
-{
-	return atomic_inc_not_zero(&mm->mm_users);
-}
-
-/* mmput gets rid of the mappings and all user-space */
-extern void mmput(struct mm_struct *);
-#ifdef CONFIG_MMU
-/* same as above but performs the slow path from the async context. Can
- * be called from the atomic context as well
- */
-extern void mmput_async(struct mm_struct *);
-#endif
-
-/* Grab a reference to a task's mm, if it is not already going away */
-extern struct mm_struct *get_task_mm(struct task_struct *task);
-/*
- * Grab a reference to a task's mm, if it is not already going away
- * and ptrace_may_access with the mode parameter passed to it
- * succeeds.
- */
-extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
-/* Remove the current tasks stale references to the old mm_struct */
-extern void mm_release(struct task_struct *, struct mm_struct *);
-
 #ifdef CONFIG_HAVE_COPY_THREAD_TLS
 extern int copy_thread_tls(unsigned long, unsigned long, unsigned long,
 			struct task_struct *, unsigned long);

commit de8f1c77313d8c908b2897f268d466c13df161d4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:41:24 2017 +0100

    sched/headers: Move autogroup APIs into <linux/sched/autogroup.h>
    
    Further reduce the size of sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78adf7a0cac1..042620729230 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2092,24 +2092,6 @@ static inline void wake_up_nohz_cpu(int cpu) { }
 extern u64 scheduler_tick_max_deferment(void);
 #endif
 
-#ifdef CONFIG_SCHED_AUTOGROUP
-extern void sched_autogroup_create_attach(struct task_struct *p);
-extern void sched_autogroup_detach(struct task_struct *p);
-extern void sched_autogroup_fork(struct signal_struct *sig);
-extern void sched_autogroup_exit(struct signal_struct *sig);
-extern void sched_autogroup_exit_task(struct task_struct *p);
-#ifdef CONFIG_PROC_FS
-extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
-extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);
-#endif
-#else
-static inline void sched_autogroup_create_attach(struct task_struct *p) { }
-static inline void sched_autogroup_detach(struct task_struct *p) { }
-static inline void sched_autogroup_fork(struct signal_struct *sig) { }
-static inline void sched_autogroup_exit(struct signal_struct *sig) { }
-static inline void sched_autogroup_exit_task(struct task_struct *p) { }
-#endif
-
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);

commit dea38c74cb9205341f52b8d8ae18f61247a43ea8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:25:37 2017 +0100

    sched/headers: Move loadavg related definitions from <linux/sched.h> to <linux/sched/loadavg.h>
    
    Move these bits to <linux/sched/loadavg.h>, to reduce the size and
    complexity of <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c481906e835..78adf7a0cac1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -71,31 +71,6 @@ struct blk_plug;
 struct filename;
 struct nameidata;
 
-/*
- * These are the constant used to fake the fixed-point load-average
- * counting. Some notes:
- *  - 11 bit fractions expand to 22 bits by the multiplies: this gives
- *    a load-average precision of 10 bits integer + 11 bits fractional
- *  - if you want to count load-averages more often, you need more
- *    precision, or rounding will get you. With 2-second counting freq,
- *    the EXP_n values would be 1981, 2034 and 2043 if still using only
- *    11 bit fractions.
- */
-extern unsigned long avenrun[];		/* Load averages */
-extern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);
-
-#define FSHIFT		11		/* nr of bits of precision */
-#define FIXED_1		(1<<FSHIFT)	/* 1.0 as fixed-point */
-#define LOAD_FREQ	(5*HZ+1)	/* 5 sec intervals */
-#define EXP_1		1884		/* 1/exp(5sec/1min) as fixed-point */
-#define EXP_5		2014		/* 1/exp(5sec/5min) */
-#define EXP_15		2037		/* 1/exp(5sec/15min) */
-
-#define CALC_LOAD(load,exp,n) \
-	load *= exp; \
-	load += n*(FIXED_1-exp); \
-	load >>= FSHIFT;
-
 extern unsigned long total_forks;
 extern int nr_threads;
 DECLARE_PER_CPU(unsigned long, process_counts);
@@ -106,8 +81,6 @@ extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
 extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 
-extern void calc_global_load(unsigned long ticks);
-
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void cpu_load_update_nohz_start(void);
 extern void cpu_load_update_nohz_stop(void);

commit e2d1e2aec572a2138dea74d53be54a1406d419c0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:07:51 2017 +0100

    sched/headers: Move various ABI definitions to <uapi/linux/sched/types.h>
    
    Move scheduler ABI types (struct sched_attr, struct sched_param, etc.) into
    the new UAPI header.
    
    This further reduces the size and complexity of <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d6998858fa3..5c481906e835 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -5,11 +5,6 @@
 
 #include <linux/sched/prio.h>
 
-
-struct sched_param {
-	int sched_priority;
-};
-
 #include <asm/param.h>	/* for HZ */
 
 #include <linux/capability.h>
@@ -64,69 +59,8 @@ struct sched_param {
 
 #include <asm/processor.h>
 
-#define SCHED_ATTR_SIZE_VER0	48	/* sizeof first published struct */
-
-/*
- * Extended scheduling parameters data structure.
- *
- * This is needed because the original struct sched_param can not be
- * altered without introducing ABI issues with legacy applications
- * (e.g., in sched_getparam()).
- *
- * However, the possibility of specifying more than just a priority for
- * the tasks may be useful for a wide variety of application fields, e.g.,
- * multimedia, streaming, automation and control, and many others.
- *
- * This variant (sched_attr) is meant at describing a so-called
- * sporadic time-constrained task. In such model a task is specified by:
- *  - the activation period or minimum instance inter-arrival time;
- *  - the maximum (or average, depending on the actual scheduling
- *    discipline) computation time of all instances, a.k.a. runtime;
- *  - the deadline (relative to the actual activation time) of each
- *    instance.
- * Very briefly, a periodic (sporadic) task asks for the execution of
- * some specific computation --which is typically called an instance--
- * (at most) every period. Moreover, each instance typically lasts no more
- * than the runtime and must be completed by time instant t equal to
- * the instance activation time + the deadline.
- *
- * This is reflected by the actual fields of the sched_attr structure:
- *
- *  @size		size of the structure, for fwd/bwd compat.
- *
- *  @sched_policy	task's scheduling policy
- *  @sched_flags	for customizing the scheduler behaviour
- *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
- *  @sched_priority	task's static priority (SCHED_FIFO/RR)
- *  @sched_deadline	representative of the task's deadline
- *  @sched_runtime	representative of the task's runtime
- *  @sched_period	representative of the task's period
- *
- * Given this task model, there are a multiplicity of scheduling algorithms
- * and policies, that can be used to ensure all the tasks will make their
- * timing constraints.
- *
- * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
- * only user of this new interface. More information about the algorithm
- * available in the scheduling class file or in Documentation/.
- */
-struct sched_attr {
-	u32 size;
-
-	u32 sched_policy;
-	u64 sched_flags;
-
-	/* SCHED_NORMAL, SCHED_BATCH */
-	s32 sched_nice;
-
-	/* SCHED_FIFO, SCHED_RR */
-	u32 sched_priority;
-
-	/* SCHED_DEADLINE */
-	u64 sched_runtime;
-	u64 sched_deadline;
-	u64 sched_period;
-};
+struct sched_attr;
+struct sched_param;
 
 struct futex_pi_state;
 struct robust_list_head;

commit 47913d4ebd99c827c82c4f29eb282a119c3f2aeb
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:00:26 2017 +0100

    sched/headers, delayacct: Move the 'struct task_delay_info' definition from <linux/sched.h> to <linux/delayacct.h>
    
    The 'struct task_delay_info' definition does not have to be in sched.h,
    because task_struct only has a pointer to it.
    
    So move it to <linux/delayacct.h> to reduce the size of <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b42d2f9d673a..7d6998858fa3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -893,39 +893,7 @@ struct sched_info {
 };
 #endif /* CONFIG_SCHED_INFO */
 
-#ifdef CONFIG_TASK_DELAY_ACCT
-struct task_delay_info {
-	spinlock_t	lock;
-	unsigned int	flags;	/* Private per-task flags */
-
-	/* For each stat XXX, add following, aligned appropriately
-	 *
-	 * struct timespec XXX_start, XXX_end;
-	 * u64 XXX_delay;
-	 * u32 XXX_count;
-	 *
-	 * Atomicity of updates to XXX_delay, XXX_count protected by
-	 * single lock above (split into XXX_lock if contention is an issue).
-	 */
-
-	/*
-	 * XXX_count is incremented on every XXX operation, the delay
-	 * associated with the operation is added to XXX_delay.
-	 * XXX_delay contains the accumulated delay time in nanoseconds.
-	 */
-	u64 blkio_start;	/* Shared by blkio, swapin */
-	u64 blkio_delay;	/* wait for sync block io completion */
-	u64 swapin_delay;	/* wait for swapin block io completion */
-	u32 blkio_count;	/* total count of the number of sync block */
-				/* io operations performed */
-	u32 swapin_count;	/* total count of the number of swapin block */
-				/* io operations performed */
-
-	u64 freepages_start;
-	u64 freepages_delay;	/* wait for memory reclaim */
-	u32 freepages_count;	/* total count of memory reclaim */
-};
-#endif	/* CONFIG_TASK_DELAY_ACCT */
+struct task_delay_info;
 
 static inline int sched_info_on(void)
 {
@@ -1612,9 +1580,10 @@ struct task_struct {
 
 	struct page_frag task_frag;
 
-#ifdef	CONFIG_TASK_DELAY_ACCT
-	struct task_delay_info *delays;
+#ifdef CONFIG_TASK_DELAY_ACCT
+	struct task_delay_info		*delays;
 #endif
+
 #ifdef CONFIG_FAULT_INJECTION
 	int make_it_fail;
 #endif

commit 5689810360c2e88ce1619e8bcfa859852f9a1d1a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 7 12:07:18 2017 +0100

    sched/headers: Move scheduler clock interfaces to <linux/sched/clock.h>
    
    Move the sched_clock interfaces into a separate header file, to reduce
    the size of sched.h.
    
    Include <linux/sched/clock.h> in all files that made use of one of the
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5cda7cf09505..b42d2f9d673a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2190,103 +2190,6 @@ static inline void calc_load_exit_idle(void) { }
 #define cpu_relax_yield() cpu_relax()
 #endif
 
-/*
- * Do not use outside of architecture code which knows its limitations.
- *
- * sched_clock() has no promise of monotonicity or bounded drift between
- * CPUs, use (which you should not) requires disabling IRQs.
- *
- * Please use one of the three interfaces below.
- */
-extern unsigned long long notrace sched_clock(void);
-/*
- * See the comment in kernel/sched/clock.c
- */
-extern u64 running_clock(void);
-extern u64 sched_clock_cpu(int cpu);
-
-
-extern void sched_clock_init(void);
-
-#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
-static inline void sched_clock_init_late(void)
-{
-}
-
-static inline void sched_clock_tick(void)
-{
-}
-
-static inline void clear_sched_clock_stable(void)
-{
-}
-
-static inline void sched_clock_idle_sleep_event(void)
-{
-}
-
-static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
-{
-}
-
-static inline u64 cpu_clock(int cpu)
-{
-	return sched_clock();
-}
-
-static inline u64 local_clock(void)
-{
-	return sched_clock();
-}
-#else
-extern void sched_clock_init_late(void);
-/*
- * Architectures can set this to 1 if they have specified
- * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
- * but then during bootup it turns out that sched_clock()
- * is reliable after all:
- */
-extern int sched_clock_stable(void);
-extern void clear_sched_clock_stable(void);
-
-extern void sched_clock_tick(void);
-extern void sched_clock_idle_sleep_event(void);
-extern void sched_clock_idle_wakeup_event(u64 delta_ns);
-
-/*
- * As outlined in clock.c, provides a fast, high resolution, nanosecond
- * time source that is monotonic per cpu argument and has bounded drift
- * between cpus.
- *
- * ######################### BIG FAT WARNING ##########################
- * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #
- * # go backwards !!                                                  #
- * ####################################################################
- */
-static inline u64 cpu_clock(int cpu)
-{
-	return sched_clock_cpu(cpu);
-}
-
-static inline u64 local_clock(void)
-{
-	return sched_clock_cpu(raw_smp_processor_id());
-}
-#endif
-
-#ifdef CONFIG_IRQ_TIME_ACCOUNTING
-/*
- * An i/f to runtime opt-in for irq time accounting based off of sched_clock.
- * The reason for this explicit opt-in is not to have perf penalty with
- * slow sched_clocks.
- */
-extern void enable_sched_clock_irqtime(void);
-extern void disable_sched_clock_irqtime(void);
-#else
-static inline void enable_sched_clock_irqtime(void) {}
-static inline void disable_sched_clock_irqtime(void) {}
-#endif
-
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
 
@@ -2297,9 +2200,6 @@ extern void sched_exec(void);
 #define sched_exec()   {}
 #endif
 
-extern void sched_clock_idle_sleep_event(void);
-extern void sched_clock_idle_wakeup_event(u64 delta_ns);
-
 #ifdef CONFIG_HOTPLUG_CPU
 extern void idle_task_exit(void);
 #else

commit eb61baf69871b9836783a81bc451189edb0d9de2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 17:09:06 2017 +0100

    sched/headers: Move the wake-queue types and interfaces from sched.h into <linux/sched/wake_q.h>
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4f512e337584..5cda7cf09505 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -953,56 +953,6 @@ void force_schedstat_enabled(void);
 # define SCHED_FIXEDPOINT_SHIFT	10
 # define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
 
-/*
- * Wake-queues are lists of tasks with a pending wakeup, whose
- * callers have already marked the task as woken internally,
- * and can thus carry on. A common use case is being able to
- * do the wakeups once the corresponding user lock as been
- * released.
- *
- * We hold reference to each task in the list across the wakeup,
- * thus guaranteeing that the memory is still valid by the time
- * the actual wakeups are performed in wake_up_q().
- *
- * One per task suffices, because there's never a need for a task to be
- * in two wake queues simultaneously; it is forbidden to abandon a task
- * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
- * already in a wake queue, the wakeup will happen soon and the second
- * waker can just skip it.
- *
- * The DEFINE_WAKE_Q macro declares and initializes the list head.
- * wake_up_q() does NOT reinitialize the list; it's expected to be
- * called near the end of a function. Otherwise, the list can be
- * re-initialized for later re-use by wake_q_init().
- *
- * Note that this can cause spurious wakeups. schedule() callers
- * must ensure the call is done inside a loop, confirming that the
- * wakeup condition has in fact occurred.
- */
-struct wake_q_node {
-	struct wake_q_node *next;
-};
-
-struct wake_q_head {
-	struct wake_q_node *first;
-	struct wake_q_node **lastp;
-};
-
-#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)
-
-#define DEFINE_WAKE_Q(name)				\
-	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
-
-static inline void wake_q_init(struct wake_q_head *head)
-{
-	head->first = WAKE_Q_TAIL;
-	head->lastp = &head->first;
-}
-
-extern void wake_q_add(struct wake_q_head *head,
-		       struct task_struct *task);
-extern void wake_up_q(struct wake_q_head *head);
-
 struct io_context;			/* See blkdev.h */
 
 
@@ -1234,6 +1184,10 @@ enum perf_event_task_context {
 	perf_nr_task_contexts,
 };
 
+struct wake_q_node {
+	struct wake_q_node *next;
+};
+
 /* Track pages that require TLB flushes */
 struct tlbflush_unmap_batch {
 	/*

commit 5dbe91de599423d243738a205367506444ebb4a4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:47:28 2017 +0100

    sched/headers: Move idle polling methods to <linux/sched/idle.h>
    
    Further reduce the size of <linux/sched.h> by moving these APIs:
    
            tsk_is_polling()
            __current_set_polling()
            current_set_polling_and_test()
            __current_clr_polling()
            current_clr_polling_and_test()
            current_clr_polling()
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c30705c3e553..4f512e337584 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3176,82 +3176,6 @@ static inline int spin_needbreak(spinlock_t *lock)
 #endif
 }
 
-/*
- * Idle thread specific functions to determine the need_resched
- * polling state.
- */
-#ifdef TIF_POLLING_NRFLAG
-static inline int tsk_is_polling(struct task_struct *p)
-{
-	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
-}
-
-static inline void __current_set_polling(void)
-{
-	set_thread_flag(TIF_POLLING_NRFLAG);
-}
-
-static inline bool __must_check current_set_polling_and_test(void)
-{
-	__current_set_polling();
-
-	/*
-	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_curr()
-	 */
-	smp_mb__after_atomic();
-
-	return unlikely(tif_need_resched());
-}
-
-static inline void __current_clr_polling(void)
-{
-	clear_thread_flag(TIF_POLLING_NRFLAG);
-}
-
-static inline bool __must_check current_clr_polling_and_test(void)
-{
-	__current_clr_polling();
-
-	/*
-	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_curr()
-	 */
-	smp_mb__after_atomic();
-
-	return unlikely(tif_need_resched());
-}
-
-#else
-static inline int tsk_is_polling(struct task_struct *p) { return 0; }
-static inline void __current_set_polling(void) { }
-static inline void __current_clr_polling(void) { }
-
-static inline bool __must_check current_set_polling_and_test(void)
-{
-	return unlikely(tif_need_resched());
-}
-static inline bool __must_check current_clr_polling_and_test(void)
-{
-	return unlikely(tif_need_resched());
-}
-#endif
-
-static inline void current_clr_polling(void)
-{
-	__current_clr_polling();
-
-	/*
-	 * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.
-	 * Once the bit is cleared, we'll get IPIs with every new
-	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
-	 * fold.
-	 */
-	smp_mb(); /* paired with resched_curr() */
-
-	preempt_fold_need_resched();
-}
-
 static __always_inline bool need_resched(void)
 {
 	return unlikely(tif_need_resched());

commit 4437722b0486c36dc90a1adf584fca4cea6cf1de
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:58:52 2017 +0100

    sched/headers: Move the wake_up_if_idle() prototype to <linux/sched/idle.h>
    
    No need to clutter <linux/sched.h> with this rarely used prototype.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8a8252e9e0c7..c30705c3e553 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1003,8 +1003,6 @@ extern void wake_q_add(struct wake_q_head *head,
 		       struct task_struct *task);
 extern void wake_up_q(struct wake_q_head *head);
 
-extern void wake_up_if_idle(int cpu);
-
 struct io_context;			/* See blkdev.h */
 
 

commit b768917d2c08edc4b5616c86a569e524d190434b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:51:00 2017 +0100

    sched/headers: Move the 'cpu_idle_type' enum from <linux/sched.h> to <linux/sched/idle.h>
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 55480782ce7f..8a8252e9e0c7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -943,13 +943,6 @@ static inline int sched_info_on(void)
 void force_schedstat_enabled(void);
 #endif
 
-enum cpu_idle_type {
-	CPU_IDLE,
-	CPU_NOT_IDLE,
-	CPU_NEWLY_IDLE,
-	CPU_MAX_IDLE_TYPES
-};
-
 /*
  * Integer metrics need fixed point arithmetic, e.g., sched/fair
  * has a few: load, load_avg, util_avg, freq, and capacity.

commit a60b9eda67beac2318fa159545ba7d022f780736
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Move scheduler topology interfaces to <linux/sched/topology.h>
    
    The vast majority of sched.h users does not require the topology types and
    interfaces, so split them out into <linux/sched/topology.h>.
    
    This reduces the size of linux/sched.h by ~6%.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b5c6d602dfe9..55480782ce7f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -960,12 +960,6 @@ enum cpu_idle_type {
 # define SCHED_FIXEDPOINT_SHIFT	10
 # define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
 
-/*
- * Increase resolution of cpu_capacity calculations
- */
-#define SCHED_CAPACITY_SHIFT	SCHED_FIXEDPOINT_SHIFT
-#define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
-
 /*
  * Wake-queues are lists of tasks with a pending wakeup, whose
  * callers have already marked the task as woken internally,
@@ -1016,214 +1010,8 @@ extern void wake_q_add(struct wake_q_head *head,
 		       struct task_struct *task);
 extern void wake_up_q(struct wake_q_head *head);
 
-/*
- * sched-domains (multiprocessor balancing) declarations:
- */
-#ifdef CONFIG_SMP
-#define SD_LOAD_BALANCE		0x0001	/* Do load balancing on this domain. */
-#define SD_BALANCE_NEWIDLE	0x0002	/* Balance when about to become idle */
-#define SD_BALANCE_EXEC		0x0004	/* Balance on exec */
-#define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
-#define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
-#define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-#define SD_ASYM_CPUCAPACITY	0x0040  /* Groups have different max cpu capacities */
-#define SD_SHARE_CPUCAPACITY	0x0080	/* Domain members share cpu capacity */
-#define SD_SHARE_POWERDOMAIN	0x0100	/* Domain members share power domain */
-#define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
-#define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
-#define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */
-#define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
-#define SD_OVERLAP		0x2000	/* sched_domains of this level overlap */
-#define SD_NUMA			0x4000	/* cross-node balancing */
-
-#ifdef CONFIG_SCHED_SMT
-static inline int cpu_smt_flags(void)
-{
-	return SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
-}
-#endif
-
-#ifdef CONFIG_SCHED_MC
-static inline int cpu_core_flags(void)
-{
-	return SD_SHARE_PKG_RESOURCES;
-}
-#endif
-
-#ifdef CONFIG_NUMA
-static inline int cpu_numa_flags(void)
-{
-	return SD_NUMA;
-}
-#endif
-
-extern int arch_asym_cpu_priority(int cpu);
-
-struct sched_domain_attr {
-	int relax_domain_level;
-};
-
-#define SD_ATTR_INIT	(struct sched_domain_attr) {	\
-	.relax_domain_level = -1,			\
-}
-
-extern int sched_domain_level_max;
-
-struct sched_group;
-
-struct sched_domain_shared {
-	atomic_t	ref;
-	atomic_t	nr_busy_cpus;
-	int		has_idle_cores;
-};
-
-struct sched_domain {
-	/* These fields must be setup */
-	struct sched_domain *parent;	/* top domain must be null terminated */
-	struct sched_domain *child;	/* bottom domain must be null terminated */
-	struct sched_group *groups;	/* the balancing groups of the domain */
-	unsigned long min_interval;	/* Minimum balance interval ms */
-	unsigned long max_interval;	/* Maximum balance interval ms */
-	unsigned int busy_factor;	/* less balancing by factor if busy */
-	unsigned int imbalance_pct;	/* No balance until over watermark */
-	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
-	unsigned int busy_idx;
-	unsigned int idle_idx;
-	unsigned int newidle_idx;
-	unsigned int wake_idx;
-	unsigned int forkexec_idx;
-	unsigned int smt_gain;
-
-	int nohz_idle;			/* NOHZ IDLE status */
-	int flags;			/* See SD_* */
-	int level;
-
-	/* Runtime fields. */
-	unsigned long last_balance;	/* init to jiffies. units in jiffies */
-	unsigned int balance_interval;	/* initialise to 1. units in ms. */
-	unsigned int nr_balance_failed; /* initialise to 0 */
-
-	/* idle_balance() stats */
-	u64 max_newidle_lb_cost;
-	unsigned long next_decay_max_lb_cost;
-
-	u64 avg_scan_cost;		/* select_idle_sibling */
-
-#ifdef CONFIG_SCHEDSTATS
-	/* load_balance() stats */
-	unsigned int lb_count[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_failed[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_balanced[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_gained[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];
-	unsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];
-
-	/* Active load balancing */
-	unsigned int alb_count;
-	unsigned int alb_failed;
-	unsigned int alb_pushed;
-
-	/* SD_BALANCE_EXEC stats */
-	unsigned int sbe_count;
-	unsigned int sbe_balanced;
-	unsigned int sbe_pushed;
-
-	/* SD_BALANCE_FORK stats */
-	unsigned int sbf_count;
-	unsigned int sbf_balanced;
-	unsigned int sbf_pushed;
-
-	/* try_to_wake_up() stats */
-	unsigned int ttwu_wake_remote;
-	unsigned int ttwu_move_affine;
-	unsigned int ttwu_move_balance;
-#endif
-#ifdef CONFIG_SCHED_DEBUG
-	char *name;
-#endif
-	union {
-		void *private;		/* used during construction */
-		struct rcu_head rcu;	/* used during destruction */
-	};
-	struct sched_domain_shared *shared;
-
-	unsigned int span_weight;
-	/*
-	 * Span of all CPUs in this domain.
-	 *
-	 * NOTE: this field is variable length. (Allocated dynamically
-	 * by attaching extra space to the end of the structure,
-	 * depending on how many CPUs the kernel has booted up with)
-	 */
-	unsigned long span[0];
-};
-
-static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
-{
-	return to_cpumask(sd->span);
-}
-
-extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
-				    struct sched_domain_attr *dattr_new);
-
-/* Allocate an array of sched domains, for partition_sched_domains(). */
-cpumask_var_t *alloc_sched_domains(unsigned int ndoms);
-void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
-
-bool cpus_share_cache(int this_cpu, int that_cpu);
-
-typedef const struct cpumask *(*sched_domain_mask_f)(int cpu);
-typedef int (*sched_domain_flags_f)(void);
-
-#define SDTL_OVERLAP	0x01
-
-struct sd_data {
-	struct sched_domain **__percpu sd;
-	struct sched_domain_shared **__percpu sds;
-	struct sched_group **__percpu sg;
-	struct sched_group_capacity **__percpu sgc;
-};
-
-struct sched_domain_topology_level {
-	sched_domain_mask_f mask;
-	sched_domain_flags_f sd_flags;
-	int		    flags;
-	int		    numa_level;
-	struct sd_data      data;
-#ifdef CONFIG_SCHED_DEBUG
-	char                *name;
-#endif
-};
-
-extern void set_sched_topology(struct sched_domain_topology_level *tl);
 extern void wake_up_if_idle(int cpu);
 
-#ifdef CONFIG_SCHED_DEBUG
-# define SD_INIT_NAME(type)		.name = #type
-#else
-# define SD_INIT_NAME(type)
-#endif
-
-#else /* CONFIG_SMP */
-
-struct sched_domain_attr;
-
-static inline void
-partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
-			struct sched_domain_attr *dattr_new)
-{
-}
-
-static inline bool cpus_share_cache(int this_cpu, int that_cpu)
-{
-	return true;
-}
-
-#endif	/* !CONFIG_SMP */
-
-
 struct io_context;			/* See blkdev.h */
 
 

commit b69339ba109549beeaf45c45c52ac7025bfcd954
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 16:15:03 2017 +0100

    sched/headers: Prepare to remove spurious <linux/sched.h> inclusion dependencies
    
    In the following patches we are going to remove various headers
    from sched.h and other headers that sched.h includes.
    
    To make those patches build cleanly prepare the scene by adding
    dependencies to various files that learned to rely on those
    to-be-removed dependencies.
    
    These changes all make sense standalone: they add a header for
    a data type that a particular .c or .h file is using.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea05116bc3c2..b5c6d602dfe9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -18,6 +18,7 @@ struct sched_param {
 #include <linux/types.h>
 #include <linux/timex.h>
 #include <linux/jiffies.h>
+#include <linux/mutex.h>
 #include <linux/plist.h>
 #include <linux/rbtree.h>
 #include <linux/thread_info.h>

commit f361bf4a66c9bfabace46f6ff5d97005c9b524fe
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:47:37 2017 +0100

    sched/headers: Prepare for the reduction of <linux/sched.h>'s signal API dependency
    
    Instead of including the full <linux/signal.h>, we are going to include the
    types-only <linux/signal_types.h> header in <linux/sched.h>, to further
    decouple the scheduler header from the signal headers.
    
    This means that various files which relied on the full <linux/signal.h> need
    to be updated to gain an explicit dependency on it.
    
    Update the code that relies on sched.h's inclusion of the <linux/signal.h> header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8ae7b3d85658..ea05116bc3c2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -36,6 +36,7 @@ struct sched_param {
 #include <linux/signal.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
+#include <linux/signal_types.h>
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>

commit fd7712337ff09a248df424c5843c149586a3f017
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 20:56:33 2017 +0100

    sched/headers: Prepare to remove the <linux/gfp.h> include from <linux/sched.h>
    
    <linux/topology.h> is still needed - also update other headers
    and .c files that depend on sched.h including gfp.h (and its
    sub-headers) for them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e87c97e1a947..8ae7b3d85658 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -56,6 +56,7 @@ struct sched_param {
 #include <linux/llist.h>
 #include <linux/uidgid.h>
 #include <linux/gfp.h>
+#include <linux/topology.h>
 #include <linux/magic.h>
 #include <linux/cgroup-defs.h>
 

commit 314ff7851fc8ea66cbf48eaa93d8ebfb5ca084a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 11:03:31 2017 +0100

    mm/vmacache, sched/headers: Introduce 'struct vmacache' and move it from <linux/sched.h> to <linux/mm_types>
    
    The <linux/sched.h> header includes various vmacache related defines,
    which are arguably misplaced.
    
    Move them to mm_types.h and minimize the sched.h impact by putting
    all task vmacache state into a new 'struct vmacache' structure.
    
    No change in functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3f61baac928b..e87c97e1a947 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -134,10 +134,6 @@ struct blk_plug;
 struct filename;
 struct nameidata;
 
-#define VMACACHE_BITS 2
-#define VMACACHE_SIZE (1U << VMACACHE_BITS)
-#define VMACACHE_MASK (VMACACHE_SIZE - 1)
-
 /*
  * These are the constant used to fake the fixed-point load-average
  * counting. Some notes:
@@ -1550,9 +1546,10 @@ struct task_struct {
 #endif
 
 	struct mm_struct *mm, *active_mm;
-	/* per-thread vma caching */
-	u32 vmacache_seqnum;
-	struct vm_area_struct *vmacache[VMACACHE_SIZE];
+
+	/* Per-thread vma caching: */
+	struct vmacache vmacache;
+
 #if defined(SPLIT_RSS_COUNTING)
 	struct task_rss_stat	rss_stat;
 #endif

commit 780de9dd2720debc14c501dab4dc80d1f75ad50e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:50:56 2017 +0100

    sched/headers, cgroups: Remove the threadgroup_change_*() wrappery
    
    threadgroup_change_begin()/end() is a pointless wrapper around
    cgroup_threadgroup_change_begin()/end(), minus a might_sleep()
    in the !CONFIG_CGROUPS=y case.
    
    Remove the wrappery, move the might_sleep() (the down_read()
    already has a might_sleep() check).
    
    This debloats <linux/sched.h> a bit and simplifies this API.
    
    Update all call sites.
    
    No change in functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e732881517f2..3f61baac928b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3162,34 +3162,6 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 }
 
-/**
- * threadgroup_change_begin - mark the beginning of changes to a threadgroup
- * @tsk: task causing the changes
- *
- * All operations which modify a threadgroup - a new thread joining the
- * group, death of a member thread (the assertion of PF_EXITING) and
- * exec(2) dethreading the process and replacing the leader - are wrapped
- * by threadgroup_change_{begin|end}().  This is to provide a place which
- * subsystems needing threadgroup stability can hook into for
- * synchronization.
- */
-static inline void threadgroup_change_begin(struct task_struct *tsk)
-{
-	might_sleep();
-	cgroup_threadgroup_change_begin(tsk);
-}
-
-/**
- * threadgroup_change_end - mark the end of changes to a threadgroup
- * @tsk: task causing the changes
- *
- * See threadgroup_change_begin().
- */
-static inline void threadgroup_change_end(struct task_struct *tsk)
-{
-	cgroup_threadgroup_change_end(tsk);
-}
-
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 
 static inline struct thread_info *task_thread_info(struct task_struct *task)

commit 4b53a3412d6663214ce9c754eff9373a9cff9dee
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:41:03 2017 +0100

    sched/core: Remove the tsk_nr_cpus_allowed() wrapper
    
    tsk_nr_cpus_allowed() too is a pretty pointless wrapper that
    is not used consistently and which makes the code both harder
    to read and longer as well.
    
    So remove it - this also shrinks <linux/sched.h> a bit.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d1cc20cc477..e732881517f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1995,11 +1995,6 @@ static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
 }
 #endif
 
-static inline int tsk_nr_cpus_allowed(struct task_struct *p)
-{
-	return p->nr_cpus_allowed;
-}
-
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04

commit 0c98d344fe5c27f6e4bce42ac503e9e9a51c7d1d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:38:10 2017 +0100

    sched/core: Remove the tsk_cpus_allowed() wrapper
    
    So the original intention of tsk_cpus_allowed() was to 'future-proof'
    the field - but it's pretty ineffectual at that, because half of
    the code uses ->cpus_allowed directly ...
    
    Also, the wrapper makes the code longer than the original expression!
    
    So just get rid of it. This also shrinks <linux/sched.h> a bit.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index df42cac04243..6d1cc20cc477 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1995,9 +1995,6 @@ static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
 }
 #endif
 
-/* Future-safe accessor for struct task_struct's cpus_allowed. */
-#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
-
 static inline int tsk_nr_cpus_allowed(struct task_struct *p)
 {
 	return p->nr_cpus_allowed;

commit 59ddbcb2f45b958cf1f11f122b666cbcf50cd57b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:37:48 2017 +0100

    sched/core: Move the get_preempt_disable_ip() inline to sched/core.c
    
    It's defined in <linux/sched.h>, but nothing outside the scheduler
    uses it - so move it to the sched/core.c usage site.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c204613396cd..df42cac04243 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3419,15 +3419,6 @@ static inline void cond_resched_rcu(void)
 #endif
 }
 
-static inline unsigned long get_preempt_disable_ip(struct task_struct *p)
-{
-#ifdef CONFIG_DEBUG_PREEMPT
-	return p->preempt_disable_ip;
-#else
-	return 0;
-#endif
-}
-
 /*
  * Does a critical section need to be broken due to another
  * task waiting?: (technically does not depend on CONFIG_PREEMPT,

commit c930b2c0de32f45ce8f67affe936ce7a05b07b00
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 12:22:54 2017 +0100

    sched/core: Convert ___assert_task_state() link time assert to BUILD_BUG_ON()
    
    The length of TASK_STATE_TO_CHAR_STR was still checked using the old
    link-time manual error method - convert it to BUILD_BUG_ON(). This
    has a couple of advantages:
    
     - it's more obvious what's going on
    
     - it reduces the size and complexity of <linux/sched.h>
    
     - BUILD_BUG_ON() will fail during compilation, with a clearer
       error message than the link time assert.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a28deb5f210..c204613396cd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -223,9 +223,6 @@ extern void proc_sched_set_task(struct task_struct *p);
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPNn"
 
-extern char ___assert_task_state[1 - 2*!!(
-		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];
-
 /* Convenience macros for the sake of set_current_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)

commit 3fce371bfac2be0396ffc1e763600e6c6b1bb52a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:10 2017 -0800

    mm: add new mmget() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_users);/mmget\(\1\);/'
      git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_users);/mmget\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-2-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7cfa5546c840..4a28deb5f210 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2948,6 +2948,27 @@ static inline void mmdrop_async(struct mm_struct *mm)
 	}
 }
 
+/**
+ * mmget() - Pin the address space associated with a &struct mm_struct.
+ * @mm: The address space to pin.
+ *
+ * Make sure that the address space of the given &struct mm_struct doesn't
+ * go away. This does not protect against parts of the address space being
+ * modified or freed, however.
+ *
+ * Never use this function to pin this address space for an
+ * unbounded/indefinite amount of time.
+ *
+ * Use mmput() to release the reference acquired by mmget().
+ *
+ * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+ * of &mm_struct.mm_count vs &mm_struct.mm_users.
+ */
+static inline void mmget(struct mm_struct *mm)
+{
+	atomic_inc(&mm->mm_users);
+}
+
 static inline bool mmget_not_zero(struct mm_struct *mm)
 {
 	return atomic_inc_not_zero(&mm->mm_users);

commit f1f1007644ffc8051a4c11427d58b1967ae7b75a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:07 2017 -0800

    mm: add new mmgrab() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_count);/mmgrab\(\1\);/'
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_count);/mmgrab\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-1-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 451e241f32c5..7cfa5546c840 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2904,6 +2904,28 @@ static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
  */
 extern struct mm_struct * mm_alloc(void);
 
+/**
+ * mmgrab() - Pin a &struct mm_struct.
+ * @mm: The &struct mm_struct to pin.
+ *
+ * Make sure that @mm will not get freed even after the owning task
+ * exits. This doesn't guarantee that the associated address space
+ * will still exist later on and mmget_not_zero() has to be used before
+ * accessing it.
+ *
+ * This is a preferred way to to pin @mm for a longer/unbounded amount
+ * of time.
+ *
+ * Use mmdrop() to release the reference acquired by mmgrab().
+ *
+ * See also <Documentation/vm/active_mm.txt> for an in-depth explanation
+ * of &mm_struct.mm_count vs &mm_struct.mm_users.
+ */
+static inline void mmgrab(struct mm_struct *mm)
+{
+	atomic_inc(&mm->mm_count);
+}
+
 /* mmdrop drops the mm and the page tables */
 extern void __mmdrop(struct mm_struct *);
 static inline void mmdrop(struct mm_struct *mm)

commit f1ef09fde17f9b77ca1435a5b53a28b203afb81c
Merge: ef96152e6a36 ace0c791e6c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 23 20:33:51 2017 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "There is a lot here. A lot of these changes result in subtle user
      visible differences in kernel behavior. I don't expect anything will
      care but I will revert/fix things immediately if any regressions show
      up.
    
      From Seth Forshee there is a continuation of the work to make the vfs
      ready for unpriviled mounts. We had thought the previous changes
      prevented the creation of files outside of s_user_ns of a filesystem,
      but it turns we missed the O_CREAT path. Ooops.
    
      Pavel Tikhomirov and Oleg Nesterov worked together to fix a long
      standing bug in the implemenation of PR_SET_CHILD_SUBREAPER where only
      children that are forked after the prctl are considered and not
      children forked before the prctl. The only known user of this prctl
      systemd forks all children after the prctl. So no userspace
      regressions will occur. Holding earlier forked children to the same
      rules as later forked children creates a semantic that is sane enough
      to allow checkpoing of processes that use this feature.
    
      There is a long delayed change by Nikolay Borisov to limit inotify
      instances inside a user namespace.
    
      Michael Kerrisk extends the API for files used to maniuplate
      namespaces with two new trivial ioctls to allow discovery of the
      hierachy and properties of namespaces.
    
      Konstantin Khlebnikov with the help of Al Viro adds code that when a
      network namespace exits purges it's sysctl entries from the dcache. As
      in some circumstances this could use a lot of memory.
    
      Vivek Goyal fixed a bug with stacked filesystems where the permissions
      on the wrong inode were being checked.
    
      I continue previous work on ptracing across exec. Allowing a file to
      be setuid across exec while being ptraced if the tracer has enough
      credentials in the user namespace, and if the process has CAP_SETUID
      in it's own namespace. Proc files for setuid or otherwise undumpable
      executables are now owned by the root in the user namespace of their
      mm. Allowing debugging of setuid applications in containers to work
      better.
    
      A bug I introduced with permission checking and automount is now
      fixed. The big change is to mark the mounts that the kernel initiates
      as a result of an automount. This allows the permission checks in sget
      to be safely suppressed for this kind of mount. As the permission
      check happened when the original filesystem was mounted.
    
      Finally a special case in the mount namespace is removed preventing
      unbounded chains in the mount hash table, and making the semantics
      simpler which benefits CRIU.
    
      The vfs fix along with related work in ima and evm I believe makes us
      ready to finish developing and merge fully unprivileged mounts of the
      fuse filesystem. The cleanups of the mount namespace makes discussing
      how to fix the worst case complexity of umount. The stacked filesystem
      fixes pave the way for adding multiple mappings for the filesystem
      uids so that efficient and safer containers can be implemented"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      proc/sysctl: Don't grab i_lock under sysctl_lock.
      vfs: Use upper filesystem inode in bprm_fill_uid()
      proc/sysctl: prune stale dentries during unregistering
      mnt: Tuck mounts under others instead of creating shadow/side mounts.
      prctl: propagate has_child_subreaper flag to every descendant
      introduce the walk_process_tree() helper
      nsfs: Add an ioctl() to return owner UID of a userns
      fs: Better permission checking for submounts
      exit: fix the setns() && PR_SET_CHILD_SUBREAPER interaction
      vfs: open() with O_CREAT should not create inodes with unknown ids
      nsfs: Add an ioctl() to return the namespace type
      proc: Better ownership of files for non-dumpable tasks in user namespaces
      exec: Remove LSM_UNSAFE_PTRACE_CAP
      exec: Test the ptracer's saved cred to see if the tracee can gain caps
      exec: Don't reset euid and egid when the tracee has CAP_SETUID
      inotify: Convert to using per-namespace limits

commit 42e1b14b6e1455ece2ccbe474c25388d0230a590
Merge: 828cad8ea05d 95cb64c1fe61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 13:23:30 2017 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Implement wraparound-safe refcount_t and kref_t types based on
         generic atomic primitives (Peter Zijlstra)
    
       - Improve and fix the ww_mutex code (Nicolai HÃ¤hnle)
    
       - Add self-tests to the ww_mutex code (Chris Wilson)
    
       - Optimize percpu-rwsems with the 'rcuwait' mechanism (Davidlohr
         Bueso)
    
       - Micro-optimize the current-task logic all around the core kernel
         (Davidlohr Bueso)
    
       - Tidy up after recent optimizations: remove stale code and APIs,
         clean up the code (Waiman Long)
    
       - ... plus misc fixes, updates and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      fork: Fix task_struct alignment
      locking/spinlock/debug: Remove spinlock lockup detection code
      lockdep: Fix incorrect condition to print bug msgs for MAX_LOCKDEP_CHAIN_HLOCKS
      lkdtm: Convert to refcount_t testing
      kref: Implement 'struct kref' using refcount_t
      refcount_t: Introduce a special purpose refcount type
      sched/wake_q: Clarify queue reinit comment
      sched/wait, rcuwait: Fix typo in comment
      locking/mutex: Fix lockdep_assert_held() fail
      locking/rtmutex: Flip unlikely() branch to likely() in __rt_mutex_slowlock()
      locking/rwsem: Reinit wake_q after use
      locking/rwsem: Remove unnecessary atomic_long_t casts
      jump_labels: Move header guard #endif down where it belongs
      locking/atomic, kref: Implement kref_put_lock()
      locking/ww_mutex: Turn off __must_check for now
      locking/atomic, kref: Avoid more abuse
      locking/atomic, kref: Use kref_get_unless_zero() more
      locking/atomic, kref: Kill kref_sub()
      locking/atomic, kref: Add kref_read()
      locking/atomic, kref: Add KREF_INIT()
      ...

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit 0f1b92cbdd0309afae0af1963e8cccddb3d2eaff
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 30 18:06:11 2017 +0300

    introduce the walk_process_tree() helper
    
    Add the new helper to walk the process tree, the next patch adds a user.
    Note that it visits the group leaders only, proc_visitor can do
    for_each_thread itself or we can trivially extend walk_process_tree() to
    do this.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2334229167f..6261bfc12853 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3053,6 +3053,9 @@ extern bool current_is_single_threaded(void);
 #define for_each_process_thread(p, t)	\
 	for_each_process(p) for_each_thread(p, t)
 
+typedef int (*proc_visitor)(struct task_struct *p, void *data);
+void walk_process_tree(struct task_struct *top, proc_visitor, void *);
+
 static inline int get_nr_threads(struct task_struct *tsk)
 {
 	return tsk->signal->nr_threads;

commit 733ce725aa4bfa9063be053bfe7f4597d76f0dd1
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jan 27 12:38:16 2017 -0500

    sched/clock: Add dummy clear_sched_clock_stable() stub function
    
    In commit:
    
      acb04058de494 ("sched/clock: Fix hotplug crash")
    
    the PARISC code gained a call to this function.
    
    However the prototype for it is within a CONFIG_HAVE_UNSTABLE_SCHED_CLOCK=y
     #ifdef/#endif.  That, combined with this:
    
      arch/parisc/Kconfig:    select HAVE_UNSTABLE_SCHED_CLOCK if SMP
    
    means that PARISC can have it either enabled or disabled, resulting
    in the following build fail:
    
      arch/parisc/kernel/setup.c:180:2: error: implicit declaration of
      function 'clear_sched_clock_stable' [-Werror=implicit-function-declaration]
    
    Add a no-op stub for the non-SMP case to prevent this.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: acb04058de49 ("sched/clock: Fix hotplug crash")
    Link: http://lkml.kernel.org/r/20170127173816.22733-1-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d17645402767..e2ed46d3ed71 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2511,6 +2511,10 @@ static inline void sched_clock_tick(void)
 {
 }
 
+static inline void clear_sched_clock_stable(void)
+{
+}
+
 static inline void sched_clock_idle_sleep_event(void)
 {
 }

commit 0754445d71c37a7afd4f0790a9be4cf53c1b8cc4
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Jan 29 07:15:31 2017 -0800

    sched/wake_q: Clarify queue reinit comment
    
    As of:
    
      bcc9a76d5ac ("locking/rwsem: Reinit wake_q after use")
    
    the comment regarding the list reinitialization no longer applies,
    update it with the new wake_q_init() helper.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: peterz@infradead.org
    Cc: longman@redhat.com
    Cc: akpm@linux-foundation.org
    Cc: paulmck@linux.vnet.ibm.com
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/20170129151531.GA2444@linux-80c1.suse
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4e62b378bd65..1cc0dede2122 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1000,8 +1000,8 @@ enum cpu_idle_type {
  *
  * The DEFINE_WAKE_Q macro declares and initializes the list head.
  * wake_up_q() does NOT reinitialize the list; it's expected to be
- * called near the end of a function, where the fact that the queue is
- * not used again will be easy to see by inspection.
+ * called near the end of a function. Otherwise, the list can be
+ * re-initialized for later re-use by wake_q_init().
  *
  * Note that this can cause spurious wakeups. schedule() callers
  * must ensure the call is done inside a loop, confirming that the

commit 71ea47b197de9a53bc3747a8b1c667df9c6a4c68
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:36 2017 +0100

    sched/cputime: Remove temporary cputime_t accessors
    
    Now that the whole cputime conversion to nsec units is complete, we
    can remove the compatibility accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-21-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 268fdd713089..d17645402767 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -29,7 +29,6 @@ struct sched_param {
 
 #include <asm/page.h>
 #include <asm/ptrace.h>
-#include <linux/cputime.h>
 
 #include <linux/smp.h>
 #include <linux/sem.h>
@@ -613,13 +612,6 @@ struct task_cputime {
 	unsigned long long sum_exec_runtime;
 };
 
-/* Temporary type to ease cputime_t to nsecs conversion */
-struct task_cputime_t {
-	cputime_t utime;
-	cputime_t stime;
-	unsigned long long sum_exec_runtime;
-};
-
 /* Alternate field names when used to cache expirations. */
 #define virt_exp	utime
 #define prof_exp	stime
@@ -2291,27 +2283,6 @@ static inline void task_cputime_scaled(struct task_struct *t,
 }
 #endif
 
-static inline void task_cputime_t(struct task_struct *t,
-				  cputime_t *utime, cputime_t *stime)
-{
-	u64 ut, st;
-
-	task_cputime(t, &ut, &st);
-	*utime = nsecs_to_cputime(ut);
-	*stime = nsecs_to_cputime(st);
-}
-
-static inline void task_cputime_t_scaled(struct task_struct *t,
-					 cputime_t *utimescaled,
-					 cputime_t *stimescaled)
-{
-	u64 ut, st;
-
-	task_cputime_scaled(t, &ut, &st);
-	*utimescaled = nsecs_to_cputime(ut);
-	*stimescaled = nsecs_to_cputime(st);
-}
-
 extern void task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
 extern void thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
 
@@ -3527,17 +3498,6 @@ static __always_inline bool need_resched(void)
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
-static inline void thread_group_cputime_t(struct task_struct *tsk,
-					  struct task_cputime_t *cputime)
-{
-	struct task_cputime times;
-
-	thread_group_cputime(tsk, &times);
-	cputime->utime = nsecs_to_cputime(times.utime);
-	cputime->stime = nsecs_to_cputime(times.stime);
-	cputime->sum_exec_runtime = times.sum_exec_runtime;
-}
-
 /*
  * Reevaluate whether the task has signals pending delivery.
  * Wake the task if so.

commit 858cf3a8c59968e7c5f7c1a1192459a0d52d1ab4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:35 2017 +0100

    timers/itimer: Convert internal cputime_t units to nsec
    
    Use the new nsec based cputime accessors as part of the whole cputime
    conversion from cputime_t to nsecs.
    
    Also convert itimers to use nsec based internal counters. This simplifies
    it and removes the whole game with error/inc_error which served to deal
    with cputime_t random granularity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-20-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index baa6a2834e0f..268fdd713089 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -568,10 +568,8 @@ struct pacct_struct {
 };
 
 struct cpu_itimer {
-	cputime_t expires;
-	cputime_t incr;
-	u32 error;
-	u32 incr_error;
+	u64 expires;
+	u64 incr;
 };
 
 /**

commit ebd7e7fc4bc63be5eaf9da903b8060b02dd711ea
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:34 2017 +0100

    timers/posix-timers: Convert internals to use nsecs
    
    Use the new nsec based cputime accessors as part of the whole cputime
    conversion from cputime_t to nsecs.
    
    Also convert posix-cpu-timers to use nsec based internal counters to
    simplify it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-19-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc44366128d8..baa6a2834e0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -755,7 +755,7 @@ struct signal_struct {
 	struct thread_group_cputimer cputimer;
 
 	/* Earliest-expiration cache. */
-	struct task_cputime_t cputime_expires;
+	struct task_cputime cputime_expires;
 
 #ifdef CONFIG_NO_HZ_FULL
 	atomic_t tick_dep_mask;
@@ -1689,7 +1689,7 @@ struct task_struct {
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 
-	struct task_cputime_t cputime_expires;
+	struct task_cputime cputime_expires;
 	struct list_head cpu_timers[3];
 
 /* process credentials */
@@ -3527,7 +3527,7 @@ static __always_inline bool need_resched(void)
  * Thread group CPU time accounting.
  */
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
-void thread_group_cputimer(struct task_struct *tsk, struct task_cputime_t *times);
+void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
 static inline void thread_group_cputime_t(struct task_struct *tsk,
 					  struct task_cputime_t *cputime)

commit 605dc2b31a2ab235570107cb650036b41e741165
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:30 2017 +0100

    tsacct: Convert obsolete cputime type to nsecs
    
    Use the new nsec based cputime accessors as part of the whole cputime
    conversion from cputime_t to nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-15-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75fc773c158d..dc44366128d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1815,7 +1815,7 @@ struct task_struct {
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
-	cputime_t acct_timexpd;	/* stime + utime since last update */
+	u64 acct_timexpd;	/* stime + utime since last update */
 #endif
 #ifdef CONFIG_CPUSETS
 	nodemask_t mems_allowed;	/* Protected by alloc_lock */

commit d4bc42af73bf297f7182ed978b19850553242195
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:28 2017 +0100

    acct: Convert obsolete cputime type to nsecs
    
    Use the new nsec based cputime accessors as part of the whole cputime
    conversion from cputime_t to nsecs.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-13-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7ccc54b35cc..75fc773c158d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -563,7 +563,7 @@ struct pacct_struct {
 	int			ac_flag;
 	long			ac_exitcode;
 	unsigned long		ac_mem;
-	cputime_t		ac_utime, ac_stime;
+	u64			ac_utime, ac_stime;
 	unsigned long		ac_minflt, ac_majflt;
 };
 

commit 5613fda9a503cd6137b120298902a34a1386b2c1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:23 2017 +0100

    sched/cputime: Convert task/group cputime to nsecs
    
    Now that most cputime readers use the transition API which return the
    task cputime in old style cputime_t, we can safely store the cputime in
    nsecs. This will eventually make cputime statistics less opaque and more
    granular. Back and forth convertions between cputime_t and nsecs in order
    to deal with cputime_t random granularity won't be needed anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9cc722f77799..b7ccc54b35cc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -585,8 +585,8 @@ struct cpu_itimer {
  */
 struct prev_cputime {
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	cputime_t utime;
-	cputime_t stime;
+	u64 utime;
+	u64 stime;
 	raw_spinlock_t lock;
 #endif
 };
@@ -601,8 +601,8 @@ static inline void prev_cputime_init(struct prev_cputime *prev)
 
 /**
  * struct task_cputime - collected CPU time counts
- * @utime:		time spent in user mode, in &cputime_t units
- * @stime:		time spent in kernel mode, in &cputime_t units
+ * @utime:		time spent in user mode, in nanoseconds
+ * @stime:		time spent in kernel mode, in nanoseconds
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
  *
  * This structure groups together three kinds of CPU time that are tracked for
@@ -610,8 +610,8 @@ static inline void prev_cputime_init(struct prev_cputime *prev)
  * these counts together and treat all three of them in parallel.
  */
 struct task_cputime {
-	cputime_t utime;
-	cputime_t stime;
+	u64 utime;
+	u64 stime;
 	unsigned long long sum_exec_runtime;
 };
 
@@ -780,7 +780,7 @@ struct signal_struct {
 	 * in __exit_signal, except for the group leader.
 	 */
 	seqlock_t stats_lock;
-	cputime_t utime, stime, cutime, cstime;
+	u64 utime, stime, cutime, cstime;
 	u64 gtime;
 	u64 cgtime;
 	struct prev_cputime prev_cputime;
@@ -1661,9 +1661,9 @@ struct task_struct {
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
 	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
 
-	cputime_t utime, stime;
+	u64 utime, stime;
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
-	cputime_t utimescaled, stimescaled;
+	u64 utimescaled, stimescaled;
 #endif
 	u64 gtime;
 	struct prev_cputime prev_cputime;
@@ -2260,11 +2260,11 @@ struct task_struct *try_get_task_struct(struct task_struct **ptask);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 extern void task_cputime(struct task_struct *t,
-			 cputime_t *utime, cputime_t *stime);
+			 u64 *utime, u64 *stime);
 extern u64 task_gtime(struct task_struct *t);
 #else
 static inline void task_cputime(struct task_struct *t,
-				cputime_t *utime, cputime_t *stime)
+				u64 *utime, u64 *stime)
 {
 	*utime = t->utime;
 	*stime = t->stime;
@@ -2278,16 +2278,16 @@ static inline u64 task_gtime(struct task_struct *t)
 
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 static inline void task_cputime_scaled(struct task_struct *t,
-				       cputime_t *utimescaled,
-				       cputime_t *stimescaled)
+				       u64 *utimescaled,
+				       u64 *stimescaled)
 {
 	*utimescaled = t->utimescaled;
 	*stimescaled = t->stimescaled;
 }
 #else
 static inline void task_cputime_scaled(struct task_struct *t,
-				       cputime_t *utimescaled,
-				       cputime_t *stimescaled)
+				       u64 *utimescaled,
+				       u64 *stimescaled)
 {
 	task_cputime(t, utimescaled, stimescaled);
 }
@@ -2296,18 +2296,26 @@ static inline void task_cputime_scaled(struct task_struct *t,
 static inline void task_cputime_t(struct task_struct *t,
 				  cputime_t *utime, cputime_t *stime)
 {
-	task_cputime(t, utime, stime);
+	u64 ut, st;
+
+	task_cputime(t, &ut, &st);
+	*utime = nsecs_to_cputime(ut);
+	*stime = nsecs_to_cputime(st);
 }
 
 static inline void task_cputime_t_scaled(struct task_struct *t,
 					 cputime_t *utimescaled,
 					 cputime_t *stimescaled)
 {
-	task_cputime_scaled(t, utimescaled, stimescaled);
+	u64 ut, st;
+
+	task_cputime_scaled(t, &ut, &st);
+	*utimescaled = nsecs_to_cputime(ut);
+	*stimescaled = nsecs_to_cputime(st);
 }
 
-extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
-extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
+extern void task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
+extern void thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st);
 
 /*
  * Per process flags
@@ -3522,9 +3530,14 @@ void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime_t *times);
 
 static inline void thread_group_cputime_t(struct task_struct *tsk,
-					  struct task_cputime_t *times)
+					  struct task_cputime_t *cputime)
 {
-	thread_group_cputime(tsk, (struct task_cputime *)times);
+	struct task_cputime times;
+
+	thread_group_cputime(tsk, &times);
+	cputime->utime = nsecs_to_cputime(times.utime);
+	cputime->stime = nsecs_to_cputime(times.stime);
+	cputime->sum_exec_runtime = times.sum_exec_runtime;
 }
 
 /*

commit a1cecf2ba78e0a6de00ff99df34b662728535aa5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:22 2017 +0100

    sched/cputime: Introduce special task_cputime_t() API to return old-typed cputime
    
    This API returns a task's cputime in cputime_t in order to ease the
    conversion of cputime internals to use nsecs units instead. Blindly
    converting all cputime readers to use this API now will later let us
    convert more smoothly and step by step all these places to use the
    new nsec based cputime.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-7-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 252ff25983c8..9cc722f77799 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -615,6 +615,13 @@ struct task_cputime {
 	unsigned long long sum_exec_runtime;
 };
 
+/* Temporary type to ease cputime_t to nsecs conversion */
+struct task_cputime_t {
+	cputime_t utime;
+	cputime_t stime;
+	unsigned long long sum_exec_runtime;
+};
+
 /* Alternate field names when used to cache expirations. */
 #define virt_exp	utime
 #define prof_exp	stime
@@ -748,7 +755,7 @@ struct signal_struct {
 	struct thread_group_cputimer cputimer;
 
 	/* Earliest-expiration cache. */
-	struct task_cputime cputime_expires;
+	struct task_cputime_t cputime_expires;
 
 #ifdef CONFIG_NO_HZ_FULL
 	atomic_t tick_dep_mask;
@@ -1682,7 +1689,7 @@ struct task_struct {
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 
-	struct task_cputime cputime_expires;
+	struct task_cputime_t cputime_expires;
 	struct list_head cpu_timers[3];
 
 /* process credentials */
@@ -2286,6 +2293,19 @@ static inline void task_cputime_scaled(struct task_struct *t,
 }
 #endif
 
+static inline void task_cputime_t(struct task_struct *t,
+				  cputime_t *utime, cputime_t *stime)
+{
+	task_cputime(t, utime, stime);
+}
+
+static inline void task_cputime_t_scaled(struct task_struct *t,
+					 cputime_t *utimescaled,
+					 cputime_t *stimescaled)
+{
+	task_cputime_scaled(t, utimescaled, stimescaled);
+}
+
 extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 
@@ -3499,7 +3519,13 @@ static __always_inline bool need_resched(void)
  * Thread group CPU time accounting.
  */
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
-void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
+void thread_group_cputimer(struct task_struct *tsk, struct task_cputime_t *times);
+
+static inline void thread_group_cputime_t(struct task_struct *tsk,
+					  struct task_cputime_t *times)
+{
+	thread_group_cputime(tsk, (struct task_cputime *)times);
+}
 
 /*
  * Reevaluate whether the task has signals pending delivery.

commit 16a6d9be90373fb0b521850cd0185a4d460dd152
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:21 2017 +0100

    sched/cputime: Convert guest time accounting to nsecs (u64)
    
    cputime_t is being obsolete and replaced by nsecs units in order to make
    internal timestamps less opaque and more granular.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-6-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f60aed37701..252ff25983c8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -774,8 +774,8 @@ struct signal_struct {
 	 */
 	seqlock_t stats_lock;
 	cputime_t utime, stime, cutime, cstime;
-	cputime_t gtime;
-	cputime_t cgtime;
+	u64 gtime;
+	u64 cgtime;
 	struct prev_cputime prev_cputime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
@@ -1658,7 +1658,7 @@ struct task_struct {
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 	cputime_t utimescaled, stimescaled;
 #endif
-	cputime_t gtime;
+	u64 gtime;
 	struct prev_cputime prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqcount_t vtime_seqcount;
@@ -2254,7 +2254,7 @@ struct task_struct *try_get_task_struct(struct task_struct **ptask);
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 extern void task_cputime(struct task_struct *t,
 			 cputime_t *utime, cputime_t *stime);
-extern cputime_t task_gtime(struct task_struct *t);
+extern u64 task_gtime(struct task_struct *t);
 #else
 static inline void task_cputime(struct task_struct *t,
 				cputime_t *utime, cputime_t *stime)
@@ -2263,7 +2263,7 @@ static inline void task_cputime(struct task_struct *t,
 	*stime = t->stime;
 }
 
-static inline cputime_t task_gtime(struct task_struct *t)
+static inline u64 task_gtime(struct task_struct *t)
 {
 	return t->gtime;
 }

commit ba03ce822db234f8acb559de4a317a5c1f95c029
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:18 2017 +0100

    sched/cputime: Remove the unused INIT_CPUTIME macro
    
    It's a leftover from removed code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 69e6852fede1..5f60aed37701 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -620,13 +620,6 @@ struct task_cputime {
 #define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
-#define INIT_CPUTIME	\
-	(struct task_cputime) {					\
-		.utime = 0,					\
-		.stime = 0,					\
-		.sum_exec_runtime = 0,				\
-	}
-
 /*
  * This is the atomic variant of task_cputime, which can be used for
  * storing and updating task_cputime statistics without locking.

commit 9556ad6ad0c60a23a7db36af65c9ffff51bbf644
Merge: 4c45c5167c95 b18b6a9cef7f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jan 30 11:22:39 2017 +0100

    Merge branch 'fortglx/4.11/time' of https://git.linaro.org/people/john.stultz/linux into timers/core
    
     - Remove unused functions
     - Document udelay inaccuracy
     - Remove posix timer data from task struct when posix timers are off

commit b18b6a9cef7f30e9a8b7738d5fc8d568cf660855
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sat Jan 21 00:09:08 2017 -0500

    timers: Omit POSIX timer stuff from task_struct when disabled
    
    When CONFIG_POSIX_TIMERS is disabled, it is preferable to remove related
    structures from struct task_struct and struct signal_struct as they
    won't contain anything useful and shouldn't be relied upon by mistake.
    Code still referencing those structures is also disabled here.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4d1905245c7a..e8f6af59726d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -734,13 +734,14 @@ struct signal_struct {
 	unsigned int		is_child_subreaper:1;
 	unsigned int		has_child_subreaper:1;
 
+#ifdef CONFIG_POSIX_TIMERS
+
 	/* POSIX.1b Interval Timers */
 	int			posix_timer_id;
 	struct list_head	posix_timers;
 
 	/* ITIMER_REAL timer for the process */
 	struct hrtimer real_timer;
-	struct pid *leader_pid;
 	ktime_t it_real_incr;
 
 	/*
@@ -759,12 +760,16 @@ struct signal_struct {
 	/* Earliest-expiration cache. */
 	struct task_cputime cputime_expires;
 
+	struct list_head cpu_timers[3];
+
+#endif
+
+	struct pid *leader_pid;
+
 #ifdef CONFIG_NO_HZ_FULL
 	atomic_t tick_dep_mask;
 #endif
 
-	struct list_head cpu_timers[3];
-
 	struct pid *tty_old_pgrp;
 
 	/* boolean value for session group leader */
@@ -1681,8 +1686,10 @@ struct task_struct {
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 
+#ifdef CONFIG_POSIX_TIMERS
 	struct task_cputime cputime_expires;
 	struct list_head cpu_timers[3];
+#endif
 
 /* process credentials */
 	const struct cred __rcu *ptracer_cred; /* Tracer's credentials at attach */

commit 1cce1eea0aff51201753fcaca421df825b0813b6
Author: Nikolay Borisov <n.borisov.lkml@gmail.com>
Date:   Wed Dec 14 15:56:33 2016 +0200

    inotify: Convert to using per-namespace limits
    
    This patchset converts inotify to using the newly introduced
    per-userns sysctl infrastructure.
    
    Currently the inotify instances/watches are being accounted in the
    user_struct structure. This means that in setups where multiple
    users in unprivileged containers map to the same underlying
    real user (i.e. pointing to the same user_struct) the inotify limits
    are going to be shared as well, allowing one user(or application) to exhaust
    all others limits.
    
    Fix this by switching the inotify sysctls to using the
    per-namespace/per-user limits. This will allow the server admin to
    set sensible global limits, which can further be tuned inside every
    individual user namespace. Additionally, in order to preserve the
    sysctl ABI make the existing inotify instances/watches sysctls
    modify the values of the initial user namespace.
    
    Signed-off-by: Nikolay Borisov <n.borisov.lkml@gmail.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4d1905245c7a..d2334229167f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -868,10 +868,6 @@ struct user_struct {
 	atomic_t __count;	/* reference count */
 	atomic_t processes;	/* How many processes does this user have? */
 	atomic_t sigpending;	/* How many pending signals does this user have? */
-#ifdef CONFIG_INOTIFY_USER
-	atomic_t inotify_watches; /* How many inotify watches does this user have? */
-	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
-#endif
 #ifdef CONFIG_FANOTIFY
 	atomic_t fanotify_listeners;
 #endif

commit bcc9a76d5ac426bc45c9e863b1830347827ca77a
Author: Waiman Long <longman@redhat.com>
Date:   Sat Jan 21 21:33:35 2017 -0500

    locking/rwsem: Reinit wake_q after use
    
    In __rwsem_down_write_failed_common(), the same wake_q variable name
    is defined twice, with the inner wake_q hiding the one in outer scope.
    We can either use different names for the two wake_q's.
    
    Even better, we can use the same wake_q twice, if necessary.
    
    To enable the latter change, we need to define a new helper function
    wake_q_init() to enable reinitalization of wake_q after use.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1485052415-9611-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f4f9d32f12b3..4e62b378bd65 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1021,6 +1021,12 @@ struct wake_q_head {
 #define DEFINE_WAKE_Q(name)				\
 	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
 
+static inline void wake_q_init(struct wake_q_head *head)
+{
+	head->first = WAKE_Q_TAIL;
+	head->lastp = &head->first;
+}
+
 extern void wake_q_add(struct wake_q_head *head,
 		       struct task_struct *task);
 extern void wake_up_q(struct wake_q_head *head);

commit acb04058de49458010c44bb35b849d45113fd668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 19 14:36:33 2017 +0100

    sched/clock: Fix hotplug crash
    
    Mike reported that he could trigger the WARN_ON_ONCE() in
    set_sched_clock_stable() using hotplug.
    
    This exposed a fundamental problem with the interface, we should never
    mark the TSC stable if we ever find it to be unstable. Therefore
    set_sched_clock_stable() is a broken interface.
    
    The reason it existed is that not having it is a pain, it means all
    relevant architecture code needs to call clear_sched_clock_stable()
    where appropriate.
    
    Of the three architectures that select HAVE_UNSTABLE_SCHED_CLOCK ia64
    and parisc are trivial in that they never called
    set_sched_clock_stable(), so add an unconditional call to
    clear_sched_clock_stable() to them.
    
    For x86 the story is a lot more involved, and what this patch tries to
    do is ensure we preserve the status quo. So even is Cyrix or Transmeta
    have usable TSC they never called set_sched_clock_stable() so they now
    get an explicit mark unstable.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9881b024b7d7 ("sched/clock: Delay switching sched_clock to stable")
    Link: http://lkml.kernel.org/r/20170119133633.GB6536@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a8daed914eef..69e6852fede1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2547,7 +2547,6 @@ extern void sched_clock_init_late(void);
  * is reliable after all:
  */
 extern int sched_clock_stable(void);
-extern void set_sched_clock_stable(void);
 extern void clear_sched_clock_stable(void);
 
 extern void sched_clock_tick(void);

commit 10ab56434f2f633a51e432ee8b7c29e12438e163
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Oct 28 12:58:10 2016 -0400

    sched/core: Separate out io_schedule_prepare() and io_schedule_finish()
    
    Now that IO schedule accounting is done inside __schedule(),
    io_schedule() can be split into three steps - prep, schedule, and
    finish - where the schedule part doesn't need any special annotation.
    This allows marking a sleep as iowait by simply wrapping an existing
    blocking function with io_schedule_prepare() and io_schedule_finish().
    
    Because task_struct->in_iowait is single bit, the caller of
    io_schedule_prepare() needs to record and the pass its state to
    io_schedule_finish() to be safe regarding nesting.  While this isn't
    the prettiest, these functions are mostly gonna be used by core
    functions and we don't want to use more space for ->in_iowait.
    
    While at it, as it's simple to do now, reimplement io_schedule()
    without unnecessarily going through io_schedule_timeout().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adilger.kernel@dilger.ca
    Cc: jack@suse.com
    Cc: kernel-team@fb.com
    Cc: mingbo@fb.com
    Cc: tytso@mit.edu
    Link: http://lkml.kernel.org/r/1477673892-28940-3-git-send-email-tj@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 94a48bb58297..a8daed914eef 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -461,12 +461,10 @@ extern signed long schedule_timeout_idle(signed long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
 
+extern int __must_check io_schedule_prepare(void);
+extern void io_schedule_finish(int token);
 extern long io_schedule_timeout(long timeout);
-
-static inline void io_schedule(void)
-{
-	io_schedule_timeout(MAX_SCHEDULE_TIMEOUT);
-}
+extern void io_schedule(void);
 
 void __noreturn do_task_dead(void);
 

commit 9881b024b7d7671f6a014091bc96506b89081802
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 15 13:35:52 2016 +0100

    sched/clock: Delay switching sched_clock to stable
    
    Currently we switch to the stable sched_clock if we guess the TSC is
    usable, and then switch back to the unstable path if it turns out TSC
    isn't stable during SMP bringup after all.
    
    Delay switching to the stable path until after SMP bringup is
    complete. This way we'll avoid switching during the time we detect the
    worst of the TSC offences.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ad3ec9ec61f7..94a48bb58297 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2515,6 +2515,10 @@ extern u64 sched_clock_cpu(int cpu);
 extern void sched_clock_init(void);
 
 #ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
+static inline void sched_clock_init_late(void)
+{
+}
+
 static inline void sched_clock_tick(void)
 {
 }
@@ -2537,6 +2541,7 @@ static inline u64 local_clock(void)
 	return sched_clock();
 }
 #else
+extern void sched_clock_init_late(void);
 /*
  * Architectures can set this to 1 if they have specified
  * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,

commit 642fa448ae6b3a4e5e8737054a094173405b7643
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue Jan 3 13:43:14 2017 -0800

    sched/core: Remove set_task_state()
    
    This is a nasty interface and setting the state of a foreign task must
    not be done. As of the following commit:
    
      be628be0956 ("bcache: Make gc wakeup sane, remove set_task_state()")
    
    ... everyone in the kernel calls set_task_state() with current, allowing
    the helper to be removed.
    
    However, as the comment indicates, it is still around for those archs
    where computing current is more expensive than using a pointer, at least
    in theory. An important arch that is affected is arm64, however this has
    been addressed now [1] and performance is up to par making no difference
    with either calls.
    
    Of all the callers, if any, it's the locking bits that would care most
    about this -- ie: we end up passing a tsk pointer to a lot of the lock
    slowpath, and setting ->state on that. The following numbers are based
    on two tests: a custom ad-hoc microbenchmark that just measures
    latencies (for ~65 million calls) between get_task_state() vs
    get_current_state().
    
    Secondly for a higher overview, an unlink microbenchmark was used,
    which pounds on a single file with open, close,unlink combos with
    increasing thread counts (up to 4x ncpus). While the workload is quite
    unrealistic, it does contend a lot on the inode mutex or now rwsem.
    
    [1] https://lkml.kernel.org/r/1483468021-8237-1-git-send-email-mark.rutland@arm.com
    
    == 1. x86-64 ==
    
    Avg runtime set_task_state():    601 msecs
    Avg runtime set_current_state(): 552 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      36089.26 (  0.00%)    38977.33 (  8.00%)
    Hmean    unlink1-processes-5      28555.01 (  0.00%)    29832.55 (  4.28%)
    Hmean    unlink1-processes-8      37323.75 (  0.00%)    44974.57 ( 20.50%)
    Hmean    unlink1-processes-12     43571.88 (  0.00%)    44283.01 (  1.63%)
    Hmean    unlink1-processes-21     34431.52 (  0.00%)    38284.45 ( 11.19%)
    Hmean    unlink1-processes-30     34813.26 (  0.00%)    37975.17 (  9.08%)
    Hmean    unlink1-processes-48     37048.90 (  0.00%)    39862.78 (  7.59%)
    Hmean    unlink1-processes-79     35630.01 (  0.00%)    36855.30 (  3.44%)
    Hmean    unlink1-processes-110    36115.85 (  0.00%)    39843.91 ( 10.32%)
    Hmean    unlink1-processes-141    32546.96 (  0.00%)    35418.52 (  8.82%)
    Hmean    unlink1-processes-172    34674.79 (  0.00%)    36899.21 (  6.42%)
    Hmean    unlink1-processes-203    37303.11 (  0.00%)    36393.04 ( -2.44%)
    Hmean    unlink1-processes-224    35712.13 (  0.00%)    36685.96 (  2.73%)
    
    == 2. ppc64le ==
    
    Avg runtime set_task_state():  938 msecs
    Avg runtime set_current_state: 940 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      19269.19 (  0.00%)    30704.50 ( 59.35%)
    Hmean    unlink1-processes-5      20106.15 (  0.00%)    21804.15 (  8.45%)
    Hmean    unlink1-processes-8      17496.97 (  0.00%)    17243.28 ( -1.45%)
    Hmean    unlink1-processes-12     14224.15 (  0.00%)    17240.21 ( 21.20%)
    Hmean    unlink1-processes-21     14155.66 (  0.00%)    15681.23 ( 10.78%)
    Hmean    unlink1-processes-30     14450.70 (  0.00%)    15995.83 ( 10.69%)
    Hmean    unlink1-processes-48     16945.57 (  0.00%)    16370.42 ( -3.39%)
    Hmean    unlink1-processes-79     15788.39 (  0.00%)    14639.27 ( -7.28%)
    Hmean    unlink1-processes-110    14268.48 (  0.00%)    14377.40 (  0.76%)
    Hmean    unlink1-processes-141    14023.65 (  0.00%)    16271.69 ( 16.03%)
    Hmean    unlink1-processes-172    13417.62 (  0.00%)    16067.55 ( 19.75%)
    Hmean    unlink1-processes-203    15293.08 (  0.00%)    15440.40 (  0.96%)
    Hmean    unlink1-processes-234    13719.32 (  0.00%)    16190.74 ( 18.01%)
    Hmean    unlink1-processes-265    16400.97 (  0.00%)    16115.22 ( -1.74%)
    Hmean    unlink1-processes-296    14388.60 (  0.00%)    16216.13 ( 12.70%)
    Hmean    unlink1-processes-320    15771.85 (  0.00%)    15905.96 (  0.85%)
    
    x86-64 (known to be fast for get_current()/this_cpu_read_stable() caching)
    and ppc64 (with paca) show similar improvements in the unlink microbenches.
    The small delta for ppc64 (2ms), does not represent the gains on the unlink
    runs. In the case of x86, there was a decent amount of variation in the
    latency runs, but always within a 20 to 50ms increase), ppc was more constant.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: mark.rutland@arm.com
    Link: http://lkml.kernel.org/r/1483479794-14013-5-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ad3ec9ec61f7..f4f9d32f12b3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -227,7 +227,7 @@ extern void proc_sched_set_task(struct task_struct *p);
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];
 
-/* Convenience macros for the sake of set_task_state */
+/* Convenience macros for the sake of set_current_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
 #define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)
@@ -254,17 +254,6 @@ extern char ___assert_task_state[1 - 2*!!(
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
-#define __set_task_state(tsk, state_value)			\
-	do {							\
-		(tsk)->task_state_change = _THIS_IP_;		\
-		(tsk)->state = (state_value);			\
-	} while (0)
-#define set_task_state(tsk, state_value)			\
-	do {							\
-		(tsk)->task_state_change = _THIS_IP_;		\
-		smp_store_mb((tsk)->state, (state_value));	\
-	} while (0)
-
 #define __set_current_state(state_value)			\
 	do {							\
 		current->task_state_change = _THIS_IP_;		\
@@ -277,20 +266,6 @@ extern char ___assert_task_state[1 - 2*!!(
 	} while (0)
 
 #else
-
-/*
- * @tsk had better be current, or you get to keep the pieces.
- *
- * The only reason is that computing current can be more expensive than
- * using a pointer that's already available.
- *
- * Therefore, see set_current_state().
- */
-#define __set_task_state(tsk, state_value)		\
-	do { (tsk)->state = (state_value); } while (0)
-#define set_task_state(tsk, state_value)		\
-	smp_store_mb((tsk)->state, (state_value))
-
 /*
  * set_current_state() includes a barrier so that the write of current->state
  * is correctly serialised wrt the caller's subsequent test of whether to

commit 2d39b3cd34e6d323720d4c61bd714f5ae202c022
Author: Jamie Iles <jamie.iles@oracle.com>
Date:   Tue Jan 10 16:57:54 2017 -0800

    signal: protect SIGNAL_UNKILLABLE from unintentional clearing.
    
    Since commit 00cd5c37afd5 ("ptrace: permit ptracing of /sbin/init") we
    can now trace init processes.  init is initially protected with
    SIGNAL_UNKILLABLE which will prevent fatal signals such as SIGSTOP, but
    there are a number of paths during tracing where SIGNAL_UNKILLABLE can
    be implicitly cleared.
    
    This can result in init becoming stoppable/killable after tracing.  For
    example, running:
    
      while true; do kill -STOP 1; done &
      strace -p 1
    
    and then stopping strace and the kill loop will result in init being
    left in state TASK_STOPPED.  Sending SIGCONT to init will resume it, but
    init will now respond to future SIGSTOP signals rather than ignoring
    them.
    
    Make sure that when setting SIGNAL_STOP_CONTINUED/SIGNAL_STOP_STOPPED
    that we don't clear SIGNAL_UNKILLABLE.
    
    Link: http://lkml.kernel.org/r/20170104122017.25047-1-jamie.iles@oracle.com
    Signed-off-by: Jamie Iles <jamie.iles@oracle.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4d1905245c7a..ad3ec9ec61f7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -854,6 +854,16 @@ struct signal_struct {
 
 #define SIGNAL_UNKILLABLE	0x00000040 /* for init: ignore fatal signals */
 
+#define SIGNAL_STOP_MASK (SIGNAL_CLD_MASK | SIGNAL_STOP_STOPPED | \
+			  SIGNAL_STOP_CONTINUED)
+
+static inline void signal_set_stop_flags(struct signal_struct *sig,
+					 unsigned int flags)
+{
+	WARN_ON(sig->flags & (SIGNAL_GROUP_EXIT|SIGNAL_GROUP_COREDUMP));
+	sig->flags = (sig->flags & ~SIGNAL_STOP_MASK) | flags;
+}
+
 /* If true, all threads except ->group_exit_task have pending SIGKILL */
 static inline int signal_group_exit(const struct signal_struct *sig)
 {

commit eb254f323bd50ab7e3cc385f2fc641a595cc8b37
Merge: f79f7b1b4f91 76ae054c69a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 22 09:25:45 2016 -0800

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache allocation interface from Thomas Gleixner:
     "This provides support for Intel's Cache Allocation Technology, a cache
      partitioning mechanism.
    
      The interface is odd, but the hardware interface of that CAT stuff is
      odd as well.
    
      We tried hard to come up with an abstraction, but that only allows
      rather simple partitioning, but no way of sharing and dealing with the
      per package nature of this mechanism.
    
      In the end we decided to expose the allocation bitmaps directly so all
      combinations of the hardware can be utilized.
    
      There are two ways of associating a cache partition:
    
       - Task
    
         A task can be added to a resource group. It uses the cache
         partition associated to the group.
    
       - CPU
    
         All tasks which are not member of a resource group use the group to
         which the CPU they are running on is associated with.
    
         That allows for simple CPU based partitioning schemes.
    
      The main expected user sare:
    
       - Virtualization so a VM can only trash only the associated part of
         the cash w/o disturbing others
    
       - Real-Time systems to seperate RT and general workloads.
    
       - Latency sensitive enterprise workloads
    
       - In theory this also can be used to protect against cache side
         channel attacks"
    
    [ Intel RDT is "Resource Director Technology". The interface really is
      rather odd and very specific, which delayed this pull request while I
      was thinking about it. The pull request itself came in early during
      the merge window, I just delayed it until things had calmed down and I
      had more time.
    
      But people tell me they'll use this, and the good news is that it is
      _so_ specific that it's rather independent of anything else, and no
      user is going to depend on the interface since it's pretty rare. So if
      push comes to shove, we can just remove the interface and nothing will
      break ]
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (31 commits)
      x86/intel_rdt: Implement show_options() for resctrlfs
      x86/intel_rdt: Call intel_rdt_sched_in() with preemption disabled
      x86/intel_rdt: Update task closid immediately on CPU in rmdir and unmount
      x86/intel_rdt: Fix setting of closid when adding CPUs to a group
      x86/intel_rdt: Update percpu closid immeditately on CPUs affected by changee
      x86/intel_rdt: Reset per cpu closids on unmount
      x86/intel_rdt: Select KERNFS when enabling INTEL_RDT_A
      x86/intel_rdt: Prevent deadlock against hotplug lock
      x86/intel_rdt: Protect info directory from removal
      x86/intel_rdt: Add info files to Documentation
      x86/intel_rdt: Export the minimum number of set mask bits in sysfs
      x86/intel_rdt: Propagate error in rdt_mount() properly
      x86/intel_rdt: Add a missing #include
      MAINTAINERS: Add maintainer for Intel RDT resource allocation
      x86/intel_rdt: Add scheduler hook
      x86/intel_rdt: Add schemata file
      x86/intel_rdt: Add tasks files
      x86/intel_rdt: Add cpus file
      x86/intel_rdt: Add mkdir to resctrl file system
      x86/intel_rdt: Add "info" files to resctrl file system
      ...

commit 412ac77a9d3ec015524dacea905471d66480b7ac
Merge: dcdaa2f9480c 19339c251607
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 14:09:48 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "After a lot of discussion and work we have finally reachanged a basic
      understanding of what is necessary to make unprivileged mounts safe in
      the presence of EVM and IMA xattrs which the last commit in this
      series reflects. While technically it is a revert the comments it adds
      are important for people not getting confused in the future. Clearing
      up that confusion allows us to seriously work on unprivileged mounts
      of fuse in the next development cycle.
    
      The rest of the fixes in this set are in the intersection of user
      namespaces, ptrace, and exec. I started with the first fix which
      started a feedback cycle of finding additional issues during review
      and fixing them. Culiminating in a fix for a bug that has been present
      since at least Linux v1.0.
    
      Potentially these fixes were candidates for being merged during the rc
      cycle, and are certainly backport candidates but enough little things
      turned up during review and testing that I decided they should be
      handled as part of the normal development process just to be certain
      there were not any great surprises when it came time to backport some
      of these fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      Revert "evm: Translate user/group ids relative to s_user_ns when computing HMAC"
      exec: Ensure mm->user_ns contains the execed files
      ptrace: Don't allow accessing an undumpable mm
      ptrace: Capture the ptracer's creds not PT_PTRACE_CAP
      mm: Add a user_ns owner to mm_struct and fix ptrace permission checks

commit 7b9dc3f75fc8be046e76387a22a21f421ce55b53
Merge: 36869cb93d36 bbc17bb8a89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 10:41:53 2016 -0800

    Merge tag 'pm-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "Again, cpufreq gets more changes than the other parts this time (one
      new driver, one old driver less, a bunch of enhancements of the
      existing code, new CPU IDs, fixes, cleanups)
    
      There also are some changes in cpuidle (idle injection rework, a
      couple of new CPU IDs, online/offline rework in intel_idle, fixes and
      cleanups), in the generic power domains framework (mostly related to
      supporting power domains containing CPUs), and in the Operating
      Performance Points (OPP) library (mostly related to supporting devices
      with multiple voltage regulators)
    
      In addition to that, the system sleep state selection interface is
      modified to make it easier for distributions with unchanged user space
      to support suspend-to-idle as the default system suspend method, some
      issues are fixed in the PM core, the latency tolerance PM QoS
      framework is improved a bit, the Intel RAPL power capping driver is
      cleaned up and there are some fixes and cleanups in the devfreq
      subsystem
    
      Specifics:
    
       - New cpufreq driver for Broadcom STB SoCs and a Device Tree binding
         for it (Markus Mayer)
    
       - Support for ARM Integrator/AP and Integrator/CP in the generic DT
         cpufreq driver and elimination of the old Integrator cpufreq driver
         (Linus Walleij)
    
       - Support for the zx296718, r8a7743 and r8a7745, Socionext UniPhier,
         and PXA SoCs in the the generic DT cpufreq driver (Baoyou Xie,
         Geert Uytterhoeven, Masahiro Yamada, Robert Jarzmik)
    
       - cpufreq core fix to eliminate races that may lead to using inactive
         policy objects and related cleanups (Rafael Wysocki)
    
       - cpufreq schedutil governor update to make it use SCHED_FIFO kernel
         threads (instead of regular workqueues) for doing delayed work (to
         reduce the response latency in some cases) and related cleanups
         (Viresh Kumar)
    
       - New cpufreq sysfs attribute for resetting statistics (Markus Mayer)
    
       - cpufreq governors fixes and cleanups (Chen Yu, Stratos Karafotis,
         Viresh Kumar)
    
       - Support for using generic cpufreq governors in the intel_pstate
         driver (Rafael Wysocki)
    
       - Support for per-logical-CPU P-state limits and the EPP/EPB (Energy
         Performance Preference/Energy Performance Bias) knobs in the
         intel_pstate driver (Srinivas Pandruvada)
    
       - New CPU ID for Knights Mill in intel_pstate (Piotr Luc)
    
       - intel_pstate driver modification to use the P-state selection
         algorithm based on CPU load on platforms with the system profile in
         the ACPI tables set to "mobile" (Srinivas Pandruvada)
    
       - intel_pstate driver cleanups (Arnd Bergmann, Rafael Wysocki,
         Srinivas Pandruvada)
    
       - cpufreq powernv driver updates including fast switching support
         (for the schedutil governor), fixes and cleanus (Akshay Adiga,
         Andrew Donnellan, Denis Kirjanov)
    
       - acpi-cpufreq driver rework to switch it over to the new CPU
         offline/online state machine (Sebastian Andrzej Siewior)
    
       - Assorted cleanups in cpufreq drivers (Wei Yongjun, Prashanth
         Prakash)
    
       - Idle injection rework (to make it use the regular idle path instead
         of a home-grown custom one) and related powerclamp thermal driver
         updates (Peter Zijlstra, Jacob Pan, Petr Mladek, Sebastian Andrzej
         Siewior)
    
       - New CPU IDs for Atom Z34xx and Knights Mill in intel_idle (Andy
         Shevchenko, Piotr Luc)
    
       - intel_idle driver cleanups and switch over to using the new CPU
         offline/online state machine (Anna-Maria Gleixner, Sebastian
         Andrzej Siewior)
    
       - cpuidle DT driver update to support suspend-to-idle properly
         (Sudeep Holla)
    
       - cpuidle core cleanups and misc updates (Daniel Lezcano, Pan Bian,
         Rafael Wysocki)
    
       - Preliminary support for power domains including CPUs in the generic
         power domains (genpd) framework and related DT bindings (Lina Iyer)
    
       - Assorted fixes and cleanups in the generic power domains (genpd)
         framework (Colin Ian King, Dan Carpenter, Geert Uytterhoeven)
    
       - Preliminary support for devices with multiple voltage regulators
         and related fixes and cleanups in the Operating Performance Points
         (OPP) library (Viresh Kumar, Masahiro Yamada, Stephen Boyd)
    
       - System sleep state selection interface rework to make it easier to
         support suspend-to-idle as the default system suspend method
         (Rafael Wysocki)
    
       - PM core fixes and cleanups, mostly related to the interactions
         between the system suspend and runtime PM frameworks (Ulf Hansson,
         Sahitya Tummala, Tony Lindgren)
    
       - Latency tolerance PM QoS framework imorovements (Andrew Lutomirski)
    
       - New Knights Mill CPU ID for the Intel RAPL power capping driver
         (Piotr Luc)
    
       - Intel RAPL power capping driver fixes, cleanups and switch over to
         using the new CPU offline/online state machine (Jacob Pan, Thomas
         Gleixner, Sebastian Andrzej Siewior)
    
       - Fixes and cleanups in the exynos-ppmu, exynos-nocp, rk3399_dmc,
         rockchip-dfi devfreq drivers and the devfreq core (Axel Lin,
         Chanwoo Choi, Javier Martinez Canillas, MyungJoo Ham, Viresh Kumar)
    
       - Fix for false-positive KASAN warnings during resume from ACPI S3
         (suspend-to-RAM) on x86 (Josh Poimboeuf)
    
       - Memory map verification during resume from hibernation on x86 to
         ensure a consistent address space layout (Chen Yu)
    
       - Wakeup sources debugging enhancement (Xing Wei)
    
       - rockchip-io AVS driver cleanup (Shawn Lin)"
    
    * tag 'pm-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (127 commits)
      devfreq: rk3399_dmc: Don't use OPP structures outside of RCU locks
      devfreq: rk3399_dmc: Remove dangling rcu_read_unlock()
      devfreq: exynos: Don't use OPP structures outside of RCU locks
      Documentation: intel_pstate: Document HWP energy/performance hints
      cpufreq: intel_pstate: Support for energy performance hints with HWP
      cpufreq: intel_pstate: Add locking around HWP requests
      PM / sleep: Print active wakeup sources when blocking on wakeup_count reads
      PM / core: Fix bug in the error handling of async suspend
      PM / wakeirq: Fix dedicated wakeirq for drivers not using autosuspend
      PM / Domains: Fix compatible for domain idle state
      PM / OPP: Don't WARN on multiple calls to dev_pm_opp_set_regulators()
      PM / OPP: Allow platform specific custom set_opp() callbacks
      PM / OPP: Separate out _generic_set_opp()
      PM / OPP: Add infrastructure to manage multiple regulators
      PM / OPP: Pass struct dev_pm_opp_supply to _set_opp_voltage()
      PM / OPP: Manage supply's voltage/current in a separate structure
      PM / OPP: Don't use OPP structure outside of rcu protected section
      PM / OPP: Reword binding supporting multiple regulators per device
      PM / OPP: Fix incorrect cpu-supply property in binding
      cpuidle: Add a kerneldoc comment to cpuidle_use_deepest_state()
      ..

commit 3fb4afd9a504c2386b8435028d43283216bf588e
Author: Stanislav Kinsburskiy <skinsbursky@virtuozzo.com>
Date:   Mon Dec 12 16:40:42 2016 -0800

    prctl: remove one-shot limitation for changing exe link
    
    This limitation came with the reason to remove "another way for
    malicious code to obscure a compromised program and masquerade as a
    benign process" by allowing "security-concious program can use this
    prctl once during its early initialization to ensure the prctl cannot
    later be abused for this purpose":
    
        http://marc.info/?l=linux-kernel&m=133160684517468&w=2
    
    This explanation doesn't look sufficient.  The only thing "exe" link is
    indicating is the file, used to execve, which is basically nothing and
    not reliable immediately after process has returned from execve system
    call.
    
    Moreover, to use this feture, all the mappings to previous exe file have
    to be unmapped and all the new exe file permissions must be satisfied.
    
    Which means, that changing exe link is very similar to calling execve on
    the binary.
    
    The need to remove this limitations comes from migration of NFS mount
    point, which is not accessible during restore and replaced by other file
    system.  Because of this exe link has to be changed twice.
    
    [akpm@linux-foundation.org: fix up comment]
    Link: http://lkml.kernel.org/r/20160927153755.9337.69650.stgit@localhost.localdomain
    Signed-off-by: Stanislav Kinsburskiy <skinsbursky@virtuozzo.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Pavel Emelyanov <xemul@virtuozzo.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7551d3e2ab70..0e90f2973719 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -540,7 +540,11 @@ static inline int get_dumpable(struct mm_struct *mm)
 					/* leave room for more dump flags */
 #define MMF_VM_MERGEABLE	16	/* KSM may merge identical pages */
 #define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
-#define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
+/*
+ * This one-shot flag is dropped due to necessity of changing exe once again
+ * on NFS restore
+ */
+//#define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
 
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */

commit 92c020d08d83673ecd15a9069d4457378668da31
Merge: bca13ce4554a 6b94780e45c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 12:15:10 2016 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main scheduler changes in this cycle were:
    
       - support Intel Turbo Boost Max Technology 3.0 (TBM3) by introducig a
         notion of 'better cores', which the scheduler will prefer to
         schedule single threaded workloads on. (Tim Chen, Srinivas
         Pandruvada)
    
       - enhance the handling of asymmetric capacity CPUs further (Morten
         Rasmussen)
    
       - improve/fix load handling when moving tasks between task groups
         (Vincent Guittot)
    
       - simplify and clean up the cputime code (Stanislaw Gruszka)
    
       - improve mass fork()ed task spread a.k.a. hackbench speedup (Vincent
         Guittot)
    
       - make struct kthread kmalloc()ed and related fixes (Oleg Nesterov)
    
       - add uaccess atomicity debugging (when using access_ok() in the
         wrong context), under CONFIG_DEBUG_ATOMIC_SLEEP=y (Peter Zijlstra)
    
       - implement various fixes, cleanups and other enhancements (Daniel
         Bristot de Oliveira, Martin Schwidefsky, Rafael J. Wysocki)"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (41 commits)
      sched/core: Use load_avg for selecting idlest group
      sched/core: Fix find_idlest_group() for fork
      kthread: Don't abuse kthread_create_on_cpu() in __kthread_create_worker()
      kthread: Don't use to_live_kthread() in kthread_[un]park()
      kthread: Don't use to_live_kthread() in kthread_stop()
      Revert "kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function"
      kthread: Make struct kthread kmalloc'ed
      x86/uaccess, sched/preempt: Verify access_ok() context
      sched/x86: Make CONFIG_SCHED_MC_PRIO=y easier to enable
      sched/x86: Change CONFIG_SCHED_ITMT to CONFIG_SCHED_MC_PRIO
      x86/sched: Use #include <linux/mutex.h> instead of #include <asm/mutex.h>
      cpufreq/intel_pstate: Use CPPC to get max performance
      acpi/bus: Set _OSC for diverse core support
      acpi/bus: Enable HWP CPPC objects
      x86/sched: Add SD_ASYM_PACKING flags to x86 ITMT CPU
      x86/sysctl: Add sysctl for ITMT scheduling feature
      x86: Enable Intel Turbo Boost Max Technology 3.0
      x86/topology: Define x86's arch_update_cpu_topology
      sched: Extend scheduler's asym packing
      sched/fair: Clean up the tunable parameter definitions
      ...

commit 1b95b1a06cb27badb3e53329fb56af2a2113fd80
Merge: 3cded4179481 1be5d4fa0af3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 2 11:13:44 2016 +0100

    Merge branch 'locking/urgent' into locking/core, to pick up dependent fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4e28ec3d5fe0b994fe87b2406d75d9c351ef4940
Merge: a2c1bc645e87 6af33995318f
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Dec 1 14:39:51 2016 +0100

    Merge back earlier cpuidle material for v4.10.

commit c1de45ca831acee9b72c9320dde447edafadb43f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 28 23:03:05 2016 -0800

    sched/idle: Add support for tasks that inject idle
    
    Idle injection drivers such as Intel powerclamp and ACPI PAD drivers use
    realtime tasks to take control of CPU then inject idle. There are two
    issues with this approach:
    
     1. Low efficiency: injected idle task is treated as busy so sched ticks
        do not stop during injected idle period, the result of these
        unwanted wakeups can be ~20% loss in power savings.
    
     2. Idle accounting: injected idle time is presented to user as busy.
    
    This patch addresses the issues by introducing a new PF_IDLE flag which
    allows any given task to be treated as idle task while the flag is set.
    Therefore, idle injection tasks can run through the normal flow of NOHZ
    idle enter/exit to get the correct accounting as well as tick stop when
    possible.
    
    The implication is that idle task is then no longer limited to PID == 0.
    
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..114c7fcb6af6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2254,6 +2254,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 /*
  * Per process flags
  */
+#define PF_IDLE		0x00000002	/* I am an IDLE thread */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
@@ -2609,7 +2610,7 @@ extern struct task_struct *idle_task(int cpu);
  */
 static inline bool is_idle_task(const struct task_struct *p)
 {
-	return p->pid == 0;
+	return !!(p->flags & PF_IDLE);
 }
 extern struct task_struct *curr_task(int cpu);
 extern void ia64_set_curr_task(int cpu, struct task_struct *p);

commit afe06efdf07c12fd9370d5cce5383398cedf6c90
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Tue Nov 22 12:23:53 2016 -0800

    sched: Extend scheduler's asym packing
    
    We generalize the scheduler's asym packing to provide an ordering
    of the cpu beyond just the cpu number.  This allows the use of the
    ASYM_PACKING scheduler machinery to move loads to preferred CPU in a
    sched domain. The preference is defined with the cpu priority
    given by arch_asym_cpu_priority(cpu).
    
    We also record the most preferred cpu in a sched group when
    we build the cpu's capacity for fast lookup of preferred cpu
    during load balancing.
    
    Co-developed-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-pm@vger.kernel.org
    Cc: jolsa@redhat.com
    Cc: rjw@rjwysocki.net
    Cc: linux-acpi@vger.kernel.org
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/0e73ae12737dfaafa46c07066cc7c5d3f1675e46.1479844244.git.tim.c.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 19abba04ceca..fe9a499d5aa4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1077,6 +1077,8 @@ static inline int cpu_numa_flags(void)
 }
 #endif
 
+extern int arch_asym_cpu_priority(int cpu);
+
 struct sched_domain_attr {
 	int relax_domain_level;
 };

commit ec84f0056711efe93f034c86dd65e0de8d3531ff
Merge: d03266910a53 23400ac99706
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 23 10:23:09 2016 +0100

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 64b875f7ac8a5d60a4e191479299e931ee949b67
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 14 18:48:07 2016 -0600

    ptrace: Capture the ptracer's creds not PT_PTRACE_CAP
    
    When the flag PT_PTRACE_CAP was added the PTRACE_TRACEME path was
    overlooked.  This can result in incorrect behavior when an application
    like strace traces an exec of a setuid executable.
    
    Further PT_PTRACE_CAP does not have enough information for making good
    security decisions as it does not report which user namespace the
    capability is in.  This has already allowed one mistake through
    insufficient granulariy.
    
    I found this issue when I was testing another corner case of exec and
    discovered that I could not get strace to set PT_PTRACE_CAP even when
    running strace as root with a full set of caps.
    
    This change fixes the above issue with strace allowing stracing as
    root a setuid executable without disabling setuid.  More fundamentaly
    this change allows what is allowable at all times, by using the correct
    information in it's decision.
    
    Cc: stable@vger.kernel.org
    Fixes: 4214e42f96d4 ("v2.4.9.11 -> v2.4.9.12")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..e9f693598e15 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1656,6 +1656,7 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
+	const struct cred __rcu *ptracer_cred; /* Tracer's credentials at attach */
 	const struct cred __rcu *real_cred; /* objective and real subjective task
 					 * credentials (COW) */
 	const struct cred __rcu *cred;	/* effective (overridable) subjective task

commit d9345c65eb7930ac6755cf593ee7686f4029ccf4
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Wed Nov 2 05:08:28 2016 -0400

    sched/core: Introduce the vcpu_is_preempted(cpu) interface
    
    This patch is the first step to add support to improve lock holder
    preemption beaviour.
    
    vcpu_is_preempted(cpu) does the obvious thing: it tells us whether a
    vCPU is preempted or not.
    
    Defaults to false on architectures that don't support it.
    
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ Translated the changelog to English. ]
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: boqun.feng@gmail.com
    Cc: bsingharora@gmail.com
    Cc: dave@stgolabs.net
    Cc: kernellwp@gmail.com
    Cc: konrad.wilk@oracle.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: paulmck@linux.vnet.ibm.com
    Cc: paulus@samba.org
    Cc: rkrcmar@redhat.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: will.deacon@arm.com
    Cc: xen-devel-request@lists.xenproject.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1478077718-37424-2-git-send-email-xinhui.pan@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc37cbe2b13c..37261afbf16a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3510,6 +3510,18 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 
 #endif /* CONFIG_SMP */
 
+/*
+ * In order to reduce various lock holder preemption latencies provide an
+ * interface to see if a vCPU is currently running or not.
+ *
+ * This allows us to terminate optimistic spin loops and block, analogous to
+ * the native optimistic spin heuristic of testing if the lock owner task is
+ * running or not.
+ */
+#ifndef vcpu_is_preempted
+# define vcpu_is_preempted(cpu)	false
+#endif
+
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 

commit 8e5bfa8c1f8471aa4a2d30be631ef2b50e10abaf
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Nov 14 19:46:12 2016 +0100

    sched/autogroup: Do not use autogroup->tg in zombie threads
    
    Exactly because for_each_thread() in autogroup_move_group() can't see it
    and update its ->sched_task_group before _put() and possibly free().
    
    So the exiting task needs another sched_move_task() before exit_notify()
    and we need to re-introduce the PF_EXITING (or similar) check removed by
    the previous change for another reason.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: hartsjc@redhat.com
    Cc: vbendel@redhat.com
    Cc: vlovejoy@redhat.com
    Link: http://lkml.kernel.org/r/20161114184612.GA15968@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..e9c009dc3a4a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2567,6 +2567,7 @@ extern void sched_autogroup_create_attach(struct task_struct *p);
 extern void sched_autogroup_detach(struct task_struct *p);
 extern void sched_autogroup_fork(struct signal_struct *sig);
 extern void sched_autogroup_exit(struct signal_struct *sig);
+extern void sched_autogroup_exit_task(struct task_struct *p);
 #ifdef CONFIG_PROC_FS
 extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
 extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);
@@ -2576,6 +2577,7 @@ static inline void sched_autogroup_create_attach(struct task_struct *p) { }
 static inline void sched_autogroup_detach(struct task_struct *p) { }
 static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 static inline void sched_autogroup_exit(struct signal_struct *sig) { }
+static inline void sched_autogroup_exit_task(struct task_struct *p) { }
 #endif
 
 extern int yield_to(struct task_struct *p, bool preempt);

commit 194a6b5b9cb6b91a5f7d86984165a3bc55188599
Author: Waiman Long <longman@redhat.com>
Date:   Thu Nov 17 11:46:38 2016 -0500

    sched/wake_q: Rename WAKE_Q to DEFINE_WAKE_Q
    
    Currently the wake_q data structure is defined by the WAKE_Q() macro.
    This macro, however, looks like a function doing something as "wake" is
    a verb. Even checkpatch.pl was confused as it reported warnings like
    
      WARNING: Missing a blank line after declarations
      #548: FILE: kernel/futex.c:3665:
      +     int ret;
      +     WAKE_Q(wake_q);
    
    This patch renames the WAKE_Q() macro to DEFINE_WAKE_Q() which clarifies
    what the macro is doing and eliminates the checkpatch.pl warnings.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1479401198-1765-1-git-send-email-longman@redhat.com
    [ Resolved conflict and added missing rename. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c1aa3b02f6ac..dc37cbe2b13c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -989,7 +989,7 @@ enum cpu_idle_type {
  * already in a wake queue, the wakeup will happen soon and the second
  * waker can just skip it.
  *
- * The WAKE_Q macro declares and initializes the list head.
+ * The DEFINE_WAKE_Q macro declares and initializes the list head.
  * wake_up_q() does NOT reinitialize the list; it's expected to be
  * called near the end of a function, where the fact that the queue is
  * not used again will be easy to see by inspection.
@@ -1009,7 +1009,7 @@ struct wake_q_head {
 
 #define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)
 
-#define WAKE_Q(name)					\
+#define DEFINE_WAKE_Q(name)				\
 	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
 
 extern void wake_q_add(struct wake_q_head *head,

commit 6d0d287891a022ebba572327cbd70b5de69a63a2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 16 13:23:05 2016 +0100

    locking/core: Provide common cpu_relax_yield() definition
    
    No need to duplicate the same define everywhere. Since
    the only user is stop-machine and the only provider is
    s390, we can use a default implementation of cpu_relax_yield()
    in sched.h.
    
    Suggested-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-s390 <linux-s390@vger.kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: sparclinux@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1479298985-191589-1-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..c1aa3b02f6ac 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2444,6 +2444,10 @@ static inline void calc_load_enter_idle(void) { }
 static inline void calc_load_exit_idle(void) { }
 #endif /* CONFIG_NO_HZ_COMMON */
 
+#ifndef cpu_relax_yield
+#define cpu_relax_yield() cpu_relax()
+#endif
+
 /*
  * Do not use outside of architecture code which knows its limitations.
  *

commit 353c50ebe329daaf2c94dc41c1c481cbba2a31fd
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Tue Nov 15 03:06:52 2016 +0100

    sched/cputime: Simplify task_cputime()
    
    Now since fetch_task_cputime() has no other users than task_cputime(),
    its code could be used directly in task_cputime().
    
    Moreover since only 2 task_cputime() calls of 17 use a NULL argument,
    we can add dummy variables to those calls and remove NULL checks from
    task_cputimes().
    
    Also remove NULL checks from task_cputimes_scaled().
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1479175612-14718-5-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f72e81395dac..fe3ce46cfd03 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2248,10 +2248,8 @@ extern cputime_t task_gtime(struct task_struct *t);
 static inline void task_cputime(struct task_struct *t,
 				cputime_t *utime, cputime_t *stime)
 {
-	if (utime)
-		*utime = t->utime;
-	if (stime)
-		*stime = t->stime;
+	*utime = t->utime;
+	*stime = t->stime;
 }
 
 static inline cputime_t task_gtime(struct task_struct *t)
@@ -2265,10 +2263,8 @@ static inline void task_cputime_scaled(struct task_struct *t,
 				       cputime_t *utimescaled,
 				       cputime_t *stimescaled)
 {
-	if (utimescaled)
-		*utimescaled = t->utimescaled;
-	if (stimescaled)
-		*stimescaled = t->stimescaled;
+	*utimescaled = t->utimescaled;
+	*stimescaled = t->stimescaled;
 }
 #else
 static inline void task_cputime_scaled(struct task_struct *t,

commit 40565b5aedd6d0ca88b7dfd3859d709d2f6f8cf9
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Tue Nov 15 03:06:51 2016 +0100

    sched/cputime, powerpc, s390: Make scaled cputime arch specific
    
    Only s390 and powerpc have hardware facilities allowing to measure
    cputimes scaled by frequency. On all other architectures
    utimescaled/stimescaled are equal to utime/stime (however they are
    accounted separately).
    
    Remove {u,s}timescaled accounting on all architectures except
    powerpc and s390, where those values are explicitly accounted
    in the proper places.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161031162143.GB12646@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3762fe4e3a80..f72e81395dac 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1647,7 +1647,10 @@ struct task_struct {
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
 	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
 
-	cputime_t utime, stime, utimescaled, stimescaled;
+	cputime_t utime, stime;
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
+	cputime_t utimescaled, stimescaled;
+#endif
 	cputime_t gtime;
 	struct prev_cputime prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
@@ -2240,8 +2243,6 @@ struct task_struct *try_get_task_struct(struct task_struct **ptask);
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 extern void task_cputime(struct task_struct *t,
 			 cputime_t *utime, cputime_t *stime);
-extern void task_cputime_scaled(struct task_struct *t,
-				cputime_t *utimescaled, cputime_t *stimescaled);
 extern cputime_t task_gtime(struct task_struct *t);
 #else
 static inline void task_cputime(struct task_struct *t,
@@ -2253,6 +2254,13 @@ static inline void task_cputime(struct task_struct *t,
 		*stime = t->stime;
 }
 
+static inline cputime_t task_gtime(struct task_struct *t)
+{
+	return t->gtime;
+}
+#endif
+
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 static inline void task_cputime_scaled(struct task_struct *t,
 				       cputime_t *utimescaled,
 				       cputime_t *stimescaled)
@@ -2262,12 +2270,15 @@ static inline void task_cputime_scaled(struct task_struct *t,
 	if (stimescaled)
 		*stimescaled = t->stimescaled;
 }
-
-static inline cputime_t task_gtime(struct task_struct *t)
+#else
+static inline void task_cputime_scaled(struct task_struct *t,
+				       cputime_t *utimescaled,
+				       cputime_t *stimescaled)
 {
-	return t->gtime;
+	task_cputime(t, utimescaled, stimescaled);
 }
 #endif
+
 extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 

commit e02737d5b82640497637d18428e2793bb7f02881
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Fri Oct 28 15:04:46 2016 -0700

    x86/intel_rdt: Add tasks files
    
    The root directory all subdirectories are automatically populated with a
    read/write (mode 0644) file named "tasks". When read it will show all the
    task IDs assigned to the resource group. Tasks can be added (one at a time)
    to a group by writing the task ID to the file.  E.g.
    
    Membership in a resource group is indicated by a new field in the
    task_struct "int closid" which holds the CLOSID for each task. The default
    resource group uses CLOSID=0 which means that all existing tasks when the
    resctrl file system is mounted belong to the default group.
    
    If a group is removed, tasks which are members of that group are moved to
    the default group.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Cc: "Ravi V Shankar" <ravi.v.shankar@intel.com>
    Cc: "Tony Luck" <tony.luck@intel.com>
    Cc: "Shaohua Li" <shli@fb.com>
    Cc: "Sai Prakhya" <sai.praneeth.prakhya@intel.com>
    Cc: "Peter Zijlstra" <peterz@infradead.org>
    Cc: "Stephane Eranian" <eranian@google.com>
    Cc: "Dave Hansen" <dave.hansen@intel.com>
    Cc: "David Carrillo-Cisneros" <davidcc@google.com>
    Cc: "Nilay Vaish" <nilayvaish@gmail.com>
    Cc: "Vikas Shivappa" <vikas.shivappa@linux.intel.com>
    Cc: "Ingo Molnar" <mingo@elte.hu>
    Cc: "Borislav Petkov" <bp@suse.de>
    Cc: "H. Peter Anvin" <h.peter.anvin@intel.com>
    Link: http://lkml.kernel.org/r/1477692289-37412-8-git-send-email-fenghua.yu@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..c8f4152e7265 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1791,6 +1791,9 @@ struct task_struct {
 	/* cg_list protected by css_set_lock and tsk->alloc_lock */
 	struct list_head cg_list;
 #endif
+#ifdef CONFIG_INTEL_RDT_A
+	int closid;
+#endif
 #ifdef CONFIG_FUTEX
 	struct robust_list_head __user *robust_list;
 #ifdef CONFIG_COMPAT

commit a225023828038a1aaea876a65313c863ec23fa44
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 19 15:45:27 2016 +0200

    sched/core: Explain sleep/wakeup in a better way
    
    There were a few questions wrt. how sleep-wakeup works. Try and explain
    it more.
    
    Requested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 348f51b0ec92..3762fe4e3a80 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -262,20 +262,9 @@ extern char ___assert_task_state[1 - 2*!!(
 #define set_task_state(tsk, state_value)			\
 	do {							\
 		(tsk)->task_state_change = _THIS_IP_;		\
-		smp_store_mb((tsk)->state, (state_value));		\
+		smp_store_mb((tsk)->state, (state_value));	\
 	} while (0)
 
-/*
- * set_current_state() includes a barrier so that the write of current->state
- * is correctly serialised wrt the caller's subsequent test of whether to
- * actually sleep:
- *
- *	set_current_state(TASK_UNINTERRUPTIBLE);
- *	if (do_i_need_to_sleep())
- *		schedule();
- *
- * If the caller does not need such serialisation then use __set_current_state()
- */
 #define __set_current_state(state_value)			\
 	do {							\
 		current->task_state_change = _THIS_IP_;		\
@@ -284,11 +273,19 @@ extern char ___assert_task_state[1 - 2*!!(
 #define set_current_state(state_value)				\
 	do {							\
 		current->task_state_change = _THIS_IP_;		\
-		smp_store_mb(current->state, (state_value));		\
+		smp_store_mb(current->state, (state_value));	\
 	} while (0)
 
 #else
 
+/*
+ * @tsk had better be current, or you get to keep the pieces.
+ *
+ * The only reason is that computing current can be more expensive than
+ * using a pointer that's already available.
+ *
+ * Therefore, see set_current_state().
+ */
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
 #define set_task_state(tsk, state_value)		\
@@ -299,11 +296,34 @@ extern char ___assert_task_state[1 - 2*!!(
  * is correctly serialised wrt the caller's subsequent test of whether to
  * actually sleep:
  *
+ *   for (;;) {
  *	set_current_state(TASK_UNINTERRUPTIBLE);
- *	if (do_i_need_to_sleep())
- *		schedule();
+ *	if (!need_sleep)
+ *		break;
+ *
+ *	schedule();
+ *   }
+ *   __set_current_state(TASK_RUNNING);
+ *
+ * If the caller does not need such serialisation (because, for instance, the
+ * condition test and condition change and wakeup are under the same lock) then
+ * use __set_current_state().
+ *
+ * The above is typically ordered against the wakeup, which does:
+ *
+ *	need_sleep = false;
+ *	wake_up_state(p, TASK_UNINTERRUPTIBLE);
+ *
+ * Where wake_up_state() (and all other wakeup primitives) imply enough
+ * barriers to order the store of the variable against wakeup.
+ *
+ * Wakeup will do: if (@state & p->state) p->state = TASK_RUNNING, that is,
+ * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a
+ * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).
+ *
+ * This is obviously fine, since they both store the exact same value.
  *
- * If the caller does not need such serialisation then use __set_current_state()
+ * Also see the comments of try_to_wake_up().
  */
 #define __set_current_state(state_value)		\
 	do { current->state = (state_value); } while (0)

commit 6fcb52a56ff60d240f06296b12827e7f20d45f63
Author: Aaron Lu <aaron.lu@intel.com>
Date:   Fri Oct 7 17:00:08 2016 -0700

    thp: reduce usage of huge zero page's atomic counter
    
    The global zero page is used to satisfy an anonymous read fault.  If
    THP(Transparent HugePage) is enabled then the global huge zero page is
    used.  The global huge zero page uses an atomic counter for reference
    counting and is allocated/freed dynamically according to its counter
    value.
    
    CPU time spent on that counter will greatly increase if there are a lot
    of processes doing anonymous read faults.  This patch proposes a way to
    reduce the access to the global counter so that the CPU load can be
    reduced accordingly.
    
    To do this, a new flag of the mm_struct is introduced:
    MMF_USED_HUGE_ZERO_PAGE.  With this flag, the process only need to touch
    the global counter in two cases:
    
     1 The first time it uses the global huge zero page;
     2 The time when mm_user of its mm_struct reaches zero.
    
    Note that right now, the huge zero page is eligible to be freed as soon
    as its last use goes away.  With this patch, the page will not be
    eligible to be freed until the exit of the last process from which it
    was ever used.
    
    And with the use of mm_user, the kthread is not eligible to use huge
    zero page either.  Since no kthread is using huge zero page today, there
    is no difference after applying this patch.  But if that is not desired,
    I can change it to when mm_count reaches zero.
    
    Case used for test on Haswell EP:
    
      usemem -n 72 --readonly -j 0x200000 100G
    
    Which spawns 72 processes and each will mmap 100G anonymous space and
    then do read only access to that space sequentially with a step of 2MB.
    
      CPU cycles from perf report for base commit:
          54.03%  usemem   [kernel.kallsyms]   [k] get_huge_zero_page
      CPU cycles from perf report for this commit:
           0.11%  usemem   [kernel.kallsyms]   [k] mm_get_huge_zero_page
    
    Performance(throughput) of the workload for base commit: 1784430792
    Performance(throughput) of the workload for this commit: 4726928591
    164% increase.
    
    Runtime of the workload for base commit: 707592 us
    Runtime of the workload for this commit: 303970 us
    50% drop.
    
    Link: http://lkml.kernel.org/r/fe51a88f-446a-4622-1363-ad1282d71385@intel.com
    Signed-off-by: Aaron Lu <aaron.lu@intel.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Ebru Akagunduz <ebru.akagunduz@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6bee6f988912..348f51b0ec92 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -526,6 +526,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 #define MMF_OOM_SKIP		21	/* mm is of no interest for the OOM killer */
 #define MMF_UNSTABLE		22	/* mm is unstable for copy_from_user */
+#define MMF_HUGE_ZERO_PAGE	23      /* mm has ever used the global huge zero page */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit 3f70dc38cec2ad6e5355f80c4c7a15a3f7e97a19
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:59:06 2016 -0700

    mm: make sure that kthreads will not refault oom reaped memory
    
    There are only few use_mm() users in the kernel right now.  Most of them
    write to the target memory but vhost driver relies on
    copy_from_user/get_user from a kernel thread context.  This makes it
    impossible to reap the memory of an oom victim which shares the mm with
    the vhost kernel thread because it could see a zero page unexpectedly
    and theoretically make an incorrect decision visible outside of the
    killed task context.
    
    To quote Michael S. Tsirkin:
    : Getting an error from __get_user and friends is handled gracefully.
    : Getting zero instead of a real value will cause userspace
    : memory corruption.
    
    The vhost kernel thread is bound to an open fd of the vhost device which
    is not tight to the mm owner life cycle in general.  The device fd can
    be inherited or passed over to another process which means that we
    really have to be careful about unexpected memory corruption because
    unlike for normal oom victims the result will be visible outside of the
    oom victim context.
    
    Make sure that no kthread context (users of use_mm) can ever see
    corrupted data because of the oom reaper and hook into the page fault
    path by checking MMF_UNSTABLE mm flag.  __oom_reap_task_mm will set the
    flag before it starts unmapping the address space while the flag is
    checked after the page fault has been handled.  If the flag is set then
    SIGBUS is triggered so any g-u-p user will get a error code.
    
    Regular tasks do not need this protection because all which share the mm
    are killed when the mm is reaped and so the corruption will not outlive
    them.
    
    This patch shouldn't have any visible effect at this moment because the
    OOM killer doesn't invoke oom reaper for tasks with mm shared with
    kthreads yet.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-9-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af0721364788..6bee6f988912 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -525,6 +525,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 #define MMF_OOM_SKIP		21	/* mm is of no interest for the OOM killer */
+#define MMF_UNSTABLE		22	/* mm is unstable for copy_from_user */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit 862e3073b3eed13f17bd6be6ca6052db15c0b728
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:57 2016 -0700

    mm, oom: get rid of signal_struct::oom_victims
    
    After "oom: keep mm of the killed task available" we can safely detect
    an oom victim by checking task->signal->oom_mm so we do not need the
    signal_struct counter anymore so let's get rid of it.
    
    This alone wouldn't be sufficient for nommu archs because
    exit_oom_victim doesn't hide the process from the oom killer anymore.
    We can, however, mark the mm with a MMF flag in __mmput.  We can reuse
    MMF_OOM_REAPED and rename it to a more generic MMF_OOM_SKIP.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-6-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c4b588358296..af0721364788 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -524,7 +524,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
-#define MMF_OOM_REAPED		21	/* mm has been already reaped */
+#define MMF_OOM_SKIP		21	/* mm is of no interest for the OOM killer */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
@@ -672,7 +672,6 @@ struct signal_struct {
 	atomic_t		sigcnt;
 	atomic_t		live;
 	int			nr_threads;
-	atomic_t oom_victims; /* # of TIF_MEDIE threads in this thread group */
 	struct list_head	thread_head;
 
 	wait_queue_head_t	wait_chldexit;	/* for wait4() */

commit 7283094ec3db318e87ec9e31cf75f136ac2a4dd3
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:54 2016 -0700

    kernel, oom: fix potential pgd_lock deadlock from __mmdrop
    
    Lockdep complains that __mmdrop is not safe from the softirq context:
    
      =================================
      [ INFO: inconsistent lock state ]
      4.6.0-oomfortification2-00011-geeb3eadeab96-dirty #949 Tainted: G        W
      ---------------------------------
      inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
      swapper/1/0 [HC0[0]:SC1[1]:HE1:SE0] takes:
       (pgd_lock){+.?...}, at: pgd_free+0x19/0x6b
      {SOFTIRQ-ON-W} state was registered at:
         __lock_acquire+0xa06/0x196e
         lock_acquire+0x139/0x1e1
         _raw_spin_lock+0x32/0x41
         __change_page_attr_set_clr+0x2a5/0xacd
         change_page_attr_set_clr+0x16f/0x32c
         set_memory_nx+0x37/0x3a
         free_init_pages+0x9e/0xc7
         alternative_instructions+0xa2/0xb3
         check_bugs+0xe/0x2d
         start_kernel+0x3ce/0x3ea
         x86_64_start_reservations+0x2a/0x2c
         x86_64_start_kernel+0x17a/0x18d
      irq event stamp: 105916
      hardirqs last  enabled at (105916): free_hot_cold_page+0x37e/0x390
      hardirqs last disabled at (105915): free_hot_cold_page+0x2c1/0x390
      softirqs last  enabled at (105878): _local_bh_enable+0x42/0x44
      softirqs last disabled at (105879): irq_exit+0x6f/0xd1
    
      other info that might help us debug this:
       Possible unsafe locking scenario:
    
             CPU0
             ----
        lock(pgd_lock);
        <Interrupt>
          lock(pgd_lock);
    
       *** DEADLOCK ***
    
      1 lock held by swapper/1/0:
       #0:  (rcu_callback){......}, at: rcu_process_callbacks+0x390/0x800
    
      stack backtrace:
      CPU: 1 PID: 0 Comm: swapper/1 Tainted: G        W       4.6.0-oomfortification2-00011-geeb3eadeab96-dirty #949
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Debian-1.8.2-1 04/01/2014
      Call Trace:
       <IRQ>
        print_usage_bug.part.25+0x259/0x268
        mark_lock+0x381/0x567
        __lock_acquire+0x993/0x196e
        lock_acquire+0x139/0x1e1
        _raw_spin_lock+0x32/0x41
        pgd_free+0x19/0x6b
        __mmdrop+0x25/0xb9
        __put_task_struct+0x103/0x11e
        delayed_put_task_struct+0x157/0x15e
        rcu_process_callbacks+0x660/0x800
        __do_softirq+0x1ec/0x4d5
        irq_exit+0x6f/0xd1
        smp_apic_timer_interrupt+0x42/0x4d
        apic_timer_interrupt+0x8e/0xa0
       <EOI>
        arch_cpu_idle+0xf/0x11
        default_idle_call+0x32/0x34
        cpu_startup_entry+0x20c/0x399
        start_secondary+0xfe/0x101
    
    More over commit a79e53d85683 ("x86/mm: Fix pgd_lock deadlock") was
    explicit about pgd_lock not to be called from the irq context.  This
    means that __mmdrop called from free_signal_struct has to be postponed
    to a user context.  We already have a similar mechanism for mmput_async
    so we can use it here as well.  This is safe because mm_count is pinned
    by mm_users.
    
    This fixes bug introduced by "oom: keep mm of the killed task available"
    
    Link: http://lkml.kernel.org/r/1472119394-11342-5-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67ea79610e67..c4b588358296 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2877,6 +2877,20 @@ static inline void mmdrop(struct mm_struct *mm)
 		__mmdrop(mm);
 }
 
+static inline void mmdrop_async_fn(struct work_struct *work)
+{
+	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
+	__mmdrop(mm);
+}
+
+static inline void mmdrop_async(struct mm_struct *mm)
+{
+	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+		schedule_work(&mm->async_put_work);
+	}
+}
+
 static inline bool mmget_not_zero(struct mm_struct *mm)
 {
 	return atomic_inc_not_zero(&mm->mm_users);

commit 26db62f179d112d345031e14926a4cda9cd40d6e
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:51 2016 -0700

    oom: keep mm of the killed task available
    
    oom_reap_task has to call exit_oom_victim in order to make sure that the
    oom vicim will not block the oom killer for ever.  This is, however,
    opening new problems (e.g oom_killer_disable exclusion - see commit
    74070542099c ("oom, suspend: fix oom_reaper vs.  oom_killer_disable
    race")).  exit_oom_victim should be only called from the victim's
    context ideally.
    
    One way to achieve this would be to rely on per mm_struct flags.  We
    already have MMF_OOM_REAPED to hide a task from the oom killer since
    "mm, oom: hide mm which is shared with kthread or global init". The
    problem is that the exit path:
    
      do_exit
        exit_mm
          tsk->mm = NULL;
          mmput
            __mmput
          exit_oom_victim
    
    doesn't guarantee that exit_oom_victim will get called in a bounded
    amount of time.  At least exit_aio depends on IO which might get blocked
    due to lack of memory and who knows what else is lurking there.
    
    This patch takes a different approach.  We remember tsk->mm into the
    signal_struct and bind it to the signal struct life time for all oom
    victims.  __oom_reap_task_mm as well as oom_scan_process_thread do not
    have to rely on find_lock_task_mm anymore and they will have a reliable
    reference to the mm struct.  As a result all the oom specific
    communication inside the OOM killer can be done via tsk->signal->oom_mm.
    
    Increasing the signal_struct for something as unlikely as the oom killer
    is far from ideal but this approach will make the code much more
    reasonable and long term we even might want to move task->mm into the
    signal_struct anyway.  In the next step we might want to make the oom
    killer exclusion and access to memory reserves completely independent
    which would be also nice.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-4-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b48cd32be445..67ea79610e67 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -805,6 +805,8 @@ struct signal_struct {
 	short oom_score_adj;		/* OOM kill score adjustment */
 	short oom_score_adj_min;	/* OOM kill score adjustment min value.
 					 * Only settable by CAP_SYS_RESOURCE. */
+	struct mm_struct *oom_mm;	/* recorded mm when the thread group got
+					 * killed by the oom killer */
 
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations

commit 8496afaba93ece80a83cbd096f0675a1020ddfc4
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Oct 7 16:58:48 2016 -0700

    mm,oom_reaper: do not attempt to reap a task twice
    
    "mm, oom_reaper: do not attempt to reap a task twice" tried to give the
    OOM reaper one more chance to retry using MMF_OOM_NOT_REAPABLE flag.
    But the usefulness of the flag is rather limited and actually never
    shown in practice.  If the flag is set, it means that the holder of
    mm->mmap_sem cannot call up_write() due to presumably being blocked at
    unkillable wait waiting for other thread's memory allocation.  But since
    one of threads sharing that mm will queue that mm immediately via
    task_will_free_mem() shortcut (otherwise, oom_badness() will select the
    same mm again due to oom_score_adj value unchanged), retrying
    MMF_OOM_NOT_REAPABLE mm is unlikely helpful.
    
    Let's always set MMF_OOM_REAPED.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-3-git-send-email-mhocko@kernel.org
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7543a476178b..b48cd32be445 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -525,7 +525,6 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 #define MMF_OOM_REAPED		21	/* mm has been already reaped */
-#define MMF_OOM_NOT_REAPABLE	22	/* mm couldn't be reaped */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit 1a4a2bc460721bc8f91e4c1294d39b38e5af132f
Merge: 110a9e42b687 1ef55be16ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 16:13:28 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull low-level x86 updates from Ingo Molnar:
     "In this cycle this topic tree has become one of those 'super topics'
      that accumulated a lot of changes:
    
       - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on
         x86 - preceded by an array of changes. v4.8 saw preparatory changes
         in this area already - this is the rest of the work. Includes the
         thread stack caching performance optimization. (Andy Lutomirski)
    
       - switch_to() cleanups and all around enhancements. (Brian Gerst)
    
       - A large number of dumpstack infrastructure enhancements and an
         unwinder abstraction. The secret long term plan is safe(r) live
         patching plus maybe another attempt at debuginfo based unwinding -
         but all these current bits are standalone enhancements in a frame
         pointer based debug environment as well. (Josh Poimboeuf)
    
       - More __ro_after_init and const annotations. (Kees Cook)
    
       - Enable KASLR for the vmemmap memory region. (Thomas Garnier)"
    
    [ The virtually mapped stack changes are pretty fundamental, and not
      x86-specific per se, even if they are only used on x86 right now. ]
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/asm: Get rid of __read_cr4_safe()
      thread_info: Use unsigned long for flags
      x86/alternatives: Add stack frame dependency to alternative_call_2()
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/unwind: Add new unwind interface and implementations
      x86/dumpstack: Remove NULL task pointer convention
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      lib/syscall: Pin the task stack in collect_syscall()
      x86/process: Pin the target stack in get_wchan()
      x86/dumpstack: Pin the target stack when dumping it
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/entry/64: Fix a minor comment rebase error
      iommu/amd: Don't put completion-wait semaphore on stack
      ...

commit af79ad2b1f337a00aa150b993635b10bc68dc842
Merge: e606d81d2d95 447976ef4fd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 13:39:00 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes are:
    
       - irqtime accounting cleanups and enhancements. (Frederic Weisbecker)
    
       - schedstat debugging enhancements, make it more broadly runtime
         available. (Josh Poimboeuf)
    
       - More work on asymmetric topology/capacity scheduling. (Morten
         Rasmussen)
    
       - sched/wait fixes and cleanups. (Oleg Nesterov)
    
       - PELT (per entity load tracking) improvements. (Peter Zijlstra)
    
       - Rewrite and enhance select_idle_siblings(). (Peter Zijlstra)
    
       - sched/numa enhancements/fixes (Rik van Riel)
    
       - sched/cputime scalability improvements (Stanislaw Gruszka)
    
       - Load calculation arithmetics fixes. (Dietmar Eggemann)
    
       - sched/deadline enhancements (Tommaso Cucinotta)
    
       - Fix utilization accounting when switching to the SCHED_NORMAL
         policy. (Vincent Guittot)
    
       - ... plus misc cleanups and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      sched/irqtime: Consolidate irqtime flushing code
      sched/irqtime: Consolidate accounting synchronization with u64_stats API
      u64_stats: Introduce IRQs disabled helpers
      sched/irqtime: Remove needless IRQs disablement on kcpustat update
      sched/irqtime: No need for preempt-safe accessors
      sched/fair: Fix min_vruntime tracking
      sched/debug: Add SCHED_WARN_ON()
      sched/core: Fix set_user_nice()
      sched/fair: Introduce set_curr_task() helper
      sched/core, ia64: Rename set_curr_task()
      sched/core: Fix incorrect utilization accounting when switching to fair class
      sched/core: Optimize SCHED_SMT
      sched/core: Rewrite and improve select_idle_siblings()
      sched/core: Replace sd_busy/nr_busy_cpus with sched_domain_shared
      sched/core: Introduce 'struct sched_domain_shared'
      sched/core: Restructure destroy_sched_domain()
      sched/core: Remove unused @cpu argument from destroy_sched_domain*()
      sched/wait: Introduce init_wait_entry()
      sched/wait: Avoid abort_exclusive_wait() in __wait_on_bit_lock()
      sched/wait: Avoid abort_exclusive_wait() in ___wait_event()
      ...

commit a458ae2ea616420f74480f0f5ed67ca0f3b5dbf7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 20 20:29:40 2016 +0200

    sched/core, ia64: Rename set_curr_task()
    
    Rename the ia64 only set_curr_task() function to free up the name.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c30ed860d66..ad51978ff15e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2581,7 +2581,7 @@ static inline bool is_idle_task(const struct task_struct *p)
 	return p->pid == 0;
 }
 extern struct task_struct *curr_task(int cpu);
-extern void set_curr_task(int cpu, struct task_struct *p);
+extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 
 void yield(void);
 

commit 10e2f1acd0106c05229f94c70a344ce3a2c8008b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:38:05 2016 +0200

    sched/core: Rewrite and improve select_idle_siblings()
    
    select_idle_siblings() is a known pain point for a number of
    workloads; it either does too much or not enough and sometimes just
    does plain wrong.
    
    This rewrite attempts to address a number of issues (but sadly not
    all).
    
    The current code does an unconditional sched_domain iteration; with
    the intent of finding an idle core (on SMT hardware). The problems
    which this patch tries to address are:
    
     - its pointless to look for idle cores if the machine is real busy;
       at which point you're just wasting cycles.
    
     - it's behaviour is inconsistent between SMT and !SMT hardware in
       that !SMT hardware ends up doing a scan for any idle CPU in the LLC
       domain, while SMT hardware does a scan for idle cores and if that
       fails, falls back to a scan for idle threads on the 'target' core.
    
    The new code replaces the sched_domain scan with 3 explicit scans:
    
     1) search for an idle core in the LLC
     2) search for an idle CPU in the LLC
     3) search for an idle thread in the 'target' core
    
    where 1 and 3 are conditional on SMT support and 1 and 2 have runtime
    heuristics to skip the step.
    
    Step 1) is conditional on sd_llc_shared->has_idle_cores; when a cpu
    goes idle and sd_llc_shared->has_idle_cores is false, we scan all SMT
    siblings of the CPU going idle. Similarly, we clear
    sd_llc_shared->has_idle_cores when we fail to find an idle core.
    
    Step 2) tracks the average cost of the scan and compares this to the
    average idle time guestimate for the CPU doing the wakeup. There is a
    significant fudge factor involved to deal with the variability of the
    averages. Esp. hackbench was sensitive to this.
    
    Step 3) is unconditional; we assume (also per step 1) that scanning
    all SMT siblings in a core is 'cheap'.
    
    With this; SMT systems gain step 2, which cures a few benchmarks --
    notably one from Facebook.
    
    One 'feature' of the sched_domain iteration, which we preserve in the
    new code, is that it would start scanning from the 'target' CPU,
    instead of scanning the cpumask in cpu id order. This avoids multiple
    CPUs in the LLC scanning for idle to gang up and find the same CPU
    quite as much. The down side is that tasks can end up hopping across
    the LLC for no apparent reason.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 98888f1a03bc..2c30ed860d66 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1070,6 +1070,7 @@ struct sched_group;
 struct sched_domain_shared {
 	atomic_t	ref;
 	atomic_t	nr_busy_cpus;
+	int		has_idle_cores;
 };
 
 struct sched_domain {
@@ -1102,6 +1103,8 @@ struct sched_domain {
 	u64 max_newidle_lb_cost;
 	unsigned long next_decay_max_lb_cost;
 
+	u64 avg_scan_cost;		/* select_idle_sibling */
+
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */
 	unsigned int lb_count[CPU_MAX_IDLE_TYPES];

commit 0e369d757578b23ac50b893f920aa50fdbc45fb6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:38:01 2016 +0200

    sched/core: Replace sd_busy/nr_busy_cpus with sched_domain_shared
    
    Move the nr_busy_cpus thing from its hacky sd->parent->groups->sgc
    location into the much more natural sched_domain_shared location.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8a878b9649a1..98888f1a03bc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1069,6 +1069,7 @@ struct sched_group;
 
 struct sched_domain_shared {
 	atomic_t	ref;
+	atomic_t	nr_busy_cpus;
 };
 
 struct sched_domain {

commit 24fc7edb92eea05946119cc0258c891c26b3b469
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:37:59 2016 +0200

    sched/core: Introduce 'struct sched_domain_shared'
    
    Since struct sched_domain is strictly per cpu; introduce a structure
    that is shared between all 'identical' sched_domains.
    
    Limit to SD_SHARE_PKG_RESOURCES domains for now, as we'll only use it
    for shared cache state; if another use comes up later we can easily
    relax this.
    
    While the sched_group's are normally shared between CPUs, these are
    not natural to use when we need some shared state on a domain level --
    since that would require the domain to have a parent, which is not a
    given.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b99fcd1b341e..8a878b9649a1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1067,6 +1067,10 @@ extern int sched_domain_level_max;
 
 struct sched_group;
 
+struct sched_domain_shared {
+	atomic_t	ref;
+};
+
 struct sched_domain {
 	/* These fields must be setup */
 	struct sched_domain *parent;	/* top domain must be null terminated */
@@ -1135,6 +1139,7 @@ struct sched_domain {
 		void *private;		/* used during construction */
 		struct rcu_head rcu;	/* used during destruction */
 	};
+	struct sched_domain_shared *shared;
 
 	unsigned int span_weight;
 	/*
@@ -1168,6 +1173,7 @@ typedef int (*sched_domain_flags_f)(void);
 
 struct sd_data {
 	struct sched_domain **__percpu sd;
+	struct sched_domain_shared **__percpu sds;
 	struct sched_group **__percpu sg;
 	struct sched_group_capacity **__percpu sgc;
 };

commit 35a773a07926a22bf19d77ee00024522279c4e68
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 19 12:57:53 2016 +0200

    sched/core: Avoid _cond_resched() for PREEMPT=y
    
    On fully preemptible kernels _cond_resched() is pointless, so avoid
    emitting any code for it.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f00ee8e90a29..b99fcd1b341e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3209,7 +3209,11 @@ static inline int signal_pending_state(long state, struct task_struct *p)
  * cond_resched_lock() will drop the spinlock before scheduling,
  * cond_resched_softirq() will enable bhs before scheduling.
  */
+#ifndef CONFIG_PREEMPT
 extern int _cond_resched(void);
+#else
+static inline int _cond_resched(void) { return 0; }
+#endif
 
 #define cond_resched() ({			\
 	___might_sleep(__FILE__, __LINE__, 0);	\

commit 9af6528ee9b682df7f29dbee86fbba0b67eab944
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 13 18:37:29 2016 +0200

    sched/core: Optimize __schedule()
    
    Oleg noted that by making do_exit() use __schedule() for the TASK_DEAD
    context switch, we can avoid the TASK_DEAD special case currently in
    __schedule() because that avoids the extra preempt_disable() from
    schedule().
    
    In order to facilitate this, create a do_task_dead() helper which we
    place in the scheduler code, such that it can access __schedule().
    
    Also add some __noreturn annotations to the functions, there's no
    coming back from do_exit().
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Cheng Chao <cs.os.kernel@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: chris@chris-wilson.co.uk
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/20160913163729.GB5012@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d75024053e9b..f00ee8e90a29 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -448,6 +448,8 @@ static inline void io_schedule(void)
 	io_schedule_timeout(MAX_SCHEDULE_TIMEOUT);
 }
 
+void __noreturn do_task_dead(void);
+
 struct nsproxy;
 struct user_namespace;
 

commit 68f24b08ee892d47bdef925d676e1ae1ccc316f8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:48 2016 -0700

    sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
    
    We currently keep every task's stack around until the task_struct
    itself is freed.  This means that we keep the stack allocation alive
    for longer than necessary and that, under load, we free stacks in
    big batches whenever RCU drops the last task reference.  Neither of
    these is good for reuse of cache-hot memory, and freeing in batches
    prevents us from usefully caching small numbers of vmalloced stacks.
    
    On architectures that have thread_info on the stack, we can't easily
    change this, but on architectures that set THREAD_INFO_IN_TASK, we
    can free it as soon as the task is dead.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/08ca06cde00ebed0046c5d26cbbf3fbb7ef5b812.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a95867267e9f..abb795afc823 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1936,6 +1936,10 @@ struct task_struct {
 #ifdef CONFIG_VMAP_STACK
 	struct vm_struct *stack_vm_area;
 #endif
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	/* A live task holds one reference. */
+	atomic_t stack_refcount;
+#endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /*
@@ -3143,12 +3147,22 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 
 #endif
 
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+static inline void *try_get_task_stack(struct task_struct *tsk)
+{
+	return atomic_inc_not_zero(&tsk->stack_refcount) ?
+		task_stack_page(tsk) : NULL;
+}
+
+extern void put_task_stack(struct task_struct *tsk);
+#else
 static inline void *try_get_task_stack(struct task_struct *tsk)
 {
 	return task_stack_page(tsk);
 }
 
 static inline void put_task_stack(struct task_struct *tsk) {}
+#endif
 
 #define task_stack_end_corrupted(task) \
 		(*(end_of_stack(task)) != STACK_END_MAGIC)

commit c6c314a613cd7d03fb97713e0d642b493de42e69
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:43 2016 -0700

    sched/core: Add try_get_task_stack() and put_task_stack()
    
    There are a few places in the kernel that access stack memory
    belonging to a different task.  Before we can start freeing task
    stacks before the task_struct is freed, we need a way for those code
    paths to pin the stack.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/17a434f50ad3d77000104f21666575e10a9c1fbd.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a287e8b13549..a95867267e9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3094,11 +3094,19 @@ static inline struct thread_info *task_thread_info(struct task_struct *task)
 {
 	return &task->thread_info;
 }
+
+/*
+ * When accessing the stack of a non-current task that might exit, use
+ * try_get_task_stack() instead.  task_stack_page will return a pointer
+ * that could get freed out from under you.
+ */
 static inline void *task_stack_page(const struct task_struct *task)
 {
 	return task->stack;
 }
+
 #define setup_thread_stack(new,old)	do { } while(0)
+
 static inline unsigned long *end_of_stack(const struct task_struct *task)
 {
 	return task->stack;
@@ -3134,6 +3142,14 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 }
 
 #endif
+
+static inline void *try_get_task_stack(struct task_struct *tsk)
+{
+	return task_stack_page(tsk);
+}
+
+static inline void put_task_stack(struct task_struct *tsk) {}
+
 #define task_stack_end_corrupted(task) \
 		(*(end_of_stack(task)) != STACK_END_MAGIC)
 

commit c65eacbe290b8141554c71b2c94489e73ade8c8d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:24 2016 -0700

    sched/core: Allow putting thread_info into task_struct
    
    If an arch opts in by setting CONFIG_THREAD_INFO_IN_TASK_STRUCT,
    then thread_info is defined as a single 'u32 flags' and is the first
    entry of task_struct.  thread_info::task is removed (it serves no
    purpose if thread_info is embedded in task_struct), and
    thread_info::cpu gets its own slot in task_struct.
    
    This is heavily based on a patch written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a0898196f0476195ca02713691a5037a14f2aac5.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20f9f47bcfd0..a287e8b13549 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1458,6 +1458,13 @@ struct tlbflush_unmap_batch {
 };
 
 struct task_struct {
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	/*
+	 * For reasons of header soup (see current_thread_info()), this
+	 * must be the first element of task_struct.
+	 */
+	struct thread_info thread_info;
+#endif
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
 	atomic_t usage;
@@ -1467,6 +1474,9 @@ struct task_struct {
 #ifdef CONFIG_SMP
 	struct llist_node wake_entry;
 	int on_cpu;
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	unsigned int cpu;	/* current CPU */
+#endif
 	unsigned int wakee_flips;
 	unsigned long wakee_flip_decay_ts;
 	struct task_struct *last_wakee;
@@ -2588,7 +2598,9 @@ extern void set_curr_task(int cpu, struct task_struct *p);
 void yield(void);
 
 union thread_union {
+#ifndef CONFIG_THREAD_INFO_IN_TASK
 	struct thread_info thread_info;
+#endif
 	unsigned long stack[THREAD_SIZE/sizeof(long)];
 };
 
@@ -3076,10 +3088,26 @@ static inline void threadgroup_change_end(struct task_struct *tsk)
 	cgroup_threadgroup_change_end(tsk);
 }
 
-#ifndef __HAVE_THREAD_FUNCTIONS
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+
+static inline struct thread_info *task_thread_info(struct task_struct *task)
+{
+	return &task->thread_info;
+}
+static inline void *task_stack_page(const struct task_struct *task)
+{
+	return task->stack;
+}
+#define setup_thread_stack(new,old)	do { } while(0)
+static inline unsigned long *end_of_stack(const struct task_struct *task)
+{
+	return task->stack;
+}
+
+#elif !defined(__HAVE_THREAD_FUNCTIONS)
 
 #define task_thread_info(task)	((struct thread_info *)(task)->stack)
-#define task_stack_page(task)	((task)->stack)
+#define task_stack_page(task)	((void *)(task)->stack)
 
 static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
 {
@@ -3379,7 +3407,11 @@ static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)
 
 static inline unsigned int task_cpu(const struct task_struct *p)
 {
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	return p->cpu;
+#else
 	return task_thread_info(p)->cpu;
+#endif
 }
 
 static inline int task_node(const struct task_struct *p)

commit 8c34ab1910a79319731107ec8ecd2e80893ea30c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Sep 9 23:59:33 2016 +0200

    cpufreq / sched: SCHED_CPUFREQ_IOWAIT flag to indicate iowait condition
    
    Testing indicates that it is possible to improve performace
    significantly without increasing energy consumption too much by
    teaching cpufreq governors to bump up the CPU performance level if
    the in_iowait flag is set for the task in enqueue_task_fair().
    
    For this purpose, define a new cpufreq_update_util() flag
    SCHED_CPUFREQ_IOWAIT and modify enqueue_task_fair() to pass that
    flag to cpufreq_update_util() in the in_iowait case.  That generally
    requires cpufreq_update_util() to be called directly from there,
    because update_load_avg() may not be invoked in that case.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Looks-good-to: Steve Muckle <smuckle@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0fa726b7f31..98fe95fea30c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3471,6 +3471,7 @@ static inline unsigned long rlimit_max(unsigned int limit)
 
 #define SCHED_CPUFREQ_RT	(1U << 0)
 #define SCHED_CPUFREQ_DL	(1U << 1)
+#define SCHED_CPUFREQ_IOWAIT	(1U << 2)
 
 #define SCHED_CPUFREQ_RT_DL	(SCHED_CPUFREQ_RT | SCHED_CPUFREQ_DL)
 

commit ba14a194a434ccc8f733e263ad2ce941e35e5787
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Aug 11 02:35:21 2016 -0700

    fork: Add generic vmalloced stack support
    
    If CONFIG_VMAP_STACK=y is selected, kernel stacks are allocated with
    __vmalloc_node_range().
    
    Grsecurity has had a similar feature (called GRKERNSEC_KSTACKOVERFLOW=y)
    for a long time.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/14c07d4fd173a5b117f51e8b939f9f4323e39899.1470907718.git.luto@kernel.org
    [ Minor edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 62c68e513e39..20f9f47bcfd0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1923,6 +1923,9 @@ struct task_struct {
 #ifdef CONFIG_MMU
 	struct task_struct *oom_reaper_list;
 #endif
+#ifdef CONFIG_VMAP_STACK
+	struct vm_struct *stack_vm_area;
+#endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /*
@@ -1939,6 +1942,18 @@ extern int arch_task_struct_size __read_mostly;
 # define arch_task_struct_size (sizeof(struct task_struct))
 #endif
 
+#ifdef CONFIG_VMAP_STACK
+static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
+{
+	return t->stack_vm_area;
+}
+#else
+static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)
+{
+	return NULL;
+}
+#endif
+
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 

commit 1f6e6c7cb9bcd58abb5ee11243e0eefe6b36fc8e
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Mon Jul 25 14:34:22 2016 +0100

    sched/core: Introduce SD_ASYM_CPUCAPACITY sched_domain topology flag
    
    Add a topology flag to the sched_domain hierarchy indicating the lowest
    domain level where the full range of CPU capacities is represented by
    the domain members for asymmetric capacity topologies (e.g. ARM
    big.LITTLE).
    
    The flag is intended to indicate that extra care should be taken when
    placing tasks on CPUs and this level spans all the different types of
    CPUs found in the system (no need to look further up the domain
    hierarchy). This information is currently only available through
    iterating through the capacities of all the CPUs at parent levels in the
    sched_domain hierarchy.
    
      SD 2      [  0      1      2      3]  SD_ASYM_CPUCAPACITY
    
      SD 1      [  0      1] [   2      3]  !SD_ASYM_CPUCAPACITY
    
      CPU:         0      1      2      3
      capacity:  756    756   1024   1024
    
    If the topology in the example above is duplicated to create an eight
    CPU example with third sched_domain level on top (SD 3), this level
    should not have the flag set (!SD_ASYM_CPUCAPACITY) as its two group
    would both have all CPU capacities represented within them.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: freedom.tan@mediatek.com
    Cc: keita.kobayashi.ym@renesas.com
    Cc: mgalbraith@suse.de
    Cc: sgurrappadi@nvidia.com
    Cc: vincent.guittot@linaro.org
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1469453670-2660-6-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7f64e89a5873..d75024053e9b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1022,6 +1022,7 @@ extern void wake_up_q(struct wake_q_head *head);
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
+#define SD_ASYM_CPUCAPACITY	0x0040  /* Groups have different max cpu capacities */
 #define SD_SHARE_CPUCAPACITY	0x0080	/* Domain members share cpu capacity */
 #define SD_SHARE_POWERDOMAIN	0x0100	/* Domain members share power domain */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */

commit 58919e83c85c3a3c5fb34025dc0e95ddd998c478
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 16 22:14:55 2016 +0200

    cpufreq / sched: Pass flags to cpufreq_update_util()
    
    It is useful to know the reason why cpufreq_update_util() has just
    been called and that can be passed as flags to cpufreq_update_util()
    and to the ->func() callback in struct update_util_data.  However,
    doing that in addition to passing the util and max arguments they
    already take would be clumsy, so avoid it.
    
    Instead, use the observation that the schedutil governor is part
    of the scheduler proper, so it can access scheduler data directly.
    This allows the util and max arguments of cpufreq_update_util()
    and the ->func() callback in struct update_util_data to be replaced
    with a flags one, but schedutil has to be modified to follow.
    
    Thus make the schedutil governor obtain the CFS utilization
    information from the scheduler and use the "RT" and "DL" flags
    instead of the special utilization value of ULONG_MAX to track
    updates from the RT and DL sched classes.  Make it non-modular
    too to avoid having to export scheduler variables to modules at
    large.
    
    Next, update all of the other users of cpufreq_update_util()
    and the ->func() callback in struct update_util_data accordingly.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 62c68e513e39..b0fa726b7f31 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3469,15 +3469,19 @@ static inline unsigned long rlimit_max(unsigned int limit)
 	return task_rlimit_max(current, limit);
 }
 
+#define SCHED_CPUFREQ_RT	(1U << 0)
+#define SCHED_CPUFREQ_DL	(1U << 1)
+
+#define SCHED_CPUFREQ_RT_DL	(SCHED_CPUFREQ_RT | SCHED_CPUFREQ_DL)
+
 #ifdef CONFIG_CPU_FREQ
 struct update_util_data {
-	void (*func)(struct update_util_data *data,
-		     u64 time, unsigned long util, unsigned long max);
+       void (*func)(struct update_util_data *data, u64 time, unsigned int flags);
 };
 
 void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,
-			void (*func)(struct update_util_data *data, u64 time,
-				     unsigned long util, unsigned long max));
+                       void (*func)(struct update_util_data *data, u64 time,
+				    unsigned int flags));
 void cpufreq_remove_update_util_hook(int cpu);
 #endif /* CONFIG_CPU_FREQ */
 

commit d1c6d149cf04d6c7c3c3ebf4b66c82500cbcf6e1
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Sat Jul 23 09:46:39 2016 +0200

    sched/debug: Make the "Preemption disabled at ..." message more useful
    
    This message is currently really useless since it always prints a value
    that comes from the printk() we just did, e.g.:
    
        BUG: sleeping function called from invalid context at mm/slab.h:388
        in_atomic(): 0, irqs_disabled(): 0, pid: 31996, name: trinity-c1
        Preemption disabled at:[<ffffffff8119db33>] down_trylock+0x13/0x80
    
        BUG: sleeping function called from invalid context at include/linux/freezer.h:56
        in_atomic(): 0, irqs_disabled(): 0, pid: 31996, name: trinity-c1
        Preemption disabled at:[<ffffffff811aaa37>] console_unlock+0x2f7/0x930
    
    Here, both down_trylock() and console_unlock() is somewhere in the
    printk() path.
    
    We should save the value before calling printk() and use the saved value
    instead. That immediately reveals the offending callsite:
    
        BUG: sleeping function called from invalid context at mm/slab.h:388
        in_atomic(): 0, irqs_disabled(): 0, pid: 14971, name: trinity-c2
        Preemption disabled at:[<ffffffff819bcd46>] rhashtable_walk_start+0x46/0x150
    
    Bug report:
    
      http://marc.info/?l=linux-netdev&m=146925979821849&w=2
    
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russel <rusty@rustcorp.com.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f3db596efd2c..7f64e89a5873 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3236,6 +3236,15 @@ static inline void cond_resched_rcu(void)
 #endif
 }
 
+static inline unsigned long get_preempt_disable_ip(struct task_struct *p)
+{
+#ifdef CONFIG_DEBUG_PREEMPT
+	return p->preempt_disable_ip;
+#else
+	return 0;
+#endif
+}
+
 /*
  * Does a critical section need to be broken due to another
  * task waiting?: (technically does not depend on CONFIG_PREEMPT,

commit bd425d4bfc7a1a6064dbbadfbac9c7eec0e426ec
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Wed Jun 22 18:03:12 2016 +0100

    sched/core: Fix power to capacity renaming in comment
    
    It is seems that this one escaped Nico's renaming of cpu_power to
    cpu_capacity a while back.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: linux-kernel@vger.kernel.org
    Cc: mgalbraith@suse.de
    Cc: vincent.guittot@linaro.org
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1466615004-3503-2-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 62c68e513e39..f3db596efd2c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1022,7 +1022,7 @@ extern void wake_up_q(struct wake_q_head *head);
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-#define SD_SHARE_CPUCAPACITY	0x0080	/* Domain members share cpu power */
+#define SD_SHARE_CPUCAPACITY	0x0080	/* Domain members share cpu capacity */
 #define SD_SHARE_POWERDOMAIN	0x0100	/* Domain members share power domain */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */

commit 7e7814180b334dff97ef8f56c7c40c277ad4531c
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Aug 2 14:05:36 2016 -0700

    signal: consolidate {TS,TLF}_RESTORE_SIGMASK code
    
    In general, there's no need for the "restore sigmask" flag to live in
    ti->flags.  alpha, ia64, microblaze, powerpc, sh, sparc (64-bit only),
    tile, and x86 use essentially identical alternative implementations,
    placing the flag in ti->status.
    
    Replace those optimized implementations with an equally good common
    implementation that stores it in a bitfield in struct task_struct and
    drop the custom implementations.
    
    Additional architectures can opt in by removing their
    TIF_RESTORE_SIGMASK defines.
    
    Link: http://lkml.kernel.org/r/8a14321d64a28e40adfddc90e18a96c086a6d6f9.1468522723.git.luto@kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>        [powerpc]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dmitry Safonov <dsafonov@virtuozzo.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 553af2923824..62c68e513e39 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1547,6 +1547,9 @@ struct task_struct {
 	/* unserialized, strictly 'current' */
 	unsigned in_execve:1; /* bit to tell LSMs we're in execve */
 	unsigned in_iowait:1;
+#if !defined(TIF_RESTORE_SIGMASK)
+	unsigned restore_sigmask:1;
+#endif
 #ifdef CONFIG_MEMCG
 	unsigned memcg_may_oom:1;
 #ifndef CONFIG_SLOB
@@ -2680,6 +2683,66 @@ extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 
+#ifdef TIF_RESTORE_SIGMASK
+/*
+ * Legacy restore_sigmask accessors.  These are inefficient on
+ * SMP architectures because they require atomic operations.
+ */
+
+/**
+ * set_restore_sigmask() - make sure saved_sigmask processing gets done
+ *
+ * This sets TIF_RESTORE_SIGMASK and ensures that the arch signal code
+ * will run before returning to user mode, to process the flag.  For
+ * all callers, TIF_SIGPENDING is already set or it's no harm to set
+ * it.  TIF_RESTORE_SIGMASK need not be in the set of bits that the
+ * arch code will notice on return to user mode, in case those bits
+ * are scarce.  We set TIF_SIGPENDING here to ensure that the arch
+ * signal code always gets run when TIF_RESTORE_SIGMASK is set.
+ */
+static inline void set_restore_sigmask(void)
+{
+	set_thread_flag(TIF_RESTORE_SIGMASK);
+	WARN_ON(!test_thread_flag(TIF_SIGPENDING));
+}
+static inline void clear_restore_sigmask(void)
+{
+	clear_thread_flag(TIF_RESTORE_SIGMASK);
+}
+static inline bool test_restore_sigmask(void)
+{
+	return test_thread_flag(TIF_RESTORE_SIGMASK);
+}
+static inline bool test_and_clear_restore_sigmask(void)
+{
+	return test_and_clear_thread_flag(TIF_RESTORE_SIGMASK);
+}
+
+#else	/* TIF_RESTORE_SIGMASK */
+
+/* Higher-quality implementation, used if TIF_RESTORE_SIGMASK doesn't exist. */
+static inline void set_restore_sigmask(void)
+{
+	current->restore_sigmask = true;
+	WARN_ON(!test_thread_flag(TIF_SIGPENDING));
+}
+static inline void clear_restore_sigmask(void)
+{
+	current->restore_sigmask = false;
+}
+static inline bool test_restore_sigmask(void)
+{
+	return current->restore_sigmask;
+}
+static inline bool test_and_clear_restore_sigmask(void)
+{
+	if (!current->restore_sigmask)
+		return false;
+	current->restore_sigmask = false;
+	return true;
+}
+#endif
+
 static inline void restore_saved_sigmask(void)
 {
 	if (test_and_clear_restore_sigmask())

commit 11a410d516e89320fe0817606eeab58f36c22968
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 28 15:44:58 2016 -0700

    mm, oom_reaper: do not attempt to reap a task more than twice
    
    oom_reaper relies on the mmap_sem for read to do its job.  Many places
    which might block readers have been converted to use down_write_killable
    and that has reduced chances of the contention a lot.  Some paths where
    the mmap_sem is held for write can take other locks and they might
    either be not prepared to fail due to fatal signal pending or too
    impractical to be changed.
    
    This patch introduces MMF_OOM_NOT_REAPABLE flag which gets set after the
    first attempt to reap a task's mm fails.  If the flag is present after
    the failure then we set MMF_OOM_REAPED to hide this mm from the oom
    killer completely so it can go and chose another victim.
    
    As a result a risk of OOM deadlock when the oom victim would be blocked
    indefinetly and so the oom killer cannot make any progress should be
    mitigated considerably while we still try really hard to perform all
    reclaim attempts and stay predictable in the behavior.
    
    Link: http://lkml.kernel.org/r/1466426628-15074-10-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c0efd80ba40f..553af2923824 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -523,6 +523,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 #define MMF_OOM_REAPED		21	/* mm has been already reaped */
+#define MMF_OOM_NOT_REAPABLE	22	/* mm couldn't be reaped */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit b18dc5f291c07ddaf31562b9f27b3a122f1f9b7e
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 28 15:44:46 2016 -0700

    mm, oom: skip vforked tasks from being selected
    
    vforked tasks are not really sitting on any memory.  They are sharing the
    mm with parent until they exec into a new code.  Until then it is just
    pinning the address space.  OOM killer will kill the vforked task along
    with its parent but we still can end up selecting vforked task when the
    parent wouldn't be selected.  E.g.  init doing vfork to launch a task or
    vforked being a child of oom unkillable task with an updated oom_score_adj
    to be killable.
    
    Add a new helper to check whether a task is in the vfork sharing memory
    with its parent and use it in oom_badness to skip over these tasks.
    
    Link: http://lkml.kernel.org/r/1466426628-15074-6-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d99218a1e043..c0efd80ba40f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1949,6 +1949,32 @@ static inline int tsk_nr_cpus_allowed(struct task_struct *p)
 #define TNF_FAULT_LOCAL	0x08
 #define TNF_MIGRATE_FAIL 0x10
 
+static inline bool in_vfork(struct task_struct *tsk)
+{
+	bool ret;
+
+	/*
+	 * need RCU to access ->real_parent if CLONE_VM was used along with
+	 * CLONE_PARENT.
+	 *
+	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
+	 * imply CLONE_VM
+	 *
+	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
+	 * ->real_parent is not necessarily the task doing vfork(), so in
+	 * theory we can't rely on task_lock() if we want to dereference it.
+	 *
+	 * And in this case we can't trust the real_parent->mm == tsk->mm
+	 * check, it can be false negative. But we do not care, if init or
+	 * another oom-unkillable task does this it should blame itself.
+	 */
+	rcu_read_lock();
+	ret = tsk->vfork_done && tsk->real_parent->mm == tsk->mm;
+	rcu_read_unlock();
+
+	return ret;
+}
+
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);

commit 7dc603c9028ea5d4354e0e317e8481df99b06d7e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 16 13:29:28 2016 +0200

    sched/fair: Fix PELT integrity for new tasks
    
    Vincent and Yuyang found another few scenarios in which entity
    tracking goes wobbly.
    
    The scenarios are basically due to the fact that new tasks are not
    immediately attached and thereby differ from the normal situation -- a
    task is always attached to a cfs_rq load average (such that it
    includes its blocked contribution) and are explicitly
    detached/attached on migration to another cfs_rq.
    
    Scenario 1: switch to fair class
    
      p->sched_class = fair_class;
      if (queued)
        enqueue_task(p);
          ...
            enqueue_entity()
              enqueue_entity_load_avg()
                migrated = !sa->last_update_time (true)
                if (migrated)
                  attach_entity_load_avg()
      check_class_changed()
        switched_from() (!fair)
        switched_to()   (fair)
          switched_to_fair()
            attach_entity_load_avg()
    
    If @p is a new task that hasn't been fair before, it will have
    !last_update_time and, per the above, end up in
    attach_entity_load_avg() _twice_.
    
    Scenario 2: change between cgroups
    
      sched_move_group(p)
        if (queued)
          dequeue_task()
        task_move_group_fair()
          detach_task_cfs_rq()
            detach_entity_load_avg()
          set_task_rq()
          attach_task_cfs_rq()
            attach_entity_load_avg()
        if (queued)
          enqueue_task();
            ...
              enqueue_entity()
                enqueue_entity_load_avg()
                  migrated = !sa->last_update_time (true)
                  if (migrated)
                    attach_entity_load_avg()
    
    Similar as with scenario 1, if @p is a new task, it will have
    !load_update_time and we'll end up in attach_entity_load_avg()
    _twice_.
    
    Furthermore, notice how we do a detach_entity_load_avg() on something
    that wasn't attached to begin with.
    
    As stated above; the problem is that the new task isn't yet attached
    to the load tracking and thereby violates the invariant assumption.
    
    This patch remedies this by ensuring a new task is indeed properly
    attached to the load tracking on creation, through
    post_init_entity_util_avg().
    
    Of course, this isn't entirely as straightforward as one might think,
    since the task is hashed before we call wake_up_new_task() and thus
    can be poked at. We avoid this by adding TASK_NEW and teaching
    cpu_cgroup_can_attach() to refuse such tasks.
    
    Reported-by: Yuyang Du <yuyang.du@intel.com>
    Reported-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b45acfd18f4e..d99218a1e043 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -219,9 +219,10 @@ extern void proc_sched_set_task(struct task_struct *p);
 #define TASK_WAKING		256
 #define TASK_PARKED		512
 #define TASK_NOLOAD		1024
-#define TASK_STATE_MAX		2048
+#define TASK_NEW		2048
+#define TASK_STATE_MAX		4096
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPN"
+#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPNn"
 
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];

commit 630741fb60ac4e286f5396403c0d864d924c02bc
Merge: 807e5b80687c ea1dc6fc6242
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 27 11:35:02 2016 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b235beea9e996a4d36fed6cfef4801a3e7d7a9a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 24 15:09:37 2016 -0700

    Clarify naming of thread info/stack allocators
    
    We've had the thread info allocated together with the thread stack for
    most architectures for a long time (since the thread_info was split off
    from the task struct), but that is about to change.
    
    But the patches that move the thread info to be off-stack (and a part of
    the task struct instead) made it clear how confused the allocator and
    freeing functions are.
    
    Because the common case was that we share an allocation with the thread
    stack and the thread_info, the two pointers were identical.  That
    identity then meant that we would have things like
    
            ti = alloc_thread_info_node(tsk, node);
            ...
            tsk->stack = ti;
    
    which certainly _worked_ (since stack and thread_info have the same
    value), but is rather confusing: why are we assigning a thread_info to
    the stack? And if we move the thread_info away, the "confusing" code
    just gets to be entirely bogus.
    
    So remove all this confusion, and make it clear that we are doing the
    stack allocation by renaming and clarifying the function names to be
    about the stack.  The fact that the thread_info then shares the
    allocation is an implementation detail, and not really about the
    allocation itself.
    
    This is a pure renaming and type fix: we pass in the same pointer, it's
    just that we clarify what the pointer means.
    
    The ia64 code that actually only has one single allocation (for all of
    task_struct, thread_info and kernel thread stack) now looks a bit odd,
    but since "tsk->stack" is actually not even used there, that oddity
    doesn't matter.  It would be a separate thing to clean that up, I
    intentionally left the ia64 changes as a pure brute-force renaming and
    type change.
    
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6e42ada26345..253538f29ade 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3007,7 +3007,7 @@ static inline int object_is_on_stack(void *obj)
 	return (obj >= stack) && (obj < (stack + THREAD_SIZE));
 }
 
-extern void thread_info_cache_init(void);
+extern void thread_stack_cache_init(void);
 
 #ifdef CONFIG_DEBUG_STACK_USAGE
 static inline unsigned long stack_not_used(struct task_struct *p)

commit 150593bf869393d10a79f6bd3df2585ecc20a9bb
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 18 19:02:18 2016 +0200

    sched/api: Introduce task_rcu_dereference() and try_get_task_struct()
    
    Generally task_struct is only protected by RCU if it was found on a
    RCU protected list (say, for_each_process() or find_task_by_vpid()).
    
    As Kirill pointed out rq->curr isn't protected by RCU, the scheduler
    drops the (potentially) last reference without RCU gp, this means
    that we need to fix the code which uses foreign_rq->curr under
    rcu_read_lock().
    
    Add a new helper which can be used to dereference rq->curr or any
    other pointer to task_struct assuming that it should be cleared or
    updated before the final put_task_struct(). It returns non-NULL
    only if this task can't go away before rcu_read_unlock().
    
    ( Also add try_get_task_struct() to make it easier to use this API
      correctly. )
    
    Suggested-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [ Updated comments; added try_get_task_struct()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Link: http://lkml.kernel.org/r/20160518170218.GY3192@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6e42ada26345..dee41bf59e6b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2139,6 +2139,9 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
+struct task_struct *task_rcu_dereference(struct task_struct **ptask);
+struct task_struct *try_get_task_struct(struct task_struct **ptask);
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 extern void task_cputime(struct task_struct *t,
 			 cputime_t *utime, cputime_t *stime);

commit 7ef949d77f95f0d129f0d404b336459a34a00101
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu May 26 15:16:22 2016 -0700

    mm: oom_reaper: remove some bloat
    
    mmput_async is currently used only from the oom_reaper which is defined
    only for CONFIG_MMU.  We can save work_struct in mm_struct for
    !CONFIG_MMU.
    
    [akpm@linux-foundation.org: fix typo, per Minchan]
    Link: http://lkml.kernel.org/r/20160520061658.GB19172@dhcp22.suse.cz
    Reported-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 23e075dcdfe4..6e42ada26345 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2745,10 +2745,12 @@ static inline bool mmget_not_zero(struct mm_struct *mm)
 
 /* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);
-/* same as above but performs the slow path from the async kontext. Can
+#ifdef CONFIG_MMU
+/* same as above but performs the slow path from the async context. Can
  * be called from the atomic context as well
  */
 extern void mmput_async(struct mm_struct *);
+#endif
 
 /* Grab a reference to a task's mm, if it is not already going away */
 extern struct mm_struct *get_task_mm(struct task_struct *task);

commit f89eae4ee7e075e576bd4b4d2db901023421a3be
Merge: bdc6b758e443 b7e7ade34e61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 25 17:11:43 2016 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Two fixes: one for a lost wakeup, the other to fix the compiler
      optimizing out preempt operations on ARM64 (and possibly other non-x86
      architectures)"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/core: Fix remote wakeups
      sched/preempt: Fix preempt_count manipulations

commit b7e7ade34e6188bee2e3b0d42b51d25137d9e2a5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 23 11:19:07 2016 +0200

    sched/core: Fix remote wakeups
    
    Commit:
    
      b5179ac70de8 ("sched/fair: Prepare to fix fairness problems on migration")
    
    ... introduced a bug: Mike Galbraith found that it introduced a
    performance regression, while Paul E. McKenney reported lost
    wakeups and bisected it to this commit.
    
    The reason is that I mis-read ttwu_queue() such that I assumed any
    wakeup that got a remote queue must have had the task migrated.
    
    Since this is not so; we need to transfer this information between
    queueing the wakeup and actually doing the wakeup. Use a new
    task_struct::sched_flag for this, we already write to
    sched_contributes_to_load in the wakeup path so this is a hot and
    modified cacheline.
    
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Tested-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Pavan Kondeti <pkondeti@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: byungchul.park@lge.com
    Fixes: b5179ac70de8 ("sched/fair: Prepare to fix fairness problems on migration")
    Link: http://lkml.kernel.org/r/20160523091907.GD15728@worktop.ger.corp.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6cc0df970f1a..e053517a88b6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1533,6 +1533,7 @@ struct task_struct {
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 	unsigned sched_migrated:1;
+	unsigned sched_remote_wakeup:1;
 	unsigned :0; /* force alignment to the next boundary */
 
 	/* unserialized, strictly 'current' */

commit c96fc2d85f4a827e3bb2abe7de2394a1fb8a0fe7
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Mon May 23 16:23:57 2016 -0700

    signal: make oom_flags a bool
    
    Currently the size of "struct signal_struct"->oom_flags member is
    sizeof(unsigned) bytes, but only one flag OOM_FLAG_ORIGIN which is
    updated by current thread is defined.  We can convert OOM_FLAG_ORIGIN
    into a bool, and reuse the saved bytes for updating from the OOM killer
    and/or the OOM reaper thread.
    
    By the way, do we care about a race window between run_store() and
    swapoff() because it would be theoretically possible that two threads
    sharing the "struct signal_struct" concurrently call respective
    functions? If we care, we can make oom_flags an atomic_t.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c036de6c1ee..21c26e78aec5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -794,7 +794,11 @@ struct signal_struct {
 	struct tty_audit_buf *tty_audit_buf;
 #endif
 
-	oom_flags_t oom_flags;
+	/*
+	 * Thread is the potential origin of an oom condition; kill first on
+	 * oom
+	 */
+	bool oom_flag_origin;
 	short oom_score_adj;		/* OOM kill score adjustment */
 	short oom_score_adj_min;	/* OOM kill score adjustment min value.
 					 * Only settable by CAP_SYS_RESOURCE. */

commit 5469dc270cd44c451590d40c031e6a71c1f637e8
Merge: 2f37dd131c5d ea9b50133ffe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 22:31:33 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - KASAN updates
    
     - procfs updates
    
     - exit, fork updates
    
     - printk updates
    
     - lib/ updates
    
     - radix-tree testsuite updates
    
     - checkpatch updates
    
     - kprobes updates
    
     - a few other misc bits
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (162 commits)
      samples/kprobes: print out the symbol name for the hooks
      samples/kprobes: add a new module parameter
      kprobes: add the "tls" argument for j_do_fork
      init/main.c: simplify initcall_blacklisted()
      fs/efs/super.c: fix return value
      checkpatch: improve --git <commit-count> shortcut
      checkpatch: reduce number of `git log` calls with --git
      checkpatch: add support to check already applied git commits
      checkpatch: add --list-types to show message types to show or ignore
      checkpatch: advertise the --fix and --fix-inplace options more
      checkpatch: whine about ACCESS_ONCE
      checkpatch: add test for keywords not starting on tabstops
      checkpatch: improve CONSTANT_COMPARISON test for structure members
      checkpatch: add PREFER_IS_ENABLED test
      lib/GCD.c: use binary GCD algorithm instead of Euclidean
      radix-tree: free up the bottom bit of exceptional entries for reuse
      dax: move RADIX_DAX_ definitions to dax.c
      radix-tree: make radix_tree_descend() more useful
      radix-tree: introduce radix_tree_replace_clear_tags()
      radix-tree: tidy up __radix_tree_create()
      ...

commit 2f37dd131c5d3a2eac21cd5baf80658b1b02a8ac
Merge: 3aa2fc1667ac ffc83a79b44e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 22:20:48 2016 -0700

    Merge tag 'staging-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging
    
    Pull staging and IIO driver updates from Greg KH:
     "Here's the big staging and iio driver update for 4.7-rc1.
    
      I think we almost broke even with this release, only adding a few more
      lines than we removed, which isn't bad overall given that there's a
      bunch of new iio drivers added.
    
      The Lustre developers seem to have woken up from their sleep and have
      been doing a great job in cleaning up the code and pruning unused or
      old cruft, the filesystem is almost readable :)
    
      Other than that, just a lot of basic coding style cleanups in the
      churn.  All have been in linux-next for a while with no reported
      issues"
    
    * tag 'staging-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/staging: (938 commits)
      Staging: emxx_udc: emxx_udc: fixed coding style issue
      staging/gdm724x: fix "alignment should match open parenthesis" issues
      staging/gdm724x: Fix avoid CamelCase
      staging: unisys: rename misleading var ii with frag
      staging: unisys: visorhba: switch success handling to error handling
      staging: unisys: visorhba: main path needs to flow down the left margin
      staging: unisys: visorinput: handle_locking_key() simplifications
      staging: unisys: visorhba: fail gracefully for thread creation failures
      staging: unisys: visornic: comment restructuring and removing bad diction
      staging: unisys: fix format string %Lx to %llx for u64
      staging: unisys: remove unused struct members
      staging: unisys: visorchannel: correct variable misspelling
      staging: unisys: visorhba: replace functionlike macro with function
      staging: dgnc: Need to check for NULL of ch
      staging: dgnc: remove redundant condition check
      staging: dgnc: fix 'line over 80 characters'
      staging: dgnc: clean up the dgnc_get_modem_info()
      staging: lustre: lnet: enable configuration per NI interface
      staging: lustre: o2iblnd: properly set ibr_why
      staging: lustre: o2iblnd: remove last of kiblnd_tunables_fini
      ...

commit e64646946ed32902fd597fa6e514b1da84642de3
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:20 2016 -0700

    exit_thread: accept a task parameter to be exited
    
    We need to call exit_thread from copy_process in a fail path.  So make it
    accept task_struct as a parameter.
    
    [v2]
    * s390: exit_thread_runtime_instr doesn't make sense to be called for
      non-current tasks.
    * arm: fix the comment in vfp_thread_copy
    * change 'me' to 'tsk' for task_struct
    * now we can change only archs that actually have exit_thread
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 167c0d4bf3fa..02bdab4d6db7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2771,9 +2771,9 @@ static inline int copy_thread_tls(
 extern void flush_thread(void);
 
 #ifdef CONFIG_HAVE_EXIT_THREAD
-extern void exit_thread(void);
+extern void exit_thread(struct task_struct *tsk);
 #else
-static inline void exit_thread(void)
+static inline void exit_thread(struct task_struct *tsk)
 {
 }
 #endif

commit 5f56a5dfdb9bcb3bca03df59980d4d2f012cbb53
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:16 2016 -0700

    exit_thread: remove empty bodies
    
    Define HAVE_EXIT_THREAD for archs which want to do something in
    exit_thread. For others, let's define exit_thread as an empty inline.
    
    This is a cleanup before we change the prototype of exit_thread to
    accept a task parameter.
    
    [akpm@linux-foundation.org: fix mips]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6b3213d96da6..167c0d4bf3fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2769,7 +2769,14 @@ static inline int copy_thread_tls(
 }
 #endif
 extern void flush_thread(void);
+
+#ifdef CONFIG_HAVE_EXIT_THREAD
 extern void exit_thread(void);
+#else
+static inline void exit_thread(void)
+{
+}
+#endif
 
 extern void exit_files(struct task_struct *);
 extern void __cleanup_sighand(struct sighand_struct *);

commit d2005e3f41d4f9299e2df6a967c8beb5086967a9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 20 16:58:36 2016 -0700

    userfaultfd: don't pin the user memory in userfaultfd_file_create()
    
    userfaultfd_file_create() increments mm->mm_users; this means that the
    memory won't be unmapped/freed if mm owner exits/execs, and UFFDIO_COPY
    after that can populate the orphaned mm more.
    
    Change userfaultfd_file_create() and userfaultfd_ctx_put() to use
    mm->mm_count to pin mm_struct.  This means that
    atomic_inc_not_zero(mm->mm_users) is needed when we are going to
    actually play with this memory.  Except handle_userfault() path doesn't
    need this, the caller must already have a reference.
    
    The patch adds the new trivial helper, mmget_not_zero(), it can have
    more users.
    
    Link: http://lkml.kernel.org/r/20160516172254.GA8595@redhat.com
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 01fe1bb68754..6b3213d96da6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2723,12 +2723,17 @@ extern struct mm_struct * mm_alloc(void);
 
 /* mmdrop drops the mm and the page tables */
 extern void __mmdrop(struct mm_struct *);
-static inline void mmdrop(struct mm_struct * mm)
+static inline void mmdrop(struct mm_struct *mm)
 {
 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 		__mmdrop(mm);
 }
 
+static inline bool mmget_not_zero(struct mm_struct *mm)
+{
+	return atomic_inc_not_zero(&mm->mm_users);
+}
+
 /* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);
 /* same as above but performs the slow path from the async kontext. Can

commit f44666b04605d1c7fd94ab90b7ccf633e7eff228
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri May 20 16:57:27 2016 -0700

    mm,oom: speed up select_bad_process() loop
    
    Since commit 3a5dda7a17cf ("oom: prevent unnecessary oom kills or kernel
    panics"), select_bad_process() is using for_each_process_thread().
    
    Since oom_unkillable_task() scans all threads in the caller's thread
    group and oom_task_origin() scans signal_struct of the caller's thread
    group, we don't need to call oom_unkillable_task() and oom_task_origin()
    on each thread.  Also, since !mm test will be done later at
    oom_badness(), we don't need to do !mm test on each thread.  Therefore,
    we only need to do TIF_MEMDIE test on each thread.
    
    Although the original code was correct it was quite inefficient because
    each thread group was scanned num_threads times which can be a lot
    especially with processes with many threads.  Even though the OOM is
    extremely cold path it is always good to be as effective as possible
    when we are inside rcu_read_lock() - aka unpreemptible context.
    
    If we track number of TIF_MEMDIE threads inside signal_struct, we don't
    need to do TIF_MEMDIE test on each thread.  This will allow
    select_bad_process() to use for_each_process().
    
    This patch adds a counter to signal_struct for tracking how many
    TIF_MEMDIE threads are in a given thread group, and check it at
    oom_scan_process_thread() so that select_bad_process() can use
    for_each_process() rather than for_each_process_thread().
    
    [mhocko@suse.com: do not blow the signal_struct size]
      Link: http://lkml.kernel.org/r/20160520075035.GF19172@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/201605182230.IDC73435.MVSOHLFOQFOJtF@I-love.SAKURA.ne.jp
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 479e3cade7e9..01fe1bb68754 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -669,6 +669,7 @@ struct signal_struct {
 	atomic_t		sigcnt;
 	atomic_t		live;
 	int			nr_threads;
+	atomic_t oom_victims; /* # of TIF_MEDIE threads in this thread group */
 	struct list_head	thread_head;
 
 	wait_queue_head_t	wait_chldexit;	/* for wait4() */

commit ec8d7c14ea14922fe21945b458a75e39f11dd832
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri May 20 16:57:21 2016 -0700

    mm, oom_reaper: do not mmput synchronously from the oom reaper context
    
    Tetsuo has properly noted that mmput slow path might get blocked waiting
    for another party (e.g.  exit_aio waits for an IO).  If that happens the
    oom_reaper would be put out of the way and will not be able to process
    next oom victim.  We should strive for making this context as reliable
    and independent on other subsystems as much as possible.
    
    Introduce mmput_async which will perform the slow path from an async
    (WQ) context.  This will delay the operation but that shouldn't be a
    problem because the oom_reaper has reclaimed the victim's address space
    for most cases as much as possible and the remaining context shouldn't
    bind too much memory anymore.  The only exception is when mmap_sem
    trylock has failed which shouldn't happen too often.
    
    The issue is only theoretical but not impossible.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 40eabf176ce2..479e3cade7e9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2730,6 +2730,11 @@ static inline void mmdrop(struct mm_struct * mm)
 
 /* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);
+/* same as above but performs the slow path from the async kontext. Can
+ * be called from the atomic context as well
+ */
+extern void mmput_async(struct mm_struct *);
+
 /* Grab a reference to a task's mm, if it is not already going away */
 extern struct mm_struct *get_task_mm(struct task_struct *task);
 /*

commit bb8a4b7fd1266ef888b3a80aa5f266062b224ef4
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri May 20 16:57:18 2016 -0700

    mm, oom_reaper: hide oom reaped tasks from OOM killer more carefully
    
    Commit 36324a990cf5 ("oom: clear TIF_MEMDIE after oom_reaper managed to
    unmap the address space") not only clears TIF_MEMDIE for oom reaped task
    but also set OOM_SCORE_ADJ_MIN for the target task to hide it from the
    oom killer.  This works in simple cases but it is not sufficient for
    (unlikely) cases where the mm is shared between independent processes
    (as they do not share signal struct).  If the mm had only small amount
    of memory which could be reaped then another task sharing the mm could
    be selected and that wouldn't help to move out from the oom situation.
    
    Introduce MMF_OOM_REAPED mm flag which is checked in oom_badness (same
    as OOM_SCORE_ADJ_MIN) and task is skipped if the flag is set.  Set the
    flag after __oom_reap_task is done with a task.  This will force the
    select_bad_process() to ignore all already oom reaped tasks as well as
    no such task is sacrificed for its parent.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 31bd0d97d178..40eabf176ce2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -521,6 +521,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
+#define MMF_OOM_REAPED		21	/* mm has been already reaped */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit d57d39431924d1628ac9b93a2de7f806fc80680a
Merge: 3e21e5dda490 27c4a1c5ef61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 19:17:22 2016 -0700

    Merge tag 'pm-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "The majority of changes go into the cpufreq subsystem this time.
    
      To me, quite obviously, the biggest ticket item is the new "schedutil"
      governor.  Interestingly enough, it's the first new cpufreq governor
      since the beginning of the git era (except for some out-of-the-tree
      ones).
    
      There are two main differences between it and the existing governors.
      First, it uses the information provided by the scheduler directly for
      making its decisions, so it doesn't have to track anything by itself.
      Second, it can invoke drivers (supporting that feature) to adjust CPU
      performance right away without having to spawn work items to be
      executed in process context or similar.  Currently, the acpi-cpufreq
      driver is the only one supporting that mode of operation, but then it
      is used on a large number of systems.
    
      The "schedutil" governor as included here is very simple and mostly
      regarded as a foundation for future work on the integration of the
      scheduler with CPU power management (in fact, there is work in
      progress on top of it already).  Nevertheless it works and the
      preliminary results obtained with it are encouraging.
    
      There also is some consolidation of CPU frequency management for ARM
      platforms that can add their machine IDs the the new stub dt-platdev
      driver now and that will take care of creating the requisite platform
      device for cpufreq-dt, so it is not necessary to do that in platform
      code any more.  Several ARM platforms are switched over to using this
      generic mechanism.
    
      In addition to that, the intel_pstate driver is now going to respect
      CPU frequency limits set by the platform firmware (or a BMC) and
      provided via the ACPI _PPC object.
    
      The devfreq subsystem is getting a new "passive" governor for SoCs
      subsystems that will depend on somebody else to manage their voltage
      rails and its support for Samsung Exynos SoCs is consolidated.
    
      The rest is support for new hardware (Intel Broxton support in
      intel_idle for one example), bug fixes, optimizations and cleanups in
      a number of places.
    
      Specifics:
    
       - New cpufreq "schedutil" governor (making decisions based on CPU
         utilization information provided by the scheduler and capable of
         switching CPU frequencies right away if the underlying driver
         supports that) and support for fast frequency switching in the
         acpi-cpufreq driver (Rafael Wysocki)
    
       - Consolidation of CPU frequency management on ARM platforms allowing
         them to get rid of some platform-specific boilerplate code if they
         are going to use the cpufreq-dt driver (Viresh Kumar, Finley Xiao,
         Marc Gonzalez)
    
       - Support for ACPI _PPC and CPU frequency limits in the intel_pstate
         driver (Srinivas Pandruvada)
    
       - Fixes and cleanups in the cpufreq core and generic governor code
         (Rafael Wysocki, Sai Gurrappadi)
    
       - intel_pstate driver optimizations and cleanups (Rafael Wysocki,
         Philippe Longepe, Chen Yu, Joe Perches)
    
       - cpufreq powernv driver fixes and cleanups (Akshay Adiga, Shilpasri
         Bhat)
    
       - cpufreq qoriq driver fixes and cleanups (Jia Hongtao)
    
       - ACPI cpufreq driver cleanups (Viresh Kumar)
    
       - Assorted cpufreq driver updates (Ashwin Chaugule, Geliang Tang,
         Javier Martinez Canillas, Paul Gortmaker, Sudeep Holla)
    
       - Assorted cpufreq fixes and cleanups (Joe Perches, Arnd Bergmann)
    
       - Fixes and cleanups in the OPP (Operating Performance Points)
         framework, mostly related to OPP sharing, and reorganization of
         OF-dependent code in it (Viresh Kumar, Arnd Bergmann, Sudeep Holla)
    
       - New "passive" governor for devfreq (for SoC subsystems that will
         rely on someone else for the management of their power resources)
         and consolidation of devfreq support for Exynos platforms, coding
         style and typo fixes for devfreq (Chanwoo Choi, MyungJoo Ham)
    
       - PM core fixes and cleanups, mostly to make it work better with the
         generic power domains (genpd) framework, and updates for that
         framework (Ulf Hansson, Thierry Reding, Colin Ian King)
    
       - Intel Broxton support for the intel_idle driver (Len Brown)
    
       - cpuidle core optimization and fix (Daniel Lezcano, Dave Gerlach)
    
       - ARM cpuidle cleanups (Jisheng Zhang)
    
       - Intel Kabylake support for the RAPL power capping driver (Jacob
         Pan)
    
       - AVS (Adaptive Voltage Switching) rockchip-io driver update (Heiko
         Stuebner)
    
       - Updates for the cpupower tool (Arjun Sreedharan, Colin Ian King,
         Mattia Dongili, Thomas Renninger)"
    
    * tag 'pm-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (112 commits)
      intel_pstate: Clean up get_target_pstate_use_performance()
      intel_pstate: Use sample.core_avg_perf in get_avg_pstate()
      intel_pstate: Clarify average performance computation
      intel_pstate: Avoid unnecessary synchronize_sched() during initialization
      cpufreq: schedutil: Make default depend on CONFIG_SMP
      cpufreq: powernv: del_timer_sync when global and local pstate are equal
      cpufreq: powernv: Move smp_call_function_any() out of irq safe block
      intel_pstate: Clean up intel_pstate_get()
      cpufreq: schedutil: Make it depend on CONFIG_SMP
      cpufreq: governor: Fix handling of special cases in dbs_update()
      PM / OPP: Move CONFIG_OF dependent code in a separate file
      cpufreq: intel_pstate: Ignore _PPC processing under HWP
      cpufreq: arm_big_little: use generic OPP functions for {init, free}_opp_table
      PM / OPP: add non-OF versions of dev_pm_opp_{cpumask_, }remove_table
      cpufreq: tango: Use generic platdev driver
      PM / OPP: pass cpumask by reference
      cpufreq: Fix GOV_LIMITS handling for the userspace governor
      cpupower: fix potential memory leak
      PM / devfreq: style/typo fixes
      PM / devfreq: exynos: Add the detailed correlation for Exynos5422 bus
      ..

commit 825a3b2605c3aa193e0075d0f9c72e33c17ab16a
Merge: cf6ed9a6682d ef0491ea17f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 14:47:16 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - massive CPU hotplug rework (Thomas Gleixner)
    
     - improve migration fairness (Peter Zijlstra)
    
     - CPU load calculation updates/cleanups (Yuyang Du)
    
     - cpufreq updates (Steve Muckle)
    
     - nohz optimizations (Frederic Weisbecker)
    
     - switch_mm() micro-optimization on x86 (Andy Lutomirski)
    
     - ... lots of other enhancements, fixes and cleanups.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (66 commits)
      ARM: Hide finish_arch_post_lock_switch() from modules
      sched/core: Provide a tsk_nr_cpus_allowed() helper
      sched/core: Use tsk_cpus_allowed() instead of accessing ->cpus_allowed
      sched/loadavg: Fix loadavg artifacts on fully idle and on fully loaded systems
      sched/fair: Correct unit of load_above_capacity
      sched/fair: Clean up scale confusion
      sched/nohz: Fix affine unpinned timers mess
      sched/fair: Fix fairness issue on migration
      sched/core: Kill sched_class::task_waking to clean up the migration logic
      sched/fair: Prepare to fix fairness problems on migration
      sched/fair: Move record_wakee()
      sched/core: Fix comment typo in wake_q_add()
      sched/core: Remove unused variable
      sched: Make hrtick_notifier an explicit call
      sched/fair: Make ilb_notifier an explicit call
      sched/hotplug: Make activate() the last hotplug step
      sched/hotplug: Move migration CPU_DYING to sched_cpu_dying()
      sched/migration: Move CPU_ONLINE into scheduler state
      sched/migration: Move calc_load_migrate() into CPU_DYING
      sched/migration: Move prepare transition to SCHED_STARTING state
      ...

commit 230e51f21101e49c8d73018d414adbd0d57459a1
Merge: a3871bd434cf 91c6180572e2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 12:25:25 2016 -0700

    Merge branch 'core-signals-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core signal updates from Ingo Molnar:
     "These updates from Stas Sergeev and Andy Lutomirski, improve the
      sigaltstack interface by extending its ABI with the SS_AUTODISARM
      feature, which makes it possible to use swapcontext() in a sighandler
      that works on sigaltstack.  Without this flag, the subsequent signal
      will corrupt the state of the switched-away sighandler.
    
      The inspiration is more robust dosemu signal handling"
    
    * 'core-signals-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      signals/sigaltstack: Change SS_AUTODISARM to (1U << 31)
      signals/sigaltstack: Report current flag bits in sigaltstack()
      selftests/sigaltstack: Fix the sigaltstack test on old kernels
      signals/sigaltstack: If SS_AUTODISARM, bypass on_sig_stack()
      selftests/sigaltstack: Add new testcase for sigaltstack(SS_ONSTACK|SS_AUTODISARM)
      signals/sigaltstack: Implement SS_AUTODISARM flag
      signals/sigaltstack: Prepare to add new SS_xxx flags
      signals/sigaltstack, x86/signals: Unify the x86 sigaltstack check with other architectures

commit 0052af4411b048eb6c0b0adb73d0fb4803ba1794
Merge: 0fed3ac866ea d18d12d0ff07
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 11:36:02 2016 -0700

    Merge branch 'core-lib-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core/lib update from Ingo Molnar:
     "This contains a single commit that removes an unused facility that the
      scheduler used to make use of"
    
    * 'core-lib-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      lib/proportions: Remove unused code

commit 50605ffbdaf6d7ccab70d4631fd8347fc78af14f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 11 14:23:31 2016 +0200

    sched/core: Provide a tsk_nr_cpus_allowed() helper
    
    tsk_nr_cpus_allowed() is an accessor for task->nr_cpus_allowed which allows
    us to change the representation of ->nr_cpus_allowed if required.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1462969411-17735-2-git-send-email-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f8fea8a5b5ab..38526b67e787 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1930,6 +1930,11 @@ extern int arch_task_struct_size __read_mostly;
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
+static inline int tsk_nr_cpus_allowed(struct task_struct *p)
+{
+	return p->nr_cpus_allowed;
+}
+
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04

commit 4eb867651721228ee2eeae142c53378375303e8b
Merge: eb60b3e5e8df e5ef27d0f5ac
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 12 09:51:36 2016 +0200

    Merge branch 'smp/hotplug' into sched/core, to resolve conflicts
    
    Conflicts:
            kernel/sched/core.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f2785ddb5367e217365099294b89d6a84668069e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 10 12:54:18 2016 +0100

    sched/hotplug: Move migration CPU_DYING to sched_cpu_dying()
    
    Remove the hotplug notifier and make it an explicit state.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160310120025.502222097@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e5f961b1a74..47835cf8aefa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -376,6 +376,12 @@ extern int sched_cpu_starting(unsigned int cpu);
 extern int sched_cpu_activate(unsigned int cpu);
 extern int sched_cpu_deactivate(unsigned int cpu);
 
+#ifdef CONFIG_HOTPLUG_CPU
+extern int sched_cpu_dying(unsigned int cpu);
+#else
+# define sched_cpu_dying	NULL
+#endif
+
 extern void sched_show_task(struct task_struct *p);
 
 #ifdef CONFIG_LOCKUP_DETECTOR

commit 40190a78f85fec29f0fdd21f6b4415712085711e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 10 12:54:13 2016 +0100

    sched/hotplug: Convert cpu_[in]active notifiers to state machine
    
    Now that we reduced everything into single notifiers, it's simple to move them
    into the hotplug state machine space.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 39597d0a005e..1e5f961b1a74 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -373,6 +373,8 @@ extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 extern int sched_cpu_starting(unsigned int cpu);
+extern int sched_cpu_activate(unsigned int cpu);
+extern int sched_cpu_deactivate(unsigned int cpu);
 
 extern void sched_show_task(struct task_struct *p);
 

commit 9cf7243d5d83d27aca47f842107bfa02b5f11d16
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 10 12:54:09 2016 +0100

    sched: Make set_cpu_rq_start_time() a built in hotplug state
    
    Start distangling the maze of hotplug notifiers in the scheduler.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 52c4847b05e2..39597d0a005e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -372,6 +372,7 @@ extern void cpu_init (void);
 extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
+extern int sched_cpu_starting(unsigned int cpu);
 
 extern void sched_show_task(struct task_struct *p);
 

commit 7b5953345efe4f226bb52cbea04558d16ec7ebfa
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Tue Apr 5 12:12:28 2016 +0800

    sched/fair: Add detailed description to the sched load avg metrics
    
    These sched metrics have become complex enough, so describe them
    in detail at their definition.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ Fixed the text to improve its spelling and typography. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: lizefan@huawei.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1459829551-21625-4-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d779d70a3a5..57faf789c88f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1211,18 +1211,56 @@ struct load_weight {
 };
 
 /*
- * The load_avg/util_avg accumulates an infinite geometric series.
- * 1) load_avg factors frequency scaling into the amount of time that a
- * sched_entity is runnable on a rq into its weight. For cfs_rq, it is the
- * aggregated such weights of all runnable and blocked sched_entities.
- * 2) util_avg factors frequency and cpu capacity scaling into the amount of time
- * that a sched_entity is running on a CPU, in the range [0..SCHED_CAPACITY_SCALE].
- * For cfs_rq, it is the aggregated such times of all runnable and
+ * The load_avg/util_avg accumulates an infinite geometric series
+ * (see __update_load_avg() in kernel/sched/fair.c).
+ *
+ * [load_avg definition]
+ *
+ *   load_avg = runnable% * scale_load_down(load)
+ *
+ * where runnable% is the time ratio that a sched_entity is runnable.
+ * For cfs_rq, it is the aggregated load_avg of all runnable and
  * blocked sched_entities.
- * The 64 bit load_sum can:
- * 1) for cfs_rq, afford 4353082796 (=2^64/47742/88761) entities with
- * the highest weight (=88761) always runnable, we should not overflow
- * 2) for entity, support any load.weight always runnable
+ *
+ * load_avg may also take frequency scaling into account:
+ *
+ *   load_avg = runnable% * scale_load_down(load) * freq%
+ *
+ * where freq% is the CPU frequency normalized to the highest frequency.
+ *
+ * [util_avg definition]
+ *
+ *   util_avg = running% * SCHED_CAPACITY_SCALE
+ *
+ * where running% is the time ratio that a sched_entity is running on
+ * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable
+ * and blocked sched_entities.
+ *
+ * util_avg may also factor frequency scaling and CPU capacity scaling:
+ *
+ *   util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%
+ *
+ * where freq% is the same as above, and capacity% is the CPU capacity
+ * normalized to the greatest capacity (due to uarch differences, etc).
+ *
+ * N.B., the above ratios (runnable%, running%, freq%, and capacity%)
+ * themselves are in the range of [0, 1]. To do fixed point arithmetics,
+ * we therefore scale them to as large a range as necessary. This is for
+ * example reflected by util_avg's SCHED_CAPACITY_SCALE.
+ *
+ * [Overflow issue]
+ *
+ * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities
+ * with the highest load (=88761), always runnable on a single cfs_rq,
+ * and should not overflow as the number already hits PID_MAX_LIMIT.
+ *
+ * For all other cases (including 32-bit kernels), struct load_weight's
+ * weight will overflow first before we do, because:
+ *
+ *    Max(load_avg) <= Max(load.weight)
+ *
+ * Then it is the load_weight's responsibility to consider overflow
+ * issues.
  */
 struct sched_avg {
 	u64 last_update_time, load_sum;

commit 6ecdd74962f246dfe8750b7bea481a1c0816315d
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Tue Apr 5 12:12:26 2016 +0800

    sched/fair: Generalize the load/util averages resolution definition
    
    Integer metric needs fixed point arithmetic. In sched/fair, a few
    metrics, e.g., weight, load, load_avg, util_avg, freq, and capacity,
    may have different fixed point ranges, which makes their update and
    usage error-prone.
    
    In order to avoid the errors relating to the fixed point range, we
    definie a basic fixed point range, and then formalize all metrics to
    base on the basic range.
    
    The basic range is 1024 or (1 << 10). Further, one can recursively
    apply the basic range to have larger range.
    
    Pointed out by Ben Segall, weight (visible to user, e.g., NICE-0 has
    1024) and load (e.g., NICE_0_LOAD) have independent ranges, but they
    must be well calibrated.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: lizefan@huawei.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1459829551-21625-2-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d894f2d61388..7d779d70a3a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -936,10 +936,20 @@ enum cpu_idle_type {
 	CPU_MAX_IDLE_TYPES
 };
 
+/*
+ * Integer metrics need fixed point arithmetic, e.g., sched/fair
+ * has a few: load, load_avg, util_avg, freq, and capacity.
+ *
+ * We define a basic fixed point arithmetic range, and then formalize
+ * all these metrics based on that basic range.
+ */
+# define SCHED_FIXEDPOINT_SHIFT	10
+# define SCHED_FIXEDPOINT_SCALE	(1L << SCHED_FIXEDPOINT_SHIFT)
+
 /*
  * Increase resolution of cpu_capacity calculations
  */
-#define SCHED_CAPACITY_SHIFT	10
+#define SCHED_CAPACITY_SHIFT	SCHED_FIXEDPOINT_SHIFT
 #define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
 
 /*
@@ -1205,8 +1215,8 @@ struct load_weight {
  * 1) load_avg factors frequency scaling into the amount of time that a
  * sched_entity is runnable on a rq into its weight. For cfs_rq, it is the
  * aggregated such weights of all runnable and blocked sched_entities.
- * 2) util_avg factors frequency and cpu scaling into the amount of time
- * that a sched_entity is running on a CPU, in the range [0..SCHED_LOAD_SCALE].
+ * 2) util_avg factors frequency and cpu capacity scaling into the amount of time
+ * that a sched_entity is running on a CPU, in the range [0..SCHED_CAPACITY_SCALE].
  * For cfs_rq, it is the aggregated such times of all runnable and
  * blocked sched_entities.
  * The 64 bit load_sum can:

commit c876eeab6432687846d4cd5fe1e43dbc348de134
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue May 3 10:31:49 2016 -0700

    signals/sigaltstack: If SS_AUTODISARM, bypass on_sig_stack()
    
    If a signal stack is set up with SS_AUTODISARM, then the kernel
    inherently avoids incorrectly resetting the signal stack if signals
    recurse: the signal stack will be reset on the first signal
    delivery.  This means that we don't need check the stack pointer
    when delivering signals if SS_AUTODISARM is set.
    
    This will make segmented x86 programs more robust: currently there's
    a hole that could be triggered if ESP/RSP appears to point to the
    signal stack but actually doesn't due to a nonzero SS base.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Amanieu d'Antras <amanieu@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Paul Moore <pmoore@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Stas Sergeev <stsp@list.ru>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/c46bee4654ca9e68c498462fd11746e2bd0d98c8.1462296606.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2950c5cd3005..77fd49f20c5f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2576,6 +2576,18 @@ static inline int kill_cad_pid(int sig, int priv)
  */
 static inline int on_sig_stack(unsigned long sp)
 {
+	/*
+	 * If the signal stack is SS_AUTODISARM then, by construction, we
+	 * can't be on the signal stack unless user code deliberately set
+	 * SS_AUTODISARM when we were already on it.
+	 *
+	 * This improves reliability: if user state gets corrupted such that
+	 * the stack pointer points very close to the end of the signal stack,
+	 * then this check will enable the signal to be handled anyway.
+	 */
+	if (current->sas_ss_flags & SS_AUTODISARM)
+		return 0;
+
 #ifdef CONFIG_STACK_GROWSUP
 	return sp >= current->sas_ss_sp &&
 		sp - current->sas_ss_sp < current->sas_ss_size;

commit 2a74213838104a41588d86fd5e8d344972891ace
Author: Stas Sergeev <stsp@list.ru>
Date:   Thu Apr 14 23:20:04 2016 +0300

    signals/sigaltstack: Implement SS_AUTODISARM flag
    
    This patch implements the SS_AUTODISARM flag that can be OR-ed with
    SS_ONSTACK when forming ss_flags.
    
    When this flag is set, sigaltstack will be disabled when entering
    the signal handler; more precisely, after saving sas to uc_stack.
    When leaving the signal handler, the sigaltstack is restored by
    uc_stack.
    
    When this flag is used, it is safe to switch from sighandler with
    swapcontext(). Without this flag, the subsequent signal will corrupt
    the state of the switched-away sighandler.
    
    To detect the support of this functionality, one can do:
    
      err = sigaltstack(SS_DISABLE | SS_AUTODISARM);
      if (err && errno == EINVAL)
            unsupported();
    
    Signed-off-by: Stas Sergeev <stsp@list.ru>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Amanieu d'Antras <amanieu@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Paul Moore <pmoore@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: linux-api@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1460665206-13646-4-git-send-email-stsp@list.ru
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 52c4847b05e2..2950c5cd3005 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1596,6 +1596,7 @@ struct task_struct {
 
 	unsigned long sas_ss_sp;
 	size_t sas_ss_size;
+	unsigned sas_ss_flags;
 
 	struct callback_head *task_works;
 
@@ -2592,6 +2593,13 @@ static inline int sas_ss_flags(unsigned long sp)
 	return on_sig_stack(sp) ? SS_ONSTACK : 0;
 }
 
+static inline void sas_ss_reset(struct task_struct *p)
+{
+	p->sas_ss_sp = 0;
+	p->sas_ss_size = 0;
+	p->sas_ss_flags = SS_DISABLE;
+}
+
 static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
 {
 	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))

commit 29c5e7b2bc875c4ad5d3fd9e3c4ed2702e705bba
Merge: 6f707daa3833 b4f4b4b37133
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 28 15:19:31 2016 +0200

    Merge back earlier cpufreq material for v4.7.

commit 1f41906a6fda1114debd3898668bd7ab6470ee41
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Apr 13 15:56:51 2016 +0200

    sched/fair: Correctly handle nohz ticks CPU load accounting
    
    Ticks can happen while the CPU is in dynticks-idle or dynticks-singletask
    mode. In fact "nohz" or "dynticks" only mean that we exit the periodic
    mode and we try to minimize the ticks as much as possible. The nohz
    subsystem uses a confusing terminology with the internal state
    "ts->tick_stopped" which is also available through its public interface
    with tick_nohz_tick_stopped(). This is a misnomer as the tick is instead
    reduced with the best effort rather than stopped. In the best case the
    tick can indeed be actually stopped but there is no guarantee about that.
    If a timer needs to fire one second later, a tick will fire while the
    CPU is in nohz mode and this is a very common scenario.
    
    Now this confusion happens to be a problem with CPU load updates:
    cpu_load_update_active() doesn't handle nohz ticks correctly because it
    assumes that ticks are completely stopped in nohz mode and that
    cpu_load_update_active() can't be called in dynticks mode. When that
    happens, the whole previous tickless load is ignored and the function
    just records the load for the current tick, ignoring potentially long
    idle periods behind.
    
    In order to solve this, we could account the current load for the
    previous nohz time but there is a risk that we account the load of a
    task that got freshly enqueued for the whole nohz period.
    
    So instead, lets record the dynticks load on nohz frame entry so we know
    what to record in case of nohz ticks, then use this record to account
    the tickless load on nohz ticks and nohz frame end.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1460555812-25375-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0b7f6028a50b..d894f2d61388 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -178,9 +178,11 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 extern void calc_global_load(unsigned long ticks);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
-extern void cpu_load_update_nohz(int active);
+extern void cpu_load_update_nohz_start(void);
+extern void cpu_load_update_nohz_stop(void);
 #else
-static inline void cpu_load_update_nohz(int active) { }
+static inline void cpu_load_update_nohz_start(void) { }
+static inline void cpu_load_update_nohz_stop(void) { }
 #endif
 
 extern void dump_cpu_task(int cpu);

commit cee1afce3053e7aa0793fbd5f2e845fa2cef9e33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Apr 13 15:56:50 2016 +0200

    sched/fair: Gather CPU load functions under a more conventional namespace
    
    The CPU load update related functions have a weak naming convention
    currently, starting with update_cpu_load_*() which isn't ideal as
    "update" is a very generic concept.
    
    Since two of these functions are public already (and a third is to come)
    that's enough to introduce a more conventional naming scheme. So let's
    do the following rename instead:
    
            update_cpu_load_*() -> cpu_load_update_*()
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1460555812-25375-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 13c1c1d07270..0b7f6028a50b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -178,9 +178,9 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 extern void calc_global_load(unsigned long ticks);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
-extern void update_cpu_load_nohz(int active);
+extern void cpu_load_update_nohz(int active);
 #else
-static inline void update_cpu_load_nohz(int active) { }
+static inline void cpu_load_update_nohz(int active) { }
 #endif
 
 extern void dump_cpu_task(int cpu);

commit 2c923e94cd9c6acff3b22f0ae29cfe65e2658b40
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Apr 11 16:38:34 2016 +0200

    sched/clock: Make local_clock()/cpu_clock() inline
    
    The local_clock/cpu_clock functions were changed to prevent a double
    identical test with sched_clock_cpu() when HAVE_UNSTABLE_SCHED_CLOCK
    is set. That resulted in one line functions.
    
    As these functions are in all the cases one line functions and in the
    hot path, it is useful to specify them as static inline in order to
    give a strong hint to the compiler.
    
    After verification, it appears the compiler does not inline them
    without this hint. Change those functions to static inline.
    
    sched_clock_cpu() is called via the inlined local_clock()/cpu_clock()
    functions from sched.h. So any module code including sched.h will
    reference sched_clock_cpu(). Thus it must be exported with the
    EXPORT_SYMBOL_GPL macro.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1460385514-14700-2-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 52c4847b05e2..13c1c1d07270 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2303,8 +2303,6 @@ extern unsigned long long notrace sched_clock(void);
 /*
  * See the comment in kernel/sched/clock.c
  */
-extern u64 cpu_clock(int cpu);
-extern u64 local_clock(void);
 extern u64 running_clock(void);
 extern u64 sched_clock_cpu(int cpu);
 
@@ -2323,6 +2321,16 @@ static inline void sched_clock_idle_sleep_event(void)
 static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
+
+static inline u64 cpu_clock(int cpu)
+{
+	return sched_clock();
+}
+
+static inline u64 local_clock(void)
+{
+	return sched_clock();
+}
 #else
 /*
  * Architectures can set this to 1 if they have specified
@@ -2337,6 +2345,26 @@ extern void clear_sched_clock_stable(void);
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
+
+/*
+ * As outlined in clock.c, provides a fast, high resolution, nanosecond
+ * time source that is monotonic per cpu argument and has bounded drift
+ * between cpus.
+ *
+ * ######################### BIG FAT WARNING ##########################
+ * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #
+ * # go backwards !!                                                  #
+ * ####################################################################
+ */
+static inline u64 cpu_clock(int cpu)
+{
+	return sched_clock_cpu(cpu);
+}
+
+static inline u64 local_clock(void)
+{
+	return sched_clock_cpu(raw_smp_processor_id());
+}
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING

commit 5f47992491ffe2d5b2b4ea3556bc0f3c0ec9bc8b
Merge: 148e45dc87cb bf1620068911
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Mon Apr 11 09:30:50 2016 -0700

    Merge 4.6-rc3 into staging-next
    
    This resolves a lot of merge issues with PAGE_CACHE_* changes, and an
    iio driver merge issue.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 77ed2c5745d93416992857d124f35834b62b3e70
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Tue Mar 8 20:01:32 2016 +0900

    android,lowmemorykiller: Don't abuse TIF_MEMDIE.
    
    Currently, lowmemorykiller (LMK) is using TIF_MEMDIE for two purposes.
    One is to remember processes killed by LMK, and the other is to
    accelerate termination of processes killed by LMK.
    
    But since LMK is invoked as a memory shrinker function, there still
    should be some memory available. It is very likely that memory
    allocations by processes killed by LMK will succeed without using
    ALLOC_NO_WATERMARKS via TIF_MEMDIE. Even if their allocations cannot
    escape from memory allocation loop unless they use ALLOC_NO_WATERMARKS,
    lowmem_deathpending_timeout can guarantee forward progress by choosing
    next victim process.
    
    On the other hand, mark_oom_victim() assumes that it must be called with
    oom_lock held and it must not be called after oom_killer_disable() was
    called. But LMK is calling it without holding oom_lock and checking
    oom_killer_disabled. It is possible that LMK calls mark_oom_victim()
    due to allocation requests by kernel threads after current thread
    returned from oom_killer_disabled(). This will break synchronization
    for PM/suspend.
    
    This patch introduces per a task_struct flag for remembering processes
    killed by LMK, and replaces TIF_MEMDIE with that flag. By applying this
    patch, assumption by mark_oom_victim() becomes true.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Arve Hjonnevag <arve@android.com>
    Cc: Riley Andrews <riandrews@android.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 60bba7e032dc..9dff190e6a0a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2184,6 +2184,7 @@ static inline void memalloc_noio_restore(unsigned int flags)
 #define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 #define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
 #define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
+#define PFA_LMK_WAITING  3      /* Lowmemorykiller is waiting */
 
 
 #define TASK_PFA_TEST(name, func)					\
@@ -2207,6 +2208,9 @@ TASK_PFA_TEST(SPREAD_SLAB, spread_slab)
 TASK_PFA_SET(SPREAD_SLAB, spread_slab)
 TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 
+TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
+TASK_PFA_SET(LMK_WAITING, lmk_waiting)
+
 /*
  * task->jobctl flags
  */

commit 0bed612be638e41456cd8cb270a2b411a5b43d63
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Apr 2 01:08:43 2016 +0200

    cpufreq: sched: Helpers to add and remove update_util hooks
    
    Replace the single helper for adding and removing cpufreq utilization
    update hooks, cpufreq_set_update_util_data(), with a pair of helpers,
    cpufreq_add_update_util_hook() and cpufreq_remove_update_util_hook(),
    and modify the users of cpufreq_set_update_util_data() accordingly.
    
    With the new helpers, the code using them doesn't need to worry
    about the internals of struct update_util_data and in particular
    it doesn't need to worry about populating the func field in it
    properly upfront.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 60bba7e032dc..1b8825922597 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3240,7 +3240,10 @@ struct update_util_data {
 		     u64 time, unsigned long util, unsigned long max);
 };
 
-void cpufreq_set_update_util_data(int cpu, struct update_util_data *data);
+void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,
+			void (*func)(struct update_util_data *data, u64 time,
+				     unsigned long util, unsigned long max));
+void cpufreq_remove_update_util_hook(int cpu);
 #endif /* CONFIG_CPU_FREQ */
 
 #endif

commit d18d12d0ff07c47fb913f297c174f30a3f96042d
Author: Richard Cochran <rcochran@linutronix.de>
Date:   Thu Mar 31 15:51:32 2016 +0200

    lib/proportions: Remove unused code
    
    By accident I stumbled across code that is no longer used.  According
    to git grep, the global functions in lib/proportions.c are not used
    anywhere.  This patch removes the old, unused code.
    
    Peter Zijlstra further commented:
    
     "Ah indeed, that got replaced with the flex proportion code a while back."
    
    Signed-off-by: Richard Cochran <rcochran@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/4265b49bed713fbe3faaf8c05da0e1792f09c0b3.1459432020.git.rcochran@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 60bba7e032dc..6dd25d1869b5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -40,7 +40,6 @@ struct sched_param {
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>
-#include <linux/proportions.h>
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
 #include <linux/rculist.h>

commit f009a7a767e792d5ab0b46c08d46236ea5271dd9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Mar 24 15:38:00 2016 +0100

    timers/nohz: Convert tick dependency mask to atomic_t
    
    The tick dependency mask was intially unsigned long because this is the
    type on which clear_bit() operates on and fetch_or() accepts it.
    
    But now that we have atomic_fetch_or(), we can instead use
    atomic_andnot() to clear the bit. This consolidates the type of our
    tick dependency mask, reduce its size on structures and benefit from
    possible architecture optimizations on atomic_t operations.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1458830281-4255-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 60bba7e032dc..52c4847b05e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -720,7 +720,7 @@ struct signal_struct {
 	struct task_cputime cputime_expires;
 
 #ifdef CONFIG_NO_HZ_FULL
-	unsigned long tick_dep_mask;
+	atomic_t tick_dep_mask;
 #endif
 
 	struct list_head cpu_timers[3];
@@ -1549,7 +1549,7 @@ struct task_struct {
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL
-	unsigned long tick_dep_mask;
+	atomic_t tick_dep_mask;
 #endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	u64 start_time;		/* monotonic time in nsec */

commit bb29902a7515208846114b3b36a4281a9bbf766a
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Mar 25 14:20:44 2016 -0700

    oom, oom_reaper: protect oom_reaper_list using simpler way
    
    "oom, oom_reaper: disable oom_reaper for oom_kill_allocating_task" tried
    to protect oom_reaper_list using MMF_OOM_KILLED flag.  But we can do it
    by simply checking tsk->oom_reaper_list != NULL.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 49b1febcf7c3..60bba7e032dc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -512,8 +512,6 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 
-#define MMF_OOM_KILLED		21	/* OOM killer has chosen this mm */
-
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
 struct sighand_struct {

commit 29c696e1c6eceb5db6b21f0c89495fcfcd40c0eb
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Fri Mar 25 14:20:39 2016 -0700

    oom: make oom_reaper_list single linked
    
    Entries are only added/removed from oom_reaper_list at head so we can
    use a single linked list and hence save a word in task_struct.
    
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c2d2d7c5d463..49b1febcf7c3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1852,7 +1852,7 @@ struct task_struct {
 #endif
 	int pagefault_disabled;
 #ifdef CONFIG_MMU
-	struct list_head oom_reaper_list;
+	struct task_struct *oom_reaper_list;
 #endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;

commit 855b018325737f7691f9b7d86339df40aa4e47c3
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Mar 25 14:20:36 2016 -0700

    oom, oom_reaper: disable oom_reaper for oom_kill_allocating_task
    
    Tetsuo has reported that oom_kill_allocating_task=1 will cause
    oom_reaper_list corruption because oom_kill_process doesn't follow
    standard OOM exclusion (aka ignores TIF_MEMDIE) and allows to enqueue
    the same task multiple times - e.g.  by sacrificing the same child
    multiple times.
    
    This patch fixes the issue by introducing a new MMF_OOM_KILLED mm flag
    which is set in oom_kill_process atomically and oom reaper is disabled
    if the flag was already set.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 788f223f8f8f..c2d2d7c5d463 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -512,6 +512,8 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_HAS_UPROBES		19	/* has uprobes */
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 
+#define MMF_OOM_KILLED		21	/* OOM killer has chosen this mm */
+
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
 struct sighand_struct {

commit 03049269de433cb5fe2859be9ae4469ceb1163ed
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Mar 25 14:20:33 2016 -0700

    mm, oom_reaper: implement OOM victims queuing
    
    wake_oom_reaper has allowed only 1 oom victim to be queued.  The main
    reason for that was the simplicity as other solutions would require some
    way of queuing.  The current approach is racy and that was deemed
    sufficient as the oom_reaper is considered a best effort approach to
    help with oom handling when the OOM victim cannot terminate in a
    reasonable time.  The race could lead to missing an oom victim which can
    get stuck
    
    out_of_memory
      wake_oom_reaper
        cmpxchg // OK
                            oom_reaper
                              oom_reap_task
                                __oom_reap_task
    oom_victim terminates
                                  atomic_inc_not_zero // fail
    out_of_memory
      wake_oom_reaper
        cmpxchg // fails
                              task_to_reap = NULL
    
    This race requires 2 OOM invocations in a short time period which is not
    very likely but certainly not impossible.  E.g.  the original victim
    might have not released a lot of memory for some reason.
    
    The situation would improve considerably if wake_oom_reaper used a more
    robust queuing.  This is what this patch implements.  This means adding
    oom_reaper_list list_head into task_struct (eat a hole before embeded
    thread_struct for that purpose) and a oom_reaper_lock spinlock for
    queuing synchronization.  wake_oom_reaper will then add the task on the
    queue and oom_reaper will dequeue it.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Andrea Argangeli <andrea@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 478b41de7f7d..788f223f8f8f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1849,6 +1849,9 @@ struct task_struct {
 	unsigned long	task_state_change;
 #endif
 	int pagefault_disabled;
+#ifdef CONFIG_MMU
+	struct list_head oom_reaper_list;
+#endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /*

commit 69b27baf00fa9b7b14b3263c105390d1683425b2
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Mar 25 14:20:21 2016 -0700

    sched: add schedule_timeout_idle()
    
    This will be needed in the patch "mm, oom: introduce oom reaper".
    
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 589c4780b077..478b41de7f7d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -426,6 +426,7 @@ extern signed long schedule_timeout(signed long timeout);
 extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
+extern signed long schedule_timeout_idle(signed long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
 

commit 6c31da3464b4d28825d1827ee41a3a217b2dcf0e
Author: Helge Deller <deller@gmx.de>
Date:   Sat Mar 19 17:54:10 2016 +0100

    parisc,metag: Implement CONFIG_DEBUG_STACK_USAGE option
    
    On parisc and metag the stack grows upwards, so for those we need to
    scan the stack downwards in order to calculate how much stack a process
    has used.
    
    Tested on a 64bit parisc kernel.
    
    Signed-off-by: Helge Deller <deller@gmx.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 34495d2d2d7b..589c4780b077 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2870,10 +2870,18 @@ static inline unsigned long stack_not_used(struct task_struct *p)
 	unsigned long *n = end_of_stack(p);
 
 	do { 	/* Skip over canary */
+# ifdef CONFIG_STACK_GROWSUP
+		n--;
+# else
 		n++;
+# endif
 	} while (!*n);
 
+# ifdef CONFIG_STACK_GROWSUP
+	return (unsigned long)end_of_stack(p) - (unsigned long)n;
+# else
 	return (unsigned long)n - (unsigned long)end_of_stack(p);
+# endif
 }
 #endif
 extern void set_task_stack_end_magic(struct task_struct *tsk);

commit 5c9a8750a6409c63a0f01d51a9024861022f6593
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Mar 22 14:27:30 2016 -0700

    kernel: add kcov code coverage
    
    kcov provides code coverage collection for coverage-guided fuzzing
    (randomized testing).  Coverage-guided fuzzing is a testing technique
    that uses coverage feedback to determine new interesting inputs to a
    system.  A notable user-space example is AFL
    (http://lcamtuf.coredump.cx/afl/).  However, this technique is not
    widely used for kernel testing due to missing compiler and kernel
    support.
    
    kcov does not aim to collect as much coverage as possible.  It aims to
    collect more or less stable coverage that is function of syscall inputs.
    To achieve this goal it does not collect coverage in soft/hard
    interrupts and instrumentation of some inherently non-deterministic or
    non-interesting parts of kernel is disbled (e.g.  scheduler, locking).
    
    Currently there is a single coverage collection mode (tracing), but the
    API anticipates additional collection modes.  Initially I also
    implemented a second mode which exposes coverage in a fixed-size hash
    table of counters (what Quentin used in his original patch).  I've
    dropped the second mode for simplicity.
    
    This patch adds the necessary support on kernel side.  The complimentary
    compiler support was added in gcc revision 231296.
    
    We've used this support to build syzkaller system call fuzzer, which has
    found 90 kernel bugs in just 2 months:
    
      https://github.com/google/syzkaller/wiki/Found-Bugs
    
    We've also found 30+ bugs in our internal systems with syzkaller.
    Another (yet unexplored) direction where kcov coverage would greatly
    help is more traditional "blob mutation".  For example, mounting a
    random blob as a filesystem, or receiving a random blob over wire.
    
    Why not gcov.  Typical fuzzing loop looks as follows: (1) reset
    coverage, (2) execute a bit of code, (3) collect coverage, repeat.  A
    typical coverage can be just a dozen of basic blocks (e.g.  an invalid
    input).  In such context gcov becomes prohibitively expensive as
    reset/collect coverage steps depend on total number of basic
    blocks/edges in program (in case of kernel it is about 2M).  Cost of
    kcov depends only on number of executed basic blocks/edges.  On top of
    that, kernel requires per-thread coverage because there are always
    background threads and unrelated processes that also produce coverage.
    With inlined gcov instrumentation per-thread coverage is not possible.
    
    kcov exposes kernel PCs and control flow to user-space which is
    insecure.  But debugfs should not be mapped as user accessible.
    
    Based on a patch by Quentin Casasnovas.
    
    [akpm@linux-foundation.org: make task_struct.kcov_mode have type `enum kcov_mode']
    [akpm@linux-foundation.org: unbreak allmodconfig]
    [akpm@linux-foundation.org: follow x86 Makefile layout standards]
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: syzkaller <syzkaller@googlegroups.com>
    Cc: Vegard Nossum <vegard.nossum@oracle.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tavis Ormandy <taviso@google.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: David Drysdale <drysdale@google.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 084ed9fba620..34495d2d2d7b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -51,6 +51,7 @@ struct sched_param {
 #include <linux/resource.h>
 #include <linux/timer.h>
 #include <linux/hrtimer.h>
+#include <linux/kcov.h>
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
 #include <linux/cred.h>
@@ -1818,6 +1819,16 @@ struct task_struct {
 	/* bitmask and counter of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
+#ifdef CONFIG_KCOV
+	/* Coverage collection mode enabled for this task (0 if disabled). */
+	enum kcov_mode kcov_mode;
+	/* Size of the kcov_area. */
+	unsigned	kcov_size;
+	/* Buffer for coverage collection. */
+	void		*kcov_area;
+	/* kcov desciptor wired with this task or NULL. */
+	struct kcov	*kcov;
+#endif
 #ifdef CONFIG_MEMCG
 	struct mem_cgroup *memcg_in_oom;
 	gfp_t memcg_oom_gfp_mask;

commit 814a2bf957739f367cbebfa1b60237387b72d0ee
Merge: 237045fc3c67 f9310b2f9a19
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 18 19:26:54 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge second patch-bomb from Andrew Morton:
    
     - a couple of hotfixes
    
     - the rest of MM
    
     - a new timer slack control in procfs
    
     - a couple of procfs fixes
    
     - a few misc things
    
     - some printk tweaks
    
     - lib/ updates, notably to radix-tree.
    
     - add my and Nick Piggin's old userspace radix-tree test harness to
       tools/testing/radix-tree/.  Matthew said it was a godsend during the
       radix-tree work he did.
    
     - a few code-size improvements, switching to __always_inline where gcc
       screwed up.
    
     - partially implement character sets in sscanf
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (118 commits)
      sscanf: implement basic character sets
      lib/bug.c: use common WARN helper
      param: convert some "on"/"off" users to strtobool
      lib: add "on"/"off" support to kstrtobool
      lib: update single-char callers of strtobool()
      lib: move strtobool() to kstrtobool()
      include/linux/unaligned: force inlining of byteswap operations
      include/uapi/linux/byteorder, swab: force inlining of some byteswap operations
      include/asm-generic/atomic-long.h: force inlining of some atomic_long operations
      usb: common: convert to use match_string() helper
      ide: hpt366: convert to use match_string() helper
      ata: hpt366: convert to use match_string() helper
      power: ab8500: convert to use match_string() helper
      power: charger_manager: convert to use match_string() helper
      drm/edid: convert to use match_string() helper
      pinctrl: convert to use match_string() helper
      device property: convert to use match_string() helper
      lib/string: introduce match_string() helper
      radix-tree tests: add test for radix_tree_iter_next
      radix-tree tests: add regression3 test
      ...

commit da8b44d5a9f8bf26da637b7336508ca534d6b319
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Mar 17 14:20:51 2016 -0700

    timer: convert timer_slack_ns from unsigned long to u64
    
    This patchset introduces a /proc/<pid>/timerslack_ns interface which
    would allow controlling processes to be able to set the timerslack value
    on other processes in order to save power by avoiding wakeups (Something
    Android currently does via out-of-tree patches).
    
    The first patch tries to fix the internal timer_slack_ns usage which was
    defined as a long, which limits the slack range to ~4 seconds on 32bit
    systems.  It converts it to a u64, which provides the same basically
    unlimited slack (500 years) on both 32bit and 64bit machines.
    
    The second patch introduces the /proc/<pid>/timerslack_ns interface
    which allows the full 64bit slack range for a task to be read or set on
    both 32bit and 64bit machines.
    
    With these two patches, on a 32bit machine, after setting the slack on
    bash to 10 seconds:
    
    $ time sleep 1
    
    real    0m10.747s
    user    0m0.001s
    sys     0m0.005s
    
    The first patch is a little ugly, since I had to chase the slack delta
    arguments through a number of functions converting them to u64s.  Let me
    know if it makes sense to break that up more or not.
    
    Other than that things are fairly straightforward.
    
    This patch (of 2):
    
    The timer_slack_ns value in the task struct is currently a unsigned
    long.  This means that on 32bit applications, the maximum slack is just
    over 4 seconds.  However, on 64bit machines, its much much larger (~500
    years).
    
    This disparity could make application development a little (as well as
    the default_slack) to a u64.  This means both 32bit and 64bit systems
    have the same effective internal slack range.
    
    Now the existing ABI via PR_GET_TIMERSLACK and PR_SET_TIMERSLACK specify
    the interface as a unsigned long, so we preserve that limitation on
    32bit systems, where SET_TIMERSLACK can only set the slack to a unsigned
    long value, and GET_TIMERSLACK will return ULONG_MAX if the slack is
    actually larger then what can be stored by an unsigned long.
    
    This patch also modifies hrtimer functions which specified the slack
    delta as a unsigned long.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oren Laadan <orenl@cellrox.com>
    Cc: Ruchi Kandoi <kandoiruchi@google.com>
    Cc: Rom Lemarchand <romlem@android.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Android Kernel Team <kernel-team@android.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eb7f2f84009b..3284d07edec7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1792,8 +1792,8 @@ struct task_struct {
 	 * time slack values; these are used to round up poll() and
 	 * select() etc timeout values. These are in nanoseconds.
 	 */
-	unsigned long timer_slack_ns;
-	unsigned long default_timer_slack_ns;
+	u64 timer_slack_ns;
+	u64 default_timer_slack_ns;
 
 #ifdef CONFIG_KASAN
 	unsigned int kasan_depth;

commit 96b9b1c95660d4bc5510c5d798d3817ae9f0b391
Merge: 8eee93e2576c a95fc9c8e576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 13:53:25 2016 -0700

    Merge tag 'tty-4.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty
    
    Pull tty/serial updates from Greg KH:
     "Here's the big tty/serial driver pull request for 4.6-rc1.
    
      Lots of changes in here, Peter has been on a tear again, with lots of
      refactoring and bugs fixes, many thanks to the great work he has been
      doing.  Lots of driver updates and fixes as well, full details in the
      shortlog.
    
      All have been in linux-next for a while with no reported issues"
    
    * tag 'tty-4.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty: (220 commits)
      serial: 8250: describe CONFIG_SERIAL_8250_RSA
      serial: samsung: optimize UART rx fifo access routine
      serial: pl011: add mark/space parity support
      serial: sa1100: make sa1100_register_uart_fns a function
      tty: serial: 8250: add MOXA Smartio MUE boards support
      serial: 8250: convert drivers to use up_to_u8250p()
      serial: 8250/mediatek: fix building with SERIAL_8250=m
      serial: 8250/ingenic: fix building with SERIAL_8250=m
      serial: 8250/uniphier: fix modular build
      Revert "drivers/tty/serial: make 8250/8250_ingenic.c explicitly non-modular"
      Revert "drivers/tty/serial: make 8250/8250_mtk.c explicitly non-modular"
      serial: mvebu-uart: initial support for Armada-3700 serial port
      serial: mctrl_gpio: Add missing module license
      serial: ifx6x60: avoid uninitialized variable use
      tty/serial: at91: fix bad offset for UART timeout register
      tty/serial: at91: restore dynamic driver binding
      serial: 8250: Add hardware dependency to RT288X option
      TTY, devpts: document pty count limiting
      tty: goldfish: support platform_device with id -1
      drivers: tty: goldfish: Add device tree bindings
      ...

commit 277edbabf6fece057b14fb6db5e3a34e00f42f42
Merge: 271ecc5253e2 0d571b62dd8e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 14:10:53 2016 -0700

    Merge tag 'pm+acpi-4.6-rc1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "This time the majority of changes go into cpufreq and they are
      significant.
    
      First off, the way CPU frequency updates are triggered is different
      now.  Instead of having to set up and manage a deferrable timer for
      each CPU in the system to evaluate and possibly change its frequency
      periodically, cpufreq governors set up callbacks to be invoked by the
      scheduler on a regular basis (basically on utilization updates).  The
      "old" governors, "ondemand" and "conservative", still do all of their
      work in process context (although that is triggered by the scheduler
      now), but intel_pstate does it all in the callback invoked by the
      scheduler with no need for any additional asynchronous processing.
    
      Of course, this eliminates the overhead related to the management of
      all those timers, but also it allows the cpufreq governor code to be
      simplified quite a bit.  On top of that, the common code and data
      structures used by the "ondemand" and "conservative" governors are
      cleaned up and made more straightforward and some long-standing and
      quite annoying problems are addressed.  In particular, the handling of
      governor sysfs attributes is modified and the related locking becomes
      more fine grained which allows some concurrency problems to be avoided
      (particularly deadlocks with the core cpufreq code).
    
      In principle, the new mechanism for triggering frequency updates
      allows utilization information to be passed from the scheduler to
      cpufreq.  Although the current code doesn't make use of it, in the
      works is a new cpufreq governor that will make decisions based on the
      scheduler's utilization data.  That should allow the scheduler and
      cpufreq to work more closely together in the long run.
    
      In addition to the core and governor changes, cpufreq drivers are
      updated too.  Fixes and optimizations go into intel_pstate, the
      cpufreq-dt driver is updated on top of some modification in the
      Operating Performance Points (OPP) framework and there are fixes and
      other updates in the powernv cpufreq driver.
    
      Apart from the cpufreq updates there is some new ACPICA material,
      including a fix for a problem introduced by previous ACPICA updates,
      and some less significant changes in the ACPI code, like CPPC code
      optimizations, ACPI processor driver cleanups and support for loading
      ACPI tables from initrd.
    
      Also updated are the generic power domains framework, the Intel RAPL
      power capping driver and the turbostat utility and we have a bunch of
      traditional assorted fixes and cleanups.
    
      Specifics:
    
       - Redesign of cpufreq governors and the intel_pstate driver to make
         them use callbacks invoked by the scheduler to trigger CPU
         frequency evaluation instead of using per-CPU deferrable timers for
         that purpose (Rafael Wysocki).
    
       - Reorganization and cleanup of cpufreq governor code to make it more
         straightforward and fix some concurrency problems in it (Rafael
         Wysocki, Viresh Kumar).
    
       - Cleanup and improvements of locking in the cpufreq core (Viresh
         Kumar).
    
       - Assorted cleanups in the cpufreq core (Rafael Wysocki, Viresh
         Kumar, Eric Biggers).
    
       - intel_pstate driver updates including fixes, optimizations and a
         modification to make it enable enable hardware-coordinated P-state
         selection (HWP) by default if supported by the processor (Philippe
         Longepe, Srinivas Pandruvada, Rafael Wysocki, Viresh Kumar, Felipe
         Franciosi).
    
       - Operating Performance Points (OPP) framework updates to improve its
         handling of voltage regulators and device clocks and updates of the
         cpufreq-dt driver on top of that (Viresh Kumar, Jon Hunter).
    
       - Updates of the powernv cpufreq driver to fix initialization and
         cleanup problems in it and correct its worker thread handling with
         respect to CPU offline, new powernv_throttle tracepoint (Shilpasri
         Bhat).
    
       - ACPI cpufreq driver optimization and cleanup (Rafael Wysocki).
    
       - ACPICA updates including one fix for a regression introduced by
         previos changes in the ACPICA code (Bob Moore, Lv Zheng, David Box,
         Colin Ian King).
    
       - Support for installing ACPI tables from initrd (Lv Zheng).
    
       - Optimizations of the ACPI CPPC code (Prashanth Prakash, Ashwin
         Chaugule).
    
       - Support for _HID(ACPI0010) devices (ACPI processor containers) and
         ACPI processor driver cleanups (Sudeep Holla).
    
       - Support for ACPI-based enumeration of the AMBA bus (Graeme Gregory,
         Aleksey Makarov).
    
       - Modification of the ACPI PCI IRQ management code to make it treat
         255 in the Interrupt Line register as "not connected" on x86 (as
         per the specification) and avoid attempts to use that value as a
         valid interrupt vector (Chen Fan).
    
       - ACPI APEI fixes related to resource leaks (Josh Hunt).
    
       - Removal of modularity from a few ACPI drivers (BGRT, GHES,
         intel_pmic_crc) that cannot be built as modules in practice (Paul
         Gortmaker).
    
       - PNP framework update to make it treat ACPI_RESOURCE_TYPE_SERIAL_BUS
         as a valid resource type (Harb Abdulhamid).
    
       - New device ID (future AMD I2C controller) in the ACPI driver for
         AMD SoCs (APD) and in the designware I2C driver (Xiangliang Yu).
    
       - Assorted ACPI cleanups (Colin Ian King, Kaiyen Chang, Oleg Drokin).
    
       - cpuidle menu governor optimization to avoid a square root
         computation in it (Rasmus Villemoes).
    
       - Fix for potential use-after-free in the generic device properties
         framework (Heikki Krogerus).
    
       - Updates of the generic power domains (genpd) framework including
         support for multiple power states of a domain, fixes and debugfs
         output improvements (Axel Haslam, Jon Hunter, Laurent Pinchart,
         Geert Uytterhoeven).
    
       - Intel RAPL power capping driver updates to reduce IPI overhead in
         it (Jacob Pan).
    
       - System suspend/hibernation code cleanups (Eric Biggers, Saurabh
         Sengar).
    
       - Year 2038 fix for the process freezer (Abhilash Jindal).
    
       - turbostat utility updates including new features (decoding of more
         registers and CPUID fields, sub-second intervals support, GFX MHz
         and RC6 printout, --out command line option), fixes (syscall jitter
         detection and workaround, reductioin of the number of syscalls
         made, fixes related to Xeon x200 processors, compiler warning
         fixes) and cleanups (Len Brown, Hubert Chrzaniuk, Chen Yu)"
    
    * tag 'pm+acpi-4.6-rc1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (182 commits)
      tools/power turbostat: bugfix: TDP MSRs print bits fixing
      tools/power turbostat: correct output for MSR_NHM_SNB_PKG_CST_CFG_CTL dump
      tools/power turbostat: call __cpuid() instead of __get_cpuid()
      tools/power turbostat: indicate SMX and SGX support
      tools/power turbostat: detect and work around syscall jitter
      tools/power turbostat: show GFX%rc6
      tools/power turbostat: show GFXMHz
      tools/power turbostat: show IRQs per CPU
      tools/power turbostat: make fewer systems calls
      tools/power turbostat: fix compiler warnings
      tools/power turbostat: add --out option for saving output in a file
      tools/power turbostat: re-name "%Busy" field to "Busy%"
      tools/power turbostat: Intel Xeon x200: fix turbo-ratio decoding
      tools/power turbostat: Intel Xeon x200: fix erroneous bclk value
      tools/power turbostat: allow sub-sec intervals
      ACPI / APEI: ERST: Fixed leaked resources in erst_init
      ACPI / APEI: Fix leaked resources
      intel_pstate: Do not skip samples partially
      intel_pstate: Remove freq calculation from intel_pstate_calc_busy()
      intel_pstate: Move intel_pstate_calc_busy() into get_target_pstate_use_performance()
      ...

commit e23604edac2a7be6a8808a5d13fac6b9df4eb9a8
Merge: d4e796152a04 1f25184656a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 19:44:38 2016 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ updates from Ingo Molnar:
     "NOHZ enhancements, by Frederic Weisbecker, which reorganizes/refactors
      the NOHZ 'can the tick be stopped?' infrastructure and related code to
      be data driven, and harmonizes the naming and handling of all the
      various properties"
    
    [ This makes the ugly "fetch_or()" macro that the scheduler used
      internally a new generic helper, and does a bad job at it.
    
      I'm pulling it, but I've asked Ingo and Frederic to get this
      fixed up ]
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched-clock: Migrate to use new tick dependency mask model
      posix-cpu-timers: Migrate to use new tick dependency mask model
      sched: Migrate sched to use new tick dependency mask model
      sched: Account rr tasks
      perf: Migrate perf to use new tick dependency mask model
      nohz: Use enum code for tick stop failure tracing message
      nohz: New tick dependency mask
      nohz: Implement wide kick on top of irq work
      atomic: Export fetch_or()

commit adaf9fcd136970e480d7ca834c0cf25ce922ea74
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 10 20:44:47 2016 +0100

    cpufreq: Move scheduler-related code to the sched directory
    
    Create cpufreq.c under kernel/sched/ and move the cpufreq code
    related to the scheduler to that file and to sched.h.
    
    Redefine cpufreq_update_util() as a static inline function to avoid
    function calls at its call sites in the scheduler code (as suggested
    by Peter Zijlstra).
    
    Also move the definition of struct update_util_data and declaration
    of cpufreq_set_update_util_data() from include/linux/cpufreq.h to
    include/linux/sched.h.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a10494a94cc3..913e755ef7b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3207,4 +3207,13 @@ static inline unsigned long rlimit_max(unsigned int limit)
 	return task_rlimit_max(current, limit);
 }
 
+#ifdef CONFIG_CPU_FREQ
+struct update_util_data {
+	void (*func)(struct update_util_data *data,
+		     u64 time, unsigned long util, unsigned long max);
+};
+
+void cpufreq_set_update_util_data(int cpu, struct update_util_data *data);
+#endif /* CONFIG_CPU_FREQ */
+
 #endif

commit 72f9f3fdc928dc3ecd223e801b32d930b662b6ed
Author: Luca Abeni <luca.abeni@unitn.it>
Date:   Mon Mar 7 12:27:04 2016 +0100

    sched/deadline: Remove dl_new from struct sched_dl_entity
    
    The dl_new field of struct sched_dl_entity is currently used to
    identify new deadline tasks, so that their deadline and runtime
    can be properly initialised.
    
    However, these tasks can be easily identified by checking if
    their deadline is smaller than the current time when they switch
    to SCHED_DEADLINE. So, dl_new can be removed by introducing this
    check in switched_to_dl(); this allows to simplify the
    SCHED_DEADLINE code.
    
    Signed-off-by: Luca Abeni <luca.abeni@unitn.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1457350024-7825-2-git-send-email-luca.abeni@unitn.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9519b66fc21f..838a89a78332 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1333,10 +1333,6 @@ struct sched_dl_entity {
 	 * task has to wait for a replenishment to be performed at the
 	 * next firing of dl_timer.
 	 *
-	 * @dl_new tells if a new instance arrived. If so we must
-	 * start executing it with full runtime and reset its absolute
-	 * deadline;
-	 *
 	 * @dl_boosted tells if we are boosted due to DI. If so we are
 	 * outside bandwidth enforcement mechanism (but only until we
 	 * exit the critical section);
@@ -1344,7 +1340,7 @@ struct sched_dl_entity {
 	 * @dl_yielded tells if task gave up the cpu before consuming
 	 * all its available runtime during the last job.
 	 */
-	int dl_throttled, dl_new, dl_boosted, dl_yielded;
+	int dl_throttled, dl_boosted, dl_yielded;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its

commit 76d92ac305f23cada3a9b3c48a7ccea5f71019cb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 17 22:25:49 2015 +0200

    sched: Migrate sched to use new tick dependency mask model
    
    Instead of providing asynchronous checks for the nohz subsystem to verify
    sched tick dependency, migrate sched to the new mask.
    
    Everytime a task is enqueued or dequeued, we evaluate the state of the
    tick dependency on top of the policy of the tasks in the runqueue, by
    order of priority:
    
    SCHED_DEADLINE: Need the tick in order to periodically check for runtime
    SCHED_FIFO    : Don't need the tick (no round-robin)
    SCHED_RR      : Need the tick if more than 1 task of the same priority
                    for round robin (simplified with checking if more than
                    one SCHED_RR task no matter what priority).
    SCHED_NORMAL  : Need the tick if more than 1 task for round-robin.
    
    We could optimize that further with one flag per sched policy on the tick
    dependency mask and perform only the checks relevant to the policy
    concerned by an enqueue/dequeue operation.
    
    Since the checks aren't based on the current task anymore, we could get
    rid of the task switch hook but it's still needed for posix cpu
    timers.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 307e48375f21..2b10348806d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2364,10 +2364,7 @@ static inline void wake_up_nohz_cpu(int cpu) { }
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL
-extern bool sched_can_stop_tick(void);
 extern u64 scheduler_tick_max_deferment(void);
-#else
-static inline bool sched_can_stop_tick(void) { return false; }
 #endif
 
 #ifdef CONFIG_SCHED_AUTOGROUP

commit d027d45d8a17a4145eab2d841140e9acbb7feb59
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jun 7 15:54:30 2015 +0200

    nohz: New tick dependency mask
    
    The tick dependency is evaluated on every IRQ and context switch. This
    consists is a batch of checks which determine whether it is safe to
    stop the tick or not. These checks are often split in many details:
    posix cpu timers, scheduler, sched clock, perf events.... each of which
    are made of smaller details: posix cpu timer involves checking process
    wide timers then thread wide timers. Perf involves checking freq events
    then more per cpu details.
    
    Checking these informations asynchronously every time we update the full
    dynticks state bring avoidable overhead and a messy layout.
    
    Let's introduce instead tick dependency masks: one for system wide
    dependency (unstable sched clock, freq based perf events), one for CPU
    wide dependency (sched, throttling perf events), and task/signal level
    dependencies (posix cpu timers). The subsystems are responsible
    for setting and clearing their dependency through a set of APIs that will
    take care of concurrent dependency mask modifications and kick targets
    to restart the relevant CPU tick whenever needed.
    
    This new dependency engine stays beside the old one until all subsystems
    having a tick dependency are converted to it.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a10494a94cc3..307e48375f21 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -719,6 +719,10 @@ struct signal_struct {
 	/* Earliest-expiration cache. */
 	struct task_cputime cputime_expires;
 
+#ifdef CONFIG_NO_HZ_FULL
+	unsigned long tick_dep_mask;
+#endif
+
 	struct list_head cpu_timers[3];
 
 	struct pid *tty_old_pgrp;
@@ -1542,6 +1546,10 @@ struct task_struct {
 		VTIME_SYS,
 	} vtime_snap_whence;
 #endif
+
+#ifdef CONFIG_NO_HZ_FULL
+	unsigned long tick_dep_mask;
+#endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	u64 start_time;		/* monotonic time in nsec */
 	u64 real_start_time;	/* boot based time in nsec */

commit f904f58263e1df5f70feb8b283f4bbe662847334
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 26 14:54:56 2016 +0100

    sched/debug: Fix preempt_disable_ip recording for preempt_disable()
    
    The preempt_disable() invokes preempt_count_add() which saves the caller
    in ->preempt_disable_ip. It uses CALLER_ADDR1 which does not look for
    its caller but for the parent of the caller. Which means we get the correct
    caller for something like spin_lock() unless the architectures inlines
    those invocations. It is always wrong for preempt_disable() or
    local_bh_disable().
    
    This patch makes the function get_lock_parent_ip() which tries
    CALLER_ADDR0,1,2 if the former is a locking function.
    This seems to record the preempt_disable() caller properly for
    preempt_disable() itself as well as for get_cpu_var() or
    local_bh_disable().
    
    Steven asked for the get_parent_ip() -> get_lock_parent_ip() rename.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160226135456.GB18244@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 87e5f9886ac8..9519b66fc21f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -182,8 +182,6 @@ extern void update_cpu_load_nohz(int active);
 static inline void update_cpu_load_nohz(int active) { }
 #endif
 
-extern unsigned long get_parent_ip(unsigned long addr);
-
 extern void dump_cpu_task(int cpu);
 
 struct seq_file;

commit ff77e468535987b3d21b7bd4da15608ea3ce7d0b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 18 15:27:07 2016 +0100

    sched/rt: Fix PI handling vs. sched_setscheduler()
    
    Andrea Parri reported:
    
    > I found that the following scenario (with CONFIG_RT_GROUP_SCHED=y) is not
    > handled correctly:
    >
    >     T1 (prio = 20)
    >        lock(rtmutex);
    >
    >     T2 (prio = 20)
    >        blocks on rtmutex  (rt_nr_boosted = 0 on T1's rq)
    >
    >     T1 (prio = 20)
    >        sys_set_scheduler(prio = 0)
    >           [new_effective_prio == oldprio]
    >           T1 prio = 20    (rt_nr_boosted = 0 on T1's rq)
    >
    > The last step is incorrect as T1 is now boosted (c.f., rt_se_boosted());
    > in particular, if we continue with
    >
    >    T1 (prio = 20)
    >       unlock(rtmutex)
    >          wakeup(T2)
    >          adjust_prio(T1)
    >             [prio != rt_mutex_getprio(T1)]
    >           dequeue(T1)
    >              rt_nr_boosted = (unsigned long)(-1)
    >              ...
    >             T1 prio = 0
    >
    > then we end up leaving rt_nr_boosted in an "inconsistent" state.
    >
    > The simple program attached could reproduce the previous scenario; note
    > that, as a consequence of the presence of this state, the "assertion"
    >
    >     WARN_ON(!rt_nr_running && rt_nr_boosted)
    >
    > from dec_rt_group() may trigger.
    
    So normally we dequeue/enqueue tasks in sched_setscheduler(), which
    would ensure the accounting stays correct. However in the early PI path
    we fail to do so.
    
    So this was introduced at around v3.14, by:
    
      c365c292d059 ("sched: Consider pi boosting in setscheduler()")
    
    which fixed another problem exactly because that dequeue/enqueue, joy.
    
    Fix this by teaching rt about DEQUEUE_SAVE/ENQUEUE_RESTORE and have it
    preserve runqueue location with that option. This requires decoupling
    the on_rt_rq() state from being on the list.
    
    In order to allow for explicit movement during the SAVE/RESTORE,
    introduce {DE,EN}QUEUE_MOVE. We still must use SAVE/RESTORE in these
    cases to preserve other invariants.
    
    Respecting the SAVE/RESTORE flags also has the (nice) side-effect that
    things like sys_nice()/sys_sched_setaffinity() also do not reorder
    FIFO tasks (whereas they used to before this patch).
    
    Reported-by: Andrea Parri <parri.andrea@gmail.com>
    Tested-by: Andrea Parri <parri.andrea@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a292c4b7e94c..87e5f9886ac8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1293,6 +1293,8 @@ struct sched_rt_entity {
 	unsigned long timeout;
 	unsigned long watchdog_stamp;
 	unsigned int time_slice;
+	unsigned short on_rq;
+	unsigned short on_list;
 
 	struct sched_rt_entity *back;
 #ifdef CONFIG_RT_GROUP_SCHED

commit cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Feb 5 09:08:36 2016 +0000

    sched/debug: Make schedstats a runtime tunable that is disabled by default
    
    schedstats is very useful during debugging and performance tuning but it
    incurs overhead to calculate the stats. As such, even though it can be
    disabled at build time, it is often enabled as the information is useful.
    
    This patch adds a kernel command-line and sysctl tunable to enable or
    disable schedstats on demand (when it's built in). It is disabled
    by default as someone who knows they need it can also learn to enable
    it when necessary.
    
    The benefits are dependent on how scheduler-intensive the workload is.
    If it is then the patch reduces the number of cycles spent calculating
    the stats with a small benefit from reducing the cache footprint of the
    scheduler.
    
    These measurements were taken from a 48-core 2-socket
    machine with Xeon(R) E5-2670 v3 cpus although they were also tested on a
    single socket machine 8-core machine with Intel i7-3770 processors.
    
    netperf-tcp
                               4.5.0-rc1             4.5.0-rc1
                                 vanilla          nostats-v3r1
    Hmean    64         560.45 (  0.00%)      575.98 (  2.77%)
    Hmean    128        766.66 (  0.00%)      795.79 (  3.80%)
    Hmean    256        950.51 (  0.00%)      981.50 (  3.26%)
    Hmean    1024      1433.25 (  0.00%)     1466.51 (  2.32%)
    Hmean    2048      2810.54 (  0.00%)     2879.75 (  2.46%)
    Hmean    3312      4618.18 (  0.00%)     4682.09 (  1.38%)
    Hmean    4096      5306.42 (  0.00%)     5346.39 (  0.75%)
    Hmean    8192     10581.44 (  0.00%)    10698.15 (  1.10%)
    Hmean    16384    18857.70 (  0.00%)    18937.61 (  0.42%)
    
    Small gains here, UDP_STREAM showed nothing intresting and neither did
    the TCP_RR tests. The gains on the 8-core machine were very similar.
    
    tbench4
                                     4.5.0-rc1             4.5.0-rc1
                                       vanilla          nostats-v3r1
    Hmean    mb/sec-1         500.85 (  0.00%)      522.43 (  4.31%)
    Hmean    mb/sec-2         984.66 (  0.00%)     1018.19 (  3.41%)
    Hmean    mb/sec-4        1827.91 (  0.00%)     1847.78 (  1.09%)
    Hmean    mb/sec-8        3561.36 (  0.00%)     3611.28 (  1.40%)
    Hmean    mb/sec-16       5824.52 (  0.00%)     5929.03 (  1.79%)
    Hmean    mb/sec-32      10943.10 (  0.00%)    10802.83 ( -1.28%)
    Hmean    mb/sec-64      15950.81 (  0.00%)    16211.31 (  1.63%)
    Hmean    mb/sec-128     15302.17 (  0.00%)    15445.11 (  0.93%)
    Hmean    mb/sec-256     14866.18 (  0.00%)    15088.73 (  1.50%)
    Hmean    mb/sec-512     15223.31 (  0.00%)    15373.69 (  0.99%)
    Hmean    mb/sec-1024    14574.25 (  0.00%)    14598.02 (  0.16%)
    Hmean    mb/sec-2048    13569.02 (  0.00%)    13733.86 (  1.21%)
    Hmean    mb/sec-3072    12865.98 (  0.00%)    13209.23 (  2.67%)
    
    Small gains of 2-4% at low thread counts and otherwise flat.  The
    gains on the 8-core machine were slightly different
    
    tbench4 on 8-core i7-3770 single socket machine
    Hmean    mb/sec-1        442.59 (  0.00%)      448.73 (  1.39%)
    Hmean    mb/sec-2        796.68 (  0.00%)      794.39 ( -0.29%)
    Hmean    mb/sec-4       1322.52 (  0.00%)     1343.66 (  1.60%)
    Hmean    mb/sec-8       2611.65 (  0.00%)     2694.86 (  3.19%)
    Hmean    mb/sec-16      2537.07 (  0.00%)     2609.34 (  2.85%)
    Hmean    mb/sec-32      2506.02 (  0.00%)     2578.18 (  2.88%)
    Hmean    mb/sec-64      2511.06 (  0.00%)     2569.16 (  2.31%)
    Hmean    mb/sec-128     2313.38 (  0.00%)     2395.50 (  3.55%)
    Hmean    mb/sec-256     2110.04 (  0.00%)     2177.45 (  3.19%)
    Hmean    mb/sec-512     2072.51 (  0.00%)     2053.97 ( -0.89%)
    
    In constract, this shows a relatively steady 2-3% gain at higher thread
    counts. Due to the nature of the patch and the type of workload, it's
    not a surprise that the result will depend on the CPU used.
    
    hackbench-pipes
                             4.5.0-rc1             4.5.0-rc1
                               vanilla          nostats-v3r1
    Amean    1        0.0637 (  0.00%)      0.0660 ( -3.59%)
    Amean    4        0.1229 (  0.00%)      0.1181 (  3.84%)
    Amean    7        0.1921 (  0.00%)      0.1911 (  0.52%)
    Amean    12       0.3117 (  0.00%)      0.2923 (  6.23%)
    Amean    21       0.4050 (  0.00%)      0.3899 (  3.74%)
    Amean    30       0.4586 (  0.00%)      0.4433 (  3.33%)
    Amean    48       0.5910 (  0.00%)      0.5694 (  3.65%)
    Amean    79       0.8663 (  0.00%)      0.8626 (  0.43%)
    Amean    110      1.1543 (  0.00%)      1.1517 (  0.22%)
    Amean    141      1.4457 (  0.00%)      1.4290 (  1.16%)
    Amean    172      1.7090 (  0.00%)      1.6924 (  0.97%)
    Amean    192      1.9126 (  0.00%)      1.9089 (  0.19%)
    
    Some small gains and losses and while the variance data is not included,
    it's close to the noise. The UMA machine did not show anything particularly
    different
    
    pipetest
                                 4.5.0-rc1             4.5.0-rc1
                                   vanilla          nostats-v2r2
    Min         Time        4.13 (  0.00%)        3.99 (  3.39%)
    1st-qrtle   Time        4.38 (  0.00%)        4.27 (  2.51%)
    2nd-qrtle   Time        4.46 (  0.00%)        4.39 (  1.57%)
    3rd-qrtle   Time        4.56 (  0.00%)        4.51 (  1.10%)
    Max-90%     Time        4.67 (  0.00%)        4.60 (  1.50%)
    Max-93%     Time        4.71 (  0.00%)        4.65 (  1.27%)
    Max-95%     Time        4.74 (  0.00%)        4.71 (  0.63%)
    Max-99%     Time        4.88 (  0.00%)        4.79 (  1.84%)
    Max         Time        4.93 (  0.00%)        4.83 (  2.03%)
    Mean        Time        4.48 (  0.00%)        4.39 (  1.91%)
    Best99%Mean Time        4.47 (  0.00%)        4.39 (  1.91%)
    Best95%Mean Time        4.46 (  0.00%)        4.38 (  1.93%)
    Best90%Mean Time        4.45 (  0.00%)        4.36 (  1.98%)
    Best50%Mean Time        4.36 (  0.00%)        4.25 (  2.49%)
    Best10%Mean Time        4.23 (  0.00%)        4.10 (  3.13%)
    Best5%Mean  Time        4.19 (  0.00%)        4.06 (  3.20%)
    Best1%Mean  Time        4.13 (  0.00%)        4.00 (  3.39%)
    
    Small improvement and similar gains were seen on the UMA machine.
    
    The gain is small but it stands to reason that doing less work in the
    scheduler is a good thing. The downside is that the lack of schedstats and
    tracepoints may be surprising to experts doing performance analysis until
    they find the existence of the schedstats= parameter or schedstats sysctl.
    It will be automatically activated for latencytop and sleep profiling to
    alleviate the problem. For tracepoints, there is a simple warning as it's
    not safe to activate schedstats in the context when it's known the tracepoint
    may be wanted but is unavailable.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454663316-22048-1-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a10494a94cc3..a292c4b7e94c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -920,6 +920,10 @@ static inline int sched_info_on(void)
 #endif
 }
 
+#ifdef CONFIG_SCHEDSTATS
+void force_schedstat_enabled(void);
+#endif
+
 enum cpu_idle_type {
 	CPU_IDLE,
 	CPU_NOT_IDLE,

commit 2e28d38ae1d9ced6ac2deb4001aca3f267304cdb
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat Jan 9 22:55:33 2016 -0800

    tty: audit: Handle tty audit enable atomically
    
    The audit_tty and audit_tty_log_passwd fields are actually bool
    values, so merge into single memory location to access atomically.
    
    NB: audit log operations may still occur after tty audit is disabled
    which is consistent with the existing functionality
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a10494a94cc3..a389222e9703 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -775,7 +775,6 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_AUDIT
 	unsigned audit_tty;
-	unsigned audit_tty_log_passwd;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
 

commit eadee0ce6fd33defe449c97e671bf83fa230b5de
Merge: 6fb11e6508ea 117aa41e8020
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 22 10:24:03 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull more vfs updates from Al Viro:
     "Embarrassing braino fix + pipe page accounting + fixing an eyesore in
      find_filesystem() (checking that s1 is equal to prefix of s2 of given
      length can be done in many ways, but "compare strlen(s1) with length
      and then do strncmp()" is not a good one...)"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      [regression] fix braino in fs/dlm/user.c
      pipe: limit the per-user amount of pages allocated in pipes
      find_filesystem(): simplify comparison

commit 127424c86bb6cb87f0b563d9fdcfbbaf3c86ecec
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 20 15:02:32 2016 -0800

    mm: memcontrol: move kmem accounting code to CONFIG_MEMCG
    
    The cgroup2 memory controller will account important in-kernel memory
    consumers per default.  Move all necessary components to CONFIG_MEMCG.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 02dabf281b2f..f1e81e128592 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1476,10 +1476,10 @@ struct task_struct {
 	unsigned in_iowait:1;
 #ifdef CONFIG_MEMCG
 	unsigned memcg_may_oom:1;
-#endif
-#ifdef CONFIG_MEMCG_KMEM
+#ifndef CONFIG_SLOB
 	unsigned memcg_kmem_skip_account:1;
 #endif
+#endif
 #ifdef CONFIG_COMPAT_BRK
 	unsigned brk_randomized:1;
 #endif

commit c6d308534aef6c99904bf5862066360ae067abc4
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Wed Jan 20 15:00:55 2016 -0800

    UBSAN: run-time undefined behavior sanity checker
    
    UBSAN uses compile-time instrumentation to catch undefined behavior
    (UB).  Compiler inserts code that perform certain kinds of checks before
    operations that could cause UB.  If check fails (i.e.  UB detected)
    __ubsan_handle_* function called to print error message.
    
    So the most of the work is done by compiler.  This patch just implements
    ubsan handlers printing errors.
    
    GCC has this capability since 4.9.x [1] (see -fsanitize=undefined
    option and its suboptions).
    However GCC 5.x has more checkers implemented [2].
    Article [3] has a bit more details about UBSAN in the GCC.
    
    [1] - https://gcc.gnu.org/onlinedocs/gcc-4.9.0/gcc/Debugging-Options.html
    [2] - https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html
    [3] - http://developerblog.redhat.com/2014/10/16/gcc-undefined-behavior-sanitizer-ubsan/
    
    Issues which UBSAN has found thus far are:
    
    Found bugs:
    
     * out-of-bounds access - 97840cb67ff5 ("netfilter: nfnetlink: fix
       insufficient validation in nfnetlink_bind")
    
    undefined shifts:
    
     * d48458d4a768 ("jbd2: use a better hash function for the revoke
       table")
    
     * 10632008b9e1 ("clockevents: Prevent shift out of bounds")
    
     * 'x << -1' shift in ext4 -
       http://lkml.kernel.org/r/<5444EF21.8020501@samsung.com>
    
     * undefined rol32(0) -
       http://lkml.kernel.org/r/<1449198241-20654-1-git-send-email-sasha.levin@oracle.com>
    
     * undefined dirty_ratelimit calculation -
       http://lkml.kernel.org/r/<566594E2.3050306@odin.com>
    
     * undefined roundown_pow_of_two(0) -
       http://lkml.kernel.org/r/<1449156616-11474-1-git-send-email-sasha.levin@oracle.com>
    
     * [WONTFIX] undefined shift in __bpf_prog_run -
       http://lkml.kernel.org/r/<CACT4Y+ZxoR3UjLgcNdUm4fECLMx2VdtfrENMtRRCdgHB2n0bJA@mail.gmail.com>
    
       WONTFIX here because it should be fixed in bpf program, not in kernel.
    
    signed overflows:
    
     * 32a8df4e0b33f ("sched: Fix odd values in effective_load()
       calculations")
    
     * mul overflow in ntp -
       http://lkml.kernel.org/r/<1449175608-1146-1-git-send-email-sasha.levin@oracle.com>
    
     * incorrect conversion into rtc_time in rtc_time64_to_tm() -
       http://lkml.kernel.org/r/<1449187944-11730-1-git-send-email-sasha.levin@oracle.com>
    
     * unvalidated timespec in io_getevents() -
       http://lkml.kernel.org/r/<CACT4Y+bBxVYLQ6LtOKrKtnLthqLHcw-BMp3aqP3mjdAvr9FULQ@mail.gmail.com>
    
     * [NOTABUG] signed overflow in ktime_add_safe() -
       http://lkml.kernel.org/r/<CACT4Y+aJ4muRnWxsUe1CMnA6P8nooO33kwG-c8YZg=0Xc8rJqw@mail.gmail.com>
    
    [akpm@linux-foundation.org: fix unused local warning]
    [akpm@linux-foundation.org: fix __int128 build woes]
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Michal Marek <mmarek@suse.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Yury Gribov <y.gribov@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 61aa9bbea871..02dabf281b2f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1643,6 +1643,9 @@ struct task_struct {
 	struct held_lock held_locks[MAX_LOCK_DEPTH];
 	gfp_t lockdep_reclaim_gfp;
 #endif
+#ifdef CONFIG_UBSAN
+	unsigned int in_ubsan;
+#endif
 
 /* journalling filesystem info */
 	void *journal_info;

commit 759c01142a5d0f364a462346168a56de28a80f52
Author: Willy Tarreau <w@1wt.eu>
Date:   Mon Jan 18 16:36:09 2016 +0100

    pipe: limit the per-user amount of pages allocated in pipes
    
    On no-so-small systems, it is possible for a single process to cause an
    OOM condition by filling large pipes with data that are never read. A
    typical process filling 4000 pipes with 1 MB of data will use 4 GB of
    memory. On small systems it may be tricky to set the pipe max size to
    prevent this from happening.
    
    This patch makes it possible to enforce a per-user soft limit above
    which new pipes will be limited to a single page, effectively limiting
    them to 4 kB each, as well as a hard limit above which no new pipes may
    be created for this user. This has the effect of protecting the system
    against memory abuse without hurting other users, and still allowing
    pipes to work correctly though with less data at once.
    
    The limit are controlled by two new sysctls : pipe-user-pages-soft, and
    pipe-user-pages-hard. Both may be disabled by setting them to zero. The
    default soft limit allows the default number of FDs per process (1024)
    to create pipes of the default size (64kB), thus reaching a limit of 64MB
    before starting to create only smaller pipes. With 256 processes limited
    to 1024 FDs each, this results in 1024*64kB + (256*1024 - 1024) * 4kB =
    1084 MB of memory allocated for a user. The hard limit is disabled by
    default to avoid breaking existing applications that make intensive use
    of pipes (eg: for splicing).
    
    Reported-by: socketpair@gmail.com
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Mitigates: CVE-2013-4312 (Linux 2.0+)
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Willy Tarreau <w@1wt.eu>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 61aa9bbea871..1589ddc88e38 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -835,6 +835,7 @@ struct user_struct {
 #endif
 	unsigned long locked_shm; /* How many pages of mlocked shm ? */
 	unsigned long unix_inflight;	/* How many files in flight in unix sockets */
+	atomic_long_t pipe_bufs;  /* how many pages are allocated in pipe buffers */
 
 #ifdef CONFIG_KEYS
 	struct key *uid_keyring;	/* UID specific keyring */

commit aee3bfa3307cd0da2126bdc0ea359dabea5ee8f7
Merge: c597b6bcd5c6 415b6f19e87e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 18:57:02 2016 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from Davic Miller:
    
     1) Support busy polling generically, for all NAPI drivers.  From Eric
        Dumazet.
    
     2) Add byte/packet counter support to nft_ct, from Floriani Westphal.
    
     3) Add RSS/XPS support to mvneta driver, from Gregory Clement.
    
     4) Implement IPV6_HDRINCL socket option for raw sockets, from Hannes
        Frederic Sowa.
    
     5) Add support for T6 adapter to cxgb4 driver, from Hariprasad Shenai.
    
     6) Add support for VLAN device bridging to mlxsw switch driver, from
        Ido Schimmel.
    
     7) Add driver for Netronome NFP4000/NFP6000, from Jakub Kicinski.
    
     8) Provide hwmon interface to mlxsw switch driver, from Jiri Pirko.
    
     9) Reorganize wireless drivers into per-vendor directories just like we
        do for ethernet drivers.  From Kalle Valo.
    
    10) Provide a way for administrators "destroy" connected sockets via the
        SOCK_DESTROY socket netlink diag operation.  From Lorenzo Colitti.
    
    11) Add support to add/remove multicast routes via netlink, from Nikolay
        Aleksandrov.
    
    12) Make TCP keepalive settings per-namespace, from Nikolay Borisov.
    
    13) Add forwarding and packet duplication facilities to nf_tables, from
        Pablo Neira Ayuso.
    
    14) Dead route support in MPLS, from Roopa Prabhu.
    
    15) TSO support for thunderx chips, from Sunil Goutham.
    
    16) Add driver for IBM's System i/p VNIC protocol, from Thomas Falcon.
    
    17) Rationalize, consolidate, and more completely document the checksum
        offloading facilities in the networking stack.  From Tom Herbert.
    
    18) Support aborting an ongoing scan in mac80211/cfg80211, from
        Vidyullatha Kanchanapally.
    
    19) Use per-bucket spinlock for bpf hash facility, from Tom Leiming.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1375 commits)
      net: bnxt: always return values from _bnxt_get_max_rings
      net: bpf: reject invalid shifts
      phonet: properly unshare skbs in phonet_rcv()
      dwc_eth_qos: Fix dma address for multi-fragment skbs
      phy: remove an unneeded condition
      mdio: remove an unneed condition
      mdio_bus: NULL dereference on allocation error
      net: Fix typo in netdev_intersect_features
      net: freescale: mac-fec: Fix build error from phy_device API change
      net: freescale: ucc_geth: Fix build error from phy_device API change
      bonding: Prevent IPv6 link local address on enslaved devices
      IB/mlx5: Add flow steering support
      net/mlx5_core: Export flow steering API
      net/mlx5_core: Make ipv4/ipv6 location more clear
      net/mlx5_core: Enable flow steering support for the IB driver
      net/mlx5_core: Initialize namespaces only when supported by device
      net/mlx5_core: Set priority attributes
      net/mlx5_core: Connect flow tables
      net/mlx5_core: Introduce modify flow table command
      net/mlx5_core: Managing root flow table
      ...

commit 0f8c7901039f8b1366ae364462743c8f4125822e
Merge: 3d116a66ed9d 6201171e3b2c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 18:53:13 2016 -0800

    Merge branch 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue update from Tejun Heo:
     "Workqueue changes for v4.5.  One cleanup patch and three to improve
      the debuggability.
    
      Workqueue now has a stall detector which dumps workqueue state if any
      worker pool hasn't made forward progress over a certain amount of time
      (30s by default) and also triggers a warning if a workqueue which can
      be used in memory reclaim path tries to wait on something which can't
      be.
    
      These should make workqueue hangs a lot easier to debug."
    
    * 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: simplify the apply_workqueue_attrs_locked()
      workqueue: implement lockup detector
      watchdog: introduce touch_softlockup_watchdog_sched()
      workqueue: warn if memory reclaim tries to flush !WQ_MEM_RECLAIM workqueue

commit 712f4aad406bb1ed67f3f98d04c044191f0ff593
Author: willy tarreau <w@1wt.eu>
Date:   Sun Jan 10 07:54:56 2016 +0100

    unix: properly account for FDs passed over unix sockets
    
    It is possible for a process to allocate and accumulate far more FDs than
    the process' limit by sending them over a unix socket then closing them
    to keep the process' fd count low.
    
    This change addresses this problem by keeping track of the number of FDs
    in flight per user and preventing non-privileged processes from having
    more FDs in flight than their configured FD limit.
    
    Reported-by: socketpair@gmail.com
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Mitigates: CVE-2013-4312 (Linux 2.0+)
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Willy Tarreau <w@1wt.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edad7a43edea..fbf25f19b3b5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -830,6 +830,7 @@ struct user_struct {
 	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
 #endif
 	unsigned long locked_shm; /* How many pages of mlocked shm ? */
+	unsigned long unix_inflight;	/* How many files in flight in unix sockets */
 
 #ifdef CONFIG_KEYS
 	struct key *uid_keyring;	/* UID specific keyring */

commit 5a1078043f844074cbd53981432778a8d5dd56e9
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Tue Dec 8 21:23:59 2015 +0100

    sched/core: Move sched_entity::avg into separate cache line
    
    The sched_entity::avg collides with read-mostly sched_entity data.
    
    The perf c2c tool showed many read HITM accesses across
    many CPUs for sched_entity's cfs_rq and my_q, while having
    at the same time tons of stores for avg.
    
    After placing sched_entity::avg into separate cache line,
    the perf bench sched pipe showed around 20 seconds speedup.
    
    NOTE I cut out all perf events except for cycles and
    instructions from following output.
    
    Before:
      $ perf stat -r 5 perf bench sched pipe -l 10000000
      # Running 'sched/pipe' benchmark:
      # Executed 10000000 pipe operations between two processes
    
           Total time: 270.348 [sec]
    
            27.034805 usecs/op
                36989 ops/sec
       ...
    
         245,537,074,035      cycles                    #    1.433 GHz
         187,264,548,519      instructions              #    0.77  insns per cycle
    
           272.653840535 seconds time elapsed           ( +-  1.31% )
    
    After:
      $ perf stat -r 5 perf bench sched pipe -l 10000000
      # Running 'sched/pipe' benchmark:
      # Executed 10000000 pipe operations between two processes
    
           Total time: 251.076 [sec]
    
            25.107678 usecs/op
                39828 ops/sec
      ...
    
         244,573,513,928      cycles                    #    1.572 GHz
         187,409,641,157      instructions              #    0.76  insns per cycle
    
           251.679315188 seconds time elapsed           ( +-  0.31% )
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Joe Mario <jmario@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1449606239-28602-1-git-send-email-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 791b47e40317..0c0e78102850 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1268,8 +1268,13 @@ struct sched_entity {
 #endif
 
 #ifdef CONFIG_SMP
-	/* Per entity load average tracking */
-	struct sched_avg	avg;
+	/*
+	 * Per entity load average tracking.
+	 *
+	 * Put into separate cache line so it does not
+	 * collide with read-mostly values above.
+	 */
+	struct sched_avg	avg ____cacheline_aligned_in_smp;
 #endif
 };
 

commit 567bee2803cb46caeb6011de5b738fde33dc3896
Merge: aa0b7ae06387 093e5840ae76
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 6 11:02:29 2016 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before merging new patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit be958bdc96f18bc1356177bbb79d46ea0c037b96
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 25 16:02:07 2015 +0100

    sched/core: Fix unserialized r-m-w scribbling stuff
    
    Some of the sched bitfieds (notably sched_reset_on_fork) can be set
    on other than current, this can cause the r-m-w to race with other
    updates.
    
    Since all the sched bits are serialized by scheduler locks, pull them
    in a separate word.
    
    Reported-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: hannes@cmpxchg.org
    Cc: mhocko@kernel.org
    Cc: vdavydov@parallels.com
    Link: http://lkml.kernel.org/r/20151125150207.GM11639@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9cf9dd1c4cbe..fa39434e3fdd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1455,14 +1455,15 @@ struct task_struct {
 	/* Used for emulating ABI behavior of previous Linux versions */
 	unsigned int personality;
 
-	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
-				 * execve */
-	unsigned in_iowait:1;
-
-	/* Revert to default priority/policy when forking */
+	/* scheduler bits, serialized by scheduler locks */
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 	unsigned sched_migrated:1;
+	unsigned :0; /* force alignment to the next boundary */
+
+	/* unserialized, strictly 'current' */
+	unsigned in_execve:1; /* bit to tell LSMs we're in execve */
+	unsigned in_iowait:1;
 #ifdef CONFIG_MEMCG
 	unsigned memcg_may_oom:1;
 #endif

commit 570f52412ae9432c56897472791ea8db420cbaf1
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Fri Jan 1 23:03:01 2016 +0900

    sched/core: Check tgid in is_global_init()
    
    Our global init task can have sub-threads, so ->pid check is not reliable
    enough for is_global_init(), we need to check tgid instead. This has been
    spotted by Oleg and a fix was proposed by Richard a long time ago (see the
    link below).
    
    Oleg wrote:
    
      : Because is_global_init() is only true for the main thread of /sbin/init.
      :
      : Just look at oom_unkillable_task(). It tries to not kill init. But, say,
      : select_bad_process() can happily find a sub-thread of is_global_init()
      : and still kill it.
    
    I recently hit the problem in question; re-sending the patch (to the
    best of my knowledge it has never been submitted) with updated function
    comment. Credit goes to Oleg and Richard.
    
    Suggested-by: Richard Guy Briggs <rgb@redhat.com>
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Eric W . Biederman <ebiederm@xmission.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Serge E . Hallyn <serge.hallyn@ubuntu.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://www.redhat.com/archives/linux-audit/2013-December/msg00086.html
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edad7a43edea..9cf9dd1c4cbe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2002,7 +2002,8 @@ static inline int pid_alive(const struct task_struct *p)
 }
 
 /**
- * is_global_init - check if a task structure is init
+ * is_global_init - check if a task structure is init. Since init
+ * is free to have sub-threads we need to check tgid.
  * @tsk: Task structure to be checked.
  *
  * Check if a task structure is the first user space task the kernel created.
@@ -2011,7 +2012,7 @@ static inline int pid_alive(const struct task_struct *p)
  */
 static inline int is_global_init(struct task_struct *tsk)
 {
-	return tsk->pid == 1;
+	return task_tgid_nr(tsk) == 1;
 }
 
 extern struct pid *cad_pid;

commit 03e0d4610bf4d4a93bfa16b2474ed4fd5243aa71
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Dec 8 11:28:04 2015 -0500

    watchdog: introduce touch_softlockup_watchdog_sched()
    
    touch_softlockup_watchdog() is used to tell watchdog that scheduler
    stall is expected.  One group of usage is from paths where the task
    may not be able to yield for a long time such as performing slow PIO
    to finicky device and coming out of suspend.  The other is to account
    for scheduler and timer going idle.
    
    For scheduler softlockup detection, there's no reason to distinguish
    the two cases; however, workqueue lockup detector is planned and it
    can use the same signals from the former group while the latter would
    spuriously prevent detection.  This patch introduces a new function
    touch_softlockup_watchdog_sched() and convert the latter group to call
    it instead.  For now, it just calls touch_softlockup_watchdog() and
    there's no functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edad7a43edea..d56cdde2f12c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -377,6 +377,7 @@ extern void scheduler_tick(void);
 extern void sched_show_task(struct task_struct *p);
 
 #ifdef CONFIG_LOCKUP_DETECTOR
+extern void touch_softlockup_watchdog_sched(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_softlockup_watchdog_sync(void);
 extern void touch_all_softlockup_watchdogs(void);
@@ -387,6 +388,9 @@ extern unsigned int  softlockup_panic;
 extern unsigned int  hardlockup_panic;
 void lockup_detector_init(void);
 #else
+static inline void touch_softlockup_watchdog_sched(void)
+{
+}
 static inline void touch_softlockup_watchdog(void)
 {
 }

commit b7ce2277f087fd052e7e1bbf432f7fecbee82bb6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 19 16:47:34 2015 +0100

    sched/cputime: Convert vtime_seqlock to seqcount
    
    The cputime can only be updated by the current task itself, even in
    vtime case. So we can safely use seqcount instead of seqlock as there
    is no writer concurrency involved.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447948054-28668-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3533168fe7d1..3b0de68bce41 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1519,7 +1519,7 @@ struct task_struct {
 	cputime_t gtime;
 	struct prev_cputime prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_t vtime_seqlock;
+	seqcount_t vtime_seqcount;
 	unsigned long long vtime_snap;
 	enum {
 		/* Task is sleeping or running in a CPU with VTIME inactive */

commit 7098c1eac75dc03fdbb7249171a6e68ce6044a5a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 19 16:47:30 2015 +0100

    sched/cputime: Clarify vtime symbols and document them
    
    VTIME_SLEEPING state happens either when:
    
    1) The task is sleeping and no tickless delta is to be added on the task
       cputime stats.
    2) The CPU isn't running vtime at all, so the same properties of 1) applies.
    
    Lets rename the vtime symbol to reflect both states.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447948054-28668-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f425aac63317..3533168fe7d1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1522,8 +1522,11 @@ struct task_struct {
 	seqlock_t vtime_seqlock;
 	unsigned long long vtime_snap;
 	enum {
-		VTIME_SLEEPING = 0,
+		/* Task is sleeping or running in a CPU with VTIME inactive */
+		VTIME_INACTIVE = 0,
+		/* Task runs in userspace in a CPU with VTIME active */
 		VTIME_USER,
+		/* Task runs in kernelspace in a CPU with VTIME active */
 		VTIME_SYS,
 	} vtime_snap_whence;
 #endif

commit 525705d15e63b7455977408e4601e76e6bc41524
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Tue Nov 10 09:36:02 2015 +0900

    sched/fair: Consider missed ticks in NOHZ_FULL in update_cpu_load_nohz()
    
    Usually the tick can be stopped for an idle CPU in NOHZ. However in NOHZ_FULL
    mode, a non-idle CPU's tick can also be stopped. However, update_cpu_load_nohz()
    does not consider the case a non-idle CPU's tick has been stopped at all.
    
    This patch makes the update_cpu_load_nohz() know if the calling path comes
    from NOHZ_FULL or idle NOHZ.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447115762-19734-3-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edad7a43edea..f425aac63317 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -177,9 +177,9 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 extern void calc_global_load(unsigned long ticks);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
-extern void update_cpu_load_nohz(void);
+extern void update_cpu_load_nohz(int active);
 #else
-static inline void update_cpu_load_nohz(void) { }
+static inline void update_cpu_load_nohz(int active) { }
 #endif
 
 extern unsigned long get_parent_ip(unsigned long addr);

commit 264015f8a83fefc62c5125d761fbbadf924e520c
Merge: d55fc3785624 ab27a8d04b32
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 10 12:07:22 2015 -0800

    Merge tag 'libnvdimm-for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "Outside of the new ACPI-NFIT hot-add support this pull request is more
      notable for what it does not contain, than what it does.  There were a
      handful of development topics this cycle, dax get_user_pages, dax
      fsync, and raw block dax, that need more more iteration and will wait
      for 4.5.
    
      The patches to make devm and the pmem driver NUMA aware have been in
      -next for several weeks.  The hot-add support has not, but is
      contained to the NFIT driver and is passing unit tests.  The coredump
      support is straightforward and was looked over by Jeff.  All of it has
      received a 0day build success notification across 107 configs.
    
      Summary:
    
       - Add support for the ACPI 6.0 NFIT hot add mechanism to process
         updates of the NFIT at runtime.
    
       - Teach the coredump implementation how to filter out DAX mappings.
    
       - Introduce NUMA hints for allocations made by the pmem driver, and
         as a side effect all devm allocations now hint their NUMA node by
         default"
    
    * tag 'libnvdimm-for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm:
      coredump: add DAX filtering for FDPIC ELF coredumps
      coredump: add DAX filtering for ELF coredumps
      acpi: nfit: Add support for hot-add
      nfit: in acpi_nfit_init, break on a 0-length table
      pmem, memremap: convert to numa aware allocations
      devm_memremap_pages: use numa_mem_id
      devm: make allocations numa aware by default
      devm_memremap: convert to return ERR_PTR
      devm_memunmap: use devres_release()
      pmem: kill memremap_pmem()
      x86, mm: quiet arch_add_memory()

commit 5037835c1f3eabf4f22163fc0278dd87165f8957
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Mon Oct 5 16:33:36 2015 -0600

    coredump: add DAX filtering for ELF coredumps
    
    Add two new flags to the existing coredump mechanism for ELF files to
    allow us to explicitly filter DAX mappings.  This is desirable because
    DAX mappings, like hugetlb mappings, have the potential to be very
    large.
    
    Update the coredump_filter documentation in
    Documentation/filesystems/proc.txt so that it addresses the new DAX
    coredump flags.  Also update the documented default value of
    coredump_filter to be consistent with the core(5) man page.  The
    documentation being updated talks about bit 4, Dump ELF headers, which
    is enabled if CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is turned on in the
    kernel config.  This kernel config option defaults to "y" if both ELF
    binaries and coredump are enabled.
    
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Acked-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7b9501b41af..3c02d92ed23b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -483,9 +483,11 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_DUMP_ELF_HEADERS	6
 #define MMF_DUMP_HUGETLB_PRIVATE 7
 #define MMF_DUMP_HUGETLB_SHARED  8
+#define MMF_DUMP_DAX_PRIVATE	9
+#define MMF_DUMP_DAX_SHARED	10
 
 #define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
-#define MMF_DUMP_FILTER_BITS	7
+#define MMF_DUMP_FILTER_BITS	9
 #define MMF_DUMP_FILTER_MASK \
 	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
 #define MMF_DUMP_FILTER_DEFAULT \

commit 9a13049e83f346cb1cbd60c64e520a73c396af16
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Nov 6 16:32:25 2015 -0800

    signal: introduce kernel_signal_stop() to fix jffs2_garbage_collect_thread()
    
    jffs2_garbage_collect_thread() can race with SIGCONT and sleep in
    TASK_STOPPED state after it was already sent. Add the new helper,
    kernel_signal_stop(), which does this correctly.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Markus Pargmann <mpa@pengutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d54924b4b86..4069febaa34a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2475,6 +2475,16 @@ static inline int kernel_dequeue_signal(siginfo_t *info)
 	return ret;
 }
 
+static inline void kernel_signal_stop(void)
+{
+	spin_lock_irq(&current->sighand->siglock);
+	if (current->jobctl & JOBCTL_STOP_DEQUEUED)
+		__set_current_state(TASK_STOPPED);
+	spin_unlock_irq(&current->sighand->siglock);
+
+	schedule();
+}
+
 extern void release_task(struct task_struct * p);
 extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int force_sigsegv(int, struct task_struct *);

commit be0e6f290f78b84a3b21b8c8c46819c4514fe632
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Nov 6 16:32:22 2015 -0800

    signal: turn dequeue_signal_lock() into kernel_dequeue_signal()
    
    1. Rename dequeue_signal_lock() to kernel_dequeue_signal(). This
       matches another "for kthreads only" kernel_sigaction() helper.
    
    2. Remove the "tsk" and "mask" arguments, they are always current
       and current->blocked. And it is simply wrong if tsk != current.
    
    3. We could also remove the 3rd "siginfo_t *info" arg but it looks
       potentially useful. However we can simplify the callers if we
       change kernel_dequeue_signal() to accept info => NULL.
    
    4. Remove _irqsave, it is never called from atomic context.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Markus Pargmann <mpa@pengutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 923ec1a9b2b4..3d54924b4b86 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2462,14 +2462,15 @@ extern void ignore_signals(struct task_struct *);
 extern void flush_signal_handlers(struct task_struct *, int force_default);
 extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
 
-static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
+static inline int kernel_dequeue_signal(siginfo_t *info)
 {
-	unsigned long flags;
+	struct task_struct *tsk = current;
+	siginfo_t __info;
 	int ret;
 
-	spin_lock_irqsave(&tsk->sighand->siglock, flags);
-	ret = dequeue_signal(tsk, mask, info);
-	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
+	spin_lock_irq(&tsk->sighand->siglock);
+	ret = dequeue_signal(tsk, &tsk->blocked, info ?: &__info);
+	spin_unlock_irq(&tsk->sighand->siglock);
 
 	return ret;
 }

commit 2e01fabe67ccaff1d59bda01e60a61f5fb0aa7b6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Nov 6 16:32:19 2015 -0800

    signals: kill block_all_signals() and unblock_all_signals()
    
    It is hardly possible to enumerate all problems with block_all_signals()
    and unblock_all_signals().  Just for example,
    
    1. block_all_signals(SIGSTOP/etc) simply can't help if the caller is
       multithreaded. Another thread can dequeue the signal and force the
       group stop.
    
    2. Even is the caller is single-threaded, it will "stop" anyway. It
       will not sleep, but it will spin in kernel space until SIGCONT or
       SIGKILL.
    
    And a lot more. In short, this interface doesn't work at all, at least
    the last 10+ years.
    
    Daniel said:
    
      Yeah the only times I played around with the DRM_LOCK stuff was when
      old drivers accidentally deadlocked - my impression is that the entire
      DRM_LOCK thing was never really tested properly ;-) Hence I'm all for
      purging where this leaks out of the drm subsystem.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Acked-by: Dave Airlie <airlied@redhat.com>
    Cc: Richard Weinberger <richard@nod.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eeb5066a44fb..923ec1a9b2b4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1570,9 +1570,7 @@ struct task_struct {
 
 	unsigned long sas_ss_sp;
 	size_t sas_ss_size;
-	int (*notifier)(void *priv);
-	void *notifier_data;
-	sigset_t *notifier_mask;
+
 	struct callback_head *task_works;
 
 	struct audit_context *audit_context;
@@ -2476,9 +2474,6 @@ static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, s
 	return ret;
 }
 
-extern void block_all_signals(int (*notifier)(void *priv), void *priv,
-			      sigset_t *mask);
-extern void unblock_all_signals(void);
 extern void release_task(struct task_struct * p);
 extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int force_sigsegv(int, struct task_struct *);

commit 2e3078af2c67730c479f1d183af5b367f5d95337
Merge: ea5c58e70c3a b3b0d09c7a23
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 23:10:54 2015 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge patch-bomb from Andrew Morton:
    
     - inotify tweaks
    
     - some ocfs2 updates (many more are awaiting review)
    
     - various misc bits
    
     - kernel/watchdog.c updates
    
     - Some of mm.  I have a huge number of MM patches this time and quite a
       lot of it is quite difficult and much will be held over to next time.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (162 commits)
      selftests: vm: add tests for lock on fault
      mm: mlock: add mlock flags to enable VM_LOCKONFAULT usage
      mm: introduce VM_LOCKONFAULT
      mm: mlock: add new mlock system call
      mm: mlock: refactor mlock, munlock, and munlockall code
      kasan: always taint kernel on report
      mm, slub, kasan: enable user tracking by default with KASAN=y
      kasan: use IS_ALIGNED in memory_is_poisoned_8()
      kasan: Fix a type conversion error
      lib: test_kasan: add some testcases
      kasan: update reference to kasan prototype repo
      kasan: move KASAN_SANITIZE in arch/x86/boot/Makefile
      kasan: various fixes in documentation
      kasan: update log messages
      kasan: accurately determine the type of the bad access
      kasan: update reported bug types for kernel memory accesses
      kasan: update reported bug types for not user nor kernel memory accesses
      mm/kasan: prevent deadlock in kasan reporting
      mm/kasan: don't use kasan shadow pointer in generic functions
      mm/kasan: MODULE_VADDR is not available on all archs
      ...

commit b23afb93d317c65cef553b804f08dec8a7a0f7e1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Nov 5 18:46:11 2015 -0800

    memcg: punt high overage reclaim to return-to-userland path
    
    Currently, try_charge() tries to reclaim memory synchronously when the
    high limit is breached; however, if the allocation doesn't have
    __GFP_WAIT, synchronous reclaim is skipped.  If a process performs only
    speculative allocations, it can blow way past the high limit.  This is
    actually easily reproducible by simply doing "find /".  slab/slub
    allocator tries speculative allocations first, so as long as there's
    memory which can be consumed without blocking, it can keep allocating
    memory regardless of the high limit.
    
    This patch makes try_charge() always punt the over-high reclaim to the
    return-to-userland path.  If try_charge() detects that high limit is
    breached, it adds the overage to current->memcg_nr_pages_over_high and
    schedules execution of mem_cgroup_handle_over_high() which performs
    synchronous reclaim from the return-to-userland path.
    
    As long as kernel doesn't have a run-away allocation spree, this should
    provide enough protection while making kmemcg behave more consistently.
    It also has the following benefits.
    
    - All over-high reclaims can use GFP_KERNEL regardless of the specific
      gfp mask in use, e.g. GFP_NOFS, when the limit was breached.
    
    - It copes with prio inversion.  Previously, a low-prio task with
      small memory.high might perform over-high reclaim with a bunch of
      locks held.  If a higher prio task needed any of these locks, it
      would have to wait until the low prio task finished reclaim and
      released the locks.  By handing over-high reclaim to the task exit
      path this issue can be avoided.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Reviewed-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17bf8b845aa0..055f2ee3b0f0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1809,6 +1809,9 @@ struct task_struct {
 	struct mem_cgroup *memcg_in_oom;
 	gfp_t memcg_oom_gfp_mask;
 	int memcg_oom_order;
+
+	/* number of pages to reclaim on returning to userland */
+	unsigned int memcg_nr_pages_over_high;
 #endif
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;

commit 626ebc4100285be56fe3546f29b6afeb36b6871a
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Nov 5 18:46:09 2015 -0800

    memcg: flatten task_struct->memcg_oom
    
    task_struct->memcg_oom is a sub-struct containing fields which are used
    for async memcg oom handling.  Most task_struct fields aren't packaged
    this way and it can lead to unnecessary alignment paddings.  This patch
    flattens it.
    
    * task.memcg_oom.memcg          -> task.memcg_in_oom
    * task.memcg_oom.gfp_mask       -> task.memcg_oom_gfp_mask
    * task.memcg_oom.order          -> task.memcg_oom_order
    * task.memcg_oom.may_oom        -> task.memcg_may_oom
    
    In addition, task.memcg_may_oom is relocated to where other bitfields are
    which reduces the size of task_struct.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5423b9c82fee..17bf8b845aa0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1473,7 +1473,9 @@ struct task_struct {
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 	unsigned sched_migrated:1;
-
+#ifdef CONFIG_MEMCG
+	unsigned memcg_may_oom:1;
+#endif
 #ifdef CONFIG_MEMCG_KMEM
 	unsigned memcg_kmem_skip_account:1;
 #endif
@@ -1804,12 +1806,9 @@ struct task_struct {
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
 #ifdef CONFIG_MEMCG
-	struct memcg_oom_info {
-		struct mem_cgroup *memcg;
-		gfp_t gfp_mask;
-		int order;
-		unsigned int may_oom:1;
-	} memcg_oom;
+	struct mem_cgroup *memcg_in_oom;
+	gfp_t memcg_oom_gfp_mask;
+	int memcg_oom_order;
 #endif
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;

commit ac1f591249d95372f3a5ab3828d4af5dfbf5efd3
Author: Don Zickus <dzickus@redhat.com>
Date:   Thu Nov 5 18:44:44 2015 -0800

    kernel/watchdog.c: add sysctl knob hardlockup_panic
    
    The only way to enable a hardlockup to panic the machine is to set
    'nmi_watchdog=panic' on the kernel command line.
    
    This makes it awkward for end users and folks who want to run automate
    tests (like myself).
    
    Mimic the softlockup_panic knob and create a /proc/sys/kernel/hardlockup_panic
    knob.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c115d617739d..5423b9c82fee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -384,6 +384,7 @@ extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
 				  void __user *buffer,
 				  size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
+extern unsigned int  hardlockup_panic;
 void lockup_detector_init(void);
 #else
 static inline void touch_softlockup_watchdog(void)

commit 69234acee54407962a20bedf90ef9c96326994b5
Merge: 11eaaadb3ea3 d57456753787
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 14:51:32 2015 -0800

    Merge branch 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "The cgroup core saw several significant updates this cycle:
    
       - percpu_rwsem for threadgroup locking is reinstated.  This was
         temporarily dropped due to down_write latency issues.  Oleg's
         rework of percpu_rwsem which is scheduled to be merged in this
         merge window resolves the issue.
    
       - On the v2 hierarchy, when controllers are enabled and disabled, all
         operations are atomic and can fail and revert cleanly.  This allows
         ->can_attach() failure which is necessary for cpu RT slices.
    
       - Tasks now stay associated with the original cgroups after exit
         until released.  This allows tracking resources held by zombies
         (e.g.  pids) and makes it easy to find out where zombies came from
         on the v2 hierarchy.  The pids controller was broken before these
         changes as zombies escaped the limits; unfortunately, updating this
         behavior required too many invasive changes and I don't think it's
         a good idea to backport them, so the pids controller on 4.3, the
         first version which included the pids controller, will stay broken
         at least until I'm sure about the cgroup core changes.
    
       - Optimization of a couple common tests using static_key"
    
    * 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (38 commits)
      cgroup: fix race condition around termination check in css_task_iter_next()
      blkcg: don't create "io.stat" on the root cgroup
      cgroup: drop cgroup__DEVEL__legacy_files_on_dfl
      cgroup: replace error handling in cgroup_init() with WARN_ON()s
      cgroup: add cgroup_subsys->free() method and use it to fix pids controller
      cgroup: keep zombies associated with their original cgroups
      cgroup: make css_set_rwsem a spinlock and rename it to css_set_lock
      cgroup: don't hold css_set_rwsem across css task iteration
      cgroup: reorganize css_task_iter functions
      cgroup: factor out css_set_move_task()
      cgroup: keep css_set and task lists in chronological order
      cgroup: make cgroup_destroy_locked() test cgroup_is_populated()
      cgroup: make css_sets pin the associated cgroups
      cgroup: relocate cgroup_[try]get/put()
      cgroup: move check_for_release() invocation
      cgroup: replace cgroup_has_tasks() with cgroup_is_populated()
      cgroup: make cgroup->nr_populated count the number of populated css_sets
      cgroup: remove an unused parameter from cgroup_task_migrate()
      cgroup: fix too early usage of static_branch_disable()
      cgroup: make cgroup_update_dfl_csses() migrate all target processes atomically
      ...

commit b0f85fa11aefc4f3e03306b4cd47f113bd57dcba
Merge: ccc9d4a6d640 f32bfb9a8ca0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 09:41:05 2015 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
    Changes of note:
    
     1) Allow to schedule ICMP packets in IPVS, from Alex Gartrell.
    
     2) Provide FIB table ID in ipv4 route dumps just as ipv6 does, from
        David Ahern.
    
     3) Allow the user to ask for the statistics to be filtered out of
        ipv4/ipv6 address netlink dumps.  From Sowmini Varadhan.
    
     4) More work to pass the network namespace context around deep into
        various packet path APIs, starting with the netfilter hooks.  From
        Eric W Biederman.
    
     5) Add layer 2 TX/RX checksum offloading to qeth driver, from Thomas
        Richter.
    
     6) Use usec resolution for SYN/ACK RTTs in TCP, from Yuchung Cheng.
    
     7) Support Very High Throughput in wireless MESH code, from Bob
        Copeland.
    
     8) Allow setting the ageing_time in switchdev/rocker.  From Scott
        Feldman.
    
     9) Properly autoload L2TP type modules, from Stephen Hemminger.
    
    10) Fix and enable offload features by default in 8139cp driver, from
        David Woodhouse.
    
    11) Support both ipv4 and ipv6 sockets in a single vxlan device, from
        Jiri Benc.
    
    12) Fix CWND limiting of thin streams in TCP, from Bendik RÃ¸nning
        Opstad.
    
    13) Fix IPSEC flowcache overflows on large systems, from Steffen
        Klassert.
    
    14) Convert bridging to track VLANs using rhashtable entries rather than
        a bitmap.  From Nikolay Aleksandrov.
    
    15) Make TCP listener handling completely lockless, this is a major
        accomplishment.  Incoming request sockets now live in the
        established hash table just like any other socket too.
    
        From Eric Dumazet.
    
    15) Provide more bridging attributes to netlink, from Nikolay
        Aleksandrov.
    
    16) Use hash based algorithm for ipv4 multipath routing, this was very
        long overdue.  From Peter NÃ¸rlund.
    
    17) Several y2038 cures, mostly avoiding timespec.  From Arnd Bergmann.
    
    18) Allow non-root execution of EBPF programs, from Alexei Starovoitov.
    
    19) Support SO_INCOMING_CPU as setsockopt, from Eric Dumazet.  This
        influences the port binding selection logic used by SO_REUSEPORT.
    
    20) Add ipv6 support to VRF, from David Ahern.
    
    21) Add support for Mellanox Spectrum switch ASIC, from Jiri Pirko.
    
    22) Add rtl8xxxu Realtek wireless driver, from Jes Sorensen.
    
    23) Implement RACK loss recovery in TCP, from Yuchung Cheng.
    
    24) Support multipath routes in MPLS, from Roopa Prabhu.
    
    25) Fix POLLOUT notification for listening sockets in AF_UNIX, from Eric
        Dumazet.
    
    26) Add new QED Qlogic river, from Yuval Mintz, Manish Chopra, and
        Sudarsana Kalluru.
    
    27) Don't fetch timestamps on AF_UNIX sockets, from Hannes Frederic
        Sowa.
    
    28) Support ipv6 geneve tunnels, from John W Linville.
    
    29) Add flood control support to switchdev layer, from Ido Schimmel.
    
    30) Fix CHECKSUM_PARTIAL handling of potentially fragmented frames, from
        Hannes Frederic Sowa.
    
    31) Support persistent maps and progs in bpf, from Daniel Borkmann.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1790 commits)
      sh_eth: use DMA barriers
      switchdev: respect SKIP_EOPNOTSUPP flag in case there is no recursion
      net: sched: kill dead code in sch_choke.c
      irda: Delete an unnecessary check before the function call "irlmp_unregister_service"
      net: dsa: mv88e6xxx: include DSA ports in VLANs
      net: dsa: mv88e6xxx: disable SA learning for DSA and CPU ports
      net/core: fix for_each_netdev_feature
      vlan: Invoke driver vlan hooks only if device is present
      arcnet/com20020: add LEDS_CLASS dependency
      bpf, verifier: annotate verbose printer with __printf
      dp83640: Only wait for timestamps for packets with timestamping enabled.
      ptp: Change ptp_class to a proper bitmask
      dp83640: Prune rx timestamp list before reading from it
      dp83640: Delay scheduled work.
      dp83640: Include hash in timestamp/packet matching
      ipv6: fix tunnel error handling
      net/mlx5e: Fix LSO vlan insertion
      net/mlx5e: Re-eanble client vlan TX acceleration
      net/mlx5e: Return error in case mlx5e_set_features() fails
      net/mlx5e: Don't allow more than max supported channels
      ...

commit 53528695ff6d8b77011bc818407c13e30914a946
Merge: b831ef2cad97 e73e85f05938
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 18:03:50 2015 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - sched/fair load tracking fixes and cleanups (Byungchul Park)
    
       - Make load tracking frequency scale invariant (Dietmar Eggemann)
    
       - sched/deadline updates (Juri Lelli)
    
       - stop machine fixes, cleanups and enhancements for bugs triggered by
         CPU hotplug stress testing (Oleg Nesterov)
    
       - scheduler preemption code rework: remove PREEMPT_ACTIVE and related
         cleanups (Peter Zijlstra)
    
       - Rework the sched_info::run_delay code to fix races (Peter Zijlstra)
    
       - Optimize per entity utilization tracking (Peter Zijlstra)
    
       - ... misc other fixes, cleanups and smaller updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      sched: Don't scan all-offline ->cpus_allowed twice if !CONFIG_CPUSETS
      sched: Move cpu_active() tests from stop_two_cpus() into migrate_swap_stop()
      sched: Start stopper early
      stop_machine: Kill cpu_stop_threads->setup() and cpu_stop_unpark()
      stop_machine: Kill smp_hotplug_thread->pre_unpark, introduce stop_machine_unpark()
      stop_machine: Change cpu_stop_queue_two_works() to rely on stopper->enabled
      stop_machine: Introduce __cpu_stop_queue_work() and cpu_stop_queue_two_works()
      stop_machine: Ensure that a queued callback will be called before cpu_stop_park()
      sched/x86: Fix typo in __switch_to() comments
      sched/core: Remove a parameter in the migrate_task_rq() function
      sched/core: Drop unlikely behind BUG_ON()
      sched/core: Fix task and run queue sched_info::run_delay inconsistencies
      sched/numa: Fix task_tick_fair() from disabling numa_balancing
      sched/core: Add preempt_count invariant check
      sched/core: More notrace annotations
      sched/core: Kill PREEMPT_ACTIVE
      sched/core, sched/x86: Kill thread_info::saved_preempt_count
      sched/core: Simplify preempt_count tests
      sched/core: Robustify preemption leak checks
      sched/core: Stop setting PREEMPT_ACTIVE
      ...

commit 281422869942c19f05a08d4017c633d08d390938
Merge: f5a8160c1e05 b33e18f61bd1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 15:40:38 2015 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Improvements to expedited grace periods (Paul E McKenney)
    
       - Performance improvements to and locktorture tests for percpu-rwsem
         (Oleg Nesterov, Paul E McKenney)
    
       - Torture-test changes (Paul E McKenney, Davidlohr Bueso)
    
       - Documentation updates (Paul E McKenney)
    
       - Miscellaneous fixes (Paul E McKenney, Boqun Feng, Oleg Nesterov,
         Patrick Marlier)"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      fs/writeback, rcu: Don't use list_entry_rcu() for pointer offsetting in bdi_split_work_to_wbs()
      rcu: Better hotplug handling for synchronize_sched_expedited()
      rcu: Enable stall warnings for synchronize_rcu_expedited()
      rcu: Add tasks to expedited stall-warning messages
      rcu: Add online/offline info to expedited stall warning message
      rcu: Consolidate expedited CPU selection
      rcu: Prepare for consolidating expedited CPU selection
      cpu: Remove try_get_online_cpus()
      rcu: Stop excluding CPU hotplug in synchronize_sched_expedited()
      rcu: Stop silencing lockdep false positive for expedited grace periods
      rcu: Switch synchronize_sched_expedited() to IPI
      locktorture: Fix module unwind when bad torture_type specified
      torture: Forgive non-plural arguments
      rcutorture: Fix unused-function warning for torturing_tasks()
      rcutorture: Fix module unwind when bad torture_type specified
      rcu_sync: Cleanup the CONFIG_PROVE_RCU checks
      locking/percpu-rwsem: Clean up the lockdep annotations in percpu_down_read()
      locking/percpu-rwsem: Fix the comments outdated by rcu_sync
      locking/percpu-rwsem: Make use of the rcu_sync infrastructure
      locking/percpu-rwsem: Make percpu_free_rwsem() after kzalloc() safe
      ...

commit c13dc31adb04c3f85d54d2fa13e34206f25742eb
Merge: 7379047d5585 39cd2dd39a8b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 19 10:09:54 2015 +0200

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
      - Miscellaneous fixes. (Paul E. McKenney, Boqun Feng, Oleg Nesterov, Patrick Marlier)
    
      - Improvements to expedited grace periods. (Paul E. McKenney)
    
      - Performance improvements to and locktorture tests for percpu-rwsem.
        (Oleg Nesterov, Paul E. McKenney)
    
      - Torture-test changes. (Paul E. McKenney, Davidlohr Bueso)
    
      - Documentation updates. (Paul E. McKenney)
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c8d75aa47dd585c9538a8205e9bb9847e12cfb84
Author: Jason Low <jason.low2@hp.com>
Date:   Wed Oct 14 12:07:56 2015 -0700

    posix_cpu_timer: Reduce unnecessary sighand lock contention
    
    It was found while running a database workload on large systems that
    significant time was spent trying to acquire the sighand lock.
    
    The issue was that whenever an itimer expired, many threads ended up
    simultaneously trying to send the signal. Most of the time, nothing
    happened after acquiring the sighand lock because another thread
    had just already sent the signal and updated the "next expire" time.
    The fastpath_timer_check() didn't help much since the "next expire"
    time was updated after the threads exit fastpath_timer_check().
    
    This patch addresses this by having the thread_group_cputimer structure
    maintain a boolean to signify when a thread in the group is already
    checking for process wide timers, and adds extra logic in the fastpath
    to check the boolean.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: George Spelvin <linux@horizon.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: hideaki.kimura@hpe.com
    Cc: terry.rudd@hpe.com
    Cc: scott.norton@hpe.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1444849677-29330-5-git-send-email-jason.low2@hp.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6c8504ade2ba..f87559df5b75 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -619,6 +619,8 @@ struct task_cputime_atomic {
  * @cputime_atomic:	atomic thread group interval timers.
  * @running:		true when there are timers running and
  *			@cputime_atomic receives updates.
+ * @checking_timer:	true when a thread in the group is in the
+ *			process of checking for thread group timers.
  *
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU timer calculations.
@@ -626,6 +628,7 @@ struct task_cputime_atomic {
 struct thread_group_cputimer {
 	struct task_cputime_atomic cputime_atomic;
 	bool running;
+	bool checking_timer;
 };
 
 #include <linux/rwsem.h>

commit d5c373eb5610686162ff50429f63f4c00c554799
Author: Jason Low <jason.low2@hp.com>
Date:   Wed Oct 14 12:07:55 2015 -0700

    posix_cpu_timer: Convert cputimer->running to bool
    
    In the next patch in this series, a new field 'checking_timer' will
    be added to 'struct thread_group_cputimer'. Both this and the
    existing 'running' integer field are just used as boolean values. To
    save space in the structure, we can make both of these fields booleans.
    
    This is a preparatory patch to convert the existing running integer
    field to a boolean.
    
    Suggested-by: George Spelvin <linux@horizon.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed: George Spelvin <linux@horizon.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: hideaki.kimura@hpe.com
    Cc: terry.rudd@hpe.com
    Cc: scott.norton@hpe.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1444849677-29330-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7b9501b41af..6c8504ade2ba 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -617,15 +617,15 @@ struct task_cputime_atomic {
 /**
  * struct thread_group_cputimer - thread group interval timer counts
  * @cputime_atomic:	atomic thread group interval timers.
- * @running:		non-zero when there are timers running and
- * 			@cputime receives updates.
+ * @running:		true when there are timers running and
+ *			@cputime_atomic receives updates.
  *
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU timer calculations.
  */
 struct thread_group_cputimer {
 	struct task_cputime_atomic cputime_atomic;
-	int running;
+	bool running;
 };
 
 #include <linux/rwsem.h>

commit aaac3ba95e4c8b496d22f68bd1bc01cfbf525eca
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Oct 7 22:23:22 2015 -0700

    bpf: charge user for creation of BPF maps and programs
    
    since eBPF programs and maps use kernel memory consider it 'locked' memory
    from user accounting point of view and charge it against RLIMIT_MEMLOCK limit.
    This limit is typically set to 64Kbytes by distros, so almost all
    bpf+tracing programs would need to increase it, since they use maps,
    but kernel charges maximum map size upfront.
    For example the hash map of 1024 elements will be charged as 64Kbyte.
    It's inconvenient for current users and changes current behavior for root,
    but probably worth doing to be consistent root vs non-root.
    
    Similar accounting logic is done by mmap of perf_event.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7b9501b41af..4817df5fffae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -840,7 +840,7 @@ struct user_struct {
 	struct hlist_node uidhash_node;
 	kuid_t uid;
 
-#ifdef CONFIG_PERF_EVENTS
+#if defined(CONFIG_PERF_EVENTS) || defined(CONFIG_BPF_SYSCALL)
 	atomic_long_t locked_vm;
 #endif
 };

commit 609ca066386b2e64d4c0b0f55da327654962a0c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 28 17:52:18 2015 +0200

    sched/core: Create preempt_count invariant
    
    Assuming units of PREEMPT_DISABLE_OFFSET for preempt_count() numbers.
    
    Now that TASK_DEAD no longer results in preempt_count() == 3 during
    scheduling, we will always call context_switch() with preempt_count()
    == 2.
    
    However, we don't always end up with preempt_count() == 2 in
    finish_task_switch() because new tasks get created with
    preempt_count() == 1.
    
    Create FORK_PREEMPT_COUNT and set it to 2 and use that in the right
    places. Note that we cannot use INIT_PREEMPT_COUNT as that serves
    another purpose (boot).
    
    After this, preempt_count() is invariant across the context switch,
    with exception of PREEMPT_ACTIVE.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e5b8cbc4b8d6..23ca455d9582 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -599,11 +599,7 @@ struct task_cputime_atomic {
 		.sum_exec_runtime = ATOMIC64_INIT(0),		\
 	}
 
-#ifdef CONFIG_PREEMPT_COUNT
-#define PREEMPT_DISABLED	(1 + PREEMPT_ENABLED)
-#else
-#define PREEMPT_DISABLED	PREEMPT_ENABLED
-#endif
+#define PREEMPT_DISABLED	(PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)
 
 /*
  * Disable preemption until the scheduler is running -- use an unconditional
@@ -613,6 +609,17 @@ struct task_cputime_atomic {
  */
 #define INIT_PREEMPT_COUNT	PREEMPT_OFFSET
 
+/*
+ * Initial preempt_count value; reflects the preempt_count schedule invariant
+ * which states that during context switches:
+ *
+ *    preempt_count() == 2*PREEMPT_DISABLE_OFFSET
+ *
+ * Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.
+ * Note: See finish_task_switch().
+ */
+#define FORK_PREEMPT_COUNT	(2*PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)
+
 /**
  * struct thread_group_cputimer - thread group interval timer counts
  * @cputime_atomic:	atomic thread group interval timers.

commit 87dcbc0610cb580c8eaf289f52aca3620af825f0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 28 17:45:40 2015 +0200

    sched/core: Simplify INIT_PREEMPT_COUNT
    
    As per the following commit:
    
      d86ee4809d03 ("sched: optimize cond_resched()")
    
    we need PREEMPT_ACTIVE to avoid cond_resched() from working before
    the scheduler is set up.
    
    However, keeping preemption disabled should do the same thing
    already, making the PREEMPT_ACTIVE part entirely redundant.
    
    The only complication is !PREEMPT_COUNT kernels, where
    PREEMPT_DISABLED ends up being 0. Instead we use an unconditional
    PREEMPT_OFFSET to set preempt_count() even on !PREEMPT_COUNT
    kernels.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d086cf0ca2c7..e5b8cbc4b8d6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -606,19 +606,18 @@ struct task_cputime_atomic {
 #endif
 
 /*
- * Disable preemption until the scheduler is running.
- * Reset by start_kernel()->sched_init()->init_idle().
+ * Disable preemption until the scheduler is running -- use an unconditional
+ * value so that it also works on !PREEMPT_COUNT kernels.
  *
- * We include PREEMPT_ACTIVE to avoid cond_resched() from working
- * before the scheduler is active -- see should_resched().
+ * Reset by start_kernel()->sched_init()->init_idle()->init_idle_preempt_count().
  */
-#define INIT_PREEMPT_COUNT	(PREEMPT_DISABLED + PREEMPT_ACTIVE)
+#define INIT_PREEMPT_COUNT	PREEMPT_OFFSET
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts
  * @cputime_atomic:	atomic thread group interval timers.
  * @running:		non-zero when there are timers running and
- * 			@cputime receives updates.
+ *			@cputime receives updates.
  *
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU timer calculations.

commit fe19159225d8516f3f57a5fe8f735c01684f0ddd
Merge: c6e1e7b5b7f0 95913d97914f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 6 17:05:36 2015 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c6e1e7b5b7f031910850ddaf7bfa65ba3b4843ea
Author: Juergen Gross <jgross@suse.com>
Date:   Tue Sep 22 12:48:59 2015 +0200

    sched/core: Make 'sched_domain_topology' declaration static
    
    The 'sched_domain_topology' variable is only used within kernel/sched/core.c.
    Make it static.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1442918939-9907-1-git-send-email-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bd38b3ee9e83..699228bb0035 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1127,8 +1127,6 @@ struct sched_domain_topology_level {
 #endif
 };
 
-extern struct sched_domain_topology_level *sched_domain_topology;
-
 extern void set_sched_topology(struct sched_domain_topology_level *tl);
 extern void wake_up_if_idle(int cpu);
 

commit 8203d6d0ee784cfb2ebf89053f7fe399abc867d7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Aug 2 13:53:17 2015 -0700

    rcu: Use single-stage IPI algorithm for RCU expedited grace period
    
    The current preemptible-RCU expedited grace-period algorithm invokes
    synchronize_sched_expedited() to enqueue all tasks currently running
    in a preemptible-RCU read-side critical section, then waits for all the
    ->blkd_tasks lists to drain.  This works, but results in both an IPI and
    a double context switch even on CPUs that do not happen to be running
    in a preemptible RCU read-side critical section.
    
    This commit implements a new algorithm that causes less OS jitter.
    This new algorithm IPIs all online CPUs that are not idle (from an
    RCU perspective), but refrains from self-IPIs.  If a CPU receiving
    this IPI is not in a preemptible RCU read-side critical section (or
    is just now exiting one), it pushes quiescence up the rcu_node tree,
    otherwise, it sets a flag that will be handled by the upcoming outermost
    rcu_read_unlock(), which will then push quiescence up the tree.
    
    The expedited grace period must of course wait on any pre-existing blocked
    readers, and newly blocked readers must be queued carefully based on
    the state of both the normal and the expedited grace periods.  This
    new queueing approach also avoids the need to update boost state,
    courtesy of the fact that blocked tasks are no longer ever migrated to
    the root rcu_node structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a4ab9daa387c..7fa8c4d372e7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1330,10 +1330,12 @@ struct sched_dl_entity {
 
 union rcu_special {
 	struct {
-		bool blocked;
-		bool need_qs;
-	} b;
-	short s;
+		u8 blocked;
+		u8 need_qs;
+		u8 exp_need_qs;
+		u8 pad;	/* Otherwise the compiler can store garbage here. */
+	} b; /* Bits. */
+	u32 s; /* Set of bits. */
 };
 struct rcu_node;
 

commit 1ed1328792ff46e4bb86a3d7f7be2971f4549f6c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 12:53:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    Note: This commit was originally committed as d59cfc09c32a but got
          reverted by 0c986253b939 due to the performance regression from
          the percpu_rwsem write down/up operations added to cgroup task
          migration path.  percpu_rwsem changes which alleviate the
          performance issue are pending for v4.4-rc1 merge window.
          Re-apply.
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7b9501b41af..a4ab9daa387c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -762,18 +762,6 @@ struct signal_struct {
 	unsigned audit_tty_log_passwd;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
-#ifdef CONFIG_CGROUPS
-	/*
-	 * group_rwsem prevents new tasks from entering the threadgroup and
-	 * member tasks from exiting,a more specifically, setting of
-	 * PF_EXITING.  fork and exit paths are protected with this rwsem
-	 * using threadgroup_change_begin/end().  Users which require
-	 * threadgroup to remain stable should use threadgroup_[un]lock()
-	 * which also takes care of exec path.  Currently, cgroup is the
-	 * only user.
-	 */
-	struct rw_semaphore group_rwsem;
-#endif
 
 	oom_flags_t oom_flags;
 	short oom_score_adj;		/* OOM kill score adjustment */

commit 0c986253b939cc14c69d4adbe2b4121bdf4aa220
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 11:51:12 2015 -0400

    Revert "sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem"
    
    This reverts commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14.
    
    d59cfc09c32a ("sched, cgroup: replace signal_struct->group_rwsem with
    a global percpu_rwsem") and b5ba75b5fc0e ("cgroup: simplify
    threadgroup locking") changed how cgroup synchronizes against task
    fork and exits so that it uses global percpu_rwsem instead of
    per-process rwsem; unfortunately, the write [un]lock paths of
    percpu_rwsem always involve synchronize_rcu_expedited() which turned
    out to be too expensive.
    
    Improvements for percpu_rwsem are scheduled to be merged in the coming
    v4.4-rc1 merge window which alleviates this issue.  For now, revert
    the two commits to restore per-process rwsem.  They will be re-applied
    for the v4.4-rc1 merge window.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: stable@vger.kernel.org # v4.2+

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a4ab9daa387c..b7b9501b41af 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -762,6 +762,18 @@ struct signal_struct {
 	unsigned audit_tty_log_passwd;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
+#ifdef CONFIG_CGROUPS
+	/*
+	 * group_rwsem prevents new tasks from entering the threadgroup and
+	 * member tasks from exiting,a more specifically, setting of
+	 * PF_EXITING.  fork and exit paths are protected with this rwsem
+	 * using threadgroup_change_begin/end().  Users which require
+	 * threadgroup to remain stable should use threadgroup_[un]lock()
+	 * which also takes care of exec path.  Currently, cgroup is the
+	 * only user.
+	 */
+	struct rw_semaphore group_rwsem;
+#endif
 
 	oom_flags_t oom_flags;
 	short oom_score_adj;		/* OOM kill score adjustment */

commit e3279a2e6d697e00e74f905851ee7cf532f72b2d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Sat Aug 15 00:04:41 2015 +0100

    sched/fair: Make utilization tracking CPU scale-invariant
    
    Besides the existing frequency scale-invariance correction factor, apply
    CPU scale-invariance correction factor to utilization tracking to
    compensate for any differences in compute capacity. This could be due to
    micro-architectural differences (i.e. instructions per seconds) between
    cpus in HMP systems (e.g. big.LITTLE), and/or differences in the current
    maximum frequency supported by individual cpus in SMP systems. In the
    existing implementation utilization isn't comparable between cpus as it
    is relative to the capacity of each individual CPU.
    
    Each segment of the sched_avg.util_sum geometric series is now scaled
    by the CPU performance factor too so the sched_avg.util_avg of each
    sched entity will be invariant from the particular CPU of the HMP/SMP
    system on which the sched entity is scheduled.
    
    With this patch, the utilization of a CPU stays relative to the max CPU
    performance of the fastest CPU in the system.
    
    In contrast to utilization (sched_avg.util_sum), load
    (sched_avg.load_sum) should not be scaled by compute capacity. The
    utilization metric is based on running time which only makes sense when
    cpus are _not_ fully utilized (utilization cannot go beyond 100% even if
    more tasks are added), where load is runnable time which isn't limited
    by the capacity of the CPU and therefore is a better metric for
    overloaded scenarios. If we run two nice-0 busy loops on two cpus with
    different compute capacity their load should be similar since their
    compute demands are the same. We have to assume that the compute demand
    of any task running on a fully utilized CPU (no spare cycles = 100%
    utilization) is high and the same no matter of the compute capacity of
    its current CPU, hence we shouldn't scale load by CPU capacity.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/55CE7409.1000700@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c8d923ba429d..bd38b3ee9e83 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1180,7 +1180,7 @@ struct load_weight {
  * 1) load_avg factors frequency scaling into the amount of time that a
  * sched_entity is runnable on a rq into its weight. For cfs_rq, it is the
  * aggregated such weights of all runnable and blocked sched_entities.
- * 2) util_avg factors frequency scaling into the amount of time
+ * 2) util_avg factors frequency and cpu scaling into the amount of time
  * that a sched_entity is running on a CPU, in the range [0..SCHED_LOAD_SCALE].
  * For cfs_rq, it is the aggregated such times of all runnable and
  * blocked sched_entities.

commit e0f5f3afd2cffa96291cd852056d83ff4e2e99c7
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Fri Aug 14 17:23:09 2015 +0100

    sched/fair: Make load tracking frequency scale-invariant
    
    Apply frequency scaling correction factor to per-entity load tracking to
    make it frequency invariant. Currently, load appears bigger when the CPU
    is running slower which affects load-balancing decisions.
    
    Each segment of the sched_avg.load_sum geometric series is now scaled by
    the current frequency so that the sched_avg.load_avg of each sched entity
    will be invariant from frequency scaling.
    
    Moreover, cfs_rq.runnable_load_sum is scaled by the current frequency as
    well.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Dietmar Eggemann <Dietmar.Eggemann@arm.com>
    Cc: Juri Lelli <Juri.Lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: daniel.lezcano@linaro.org
    Cc: mturquette@baylibre.com
    Cc: pang.xunlei@zte.com.cn
    Cc: rjw@rjwysocki.net
    Cc: sgurrappadi@nvidia.com
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1439569394-11974-2-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a4ab9daa387c..c8d923ba429d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1177,9 +1177,9 @@ struct load_weight {
 
 /*
  * The load_avg/util_avg accumulates an infinite geometric series.
- * 1) load_avg factors the amount of time that a sched_entity is
- * runnable on a rq into its weight. For cfs_rq, it is the aggregated
- * such weights of all runnable and blocked sched_entities.
+ * 1) load_avg factors frequency scaling into the amount of time that a
+ * sched_entity is runnable on a rq into its weight. For cfs_rq, it is the
+ * aggregated such weights of all runnable and blocked sched_entities.
  * 2) util_avg factors frequency scaling into the amount of time
  * that a sched_entity is running on a CPU, in the range [0..SCHED_LOAD_SCALE].
  * For cfs_rq, it is the aggregated such times of all runnable and

commit d950c9477d51f0cefc2ed3cf76e695d46af0d9c1
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Sep 4 15:47:35 2015 -0700

    mm: defer flush of writable TLB entries
    
    If a PTE is unmapped and it's dirty then it was writable recently.  Due to
    deferred TLB flushing, it's best to assume a writable TLB cache entry
    exists.  With that assumption, the TLB must be flushed before any IO can
    start or the page is freed to avoid lost writes or data corruption.  This
    patch defers flushing of potentially writable TLBs as long as possible.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3c602c20c717..a4ab9daa387c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1354,6 +1354,13 @@ struct tlbflush_unmap_batch {
 
 	/* True if any bit in cpumask is set */
 	bool flush_required;
+
+	/*
+	 * If true then the PTE was dirty when unmapped. The entry must be
+	 * flushed before IO is initiated or a stale TLB entry potentially
+	 * allows an update without redirtying the page.
+	 */
+	bool writable;
 };
 
 struct task_struct {

commit 72b252aed506b8f1a03f7abd29caef4cdf6a043b
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Sep 4 15:47:32 2015 -0700

    mm: send one IPI per CPU to TLB flush all entries after unmapping pages
    
    An IPI is sent to flush remote TLBs when a page is unmapped that was
    potentially accesssed by other CPUs.  There are many circumstances where
    this happens but the obvious one is kswapd reclaiming pages belonging to a
    running process as kswapd and the task are likely running on separate
    CPUs.
    
    On small machines, this is not a significant problem but as machine gets
    larger with more cores and more memory, the cost of these IPIs can be
    high.  This patch uses a simple structure that tracks CPUs that
    potentially have TLB entries for pages being unmapped.  When the unmapping
    is complete, the full TLB is flushed on the assumption that a refill cost
    is lower than flushing individual entries.
    
    Architectures wishing to do this must give the following guarantee.
    
            If a clean page is unmapped and not immediately flushed, the
            architecture must guarantee that a write to that linear address
            from a CPU with a cached TLB entry will trap a page fault.
    
    This is essentially what the kernel already depends on but the window is
    much larger with this patch applied and is worth highlighting.  The
    architecture should consider whether the cost of the full TLB flush is
    higher than sending an IPI to flush each individual entry.  An additional
    architecture helper called flush_tlb_local is required.  It's a trivial
    wrapper with some accounting in the x86 case.
    
    The impact of this patch depends on the workload as measuring any benefit
    requires both mapped pages co-located on the LRU and memory pressure.  The
    case with the biggest impact is multiple processes reading mapped pages
    taken from the vm-scalability test suite.  The test case uses NR_CPU
    readers of mapped files that consume 10*RAM.
    
    Linear mapped reader on a 4-node machine with 64G RAM and 48 CPUs
    
                                               4.2.0-rc1          4.2.0-rc1
                                                 vanilla       flushfull-v7
    Ops lru-file-mmap-read-elapsed      159.62 (  0.00%)   120.68 ( 24.40%)
    Ops lru-file-mmap-read-time_range    30.59 (  0.00%)     2.80 ( 90.85%)
    Ops lru-file-mmap-read-time_stddv     6.70 (  0.00%)     0.64 ( 90.38%)
    
               4.2.0-rc1    4.2.0-rc1
                 vanilla flushfull-v7
    User          581.00       611.43
    System       5804.93      4111.76
    Elapsed       161.03       122.12
    
    This is showing that the readers completed 24.40% faster with 29% less
    system CPU time.  From vmstats, it is known that the vanilla kernel was
    interrupted roughly 900K times per second during the steady phase of the
    test and the patched kernel was interrupts 180K times per second.
    
    The impact is lower on a single socket machine.
    
                                               4.2.0-rc1          4.2.0-rc1
                                                 vanilla       flushfull-v7
    Ops lru-file-mmap-read-elapsed       25.33 (  0.00%)    20.38 ( 19.54%)
    Ops lru-file-mmap-read-time_range     0.91 (  0.00%)     1.44 (-58.24%)
    Ops lru-file-mmap-read-time_stddv     0.28 (  0.00%)     0.47 (-65.34%)
    
               4.2.0-rc1    4.2.0-rc1
                 vanilla flushfull-v7
    User           58.09        57.64
    System        111.82        76.56
    Elapsed        27.29        22.55
    
    It's still a noticeable improvement with vmstat showing interrupts went
    from roughly 500K per second to 45K per second.
    
    The patch will have no impact on workloads with no memory pressure or have
    relatively few mapped pages.  It will have an unpredictable impact on the
    workload running on the CPU being flushed as it'll depend on how many TLB
    entries need to be refilled and how long that takes.  Worst case, the TLB
    will be completely cleared of active entries when the target PFNs were not
    resident at all.
    
    [sasha.levin@oracle.com: trace tlb flush after disabling preemption in try_to_unmap_flush]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 119823decc46..3c602c20c717 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1344,6 +1344,18 @@ enum perf_event_task_context {
 	perf_nr_task_contexts,
 };
 
+/* Track pages that require TLB flushes */
+struct tlbflush_unmap_batch {
+	/*
+	 * Each bit set is a CPU that potentially has a TLB entry for one of
+	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().
+	 */
+	struct cpumask cpumask;
+
+	/* True if any bit in cpumask is set */
+	bool flush_required;
+};
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -1700,6 +1712,10 @@ struct task_struct {
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
+	struct tlbflush_unmap_batch tlb_ubc;
+#endif
+
 	struct rcu_head rcu;
 
 	/*

commit a1d8561172f369ba56d636df49a6b4d6d77e2123
Merge: 3959df1dfb95 ff277d4250fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 20:26:22 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest change in this cycle is the rewrite of the main SMP load
      balancing metric: the CPU load/utilization.  The main goal was to make
      the metric more precise and more representative - see the changelog of
      this commit for the gory details:
    
        9d89c257dfb9 ("sched/fair: Rewrite runnable load and utilization average tracking")
    
      It is done in a way that significantly reduces complexity of the code:
    
        5 files changed, 249 insertions(+), 494 deletions(-)
    
      and the performance testing results are encouraging.  Nevertheless we
      need to keep an eye on potential regressions, since this potentially
      affects every SMP workload in existence.
    
      This work comes from Yuyang Du.
    
      Other changes:
    
       - SCHED_DL updates.  (Andrea Parri)
    
       - Simplify architecture callbacks by removing finish_arch_switch().
         (Peter Zijlstra et al)
    
       - cputime accounting: guarantee stime + utime == rtime.  (Peter
         Zijlstra)
    
       - optimize idle CPU wakeups some more - inspired by Facebook server
         loads.  (Mike Galbraith)
    
       - stop_machine fixes and updates.  (Oleg Nesterov)
    
       - Introduce the 'trace_sched_waking' tracepoint.  (Peter Zijlstra)
    
       - sched/numa tweaks.  (Srikar Dronamraju)
    
       - misc fixes and small cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      sched/deadline: Fix comment in enqueue_task_dl()
      sched/deadline: Fix comment in push_dl_tasks()
      sched: Change the sched_class::set_cpus_allowed() calling context
      sched: Make sched_class::set_cpus_allowed() unconditional
      sched: Fix a race between __kthread_bind() and sched_setaffinity()
      sched: Ensure a task has a non-normalized vruntime when returning back to CFS
      sched/numa: Fix NUMA_DIRECT topology identification
      tile: Reorganize _switch_to()
      sched, sparc32: Update scheduler comments in copy_thread()
      sched: Remove finish_arch_switch()
      sched, tile: Remove finish_arch_switch
      sched, sh: Fold finish_arch_switch() into switch_to()
      sched, score: Remove finish_arch_switch()
      sched, avr32: Remove finish_arch_switch()
      sched, MIPS: Get rid of finish_arch_switch()
      sched, arm: Remove finish_arch_switch()
      sched/fair: Clean up load average references
      sched/fair: Provide runnable_load_avg back to cfs_rq
      sched/fair: Remove task and group entity load when they are dead
      sched/fair: Init cfs_rq's sched_entity load average
      ...

commit 25834c73f93af7f0712c98ca4593691592e6b360
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 15 17:43:34 2015 +0200

    sched: Fix a race between __kthread_bind() and sched_setaffinity()
    
    Because sched_setscheduler() checks p->flags & PF_NO_SETAFFINITY
    without locks, a caller might observe an old value and race with the
    set_cpus_allowed_ptr() call from __kthread_bind() and effectively undo
    it:
    
            __kthread_bind()
              do_set_cpus_allowed()
                                                    <SYSCALL>
                                                      sched_setaffinity()
                                                        if (p->flags & PF_NO_SETAFFINITIY)
                                                        set_cpus_allowed_ptr()
              p->flags |= PF_NO_SETAFFINITY
    
    Fix the bug by putting everything under the regular scheduler locks.
    
    This also closes a hole in the serialization of task_struct::{nr_,}cpus_allowed.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dedekind1@gmail.com
    Cc: juri.lelli@arm.com
    Cc: mgorman@suse.de
    Cc: riel@redhat.com
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20150515154833.545640346@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 44dca5b35de6..81bb4577274b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2203,13 +2203,6 @@ static inline void calc_load_enter_idle(void) { }
 static inline void calc_load_exit_idle(void) { }
 #endif /* CONFIG_NO_HZ_COMMON */
 
-#ifndef CONFIG_CPUMASK_OFFSTACK
-static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
-{
-	return set_cpus_allowed_ptr(p, &new_mask);
-}
-#endif
-
 /*
  * Do not use outside of architecture code which knows its limitations.
  *

commit 9d89c257dfb9c51a532d69397f6eed75e5168c35
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:37 2015 +0800

    sched/fair: Rewrite runnable load and utilization average tracking
    
    The idea of runnable load average (let runnable time contribute to weight)
    was proposed by Paul Turner and Ben Segall, and it is still followed by
    this rewrite. This rewrite aims to solve the following issues:
    
    1. cfs_rq's load average (namely runnable_load_avg and blocked_load_avg) is
       updated at the granularity of an entity at a time, which results in the
       cfs_rq's load average is stale or partially updated: at any time, only
       one entity is up to date, all other entities are effectively lagging
       behind. This is undesirable.
    
       To illustrate, if we have n runnable entities in the cfs_rq, as time
       elapses, they certainly become outdated:
    
         t0: cfs_rq { e1_old, e2_old, ..., en_old }
    
       and when we update:
    
         t1: update e1, then we have cfs_rq { e1_new, e2_old, ..., en_old }
    
         t2: update e2, then we have cfs_rq { e1_old, e2_new, ..., en_old }
    
         ...
    
       We solve this by combining all runnable entities' load averages together
       in cfs_rq's avg, and update the cfs_rq's avg as a whole. This is based
       on the fact that if we regard the update as a function, then:
    
       w * update(e) = update(w * e) and
    
       update(e1) + update(e2) = update(e1 + e2), then
    
       w1 * update(e1) + w2 * update(e2) = update(w1 * e1 + w2 * e2)
    
       therefore, by this rewrite, we have an entirely updated cfs_rq at the
       time we update it:
    
         t1: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         t2: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         ...
    
    2. cfs_rq's load average is different between top rq->cfs_rq and other
       task_group's per CPU cfs_rqs in whether or not blocked_load_average
       contributes to the load.
    
       The basic idea behind runnable load average (the same for utilization)
       is that the blocked state is taken into account as opposed to only
       accounting for the currently runnable state. Therefore, the average
       should include both the runnable/running and blocked load averages.
       This rewrite does that.
    
       In addition, we also combine runnable/running and blocked averages
       of all entities into the cfs_rq's average, and update it together at
       once. This is based on the fact that:
    
         update(runnable) + update(blocked) = update(runnable + blocked)
    
       This significantly reduces the code as we don't need to separately
       maintain/update runnable/running load and blocked load.
    
    3. How task_group entities' share is calculated is complex and imprecise.
    
       We reduce the complexity in this rewrite to allow a very simple rule:
       the task_group's load_avg is aggregated from its per CPU cfs_rqs's
       load_avgs. Then group entity's weight is simply proportional to its
       own cfs_rq's load_avg / task_group's load_avg. To illustrate,
    
       if a task_group has { cfs_rq1, cfs_rq2, ..., cfs_rqn }, then,
    
       task_group_avg = cfs_rq1_avg + cfs_rq2_avg + ... + cfs_rqn_avg, then
    
       cfs_rqx's entity's share = cfs_rqx_avg / task_group_avg * task_group's share
    
    To sum up, this rewrite in principle is equivalent to the current one, but
    fixes the issues described above. Turns out, it significantly reduces the
    code complexity and hence increases clarity and efficiency. In addition,
    the new averages are more smooth/continuous (no spurious spikes and valleys)
    and updated more consistently and quickly to reflect the load dynamics.
    
    As a result, we have less load tracking overhead, better performance,
    and especially better power efficiency due to more balanced load.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-3-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c144657aace..44dca5b35de6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1175,29 +1175,24 @@ struct load_weight {
 	u32 inv_weight;
 };
 
+/*
+ * The load_avg/util_avg accumulates an infinite geometric series.
+ * 1) load_avg factors the amount of time that a sched_entity is
+ * runnable on a rq into its weight. For cfs_rq, it is the aggregated
+ * such weights of all runnable and blocked sched_entities.
+ * 2) util_avg factors frequency scaling into the amount of time
+ * that a sched_entity is running on a CPU, in the range [0..SCHED_LOAD_SCALE].
+ * For cfs_rq, it is the aggregated such times of all runnable and
+ * blocked sched_entities.
+ * The 64 bit load_sum can:
+ * 1) for cfs_rq, afford 4353082796 (=2^64/47742/88761) entities with
+ * the highest weight (=88761) always runnable, we should not overflow
+ * 2) for entity, support any load.weight always runnable
+ */
 struct sched_avg {
-	u64 last_runnable_update;
-	s64 decay_count;
-	/*
-	 * utilization_avg_contrib describes the amount of time that a
-	 * sched_entity is running on a CPU. It is based on running_avg_sum
-	 * and is scaled in the range [0..SCHED_LOAD_SCALE].
-	 * load_avg_contrib described the amount of time that a sched_entity
-	 * is runnable on a rq. It is based on both runnable_avg_sum and the
-	 * weight of the task.
-	 */
-	unsigned long load_avg_contrib, utilization_avg_contrib;
-	/*
-	 * These sums represent an infinite geometric series and so are bound
-	 * above by 1024/(1-y).  Thus we only need a u32 to store them for all
-	 * choices of y < 1-2^(-32)*1024.
-	 * running_avg_sum reflects the time that the sched_entity is
-	 * effectively running on the CPU.
-	 * runnable_avg_sum represents the amount of time a sched_entity is on
-	 * a runqueue which includes the running time that is monitored by
-	 * running_avg_sum.
-	 */
-	u32 runnable_avg_sum, avg_period, running_avg_sum;
+	u64 last_update_time, load_sum;
+	u32 util_sum, period_contrib;
+	unsigned long load_avg, util_avg;
 };
 
 #ifdef CONFIG_SCHEDSTATS
@@ -1263,7 +1258,7 @@ struct sched_entity {
 #endif
 
 #ifdef CONFIG_SMP
-	/* Per-entity load-tracking */
+	/* Per entity load average tracking */
 	struct sched_avg	avg;
 #endif
 };

commit fe32d3cd5e8eb0f82e459763374aa80797023403
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed Jul 15 12:52:04 2015 +0300

    sched/preempt: Fix cond_resched_lock() and cond_resched_softirq()
    
    These functions check should_resched() before unlocking spinlock/bh-enable:
    preempt_count always non-zero => should_resched() always returns false.
    cond_resched_lock() worked iff spin_needbreak is set.
    
    This patch adds argument "preempt_offset" to should_resched().
    
    preempt_count offset constants for that:
    
      PREEMPT_DISABLE_OFFSET  - offset after preempt_disable()
      PREEMPT_LOCK_OFFSET     - offset after spin_lock()
      SOFTIRQ_DISABLE_OFFSET  - offset after local_bh_distable()
      SOFTIRQ_LOCK_OFFSET     - offset after spin_lock_bh()
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: bdb438065890 ("sched: Extract the basic add/sub preempt_count modifiers")
    Link: http://lkml.kernel.org/r/20150715095204.12246.98268.stgit@buzz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 65a8a8651596..9c144657aace 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2891,12 +2891,6 @@ extern int _cond_resched(void);
 
 extern int __cond_resched_lock(spinlock_t *lock);
 
-#ifdef CONFIG_PREEMPT_COUNT
-#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
-#else
-#define PREEMPT_LOCK_OFFSET	0
-#endif
-
 #define cond_resched_lock(lock) ({				\
 	___might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\
 	__cond_resched_lock(lock);				\

commit 63b0e9edceec10fa41ec33393a1515a5ff444277
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Tue Jul 14 17:39:50 2015 +0200

    sched/fair: Beef up wake_wide()
    
    Josef Bacik reported that Facebook sees better performance with their
    1:N load (1 dispatch/node, N workers/node) when carrying an old patch
    to try very hard to wake to an idle CPU.  While looking at wake_wide(),
    I noticed that it doesn't pay attention to the wakeup of a many partner
    waker, returning 1 only when waking one of its many partners.
    
    Correct that, letting explicit domain flags override the heuristic.
    
    While at it, adjust task_struct bits, we don't need a 64-bit counter.
    
    Tested-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    [ Tidy things up. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-team<Kernel-team@fb.com>
    Cc: morten.rasmussen@arm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1436888390.7983.49.camel@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7412070a25cc..65a8a8651596 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1359,9 +1359,9 @@ struct task_struct {
 #ifdef CONFIG_SMP
 	struct llist_node wake_entry;
 	int on_cpu;
-	struct task_struct *last_wakee;
-	unsigned long wakee_flips;
+	unsigned int wakee_flips;
 	unsigned long wakee_flip_decay_ts;
+	struct task_struct *last_wakee;
 
 	int wake_cpu;
 #endif

commit 9d7fb04276481c59610983362d8e023d262b58ca
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 30 11:30:54 2015 +0200

    sched/cputime: Guarantee stime + utime == rtime
    
    While the current code guarantees monotonicity for stime and utime
    independently of one another, it does not guarantee that the sum of
    both is equal to the total time we started out with.
    
    This confuses things (and peoples) who look at this sum, like top, and
    will report >100% usage followed by a matching period of 0%.
    
    Rework the code to provide both individual monotonicity and a coherent
    sum.
    
    Suggested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Reported-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Tested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jason.low2@hp.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ae21f1591615..7412070a25cc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -530,39 +530,49 @@ struct cpu_itimer {
 };
 
 /**
- * struct cputime - snaphsot of system and user cputime
+ * struct prev_cputime - snaphsot of system and user cputime
  * @utime: time spent in user mode
  * @stime: time spent in system mode
+ * @lock: protects the above two fields
  *
- * Gathers a generic snapshot of user and system time.
+ * Stores previous user/system time values such that we can guarantee
+ * monotonicity.
  */
-struct cputime {
+struct prev_cputime {
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 	cputime_t utime;
 	cputime_t stime;
+	raw_spinlock_t lock;
+#endif
 };
 
+static inline void prev_cputime_init(struct prev_cputime *prev)
+{
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
+	prev->utime = prev->stime = 0;
+	raw_spin_lock_init(&prev->lock);
+#endif
+}
+
 /**
  * struct task_cputime - collected CPU time counts
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
  *
- * This is an extension of struct cputime that includes the total runtime
- * spent by the task from the scheduler point of view.
- *
- * As a result, this structure groups together three kinds of CPU time
- * that are tracked for threads and thread groups.  Most things considering
- * CPU time want to group these counts together and treat all three
- * of them in parallel.
+ * This structure groups together three kinds of CPU time that are tracked for
+ * threads and thread groups.  Most things considering CPU time want to group
+ * these counts together and treat all three of them in parallel.
  */
 struct task_cputime {
 	cputime_t utime;
 	cputime_t stime;
 	unsigned long long sum_exec_runtime;
 };
+
 /* Alternate field names when used to cache expirations. */
-#define prof_exp	stime
 #define virt_exp	utime
+#define prof_exp	stime
 #define sched_exp	sum_exec_runtime
 
 #define INIT_CPUTIME	\
@@ -715,9 +725,7 @@ struct signal_struct {
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	struct cputime prev_cputime;
-#endif
+	struct prev_cputime prev_cputime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
@@ -1481,9 +1489,7 @@ struct task_struct {
 
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	struct cputime prev_cputime;
-#endif
+	struct prev_cputime prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqlock_t vtime_seqlock;
 	unsigned long long vtime_snap;

commit 5aaeb5c01c5b6c0be7b7aadbf3ace9f3a4458c3d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 17 12:28:12 2015 +0200

    x86/fpu, sched: Introduce CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT and use it on x86
    
    Don't burden architectures without dynamic task_struct sizing
    with the overhead of dynamic sizing.
    
    Also optimize the x86 code a bit by caching task_struct_size.
    
    Acked-and-Tested-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-3-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e43a41d892b6..04b5ada460b4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1786,7 +1786,11 @@ struct task_struct {
  */
 };
 
-extern int arch_task_struct_size(void);
+#ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT
+extern int arch_task_struct_size __read_mostly;
+#else
+# define arch_task_struct_size (sizeof(struct task_struct))
+#endif
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)

commit 0c8c0f03e3a292e031596484275c14cf39c0ab7a
Author: Dave Hansen <dave@sr71.net>
Date:   Fri Jul 17 12:28:11 2015 +0200

    x86/fpu, sched: Dynamically allocate 'struct fpu'
    
    The FPU rewrite removed the dynamic allocations of 'struct fpu'.
    But, this potentially wastes massive amounts of memory (2k per
    task on systems that do not have AVX-512 for instance).
    
    Instead of having a separate slab, this patch just appends the
    space that we need to the 'task_struct' which we dynamically
    allocate already.  This saves from doing an extra slab
    allocation at fork().
    
    The only real downside here is that we have to stick everything
    and the end of the task_struct.  But, I think the
    BUILD_BUG_ON()s I stuck in there should keep that from being too
    fragile.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-2-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ae21f1591615..e43a41d892b6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1522,8 +1522,6 @@ struct task_struct {
 /* hung task detection */
 	unsigned long last_switch_count;
 #endif
-/* CPU-specific state of this task */
-	struct thread_struct thread;
 /* filesystem information */
 	struct fs_struct *fs;
 /* open file information */
@@ -1778,8 +1776,18 @@ struct task_struct {
 	unsigned long	task_state_change;
 #endif
 	int pagefault_disabled;
+/* CPU-specific state of this task */
+	struct thread_struct thread;
+/*
+ * WARNING: on x86, 'thread_struct' contains a variable-sized
+ * structure.  It *MUST* be at the end of 'task_struct'.
+ *
+ * Do not put anything below here!
+ */
 };
 
+extern int arch_task_struct_size(void);
+
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 

commit 22a093b2fb52fb656658a32adc80c24ddc200ca4
Merge: c1776a18e3b5 397f2378f136
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 4 08:56:53 2015 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Debug info and other statistics fixes and related enhancements"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/numa: Fix numa balancing stats in /proc/pid/sched
      sched/numa: Show numa_group ID in /proc/sched_debug task listings
      sched/debug: Move print_cfs_rq() declaration to kernel/sched/sched.h
      sched/stat: Expose /proc/pid/schedstat if CONFIG_SCHED_INFO=y
      sched/stat: Simplify the sched_info accounting dependency

commit 6b55c9654fccf69ae7ace23ca101dc37b903181b
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jun 25 22:51:41 2015 +0530

    sched/debug: Move print_cfs_rq() declaration to kernel/sched/sched.h
    
    Currently print_cfs_rq() is declared in include/linux/sched.h.
    However it's not used outside kernel/sched. Hence move the
    declaration to kernel/sched/sched.h
    
    Also some functions are only available for CONFIG_SCHED_DEBUG=y.
    Hence move the declarations to within the #ifdef.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Iulia Manda <iulia.manda21@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435252903-1081-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9bf4bc0e3b8b..3505352dd296 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -191,8 +191,6 @@ struct task_group;
 #ifdef CONFIG_SCHED_DEBUG
 extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
 extern void proc_sched_set_task(struct task_struct *p);
-extern void
-print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 #endif
 
 /*

commit f6db8347993256b58bd4746b0c4c5b935c32210d
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Jun 25 23:53:37 2015 +0530

    sched/stat: Simplify the sched_info accounting dependency
    
    Both CONFIG_SCHEDSTATS=y and CONFIG_TASK_DELAY_ACCT=y track task
    sched_info, which results in ugly #if clauses.
    
    Simplify the code by introducing a synthethic CONFIG_SCHED_INFO
    switch, selected by both.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: a.p.zijlstra@chello.nl
    Cc: ricklind@us.ibm.com
    Link: http://lkml.kernel.org/r/8d19eef800811a94b0f91bcbeb27430a884d7433.1435255405.git.naveen.n.rao@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6633e83e608a..9bf4bc0e3b8b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -849,7 +849,7 @@ extern struct user_struct root_user;
 struct backing_dev_info;
 struct reclaim_state;
 
-#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+#ifdef CONFIG_SCHED_INFO
 struct sched_info {
 	/* cumulative counters */
 	unsigned long pcount;	      /* # of times run on this cpu */
@@ -859,7 +859,7 @@ struct sched_info {
 	unsigned long long last_arrival,/* when we last ran on a cpu */
 			   last_queued;	/* when we were last queued to run */
 };
-#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
+#endif /* CONFIG_SCHED_INFO */
 
 #ifdef CONFIG_TASK_DELAY_ACCT
 struct task_delay_info {
@@ -1408,7 +1408,7 @@ struct task_struct {
 	int rcu_tasks_idle_cpu;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
-#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+#ifdef CONFIG_SCHED_INFO
 	struct sched_info sched_info;
 #endif
 

commit e22619a29fcdb513b7bc020e84225bb3b5914259
Merge: 78c10e556ed9 b3bddffd35a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 27 13:26:03 2015 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "The main change in this kernel is Casey's generalized LSM stacking
      work, which removes the hard-coding of Capabilities and Yama stacking,
      allowing multiple arbitrary "small" LSMs to be stacked with a default
      monolithic module (e.g.  SELinux, Smack, AppArmor).
    
      See
            https://lwn.net/Articles/636056/
    
      This will allow smaller, simpler LSMs to be incorporated into the
      mainline kernel and arbitrarily stacked by users.  Also, this is a
      useful cleanup of the LSM code in its own right"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (38 commits)
      tpm, tpm_crb: fix le64_to_cpu conversions in crb_acpi_add()
      vTPM: set virtual device before passing to ibmvtpm_reset_crq
      tpm_ibmvtpm: remove unneccessary message level.
      ima: update builtin policies
      ima: extend "mask" policy matching support
      ima: add support for new "euid" policy condition
      ima: fix ima_show_template_data_ascii()
      Smack: freeing an error pointer in smk_write_revoke_subj()
      selinux: fix setting of security labels on NFS
      selinux: Remove unused permission definitions
      selinux: enable genfscon labeling for sysfs and pstore files
      selinux: enable per-file labeling for debugfs files.
      selinux: update netlink socket classes
      signals: don't abuse __flush_signals() in selinux_bprm_committed_creds()
      selinux: Print 'sclass' as string when unrecognized netlink message occurs
      Smack: allow multiple labels in onlycap
      Smack: fix seq operations in smackfs
      ima: pass iint to ima_add_violation()
      ima: wrap event related data to the new ima_event_data structure
      integrity: add validity checks for 'path' parameter
      ...

commit bbe179f88d39274630823a0dc07d2714fd19a103
Merge: 4b703b1d4c46 8a0792ef8e01
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 19:50:04 2015 -0700

    Merge branch 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - threadgroup_lock got reorganized so that its users can pick the
       actual locking mechanism to use.  Its only user - cgroups - is
       updated to use a percpu_rwsem instead of per-process rwsem.
    
       This makes things a bit lighter on hot paths and allows cgroups to
       perform and fail multi-task (a process) migrations atomically.
       Multi-task migrations are used in several places including the
       unified hierarchy.
    
     - Delegation rule and documentation added to unified hierarchy.  This
       will likely be the last interface update from the cgroup core side
       for unified hierarchy before lifting the devel mask.
    
     - Some groundwork for the pids controller which is scheduled to be
       merged in the coming devel cycle.
    
    * 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: add delegation section to unified hierarchy documentation
      cgroup: require write perm on common ancestor when moving processes on the default hierarchy
      cgroup: separate out cgroup_procs_write_permission() from __cgroup_procs_write()
      kernfs: make kernfs_get_inode() public
      MAINTAINERS: add a cgroup core co-maintainer
      cgroup: fix uninitialised iterator in for_each_subsys_which
      cgroup: replace explicit ss_mask checking with for_each_subsys_which
      cgroup: use bitmask to filter for_each_subsys
      cgroup: add seq_file forward declaration for struct cftype
      cgroup: simplify threadgroup locking
      sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
      sched, cgroup: reorganize threadgroup locking
      cgroup: switch to unsigned long for bitmasks
      cgroup: reorganize include/linux/cgroup.h
      cgroup: separate out include/linux/cgroup-defs.h
      cgroup: fix some comment typos

commit 3033f14ab78c326871a4902591c2518410add24a
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Thu Jun 25 15:01:19 2015 -0700

    clone: support passing tls argument via C rather than pt_regs magic
    
    clone has some of the quirkiest syscall handling in the kernel, with a
    pile of special cases, historical curiosities, and architecture-specific
    calling conventions.  In particular, clone with CLONE_SETTLS accepts a
    parameter "tls" that the C entry point completely ignores and some
    assembly entry points overwrite; instead, the low-level arch-specific
    code pulls the tls parameter out of the arch-specific register captured
    as part of pt_regs on entry to the kernel.  That's a massive hack, and
    it makes the arch-specific code only work when called via the specific
    existing syscall entry points; because of this hack, any new clone-like
    system call would have to accept an identical tls argument in exactly
    the same arch-specific position, rather than providing a unified system
    call entry point across architectures.
    
    The first patch allows architectures to handle the tls argument via
    normal C parameter passing, if they opt in by selecting
    HAVE_COPY_THREAD_TLS.  The second patch makes 32-bit and 64-bit x86 opt
    into this.
    
    These two patches came out of the clone4 series, which isn't ready for
    this merge window, but these first two cleanup patches were entirely
    uncontroversial and have acks.  I'd like to go ahead and submit these
    two so that other architectures can begin building on top of this and
    opting into HAVE_COPY_THREAD_TLS.  However, I'm also happy to wait and
    send these through the next merge window (along with v3 of clone4) if
    anyone would prefer that.
    
    This patch (of 2):
    
    clone with CLONE_SETTLS accepts an argument to set the thread-local
    storage area for the new thread.  sys_clone declares an int argument
    tls_val in the appropriate point in the argument list (based on the
    various CLONE_BACKWARDS variants), but doesn't actually use or pass along
    that argument.  Instead, sys_clone calls do_fork, which calls
    copy_process, which calls the arch-specific copy_thread, and copy_thread
    pulls the corresponding syscall argument out of the pt_regs captured at
    kernel entry (knowing what argument of clone that architecture passes tls
    in).
    
    Apart from being awful and inscrutable, that also only works because only
    one code path into copy_thread can pass the CLONE_SETTLS flag, and that
    code path comes from sys_clone with its architecture-specific
    argument-passing order.  This prevents introducing a new version of the
    clone system call without propagating the same architecture-specific
    position of the tls argument.
    
    However, there's no reason to pull the argument out of pt_regs when
    sys_clone could just pass it down via C function call arguments.
    
    Introduce a new CONFIG_HAVE_COPY_THREAD_TLS for architectures to opt into,
    and a new copy_thread_tls that accepts the tls parameter as an additional
    unsigned long (syscall-argument-sized) argument.  Change sys_clone's tls
    argument to an unsigned long (which does not change the ABI), and pass
    that down to copy_thread_tls.
    
    Architectures that don't opt into copy_thread_tls will continue to ignore
    the C argument to sys_clone in favor of the pt_regs captured at kernel
    entry, and thus will be unable to introduce new versions of the clone
    syscall.
    
    Patch co-authored by Josh Triplett and Thiago Macieira.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thiago Macieira <thiago.macieira@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6633e83e608a..93ed0b682adb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2556,8 +2556,22 @@ extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
 
+#ifdef CONFIG_HAVE_COPY_THREAD_TLS
+extern int copy_thread_tls(unsigned long, unsigned long, unsigned long,
+			struct task_struct *, unsigned long);
+#else
 extern int copy_thread(unsigned long, unsigned long, unsigned long,
 			struct task_struct *);
+
+/* Architectures that haven't opted into copy_thread_tls get the tls argument
+ * via pt_regs, so ignore the tls argument passed via C. */
+static inline int copy_thread_tls(
+		unsigned long clone_flags, unsigned long sp, unsigned long arg,
+		struct task_struct *p, unsigned long tls)
+{
+	return copy_thread(clone_flags, sp, arg, p);
+}
+#endif
 extern void flush_thread(void);
 extern void exit_thread(void);
 
@@ -2576,6 +2590,7 @@ extern int do_execveat(int, struct filename *,
 		       const char __user * const __user *,
 		       const char __user * const __user *,
 		       int);
+extern long _do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *, unsigned long);
 extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);

commit 3a95398f54cbd664c749fe9f1bfc7e7dbace92d0
Merge: 43224b96af31 8cb9764fc88b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 19:20:04 2015 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ updates from Thomas Gleixner:
     "A few updates to the nohz infrastructure:
    
       - recursion protection for context tracking
    
       - make the TIF_NOHZ inheritance smarter
    
       - isolate cpus which belong to the NOHZ full set"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      nohz: Set isolcpus when nohz_full is set
      nohz: Add tick_nohz_full_add_cpus_to() API
      context_tracking: Inherit TIF_NOHZ through forks instead of context switches
      context_tracking: Protect against recursion

commit 43224b96af3154cedd7220f7b90094905f07ac78
Merge: d70b3ef54cea 1cb6c2151850
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 18:57:44 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "A rather largish update for everything time and timer related:
    
       - Cache footprint optimizations for both hrtimers and timer wheel
    
       - Lower the NOHZ impact on systems which have NOHZ or timer migration
         disabled at runtime.
    
       - Optimize run time overhead of hrtimer interrupt by making the clock
         offset updates smarter
    
       - hrtimer cleanups and removal of restrictions to tackle some
         problems in sched/perf
    
       - Some more leap second tweaks
    
       - Another round of changes addressing the 2038 problem
    
       - First step to change the internals of clock event devices by
         introducing the necessary infrastructure
    
       - Allow constant folding for usecs/msecs_to_jiffies()
    
       - The usual pile of clockevent/clocksource driver updates
    
      The hrtimer changes contain updates to sched, perf and x86 as they
      depend on them plus changes all over the tree to cleanup API changes
      and redundant code, which got copied all over the place.  The y2038
      changes touch s390 to remove the last non 2038 safe code related to
      boot/persistant clock"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      clocksource: Increase dependencies of timer-stm32 to limit build wreckage
      timer: Minimize nohz off overhead
      timer: Reduce timer migration overhead if disabled
      timer: Stats: Simplify the flags handling
      timer: Replace timer base by a cpu index
      timer: Use hlist for the timer wheel hash buckets
      timer: Remove FIFO "guarantee"
      timers: Sanitize catchup_timer_jiffies() usage
      hrtimer: Allow hrtimer::function() to free the timer
      seqcount: Introduce raw_write_seqcount_barrier()
      seqcount: Rename write_seqcount_barrier()
      hrtimer: Fix hrtimer_is_queued() hole
      hrtimer: Remove HRTIMER_STATE_MIGRATE
      selftest: Timers: Avoid signal deadlock in leap-a-day
      timekeeping: Copy the shadow-timekeeper over the real timekeeper last
      clockevents: Check state instead of mode in suspend/resume path
      selftests: timers: Add leap-second timer edge testing to leap-a-day.c
      ntp: Do leapsecond adjustment in adjtimex read path
      time: Prevent early expiry of hrtimers[CLOCK_REALTIME] at the leap second edge
      ntp: Introduce and use SECS_PER_DAY macro instead of 86400
      ...

commit 23b7776290b10297fe2cae0fb5f166a4f2c68121
Merge: 6bc4c3ad3619 6fab54101923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 15:52:04 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes are:
    
       - lockless wakeup support for futexes and IPC message queues
         (Davidlohr Bueso, Peter Zijlstra)
    
       - Replace spinlocks with atomics in thread_group_cputimer(), to
         improve scalability (Jason Low)
    
       - NUMA balancing improvements (Rik van Riel)
    
       - SCHED_DEADLINE improvements (Wanpeng Li)
    
       - clean up and reorganize preemption helpers (Frederic Weisbecker)
    
       - decouple page fault disabling machinery from the preemption
         counter, to improve debuggability and robustness (David
         Hildenbrand)
    
       - SCHED_DEADLINE documentation updates (Luca Abeni)
    
       - topology CPU masks cleanups (Bartosz Golaszewski)
    
       - /proc/sched_debug improvements (Srikar Dronamraju)"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (79 commits)
      sched/deadline: Remove needless parameter in dl_runtime_exceeded()
      sched: Remove superfluous resetting of the p->dl_throttled flag
      sched/deadline: Drop duplicate init_sched_dl_class() declaration
      sched/deadline: Reduce rq lock contention by eliminating locking of non-feasible target
      sched/deadline: Make init_sched_dl_class() __init
      sched/deadline: Optimize pull_dl_task()
      sched/preempt: Add static_key() to preempt_notifiers
      sched/preempt: Fix preempt notifiers documentation about hlist_del() within unsafe iteration
      sched/stop_machine: Fix deadlock between multiple stop_two_cpus()
      sched/debug: Add sum_sleep_runtime to /proc/<pid>/sched
      sched/debug: Replace vruntime with wait_sum in /proc/sched_debug
      sched/debug: Properly format runnable tasks in /proc/sched_debug
      sched/numa: Only consider less busy nodes as numa balancing destinations
      Revert 095bebf61a46 ("sched/numa: Do not move past the balance point if unbalanced")
      sched/fair: Prevent throttling in early pick_next_task_fair()
      preempt: Reorganize the notrace definitions a bit
      preempt: Use preempt_schedule_context() as the official tracing preemption point
      sched: Make preempt_schedule_context() function-tracing safe
      x86: Remove cpu_sibling_mask() and cpu_core_mask()
      x86: Replace cpu_**_mask() with topology_**_cpumask()
      ...

commit c58267e9fa7b0345dd9006939254701e3622ca6a
Merge: 1bf7067c6e17 a9a3cd900fbb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 15:19:21 2015 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "Kernel side changes mostly consist of work on x86 PMU drivers:
    
       - x86 Intel PT (hardware CPU tracer) improvements (Alexander
         Shishkin)
    
       - x86 Intel CQM (cache quality monitoring) improvements (Thomas
         Gleixner)
    
       - x86 Intel PEBSv3 support (Peter Zijlstra)
    
       - x86 Intel PEBS interrupt batching support for lower overhead
         sampling (Zheng Yan, Kan Liang)
    
       - x86 PMU scheduler fixes and improvements (Peter Zijlstra)
    
      There's too many tooling improvements to list them all - here are a
      few select highlights:
    
      'perf bench':
    
          - Introduce new 'perf bench futex' benchmark: 'wake-parallel', to
            measure parallel waker threads generating contention for kernel
            locks (hb->lock). (Davidlohr Bueso)
    
      'perf top', 'perf report':
    
          - Allow disabling/enabling events dynamicaly in 'perf top':
            a 'perf top' session can instantly become a 'perf report'
            one, i.e. going from dynamic analysis to a static one,
            returning to a dynamic one is possible, to toogle the
            modes, just press 'f' to 'freeze/unfreeze' the sampling. (Arnaldo Carvalho de Melo)
    
          - Make Ctrl-C stop processing on TUI, allowing interrupting the load of big
            perf.data files (Namhyung Kim)
    
      'perf probe': (Masami Hiramatsu)
    
          - Support glob wildcards for function name
          - Support $params special probe argument: Collect all function arguments
          - Make --line checks validate C-style function name.
          - Add --no-inlines option to avoid searching inline functions
          - Greatly speed up 'perf probe --list' by caching debuginfo.
          - Improve --filter support for 'perf probe', allowing using its arguments
            on other commands, as --add, --del, etc.
    
      'perf sched':
    
          - Add option in 'perf sched' to merge like comms to lat output (Josef Bacik)
    
      Plus tons of infrastructure work - in particular preparation for
      upcoming threaded perf report support, but also lots of other work -
      and fixes and other improvements.  See (much) more details in the
      shortlog and in the git log"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (305 commits)
      perf tools: Configurable per thread proc map processing time out
      perf tools: Add time out to force stop proc map processing
      perf report: Fix sort__sym_cmp to also compare end of symbol
      perf hists browser: React to unassigned hotkey pressing
      perf top: Tell the user how to unfreeze events after pressing 'f'
      perf hists browser: Honour the help line provided by builtin-{top,report}.c
      perf hists browser: Do not exit when 'f' is pressed in 'report' mode
      perf top: Replace CTRL+z with 'f' as hotkey for enable/disable events
      perf annotate: Rename source_line_percent to source_line_samples
      perf annotate: Display total number of samples with --show-total-period
      perf tools: Ensure thread-stack is flushed
      perf top: Allow disabling/enabling events dynamicly
      perf evlist: Add toggle_enable() method
      perf trace: Fix race condition at the end of started workloads
      perf probe: Speed up perf probe --list by caching debuginfo
      perf probe: Show usage even if the last event is skipped
      perf tools: Move libtraceevent dynamic list to separated LDFLAGS variable
      perf tools: Fix a problem when opening old perf.data with different byte order
      perf tools: Ignore .config-detected in .gitignore
      perf probe: Fix to return error if no probe is added
      ...

commit 1bf7067c6e173dc10411704db48338ed69c05565
Merge: fc934d40178a 68722101ec3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 14:54:22 2015 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes are:
    
       - 'qspinlock' support, enabled on x86: queued spinlocks - these are
         now the spinlock variant used by x86 as they outperform ticket
         spinlocks in every category.  (Waiman Long)
    
       - 'pvqspinlock' support on x86: paravirtualized variant of queued
         spinlocks.  (Waiman Long, Peter Zijlstra)
    
       - 'qrwlock' support, enabled on x86: queued rwlocks.  Similar to
         queued spinlocks, they are now the variant used by x86:
    
           CONFIG_ARCH_USE_QUEUED_SPINLOCKS=y
           CONFIG_QUEUED_SPINLOCKS=y
           CONFIG_ARCH_USE_QUEUED_RWLOCKS=y
           CONFIG_QUEUED_RWLOCKS=y
    
       - various lockdep fixlets
    
       - various locking primitives cleanups, further WRITE_ONCE()
         propagation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      locking/lockdep: Remove hard coded array size dependency
      locking/qrwlock: Don't contend with readers when setting _QW_WAITING
      lockdep: Do not break user-visible string
      locking/arch: Rename set_mb() to smp_store_mb()
      locking/arch: Add WRITE_ONCE() to set_mb()
      rtmutex: Warn if trylock is called from hard/softirq context
      arch: Remove __ARCH_HAVE_CMPXCHG
      locking/rtmutex: Drop usage of __HAVE_ARCH_CMPXCHG
      locking/qrwlock: Rename QUEUE_RWLOCK to QUEUED_RWLOCKS
      locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS
      locking/pvqspinlock: Replace xchg() by the more descriptive set_mb()
      locking/pvqspinlock, x86: Enable PV qspinlock for Xen
      locking/pvqspinlock, x86: Enable PV qspinlock for KVM
      locking/pvqspinlock, x86: Implement the paravirt qspinlock call patching
      locking/pvqspinlock: Implement simple paravirt support for the qspinlock
      locking/qspinlock: Revert to test-and-set on hypervisors
      locking/qspinlock: Use a simple write to grab the lock
      locking/qspinlock: Optimize for smaller NR_CPUS
      locking/qspinlock: Extract out code snippets for the next patch
      locking/qspinlock: Add pending bit
      ...

commit bc7a34b8b9ebfb0f4b8a35a72a0b134fd6c5ef50
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 26 22:50:33 2015 +0000

    timer: Reduce timer migration overhead if disabled
    
    Eric reported that the timer_migration sysctl is not really nice
    performance wise as it needs to check at every timer insertion whether
    the feature is enabled or not. Further the check does not live in the
    timer code, so we have an extra function call which checks an extra
    cache line to figure out that it is disabled.
    
    We can do better and store that information in the per cpu (hr)timer
    bases. I pondered to use a static key, but that's a nightmare to
    update from the nohz code and the timer base cache line is hot anyway
    when we select a timer base.
    
    The old logic enabled the timer migration unconditionally if
    CONFIG_NO_HZ was set even if nohz was disabled on the kernel command
    line.
    
    With this modification, we start off with migration disabled. The user
    visible sysctl is still set to enabled. If the kernel switches to NOHZ
    migration is enabled, if the user did not disable it via the sysctl
    prior to the switch. If nohz=off is on the kernel command line,
    migration stays disabled no matter what.
    
    Before:
      47.76%  hog       [.] main
      14.84%  [kernel]  [k] _raw_spin_lock_irqsave
       9.55%  [kernel]  [k] _raw_spin_unlock_irqrestore
       6.71%  [kernel]  [k] mod_timer
       6.24%  [kernel]  [k] lock_timer_base.isra.38
       3.76%  [kernel]  [k] detach_if_pending
       3.71%  [kernel]  [k] del_timer
       2.50%  [kernel]  [k] internal_add_timer
       1.51%  [kernel]  [k] get_nohz_timer_target
       1.28%  [kernel]  [k] __internal_add_timer
       0.78%  [kernel]  [k] timerfn
       0.48%  [kernel]  [k] wake_up_nohz_cpu
    
    After:
      48.10%  hog       [.] main
      15.25%  [kernel]  [k] _raw_spin_lock_irqsave
       9.76%  [kernel]  [k] _raw_spin_unlock_irqrestore
       6.50%  [kernel]  [k] mod_timer
       6.44%  [kernel]  [k] lock_timer_base.isra.38
       3.87%  [kernel]  [k] detach_if_pending
       3.80%  [kernel]  [k] del_timer
       2.67%  [kernel]  [k] internal_add_timer
       1.33%  [kernel]  [k] __internal_add_timer
       0.73%  [kernel]  [k] timerfn
       0.54%  [kernel]  [k] wake_up_nohz_cpu
    
    
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Joonwoo Park <joonwoop@codeaurora.org>
    Cc: Wenbo Wang <wenbo.wang@memblaze.com>
    Link: http://lkml.kernel.org/r/20150526224512.127050787@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..d7151460b0cf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -335,14 +335,10 @@ extern int runqueue_is_locked(int cpu);
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void nohz_balance_enter_idle(int cpu);
 extern void set_cpu_sd_state_idle(void);
-extern int get_nohz_timer_target(int pinned);
+extern int get_nohz_timer_target(void);
 #else
 static inline void nohz_balance_enter_idle(int cpu) { }
 static inline void set_cpu_sd_state_idle(void) { }
-static inline int get_nohz_timer_target(int pinned)
-{
-	return smp_processor_id();
-}
 #endif
 
 /*

commit 9e7c8f8c62c1e1cda203b5bfaba4575b141e42e7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 4 16:22:16 2015 -0400

    signals: don't abuse __flush_signals() in selinux_bprm_committed_creds()
    
    selinux_bprm_committed_creds()->__flush_signals() is not right, we
    shouldn't clear TIF_SIGPENDING unconditionally. There can be other
    reasons for signal_pending(): freezing(), JOBCTL_PENDING_MASK, and
    potentially more.
    
    Also change this code to check fatal_signal_pending() rather than
    SIGNAL_GROUP_EXIT, it looks a bit better.
    
    Now we can kill __flush_signals() before it finds another buggy user.
    
    Note: this code looks racy, we can flush a signal which was sent after
    the task SID has been updated.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Paul Moore <pmoore@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8222ae40ecb0..4f84aade8b4d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2373,7 +2373,6 @@ extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);
 extern void flush_signals(struct task_struct *);
-extern void __flush_signals(struct task_struct *);
 extern void ignore_signals(struct task_struct *);
 extern void flush_signal_handlers(struct task_struct *, int force_default);
 extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);

commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5ee290003470..add524a910bd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -743,18 +743,6 @@ struct signal_struct {
 	unsigned audit_tty_log_passwd;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
-#ifdef CONFIG_CGROUPS
-	/*
-	 * group_rwsem prevents new tasks from entering the threadgroup and
-	 * member tasks from exiting,a more specifically, setting of
-	 * PF_EXITING.  fork and exit paths are protected with this rwsem
-	 * using threadgroup_change_begin/end().  Users which require
-	 * threadgroup to remain stable should use threadgroup_[un]lock()
-	 * which also takes care of exec path.  Currently, cgroup is the
-	 * only user.
-	 */
-	struct rw_semaphore group_rwsem;
-#endif
 
 	oom_flags_t oom_flags;
 	short oom_score_adj;		/* OOM kill score adjustment */

commit 7d7efec368d537226142cbe559f45797f18672f9
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:16 2015 -0400

    sched, cgroup: reorganize threadgroup locking
    
    threadgroup_change_begin/end() are used to mark the beginning and end
    of threadgroup modifying operations to allow code paths which require
    a threadgroup to stay stable across blocking operations to synchronize
    against those sections using threadgroup_lock/unlock().
    
    It's currently implemented as a general mechanism in sched.h using
    per-signal_struct rwsem; however, this never grew non-cgroup use cases
    and becomes noop if !CONFIG_CGROUPS.  It turns out that cgroups is
    gonna be better served with a different sycnrhonization scheme and is
    a bit silly to keep cgroups specific details as a general mechanism.
    
    What's general here is identifying the places where threadgroups are
    modified.  This patch restructures threadgroup locking so that
    threadgroup_change_begin/end() become a place where subsystems which
    need to sycnhronize against threadgroup changes can hook into.
    
    cgroup_threadgroup_change_begin/end() which operate on the
    per-signal_struct rwsem are created and threadgroup_lock/unlock() are
    moved to cgroup.c and made static.
    
    This is pure reorganization which doesn't cause any functional
    changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8222ae40ecb0..5ee290003470 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -58,6 +58,7 @@ struct sched_param {
 #include <linux/uidgid.h>
 #include <linux/gfp.h>
 #include <linux/magic.h>
+#include <linux/cgroup-defs.h>
 
 #include <asm/processor.h>
 
@@ -2648,53 +2649,33 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 }
 
-#ifdef CONFIG_CGROUPS
-static inline void threadgroup_change_begin(struct task_struct *tsk)
-{
-	down_read(&tsk->signal->group_rwsem);
-}
-static inline void threadgroup_change_end(struct task_struct *tsk)
-{
-	up_read(&tsk->signal->group_rwsem);
-}
-
 /**
- * threadgroup_lock - lock threadgroup
- * @tsk: member task of the threadgroup to lock
- *
- * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter
- * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or
- * change ->group_leader/pid.  This is useful for cases where the threadgroup
- * needs to stay stable across blockable operations.
+ * threadgroup_change_begin - mark the beginning of changes to a threadgroup
+ * @tsk: task causing the changes
  *
- * fork and exit paths explicitly call threadgroup_change_{begin|end}() for
- * synchronization.  While held, no new task will be added to threadgroup
- * and no existing live task will have its PF_EXITING set.
- *
- * de_thread() does threadgroup_change_{begin|end}() when a non-leader
- * sub-thread becomes a new leader.
+ * All operations which modify a threadgroup - a new thread joining the
+ * group, death of a member thread (the assertion of PF_EXITING) and
+ * exec(2) dethreading the process and replacing the leader - are wrapped
+ * by threadgroup_change_{begin|end}().  This is to provide a place which
+ * subsystems needing threadgroup stability can hook into for
+ * synchronization.
  */
-static inline void threadgroup_lock(struct task_struct *tsk)
+static inline void threadgroup_change_begin(struct task_struct *tsk)
 {
-	down_write(&tsk->signal->group_rwsem);
+	might_sleep();
+	cgroup_threadgroup_change_begin(tsk);
 }
 
 /**
- * threadgroup_unlock - unlock threadgroup
- * @tsk: member task of the threadgroup to unlock
+ * threadgroup_change_end - mark the end of changes to a threadgroup
+ * @tsk: task causing the changes
  *
- * Reverse threadgroup_lock().
+ * See threadgroup_change_begin().
  */
-static inline void threadgroup_unlock(struct task_struct *tsk)
+static inline void threadgroup_change_end(struct task_struct *tsk)
 {
-	up_write(&tsk->signal->group_rwsem);
+	cgroup_threadgroup_change_end(tsk);
 }
-#else
-static inline void threadgroup_change_begin(struct task_struct *tsk) {}
-static inline void threadgroup_change_end(struct task_struct *tsk) {}
-static inline void threadgroup_lock(struct task_struct *tsk) {}
-static inline void threadgroup_unlock(struct task_struct *tsk) {}
-#endif
 
 #ifndef __HAVE_THREAD_FUNCTIONS
 

commit 80ed87c8a9ca0cad7ca66cf3bbdfb17559a66dcf
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 8 14:23:45 2015 +0200

    sched/wait: Introduce TASK_NOLOAD and TASK_IDLE
    
    Currently people use TASK_INTERRUPTIBLE to idle kthreads and wait for
    'work' because TASK_UNINTERRUPTIBLE contributes to the loadavg. Having
    all idle kthreads contribute to the loadavg is somewhat silly.
    
    Now mostly this works OK, because kthreads have all their signals
    masked. However there's a few sites where this is causing problems and
    TASK_UNINTERRUPTIBLE should be used, except for that loadavg issue.
    
    This patch adds TASK_NOLOAD which, when combined with
    TASK_UNINTERRUPTIBLE avoids the loadavg accounting.
    
    As most of imagined usage sites are loops where a thread wants to
    idle, waiting for work, a helper TASK_IDLE is introduced.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Julian Anastasov <ja@ssi.bg>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: NeilBrown <neilb@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dd07ac03f82a..7de815c6fa78 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -218,9 +218,10 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
 #define TASK_PARKED		512
-#define TASK_STATE_MAX		1024
+#define TASK_NOLOAD		1024
+#define TASK_STATE_MAX		2048
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWP"
+#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWPN"
 
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];
@@ -230,6 +231,8 @@ extern char ___assert_task_state[1 - 2*!!(
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
 #define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)
 
+#define TASK_IDLE		(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)
+
 /* Convenience macros for the sake of wake_up */
 #define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
 #define TASK_ALL		(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
@@ -245,7 +248,8 @@ extern char ___assert_task_state[1 - 2*!!(
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
-				 (task->flags & PF_FROZEN) == 0)
+				 (task->flags & PF_FROZEN) == 0 && \
+				 (task->state & TASK_NOLOAD) == 0)
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 

commit 8bcbde5480f9777f8b74d71493722c663e22c21b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 11 17:52:06 2015 +0200

    sched/preempt, mm/fault: Count pagefault_disable() levels in pagefault_disabled
    
    Until now, pagefault_disable()/pagefault_enabled() used the preempt
    count to track whether in an environment with pagefaults disabled (can
    be queried via in_atomic()).
    
    This patch introduces a separate counter in task_struct to count the
    level of pagefault_disable() calls. We'll keep manipulating the preempt
    count to retain compatibility to existing pagefault handlers.
    
    It is now possible to verify whether in a pagefault_disable() envionment
    by calling pagefault_disabled(). In contrast to in_atomic() it will not
    be influenced by preempt_enable()/preempt_disable().
    
    This patch is based on a patch from Ingo Molnar.
    
    Reviewed-and-tested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: borntraeger@de.ibm.com
    Cc: daniel.vetter@intel.com
    Cc: heiko.carstens@de.ibm.com
    Cc: herbert@gondor.apana.org.au
    Cc: hocko@suse.cz
    Cc: hughd@google.com
    Cc: mst@redhat.com
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: schwidefsky@de.ibm.com
    Cc: yang.shi@windriver.com
    Link: http://lkml.kernel.org/r/1431359540-32227-2-git-send-email-dahi@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c53a1784d7a9..dd07ac03f82a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1788,6 +1788,7 @@ struct task_struct {
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	unsigned long	task_state_change;
 #endif
+	int pagefault_disabled;
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 92cf211874e954027b8e91cc9a15485a50b58d6b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue May 12 16:41:46 2015 +0200

    sched/preempt: Merge preempt_mask.h into preempt.h
    
    preempt_mask.h defines all the preempt_count semantics and related
    symbols: preempt, softirq, hardirq, nmi, preempt active, need resched,
    etc...
    
    preempt.h defines the accessors and mutators of preempt_count.
    
    But there is a messy dependency game around those two header files:
    
            * preempt_mask.h includes preempt.h in order to access preempt_count()
    
            * preempt_mask.h defines all preempt_count semantic and symbols
              except PREEMPT_NEED_RESCHED that is needed by asm/preempt.h
              Thus we need to define it from preempt.h, right before including
              asm/preempt.h, instead of defining it to preempt_mask.h with the
              other preempt_count symbols. Therefore the preempt_count semantics
              happen to be spread out.
    
            * We plan to introduce preempt_active_[enter,exit]() to consolidate
              preempt_schedule*() code. But we'll need to access both preempt_count
              mutators (preempt_count_add()) and preempt_count symbols
              (PREEMPT_ACTIVE, PREEMPT_OFFSET). The usual place to define preempt
              operations is in preempt.h but then we'll need symbols in
              preempt_mask.h which already includes preempt.h. So we end up with
              a ressource circle dependency.
    
    Lets merge preempt_mask.h into preempt.h to solve these dependency issues.
    This way we gather semantic symbols and operation definition of
    preempt_count in a single file.
    
    This is a dumb copy-paste merge. Further merge re-arrangments are
    performed in a subsequent patch to ease review.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1431441711-29753-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f8defa155cf..c53a1784d7a9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -25,7 +25,7 @@ struct sched_param {
 #include <linux/errno.h>
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
-#include <linux/preempt_mask.h>
+#include <linux/preempt.h>
 
 #include <asm/page.h>
 #include <asm/ptrace.h>

commit b92b8b35a2e38bde319fd1d68ec84628c1f1b0fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:51:55 2015 +0200

    locking/arch: Rename set_mb() to smp_store_mb()
    
    Since set_mb() is really about an smp_mb() -- not a IO/DMA barrier
    like mb() rename it to match the recent smp_load_acquire() and
    smp_store_release().
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..18f197223ebd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -252,7 +252,7 @@ extern char ___assert_task_state[1 - 2*!!(
 #define set_task_state(tsk, state_value)			\
 	do {							\
 		(tsk)->task_state_change = _THIS_IP_;		\
-		set_mb((tsk)->state, (state_value));		\
+		smp_store_mb((tsk)->state, (state_value));		\
 	} while (0)
 
 /*
@@ -274,7 +274,7 @@ extern char ___assert_task_state[1 - 2*!!(
 #define set_current_state(state_value)				\
 	do {							\
 		current->task_state_change = _THIS_IP_;		\
-		set_mb(current->state, (state_value));		\
+		smp_store_mb(current->state, (state_value));		\
 	} while (0)
 
 #else
@@ -282,7 +282,7 @@ extern char ___assert_task_state[1 - 2*!!(
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
 #define set_task_state(tsk, state_value)		\
-	set_mb((tsk)->state, (state_value))
+	smp_store_mb((tsk)->state, (state_value))
 
 /*
  * set_current_state() includes a barrier so that the write of current->state
@@ -298,7 +298,7 @@ extern char ___assert_task_state[1 - 2*!!(
 #define __set_current_state(state_value)		\
 	do { current->state = (state_value); } while (0)
 #define set_current_state(state_value)			\
-	set_mb(current->state, (state_value))
+	smp_store_mb(current->state, (state_value))
 
 #endif
 

commit 89076bc31950eee576ecc06460c23466e2d50939
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue May 12 08:29:38 2015 -0400

    get rid of assorted nameidata-related debris
    
    pointless forward declarations, stale comments
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f6c9b69d66f2..a1158c954f0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -132,6 +132,7 @@ struct fs_struct;
 struct perf_event_context;
 struct blk_plug;
 struct filename;
+struct nameidata;
 
 #define VMACACHE_BITS 2
 #define VMACACHE_SIZE (1U << VMACACHE_BITS)

commit 8c8a457a60050d5922676f81913d87e4af6fd97b
Author: Nikolay Borisov <n.borisov@siteground.com>
Date:   Thu May 14 14:31:01 2015 +0300

    sched: Remove redundant #ifdef
    
    Two adjacent members in task_struct were guarded
    by the same #define, so we can merge the two blocks.
    
    Signed-off-by: Nikolay Borisov <n.borisov@siteground.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1431603061-29408-1-git-send-email-kernel@kyup.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0eceeec5a01a..5f8defa155cf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1398,8 +1398,6 @@ struct task_struct {
 	int rcu_read_lock_nesting;
 	union rcu_special rcu_read_unlock_special;
 	struct list_head rcu_node_entry;
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
-#ifdef CONFIG_PREEMPT_RCU
 	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TASKS_RCU

commit 756daf263ea53a8bfc89db26cb92e963953253a1
Author: NeilBrown <neilb@suse.de>
Date:   Mon Mar 23 13:37:38 2015 +1100

    VFS: replace {, total_}link_count in task_struct with pointer to nameidata
    
    task_struct currently contains two ad-hoc members for use by the VFS:
    link_count and total_link_count.  These are only interesting to fs/namei.c,
    so exposing them explicitly is poor layering.  Incidentally, link_count
    isn't used anymore, so it can just die.
    
    This patches replaces those with a single pointer to 'struct nameidata'.
    This structure represents the current filename lookup of which
    there can only be one per process, and is a natural place to
    store total_link_count.
    
    This will allow the current "nameidata" argument to all
    follow_link operations to be removed as current->nameidata
    can be used instead in the _very_ few instances that care about
    it at all.
    
    As there are occasional circumstances where pathname lookup can
    recurse, such as through kern_path_locked, we always save and old
    current->nameidata (if there is one) when setting a new value, and
    make sure any active link_counts are preserved.
    
    follow_mount and follow_automount now get a 'struct nameidata *'
    rather than 'int flags' so that they can directly access
    total_link_count, rather than going through 'current'.
    
    Suggested-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..f6c9b69d66f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1461,7 +1461,7 @@ struct task_struct {
 				       it with task_lock())
 				     - initialized normally by setup_new_exec */
 /* file system info */
-	int link_count, total_link_count;
+	struct nameidata *nameidata;
 #ifdef CONFIG_SYSVIPC
 /* ipc stuff */
 	struct sysv_sem sysvsem;

commit 920ce39f6c204d4ce4d8acebe7522f0dfa95f662
Author: Jason Low <jason.low2@hp.com>
Date:   Fri May 8 14:31:50 2015 -0700

    sched, timer: Fix documentation for 'struct thread_group_cputimer'
    
    Fix the docbook build bug reported by Fengguang Wu.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman.Long@hp.com
    Cc: aswin@hp.com
    Cc: bp@alien8.de
    Cc: dave@stgolabs.net
    Cc: fweisbec@gmail.com
    Cc: mgorman@suse.de
    Cc: oleg@redhat.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Cc: rostedt@goodmis.org
    Cc: scott.norton@hp.com
    Cc: torvalds@linux-foundation.org
    Cc: umgwanakikbuti@gmail.com
    Link: http://lkml.kernel.org/r/1431120710.5136.12.camel@j-VirtualBox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 254d88e80f65..0eceeec5a01a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -606,10 +606,9 @@ struct task_cputime_atomic {
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts
- * @cputime:		thread group interval timers.
+ * @cputime_atomic:	atomic thread group interval timers.
  * @running:		non-zero when there are timers running and
  * 			@cputime receives updates.
- * @lock:		lock for fields in this struct.
  *
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU timer calculations.

commit ff303e66c240ba6269e31817a386995440a18c99
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 17 20:05:30 2015 +0200

    perf: Fix software migrate events
    
    Stephane asked about PERF_COUNT_SW_CPU_MIGRATIONS and I realized it
    was borken:
    
     > The problem is that the task isn't actually scheduled while its being
     > migrated (obviously), and if its not scheduled, the counters aren't
     > scheduled either, so there's no observing of the fact.
     >
     > A further problem with migrations is that many migrations happen from
     > softirq context, which is nested inside the 'random' task context of
     > whoemever happens to run at that time, similarly for the wakeup
     > migrations triggered from (soft)irq context. All those end up being
     > accounted in the task that's currently running, eg. your 'ls'.
    
    The below cures this by marking a task as migrated and accounting it
    on the subsequent sched_in().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..2c5e6c3db654 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1356,9 +1356,6 @@ struct task_struct {
 #endif
 
 	struct mm_struct *mm, *active_mm;
-#ifdef CONFIG_COMPAT_BRK
-	unsigned brk_randomized:1;
-#endif
 	/* per-thread vma caching */
 	u32 vmacache_seqnum;
 	struct vm_area_struct *vmacache[VMACACHE_SIZE];
@@ -1381,10 +1378,14 @@ struct task_struct {
 	/* Revert to default priority/policy when forking */
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
+	unsigned sched_migrated:1;
 
 #ifdef CONFIG_MEMCG_KMEM
 	unsigned memcg_kmem_skip_account:1;
 #endif
+#ifdef CONFIG_COMPAT_BRK
+	unsigned brk_randomized:1;
+#endif
 
 	unsigned long atomic_flags; /* Flags needing atomic access. */
 

commit 7675104990ed255b9315a82ae827ff312a2a88a2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 1 08:27:50 2015 -0700

    sched: Implement lockless wake-queues
    
    This is useful for locking primitives that can effect multiple
    wakeups per operation and want to avoid lock internal lock contention
    by delaying the wakeups until we've released the lock internal locks.
    
    Alternatively it can be used to avoid issuing multiple wakeups, and
    thus save a few cycles, in packet processing. Queue all target tasks
    and wakeup once you've processed all packets. That way you avoid
    waking the target task multiple times if there were multiple packets
    for the same task.
    
    Properties of a wake_q are:
    - Lockless, as queue head must reside on the stack.
    - Being a queue, maintains wakeup order passed by the callers. This can
      be important for otherwise, in scenarios where highly contended locks
      could affect any reliance on lock fairness.
    - A queued task cannot be added again until it is woken up.
    
    This patch adds the needed infrastructure into the scheduler code
    and uses the new wake_list to delay the futex wakeups until
    after we've released the hash bucket locks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [tweaks, adjustments, comments, etc.]
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Mason <clm@fb.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: George Spelvin <linux@horizon.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430494072-30283-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4adc536a3b03..254d88e80f65 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -920,6 +920,50 @@ enum cpu_idle_type {
 #define SCHED_CAPACITY_SHIFT	10
 #define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
 
+/*
+ * Wake-queues are lists of tasks with a pending wakeup, whose
+ * callers have already marked the task as woken internally,
+ * and can thus carry on. A common use case is being able to
+ * do the wakeups once the corresponding user lock as been
+ * released.
+ *
+ * We hold reference to each task in the list across the wakeup,
+ * thus guaranteeing that the memory is still valid by the time
+ * the actual wakeups are performed in wake_up_q().
+ *
+ * One per task suffices, because there's never a need for a task to be
+ * in two wake queues simultaneously; it is forbidden to abandon a task
+ * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
+ * already in a wake queue, the wakeup will happen soon and the second
+ * waker can just skip it.
+ *
+ * The WAKE_Q macro declares and initializes the list head.
+ * wake_up_q() does NOT reinitialize the list; it's expected to be
+ * called near the end of a function, where the fact that the queue is
+ * not used again will be easy to see by inspection.
+ *
+ * Note that this can cause spurious wakeups. schedule() callers
+ * must ensure the call is done inside a loop, confirming that the
+ * wakeup condition has in fact occurred.
+ */
+struct wake_q_node {
+	struct wake_q_node *next;
+};
+
+struct wake_q_head {
+	struct wake_q_node *first;
+	struct wake_q_node **lastp;
+};
+
+#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)
+
+#define WAKE_Q(name)					\
+	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
+
+extern void wake_q_add(struct wake_q_head *head,
+		       struct task_struct *task);
+extern void wake_up_q(struct wake_q_head *head);
+
 /*
  * sched-domains (multiprocessor balancing) declarations:
  */
@@ -1532,6 +1576,8 @@ struct task_struct {
 	/* Protection of the PI data structures: */
 	raw_spinlock_t pi_lock;
 
+	struct wake_q_node wake_q;
+
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */
 	struct rb_root pi_waiters;

commit 7110744516276e906f9197e2857d026eb2343393
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:24 2015 -0700

    sched, timer: Use the atomic task_cputime in thread_group_cputimer
    
    Recent optimizations were made to thread_group_cputimer to improve its
    scalability by keeping track of cputime stats without a lock. However,
    the values were open coded to the structure, causing them to be at
    a different abstraction level from the regular task_cputime structure.
    Furthermore, any subsequent similar optimizations would not be able to
    share the new code, since they are specific to thread_group_cputimer.
    
    This patch adds the new task_cputime_atomic data structure (introduced in
    the previous patch in the series) to thread_group_cputimer for keeping
    track of the cputime atomically, which also helps generalize the code.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-6-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6eb78cd45da7..4adc536a3b03 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -615,9 +615,7 @@ struct task_cputime_atomic {
  * used for thread group CPU timer calculations.
  */
 struct thread_group_cputimer {
-	atomic64_t utime;
-	atomic64_t stime;
-	atomic64_t sum_exec_runtime;
+	struct task_cputime_atomic cputime_atomic;
 	int running;
 };
 

commit 971e8a985482c76487edb5a49811e99b96e846e1
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:23 2015 -0700

    sched, timer: Provide an atomic 'struct task_cputime' data structure
    
    This patch adds an atomic variant of the 'struct task_cputime' data structure,
    which can be used to store and update task_cputime statistics without
    needing to do locking.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-5-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a45874c3fab6..6eb78cd45da7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -572,6 +572,23 @@ struct task_cputime {
 		.sum_exec_runtime = 0,				\
 	}
 
+/*
+ * This is the atomic variant of task_cputime, which can be used for
+ * storing and updating task_cputime statistics without locking.
+ */
+struct task_cputime_atomic {
+	atomic64_t utime;
+	atomic64_t stime;
+	atomic64_t sum_exec_runtime;
+};
+
+#define INIT_CPUTIME_ATOMIC \
+	(struct task_cputime_atomic) {				\
+		.utime = ATOMIC64_INIT(0),			\
+		.stime = ATOMIC64_INIT(0),			\
+		.sum_exec_runtime = ATOMIC64_INIT(0),		\
+	}
+
 #ifdef CONFIG_PREEMPT_COUNT
 #define PREEMPT_DISABLED	(1 + PREEMPT_ENABLED)
 #else

commit 1018016c706f7ff9f56fde3a649789c47085a293
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:22 2015 -0700

    sched, timer: Replace spinlocks with atomics in thread_group_cputimer(), to improve scalability
    
    While running a database workload, we found a scalability issue with itimers.
    
    Much of the problem was caused by the thread_group_cputimer spinlock.
    Each time we account for group system/user time, we need to obtain a
    thread_group_cputimer's spinlock to update the timers. On larger systems
    (such as a 16 socket machine), this caused more than 30% of total time
    spent trying to obtain this kernel lock to update these group timer stats.
    
    This patch converts the timers to 64-bit atomic variables and use
    atomic add to update them without a lock. With this patch, the percent
    of total time spent updating thread group cputimer timers was reduced
    from 30% down to less than 1%.
    
    Note: On 32-bit systems using the generic 64-bit atomics, this causes
    sample_group_cputimer() to take locks 3 times instead of just 1 time.
    However, we tested this patch on a 32-bit system ARM system using the
    generic atomics and did not find the overhead to be much of an issue.
    An explanation for why this isn't an issue is that 32-bit systems usually
    have small numbers of CPUs, and cacheline contention from extra spinlocks
    called periodically is not really apparent on smaller systems.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d70910355b20..a45874c3fab6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -598,9 +598,10 @@ struct task_cputime {
  * used for thread group CPU timer calculations.
  */
 struct thread_group_cputimer {
-	struct task_cputime cputime;
+	atomic64_t utime;
+	atomic64_t stime;
+	atomic64_t sum_exec_runtime;
 	int running;
-	raw_spinlock_t lock;
 };
 
 #include <linux/rwsem.h>
@@ -2967,11 +2968,6 @@ static __always_inline bool need_resched(void)
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
-static inline void thread_group_cputime_init(struct signal_struct *sig)
-{
-	raw_spin_lock_init(&sig->cputimer.lock);
-}
-
 /*
  * Reevaluate whether the task has signals pending delivery.
  * Wake the task if so.

commit 316c1608d15c736439d4065ed12f306db554b3da
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:20 2015 -0700

    sched, timer: Convert usages of ACCESS_ONCE() in the scheduler to READ_ONCE()/WRITE_ONCE()
    
    ACCESS_ONCE doesn't work reliably on non-scalar types. This patch removes
    the rest of the existing usages of ACCESS_ONCE() in the scheduler, and use
    the new READ_ONCE() and WRITE_ONCE() APIs as appropriate.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430251224-5764-2-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fb650a2f4a73..d70910355b20 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3085,13 +3085,13 @@ static inline void mm_update_next_owner(struct mm_struct *mm)
 static inline unsigned long task_rlimit(const struct task_struct *tsk,
 		unsigned int limit)
 {
-	return ACCESS_ONCE(tsk->signal->rlim[limit].rlim_cur);
+	return READ_ONCE(tsk->signal->rlim[limit].rlim_cur);
 }
 
 static inline unsigned long task_rlimit_max(const struct task_struct *tsk,
 		unsigned int limit)
 {
-	return ACCESS_ONCE(tsk->signal->rlim[limit].rlim_max);
+	return READ_ONCE(tsk->signal->rlim[limit].rlim_max);
 }
 
 static inline unsigned long rlimit(unsigned int limit)

commit e7cc4173115347bcdaa5de2824dd46ef2c58425f
Author: Palmer Dabbelt <palmer@dabbelt.com>
Date:   Thu Apr 30 21:19:55 2015 -0700

    signals, ptrace, sched: Fix a misaligned load inside ptrace_attach()
    
    The misaligned load exception arises when running ptrace_attach() on
    the RISC-V (which hasn't been upstreamed yet).  The problem is that
    wait_on_bit() takes a void* but then proceeds to call test_bit(),
    which takes a long*.  This allows an int-aligned pointer to be passed
    to test_bit(), which promptly fails.  This will manifest on any other
    asm-generic port where unaligned loads trap, where sizeof(long) >
    sizeof(int), and where task_struct.jobctl ends up not being
    long-aligned.
    
    This patch changes task_struct.jobctl to be a long, which ensures it
    has the correct alignment.
    
    Signed-off-by: Palmer Dabbelt <palmer@dabbelt.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bobby.prani@gmail.com
    Cc: oleg@redhat.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: richard@nod.at
    Cc: vdavydov@parallels.com
    Link: http://lkml.kernel.org/r/1430453997-32459-2-git-send-email-palmer@dabbelt.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4f066cb625ad..fb650a2f4a73 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1374,7 +1374,7 @@ struct task_struct {
 	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
-	unsigned int jobctl;	/* JOBCTL_*, siglock protected */
+	unsigned long jobctl;	/* JOBCTL_*, siglock protected */
 
 	/* Used for emulating ABI behavior of previous Linux versions */
 	unsigned int personality;

commit b76808e6808e34e7e78131d2b8cb0535622b8e9f
Author: Palmer Dabbelt <palmer@dabbelt.com>
Date:   Thu Apr 30 21:19:57 2015 -0700

    signals, sched: Change all uses of JOBCTL_* from 'int' to 'long'
    
    c56fb6564dcd ("Fix a misaligned load inside ptrace_attach()") makes
    jobctl an "unsigned long".  It makes sense to have the masks applied
    to it match that type.  This is currently just a cosmetic change, but
    it will prevent the mask from being unexpectedly truncated if we ever
    end up with masks with more bits.
    
    One instance of "signr" is an int, but I left this alone because the
    mask ensures that it will never overflow.
    
    Signed-off-by: Palmer Dabbelt <palmer@dabbelt.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bobby.prani@gmail.com
    Cc: oleg@redhat.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: richard@nod.at
    Cc: vdavydov@parallels.com
    Link: http://lkml.kernel.org/r/1430453997-32459-4-git-send-email-palmer@dabbelt.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 85cf253bc366..4f066cb625ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2082,22 +2082,22 @@ TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
 #define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 #define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
 
-#define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
-#define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
-#define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
-#define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
-#define JOBCTL_TRAP_NOTIFY	(1 << JOBCTL_TRAP_NOTIFY_BIT)
-#define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
-#define JOBCTL_LISTENING	(1 << JOBCTL_LISTENING_BIT)
+#define JOBCTL_STOP_DEQUEUED	(1UL << JOBCTL_STOP_DEQUEUED_BIT)
+#define JOBCTL_STOP_PENDING	(1UL << JOBCTL_STOP_PENDING_BIT)
+#define JOBCTL_STOP_CONSUME	(1UL << JOBCTL_STOP_CONSUME_BIT)
+#define JOBCTL_TRAP_STOP	(1UL << JOBCTL_TRAP_STOP_BIT)
+#define JOBCTL_TRAP_NOTIFY	(1UL << JOBCTL_TRAP_NOTIFY_BIT)
+#define JOBCTL_TRAPPING		(1UL << JOBCTL_TRAPPING_BIT)
+#define JOBCTL_LISTENING	(1UL << JOBCTL_LISTENING_BIT)
 
 #define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 #define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 
 extern bool task_set_jobctl_pending(struct task_struct *task,
-				    unsigned int mask);
+				    unsigned long mask);
 extern void task_clear_jobctl_trapping(struct task_struct *task);
 extern void task_clear_jobctl_pending(struct task_struct *task,
-				      unsigned int mask);
+				      unsigned long mask);
 
 static inline void rcu_copy_process(struct task_struct *p)
 {

commit 3289bdb429884c0279bf9ab72dff7b934f19dfc6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Apr 14 13:19:42 2015 +0200

    sched: Move the loadavg code to a more obvious location
    
    I could not find the loadavg code.. turns out it was hidden in a file
    called proc.c. It further got mingled up with the cruft per rq load
    indexes (which we really want to get rid of).
    
    Move the per rq load indexes into the fair.c load-balance code (that's
    the only thing that uses them) and rename proc.c to loadavg.c so we
    can find it again.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [ Did minor cleanups to the code. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..85cf253bc366 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -173,7 +173,12 @@ extern unsigned long nr_iowait_cpu(int cpu);
 extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 
 extern void calc_global_load(unsigned long ticks);
+
+#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void update_cpu_load_nohz(void);
+#else
+static inline void update_cpu_load_nohz(void) { }
+#endif
 
 extern unsigned long get_parent_ip(unsigned long addr);
 

commit fafe870f31212a72f3c2d74e7b90e4ef39e83ee1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed May 6 18:04:24 2015 +0200

    context_tracking: Inherit TIF_NOHZ through forks instead of context switches
    
    TIF_NOHZ is used by context_tracking to force syscall slow-path
    on every task in order to track userspace roundtrips. As such,
    it must be set on all running tasks.
    
    It's currently explicitly inherited through context switches.
    There is no need to do it in this fast-path though. The flag
    could simply be set once for all on all tasks, whether they are
    running or not.
    
    Lets do this by setting the flag for the init task on early boot,
    and let it propagate through fork inheritance.
    
    While at it, mark context_tracking_cpu_set() as init code, we
    only need it at early boot time.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1430928266-24888-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26a2e6122734..185a750e4ed4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2532,6 +2532,9 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 }
 #endif
 
+#define tasklist_empty() \
+	list_empty(&init_task.tasks)
+
 #define next_task(p) \
 	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
 

commit 73459e2a1ada09a68c02cc5b73f3116fc8194b3d
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 23 13:20:18 2015 +0200

    x86: pvclock: Really remove the sched notifier for cross-cpu migrations
    
    This reverts commits 0a4e6be9ca17c54817cf814b4b5aa60478c6df27
    and 80f7fdb1c7f0f9266421f823964fd1962681f6ce.
    
    The task migration notifier was originally introduced in order to support
    the pvclock vsyscall with non-synchronized TSC, but KVM only supports it
    with synchronized TSC.  Hence, on KVM the race condition is only needed
    due to a bad implementation on the host side, and even then it's so rare
    that it's mostly theoretical.
    
    As far as KVM is concerned it's possible to fix the host, avoiding the
    additional complexity in the vDSO and the (re)introduction of the task
    migration notifier.
    
    Xen, on the other hand, hasn't yet implemented vsyscall support at
    all, so we do not care about its plans for non-synchronized TSC.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8222ae40ecb0..26a2e6122734 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -175,14 +175,6 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);
 
-/* Notifier for when a task gets migrated to a new CPU */
-struct task_migration_notifier {
-	struct task_struct *task;
-	int from_cpu;
-	int to_cpu;
-};
-extern void register_task_migration_notifier(struct notifier_block *n);
-
 extern unsigned long get_parent_ip(unsigned long addr);
 
 extern void dump_cpu_task(int cpu);

commit fa2e5c073a355465a2a8c9a2fbecf404f9857c3a
Merge: e44740c1a94b 97b2f0dc3314
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 15 13:53:55 2015 -0700

    Merge branch 'exec_domain_rip_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc
    
    Pull exec domain removal from Richard Weinberger:
     "This series removes execution domain support from Linux.
    
      The idea behind exec domains was to support different ABIs.  The
      feature was never complete nor stable.  Let's rip it out and make the
      kernel signal handling code less complicated"
    
    * 'exec_domain_rip_v2' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc: (27 commits)
      arm64: Removed unused variable
      sparc: Fix execution domain removal
      Remove rest of exec domains.
      arch: Remove exec_domain from remaining archs
      arc: Remove signal translation and exec_domain
      xtensa: Remove signal translation and exec_domain
      xtensa: Autogenerate offsets in struct thread_info
      x86: Remove signal translation and exec_domain
      unicore32: Remove signal translation and exec_domain
      um: Remove signal translation and exec_domain
      tile: Remove signal translation and exec_domain
      sparc: Remove signal translation and exec_domain
      sh: Remove signal translation and exec_domain
      s390: Remove signal translation and exec_domain
      mn10300: Remove signal translation and exec_domain
      microblaze: Remove signal translation and exec_domain
      m68k: Remove signal translation and exec_domain
      m32r: Remove signal translation and exec_domain
      m32r: Autogenerate offsets in struct thread_info
      frv: Remove signal translation and exec_domain
      ...

commit 4fd48b45ffc4addd3c2963448b05417aa14abbf7
Merge: a1480a166dd5 34ebe933417e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 16:47:11 2015 -0700

    Merge branch 'for-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Nothing too interesting.  Rik made cpuset cooperate better with
      isolcpus and there are several other cleanup patches"
    
    * 'for-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cpuset, isolcpus: document relationship between cpusets & isolcpus
      cpusets, isolcpus: exclude isolcpus from load balancing in cpusets
      sched, isolcpu: make cpu_isolated_map visible outside scheduler
      cpuset: initialize cpuset a bit early
      cgroup: Use kvfree in pidlist_free()
      cgroup: call cgroup_subsys->bind on cgroup subsys initialization

commit 49d2953c72c64182ef2dcac64f6979c0b4e25db7
Merge: cc76ee75a9d3 62a935b256f6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 10:47:34 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Major changes:
    
       - Reworked CPU capacity code, for better SMP load balancing on
         systems with assymetric CPUs. (Vincent Guittot, Morten Rasmussen)
    
       - Reworked RT task SMP balancing to be push based instead of pull
         based, to reduce latencies on large CPU count systems. (Steven
         Rostedt)
    
       - SCHED_DEADLINE support updates and fixes. (Juri Lelli)
    
       - SCHED_DEADLINE task migration support during CPU hotplug. (Wanpeng Li)
    
       - x86 mwait-idle optimizations and fixes. (Mike Galbraith, Len Brown)
    
       - sched/numa improvements. (Rik van Riel)
    
       - various cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      sched/core: Drop debugging leftover trace_printk call
      sched/deadline: Support DL task migration during CPU hotplug
      sched/core: Check for available DL bandwidth in cpuset_cpu_inactive()
      sched/deadline: Always enqueue on previous rq when dl_task_timer() fires
      sched/core: Remove unused argument from init_[rt|dl]_rq()
      sched/deadline: Fix rt runtime corruption when dl fails its global constraints
      sched/deadline: Avoid a superfluous check
      sched: Improve load balancing in the presence of idle CPUs
      sched: Optimize freq invariant accounting
      sched: Move CFS tasks to CPUs with higher capacity
      sched: Add SD_PREFER_SIBLING for SMT level
      sched: Remove unused struct sched_group_capacity::capacity_orig
      sched: Replace capacity_factor by usage
      sched: Calculate CPU's usage statistic and put it into struct sg_lb_stats::group_usage
      sched: Add struct rq::cpu_capacity_orig
      sched: Make scale_rt invariant with frequency
      sched: Make sched entity usage tracking scale-invariant
      sched: Remove frequency scaling from cpu_capacity
      sched: Track group sched_entity usage contributions
      sched: Add sched_avg::utilization_avg_contrib
      ...

commit 900360131066f192c82311a098d03d6ac6429e20
Merge: 4541fec3104b ca3f0874723f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 09:47:01 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "First batch of KVM changes for 4.1
    
      The most interesting bit here is irqfd/ioeventfd support for ARM and
      ARM64.
    
      Summary:
    
      ARM/ARM64:
         fixes for live migration, irqfd and ioeventfd support (enabling
         vhost, too), page aging
    
      s390:
         interrupt handling rework, allowing to inject all local interrupts
         via new ioctl and to get/set the full local irq state for migration
         and introspection.  New ioctls to access memory by virtual address,
         and to get/set the guest storage keys.  SIMD support.
    
      MIPS:
         FPU and MIPS SIMD Architecture (MSA) support.  Includes some
         patches from Ralf Baechle's MIPS tree.
    
      x86:
         bugfixes (notably for pvclock, the others are small) and cleanups.
         Another small latency improvement for the TSC deadline timer"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      KVM: use slowpath for cross page cached accesses
      kvm: mmu: lazy collapse small sptes into large sptes
      KVM: x86: Clear CR2 on VCPU reset
      KVM: x86: DR0-DR3 are not clear on reset
      KVM: x86: BSP in MSR_IA32_APICBASE is writable
      KVM: x86: simplify kvm_apic_map
      KVM: x86: avoid logical_map when it is invalid
      KVM: x86: fix mixed APIC mode broadcast
      KVM: x86: use MDA for interrupt matching
      kvm/ppc/mpic: drop unused IRQ_testbit
      KVM: nVMX: remove unnecessary double caching of MAXPHYADDR
      KVM: nVMX: checks for address bits beyond MAXPHYADDR on VM-entry
      KVM: x86: cache maxphyaddr CPUID leaf in struct kvm_vcpu
      KVM: vmx: pass error code with internal error #2
      x86: vdso: fix pvclock races with task migration
      KVM: remove kvm_read_hva and kvm_read_hva_atomic
      KVM: x86: optimize delivery of TSC deadline timer interrupt
      KVM: x86: extract blocking logic from __vcpu_run
      kvm: x86: fix x86 eflags fixed bit
      KVM: s390: migrate vcpu interrupt state
      ...

commit 9058f3b326dbe8cd2ebea7f3cfe367b0d101039b
Author: Richard Weinberger <richard@nod.at>
Date:   Sat Apr 11 21:45:22 2015 +0200

    Remove rest of exec domains.
    
    It is gone from all archs, now we can remove
    the final bits.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a419b65770d6..14d9117ac463 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -125,7 +125,6 @@ struct sched_attr {
 	u64 sched_period;
 };
 
-struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
 struct bio_list;
@@ -2288,11 +2287,6 @@ extern void set_curr_task(int cpu, struct task_struct *p);
 
 void yield(void);
 
-/*
- * The default (Linux) execution domain.
- */
-extern struct exec_domain	default_exec_domain;
-
 union thread_union {
 	struct thread_info thread_info;
 	unsigned long stack[THREAD_SIZE/sizeof(long)];

commit 36ee28e45df50c2c8624b978335516e42d84ae1f
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Feb 27 16:54:04 2015 +0100

    sched: Add sched_avg::utilization_avg_contrib
    
    Add new statistics which reflect the average time a task is running on the CPU
    and the sum of these running time of the tasks on a runqueue. The latter is
    named utilization_load_avg.
    
    This patch is based on the usage metric that was proposed in the 1st
    versions of the per-entity load tracking patchset by Paul Turner
    <pjt@google.com> but that has be removed afterwards. This version differs from
    the original one in the sense that it's not linked to task_group.
    
    The rq's utilization_load_avg will be used to check if a rq is overloaded or
    not instead of trying to compute how many tasks a group of CPUs can handle.
    
    Rename runnable_avg_period into avg_period as it is now used with both
    runnable_avg_sum and running_avg_sum.
    
    Add some descriptions of the variables to explain their differences.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: Paul Turner <pjt@google.com>
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d77432e14ff..fdca05c5f812 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1115,15 +1115,28 @@ struct load_weight {
 };
 
 struct sched_avg {
+	u64 last_runnable_update;
+	s64 decay_count;
+	/*
+	 * utilization_avg_contrib describes the amount of time that a
+	 * sched_entity is running on a CPU. It is based on running_avg_sum
+	 * and is scaled in the range [0..SCHED_LOAD_SCALE].
+	 * load_avg_contrib described the amount of time that a sched_entity
+	 * is runnable on a rq. It is based on both runnable_avg_sum and the
+	 * weight of the task.
+	 */
+	unsigned long load_avg_contrib, utilization_avg_contrib;
 	/*
 	 * These sums represent an infinite geometric series and so are bound
 	 * above by 1024/(1-y).  Thus we only need a u32 to store them for all
 	 * choices of y < 1-2^(-32)*1024.
+	 * running_avg_sum reflects the time that the sched_entity is
+	 * effectively running on the CPU.
+	 * runnable_avg_sum represents the amount of time a sched_entity is on
+	 * a runqueue which includes the running time that is monitored by
+	 * running_avg_sum.
 	 */
-	u32 runnable_avg_sum, runnable_avg_period;
-	u64 last_runnable_update;
-	s64 decay_count;
-	unsigned long load_avg_contrib;
+	u32 runnable_avg_sum, avg_period, running_avg_sum;
 };
 
 #ifdef CONFIG_SCHEDSTATS

commit 074c238177a75f5e79af3b2cb6a84e54823ef950
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Mar 25 15:55:42 2015 -0700

    mm: numa: slow PTE scan rate if migration failures occur
    
    Dave Chinner reported the following on https://lkml.org/lkml/2015/3/1/226
    
      Across the board the 4.0-rc1 numbers are much slower, and the degradation
      is far worse when using the large memory footprint configs. Perf points
      straight at the cause - this is from 4.0-rc1 on the "-o bhash=101073" config:
    
       -   56.07%    56.07%  [kernel]            [k] default_send_IPI_mask_sequence_phys
          - default_send_IPI_mask_sequence_phys
             - 99.99% physflat_send_IPI_mask
                - 99.37% native_send_call_func_ipi
                     smp_call_function_many
                   - native_flush_tlb_others
                      - 99.85% flush_tlb_page
                           ptep_clear_flush
                           try_to_unmap_one
                           rmap_walk
                           try_to_unmap
                           migrate_pages
                           migrate_misplaced_page
                         - handle_mm_fault
                            - 99.73% __do_page_fault
                                 trace_do_page_fault
                                 do_async_page_fault
                               + async_page_fault
                  0.63% native_send_call_func_single_ipi
                     generic_exec_single
                     smp_call_function_single
    
    This is showing excessive migration activity even though excessive
    migrations are meant to get throttled.  Normally, the scan rate is tuned
    on a per-task basis depending on the locality of faults.  However, if
    migrations fail for any reason then the PTE scanner may scan faster if
    the faults continue to be remote.  This means there is higher system CPU
    overhead and fault trapping at exactly the time we know that migrations
    cannot happen.  This patch tracks when migration failures occur and
    slows the PTE scanner.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reported-by: Dave Chinner <david@fromorbit.com>
    Tested-by: Dave Chinner <david@fromorbit.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d77432e14ff..a419b65770d6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1625,11 +1625,11 @@ struct task_struct {
 
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last
-	 * scan window were remote/local. The task scan period is adapted
-	 * based on the locality of the faults with different weights
-	 * depending on whether they were shared or private faults
+	 * scan window were remote/local or failed to migrate. The task scan
+	 * period is adapted based on the locality of the faults with different
+	 * weights depending on whether they were shared or private faults
 	 */
-	unsigned long numa_faults_locality[2];
+	unsigned long numa_faults_locality[3];
 
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
@@ -1719,6 +1719,7 @@ struct task_struct {
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04
 #define TNF_FAULT_LOCAL	0x08
+#define TNF_MIGRATE_FAIL 0x10
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);

commit 0a4e6be9ca17c54817cf814b4b5aa60478c6df27
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Mar 23 20:21:51 2015 -0300

    x86: kvm: Revert "remove sched notifier for cross-cpu migrations"
    
    The following point:
    
        2. per-CPU pvclock time info is updated if the
           underlying CPU changes.
    
    Is not true anymore since "KVM: x86: update pvclock area conditionally,
    on cpu migration".
    
    Add task migration notification back.
    
    Problem noticed by Andy Lutomirski.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    CC: stable@kernel.org # 3.11+

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d77432e14ff..be98910cc1e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -176,6 +176,14 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);
 
+/* Notifier for when a task gets migrated to a new CPU */
+struct task_migration_notifier {
+	struct task_struct *task;
+	int from_cpu;
+	int to_cpu;
+};
+extern void register_task_migration_notifier(struct notifier_block *n);
+
 extern unsigned long get_parent_ip(unsigned long addr);
 
 extern void dump_cpu_task(int cpu);

commit 3fa0818b3c85e9bb55e3ac96c9523b87e44eab9e
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Mar 9 12:12:07 2015 -0400

    sched, isolcpu: make cpu_isolated_map visible outside scheduler
    
    Needed by the next patch. Also makes cpu_isolated_map present
    when compiled without SMP and/or with CONFIG_NR_CPUS=1, like
    the other cpu masks.
    
    At some point we may want to clean things up so cpumasks do
    not exist in UP kernels. Maybe something for the CONFIG_TINY
    crowd.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: cgroups@vger.kernel.org
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d77432e14ff..ca365d79480c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -329,6 +329,8 @@ extern asmlinkage void schedule_tail(struct task_struct *prev);
 extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 
+extern cpumask_var_t cpu_isolated_map;
+
 extern int runqueue_is_locked(int cpu);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)

commit e2defd02717ebc54ae2f4862271a3093665b426a
Merge: b5aeca54d021 2636ed5f8d15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 21 10:40:02 2015 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Thiscontains misc fixes: preempt_schedule_common() and io_schedule()
      recursion fixes, sched/dl fixes, a completion_done() revert, two
      sched/rt fixes and a comment update patch"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/rt: Avoid obvious configuration fail
      sched/autogroup: Fix failure to set cpu.rt_runtime_us
      sched/dl: Do update_rq_clock() in yield_task_dl()
      sched: Prevent recursion in io_schedule()
      sched/completion: Serialize completion_done() with complete()
      sched: Fix preempt_schedule_common() triggering tracing recursion
      sched/dl: Prevent enqueue of a sleeping task in dl_task_timer()
      sched: Make dl_task_time() use task_rq_lock()
      sched: Clarify ordering between task_rq_lock() and move_queued_task()

commit 9cff8adeaa34b5d2802f03f89803da57856b3b72
Author: NeilBrown <neilb@suse.de>
Date:   Fri Feb 13 15:49:17 2015 +1100

    sched: Prevent recursion in io_schedule()
    
    io_schedule() calls blk_flush_plug() which, depending on the
    contents of current->plug, can initiate arbitrary blk-io requests.
    
    Note that this contrasts with blk_schedule_flush_plug() which requires
    all non-trivial work to be handed off to a separate thread.
    
    This makes it possible for io_schedule() to recurse, and initiating
    block requests could possibly call mempool_alloc() which, in times of
    memory pressure, uses io_schedule().
    
    Apart from any stack usage issues, io_schedule() will not behave
    correctly when called recursively as delayacct_blkio_start() does
    not allow for repeated calls.
    
    So:
     - use ->in_iowait to detect recursion.  Set it earlier, and restore
       it to the old value.
     - move the call to "raw_rq" after the call to blk_flush_plug().
       As this is some sort of per-cpu thing, we want some chance that
       we are on the right CPU
     - When io_schedule() is called recurively, use blk_schedule_flush_plug()
       which cannot further recurse.
     - as this makes io_schedule() a lot more complex and as io_schedule()
       must match io_schedule_timeout(), but all the changes in io_schedule_timeout()
       and make io_schedule a simple wrapper for that.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ Moved the now rudimentary io_schedule() into sched.h. ]
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Tony Battersby <tonyb@cybernetics.com>
    Link: http://lkml.kernel.org/r/20150213162600.059fffb2@notabene.brown
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8db31ef98d2f..cb5cdc777c8a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -363,9 +363,6 @@ extern void show_regs(struct pt_regs *);
  */
 extern void show_stack(struct task_struct *task, unsigned long *sp);
 
-void io_schedule(void);
-long io_schedule_timeout(long timeout);
-
 extern void cpu_init (void);
 extern void trap_init(void);
 extern void update_process_times(int user);
@@ -422,6 +419,13 @@ extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
 
+extern long io_schedule_timeout(long timeout);
+
+static inline void io_schedule(void)
+{
+	io_schedule_timeout(MAX_SCHEDULE_TIMEOUT);
+}
+
 struct nsproxy;
 struct user_namespace;
 

commit 0b24becc810dc3be6e3f94103a866f214c282394
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:39:17 2015 -0800

    kasan: add kernel address sanitizer infrastructure
    
    Kernel Address sanitizer (KASan) is a dynamic memory error detector.  It
    provides fast and comprehensive solution for finding use-after-free and
    out-of-bounds bugs.
    
    KASAN uses compile-time instrumentation for checking every memory access,
    therefore GCC > v4.9.2 required.  v4.9.2 almost works, but has issues with
    putting symbol aliases into the wrong section, which breaks kasan
    instrumentation of globals.
    
    This patch only adds infrastructure for kernel address sanitizer.  It's
    not available for use yet.  The idea and some code was borrowed from [1].
    
    Basic idea:
    
    The main idea of KASAN is to use shadow memory to record whether each byte
    of memory is safe to access or not, and use compiler's instrumentation to
    check the shadow memory on each memory access.
    
    Address sanitizer uses 1/8 of the memory addressable in kernel for shadow
    memory and uses direct mapping with a scale and offset to translate a
    memory address to its corresponding shadow address.
    
    Here is function to translate address to corresponding shadow address:
    
         unsigned long kasan_mem_to_shadow(unsigned long addr)
         {
                    return (addr >> KASAN_SHADOW_SCALE_SHIFT) + KASAN_SHADOW_OFFSET;
         }
    
    where KASAN_SHADOW_SCALE_SHIFT = 3.
    
    So for every 8 bytes there is one corresponding byte of shadow memory.
    The following encoding used for each shadow byte: 0 means that all 8 bytes
    of the corresponding memory region are valid for access; k (1 <= k <= 7)
    means that the first k bytes are valid for access, and other (8 - k) bytes
    are not; Any negative value indicates that the entire 8-bytes are
    inaccessible.  Different negative values used to distinguish between
    different kinds of inaccessible memory (redzones, freed memory) (see
    mm/kasan/kasan.h).
    
    To be able to detect accesses to bad memory we need a special compiler.
    Such compiler inserts a specific function calls (__asan_load*(addr),
    __asan_store*(addr)) before each memory access of size 1, 2, 4, 8 or 16.
    
    These functions check whether memory region is valid to access or not by
    checking corresponding shadow memory.  If access is not valid an error
    printed.
    
    Historical background of the address sanitizer from Dmitry Vyukov:
    
            "We've developed the set of tools, AddressSanitizer (Asan),
            ThreadSanitizer and MemorySanitizer, for user space. We actively use
            them for testing inside of Google (continuous testing, fuzzing,
            running prod services). To date the tools have found more than 10'000
            scary bugs in Chromium, Google internal codebase and various
            open-source projects (Firefox, OpenSSL, gcc, clang, ffmpeg, MySQL and
            lots of others): [2] [3] [4].
            The tools are part of both gcc and clang compilers.
    
            We have not yet done massive testing under the Kernel AddressSanitizer
            (it's kind of chicken and egg problem, you need it to be upstream to
            start applying it extensively). To date it has found about 50 bugs.
            Bugs that we've found in upstream kernel are listed in [5].
            We've also found ~20 bugs in out internal version of the kernel. Also
            people from Samsung and Oracle have found some.
    
            [...]
    
            As others noted, the main feature of AddressSanitizer is its
            performance due to inline compiler instrumentation and simple linear
            shadow memory. User-space Asan has ~2x slowdown on computational
            programs and ~2x memory consumption increase. Taking into account that
            kernel usually consumes only small fraction of CPU and memory when
            running real user-space programs, I would expect that kernel Asan will
            have ~10-30% slowdown and similar memory consumption increase (when we
            finish all tuning).
    
            I agree that Asan can well replace kmemcheck. We have plans to start
            working on Kernel MemorySanitizer that finds uses of unitialized
            memory. Asan+Msan will provide feature-parity with kmemcheck. As
            others noted, Asan will unlikely replace debug slab and pagealloc that
            can be enabled at runtime. Asan uses compiler instrumentation, so even
            if it is disabled, it still incurs visible overheads.
    
            Asan technology is easily portable to other architectures. Compiler
            instrumentation is fully portable. Runtime has some arch-dependent
            parts like shadow mapping and atomic operation interception. They are
            relatively easy to port."
    
    Comparison with other debugging features:
    ========================================
    
    KMEMCHECK:
    
      - KASan can do almost everything that kmemcheck can.  KASan uses
        compile-time instrumentation, which makes it significantly faster than
        kmemcheck.  The only advantage of kmemcheck over KASan is detection of
        uninitialized memory reads.
    
        Some brief performance testing showed that kasan could be
        x500-x600 times faster than kmemcheck:
    
    $ netperf -l 30
                    MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to localhost (127.0.0.1) port 0 AF_INET
                    Recv   Send    Send
                    Socket Socket  Message  Elapsed
                    Size   Size    Size     Time     Throughput
                    bytes  bytes   bytes    secs.    10^6bits/sec
    
    no debug:       87380  16384  16384    30.00    41624.72
    
    kasan inline:   87380  16384  16384    30.00    12870.54
    
    kasan outline:  87380  16384  16384    30.00    10586.39
    
    kmemcheck:      87380  16384  16384    30.03      20.23
    
      - Also kmemcheck couldn't work on several CPUs.  It always sets
        number of CPUs to 1.  KASan doesn't have such limitation.
    
    DEBUG_PAGEALLOC:
            - KASan is slower than DEBUG_PAGEALLOC, but KASan works on sub-page
              granularity level, so it able to find more bugs.
    
    SLUB_DEBUG (poisoning, redzones):
            - SLUB_DEBUG has lower overhead than KASan.
    
            - SLUB_DEBUG in most cases are not able to detect bad reads,
              KASan able to detect both reads and writes.
    
            - In some cases (e.g. redzone overwritten) SLUB_DEBUG detect
              bugs only on allocation/freeing of object. KASan catch
              bugs right before it will happen, so we always know exact
              place of first bad read/write.
    
    [1] https://code.google.com/p/address-sanitizer/wiki/AddressSanitizerForKernel
    [2] https://code.google.com/p/address-sanitizer/wiki/FoundBugs
    [3] https://code.google.com/p/thread-sanitizer/wiki/FoundBugs
    [4] https://code.google.com/p/memory-sanitizer/wiki/FoundBugs
    [5] https://code.google.com/p/address-sanitizer/wiki/AddressSanitizerForKernel#Trophies
    
    Based on work by Andrey Konovalov.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Acked-by: Michal Marek <mmarek@suse.cz>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 048b91b983ed..41c60e5302d7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1664,6 +1664,9 @@ struct task_struct {
 	unsigned long timer_slack_ns;
 	unsigned long default_timer_slack_ns;
 
+#ifdef CONFIG_KASAN
+	unsigned int kasan_depth;
+#endif
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored address in ret_stack */
 	int curr_ret_stack;

commit 545a2bf742fb41f17d03486dd8a8c74ad511dec2
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Feb 12 15:01:24 2015 -0800

    kernel/sched/clock.c: add another clock for use with the soft lockup watchdog
    
    When the hypervisor pauses a virtualised kernel the kernel will observe a
    jump in timebase, this can cause spurious messages from the softlockup
    detector.
    
    Whilst these messages are harmless, they are accompanied with a stack
    trace which causes undue concern and more problematically the stack trace
    in the guest has nothing to do with the observed problem and can only be
    misleading.
    
    Futhermore, on POWER8 this is completely avoidable with the introduction
    of the Virtual Time Base (VTB) register.
    
    This patch (of 2):
    
    This permits the use of arch specific clocks for which virtualised kernels
    can use their notion of 'running' time, not the elpased wall time which
    will include host execution time.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Andrew Jones <drjones@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: chai wen <chaiw.fnst@cn.fujitsu.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ben Zhang <benzh@chromium.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 22ee0d5d7f8c..048b91b983ed 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2147,6 +2147,7 @@ extern unsigned long long notrace sched_clock(void);
  */
 extern u64 cpu_clock(int cpu);
 extern u64 local_clock(void);
+extern u64 running_clock(void);
 extern u64 sched_clock_cpu(int cpu);
 
 

commit f56141e3e2d9aabf7e6b89680ab572c2cdbb2a24
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Feb 12 15:01:14 2015 -0800

    all arches, signal: move restart_block to struct task_struct
    
    If an attacker can cause a controlled kernel stack overflow, overwriting
    the restart block is a very juicy exploit target.  This is because the
    restart_block is held in the same memory allocation as the kernel stack.
    
    Moving the restart block to struct task_struct prevents this exploit by
    making the restart_block harder to locate.
    
    Note that there are other fields in thread_info that are also easy
    targets, at least on some architectures.
    
    It's also a decent simplification, since the restart code is more or less
    identical on all architectures.
    
    [james.hogan@imgtec.com: metag: align thread_info::supervisor_stack]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Richard Weinberger <richard@nod.at>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8db31ef98d2f..22ee0d5d7f8c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1370,6 +1370,8 @@ struct task_struct {
 
 	unsigned long atomic_flags; /* Flags needing atomic access. */
 
+	struct restart_block restart_block;
+
 	pid_t pid;
 	pid_t tgid;
 

commit 51f39a1f0cea1cacf8c787f652f26dfee9611874
Author: David Drysdale <drysdale@google.com>
Date:   Fri Dec 12 16:57:29 2014 -0800

    syscalls: implement execveat() system call
    
    This patchset adds execveat(2) for x86, and is derived from Meredydd
    Luff's patch from Sept 2012 (https://lkml.org/lkml/2012/9/11/528).
    
    The primary aim of adding an execveat syscall is to allow an
    implementation of fexecve(3) that does not rely on the /proc filesystem,
    at least for executables (rather than scripts).  The current glibc version
    of fexecve(3) is implemented via /proc, which causes problems in sandboxed
    or otherwise restricted environments.
    
    Given the desire for a /proc-free fexecve() implementation, HPA suggested
    (https://lkml.org/lkml/2006/7/11/556) that an execveat(2) syscall would be
    an appropriate generalization.
    
    Also, having a new syscall means that it can take a flags argument without
    back-compatibility concerns.  The current implementation just defines the
    AT_EMPTY_PATH and AT_SYMLINK_NOFOLLOW flags, but other flags could be
    added in future -- for example, flags for new namespaces (as suggested at
    https://lkml.org/lkml/2006/7/11/474).
    
    Related history:
     - https://lkml.org/lkml/2006/12/27/123 is an example of someone
       realizing that fexecve() is likely to fail in a chroot environment.
     - http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=514043 covered
       documenting the /proc requirement of fexecve(3) in its manpage, to
       "prevent other people from wasting their time".
     - https://bugzilla.redhat.com/show_bug.cgi?id=241609 described a
       problem where a process that did setuid() could not fexecve()
       because it no longer had access to /proc/self/fd; this has since
       been fixed.
    
    This patch (of 4):
    
    Add a new execveat(2) system call.  execveat() is to execve() as openat()
    is to open(): it takes a file descriptor that refers to a directory, and
    resolves the filename relative to that.
    
    In addition, if the filename is empty and AT_EMPTY_PATH is specified,
    execveat() executes the file to which the file descriptor refers.  This
    replicates the functionality of fexecve(), which is a system call in other
    UNIXen, but in Linux glibc it depends on opening "/proc/self/fd/<fd>" (and
    so relies on /proc being mounted).
    
    The filename fed to the executed program as argv[0] (or the name of the
    script fed to a script interpreter) will be of the form "/dev/fd/<fd>"
    (for an empty filename) or "/dev/fd/<fd>/<filename>", effectively
    reflecting how the executable was found.  This does however mean that
    execution of a script in a /proc-less environment won't work; also, script
    execution via an O_CLOEXEC file descriptor fails (as the file will not be
    accessible after exec).
    
    Based on patches by Meredydd Luff.
    
    Signed-off-by: David Drysdale <drysdale@google.com>
    Cc: Meredydd Luff <meredydd@senatehouse.org>
    Cc: Shuah Khan <shuah.kh@samsung.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Rich Felker <dalias@aerifal.cx>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cfdbcf8cf56..8db31ef98d2f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2485,6 +2485,10 @@ extern void do_group_exit(int);
 extern int do_execve(struct filename *,
 		     const char __user * const __user *,
 		     const char __user * const __user *);
+extern int do_execveat(int, struct filename *,
+		       const char __user * const __user *,
+		       const char __user * const __user *,
+		       int);
 extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);

commit 6f185c290edec576a2cccd6670e5b8e02e6f04db
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Dec 12 16:55:15 2014 -0800

    memcg: turn memcg_kmem_skip_account into a bit field
    
    It isn't supposed to stack, so turn it into a bit-field to save 4 bytes on
    the task_struct.
    
    Also, remove the memcg_stop/resume_kmem_account helpers - it is clearer to
    set/clear the flag inline.  Regarding the overwhelming comment to the
    helpers, which is removed by this patch too, we already have a compact yet
    accurate explanation in memcg_schedule_cache_create, no need in yet
    another one.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 55f5ee7cc3d3..4cfdbcf8cf56 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1364,6 +1364,10 @@ struct task_struct {
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 
+#ifdef CONFIG_MEMCG_KMEM
+	unsigned memcg_kmem_skip_account:1;
+#endif
+
 	unsigned long atomic_flags; /* Flags needing atomic access. */
 
 	pid_t pid;
@@ -1679,8 +1683,7 @@ struct task_struct {
 	/* bitmask and counter of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
-#ifdef CONFIG_MEMCG /* memcg uses this to do batch job */
-	unsigned int memcg_kmem_skip_account;
+#ifdef CONFIG_MEMCG
 	struct memcg_oom_info {
 		struct mem_cgroup *memcg;
 		gfp_t gfp_mask;

commit 86c6a2fddf0b89b494c7616f2c06cf915c4bff01
Merge: bee2782f30f6 fd7de1e8d5b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 9 21:21:34 2014 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - 'Nested Sleep Debugging', activated when CONFIG_DEBUG_ATOMIC_SLEEP=y.
    
         This instruments might_sleep() checks to catch places that nest
         blocking primitives - such as mutex usage in a wait loop.  Such
         bugs can result in hard to debug races/hangs.
    
         Another category of invalid nesting that this facility will detect
         is the calling of blocking functions from within schedule() ->
         sched_submit_work() -> blk_schedule_flush_plug().
    
         There's some potential for false positives (if secondary blocking
         primitives themselves are not ready yet for this facility), but the
         kernel will warn once about such bugs per bootup, so the warning
         isn't much of a nuisance.
    
         This feature comes with a number of fixes, for problems uncovered
         with it, so no messages are expected normally.
    
       - Another round of sched/numa optimizations and refinements, for
         CONFIG_NUMA_BALANCING=y.
    
       - Another round of sched/dl fixes and refinements.
    
      Plus various smaller fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      sched: Add missing rcu protection to wake_up_all_idle_cpus
      sched/deadline: Introduce start_hrtick_dl() for !CONFIG_SCHED_HRTICK
      sched/numa: Init numa balancing fields of init_task
      sched/deadline: Remove unnecessary definitions in cpudeadline.h
      sched/cpupri: Remove unnecessary definitions in cpupri.h
      sched/deadline: Fix rq->dl.pushable_tasks bug in push_dl_task()
      sched/fair: Fix stale overloaded status in the busiest group finding logic
      sched: Move p->nr_cpus_allowed check to select_task_rq()
      sched/completion: Document when to use wait_for_completion_io_*()
      sched: Update comments about CLONE_NEWUTS and CLONE_NEWIPC
      sched/fair: Kill task_struct::numa_entry and numa_group::task_list
      sched: Refactor task_struct to use numa_faults instead of numa_* pointers
      sched/deadline: Don't check CONFIG_SMP in switched_from_dl()
      sched/deadline: Reschedule from switched_from_dl() after a successful pull
      sched/deadline: Push task away if the deadline is equal to curr during wakeup
      sched/deadline: Add deadline rq status print
      sched/deadline: Fix artificial overrun introduced by yield_task_dl()
      sched/rt: Clean up check_preempt_equal_prio()
      sched/core: Use dl_bw_of() under rcu_read_lock_sched()
      sched: Check if we got a shallowest_idle_cpu before searching for least_loaded_cpu
      ...

commit 44dba3d5d6a10685fb15bd1954e62016334825e0
Author: Iulia Manda <iulia.manda21@gmail.com>
Date:   Fri Oct 31 02:13:31 2014 +0200

    sched: Refactor task_struct to use numa_faults instead of numa_* pointers
    
    This patch simplifies task_struct by removing the four numa_* pointers
    in the same array and replacing them with the array pointer. By doing this,
    on x86_64, the size of task_struct is reduced by 3 ulong pointers (24 bytes on
    x86_64).
    
    A new parameter is added to the task_faults_idx function so that it can return
    an index to the correct offset, corresponding with the old precalculated
    pointers.
    
    All of the code in sched/ that depended on task_faults_idx and numa_* was
    changed in order to match the new logic.
    
    Signed-off-by: Iulia Manda <iulia.manda21@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: mgorman@suse.de
    Cc: dave@stgolabs.net
    Cc: riel@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20141031001331.GA30662@winterfell
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4400ddc2fe73..bd7c14ba86c4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1597,27 +1597,22 @@ struct task_struct {
 	struct numa_group *numa_group;
 
 	/*
-	 * Exponential decaying average of faults on a per-node basis.
-	 * Scheduling placement decisions are made based on the these counts.
-	 * The values remain static for the duration of a PTE scan
+	 * numa_faults is an array split into four regions:
+	 * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer
+	 * in this precise order.
+	 *
+	 * faults_memory: Exponential decaying average of faults on a per-node
+	 * basis. Scheduling placement decisions are made based on these
+	 * counts. The values remain static for the duration of a PTE scan.
+	 * faults_cpu: Track the nodes the process was running on when a NUMA
+	 * hinting fault was incurred.
+	 * faults_memory_buffer and faults_cpu_buffer: Record faults per node
+	 * during the current scan window. When the scan completes, the counts
+	 * in faults_memory and faults_cpu decay and these values are copied.
 	 */
-	unsigned long *numa_faults_memory;
+	unsigned long *numa_faults;
 	unsigned long total_numa_faults;
 
-	/*
-	 * numa_faults_buffer records faults per node during the current
-	 * scan window. When the scan completes, the counts in
-	 * numa_faults_memory decay and these values are copied.
-	 */
-	unsigned long *numa_faults_buffer_memory;
-
-	/*
-	 * Track the nodes the process was running on when a NUMA hinting
-	 * fault was incurred.
-	 */
-	unsigned long *numa_faults_cpu;
-	unsigned long *numa_faults_buffer_cpu;
-
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last
 	 * scan window were remote/local. The task scan period is adapted

commit 28f6569ab7d036cd4ee94c26bb76dc1b3f3fc056
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Mon Sep 22 14:00:48 2014 -0400

    rcu: Remove redundant TREE_PREEMPT_RCU config option
    
    PREEMPT_RCU and TREE_PREEMPT_RCU serve the same function after
    TINY_PREEMPT_RCU has been removed. This patch removes TREE_PREEMPT_RCU
    and uses PREEMPT_RCU config option in its place.
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e344bbe63ec..706a9f744909 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1278,9 +1278,9 @@ struct task_struct {
 	union rcu_special rcu_read_unlock_special;
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
-#ifdef CONFIG_TREE_PREEMPT_RCU
+#ifdef CONFIG_PREEMPT_RCU
 	struct rcu_node *rcu_blocked_node;
-#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TASKS_RCU
 	unsigned long rcu_tasks_nvcsw;
 	bool rcu_tasks_holdout;

commit 3427445afd26bd2395f29241319283a93f362cd0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 10:18:56 2014 +0200

    sched: Exclude cond_resched() from nested sleep test
    
    cond_resched() is a preemption point, not strictly a blocking
    primitive, so exclude it from the ->state test.
    
    In particular, preemption preserves task_struct::state.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: ilya.dryomov@inktank.com
    Cc: umgwanakikbuti@gmail.com
    Cc: oleg@redhat.com
    Cc: Alex Elder <alex.elder@linaro.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Axel Lin <axel.lin@ingics.com>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140924082242.656559952@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4648e07f7d6f..4400ddc2fe73 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2806,7 +2806,7 @@ static inline int signal_pending_state(long state, struct task_struct *p)
 extern int _cond_resched(void);
 
 #define cond_resched() ({			\
-	__might_sleep(__FILE__, __LINE__, 0);	\
+	___might_sleep(__FILE__, __LINE__, 0);	\
 	_cond_resched();			\
 })
 
@@ -2819,14 +2819,14 @@ extern int __cond_resched_lock(spinlock_t *lock);
 #endif
 
 #define cond_resched_lock(lock) ({				\
-	__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);	\
+	___might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);\
 	__cond_resched_lock(lock);				\
 })
 
 extern int __cond_resched_softirq(void);
 
 #define cond_resched_softirq() ({					\
-	__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
+	___might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
 	__cond_resched_softirq();					\
 })
 

commit 8eb23b9f35aae413140d3fda766a98092c21e9b0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 10:18:55 2014 +0200

    sched: Debug nested sleeps
    
    Validate we call might_sleep() with TASK_RUNNING, which catches places
    where we nest blocking primitives, eg. mutex usage in a wait loop.
    
    Since all blocking is arranged through task_struct::state, nesting
    this will cause the inner primitive to set TASK_RUNNING and the outer
    will thus not block.
    
    Another observed problem is calling a blocking function from
    schedule()->sched_submit_work()->blk_schedule_flush_plug() which will
    then destroy the task state for the actual __schedule() call that
    comes after it.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: ilya.dryomov@inktank.com
    Cc: umgwanakikbuti@gmail.com
    Cc: oleg@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140924082242.591637616@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 320a9779f1b4..4648e07f7d6f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -243,6 +243,43 @@ extern char ___assert_task_state[1 - 2*!!(
 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
 				 (task->flags & PF_FROZEN) == 0)
 
+#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
+
+#define __set_task_state(tsk, state_value)			\
+	do {							\
+		(tsk)->task_state_change = _THIS_IP_;		\
+		(tsk)->state = (state_value);			\
+	} while (0)
+#define set_task_state(tsk, state_value)			\
+	do {							\
+		(tsk)->task_state_change = _THIS_IP_;		\
+		set_mb((tsk)->state, (state_value));		\
+	} while (0)
+
+/*
+ * set_current_state() includes a barrier so that the write of current->state
+ * is correctly serialised wrt the caller's subsequent test of whether to
+ * actually sleep:
+ *
+ *	set_current_state(TASK_UNINTERRUPTIBLE);
+ *	if (do_i_need_to_sleep())
+ *		schedule();
+ *
+ * If the caller does not need such serialisation then use __set_current_state()
+ */
+#define __set_current_state(state_value)			\
+	do {							\
+		current->task_state_change = _THIS_IP_;		\
+		current->state = (state_value);			\
+	} while (0)
+#define set_current_state(state_value)				\
+	do {							\
+		current->task_state_change = _THIS_IP_;		\
+		set_mb(current->state, (state_value));		\
+	} while (0)
+
+#else
+
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
 #define set_task_state(tsk, state_value)		\
@@ -259,11 +296,13 @@ extern char ___assert_task_state[1 - 2*!!(
  *
  * If the caller does not need such serialisation then use __set_current_state()
  */
-#define __set_current_state(state_value)			\
+#define __set_current_state(state_value)		\
 	do { current->state = (state_value); } while (0)
-#define set_current_state(state_value)		\
+#define set_current_state(state_value)			\
 	set_mb(current->state, (state_value))
 
+#endif
+
 /* Task command name length */
 #define TASK_COMM_LEN 16
 
@@ -1661,6 +1700,9 @@ struct task_struct {
 	unsigned int	sequential_io;
 	unsigned int	sequential_io_avg;
 #endif
+#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
+	unsigned long	task_state_change;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit f82f80426f7afcf55953924e71555984a4bd6ce6
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Tue Oct 7 09:52:11 2014 +0100

    sched/deadline: Ensure that updates to exclusive cpusets don't break AC
    
    How we deal with updates to exclusive cpusets is currently broken.
    As an example, suppose we have an exclusive cpuset composed of
    two cpus: A[cpu0,cpu1]. We can assign SCHED_DEADLINE task to it
    up to the allowed bandwidth. If we want now to modify cpusetA's
    cpumask, we have to check that removing a cpu's amount of
    bandwidth doesn't break AC guarantees. This thing isn't checked
    in the current code.
    
    This patch fixes the problem above, denying an update if the
    new cpumask won't have enough bandwidth for SCHED_DEADLINE tasks
    that are currently active.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: cgroups@vger.kernel.org
    Link: http://lkml.kernel.org/r/5433E6AF.5080105@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1d1fa081d44f..320a9779f1b4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2052,6 +2052,8 @@ static inline void tsk_restore_flags(struct task_struct *task,
 	task->flags |= orig_flags & flags;
 }
 
+extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,
+				     const struct cpumask *trial);
 extern int task_can_attach(struct task_struct *p,
 			   const struct cpumask *cs_cpus_allowed);
 #ifdef CONFIG_SMP

commit 7f51412a415d87ea8598d14722fb31e4f5701257
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Fri Sep 19 10:22:40 2014 +0100

    sched/deadline: Fix bandwidth check/update when migrating tasks between exclusive cpusets
    
    Exclusive cpusets are the only way users can restrict SCHED_DEADLINE tasks
    affinity (performing what is commonly called clustered scheduling).
    Unfortunately, such thing is currently broken for two reasons:
    
     - No check is performed when the user tries to attach a task to
       an exlusive cpuset (recall that exclusive cpusets have an
       associated maximum allowed bandwidth).
    
     - Bandwidths of source and destination cpusets are not correctly
       updated after a task is migrated between them.
    
    This patch fixes both things at once, as they are opposite faces
    of the same coin.
    
    The check is performed in cpuset_can_attach(), as there aren't any
    points of failure after that function. The updated is split in two
    halves. We first reserve bandwidth in the destination cpuset, after
    we pass the check in cpuset_can_attach(). And we then release
    bandwidth from the source cpuset when the task's affinity is
    actually changed. Even if there can be time windows when sched_setattr()
    may erroneously fail in the source cpuset, we are fine with it, as
    we can't perfom an atomic update of both cpusets at once.
    
    Reported-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Reported-by: Vincent Legout <vincent@legout.info>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dario Faggioli <raistlin@linux.it>
    Cc: Michael Trimarchi <michael@amarulasolutions.com>
    Cc: Fabio Checconi <fchecconi@gmail.com>
    Cc: michael@amarulasolutions.com
    Cc: luca.abeni@unitn.it
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: cgroups@vger.kernel.org
    Link: http://lkml.kernel.org/r/1411118561-26323-3-git-send-email-juri.lelli@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e344bbe63ec..1d1fa081d44f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2052,6 +2052,8 @@ static inline void tsk_restore_flags(struct task_struct *task,
 	task->flags |= orig_flags & flags;
 }
 
+extern int task_can_attach(struct task_struct *p,
+			   const struct cpumask *cs_cpus_allowed);
 #ifdef CONFIG_SMP
 extern void do_set_cpus_allowed(struct task_struct *p,
 			       const struct cpumask *new_mask);

commit faafcba3b5e15999cf75d5c5a513ac8e47e2545f
Merge: 13ead805c5a1 f10e00f4bf36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 16:23:15 2014 +0200

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Optimized support for Intel "Cluster-on-Die" (CoD) topologies (Dave
         Hansen)
    
       - Various sched/idle refinements for better idle handling (Nicolas
         Pitre, Daniel Lezcano, Chuansheng Liu, Vincent Guittot)
    
       - sched/numa updates and optimizations (Rik van Riel)
    
       - sysbench speedup (Vincent Guittot)
    
       - capacity calculation cleanups/refactoring (Vincent Guittot)
    
       - Various cleanups to thread group iteration (Oleg Nesterov)
    
       - Double-rq-lock removal optimization and various refactorings
         (Kirill Tkhai)
    
       - various sched/deadline fixes
    
      ... and lots of other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (72 commits)
      sched/dl: Use dl_bw_of() under rcu_read_lock_sched()
      sched/fair: Delete resched_cpu() from idle_balance()
      sched, time: Fix build error with 64 bit cputime_t on 32 bit systems
      sched: Improve sysbench performance by fixing spurious active migration
      sched/x86: Fix up typo in topology detection
      x86, sched: Add new topology for multi-NUMA-node CPUs
      sched/rt: Use resched_curr() in task_tick_rt()
      sched: Use rq->rd in sched_setaffinity() under RCU read lock
      sched: cleanup: Rename 'out_unlock' to 'out_free_new_mask'
      sched: Use dl_bw_of() under RCU read lock
      sched/fair: Remove duplicate code from can_migrate_task()
      sched, mips, ia64: Remove __ARCH_WANT_UNLOCKED_CTXSW
      sched: print_rq(): Don't use tasklist_lock
      sched: normalize_rt_tasks(): Don't use _irqsave for tasklist_lock, use task_rq_lock()
      sched: Fix the task-group check in tg_has_rt_tasks()
      sched/fair: Leverage the idle state info when choosing the "idlest" cpu
      sched: Let the scheduler see CPU idle states
      sched/deadline: Fix inter- exclusive cpusets migrations
      sched/deadline: Clear dl_entity params when setscheduling to different class
      sched/numa: Kill the wrong/dead TASK_DEAD check in task_numa_fault()
      ...

commit d6dd50e07c5bec00db2005969b1a01f8ca3d25ef
Merge: 5ff0b9e1a1da fd19bda49120
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 15:44:12 2014 +0200

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes related to No-CBs CPUs and NO_HZ_FULL
    
       - RCU-tasks implementation
    
       - torture-test updates
    
       - miscellaneous fixes
    
       - locktorture updates
    
       - RCU documentation updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (81 commits)
      workqueue: Use cond_resched_rcu_qs macro
      workqueue: Add quiescent state between work items
      locktorture: Cleanup header usage
      locktorture: Cannot hold read and write lock
      locktorture: Fix __acquire annotation for spinlock irq
      locktorture: Support rwlocks
      rcu: Eliminate deadlock between CPU hotplug and expedited grace periods
      locktorture: Document boot/module parameters
      rcutorture: Rename rcutorture_runnable parameter
      locktorture: Add test scenario for rwsem_lock
      locktorture: Add test scenario for mutex_lock
      locktorture: Make torture scripting account for new _runnable name
      locktorture: Introduce torture context
      locktorture: Support rwsems
      locktorture: Add infrastructure for torturing read locks
      torture: Address race in module cleanup
      locktorture: Make statistics generic
      locktorture: Teach about lock debugging
      locktorture: Support mutexes
      locktorture: Add documentation
      ...

commit 934f3072c17cc8886f4c043b47eeeb1b12f8de33
Author: Junxiao Bi <junxiao.bi@oracle.com>
Date:   Thu Oct 9 15:28:23 2014 -0700

    mm: clear __GFP_FS when PF_MEMALLOC_NOIO is set
    
    commit 21caf2fc1931 ("mm: teach mm by current context info to not do I/O
    during memory allocation") introduces PF_MEMALLOC_NOIO flag to avoid doing
    I/O inside memory allocation, __GFP_IO is cleared when this flag is set,
    but __GFP_FS implies __GFP_IO, it should also be cleared.  Or it may still
    run into I/O, like in superblock shrinker.  And this will make the kernel
    run into the deadlock case described in that commit.
    
    See Dave Chinner's comment about io in superblock shrinker:
    
    Filesystem shrinkers do indeed perform IO from the superblock shrinker and
    have for years.  Even clean inodes can require IO before they can be freed
    - e.g.  on an orphan list, need truncation of post-eof blocks, need to
    wait for ordered operations to complete before it can be freed, etc.
    
    IOWs, Ext4, btrfs and XFS all can issue and/or block on arbitrary amounts
    of IO in the superblock shrinker context.  XFS, in particular, has been
    doing transactions and IO from the VFS inode cache shrinker since it was
    first introduced....
    
    Fix this by clearing __GFP_FS in memalloc_noio_flags(), this function has
    masked all the gfp_mask that will be passed into fs for the processes
    setting PF_MEMALLOC_NOIO in the direct reclaim path.
    
    v1 thread at: https://lkml.org/lkml/2014/9/3/32
    
    Signed-off-by: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: joyce.xue <xuejiufei@huawei.com>
    Cc: Ming Lei <ming.lei@canonical.com>
    Cc: Trond Myklebust <trond.myklebust@primarydata.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c6353d9e63a..5e63ba59258c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1935,11 +1935,13 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
-/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */
+/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags
+ * __GFP_FS is also cleared as it implies __GFP_IO.
+ */
 static inline gfp_t memalloc_noio_flags(gfp_t flags)
 {
 	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
-		flags &= ~__GFP_IO;
+		flags &= ~(__GFP_IO | __GFP_FS);
 	return flags;
 }
 

commit 87d7bcee4f5973a593b0d50134364cfe5652ff33
Merge: 0223f9aaef94 be34c4ef693f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 06:44:48 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto update from Herbert Xu:
     - add multibuffer infrastructure (single_task_running scheduler helper,
       OKed by Peter on lkml.
     - add SHA1 multibuffer implementation for AVX2.
     - reenable "by8" AVX CTR optimisation after fixing counter overflow.
     - add APM X-Gene SoC RNG support.
     - SHA256/SHA512 now handles unaligned input correctly.
     - set lz4 decompressed length correctly.
     - fix algif socket buffer allocation failure for 64K page machines.
     - misc fixes
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (47 commits)
      crypto: sha - Handle unaligned input data in generic sha256 and sha512.
      Revert "crypto: aesni - disable "by8" AVX CTR optimization"
      crypto: aesni - remove unused defines in "by8" variant
      crypto: aesni - fix counter overflow handling in "by8" variant
      hwrng: printk replacement
      crypto: qat - Removed unneeded partial state
      crypto: qat - Fix typo in name of tasklet_struct
      crypto: caam - Dynamic allocation of addresses for various memory blocks in CAAM.
      crypto: mcryptd - Fix typos in CRYPTO_MCRYPTD description
      crypto: algif - avoid excessive use of socket buffer in skcipher
      arm64: dts: add random number generator dts node to APM X-Gene platform.
      Documentation: rng: Add X-Gene SoC RNG driver documentation
      hwrng: xgene - add support for APM X-Gene SoC RNG support
      crypto: mv_cesa - Add missing #define
      crypto: testmgr - add test for lz4 and lz4hc
      crypto: lz4,lz4hc - fix decompression
      crypto: qat - Use pci_enable_msix_exact() instead of pci_enable_msix()
      crypto: drbg - fix maximum value checks on 32 bit systems
      crypto: drbg - fix sparse warning for cpu_to_be[32|64]
      crypto: sha-mb - sha1_mb_alg_state can be static
      ...

commit 6111da3432b10b2c56a21a5d8671aee46435326d
Merge: 83692898642a 2ad654bc5e2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 27 16:45:33 2014 -0700

    Merge branch 'for-3.17-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup fixes from Tejun Heo:
     "This is quite late but these need to be backported anyway.
    
      This is the fix for a long-standing cpuset bug which existed from
      2009.  cpuset makes use of PF_SPREAD_{PAGE|SLAB} flags to modify the
      task's memory allocation behavior according to the settings of the
      cpuset it belongs to; unfortunately, when those flags have to be
      changed, cpuset did so directly even whlie the target task is running,
      which is obviously racy as task->flags may be modified by the task
      itself at any time.  This obscure bug manifested as corrupt
      PF_USED_MATH flag leading to a weird crash.
    
      The bug is fixed by moving the flag to task->atomic_flags.  The first
      two are prepatory ones to help defining atomic_flags accessors and the
      third one is the actual fix"
    
    * 'for-3.17-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cpuset: PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags
      sched: add macros to define bitops for task atomic flags
      sched: fix confusing PFA_NO_NEW_PRIVS constant

commit 2ad654bc5e2b211e92f66da1d819e47d79a866f0
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 25 09:41:02 2014 +0800

    cpuset: PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags
    
    When we change cpuset.memory_spread_{page,slab}, cpuset will flip
    PF_SPREAD_{PAGE,SLAB} bit of tsk->flags for each task in that cpuset.
    This should be done using atomic bitops, but currently we don't,
    which is broken.
    
    Tetsuo reported a hard-to-reproduce kernel crash on RHEL6, which happened
    when one thread tried to clear PF_USED_MATH while at the same time another
    thread tried to flip PF_SPREAD_PAGE/PF_SPREAD_SLAB. They both operate on
    the same task.
    
    Here's the full report:
    https://lkml.org/lkml/2014/9/19/230
    
    To fix this, we make PF_SPREAD_PAGE and PF_SPREAD_SLAB atomic flags.
    
    v4:
    - updated mm/slab.c. (Fengguang Wu)
    - updated Documentation.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Fixes: 950592f7b991 ("cpusets: update tasks' page/slab spread flags in time")
    Cc: <stable@vger.kernel.org> # 2.6.31+
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5630763956d9..7b1cafefb05e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1903,8 +1903,6 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_KTHREAD	0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */
 #define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
-#define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
-#define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_NO_SETAFFINITY 0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
@@ -1958,6 +1956,9 @@ static inline void memalloc_noio_restore(unsigned int flags)
 
 /* Per-process atomic flags. */
 #define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
+#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */
+#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */
+
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
@@ -1972,6 +1973,14 @@ static inline void memalloc_noio_restore(unsigned int flags)
 TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)
 TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)
 
+TASK_PFA_TEST(SPREAD_PAGE, spread_page)
+TASK_PFA_SET(SPREAD_PAGE, spread_page)
+TASK_PFA_CLEAR(SPREAD_PAGE, spread_page)
+
+TASK_PFA_TEST(SPREAD_SLAB, spread_slab)
+TASK_PFA_SET(SPREAD_SLAB, spread_slab)
+TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)
+
 /*
  * task->jobctl flags
  */

commit e0e5070b20e01f0321f97db4e4e174f3f6b49e50
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 25 09:40:40 2014 +0800

    sched: add macros to define bitops for task atomic flags
    
    This will simplify code when we add new flags.
    
    v3:
    - Kees pointed out that no_new_privs should never be cleared, so we
    shouldn't define task_clear_no_new_privs(). we define 3 macros instead
    of a single one.
    
    v2:
    - updated scripts/tags.sh, suggested by Peter
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 45577650f629..5630763956d9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1959,15 +1959,18 @@ static inline void memalloc_noio_restore(unsigned int flags)
 /* Per-process atomic flags. */
 #define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 
-static inline bool task_no_new_privs(struct task_struct *p)
-{
-	return test_bit(PFA_NO_NEW_PRIVS, &p->atomic_flags);
-}
-
-static inline void task_set_no_new_privs(struct task_struct *p)
-{
-	set_bit(PFA_NO_NEW_PRIVS, &p->atomic_flags);
-}
+#define TASK_PFA_TEST(name, func)					\
+	static inline bool task_##func(struct task_struct *p)		\
+	{ return test_bit(PFA_##name, &p->atomic_flags); }
+#define TASK_PFA_SET(name, func)					\
+	static inline void task_set_##func(struct task_struct *p)	\
+	{ set_bit(PFA_##name, &p->atomic_flags); }
+#define TASK_PFA_CLEAR(name, func)					\
+	static inline void task_clear_##func(struct task_struct *p)	\
+	{ clear_bit(PFA_##name, &p->atomic_flags); }
+
+TASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)
+TASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)
 
 /*
  * task->jobctl flags

commit a2b86f772227bcaf962c8b134f8d187046ac5f0e
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 25 09:40:17 2014 +0800

    sched: fix confusing PFA_NO_NEW_PRIVS constant
    
    Commit 1d4457f99928 ("sched: move no_new_privs into new atomic flags")
    defined PFA_NO_NEW_PRIVS as hexadecimal value, but it is confusing
    because it is used as bit number. Redefine it as decimal bit number.
    
    Note this changes the bit position of PFA_NOW_NEW_PRIVS from 1 to 0.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Kees Cook <keescook@chromium.org>
    [ lizf: slightly modified subject and changelog ]
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c2c885ee52b..45577650f629 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1957,7 +1957,7 @@ static inline void memalloc_noio_restore(unsigned int flags)
 }
 
 /* Per-process atomic flags. */
-#define PFA_NO_NEW_PRIVS 0x00000001	/* May not gain new privileges. */
+#define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */
 
 static inline bool task_no_new_privs(struct task_struct *p)
 {

commit 6a40281ab5c1ed8ba2253857118a5d400a2d084b
Author: Chuck Ebbert <cebbert.lkml@gmail.com>
Date:   Sat Sep 20 10:17:51 2014 -0500

    sched: Fix end_of_stack() and location of stack canary for architectures using CONFIG_STACK_GROWSUP
    
    Aaron Tomlin recently posted patches [1] to enable checking the
    stack canary on every task switch. Looking at the canary code, I
    realized that every arch (except ia64, which adds some space for
    register spill above the stack) shares a definition of
    end_of_stack() that makes it the first long after the
    threadinfo.
    
    For stacks that grow down, this low address is correct because
    the stack starts at the end of the thread area and grows toward
    lower addresses. However, for stacks that grow up, toward higher
    addresses, this is wrong. (The stack actually grows away from
    the canary.) On these archs end_of_stack() should return the
    address of the last long, at the highest possible address for the stack.
    
    [1] http://lkml.org/lkml/2014/9/12/293
    
    Signed-off-by: Chuck Ebbert <cebbert.lkml@gmail.com>
    Link: http://lkml.kernel.org/r/20140920101751.6c5166b6@as
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Tested-by: James Hogan <james.hogan@imgtec.com> [metag]
    Acked-by: James Hogan <james.hogan@imgtec.com>
    Acked-by: Aaron Tomlin <atomlin@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c2c885ee52b..1f07040d28e3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2608,9 +2608,22 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 	task_thread_info(p)->task = p;
 }
 
+/*
+ * Return the address of the last usable long on the stack.
+ *
+ * When the stack grows down, this is just above the thread
+ * info struct. Going any lower will corrupt the threadinfo.
+ *
+ * When the stack grows up, this is the highest address.
+ * Beyond that position, we corrupt data on the next page.
+ */
 static inline unsigned long *end_of_stack(struct task_struct *p)
 {
+#ifdef CONFIG_STACK_GROWSUP
+	return (unsigned long *)((unsigned long)task_thread_info(p) + THREAD_SIZE) - 1;
+#else
 	return (unsigned long *)(task_thread_info(p) + 1);
+#endif
 }
 
 #endif

commit a70857e46dd13e87ae06bf0e64cb6a2d4f436265
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Fri Sep 12 14:16:18 2014 +0100

    sched: Add helper for task stack page overrun checking
    
    This facility is used in a few places so let's introduce
    a helper function to improve code readability.
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: aneesh.kumar@linux.vnet.ibm.com
    Cc: dzickus@redhat.com
    Cc: bmr@redhat.com
    Cc: jcastillo@redhat.com
    Cc: oleg@redhat.com
    Cc: riel@redhat.com
    Cc: prarit@redhat.com
    Cc: jgh@redhat.com
    Cc: minchan@kernel.org
    Cc: mpe@ellerman.id.au
    Cc: tglx@linutronix.de
    Cc: hannes@cmpxchg.org
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1410527779-8133-3-git-send-email-atomlin@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 118dca7d5a28..18f52624eaa6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2617,6 +2617,8 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 }
 
 #endif
+#define task_stack_end_corrupted(task) \
+		(*(end_of_stack(task)) != STACK_END_MAGIC)
 
 static inline int object_is_on_stack(void *obj)
 {

commit d4311ff1a8da48d609db9500f121c15580dfeeb7
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Fri Sep 12 14:16:17 2014 +0100

    init/main.c: Give init_task a canary
    
    Tasks get their end of stack set to STACK_END_MAGIC with the
    aim to catch stack overruns. Currently this feature does not
    apply to init_task. This patch removes this restriction.
    
    Note that a similar patch was posted by Prarit Bhargava
    some time ago but was never merged:
    
      http://marc.info/?l=linux-kernel&m=127144305403241&w=2
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: aneesh.kumar@linux.vnet.ibm.com
    Cc: dzickus@redhat.com
    Cc: bmr@redhat.com
    Cc: jcastillo@redhat.com
    Cc: jgh@redhat.com
    Cc: minchan@kernel.org
    Cc: tglx@linutronix.de
    Cc: hannes@cmpxchg.org
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1410527779-8133-2-git-send-email-atomlin@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 82ff3d6efb19..118dca7d5a28 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -57,6 +57,7 @@ struct sched_param {
 #include <linux/llist.h>
 #include <linux/uidgid.h>
 #include <linux/gfp.h>
+#include <linux/magic.h>
 
 #include <asm/processor.h>
 
@@ -2638,6 +2639,7 @@ static inline unsigned long stack_not_used(struct task_struct *p)
 	return (unsigned long)n - (unsigned long)end_of_stack(p);
 }
 #endif
+extern void set_task_stack_end_magic(struct task_struct *tsk);
 
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available

commit f6be8af1c95de4a46e325e728900a70ceadb52cf
Author: Chuansheng Liu <chuansheng.liu@intel.com>
Date:   Thu Sep 4 15:17:53 2014 +0800

    sched: Add new API wake_up_if_idle() to wake up the idle cpu
    
    Implementing one new API wake_up_if_idle(), which is used to
    wake up the idle CPU.
    
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Chuansheng Liu <chuansheng.liu@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: daniel.lezcano@linaro.org
    Cc: rjw@rjwysocki.net
    Cc: linux-pm@vger.kernel.org
    Cc: changcheng.liu@intel.com
    Cc: xiaoming.wang@intel.com
    Cc: souvik.k.chakravarty@intel.com
    Cc: chuansheng.liu@intel.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1409815075-4180-1-git-send-email-chuansheng.liu@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dd9eb4807389..82ff3d6efb19 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1024,6 +1024,7 @@ struct sched_domain_topology_level {
 extern struct sched_domain_topology_level *sched_domain_topology;
 
 extern void set_sched_topology(struct sched_domain_topology_level *tl);
+extern void wake_up_if_idle(int cpu);
 
 #ifdef CONFIG_SCHED_DEBUG
 # define SD_INIT_NAME(type)		.name = #type

commit e78c3496790ee8a36522a838b59b388e8a709e65
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Aug 16 13:40:10 2014 -0400

    time, signal: Protect resource use statistics with seqlock
    
    Both times() and clock_gettime(CLOCK_PROCESS_CPUTIME_ID) have scalability
    issues on large systems, due to both functions being serialized with a
    lock.
    
    The lock protects against reporting a wrong value, due to a thread in the
    task group exiting, its statistics reporting up to the signal struct, and
    that exited task's statistics being counted twice (or not at all).
    
    Protecting that with a lock results in times() and clock_gettime() being
    completely serialized on large systems.
    
    This can be fixed by using a seqlock around the events that gather and
    propagate statistics. As an additional benefit, the protection code can
    be moved into thread_group_cputime(), slightly simplifying the calling
    functions.
    
    In the case of posix_cpu_clock_get_task() things can be simplified a
    lot, because the calling function already ensures that the task sticks
    around, and the rest is now taken care of in thread_group_cputime().
    
    This way the statistics reporting code can run lockless.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guillaume Morin <guillaume@morinfr.org>
    Cc: Ionut Alexa <ionut.m.alexa@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: fweisbec@gmail.com
    Cc: srao@redhat.com
    Cc: lwoodman@redhat.com
    Cc: atheurer@redhat.com
    Link: http://lkml.kernel.org/r/20140816134010.26a9b572@annuminas.surriel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c2c885ee52b..dd9eb4807389 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -645,6 +645,7 @@ struct signal_struct {
 	 * Live threads maintain their own counters and add to these
 	 * in __exit_signal, except for the group leader.
 	 */
+	seqlock_t stats_lock;
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;

commit 1d082fd061884a587c490c4fc8a2056ce1e47624
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 14 16:01:53 2014 -0700

    rcu: Remove local_irq_disable() in rcu_preempt_note_context_switch()
    
    The rcu_preempt_note_context_switch() function is on a scheduling fast
    path, so it would be good to avoid disabling irqs.  The reason that irqs
    are disabled is to synchronize process-level and irq-handler access to
    the task_struct ->rcu_read_unlock_special bitmask.  This commit therefore
    makes ->rcu_read_unlock_special instead be a union of bools with a short
    allowing single-access checks in RCU's __rcu_read_unlock().  This results
    in the process-level and irq-handler accesses being simple loads and
    stores, so that irqs need no longer be disabled.  This commit therefore
    removes the irq disabling from rcu_preempt_note_context_switch().
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ec8b34722bcc..42888d715fb1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1212,6 +1212,13 @@ struct sched_dl_entity {
 	struct hrtimer dl_timer;
 };
 
+union rcu_special {
+	struct {
+		bool blocked;
+		bool need_qs;
+	} b;
+	short s;
+};
 struct rcu_node;
 
 enum perf_event_task_context {
@@ -1264,7 +1271,7 @@ struct task_struct {
 
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;
-	char rcu_read_unlock_special;
+	union rcu_special rcu_read_unlock_special;
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TREE_PREEMPT_RCU
@@ -2005,16 +2012,11 @@ extern void task_clear_jobctl_trapping(struct task_struct *task);
 extern void task_clear_jobctl_pending(struct task_struct *task,
 				      unsigned int mask);
 
-#ifdef CONFIG_PREEMPT_RCU
-#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
-#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
-
 static inline void rcu_copy_process(struct task_struct *p)
 {
 #ifdef CONFIG_PREEMPT_RCU
 	p->rcu_read_lock_nesting = 0;
-	p->rcu_read_unlock_special = 0;
+	p->rcu_read_unlock_special.s = 0;
 	p->rcu_blocked_node = NULL;
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 #endif /* #ifdef CONFIG_PREEMPT_RCU */

commit 176f8f7a52cc6d09d686f0d900abda6942a52fbb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 4 17:43:50 2014 -0700

    rcu: Make TASKS_RCU handle nohz_full= CPUs
    
    Currently TASKS_RCU would ignore a CPU running a task in nohz_full=
    usermode execution.  There would be neither a context switch nor a
    scheduling-clock interrupt to tell TASKS_RCU that the task in question
    had passed through a quiescent state.  The grace period would therefore
    extend indefinitely.  This commit therefore makes RCU's dyntick-idle
    subsystem record the task_struct structure of the task that is running
    in dyntick-idle mode on each CPU.  The TASKS_RCU grace period can
    then access this information and record a quiescent state on
    behalf of any CPU running in dyntick-idle usermode.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eaacac4ae77d..ec8b34722bcc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1274,6 +1274,7 @@ struct task_struct {
 	unsigned long rcu_tasks_nvcsw;
 	bool rcu_tasks_holdout;
 	struct list_head rcu_tasks_holdout_list;
+	int rcu_tasks_idle_cpu;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
@@ -2020,6 +2021,7 @@ static inline void rcu_copy_process(struct task_struct *p)
 #ifdef CONFIG_TASKS_RCU
 	p->rcu_tasks_holdout = false;
 	INIT_LIST_HEAD(&p->rcu_tasks_holdout_list);
+	p->rcu_tasks_idle_cpu = -1;
 #endif /* #ifdef CONFIG_TASKS_RCU */
 }
 

commit 8315f42295d2667a7f942f154b73a86fd7cb2227
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 27 13:42:20 2014 -0700

    rcu: Add call_rcu_tasks()
    
    This commit adds a new RCU-tasks flavor of RCU, which provides
    call_rcu_tasks().  This RCU flavor's quiescent states are voluntary
    context switch (not preemption!) and userspace execution (not the idle
    loop -- use some sort of schedule_on_each_cpu() if you need to handle the
    idle tasks.  Note that unlike other RCU flavors, these quiescent states
    occur in tasks, not necessarily CPUs.  Includes fixes from Steven Rostedt.
    
    This RCU flavor is assumed to have very infrequent latency-tolerant
    updaters.  This assumption permits significant simplifications, including
    a single global callback list protected by a single global lock, along
    with a single task-private linked list containing all tasks that have not
    yet passed through a quiescent state.  If experience shows this assumption
    to be incorrect, the required additional complexity will be added.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c2c885ee52b..eaacac4ae77d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1270,6 +1270,11 @@ struct task_struct {
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+#ifdef CONFIG_TASKS_RCU
+	unsigned long rcu_tasks_nvcsw;
+	bool rcu_tasks_holdout;
+	struct list_head rcu_tasks_holdout_list;
+#endif /* #ifdef CONFIG_TASKS_RCU */
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
@@ -2000,28 +2005,24 @@ extern void task_clear_jobctl_pending(struct task_struct *task,
 				      unsigned int mask);
 
 #ifdef CONFIG_PREEMPT_RCU
-
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
 #define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
 
 static inline void rcu_copy_process(struct task_struct *p)
 {
+#ifdef CONFIG_PREEMPT_RCU
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_read_unlock_special = 0;
-#ifdef CONFIG_TREE_PREEMPT_RCU
 	p->rcu_blocked_node = NULL;
-#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 	INIT_LIST_HEAD(&p->rcu_node_entry);
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
+#ifdef CONFIG_TASKS_RCU
+	p->rcu_tasks_holdout = false;
+	INIT_LIST_HEAD(&p->rcu_tasks_holdout_list);
+#endif /* #ifdef CONFIG_TASKS_RCU */
 }
 
-#else
-
-static inline void rcu_copy_process(struct task_struct *p)
-{
-}
-
-#endif
-
 static inline void tsk_restore_flags(struct task_struct *task,
 				unsigned long orig_flags, unsigned long flags)
 {

commit 2ee507c472939db4b146d545352b8a7c79ef47f8
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Thu Jul 31 10:29:48 2014 -0700

    sched: Add function single_task_running to let a task check if it is the only task running on a cpu
    
    This function will help an async task processing batched jobs from
    workqueue decide if it wants to keep processing on more chunks of batched
    work that can be delayed, or to accumulate more work for more efficient
    batched processing later.
    
    If no other tasks are running on the cpu, the batching process can take
    advantgae of the available cpu cycles to a make decision to continue
    processing the existing accumulated work to minimize delay,
    otherwise it will yield.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c2c885ee52b..e6d2c056d8e0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -167,6 +167,7 @@ extern int nr_threads;
 DECLARE_PER_CPU(unsigned long, process_counts);
 extern int nr_processes(void);
 extern unsigned long nr_running(void);
+extern bool single_task_running(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
 extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);

commit c9d26423e56ce1ab4d786f92aebecf859d419293
Merge: a11c5c9ef6dc af5b7e84d022
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 14 18:13:46 2014 -0600

    Merge tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more ACPI and power management updates from Rafael Wysocki:
     "These are a couple of regression fixes, cpuidle menu governor
      optimizations, fixes for ACPI proccessor and battery drivers,
      hibernation fix to avoid problems related to the e820 memory map,
      fixes for a few cpufreq drivers and a new version of the suspend
      profiling tool analyze_suspend.py.
    
      Specifics:
    
       - Fix for an ACPI-based device hotplug regression introduced in 3.14
         that causes a kernel panic to trigger when memory hot-remove is
         attempted with CONFIG_ACPI_HOTPLUG_MEMORY unset from Tang Chen
    
       - Fix for a cpufreq regression introduced in 3.16 that triggers a
         "sleeping function called from invalid context" bug in
         dev_pm_opp_init_cpufreq_table() from Stephen Boyd
    
       - ACPI battery driver fix for a warning message added in 3.16 that
         prints silly stuff sometimes from Mariusz Ceier
    
       - Hibernation fix for safer handling of mismatches in the 820 memory
         map between the configurations during image creation and during the
         subsequent restore from Chun-Yi Lee
    
       - ACPI processor driver fix to handle CPU hotplug notifications
         correctly during system suspend/resume from Lan Tianyu
    
       - Series of four cpuidle menu governor cleanups that also should
         speed it up a bit from Mel Gorman
    
       - Fixes for the speedstep-smi, integrator, cpu0 and arm_big_little
         cpufreq drivers from Hans Wennborg, Himangi Saraogi, Markus
         Pargmann and Uwe Kleine-KÃ¶nig
    
       - Version 3.0 of the analyze_suspend.py suspend profiling tool from
         Todd E Brandt"
    
    * tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      ACPI / battery: Fix warning message in acpi_battery_get_state()
      PM / tools: analyze_suspend.py: update to v3.0
      cpufreq: arm_big_little: fix module license spec
      cpufreq: speedstep-smi: fix decimal printf specifiers
      ACPI / hotplug: Check scan handlers in acpi_scan_hot_remove()
      cpufreq: OPP: Avoid sleeping while atomic
      cpufreq: cpu0: Do not print error message when deferring
      cpufreq: integrator: Use set_cpus_allowed_ptr
      PM / hibernate: avoid unsafe pages in e820 reserved regions
      ACPI / processor: Make acpi_cpu_soft_notify() process CPU FROZEN events
      cpuidle: menu: Lookup CPU runqueues less
      cpuidle: menu: Call nr_iowait_cpu less times
      cpuidle: menu: Use ktime_to_us instead of reinventing the wheel
      cpuidle: menu: Use shifts when calculating averages where possible

commit e67ee10190e69332f929bdd6594a312363321a66
Merge: 21c806d9b189 84c91b7ae07c 39c8bbaf67b1 372ba8cb46b2
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Aug 11 23:19:48 2014 +0200

    Merge branches 'pm-sleep', 'pm-cpufreq' and 'pm-cpuidle'
    
    * pm-sleep:
      PM / hibernate: avoid unsafe pages in e820 reserved regions
    
    * pm-cpufreq:
      cpufreq: arm_big_little: fix module license spec
      cpufreq: speedstep-smi: fix decimal printf specifiers
      cpufreq: OPP: Avoid sleeping while atomic
      cpufreq: cpu0: Do not print error message when deferring
      cpufreq: integrator: Use set_cpus_allowed_ptr
    
    * pm-cpuidle:
      cpuidle: menu: Lookup CPU runqueues less
      cpuidle: menu: Call nr_iowait_cpu less times
      cpuidle: menu: Use ktime_to_us instead of reinventing the wheel
      cpuidle: menu: Use shifts when calculating averages where possible

commit 63b12bdb0d21aca527996fb2c547387bfd3e14b8
Merge: ad1f5caf3439 059ade650ae5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 9 09:58:12 2014 -0700

    Merge branch 'signal-cleanup' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc
    
    Pull arch signal handling cleanup from Richard Weinberger:
     "This patch series moves all remaining archs to the get_signal(),
      signal_setup_done() and sigsp() functions.
    
      Currently these archs use open coded variants of the said functions.
      Further, unused parameters get removed from get_signal_to_deliver(),
      tracehook_signal_handler() and signal_delivered().
    
      At the end of the day we save around 500 lines of code."
    
    * 'signal-cleanup' of git://git.kernel.org/pub/scm/linux/kernel/git/rw/misc: (43 commits)
      powerpc: Use sigsp()
      openrisc: Use sigsp()
      mn10300: Use sigsp()
      mips: Use sigsp()
      microblaze: Use sigsp()
      metag: Use sigsp()
      m68k: Use sigsp()
      m32r: Use sigsp()
      hexagon: Use sigsp()
      frv: Use sigsp()
      cris: Use sigsp()
      c6x: Use sigsp()
      blackfin: Use sigsp()
      avr32: Use sigsp()
      arm64: Use sigsp()
      arc: Use sigsp()
      sas_ss_flags: Remove nested ternary if
      Rip out get_signal_to_deliver()
      Clean up signal_delivered()
      tracehook_signal_handler: Remove sig, info, ka and regs
      ...

commit ab602f799159393143d567e5c04b936fec79d6bd
Author: Jack Miller <millerjo@us.ibm.com>
Date:   Fri Aug 8 14:23:19 2014 -0700

    shm: make exit_shm work proportional to task activity
    
    This is small set of patches our team has had kicking around for a few
    versions internally that fixes tasks getting hung on shm_exit when there
    are many threads hammering it at once.
    
    Anton wrote a simple test to cause the issue:
    
      http://ozlabs.org/~anton/junkcode/bust_shm_exit.c
    
    Before applying this patchset, this test code will cause either hanging
    tracebacks or pthread out of memory errors.
    
    After this patchset, it will still produce output like:
    
      root@somehost:~# ./bust_shm_exit 1024 160
      ...
      INFO: rcu_sched detected stalls on CPUs/tasks: {} (detected by 116, t=2111 jiffies, g=241, c=240, q=7113)
      INFO: Stall ended before state dump start
      ...
    
    But the task will continue to run along happily, so we consider this an
    improvement over hanging, even if it's a bit noisy.
    
    This patch (of 3):
    
    exit_shm obtains the ipc_ns shm rwsem for write and holds it while it
    walks every shared memory segment in the namespace.  Thus the amount of
    work is related to the number of shm segments in the namespace not the
    number of segments that might need to be cleaned.
    
    In addition, this occurs after the task has been notified the thread has
    exited, so the number of tasks waiting for the ns shm rwsem can grow
    without bound until memory is exausted.
    
    Add a list to the task struct of all shmids allocated by this task.  Init
    the list head in copy_process.  Use the ns->rwsem for locking.  Add
    segments after id is added, remove before removing from id.
    
    On unshare of NEW_IPCNS orphan any ids as if the task had exited, similar
    to handling of semaphore undo.
    
    I chose a define for the init sequence since its a simple list init,
    otherwise it would require a function call to avoid include loops between
    the semaphore code and the task struct.  Converting the list_del to
    list_del_init for the unshare cases would remove the exit followed by
    init, but I left it blow up if not inited.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Jack Miller <millerjo@us.ibm.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b21e9218c0fd..db2f6474e95e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -33,6 +33,7 @@ struct sched_param {
 
 #include <linux/smp.h>
 #include <linux/sem.h>
+#include <linux/shm.h>
 #include <linux/signal.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
@@ -1385,6 +1386,7 @@ struct task_struct {
 #ifdef CONFIG_SYSVIPC
 /* ipc stuff */
 	struct sysv_sem sysvsem;
+	struct sysv_shm sysvshm;
 #endif
 #ifdef CONFIG_DETECT_HUNG_TASK
 /* hung task detection */

commit 33144e8429bd7fceacbb869a7f5061db42e13fe6
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Aug 8 14:22:03 2014 -0700

    kernel/fork.c: make mm_init_owner static
    
    It's only used in fork.c:mm_init().
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4fcf82a4d243..b21e9218c0fd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2961,15 +2961,10 @@ static inline void inc_syscw(struct task_struct *tsk)
 
 #ifdef CONFIG_MEMCG
 extern void mm_update_next_owner(struct mm_struct *mm);
-extern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);
 #else
 static inline void mm_update_next_owner(struct mm_struct *mm)
 {
 }
-
-static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
-{
-}
 #endif /* CONFIG_MEMCG */
 
 static inline unsigned long task_rlimit(const struct task_struct *tsk,

commit 747db954cab64c6b7a95b121b517165f34751898
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Aug 8 14:19:24 2014 -0700

    mm: memcontrol: use page lists for uncharge batching
    
    Pages are now uncharged at release time, and all sources of batched
    uncharges operate on lists of pages.  Directly use those lists, and
    get rid of the per-task batching state.
    
    This also batches statistics accounting, in addition to the res
    counter charges, to reduce IRQ-disabling and re-enabling.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7c19d552dc3f..4fcf82a4d243 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1628,12 +1628,6 @@ struct task_struct {
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
 #ifdef CONFIG_MEMCG /* memcg uses this to do batch job */
-	struct memcg_batch_info {
-		int do_batch;	/* incremented when batch uncharge started */
-		struct mem_cgroup *memcg; /* target memcg of uncharge */
-		unsigned long nr_pages;	/* uncharged usage */
-		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
-	} memcg_batch;
 	unsigned int memcg_kmem_skip_account;
 	struct memcg_oom_info {
 		struct mem_cgroup *memcg;

commit 372ba8cb46b271a7662b92cbefedee56725f6bd0
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 14:19:21 2014 +0100

    cpuidle: menu: Lookup CPU runqueues less
    
    The menu governer makes separate lookups of the CPU runqueue to get
    load and number of IO waiters but it can be done with a single lookup.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..641bd954bb5d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -168,8 +168,7 @@ extern int nr_processes(void);
 extern unsigned long nr_running(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
-extern unsigned long this_cpu_load(void);
-
+extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);

commit bb2cbf5e9367d8598fecd0c48dead69560750223
Merge: e7fda6c4c3c1 478d085524c5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 6 08:06:39 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "In this release:
    
       - PKCS#7 parser for the key management subsystem from David Howells
       - appoint Kees Cook as seccomp maintainer
       - bugfixes and general maintenance across the subsystem"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (94 commits)
      X.509: Need to export x509_request_asymmetric_key()
      netlabel: shorter names for the NetLabel catmap funcs/structs
      netlabel: fix the catmap walking functions
      netlabel: fix the horribly broken catmap functions
      netlabel: fix a problem when setting bits below the previously lowest bit
      PKCS#7: X.509 certificate issuer and subject are mandatory fields in the ASN.1
      tpm: simplify code by using %*phN specifier
      tpm: Provide a generic means to override the chip returned timeouts
      tpm: missing tpm_chip_put in tpm_get_random()
      tpm: Properly clean sysfs entries in error path
      tpm: Add missing tpm_do_selftest to ST33 I2C driver
      PKCS#7: Use x509_request_asymmetric_key()
      Revert "selinux: fix the default socket labeling in sock_graft()"
      X.509: x509_request_asymmetric_keys() doesn't need string length arguments
      PKCS#7: fix sparse non static symbol warning
      KEYS: revert encrypted key change
      ima: add support for measuring and appraising firmware
      firmware_class: perform new LSM checks
      security: introduce kernel_fw_from_file hook
      PKCS#7: Missing inclusion of linux/err.h
      ...

commit 72f15c03977acc8f06080e6c8a91d93bfc655a65
Author: Richard Weinberger <richard@nod.at>
Date:   Wed Mar 5 15:15:22 2014 +0100

    sas_ss_flags: Remove nested ternary if
    
    ...to make it readable.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0376b054a0d0..795ea2bc3d4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2360,8 +2360,10 @@ static inline int on_sig_stack(unsigned long sp)
 
 static inline int sas_ss_flags(unsigned long sp)
 {
-	return (current->sas_ss_size == 0 ? SS_DISABLE
-		: on_sig_stack(sp) ? SS_ONSTACK : 0);
+	if (!current->sas_ss_size)
+		return SS_DISABLE;
+
+	return on_sig_stack(sp) ? SS_ONSTACK : 0;
 }
 
 static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)

commit e7fda6c4c3c1a7d6996dd75fd84670fa0b5d448f
Merge: 08d69a257144 953dec21aed4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 5 17:46:42 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer and time updates from Thomas Gleixner:
     "A rather large update of timers, timekeeping & co
    
       - Core timekeeping code is year-2038 safe now for 32bit machines.
         Now we just need to fix all in kernel users and the gazillion of
         user space interfaces which rely on timespec/timeval :)
    
       - Better cache layout for the timekeeping internal data structures.
    
       - Proper nanosecond based interfaces for in kernel users.
    
       - Tree wide cleanup of code which wants nanoseconds but does hoops
         and loops to convert back and forth from timespecs.  Some of it
         definitely belongs into the ugly code museum.
    
       - Consolidation of the timekeeping interface zoo.
    
       - A fast NMI safe accessor to clock monotonic for tracing.  This is a
         long standing request to support correlated user/kernel space
         traces.  With proper NTP frequency correction it's also suitable
         for correlation of traces accross separate machines.
    
       - Checkpoint/restart support for timerfd.
    
       - A few NOHZ[_FULL] improvements in the [hr]timer code.
    
       - Code move from kernel to kernel/time of all time* related code.
    
       - New clocksource/event drivers from the ARM universe.  I'm really
         impressed that despite an architected timer in the newer chips SoC
         manufacturers insist on inventing new and differently broken SoC
         specific timers.
    
    [ Ed. "Impressed"? I don't think that word means what you think it means ]
    
       - Another round of code move from arch to drivers.  Looks like most
         of the legacy mess in ARM regarding timers is sorted out except for
         a few obnoxious strongholds.
    
       - The usual updates and fixlets all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      timekeeping: Fixup typo in update_vsyscall_old definition
      clocksource: document some basic timekeeping concepts
      timekeeping: Use cached ntp_tick_length when accumulating error
      timekeeping: Rework frequency adjustments to work better w/ nohz
      timekeeping: Minor fixup for timespec64->timespec assignment
      ftrace: Provide trace clocks monotonic
      timekeeping: Provide fast and NMI safe access to CLOCK_MONOTONIC
      seqcount: Add raw_write_seqcount_latch()
      seqcount: Provide raw_read_seqcount()
      timekeeping: Use tk_read_base as argument for timekeeping_get_ns()
      timekeeping: Create struct tk_read_base and use it in struct timekeeper
      timekeeping: Restructure the timekeeper some more
      clocksource: Get rid of cycle_last
      clocksource: Move cycle_last validation to core code
      clocksource: Make delta calculation a function
      wireless: ath9k: Get rid of timespec conversions
      drm: vmwgfx: Use nsec based interfaces
      drm: i915: Use nsec based interfaces
      timekeeping: Provide ktime_get_raw()
      hangcheck-timer: Use ktime_get_ns()
      ...

commit 98959948a7ba33cf8c708626e0d2a1456397e1c6
Merge: ef35ad26f8ff cd3bd4e628a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 16:23:30 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Move the nohz kick code out of the scheduler tick to a dedicated IPI,
       from Frederic Weisbecker.
    
      This necessiated quite some background infrastructure rework,
      including:
    
       * Clean up some irq-work internals
       * Implement remote irq-work
       * Implement nohz kick on top of remote irq-work
       * Move full dynticks timer enqueue notification to new kick
       * Move multi-task notification to new kick
       * Remove unecessary barriers on multi-task notification
    
     - Remove proliferation of wait_on_bit() action functions and allow
       wait_on_bit_action() functions to support a timeout.  (Neil Brown)
    
     - Another round of sched/numa improvements, cleanups and fixes.  (Rik
       van Riel)
    
     - Implement fast idling of CPUs when the system is partially loaded,
       for better scalability.  (Tim Chen)
    
     - Restructure and fix the CPU hotplug handling code that may leave
       cfs_rq and rt_rq's throttled when tasks are migrated away from a dead
       cpu.  (Kirill Tkhai)
    
     - Robustify the sched topology setup code.  (Peterz Zijlstra)
    
     - Improve sched_feat() handling wrt.  static_keys (Jason Baron)
    
     - Misc fixes.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      sched/fair: Fix 'make xmldocs' warning caused by missing description
      sched: Use macro for magic number of -1 for setparam
      sched: Robustify topology setup
      sched: Fix sched_setparam() policy == -1 logic
      sched: Allow wait_on_bit_action() functions to support a timeout
      sched: Remove proliferation of wait_on_bit() action functions
      sched/numa: Revert "Use effective_load() to balance NUMA loads"
      sched: Fix static_key race with sched_feat()
      sched: Remove extra static_key*() function indirection
      sched/rt: Fix replenish_dl_entity() comments to match the current upstream code
      sched: Transform resched_task() into resched_curr()
      sched/deadline: Kill task_struct->pi_top_task
      sched: Rework check_for_tasks()
      sched/rt: Enqueue just unthrottled rt_rq back on the stack in __disable_runtime()
      sched/fair: Disable runtime_enabled on dying rq
      sched/numa: Change scan period code to match intent
      sched/numa: Rework best node setting in task_numa_migrate()
      sched/numa: Examine a task move when examining a task swap
      sched/numa: Simplify task_numa_compare()
      sched/numa: Use effective_load() to balance NUMA loads
      ...

commit 5bda4f638f36ef4c4e3b1397b02affc3db94356e
Merge: a45c657f28f8 01c9db827146
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 15:55:08 2014 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molar:
     "The main changes:
    
       - torture-test updates
       - callback-offloading changes
       - maintainership changes
       - update RCU documentation
       - miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (32 commits)
      rcu: Allow for NULL tick_nohz_full_mask when nohz_full= missing
      rcu: Fix a sparse warning in rcu_report_unblock_qs_rnp()
      rcu: Fix a sparse warning in rcu_initiate_boost()
      rcu: Fix __rcu_reclaim() to use true/false for bool
      rcu: Remove CONFIG_PROVE_RCU_DELAY
      rcu: Use __this_cpu_read() instead of per_cpu_ptr()
      rcu: Don't use NMIs to dump other CPUs' stacks
      rcu: Bind grace-period kthreads to non-NO_HZ_FULL CPUs
      rcu: Simplify priority boosting by putting rt_mutex in rcu_node
      rcu: Check both root and current rcu_node when setting up future grace period
      rcu: Allow post-unlock reference for rt_mutex
      rcu: Loosen __call_rcu()'s rcu_head alignment constraint
      rcu: Eliminate read-modify-write ACCESS_ONCE() calls
      rcu: Remove redundant ACCESS_ONCE() from tick_do_timer_cpu
      rcu: Make rcu node arrays static const char * const
      signal: Explain local_irq_save() call
      rcu: Handle obsolete references to TINY_PREEMPT_RCU
      rcu: Document deadlock-avoidance information for rcu_read_unlock()
      scripts: Teach get_maintainer.pl about the new "R:" tag
      rcu: Update rcu torture maintainership filename patterns
      ...

commit ca5bc6cd5de5b53eb8fd6fea39aa3fe2a1e8c3d9
Merge: c1221321b7c2 d8d28c8f00e8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jul 28 10:03:00 2014 +0200

    Merge branch 'sched/urgent' into sched/core, to merge fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9667a23db0dc0bd4892f0ada7e4e71528eaeed62
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:35 2014 +0000

    delayacct: Make accounting nanosecond based
    
    Kill the timespec juggling and calculate with plain nanoseconds.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 10c6e829927f..653744ae8d27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -813,7 +813,7 @@ struct task_delay_info {
 	 * associated with the operation is added to XXX_delay.
 	 * XXX_delay contains the accumulated delay time in nanoseconds.
 	 */
-	struct timespec blkio_start, blkio_end;	/* Shared by blkio, swapin */
+	u64 blkio_start;	/* Shared by blkio, swapin */
 	u64 blkio_delay;	/* wait for sync block io completion */
 	u64 swapin_delay;	/* wait for swapin block io completion */
 	u32 blkio_count;	/* total count of the number of sync block */
@@ -821,7 +821,7 @@ struct task_delay_info {
 	u32 swapin_count;	/* total count of the number of swapin block */
 				/* io operations performed */
 
-	struct timespec freepages_start, freepages_end;
+	u64 freepages_start;
 	u64 freepages_delay;	/* wait for memory reclaim */
 	u32 freepages_count;	/* total count of memory reclaim */
 };

commit ccbf62d8a284cf181ac28c8e8407dd077d90dd4b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:34 2014 +0000

    sched: Make task->start_time nanoseconds based
    
    Simplify the timespec to nsec/usec conversions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67678fa76f99..10c6e829927f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1367,7 +1367,7 @@ struct task_struct {
 	} vtime_snap_whence;
 #endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
-	struct timespec start_time; 		/* monotonic time */
+	u64 start_time;		/* monotonic time in nsec */
 	u64 real_start_time;	/* boot based time in nsec */
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;

commit 57e0be041d9e21a7397eed3b67a7936ac4ac83c0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:32 2014 +0000

    sched: Make task->real_start_time nanoseconds based
    
    Simplify the only user of this data by removing the timespec
    conversion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..67678fa76f99 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1368,7 +1368,7 @@ struct task_struct {
 #endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
-	struct timespec real_start_time;	/* boot based time */
+	u64 real_start_time;	/* boot based time in nsec */
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 

commit 1d4457f99928a968767f6405b4a1f50845aa15fd
Author: Kees Cook <keescook@chromium.org>
Date:   Wed May 21 15:23:46 2014 -0700

    sched: move no_new_privs into new atomic flags
    
    Since seccomp transitions between threads requires updates to the
    no_new_privs flag to be atomic, the flag must be part of an atomic flag
    set. This moves the nnp flag into a separate task field, and introduces
    accessors.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..0fd19055bb64 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1307,13 +1307,12 @@ struct task_struct {
 				 * execve */
 	unsigned in_iowait:1;
 
-	/* task may not gain privileges */
-	unsigned no_new_privs:1;
-
 	/* Revert to default priority/policy when forking */
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 
+	unsigned long atomic_flags; /* Flags needing atomic access. */
+
 	pid_t pid;
 	pid_t tgid;
 
@@ -1967,6 +1966,19 @@ static inline void memalloc_noio_restore(unsigned int flags)
 	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
 }
 
+/* Per-process atomic flags. */
+#define PFA_NO_NEW_PRIVS 0x00000001	/* May not gain new privileges. */
+
+static inline bool task_no_new_privs(struct task_struct *p)
+{
+	return test_bit(PFA_NO_NEW_PRIVS, &p->atomic_flags);
+}
+
+static inline void task_set_no_new_privs(struct task_struct *p)
+{
+	set_bit(PFA_NO_NEW_PRIVS, &p->atomic_flags);
+}
+
 /*
  * task->jobctl flags
  */

commit 8875125efe8402c4d84b08291e68f1281baba8e2
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Sun Jun 29 00:03:57 2014 +0400

    sched: Transform resched_task() into resched_curr()
    
    We always use resched_task() with rq->curr argument.
    It's not possible to reschedule any task but rq's current.
    
    The patch introduces resched_curr(struct rq *) to
    replace all of the repeating patterns. The main aim
    is cleanup, but there is a little size profit too:
    
      (before)
            $ size kernel/sched/built-in.o
               text    data     bss     dec     hex filename
            155274    16445    7042  178761   2ba49 kernel/sched/built-in.o
    
            $ size vmlinux
               text    data     bss     dec     hex filename
            7411490 1178376  991232 9581098  92322a vmlinux
    
      (after)
            $ size kernel/sched/built-in.o
               text    data     bss     dec     hex filename
            155130    16445    7042  178617   2b9b9 kernel/sched/built-in.o
    
            $ size vmlinux
               text    data     bss     dec     hex filename
            7411362 1178376  991232 9580970  9231aa vmlinux
    
            I was choosing between resched_curr() and resched_rq(),
            and the first name looks better for me.
    
    A little lie in Documentation/trace/ftrace.txt. I have not
    actually collected the tracing again. With a hope the patch
    won't make execution times much worse :)
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140628200219.1778.18735.stgit@localhost
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c9c9ff723525..41a195385081 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2786,7 +2786,7 @@ static inline bool __must_check current_set_polling_and_test(void)
 
 	/*
 	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_task()
+	 * paired by resched_curr()
 	 */
 	smp_mb__after_atomic();
 
@@ -2804,7 +2804,7 @@ static inline bool __must_check current_clr_polling_and_test(void)
 
 	/*
 	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_task()
+	 * paired by resched_curr()
 	 */
 	smp_mb__after_atomic();
 
@@ -2836,7 +2836,7 @@ static inline void current_clr_polling(void)
 	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
 	 * fold.
 	 */
-	smp_mb(); /* paired with resched_task() */
+	smp_mb(); /* paired with resched_curr() */
 
 	preempt_fold_need_resched();
 }

commit 466af29bf4270e84261712428a1304c28e3743fa
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 6 18:52:06 2014 +0200

    sched/deadline: Kill task_struct->pi_top_task
    
    Remove task_struct->pi_top_task. The only user, rt_mutex_setprio(),
    can use a local.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: Dario Faggioli <raistlin@linux.it>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Dempsky <mdempsky@chromium.org>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140606165206.GB29465@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..c9c9ff723525 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1440,8 +1440,6 @@ struct task_struct {
 	struct rb_node *pi_waiters_leftmost;
 	/* Deadlock detection and priority inheritance handling */
 	struct rt_mutex_waiter *pi_blocked_on;
-	/* Top pi_waiters task */
-	struct task_struct *pi_top_task;
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES

commit abaa93d9e1de2c29297e69ddba8ddd38f15064cf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 12 13:30:25 2014 -0700

    rcu: Simplify priority boosting by putting rt_mutex in rcu_node
    
    RCU priority boosting currently checks for boosting via a pointer in
    task_struct.  However, this is not needed: As Oleg noted, if the
    rt_mutex is placed in the rcu_node instead of on the booster's stack,
    the boostee can simply check it see if it owns the lock.  This commit
    makes this change, shrinking task_struct by one pointer and the kernel
    by thirteen lines.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..3cfbc05e66e6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1270,9 +1270,6 @@ struct task_struct {
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
-#ifdef CONFIG_RCU_BOOST
-	struct rt_mutex *rcu_boost_mutex;
-#endif /* #ifdef CONFIG_RCU_BOOST */
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
@@ -2009,9 +2006,6 @@ static inline void rcu_copy_process(struct task_struct *p)
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	p->rcu_blocked_node = NULL;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
-#ifdef CONFIG_RCU_BOOST
-	p->rcu_boost_mutex = NULL;
-#endif /* #ifdef CONFIG_RCU_BOOST */
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 

commit b6220ad66bcd4a50737eb3c08e9466aa44f3bc98
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Tue Jun 24 18:05:29 2014 -0700

    sched: Fix compiler warnings
    
    Commit 143e1e28cb (sched: Rework sched_domain topology definition)
    introduced a number of functions with a return value of 'const int'.
    gcc doesn't know what to do with that and, if the kernel is compiled
    with W=1, complains with the following warnings whenever sched.h
    is included.
    
      include/linux/sched.h:875:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:882:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:889:25: warning: type qualifiers ignored on function return type
      include/linux/sched.h:1002:21: warning: type qualifiers ignored on function return type
    
    Commits fb2aa855 (sched, ARM: Create a dedicated scheduler topology table)
    and 607b45e9a (sched, powerpc: Create a dedicated topology table) introduce
    the same warning in the arm and powerpc code.
    
    Drop 'const' from the function declarations to fix the problem.
    
    The fix for all three patches has to be applied together to avoid
    compilation failures for the affected architectures.
    
    Acked-by: Vincent Guittot <vincent.guittot@linaro.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1403658329-13196-1-git-send-email-linux@roeck-us.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 306f4f0c987a..0376b054a0d0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -872,21 +872,21 @@ enum cpu_idle_type {
 #define SD_NUMA			0x4000	/* cross-node balancing */
 
 #ifdef CONFIG_SCHED_SMT
-static inline const int cpu_smt_flags(void)
+static inline int cpu_smt_flags(void)
 {
 	return SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
 }
 #endif
 
 #ifdef CONFIG_SCHED_MC
-static inline const int cpu_core_flags(void)
+static inline int cpu_core_flags(void)
 {
 	return SD_SHARE_PKG_RESOURCES;
 }
 #endif
 
 #ifdef CONFIG_NUMA
-static inline const int cpu_numa_flags(void)
+static inline int cpu_numa_flags(void)
 {
 	return SD_NUMA;
 }
@@ -999,7 +999,7 @@ void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
 bool cpus_share_cache(int this_cpu, int that_cpu);
 
 typedef const struct cpumask *(*sched_domain_mask_f)(int cpu);
-typedef const int (*sched_domain_flags_f)(void);
+typedef int (*sched_domain_flags_f)(void);
 
 #define SDTL_OVERLAP	0x01
 

commit b2e09f633a3994ee97fa6bc734b533d9c8e6ea0f
Merge: 3737a1276163 535560d841b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:42:15 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more scheduler updates from Ingo Molnar:
     "Second round of scheduler changes:
       - try-to-wakeup and IPI reduction speedups, from Andy Lutomirski
       - continued power scheduling cleanups and refactorings, from Nicolas
         Pitre
       - misc fixes and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/deadline: Delete extraneous extern for to_ratio()
      sched/idle: Optimize try-to-wake-up IPI
      sched/idle: Simplify wake_up_idle_cpu()
      sched/idle: Clear polling before descheduling the idle thread
      sched, trace: Add a tracepoint for IPI-less remote wakeups
      cpuidle: Set polling in poll_idle
      sched: Remove redundant assignment to "rt_rq" in update_curr_rt(...)
      sched: Rename capacity related flags
      sched: Final power vs. capacity cleanups
      sched: Remove remaining dubious usage of "power"
      sched: Let 'struct sched_group_power' care about CPU capacity
      sched/fair: Disambiguate existing/remaining "capacity" usage
      sched/fair: Change "has_capacity" to "has_free_capacity"
      sched/fair: Remove "power" from 'struct numa_stats'
      sched: Fix signedness bug in yield_to()
      sched/fair: Use time_after() in record_wakee()
      sched/balancing: Reduce the rate of needless idle load balancing
      sched/fair: Fix unlocked reads of some cfs_b->quota/period

commit 3737a12761636ebde0f09ef49daebb8eed18cc8a
Merge: c29deef32e36 82b897782d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:18:49 2014 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more perf updates from Ingo Molnar:
     "A second round of perf updates:
    
       - wide reaching kprobes sanitization and robustization, with the hope
         of fixing all 'probe this function crashes the kernel' bugs, by
         Masami Hiramatsu.
    
       - uprobes updates from Oleg Nesterov: tmpfs support, corner case
         fixes and robustization work.
    
       - perf tooling updates and fixes from Jiri Olsa, Namhyung Ki, Arnaldo
         et al:
            * Add support to accumulate hist periods (Namhyung Kim)
            * various fixes, refactorings and enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      perf: Differentiate exec() and non-exec() comm events
      perf: Fix perf_event_comm() vs. exec() assumption
      uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates
      perf/documentation: Add description for conditional branch filter
      perf/x86: Add conditional branch filtering support
      perf/tool: Add conditional branch filter 'cond' to perf record
      perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
      uprobes: Teach copy_insn() to support tmpfs
      uprobes: Shift ->readpage check from __copy_insn() to uprobe_register()
      perf/x86: Use common PMU interrupt disabled code
      perf/ARM: Use common PMU interrupt disabled code
      perf: Disable sampled events if no PMU interrupt
      perf: Fix use after free in perf_remove_from_context()
      perf tools: Fix 'make help' message error
      perf record: Fix poll return value propagation
      perf tools: Move elide bool into perf_hpp_fmt struct
      perf tools: Remove elide setup for SORT_MODE__MEMORY mode
      perf tools: Fix "==" into "=" in ui_browser__warning assignment
      perf tools: Allow overriding sysfs and proc finding with env var
      perf tools: Consider header files outside perf directory in tags target
      ...

commit 535560d841b2d54f31280e05e9c6ffd19da0c4e7
Merge: f602d0632755 3cf2f34e1a3d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Jun 12 13:46:25 2014 +0200

    Merge commit '3cf2f34' into sched/core, to fix build error
    
    Fix this dependency on the locking tree's smp_mb*() API changes:
    
      kernel/sched/idle.c:247:3: error: implicit declaration of function â€˜smp_mb__after_atomicâ€™ [-Werror=implicit-function-declaration]
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0341729b4b832e753c5e745c6ba0e797f6198be0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 6 14:36:53 2014 -0700

    signals: mv {dis,}allow_signal() from sched.h/exit.c to signal.[ch]
    
    Move the declaration/definition of allow_signal/disallow_signal to
    signal.h/signal.c.  The new place is more logical and allows to use the
    static helpers in signal.c (see the next changes).
    
    While at it, make them return void and remove the valid_signal() check.
    Nobody checks the returned value, and in-kernel users must not pass the
    wrong signal number.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8fcd0e6098d9..ea74596014a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2414,9 +2414,6 @@ extern void flush_itimer_signals(void);
 
 extern void do_group_exit(int);
 
-extern int allow_signal(int);
-extern int disallow_signal(int);
-
 extern int do_execve(struct filename *,
 		     const char __user * const __user *,
 		     const char __user * const __user *);

commit 82b897782d10fcc4930c9d4a15b175348fdd2871
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Wed May 28 11:45:04 2014 +0300

    perf: Differentiate exec() and non-exec() comm events
    
    perf tools like 'perf report' can aggregate samples by comm strings,
    which generally works.  However, there are other potential use-cases.
    For example, to pair up 'calls' with 'returns' accurately (from branch
    events like Intel BTS) it is necessary to identify whether the process
    has exec'd.  Although a comm event is generated when an 'exec' happens
    it is also generated whenever the comm string is changed on a whim
    (e.g. by prctl PR_SET_NAME).  This patch adds a flag to the comm event
    to differentiate one case from the other.
    
    In order to determine whether the kernel supports the new flag, a
    selection bit named 'exec' is added to struct perf_event_attr.  The
    bit does nothing but will cause perf_event_open() to fail if the bit
    is set on kernels that do not have it defined.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/537D9EBE.7030806@intel.com
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 221b2bde3723..ad86e1d7dbc2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2379,7 +2379,11 @@ extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, i
 struct task_struct *fork_idle(int);
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 
-extern void set_task_comm(struct task_struct *tsk, const char *from);
+extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
+static inline void set_task_comm(struct task_struct *tsk, const char *from)
+{
+	__set_task_comm(tsk, from, false);
+}
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP

commit 5d4dfddd4f02b028d6ddaaa04d75d3b0cad1c9ae
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue May 27 13:50:41 2014 -0400

    sched: Rename capacity related flags
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    Let's rename the following feature flags since they do relate to capacity:
    
            SD_SHARE_CPUPOWER  -> SD_SHARE_CPUCAPACITY
            ARCH_POWER         -> ARCH_CAPACITY
            NONTASK_POWER      -> NONTASK_CAPACITY
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Andy Fleming <afleming@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: devicetree@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/n/tip-e93lpnxb87owfievqatey6b5@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 322110affe63..ce93768a3312 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -869,7 +869,7 @@ enum cpu_idle_type {
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-#define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
+#define SD_SHARE_CPUCAPACITY	0x0080	/* Domain members share cpu power */
 #define SD_SHARE_POWERDOMAIN	0x0100	/* Domain members share power domain */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
@@ -881,7 +881,7 @@ enum cpu_idle_type {
 #ifdef CONFIG_SCHED_SMT
 static inline const int cpu_smt_flags(void)
 {
-	return SD_SHARE_CPUPOWER | SD_SHARE_PKG_RESOURCES;
+	return SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
 }
 #endif
 

commit ca8ce3d0b144c318a5a9ce99649053e9029061ea
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon May 26 18:19:39 2014 -0400

    sched: Final power vs. capacity cleanups
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    This contains the architecture visible changes.  Incidentally, only ARM
    takes advantage of the available pow^H^H^Hcapacity scaling hooks and
    therefore those changes outside kernel/sched/ are confined to one ARM
    specific file.  The default arch_scale_smt_power() hook is not overridden
    by anyone.
    
    Replacements are as follows:
    
            arch_scale_freq_power  --> arch_scale_freq_capacity
            arch_scale_smt_power   --> arch_scale_smt_capacity
            SCHED_POWER_SCALE      --> SCHED_CAPACITY_SCALE
            SCHED_POWER_SHIFT      --> SCHED_CAPACITY_SHIFT
    
    The local usage of "power" in arch/arm/kernel/topology.c is also changed
    to "capacity" as appropriate.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Brown <broonie@linaro.org>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: devicetree@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-48zba9qbznvglwelgq2cfygh@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a96f03598c61..322110affe63 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -854,10 +854,10 @@ enum cpu_idle_type {
 };
 
 /*
- * Increase resolution of cpu_power calculations
+ * Increase resolution of cpu_capacity calculations
  */
-#define SCHED_POWER_SHIFT	10
-#define SCHED_POWER_SCALE	(1L << SCHED_POWER_SHIFT)
+#define SCHED_CAPACITY_SHIFT	10
+#define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
 
 /*
  * sched-domains (multiprocessor balancing) declarations:

commit 63b2ca30bdb3dbf60bc7ac5f46713c0d32308261
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon May 26 18:19:37 2014 -0400

    sched: Let 'struct sched_group_power' care about CPU capacity
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    Since struct sched_group_power is really about compute capacity of sched
    groups, let's rename it to struct sched_group_capacity. Similarly sgp
    becomes sgc. Related variables and functions dealing with groups are also
    adjusted accordingly.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-5yeix833vvgf2uyj5o36hpu9@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6790c3b42072..a96f03598c61 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1013,7 +1013,7 @@ typedef const int (*sched_domain_flags_f)(void);
 struct sd_data {
 	struct sched_domain **__percpu sd;
 	struct sched_group **__percpu sg;
-	struct sched_group_power **__percpu sgp;
+	struct sched_group_capacity **__percpu sgc;
 };
 
 struct sched_domain_topology_level {

commit fa93384f40deeb294fd29f2fdcadbd0ebc2dedf1
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri May 23 13:20:42 2014 +0300

    sched: Fix signedness bug in yield_to()
    
    yield_to() is supposed to return -ESRCH if there is no task to
    yield to, but because the type is bool that is the same as returning
    true.
    
    The only place I see which cares is kvm_vcpu_on_spin().
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Raghavendra <raghavendra.kt@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: kvm@vger.kernel.org
    Link: http://lkml.kernel.org/r/20140523102042.GA7267@mwanda
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0f91d00efd87..6790c3b42072 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2180,7 +2180,7 @@ static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 #endif
 
-extern bool yield_to(struct task_struct *p, bool preempt);
+extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
 /**

commit 34a1b7236ad6113883f6c448d1da854cad60265e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:12:19 2014 -0700

    kthreads: kill CLONE_KERNEL, change kernel_thread(kernel_init) to avoid CLONE_SIGHAND
    
    1. Remove CLONE_KERNEL, it has no users and it is dangerous.
    
       The (old) comment says "List of flags we want to share for kernel
       threads" but this is not true, we do not want to share ->sighand by
       default. This flag can only be used if the caller is sure that both
       parent/child will never play with signals (say, allow_signal/etc).
    
    2. Change rest_init() to clone kernel_init() without CLONE_SIGHAND.
    
       In this case CLONE_SIGHAND does not really hurt, and it looks like
       optimization because copy_sighand() can avoid kmem_cache_alloc().
    
       But in fact this only adds the minor pessimization. kernel_init()
       is going to exec the init process, and de_thread() will need to
       unshare ->sighand and do kmem_cache_alloc(sighand_cachep) anyway,
       but it needs to do more work and take tasklist_lock and siglock.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 611676fd4c2c..8fcd0e6098d9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -136,12 +136,6 @@ struct filename;
 #define VMACACHE_SIZE (1U << VMACACHE_BITS)
 #define VMACACHE_MASK (VMACACHE_SIZE - 1)
 
-/*
- * List of flags we want to share for kernel threads,
- * if only because they are not used by them anyway.
- */
-#define CLONE_KERNEL	(CLONE_FS | CLONE_FILES | CLONE_SIGHAND)
-
 /*
  * These are the constant used to fake the fixed-point load-average
  * counting. Some notes:

commit b300a4ea665f7fa44f015616ac1874deca891c5e
Author: Kirill A. Shutemov <kirill@shutemov.name>
Date:   Wed Jun 4 16:11:27 2014 -0700

    kernel/user.c: drop unused field 'files' from user_struct
    
    Nobody seems uses it for a long time. Let's drop it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2f2dd7d932a2..611676fd4c2c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -745,7 +745,6 @@ static inline int signal_group_exit(const struct signal_struct *sig)
 struct user_struct {
 	atomic_t __count;	/* reference count */
 	atomic_t processes;	/* How many processes does this user have? */
-	atomic_t files;		/* How many open files does this user have? */
 	atomic_t sigpending;	/* How many pending signals does this user have? */
 #ifdef CONFIG_INOTIFY_USER
 	atomic_t inotify_watches; /* How many inotify watches does this user have? */

commit f98bafa06a28fdfdd5c49f820f4d6560f636fc46
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:07:34 2014 -0700

    memcg: kill CONFIG_MM_OWNER
    
    CONFIG_MM_OWNER makes no sense.  It is not user-selectable, it is only
    selected by CONFIG_MEMCG automatically.  So we can kill this option in
    init/Kconfig and do s/CONFIG_MM_OWNER/CONFIG_MEMCG/ globally.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 70f67e4e6156..2f2dd7d932a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2967,7 +2967,7 @@ static inline void inc_syscw(struct task_struct *tsk)
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
-#ifdef CONFIG_MM_OWNER
+#ifdef CONFIG_MEMCG
 extern void mm_update_next_owner(struct mm_struct *mm);
 extern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);
 #else
@@ -2978,7 +2978,7 @@ static inline void mm_update_next_owner(struct mm_struct *mm)
 static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 {
 }
-#endif /* CONFIG_MM_OWNER */
+#endif /* CONFIG_MEMCG */
 
 static inline unsigned long task_rlimit(const struct task_struct *tsk,
 		unsigned int limit)

commit c84a1e32ee58fc1cc9d3fd42619b917cce67e30a
Merge: 3d521f9151da 096aa33863a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 3 14:00:15 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into next
    
    Pull scheduler updates from Ingo Molnar:
     "The main scheduling related changes in this cycle were:
    
       - various sched/numa updates, for better performance
    
       - tree wide cleanup of open coded nice levels
    
       - nohz fix related to rq->nr_running use
    
       - cpuidle changes and continued consolidation to improve the
         kernel/sched/idle.c high level idle scheduling logic.  As part of
         this effort I pulled cpuidle driver changes from Rafael as well.
    
       - standardized idle polling amongst architectures
    
       - continued work on preparing better power/energy aware scheduling
    
       - sched/rt updates
    
       - misc fixlets and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (49 commits)
      sched/numa: Decay ->wakee_flips instead of zeroing
      sched/numa: Update migrate_improves/degrades_locality()
      sched/numa: Allow task switch if load imbalance improves
      sched/rt: Fix 'struct sched_dl_entity' and dl_task_time() comments, to match the current upstream code
      sched: Consolidate open coded implementations of nice level frobbing into nice_to_rlimit() and rlimit_to_nice()
      sched: Initialize rq->age_stamp on processor start
      sched, nohz: Change rq->nr_running to always use wrappers
      sched: Fix the rq->next_balance logic in rebalance_domains() and idle_balance()
      sched: Use clamp() and clamp_val() to make sys_nice() more readable
      sched: Do not zero sg->cpumask and sg->sgp->power in build_sched_groups()
      sched/numa: Fix initialization of sched_domain_topology for NUMA
      sched: Call select_idle_sibling() when not affine_sd
      sched: Simplify return logic in sched_read_attr()
      sched: Simplify return logic in sched_copy_attr()
      sched: Fix exec_start/task_hot on migrated tasks
      arm64: Remove TIF_POLLING_NRFLAG
      metag: Remove TIF_POLLING_NRFLAG
      sched/idle: Make cpuidle_idle_call() void
      sched/idle: Reflow cpuidle_idle_call()
      sched/idle: Delay clearing the polling bit
      ...

commit 776edb59317ada867dfcddde40b55648beeb0078
Merge: 59a3d4c3631e 3cf2f34e1a3d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 3 12:57:53 2014 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into next
    
    Pull core locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - reduced/streamlined smp_mb__*() interface that allows more usecases
         and makes the existing ones less buggy, especially in rarer
         architectures
    
       - add rwsem implementation comments
    
       - bump up lockdep limits"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      rwsem: Add comments to explain the meaning of the rwsem's count field
      lockdep: Increase static allocations
      arch: Mass conversion of smp_mb__*()
      arch,doc: Convert smp_mb__*()
      arch,xtensa: Convert smp_mb__*()
      arch,x86: Convert smp_mb__*()
      arch,tile: Convert smp_mb__*()
      arch,sparc: Convert smp_mb__*()
      arch,sh: Convert smp_mb__*()
      arch,score: Convert smp_mb__*()
      arch,s390: Convert smp_mb__*()
      arch,powerpc: Convert smp_mb__*()
      arch,parisc: Convert smp_mb__*()
      arch,openrisc: Convert smp_mb__*()
      arch,mn10300: Convert smp_mb__*()
      arch,mips: Convert smp_mb__*()
      arch,metag: Convert smp_mb__*()
      arch,m68k: Convert smp_mb__*()
      arch,m32r: Convert smp_mb__*()
      arch,ia64: Convert smp_mb__*()
      ...

commit f02f79dbcb9c0326588c1cbe24b40887737e71d3
Merge: e6a32c3ad1e7 2b4cfe64dee0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 23 10:04:04 2014 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "The biggest commit is an irqtime accounting loop latency fix, the rest
      are misc fixes all over the place: deadline scheduling, docs, numa,
      balancer and a bad to-idle latency fix"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/numa: Initialize newidle balance stats in sd_numa_init()
      sched: Fix updating rq->max_idle_balance_cost and rq->next_balance in idle_balance()
      sched: Skip double execution of pick_next_task_fair()
      sched: Use CPUPRI_NR_PRIORITIES instead of MAX_RT_PRIO in cpupri check
      sched/deadline: Fix memory leak
      sched/deadline: Fix sched_yield() behavior
      sched: Sanitize irq accounting madness
      sched/docbook: Fix 'make htmldocs' warnings caused by missing description

commit ad0f614e4723db8cead439cf414108cbf975b224
Author: Masatake YAMATO <yamato@redhat.com>
Date:   Thu May 22 11:54:20 2014 -0700

    wait: swap EXIT_ZOMBIE(Z) and EXIT_DEAD(X) chars in TASK_STATE_TO_CHAR_STR
    
    In commit ad86622b478e ("wait: swap EXIT_ZOMBIE and EXIT_DEAD to hide
    EXIT_TRACE from user-space") the order of task state definitions were
    changed: EXIT_DEAD and EXIT_ZOMBIE were swapped.  Though the charterers
    for the states in TASK_STATE_TO_CHAR_STR string were not updated.  This
    patch synchronizes the string to the order of definitions.
    
    Signed-off-by: Masatake YAMATO <yamato@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 25f54c79f757..21fbdae61b9e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -220,7 +220,7 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 #define TASK_PARKED		512
 #define TASK_STATE_MAX		1024
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZXxKWP"
+#define TASK_STATE_TO_CHAR_STR "RSDTtXZxKWP"
 
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];

commit 4027d080854d1be96ef134a1c3024d5276114db6
Author: xiaofeng.yan <xiaofeng.yan@huawei.com>
Date:   Fri May 9 03:21:27 2014 +0000

    sched/rt: Fix 'struct sched_dl_entity' and dl_task_time() comments, to match the current upstream code
    
    Signed-off-by: xiaofeng.yan <xiaofeng.yan@huawei.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1399605687-18094-1-git-send-email-xiaofeng.yan@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 725eef121c9f..0f91d00efd87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1175,8 +1175,8 @@ struct sched_dl_entity {
 
 	/*
 	 * Original scheduling parameters. Copied here from sched_attr
-	 * during sched_setscheduler2(), they will remain the same until
-	 * the next sched_setscheduler2().
+	 * during sched_setattr(), they will remain the same until
+	 * the next sched_setattr().
 	 */
 	u64 dl_runtime;		/* maximum runtime for each instance	*/
 	u64 dl_deadline;	/* relative deadline of each instance	*/

commit 69dd0f848879328ae6c6f54c2ec80e49eef042d8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 9 14:30:10 2014 +0200

    sched/idle: Remove TS_POLLING support
    
    Now that there are no architectures left using it, kill the support
    for TS_POLLING.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Link: http://lkml.kernel.org/n/tip-6yurip2tfix2f4bfc5agu2s0@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index accb66bfd722..725eef121c9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2775,51 +2775,9 @@ static inline int spin_needbreak(spinlock_t *lock)
 
 /*
  * Idle thread specific functions to determine the need_resched
- * polling state. We have two versions, one based on TS_POLLING in
- * thread_info.status and one based on TIF_POLLING_NRFLAG in
- * thread_info.flags
+ * polling state.
  */
-#ifdef TS_POLLING
-static inline int tsk_is_polling(struct task_struct *p)
-{
-	return task_thread_info(p)->status & TS_POLLING;
-}
-static inline void __current_set_polling(void)
-{
-	current_thread_info()->status |= TS_POLLING;
-}
-
-static inline bool __must_check current_set_polling_and_test(void)
-{
-	__current_set_polling();
-
-	/*
-	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_task()
-	 */
-	smp_mb();
-
-	return unlikely(tif_need_resched());
-}
-
-static inline void __current_clr_polling(void)
-{
-	current_thread_info()->status &= ~TS_POLLING;
-}
-
-static inline bool __must_check current_clr_polling_and_test(void)
-{
-	__current_clr_polling();
-
-	/*
-	 * Polling state must be visible before we test NEED_RESCHED,
-	 * paired by resched_task()
-	 */
-	smp_mb();
-
-	return unlikely(tif_need_resched());
-}
-#elif defined(TIF_POLLING_NRFLAG)
+#ifdef TIF_POLLING_NRFLAG
 static inline int tsk_is_polling(struct task_struct *p)
 {
 	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);

commit d77b3ed5c9f8ebedf154b52b5e943c461f3d37e6
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Apr 11 11:44:40 2014 +0200

    sched: Add a new SD_SHARE_POWERDOMAIN for sched_domain
    
    A new flag SD_SHARE_POWERDOMAIN is created to reflect whether groups of CPUs
    in a sched_domain level can or not reach different power state. As an example,
    the flag should be cleared at CPU level if groups of cores can be power gated
    independently. This information can be used in the load balance decision or to
    add load balancing level between group of CPUs that can power gate
    independantly.
    This flag is part of the topology flags that can be set by arch.
    
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: schwidefsky@de.ibm.com
    Cc: cmetcalf@tilera.com
    Cc: benh@kernel.crashing.org
    Cc: preeti@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1397209481-28542-5-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 439a153b8403..accb66bfd722 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -870,6 +870,7 @@ enum cpu_idle_type {
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
 #define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
+#define SD_SHARE_POWERDOMAIN	0x0100	/* Domain members share power domain */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
 #define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */

commit 607b45e9a216e89a63351556e488eea06be0ff48
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Apr 11 11:44:39 2014 +0200

    sched, powerpc: Create a dedicated topology table
    
    Create a dedicated topology table for handling asymetric feature of powerpc.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Fleming <afleming@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: schwidefsky@de.ibm.com
    Cc: cmetcalf@tilera.com
    Cc: dietmar.eggemann@arm.com
    Cc: devicetree@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1397209481-28542-4-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 656b035c30e5..439a153b8403 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -877,8 +877,6 @@ enum cpu_idle_type {
 #define SD_OVERLAP		0x2000	/* sched_domains of this level overlap */
 #define SD_NUMA			0x4000	/* cross-node balancing */
 
-extern int __weak arch_sd_sibiling_asym_packing(void);
-
 #ifdef CONFIG_SCHED_SMT
 static inline const int cpu_smt_flags(void)
 {

commit 143e1e28cb40bed836b0a06567208bd7347c9672
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Apr 11 11:44:37 2014 +0200

    sched: Rework sched_domain topology definition
    
    We replace the old way to configure the scheduler topology with a new method
    which enables a platform to declare additionnal level (if needed).
    
    We still have a default topology table definition that can be used by platform
    that don't want more level than the SMT, MC, CPU and NUMA ones. This table can
    be overwritten by an arch which either wants to add new level where a load
    balance make sense like BOOK or powergating level or wants to change the flags
    configuration of some levels.
    
    For each level, we need a function pointer that returns cpumask for each cpu,
    a function pointer that returns the flags for the level and a name. Only flags
    that describe topology, can be set by an architecture. The current topology
    flags are:
    
     SD_SHARE_CPUPOWER
     SD_SHARE_PKG_RESOURCES
     SD_NUMA
     SD_ASYM_PACKING
    
    Then, each level must be a subset on the next one. The build sequence of the
    sched_domain will take care of removing useless levels like those with 1 CPU
    and those with the same CPU span and no more relevant information for
    load balancing than its children.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux390@de.ibm.com
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Link: http://lkml.kernel.org/r/1397209481-28542-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2a4298fb0d85..656b035c30e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -879,6 +879,27 @@ enum cpu_idle_type {
 
 extern int __weak arch_sd_sibiling_asym_packing(void);
 
+#ifdef CONFIG_SCHED_SMT
+static inline const int cpu_smt_flags(void)
+{
+	return SD_SHARE_CPUPOWER | SD_SHARE_PKG_RESOURCES;
+}
+#endif
+
+#ifdef CONFIG_SCHED_MC
+static inline const int cpu_core_flags(void)
+{
+	return SD_SHARE_PKG_RESOURCES;
+}
+#endif
+
+#ifdef CONFIG_NUMA
+static inline const int cpu_numa_flags(void)
+{
+	return SD_NUMA;
+}
+#endif
+
 struct sched_domain_attr {
 	int relax_domain_level;
 };
@@ -985,6 +1006,38 @@ void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
 
 bool cpus_share_cache(int this_cpu, int that_cpu);
 
+typedef const struct cpumask *(*sched_domain_mask_f)(int cpu);
+typedef const int (*sched_domain_flags_f)(void);
+
+#define SDTL_OVERLAP	0x01
+
+struct sd_data {
+	struct sched_domain **__percpu sd;
+	struct sched_group **__percpu sg;
+	struct sched_group_power **__percpu sgp;
+};
+
+struct sched_domain_topology_level {
+	sched_domain_mask_f mask;
+	sched_domain_flags_f sd_flags;
+	int		    flags;
+	int		    numa_level;
+	struct sd_data      data;
+#ifdef CONFIG_SCHED_DEBUG
+	char                *name;
+#endif
+};
+
+extern struct sched_domain_topology_level *sched_domain_topology;
+
+extern void set_sched_topology(struct sched_domain_topology_level *tl);
+
+#ifdef CONFIG_SCHED_DEBUG
+# define SD_INIT_NAME(type)		.name = #type
+#else
+# define SD_INIT_NAME(type)
+#endif
+
 #else /* CONFIG_SMP */
 
 struct sched_domain_attr;

commit 5bfd126e80dca70431aef8fdbc1cf14535f3c338
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Tue Apr 15 13:49:04 2014 +0200

    sched/deadline: Fix sched_yield() behavior
    
    yield_task_dl() is broken:
    
     o it forces current to be throttled setting its runtime to zero;
     o it sets current's dl_se->dl_new to one, expecting that dl_task_timer()
       will queue it back with proper parameters at replenish time.
    
    Unfortunately, dl_task_timer() has this check at the very beginning:
    
            if (!dl_task(p) || dl_se->dl_new)
                    goto unlock;
    
    So, it just bails out and the task is never replenished. It actually
    yielded forever.
    
    To fix this, introduce a new flag indicating that the task properly yielded
    the CPU before its current runtime expired. While this is a little overdoing
    at the moment, the flag would be useful in the future to discriminate between
    "good" jobs (of which remaining runtime could be reclaimed, i.e. recycled)
    and "bad" jobs (for which dl_throttled task has been set) that needed to be
    stopped.
    
    Reported-by: yjay.kim <yjay.kim@lge.com>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140429103953.e68eba1b2ac3309214e3dc5a@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 25f54c79f757..2a4298fb0d85 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1153,9 +1153,12 @@ struct sched_dl_entity {
 	 *
 	 * @dl_boosted tells if we are boosted due to DI. If so we are
 	 * outside bandwidth enforcement mechanism (but only until we
-	 * exit the critical section).
+	 * exit the critical section);
+	 *
+	 * @dl_yielded tells if task gave up the cpu before consuming
+	 * all its available runtime during the last job.
 	 */
-	int dl_throttled, dl_new, dl_boosted;
+	int dl_throttled, dl_new, dl_boosted, dl_yielded;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its

commit 4e857c58efeb99393cba5a5d0d8ec7117183137c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 17 18:06:10 2014 +0100

    arch: Mass conversion of smp_mb__*()
    
    Mostly scripted conversion of the smp_mb__* barriers.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-55dhyhocezdw1dg7u19hmh1u@git.kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 25f54c79f757..010cde3b44cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2782,10 +2782,8 @@ static inline bool __must_check current_set_polling_and_test(void)
 	/*
 	 * Polling state must be visible before we test NEED_RESCHED,
 	 * paired by resched_task()
-	 *
-	 * XXX: assumes set/clear bit are identical barrier wise.
 	 */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return unlikely(tif_need_resched());
 }
@@ -2803,7 +2801,7 @@ static inline bool __must_check current_clr_polling_and_test(void)
 	 * Polling state must be visible before we test NEED_RESCHED,
 	 * paired by resched_task()
 	 */
-	smp_mb__after_clear_bit();
+	smp_mb__after_atomic();
 
 	return unlikely(tif_need_resched());
 }

commit 0b747172dce6e0905ab173afbaffebb7a11d89bd
Merge: b7e70ca9c7d7 312103d64d0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 12 12:38:53 2014 -0700

    Merge git://git.infradead.org/users/eparis/audit
    
    Pull audit updates from Eric Paris.
    
    * git://git.infradead.org/users/eparis/audit: (28 commits)
      AUDIT: make audit_is_compat depend on CONFIG_AUDIT_COMPAT_GENERIC
      audit: renumber AUDIT_FEATURE_CHANGE into the 1300 range
      audit: do not cast audit_rule_data pointers pointlesly
      AUDIT: Allow login in non-init namespaces
      audit: define audit_is_compat in kernel internal header
      kernel: Use RCU_INIT_POINTER(x, NULL) in audit.c
      sched: declare pid_alive as inline
      audit: use uapi/linux/audit.h for AUDIT_ARCH declarations
      syscall_get_arch: remove useless function arguments
      audit: remove stray newline from audit_log_execve_info() audit_panic() call
      audit: remove stray newlines from audit_log_lost messages
      audit: include subject in login records
      audit: remove superfluous new- prefix in AUDIT_LOGIN messages
      audit: allow user processes to log from another PID namespace
      audit: anchor all pid references in the initial pid namespace
      audit: convert PPIDs to the inital PID namespace.
      pid: get pid_t ppid of task in init_pid_ns
      audit: rename the misleading audit_get_context() to audit_take_context()
      audit: Add generic compat syscall support
      audit: Add CONFIG_HAVE_ARCH_AUDITSYSCALL
      ...

commit ad86622b478eaafdc25b74237df82b10fce6326d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:46 2014 -0700

    wait: swap EXIT_ZOMBIE and EXIT_DEAD to hide EXIT_TRACE from user-space
    
    get_task_state() uses the most significant bit to report the state to
    user-space, this means that EXIT_ZOMBIE->EXIT_TRACE->EXIT_DEAD transition
    can be noticed via /proc as Z -> X -> Z change.  Note that this was
    possible even before EXIT_TRACE was introduced.
    
    This is not really bad but imho it make sense to hide EXIT_TRACE from
    user-space completely.  So the patch simply swaps EXIT_ZOMBIE and
    EXIT_DEAD, this way EXIT_TRACE will be seen as EXIT_ZOMBIE by user-space.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7781de5e5e7b..075b3056c0c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -210,8 +210,8 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 #define __TASK_STOPPED		4
 #define __TASK_TRACED		8
 /* in tsk->exit_state */
-#define EXIT_ZOMBIE		16
-#define EXIT_DEAD		32
+#define EXIT_DEAD		16
+#define EXIT_ZOMBIE		32
 #define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)
 /* in tsk->state again */
 #define TASK_DEAD		64

commit abd50b39e783e1b6c75c7534c37f1eb2d94a89cd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:42 2014 -0700

    wait: introduce EXIT_TRACE to avoid the racy EXIT_DEAD->EXIT_ZOMBIE transition
    
    wait_task_zombie() first does EXIT_ZOMBIE->EXIT_DEAD transition and
    drops tasklist_lock.  If this task is not the natural child and it is
    traced, we change its state back to EXIT_ZOMBIE for ->real_parent.
    
    The last transition is racy, this is even documented in 50b8d257486a
    "ptrace: partially fix the do_wait(WEXITED) vs EXIT_DEAD->EXIT_ZOMBIE
    race".  wait_consider_task() tries to detect this transition and clear
    ->notask_error but we can't rely on ptrace_reparented(), debugger can
    exit and do ptrace_unlink() before its sub-thread sets EXIT_ZOMBIE.
    
    And there is another problem which were missed before: this transition
    can also race with reparent_leader() which doesn't reset >exit_signal if
    EXIT_DEAD, assuming that this task must be reaped by someone else.  So
    the tracee can be re-parented with ->exit_signal != SIGCHLD, and if
    /sbin/init doesn't use __WALL it becomes unreapable.  This was fixed by
    the previous commit, but it was the temporary hack.
    
    1. Add the new exit_state, EXIT_TRACE. It means that the task is the
       traced zombie, debugger is going to detach and notify its natural
       parent.
    
       This new state is actually EXIT_ZOMBIE | EXIT_DEAD. This way we
       can avoid the changes in proc/kgdb code, get_task_state() still
       reports "X (dead)" in this case.
    
       Note: with or without this change userspace can see Z -> X -> Z
       transition. Not really bad, but probably makes sense to fix.
    
    2. Change wait_task_zombie() to use EXIT_TRACE instead of EXIT_DEAD
       if we need to notify the ->real_parent.
    
    3. Revert the previous hack in reparent_leader(), now that EXIT_DEAD
       is always the final state we can safely ignore such a task.
    
    4. Change wait_consider_task() to check EXIT_TRACE separately and kill
       the racy and no longer needed ptrace_reparented() case.
    
       If ptrace == T an EXIT_TRACE thread should be simply ignored, the
       owner of this state is going to ptrace_unlink() this task. We can
       pretend that it was already removed from ->ptraced list.
    
       Otherwise we should skip this thread too but clear ->notask_error,
       we must be the natural parent and debugger is going to untrace and
       notify us. IOW, this doesn't differ from "EXIT_ZOMBIE && p->ptrace"
       even if the task was already untraced.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Jan Kratochvil <jan.kratochvil@redhat.com>
    Reported-by: Michal Schmidt <mschmidt@redhat.com>
    Tested-by: Michal Schmidt <mschmidt@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f8497059f88c..7781de5e5e7b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -212,6 +212,7 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 /* in tsk->exit_state */
 #define EXIT_ZOMBIE		16
 #define EXIT_DEAD		32
+#define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)
 /* in tsk->state again */
 #define TASK_DEAD		64
 #define TASK_WAKEKILL		128

commit 23aebe1691a3d98a79676db6c0fd813e16478804
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:39 2014 -0700

    exec: kill bprm->tcomm[], simplify the "basename" logic
    
    Starting from commit c4ad8f98bef7 ("execve: use 'struct filename *' for
    executable name passing") bprm->filename can not go away after
    flush_old_exec(), so we do not need to save the binary name in
    bprm->tcomm[] added by 96e02d158678 ("exec: fix use-after-free bug in
    setup_new_exec()").
    
    And there was never need for filename_to_taskname-like code, we can
    simply do set_task_comm(kbasename(filename).
    
    This patch has to change set_task_comm() and trace_task_rename() to
    accept "const char *", but I think this change is also good.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6c70645eb3b6..f8497059f88c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2357,7 +2357,7 @@ extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, i
 struct task_struct *fork_idle(int);
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 
-extern void set_task_comm(struct task_struct *tsk, char *from);
+extern void set_task_comm(struct task_struct *tsk, const char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP

commit f0432d159601f96839f514f286eaa5b75c4112dc
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 7 15:37:30 2014 -0700

    mm, mempolicy: remove per-process flag
    
    PF_MEMPOLICY is an unnecessary optimization for CONFIG_SLAB users.
    There's no significant performance degradation to checking
    current->mempolicy rather than current->flags & PF_MEMPOLICY in the
    allocation path, especially since this is considered unlikely().
    
    Running TCP_RR with netperf-2.4.5 through localhost on 16 cpu machine with
    64GB of memory and without a mempolicy:
    
            threads         before          after
            16              1249409         1244487
            32              1281786         1246783
            48              1239175         1239138
            64              1244642         1241841
            80              1244346         1248918
            96              1266436         1254316
            112             1307398         1312135
            128             1327607         1326502
    
    Per-process flags are a scarce resource so we should free them up whenever
    possible and make them available.  We'll be using it shortly for memcg oom
    reserves.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Tim Hockin <thockin@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 642477dd814a..6c70645eb3b6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1851,7 +1851,6 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_NO_SETAFFINITY 0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
-#define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */

commit 615d6e8756c87149f2d4c1b93d471bca002bd849
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Mon Apr 7 15:37:25 2014 -0700

    mm: per-thread vma caching
    
    This patch is a continuation of efforts trying to optimize find_vma(),
    avoiding potentially expensive rbtree walks to locate a vma upon faults.
    The original approach (https://lkml.org/lkml/2013/11/1/410), where the
    largest vma was also cached, ended up being too specific and random,
    thus further comparison with other approaches were needed.  There are
    two things to consider when dealing with this, the cache hit rate and
    the latency of find_vma().  Improving the hit-rate does not necessarily
    translate in finding the vma any faster, as the overhead of any fancy
    caching schemes can be too high to consider.
    
    We currently cache the last used vma for the whole address space, which
    provides a nice optimization, reducing the total cycles in find_vma() by
    up to 250%, for workloads with good locality.  On the other hand, this
    simple scheme is pretty much useless for workloads with poor locality.
    Analyzing ebizzy runs shows that, no matter how many threads are
    running, the mmap_cache hit rate is less than 2%, and in many situations
    below 1%.
    
    The proposed approach is to replace this scheme with a small per-thread
    cache, maximizing hit rates at a very low maintenance cost.
    Invalidations are performed by simply bumping up a 32-bit sequence
    number.  The only expensive operation is in the rare case of a seq
    number overflow, where all caches that share the same address space are
    flushed.  Upon a miss, the proposed replacement policy is based on the
    page number that contains the virtual address in question.  Concretely,
    the following results are seen on an 80 core, 8 socket x86-64 box:
    
    1) System bootup: Most programs are single threaded, so the per-thread
       scheme does improve ~50% hit rate by just adding a few more slots to
       the cache.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 50.61%   | 19.90            |
    | patched        | 73.45%   | 13.58            |
    +----------------+----------+------------------+
    
    2) Kernel build: This one is already pretty good with the current
       approach as we're dealing with good locality.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 75.28%   | 11.03            |
    | patched        | 88.09%   | 9.31             |
    +----------------+----------+------------------+
    
    3) Oracle 11g Data Mining (4k pages): Similar to the kernel build workload.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 70.66%   | 17.14            |
    | patched        | 91.15%   | 12.57            |
    +----------------+----------+------------------+
    
    4) Ebizzy: There's a fair amount of variation from run to run, but this
       approach always shows nearly perfect hit rates, while baseline is just
       about non-existent.  The amounts of cycles can fluctuate between
       anywhere from ~60 to ~116 for the baseline scheme, but this approach
       reduces it considerably.  For instance, with 80 threads:
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 1.06%    | 91.54            |
    | patched        | 99.97%   | 14.18            |
    +----------------+----------+------------------+
    
    [akpm@linux-foundation.org: fix nommu build, per Davidlohr]
    [akpm@linux-foundation.org: document vmacache_valid() logic]
    [akpm@linux-foundation.org: attempt to untangle header files]
    [akpm@linux-foundation.org: add vmacache_find() BUG_ON]
    [hughd@google.com: add vmacache_valid_mm() (from Oleg)]
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: adjust and enhance comments]
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Michel Lespinasse <walken@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7cb07fd26680..642477dd814a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -132,6 +132,10 @@ struct perf_event_context;
 struct blk_plug;
 struct filename;
 
+#define VMACACHE_BITS 2
+#define VMACACHE_SIZE (1U << VMACACHE_BITS)
+#define VMACACHE_MASK (VMACACHE_SIZE - 1)
+
 /*
  * List of flags we want to share for kernel threads,
  * if only because they are not used by them anyway.
@@ -1235,6 +1239,9 @@ struct task_struct {
 #ifdef CONFIG_COMPAT_BRK
 	unsigned brk_randomized:1;
 #endif
+	/* per-thread vma caching */
+	u32 vmacache_seqnum;
+	struct vm_area_struct *vmacache[VMACACHE_SIZE];
 #if defined(SPLIT_RSS_COUNTING)
 	struct task_rss_stat	rss_stat;
 #endif

commit 1ead65812486cda65093683a99b8907a7242fa93
Merge: b6d739e95812 b97f0291a250
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 11:00:07 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes from Thomas Gleixner:
     "This assorted collection provides:
    
       - A new timer based timer broadcast feature for systems which do not
         provide a global accessible timer device.  That allows those
         systems to put CPUs into deep idle states where the per cpu timer
         device stops.
    
       - A few NOHZ_FULL related improvements to the timer wheel
    
       - The usual updates to timer devices found in ARM SoCs
    
       - Small improvements and updates all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      tick: Remove code duplication in tick_handle_periodic()
      tick: Fix spelling mistake in tick_handle_periodic()
      x86: hpet: Use proper destructor for delayed work
      workqueue: Provide destroy_delayed_work_on_stack()
      clocksource: CMT, MTU2, TMU and STI should depend on GENERIC_CLOCKEVENTS
      timer: Remove code redundancy while calling get_nohz_timer_target()
      hrtimer: Rearrange comments in the order struct members are declared
      timer: Use variable head instead of &work_list in __run_timers()
      clocksource: exynos_mct: silence a static checker warning
      arm: zynq: Add support for cpufreq
      arm: zynq: Don't use arm_global_timer with cpufreq
      clocksource/cadence_ttc: Overhaul clocksource frequency adjustment
      clocksource/cadence_ttc: Call clockevents_update_freq() with IRQs enabled
      clocksource: Add Kconfig entries for CMT, MTU2, TMU and STI
      sh: Remove Kconfig entries for TMU, CMT and MTU2
      ARM: shmobile: Remove CMT, TMU and STI Kconfig entries
      clocksource: armada-370-xp: Use atomic access for shared registers
      clocksource: orion: Use atomic access for shared registers
      clocksource: timer-keystone: Delete unnecessary variable
      clocksource: timer-keystone: introduce clocksource driver for Keystone
      ...

commit a21e40877ad130de837b0394583e4f68dc2ab6c5
Merge: b9b16a792241 073d8224d299
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 10:16:10 2014 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "The main purpose is to fix a full dynticks bug related to
      virtualization, where steal time accounting appears to be zero in
      /proc/stat even after a few seconds of competing guests running busy
      loops in a same host CPU.  It's not a regression though as it was
      there since the beginning.
    
      The other commits are preparatory work to fix the bug and various
      cleanups"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      arch: Remove stub cputime.h headers
      sched: Remove needless round trip nsecs <-> tick conversion of steal time
      cputime: Fix jiffies based cputime assumption on steal accounting
      cputime: Bring cputime -> nsecs conversion
      cputime: Default implementation of nsecs -> cputime conversion
      cputime: Fix nsecs_to_cputime() return type cast

commit 80e0b6e8a001361316a2d62b748fe677ec46b860
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Sun Mar 16 14:00:19 2014 -0400

    sched: declare pid_alive as inline
    
    We accidentally declared pid_alive without any extern/inline connotation.
    Some platforms were fine with this, some like ia64 and mips were very angry.
    If the function is inline, the prototype should be inline!
    
    on ia64:
    include/linux/sched.h:1718: warning: 'pid_alive' declared inline after
    being called
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 116e301fb13f..0f72548732f1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1561,7 +1561,7 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 }
 
 
-static int pid_alive(const struct task_struct *p);
+static inline int pid_alive(const struct task_struct *p);
 static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
 {
 	pid_t pid = 0;

commit ad36d28293936b03d6b7996e9d6aadfd73c0eb08
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Thu Aug 15 18:05:12 2013 -0400

    pid: get pid_t ppid of task in init_pid_ns
    
    Added the functions task_ppid_nr_ns() and task_ppid_nr() to abstract the lookup
    of the PPID (real_parent's pid_t) of a process, including rcu locking, in the
    arbitrary and init_pid_ns.
    This provides an alternative to sys_getppid(), which is relative to the child
    process' pid namespace.
    
    (informed by ebiederman's 6c621b7e)
    Cc: stable@vger.kernel.org
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 53f97eb8dbc7..116e301fb13f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1561,6 +1561,24 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 }
 
 
+static int pid_alive(const struct task_struct *p);
+static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
+{
+	pid_t pid = 0;
+
+	rcu_read_lock();
+	if (pid_alive(tsk))
+		pid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);
+	rcu_read_unlock();
+
+	return pid;
+}
+
+static inline pid_t task_ppid_nr(const struct task_struct *tsk)
+{
+	return task_ppid_nr_ns(tsk, &init_pid_ns);
+}
+
 static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
 					struct pid_namespace *ns)
 {
@@ -1600,7 +1618,7 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
  *
  * Return: 1 if the process is alive. 0 otherwise.
  */
-static inline int pid_alive(struct task_struct *p)
+static inline int pid_alive(const struct task_struct *p)
 {
 	return p->pids[PIDTYPE_PID].pid != NULL;
 }

commit 6201b4d61fbf194df6371fb3376c5026cb8f5eec
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Mar 18 16:26:07 2014 +0530

    timer: Remove code redundancy while calling get_nohz_timer_target()
    
    There are only two users of get_nohz_timer_target(): timer and hrtimer. Both
    call it under same circumstances, i.e.
    
            #ifdef CONFIG_NO_HZ_COMMON
                   if (!pinned && get_sysctl_timer_migration() && idle_cpu(this_cpu))
                           return get_nohz_timer_target();
            #endif
    
    So, it makes more sense to get all this as part of get_nohz_timer_target()
    instead of duplicating code at two places. For this another parameter is
    required to be passed to this routine, pinned.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: fweisbec@gmail.com
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/1e1b53537217d58d48c2d7a222a9c3ac47d5b64c.1395140107.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68a0e84463a0..6f6c56f63c68 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -291,10 +291,14 @@ extern int runqueue_is_locked(int cpu);
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void nohz_balance_enter_idle(int cpu);
 extern void set_cpu_sd_state_idle(void);
-extern int get_nohz_timer_target(void);
+extern int get_nohz_timer_target(int pinned);
 #else
 static inline void nohz_balance_enter_idle(int cpu) { }
 static inline void set_cpu_sd_state_idle(void) { }
+static inline int get_nohz_timer_target(int pinned)
+{
+	return smp_processor_id();
+}
 #endif
 
 /*

commit bfc3f0281e08066fa8111c3972cff6edc1049864
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Mar 5 16:33:42 2014 +0100

    cputime: Default implementation of nsecs -> cputime conversion
    
    The architectures that override cputime_t (s390, ppc) don't provide
    any version of nsecs_to_cputime(). Indeed this cputime_t implementation
    by backend only happens when CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y under
    which the core code doesn't make any use of nsecs_to_cputime().
    
    At least for now.
    
    We are going to make a broader use of it so lets provide a default
    version with a per usecs granularity. It should be good enough for most
    usecases.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68a0e84463a0..1ac566c48d3d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -27,7 +27,7 @@ struct sched_param {
 
 #include <asm/page.h>
 #include <asm/ptrace.h>
-#include <asm/cputime.h>
+#include <linux/cputime.h>
 
 #include <linux/smp.h>
 #include <linux/sem.h>

commit 8f47b1871b8aac98f1a9d93bc3467fb97b65199a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 7 20:58:39 2014 +0100

    sched: Add better debug output for might_sleep()
    
    might_sleep() can tell us where interrupts have been disabled, but we
    have no idea what disabled preemption. Add some debug infrastructure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1391803122-4425-4-git-send-email-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c49a2585ff7d..825ed838d4b9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1463,6 +1463,9 @@ struct task_struct {
 	struct mutex perf_event_mutex;
 	struct list_head perf_event_list;
 #endif
+#ifdef CONFIG_DEBUG_PREEMPT
+	unsigned long preempt_disable_ip;
+#endif
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
 	short il_next;

commit d97a860c4f3de98ba5040a22f305b7159fe17cff
Merge: 3f67d962c64d d158fc7f36a2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 21 21:36:40 2014 +0100

    Merge branch 'linus' into sched/core
    
    Reason: Bring bakc upstream modification to resolve conflicts
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit fed14d45f945042a15b09de48d7d3d58d9455fc4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Feb 11 06:05:00 2012 +0100

    sched/fair: Track cgroup depth
    
    Track depth in cgroup tree, this is useful for things like
    find_matching_se() where you need to get to a common parent of two
    sched entities.
    
    Keeping the depth avoids having to calculate it on the spot, which
    saves a number of possible cache-misses.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1328936700.2476.17.camel@laptop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e3d556427b2e..555e27d717c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1078,6 +1078,7 @@ struct sched_entity {
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	int			depth;
 	struct sched_entity	*parent;
 	/* rq on which this entity is (to be) queued: */
 	struct cfs_rq		*cfs_rq;

commit d0ea026808ad81de2af14938448419a95211b938
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Mon Jan 27 22:00:45 2014 -0500

    sched: Implement task_nice() as static inline function
    
    As patch "sched: Move the priority specific bits into a new header file" exposes
    the priority related macros in linux/sched/prio.h, we don't have to implement
    task_nice() in kernel/sched/core.c any more.
    
    This patch implements it in linux/sched/sched.h as static inline function,
    saving the kernel stack and enhancing performance a bit.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: clark.williams@gmail.com
    Cc: rostedt@goodmis.org
    Cc: raistlin@linux.it
    Cc: juri.lelli@gmail.com
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1390878045-7096-1-git-send-email-yangds.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d97d0a8e87dc..e3d556427b2e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2094,7 +2094,16 @@ static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 extern bool yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
-extern int task_nice(const struct task_struct *p);
+/**
+ * task_nice - return the nice value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The nice value [ -20 ... 0 ... 19 ].
+ */
+static inline int task_nice(const struct task_struct *p)
+{
+	return PRIO_TO_NICE((p)->static_prio);
+}
 extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);

commit 5c228079ce8a9bb043a423069a6674dfb9268037
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Mon Jan 27 17:15:37 2014 -0500

    sched: Move the priority specific bits into a new header file
    
    Some bits about priority are defined in linux/sched/rt.h, but
    some of them are not only for rt scheduler, such as MAX_PRIO.
    
    This patch move them all into a new header file, linux/sched/prio.h.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: clark.williams@gmail.com
    Cc: rostedt@goodmis.org
    Cc: raistlin@linux.it
    Cc: juri.lelli@gmail.com
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/f7549508a1588da2c613d601748ca9de30fa5dcf.1390859827.git.yangds.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ed867797fe5a..d97d0a8e87dc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -3,6 +3,8 @@
 
 #include <uapi/linux/sched.h>
 
+#include <linux/sched/prio.h>
+
 
 struct sched_param {
 	int sched_priority;

commit c4ad8f98bef77c7356aa6a9ad9188a6acc6b849d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 5 12:54:53 2014 -0800

    execve: use 'struct filename *' for executable name passing
    
    This changes 'do_execve()' to get the executable name as a 'struct
    filename', and to free it when it is done.  This is what the normal
    users want, and it simplifies and streamlines their error handling.
    
    The controlled lifetime of the executable name also fixes a
    use-after-free problem with the trace_sched_process_exec tracepoint: the
    lifetime of the passed-in string for kernel users was not at all
    obvious, and the user-mode helper code used UMH_WAIT_EXEC to serialize
    the pathname allocation lifetime with the execve() having finished,
    which in turn meant that the trace point that happened after
    mm_release() of the old process VM ended up using already free'd memory.
    
    To solve the kernel string lifetime issue, this simply introduces
    "getname_kernel()" that works like the normal user-space getname()
    function, except with the source coming from kernel memory.
    
    As Oleg points out, this also means that we could drop the tcomm[] array
    from 'struct linux_binprm', since the pathname lifetime now covers
    setup_new_exec().  That would be a separate cleanup.
    
    Reported-by: Igor Zhbanov <i.zhbanov@samsung.com>
    Tested-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68a0e84463a0..a781dec1cd0b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -128,6 +128,7 @@ struct bio_list;
 struct fs_struct;
 struct perf_event_context;
 struct blk_plug;
+struct filename;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -2311,7 +2312,7 @@ extern void do_group_exit(int);
 extern int allow_signal(int);
 extern int disallow_signal(int);
 
-extern int do_execve(const char *,
+extern int do_execve(struct filename *,
 		     const char __user * const __user *,
 		     const char __user * const __user *);
 extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);

commit eaa4e4fcf1b5c60e656d93242f7fe422173f25b2
Merge: be1e4e760d94 5cb480f6b488
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 2 09:45:39 2014 +0100

    Merge branch 'linus' into sched/core, to resolve conflicts
    
    Conflicts:
            kernel/sysctl.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7e2703e6099609adc93679c4d45cd6247f565971
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:45 2014 -0500

    sched/numa: Normalize faults_cpu stats and weigh by CPU use
    
    Tracing the code that decides the active nodes has made it abundantly clear
    that the naive implementation of the faults_from code has issues.
    
    Specifically, the garbage collector in some workloads will access orders
    of magnitudes more memory than the threads that do all the active work.
    This resulted in the node with the garbage collector being marked the only
    active node in the group.
    
    This issue is avoided if we weigh the statistics by CPU use of each task in
    the numa group, instead of by how many faults each thread has occurred.
    
    To achieve this, we normalize the number of faults to the fraction of faults
    that occurred on each node, and then multiply that fraction by the fraction
    of CPU time the task has used since the last time task_numa_placement was
    invoked.
    
    This way the nodes in the active node mask will be the ones where the tasks
    from the numa group are most actively running, and the influence of eg. the
    garbage collector and other do-little threads is properly minimized.
    
    On a 4 node system, using CPU use statistics calculated over a longer interval
    results in about 1% fewer page migrations with two 32-warehouse specjbb runs
    on a 4 node system, and about 5% fewer page migrations, as well as 1% better
    throughput, with two 8-warehouse specjbb runs, as compared with the shorter
    term statistics kept by the scheduler.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-7-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5ab3b89fc33e..ef92953764f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1459,6 +1459,8 @@ struct task_struct {
 	int numa_preferred_nid;
 	unsigned long numa_migrate_retry;
 	u64 node_stamp;			/* migration stamp  */
+	u64 last_task_numa_placement;
+	u64 last_sum_exec_runtime;
 	struct callback_head numa_work;
 
 	struct list_head numa_entry;

commit 10f39042711ba21773763f267b4943a2c66c8bef
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:44 2014 -0500

    sched/numa, mm: Use active_nodes nodemask to limit numa migrations
    
    Use the active_nodes nodemask to make smarter decisions on NUMA migrations.
    
    In order to maximize performance of workloads that do not fit in one NUMA
    node, we want to satisfy the following criteria:
    
      1) keep private memory local to each thread
    
      2) avoid excessive NUMA migration of pages
    
      3) distribute shared memory across the active nodes, to
         maximize memory bandwidth available to the workload
    
    This patch accomplishes that by implementing the following policy for
    NUMA migrations:
    
      1) always migrate on a private fault
    
      2) never migrate to a node that is not in the set of active nodes
         for the numa_group
    
      3) always migrate from a node outside of the set of active nodes,
         to a node that is in that set
    
      4) within the set of active nodes in the numa_group, only migrate
         from a node with more NUMA page faults, to a node with fewer
         NUMA page faults, with a 25% margin to avoid ping-ponging
    
    This results in most pages of a workload ending up on the actively
    used nodes, with reduced ping-ponging of pages between those nodes.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-6-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5fb0cfb43ecf..5ab3b89fc33e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1589,6 +1589,8 @@ extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 extern void task_numa_free(struct task_struct *p);
+extern bool should_numa_migrate_memory(struct task_struct *p, struct page *page,
+					int src_nid, int dst_cpu);
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)
@@ -1604,6 +1606,11 @@ static inline void set_numabalancing_state(bool enabled)
 static inline void task_numa_free(struct task_struct *p)
 {
 }
+static inline bool should_numa_migrate_memory(struct task_struct *p,
+				struct page *page, int src_nid, int dst_cpu)
+{
+	return true;
+}
 #endif
 
 static inline struct pid *task_pid(struct task_struct *task)

commit 50ec8a401fed6d246ab65e6011d61ac91c34af70
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:42 2014 -0500

    sched/numa: Track from which nodes NUMA faults are triggered
    
    Track which nodes NUMA faults are triggered from, in other words
    the CPUs on which the NUMA faults happened. This uses a similar
    mechanism to what is used to track the memory involved in numa faults.
    
    The next patches use this to build up a bitmap of which nodes a
    workload is actively running on.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-4-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 144d509df053..5fb0cfb43ecf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1479,6 +1479,13 @@ struct task_struct {
 	 */
 	unsigned long *numa_faults_buffer_memory;
 
+	/*
+	 * Track the nodes the process was running on when a NUMA hinting
+	 * fault was incurred.
+	 */
+	unsigned long *numa_faults_cpu;
+	unsigned long *numa_faults_buffer_cpu;
+
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last
 	 * scan window were remote/local. The task scan period is adapted
@@ -1582,8 +1589,6 @@ extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 extern void task_numa_free(struct task_struct *p);
-
-extern unsigned int sysctl_numa_balancing_migrate_deferred;
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)

commit ff1df896aef8e0ec1556a5c44f424bd45bfa2cbe
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:41 2014 -0500

    sched/numa: Rename p->numa_faults to numa_faults_memory
    
    In order to get a more consistent naming scheme, making it clear
    which fault statistics track memory locality, and which track
    CPU locality, rename the memory fault statistics.
    
    Suggested-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d572d5ba650f..144d509df053 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1469,15 +1469,15 @@ struct task_struct {
 	 * Scheduling placement decisions are made based on the these counts.
 	 * The values remain static for the duration of a PTE scan
 	 */
-	unsigned long *numa_faults;
+	unsigned long *numa_faults_memory;
 	unsigned long total_numa_faults;
 
 	/*
 	 * numa_faults_buffer records faults per node during the current
-	 * scan window. When the scan completes, the counts in numa_faults
-	 * decay and these values are copied.
+	 * scan window. When the scan completes, the counts in
+	 * numa_faults_memory decay and these values are copied.
 	 */
-	unsigned long *numa_faults_buffer;
+	unsigned long *numa_faults_buffer_memory;
 
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last

commit 52bf84aa206cd2c2516dfa3e03b578edf8a3242f
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:40 2014 -0500

    sched/numa, mm: Remove p->numa_migrate_deferred
    
    Excessive migration of pages can hurt the performance of workloads
    that span multiple NUMA nodes.  However, it turns out that the
    p->numa_migrate_deferred knob is a really big hammer, which does
    reduce migration rates, but does not actually help performance.
    
    Now that the second stage of the automatic numa balancing code
    has stabilized, it is time to replace the simplistic migration
    deferral code with something smarter.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-2-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ffccdad050b5..d572d5ba650f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1457,7 +1457,6 @@ struct task_struct {
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
 	int numa_preferred_nid;
-	int numa_migrate_deferred;
 	unsigned long numa_migrate_retry;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;

commit 98611e4e6a2b4a03fd2d4750cce8e4455a995c8d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 23 15:55:52 2014 -0800

    exec: kill task_struct->did_exec
    
    We can kill either task->did_exec or PF_FORKNOEXEC, they are mutually
    exclusive.  The patch kills ->did_exec because it has a single user.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 66a17ad55bcb..68a0e84463a0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1239,7 +1239,6 @@ struct task_struct {
 	/* Used for emulating ABI behavior of previous Linux versions */
 	unsigned int personality;
 
-	unsigned did_exec:1;
 	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
 				 * execve */
 	unsigned in_iowait:1;

commit ff252c1fc537b0c9e40f62da0a9d11bf0737b7db
Author: DaeSeok Youn <daeseok.youn@gmail.com>
Date:   Thu Jan 23 15:55:46 2014 -0800

    kernel/fork.c: make dup_mm() static
    
    dup_mm() is used only in kernel/fork.c
    
    Signed-off-by: Daeseok Youn <daeseok.youn@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 33e4e9e1f621..66a17ad55bcb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2295,8 +2295,6 @@ extern struct mm_struct *get_task_mm(struct task_struct *task);
 extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
-/* Allocate a new mm structure and copy contents from tsk->mm */
-extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
 extern int copy_thread(unsigned long, unsigned long, unsigned long,
 			struct task_struct *);

commit 74e37200de8e9c4e09b70c21c3f13c2071e77457
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 23 15:55:35 2014 -0800

    proc: cleanup/simplify get_task_state/task_state_array
    
    get_task_state() and task_state_array[] look confusing and suboptimal, it
    is not clear what it can actually report to user-space and
    task_state_array[] blows .data for no reason.
    
    1. state = (tsk->state & TASK_REPORT) | tsk->exit_state is not
       clear. TASK_REPORT is self-documenting but it is not clear
       what ->exit_state can add.
    
       Move the potential exit_state's (EXIT_ZOMBIE and EXIT_DEAD)
       into TASK_REPORT and use it to calculate the final result.
    
    2. With the change above it is obvious that task_state_array[]
       has the unused entries just to make BUILD_BUG_ON() happy.
    
       Change this BUILD_BUG_ON() to use TASK_REPORT rather than
       TASK_STATE_MAX and shrink task_state_array[].
    
    3. Turn the "while (state)" loop into fls(state).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: David Laight <David.Laight@ACULAB.COM>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cf9e414dbb9e..33e4e9e1f621 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -229,7 +229,7 @@ extern char ___assert_task_state[1 - 2*!!(
 /* get_task_state() */
 #define TASK_REPORT		(TASK_RUNNING | TASK_INTERRUPTIBLE | \
 				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
-				 __TASK_TRACED)
+				 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
 
 #define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
 #define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)

commit 942be3875a1931c379bbc37053829dd6847e0f3f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 23 15:55:34 2014 -0800

    coredump: make __get_dumpable/get_dumpable inline, kill fs/coredump.h
    
    1. Remove fs/coredump.h. It is not clear why do we need it,
       it only declares __get_dumpable(), signal.c includes it
       for no reason.
    
    2. Now that get_dumpable() and __get_dumpable() are really
       trivial make them inline in linux/sched.h.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Alex Kelly <alex.page.kelly@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Petr Matousek <pmatouse@redhat.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 124430ba569b..cf9e414dbb9e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -391,10 +391,6 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-
-extern void set_dumpable(struct mm_struct *mm, int value);
-extern int get_dumpable(struct mm_struct *mm);
-
 #define SUID_DUMP_DISABLE	0	/* No setuid dumping */
 #define SUID_DUMP_USER		1	/* Dump as user of process */
 #define SUID_DUMP_ROOT		2	/* Dump as root */
@@ -405,6 +401,23 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_DUMPABLE_BITS 2
 #define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
 
+extern void set_dumpable(struct mm_struct *mm, int value);
+/*
+ * This returns the actual value of the suid_dumpable flag. For things
+ * that are using this for checking for privilege transitions, it must
+ * test against SUID_DUMP_USER rather than treating it as a boolean
+ * value.
+ */
+static inline int __get_dumpable(unsigned long mm_flags)
+{
+	return mm_flags & MMF_DUMPABLE_MASK;
+}
+
+static inline int get_dumpable(struct mm_struct *mm)
+{
+	return __get_dumpable(mm->flags);
+}
+
 /* coredump filter bits */
 #define MMF_DUMP_ANON_PRIVATE	2
 #define MMF_DUMP_ANON_SHARED	3

commit 7288e1187ba935996232246916418c64bb88da30
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 23 15:55:32 2014 -0800

    coredump: kill MMF_DUMPABLE and MMF_DUMP_SECURELY
    
    Nobody actually needs MMF_DUMPABLE/MMF_DUMP_SECURELY, they are only used
    to enforce the encoding of SUID_DUMP_* enum in mm->flags &
    MMF_DUMPABLE_MASK.
    
    Now that set_dumpable() updates both bits atomically we can kill them and
    simply store the value "as is" in 2 lower bits.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Alex Kelly <alex.page.kelly@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Petr Matousek <pmatouse@redhat.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 485234d2fd42..124430ba569b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -400,10 +400,8 @@ extern int get_dumpable(struct mm_struct *mm);
 #define SUID_DUMP_ROOT		2	/* Dump as root */
 
 /* mm flags */
-/* dumpable bits */
-#define MMF_DUMPABLE      0  /* core dump is permitted */
-#define MMF_DUMP_SECURELY 1  /* core file is readable only by root */
 
+/* for SUID_DUMP_* above */
 #define MMF_DUMPABLE_BITS 2
 #define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
 

commit 0c740d0afc3bff0a097ad03a1c8df92757516f5c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 21 15:49:56 2014 -0800

    introduce for_each_thread() to replace the buggy while_each_thread()
    
    while_each_thread() and next_thread() should die, almost every lockless
    usage is wrong.
    
    1. Unless g == current, the lockless while_each_thread() is not safe.
    
       while_each_thread(g, t) can loop forever if g exits, next_thread()
       can't reach the unhashed thread in this case. Note that this can
       happen even if g is the group leader, it can exec.
    
    2. Even if while_each_thread() itself was correct, people often use
       it wrongly.
    
       It was never safe to just take rcu_read_lock() and loop unless
       you verify that pid_alive(g) == T, even the first next_thread()
       can point to the already freed/reused memory.
    
    This patch adds signal_struct->thread_head and task->thread_node to
    create the normal rcu-safe list with the stable head.  The new
    for_each_thread(g, t) helper is always safe under rcu_read_lock() as
    long as this task_struct can't go away.
    
    Note: of course it is ugly to have both task_struct->thread_node and the
    old task_struct->thread_group, we will kill it later, after we change
    the users of while_each_thread() to use for_each_thread().
    
    Perhaps we can kill it even before we convert all users, we can
    reimplement next_thread(t) using the new thread_head/thread_node.  But
    we can't do this right now because this will lead to subtle behavioural
    changes.  For example, do/while_each_thread() always sees at least one
    task, while for_each_thread() can do nothing if the whole thread group
    has died.  Or thread_group_empty(), currently its semantics is not clear
    unless thread_group_leader(p) and we need to audit the callers before we
    can change it.
    
    So this patch adds the new interface which has to coexist with the old
    one for some time, hopefully the next changes will be more or less
    straightforward and the old one will go away soon.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Sergey Dyasly <dserrg@gmail.com>
    Tested-by: Sergey Dyasly <dserrg@gmail.com>
    Reviewed-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: "Ma, Xindong" <xindong.ma@intel.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: "Tu, Xiaobing" <xiaobing.tu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ffccdad050b5..485234d2fd42 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -549,6 +549,7 @@ struct signal_struct {
 	atomic_t		sigcnt;
 	atomic_t		live;
 	int			nr_threads;
+	struct list_head	thread_head;
 
 	wait_queue_head_t	wait_chldexit;	/* for wait4() */
 
@@ -1271,6 +1272,7 @@ struct task_struct {
 	/* PID/PID hash table linkage. */
 	struct pid_link pids[PIDTYPE_MAX];
 	struct list_head thread_group;
+	struct list_head thread_node;
 
 	struct completion *vfork_done;		/* for vfork() */
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
@@ -2341,6 +2343,16 @@ extern bool current_is_single_threaded(void);
 #define while_each_thread(g, t) \
 	while ((t = next_thread(t)) != g)
 
+#define __for_each_thread(signal, t)	\
+	list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)
+
+#define for_each_thread(p, t)		\
+	__for_each_thread((p)->signal, t)
+
+/* Careful: this is a double loop, 'break' won't work as expected. */
+#define for_each_process_thread(p, t)	\
+	for_each_process(p) for_each_thread(p, t)
+
 static inline int get_nr_threads(struct task_struct *tsk)
 {
 	return tsk->signal->nr_threads;

commit 8cb75e0c4ec9786b81439761eac1d18d4a931af3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 20 12:22:37 2013 +0100

    sched/preempt: Fix up missed PREEMPT_NEED_RESCHED folding
    
    With various drivers wanting to inject idle time; we get people
    calling idle routines outside of the idle loop proper.
    
    Therefore we need to be extra careful about not missing
    TIF_NEED_RESCHED -> PREEMPT_NEED_RESCHED propagations.
    
    While looking at this, I also realized there's a small window in the
    existing idle loop where we can miss TIF_NEED_RESCHED; when it hits
    right after the tif_need_resched() test at the end of the loop but
    right before the need_resched() test at the start of the loop.
    
    So move preempt_fold_need_resched() out of the loop where we're
    guaranteed to have TIF_NEED_RESCHED set.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-x9jgh45oeayzajz2mjt0y7d6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a03875221663..ffccdad050b5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2745,6 +2745,21 @@ static inline bool __must_check current_clr_polling_and_test(void)
 }
 #endif
 
+static inline void current_clr_polling(void)
+{
+	__current_clr_polling();
+
+	/*
+	 * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.
+	 * Once the bit is cleared, we'll get IPIs with every new
+	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
+	 * fold.
+	 */
+	smp_mb(); /* paired with resched_task() */
+
+	preempt_fold_need_resched();
+}
+
 static __always_inline bool need_resched(void)
 {
 	return unlikely(tif_need_resched());

commit 35af99e646c7f7ea46dc2977601e9e71a51dadd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:38:42 2013 +0100

    sched/clock, x86: Use a static_key for sched_clock_stable
    
    In order to avoid the runtime condition and variable load turn
    sched_clock_stable into a static_key.
    
    Also provide a shorter implementation of local_clock() and
    cpu_clock(int) when sched_clock_stable==1.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     221876    215295
        (cold) local_clock: 301773     234692    220773
        (warm) sched_clock: 38375      25602     25659
        (warm) local_clock: 100371     33265     27242
        (warm) rdtsc:       27340      24214     24208
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     235941    237019
        (cold) local_clock: 396890     297017    294819
        (warm) sched_clock: 38194      25233     25609
        (warm) local_clock: 143452     71234     71232
        (warm) rdtsc:       27345      24245     24243
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-eummbdechzz37mwmpags1gjr@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a196cb7fc6f2..a03875221663 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1994,7 +1994,9 @@ static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
  * but then during bootup it turns out that sched_clock()
  * is reliable after all:
  */
-extern int sched_clock_stable;
+extern int sched_clock_stable(void);
+extern void set_sched_clock_stable(void);
+extern void clear_sched_clock_stable(void);
 
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);

commit 332ac17ef5bfcff4766dfdfd3b4cdf10b8f8f155
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:45 2013 +0100

    sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks
    
    In order of deadline scheduling to be effective and useful, it is
    important that some method of having the allocation of the available
    CPU bandwidth to tasks and task groups under control.
    This is usually called "admission control" and if it is not performed
    at all, no guarantee can be given on the actual scheduling of the
    -deadline tasks.
    
    Since when RT-throttling has been introduced each task group have a
    bandwidth associated to itself, calculated as a certain amount of
    runtime over a period. Moreover, to make it possible to manipulate
    such bandwidth, readable/writable controls have been added to both
    procfs (for system wide settings) and cgroupfs (for per-group
    settings).
    
    Therefore, the same interface is being used for controlling the
    bandwidth distrubution to -deadline tasks and task groups, i.e.,
    new controls but with similar names, equivalent meaning and with
    the same usage paradigm are added.
    
    However, more discussion is needed in order to figure out how
    we want to manage SCHED_DEADLINE bandwidth at the task group level.
    Therefore, this patch adds a less sophisticated, but actually
    very sensible, mechanism to ensure that a certain utilization
    cap is not overcome per each root_domain (the single rq for !SMP
    configurations).
    
    Another main difference between deadline bandwidth management and
    RT-throttling is that -deadline tasks have bandwidth on their own
    (while -rt ones doesn't!), and thus we don't need an higher level
    throttling mechanism to enforce the desired bandwidth.
    
    This patch, therefore:
    
     - adds system wide deadline bandwidth management by means of:
        * /proc/sys/kernel/sched_dl_runtime_us,
        * /proc/sys/kernel/sched_dl_period_us,
       that determine (i.e., runtime / period) the total bandwidth
       available on each CPU of each root_domain for -deadline tasks;
    
     - couples the RT and deadline bandwidth management, i.e., enforces
       that the sum of how much bandwidth is being devoted to -rt
       -deadline tasks to stay below 100%.
    
    This means that, for a root_domain comprising M CPUs, -deadline tasks
    can be created until the sum of their bandwidths stay below:
    
        M * (sched_dl_runtime_us / sched_dl_period_us)
    
    It is also possible to disable this bandwidth management logic, and
    be thus free of oversubscribing the system up to any arbitrary level.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-12-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 13c53a99920f..a196cb7fc6f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1104,6 +1104,7 @@ struct sched_dl_entity {
 	u64 dl_runtime;		/* maximum runtime for each instance	*/
 	u64 dl_deadline;	/* relative deadline of each instance	*/
 	u64 dl_period;		/* separation of two instances (period) */
+	u64 dl_bw;		/* dl_runtime / dl_deadline		*/
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,

commit 2d3d891d3344159d5b452a645e355bbe29591e8b
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:44 2013 +0100

    sched/deadline: Add SCHED_DEADLINE inheritance logic
    
    Some method to deal with rt-mutexes and make sched_dl interact with
    the current PI-coded is needed, raising all but trivial issues, that
    needs (according to us) to be solved with some restructuring of
    the pi-code (i.e., going toward a proxy execution-ish implementation).
    
    This is under development, in the meanwhile, as a temporary solution,
    what this commits does is:
    
     - ensure a pi-lock owner with waiters is never throttled down. Instead,
       when it runs out of runtime, it immediately gets replenished and it's
       deadline is postponed;
    
     - the scheduling parameters (relative deadline and default runtime)
       used for that replenishments --during the whole period it holds the
       pi-lock-- are the ones of the waiting task with earliest deadline.
    
    Acting this way, we provide some kind of boosting to the lock-owner,
    still by using the existing (actually, slightly modified by the previous
    commit) pi-architecture.
    
    We would stress the fact that this is only a surely needed, all but
    clean solution to the problem. In the end it's only a way to re-start
    discussion within the community. So, as always, comments, ideas, rants,
    etc.. are welcome! :-)
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    [ Added !RT_MUTEXES build fix. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-11-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9ea15019a5b6..13c53a99920f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1124,8 +1124,12 @@ struct sched_dl_entity {
 	 * @dl_new tells if a new instance arrived. If so we must
 	 * start executing it with full runtime and reset its absolute
 	 * deadline;
+	 *
+	 * @dl_boosted tells if we are boosted due to DI. If so we are
+	 * outside bandwidth enforcement mechanism (but only until we
+	 * exit the critical section).
 	 */
-	int dl_throttled, dl_new;
+	int dl_throttled, dl_new, dl_boosted;
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its
@@ -1359,6 +1363,8 @@ struct task_struct {
 	struct rb_node *pi_waiters_leftmost;
 	/* Deadlock detection and priority inheritance handling */
 	struct rt_mutex_waiter *pi_blocked_on;
+	/* Top pi_waiters task */
+	struct task_struct *pi_top_task;
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES

commit fb00aca474405f4fa8a8519c3179fed722eabd83
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 7 14:43:43 2013 +0100

    rtmutex: Turn the plist into an rb-tree
    
    Turn the pi-chains from plist to rb-tree, in the rt_mutex code,
    and provide a proper comparison function for -deadline and
    -priority tasks.
    
    This is done mainly because:
     - classical prio field of the plist is just an int, which might
       not be enough for representing a deadline;
     - manipulating such a list would become O(nr_deadline_tasks),
       which might be to much, as the number of -deadline task increases.
    
    Therefore, an rb-tree is used, and tasks are queued in it according
    to the following logic:
     - among two -priority (i.e., SCHED_BATCH/OTHER/RR/FIFO) tasks, the
       one with the higher (lower, actually!) prio wins;
     - among a -priority and a -deadline task, the latter always wins;
     - among two -deadline tasks, the one with the earliest deadline
       wins.
    
    Queueing and dequeueing functions are changed accordingly, for both
    the list of a task's pi-waiters and the list of tasks blocked on
    a pi-lock.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-again-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-10-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 158f4c2dd852..9ea15019a5b6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -16,6 +16,7 @@ struct sched_param {
 #include <linux/types.h>
 #include <linux/timex.h>
 #include <linux/jiffies.h>
+#include <linux/plist.h>
 #include <linux/rbtree.h>
 #include <linux/thread_info.h>
 #include <linux/cpumask.h>
@@ -1354,7 +1355,8 @@ struct task_struct {
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */
-	struct plist_head pi_waiters;
+	struct rb_root pi_waiters;
+	struct rb_node *pi_waiters_leftmost;
 	/* Deadlock detection and priority inheritance handling */
 	struct rt_mutex_waiter *pi_blocked_on;
 #endif

commit 755378a47192a3d1f7c3a8ca6c15c1cf76de0af2
Author: Harald Gustafsson <harald.gustafsson@ericsson.com>
Date:   Thu Nov 7 14:43:40 2013 +0100

    sched/deadline: Add period support for SCHED_DEADLINE tasks
    
    Make it possible to specify a period (different or equal than
    deadline) for -deadline tasks. Relative deadlines (D_i) are used on
    task arrivals to generate new scheduling (absolute) deadlines as "d =
    t + D_i", and periods (P_i) to postpone the scheduling deadlines as "d
    = d + P_i" when the budget is zero.
    
    This is in general useful to model (and schedule) tasks that have slow
    activation rates (long periods), but have to be scheduled soon once
    activated (short deadlines).
    
    Signed-off-by: Harald Gustafsson <harald.gustafsson@ericsson.com>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-7-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cc66f2615a6d..158f4c2dd852 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1102,6 +1102,7 @@ struct sched_dl_entity {
 	 */
 	u64 dl_runtime;		/* maximum runtime for each instance	*/
 	u64 dl_deadline;	/* relative deadline of each instance	*/
+	u64 dl_period;		/* separation of two instances (period) */
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,

commit 1baca4ce16b8cc7d4f50be1f7914799af30a2861
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Thu Nov 7 14:43:38 2013 +0100

    sched/deadline: Add SCHED_DEADLINE SMP-related data structures & logic
    
    Introduces data structures relevant for implementing dynamic
    migration of -deadline tasks and the logic for checking if
    runqueues are overloaded with -deadline tasks and for choosing
    where a task should migrate, when it is the case.
    
    Adds also dynamic migrations to SCHED_DEADLINE, so that tasks can
    be moved among CPUs when necessary. It is also possible to bind a
    task to a (set of) CPU(s), thus restricting its capability of
    migrating, or forbidding migrations at all.
    
    The very same approach used in sched_rt is utilised:
     - -deadline tasks are kept into CPU-specific runqueues,
     - -deadline tasks are migrated among runqueues to achieve the
       following:
        * on an M-CPU system the M earliest deadline ready tasks
          are always running;
        * affinity/cpusets settings of all the -deadline tasks is
          always respected.
    
    Therefore, this very special form of "load balancing" is done with
    an active method, i.e., the scheduler pushes or pulls tasks between
    runqueues when they are woken up and/or (de)scheduled.
    IOW, every time a preemption occurs, the descheduled task might be sent
    to some other CPU (depending on its deadline) to continue executing
    (push). On the other hand, every time a CPU becomes idle, it might pull
    the second earliest deadline ready task from some other CPU.
    
    To enforce this, a pull operation is always attempted before taking any
    scheduling decision (pre_schedule()), as well as a push one after each
    scheduling decision (post_schedule()). In addition, when a task arrives
    or wakes up, the best CPU where to resume it is selected taking into
    account its affinity mask, the system topology, but also its deadline.
    E.g., from the scheduling point of view, the best CPU where to wake
    up (and also where to push) a task is the one which is running the task
    with the latest deadline among the M executing ones.
    
    In order to facilitate these decisions, per-runqueue "caching" of the
    deadlines of the currently running and of the first ready task is used.
    Queued but not running tasks are also parked in another rb-tree to
    speed-up pushes.
    
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-5-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6c196794fc12..cc66f2615a6d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1201,6 +1201,7 @@ struct task_struct {
 	struct list_head tasks;
 #ifdef CONFIG_SMP
 	struct plist_node pushable_tasks;
+	struct rb_node pushable_dl_tasks;
 #endif
 
 	struct mm_struct *mm, *active_mm;

commit aab03e05e8f7e26f51dee792beddcb5cca9215a5
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 28 11:14:43 2013 +0100

    sched/deadline: Add SCHED_DEADLINE structures & implementation
    
    Introduces the data structures, constants and symbols needed for
    SCHED_DEADLINE implementation.
    
    Core data structure of SCHED_DEADLINE are defined, along with their
    initializers. Hooks for checking if a task belong to the new policy
    are also added where they are needed.
    
    Adds a scheduling class, in sched/dl.c and a new policy called
    SCHED_DEADLINE. It is an implementation of the Earliest Deadline
    First (EDF) scheduling algorithm, augmented with a mechanism (called
    Constant Bandwidth Server, CBS) that makes it possible to isolate
    the behaviour of tasks between each other.
    
    The typical -deadline task will be made up of a computation phase
    (instance) which is activated on a periodic or sporadic fashion. The
    expected (maximum) duration of such computation is called the task's
    runtime; the time interval by which each instance need to be completed
    is called the task's relative deadline. The task's absolute deadline
    is dynamically calculated as the time instant a task (better, an
    instance) activates plus the relative deadline.
    
    The EDF algorithms selects the task with the smallest absolute
    deadline as the one to be executed first, while the CBS ensures each
    task to run for at most its runtime every (relative) deadline
    length time interval, avoiding any interference between different
    tasks (bandwidth isolation).
    Thanks to this feature, also tasks that do not strictly comply with
    the computational model sketched above can effectively use the new
    policy.
    
    To summarize, this patch:
     - introduces the data structures, constants and symbols needed;
     - implements the core logic of the scheduling algorithm in the new
       scheduling class file;
     - provides all the glue code between the new scheduling class and
       the core scheduler and refines the interactions between sched/dl
       and the other existing scheduling classes.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Michael Trimarchi <michael@amarulasolutions.com>
    Signed-off-by: Fabio Checconi <fchecconi@gmail.com>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-4-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 86025b6c6387..6c196794fc12 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -97,6 +97,10 @@ struct sched_param {
  * Given this task model, there are a multiplicity of scheduling algorithms
  * and policies, that can be used to ensure all the tasks will make their
  * timing constraints.
+ *
+ * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
+ * only user of this new interface. More information about the algorithm
+ * available in the scheduling class file or in Documentation/.
  */
 struct sched_attr {
 	u32 size;
@@ -1088,6 +1092,45 @@ struct sched_rt_entity {
 #endif
 };
 
+struct sched_dl_entity {
+	struct rb_node	rb_node;
+
+	/*
+	 * Original scheduling parameters. Copied here from sched_attr
+	 * during sched_setscheduler2(), they will remain the same until
+	 * the next sched_setscheduler2().
+	 */
+	u64 dl_runtime;		/* maximum runtime for each instance	*/
+	u64 dl_deadline;	/* relative deadline of each instance	*/
+
+	/*
+	 * Actual scheduling parameters. Initialized with the values above,
+	 * they are continously updated during task execution. Note that
+	 * the remaining runtime could be < 0 in case we are in overrun.
+	 */
+	s64 runtime;		/* remaining runtime for this instance	*/
+	u64 deadline;		/* absolute deadline for this instance	*/
+	unsigned int flags;	/* specifying the scheduler behaviour	*/
+
+	/*
+	 * Some bool flags:
+	 *
+	 * @dl_throttled tells if we exhausted the runtime. If so, the
+	 * task has to wait for a replenishment to be performed at the
+	 * next firing of dl_timer.
+	 *
+	 * @dl_new tells if a new instance arrived. If so we must
+	 * start executing it with full runtime and reset its absolute
+	 * deadline;
+	 */
+	int dl_throttled, dl_new;
+
+	/*
+	 * Bandwidth enforcement timer. Each -deadline task has its
+	 * own bandwidth to be enforced, thus we need one timer per task.
+	 */
+	struct hrtimer dl_timer;
+};
 
 struct rcu_node;
 
@@ -1124,6 +1167,7 @@ struct task_struct {
 #ifdef CONFIG_CGROUP_SCHED
 	struct task_group *sched_task_group;
 #endif
+	struct sched_dl_entity dl;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -2099,7 +2143,7 @@ extern void wake_up_new_task(struct task_struct *tsk);
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void sched_fork(unsigned long clone_flags, struct task_struct *p);
+extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);

commit d50dde5a10f305253cbc3855307f608f8a3c5f73
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:36 2013 +0100

    sched: Add new scheduler syscalls to support an extended scheduling parameters ABI
    
    Add the syscalls needed for supporting scheduling algorithms
    with extended scheduling parameters (e.g., SCHED_DEADLINE).
    
    In general, it makes possible to specify a periodic/sporadic task,
    that executes for a given amount of runtime at each instance, and is
    scheduled according to the urgency of their own timing constraints,
    i.e.:
    
     - a (maximum/typical) instance execution time,
     - a minimum interval between consecutive instances,
     - a time constraint by which each instance must be completed.
    
    Thus, both the data structure that holds the scheduling parameters of
    the tasks and the system calls dealing with it must be extended.
    Unfortunately, modifying the existing struct sched_param would break
    the ABI and result in potentially serious compatibility issues with
    legacy binaries.
    
    For these reasons, this patch:
    
     - defines the new struct sched_attr, containing all the fields
       that are necessary for specifying a task in the computational
       model described above;
    
     - defines and implements the new scheduling related syscalls that
       manipulate it, i.e., sched_setattr() and sched_getattr().
    
    Syscalls are introduced for x86 (32 and 64 bits) and ARM only, as a
    proof of concept and for developing and testing purposes. Making them
    available on other architectures is straightforward.
    
    Since no "user" for these new parameters is introduced in this patch,
    the implementation of the new system calls is just identical to their
    already existing counterpart. Future patches that implement scheduling
    policies able to exploit the new data structure must also take care of
    modifying the sched_*attr() calls accordingly with their own purposes.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    [ Rewrote to use sched_attr. ]
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    [ Removed sched_setscheduler2() for now. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-3-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3a1e9857b393..86025b6c6387 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -56,6 +56,66 @@ struct sched_param {
 
 #include <asm/processor.h>
 
+#define SCHED_ATTR_SIZE_VER0	48	/* sizeof first published struct */
+
+/*
+ * Extended scheduling parameters data structure.
+ *
+ * This is needed because the original struct sched_param can not be
+ * altered without introducing ABI issues with legacy applications
+ * (e.g., in sched_getparam()).
+ *
+ * However, the possibility of specifying more than just a priority for
+ * the tasks may be useful for a wide variety of application fields, e.g.,
+ * multimedia, streaming, automation and control, and many others.
+ *
+ * This variant (sched_attr) is meant at describing a so-called
+ * sporadic time-constrained task. In such model a task is specified by:
+ *  - the activation period or minimum instance inter-arrival time;
+ *  - the maximum (or average, depending on the actual scheduling
+ *    discipline) computation time of all instances, a.k.a. runtime;
+ *  - the deadline (relative to the actual activation time) of each
+ *    instance.
+ * Very briefly, a periodic (sporadic) task asks for the execution of
+ * some specific computation --which is typically called an instance--
+ * (at most) every period. Moreover, each instance typically lasts no more
+ * than the runtime and must be completed by time instant t equal to
+ * the instance activation time + the deadline.
+ *
+ * This is reflected by the actual fields of the sched_attr structure:
+ *
+ *  @size		size of the structure, for fwd/bwd compat.
+ *
+ *  @sched_policy	task's scheduling policy
+ *  @sched_flags	for customizing the scheduler behaviour
+ *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
+ *  @sched_priority	task's static priority (SCHED_FIFO/RR)
+ *  @sched_deadline	representative of the task's deadline
+ *  @sched_runtime	representative of the task's runtime
+ *  @sched_period	representative of the task's period
+ *
+ * Given this task model, there are a multiplicity of scheduling algorithms
+ * and policies, that can be used to ensure all the tasks will make their
+ * timing constraints.
+ */
+struct sched_attr {
+	u32 size;
+
+	u32 sched_policy;
+	u64 sched_flags;
+
+	/* SCHED_NORMAL, SCHED_BATCH */
+	s32 sched_nice;
+
+	/* SCHED_FIFO, SCHED_RR */
+	u32 sched_priority;
+
+	/* SCHED_DEADLINE */
+	u64 sched_runtime;
+	u64 sched_deadline;
+	u64 sched_period;
+};
+
 struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
@@ -1958,6 +2018,8 @@ extern int sched_setscheduler(struct task_struct *, int,
 			      const struct sched_param *);
 extern int sched_setscheduler_nocheck(struct task_struct *, int,
 				      const struct sched_param *);
+extern int sched_setattr(struct task_struct *,
+			 const struct sched_attr *);
 extern struct task_struct *idle_task(int cpu);
 /**
  * is_idle_task - is the specified task an idle task?

commit ffe732c2430c55074bebb172d33d909c662cd0e3
Merge: 40ea2b42d7c4 757dfcaa4184
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 17 15:22:35 2013 +0100

    Merge branch 'sched/urgent' into sched/core
    
    Merge the latest batch of fixes before applying development patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9dbdb155532395ba000c5d5d187658b0e17e529f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 18 18:27:06 2013 +0100

    sched/fair: Rework sched_fair time accounting
    
    Christian suffers from a bad BIOS that wrecks his i5's TSC sync. This
    results in him occasionally seeing time going backwards - which
    crashes the scheduler ...
    
    Most of our time accounting can actually handle that except the most
    common one; the tick time update of sched_fair.
    
    There is a further problem with that code; previously we assumed that
    because we get a tick every TICK_NSEC our time delta could never
    exceed 32bits and math was simpler.
    
    However, ever since Frederic managed to get NO_HZ_FULL merged; this is
    no longer the case since now a task can run for a long time indeed
    without getting a tick. It only takes about ~4.2 seconds to overflow
    our u32 in nanoseconds.
    
    This means we not only need to better deal with time going backwards;
    but also means we need to be able to deal with large deltas.
    
    This patch reworks the entire code and uses mul_u64_u32_shr() as
    proposed by Andy a long while ago.
    
    We express our virtual time scale factor in a u32 multiplier and shift
    right and the 32bit mul_u64_u32_shr() implementation reduces to a
    single 32x32->64 multiply if the time delta is still short (common
    case).
    
    For 64bit a 64x64->128 multiply can be used if ARCH_SUPPORTS_INT128.
    
    Reported-and-Tested-by: Christian Engelmayer <cengelma@gmx.at>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: fweisbec@gmail.com
    Cc: Paul Turner <pjt@google.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20131118172706.GI3866@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 96d674ba3876..53f97eb8dbc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -930,7 +930,8 @@ struct pipe_inode_info;
 struct uts_namespace;
 
 struct load_weight {
-	unsigned long weight, inv_weight;
+	unsigned long weight;
+	u32 inv_weight;
 };
 
 struct sched_avg {

commit ba1f14fbe70965ae0fb1655a5275a62723f65b77
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 14:26:41 2013 +0100

    sched: Remove PREEMPT_NEED_RESCHED from generic code
    
    While hunting a preemption issue with Alexander, Ben noticed that the
    currently generic PREEMPT_NEED_RESCHED stuff is horribly broken for
    load-store architectures.
    
    We currently rely on the IPI to fold TIF_NEED_RESCHED into
    PREEMPT_NEED_RESCHED, but when this IPI lands while we already have
    a load for the preempt-count but before the store, the store will erase
    the PREEMPT_NEED_RESCHED change.
    
    The current preempt-count only works on load-store archs because
    interrupts are assumed to be completely balanced wrt their preempt_count
    fiddling; the previous preempt_count load will match the preempt_count
    state after the interrupt and therefore nothing gets lost.
    
    This patch removes the PREEMPT_NEED_RESCHED usage from generic code and
    pushes it into x86 arch code; the generic code goes back to relying on
    TIF_NEED_RESCHED.
    
    Boot tested on x86_64 and compile tested on ppc64.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Reported-and-Tested-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20131128132641.GP10022@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 768b037dfacb..96d674ba3876 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -440,8 +440,6 @@ struct task_cputime {
 		.sum_exec_runtime = 0,				\
 	}
 
-#define PREEMPT_ENABLED		(PREEMPT_NEED_RESCHED)
-
 #ifdef CONFIG_PREEMPT_COUNT
 #define PREEMPT_DISABLED	(1 + PREEMPT_ENABLED)
 #else

commit a0b57ca33ec1cd915ba49051512b3463fa44b4e3
Merge: e321ae4c207c 96739d6e548e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 2 10:13:44 2013 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Various smaller fixlets, all over the place"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/doc: Fix generation of device-drivers
      sched: Expose preempt_schedule_irq()
      sched: Fix a trivial typo in comments
      sched: Remove unused variable in 'struct sched_domain'
      sched: Avoid NULL dereference on sd_busy
      sched: Check sched_domain before computing group power
      MAINTAINERS: Update file patterns in the lockdep and scheduler entries

commit 86506a99a62400e9f7b7d1344bcc9ea235faf98f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Nov 13 15:36:14 2013 +0100

    tasks/exit: Remove unused task_is_dead() method
    
    task_is_dead() has no users since commit 43e13cc107cf
    ("cred: remove task_is_dead() from __task_cred() validation"), and
    nobody except exit.c should rely on ->exit_state (we still have
    the users which should be changed).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: David Laight <David.Laight@ACULAB.COM>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20131113143614.GA10547@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7e35d4b9e14a..cda8d634bc47 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -168,7 +168,6 @@ extern char ___assert_task_state[1 - 2*!!(
 
 #define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
 #define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
-#define task_is_dead(task)	((task)->exit_state != 0)
 #define task_is_stopped_or_traced(task)	\
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\

commit 4007162647b3b2e2e438904471b620aba013c4af
Merge: 801a76050bcf 00d1a39e69d5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 19 10:40:00 2013 -0800

    Merge branch 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq cleanups from Ingo Molnar:
     "This is a multi-arch cleanup series from Thomas Gleixner, which we
      kept to near the end of the merge window, to not interfere with
      architecture updates.
    
      This series (motivated by the -rt kernel) unifies more aspects of IRQ
      handling and generalizes PREEMPT_ACTIVE"
    
    * 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      preempt: Make PREEMPT_ACTIVE generic
      sparc: Use preempt_schedule_irq
      ia64: Use preempt_schedule_irq
      m32r: Use preempt_schedule_irq
      hardirq: Make hardirq bits generic
      m68k: Simplify low level interrupt handling code
      genirq: Prevent spurious detection for unconditionally polled interrupts

commit b972fc308c2763096b61b62169f2167ee0ca5a19
Author: Alex Shi <alex.shi@linaro.org>
Date:   Tue Nov 19 17:21:52 2013 +0800

    sched: Remove unused variable in 'struct sched_domain'
    
    The 'u64 last_update' variable isn't used now, remove it to save a bit of space.
    
    Signed-off-by: Alex Shi <alex.shi@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: linaro-kernel@lists.linaro.org
    Link: http://lkml.kernel.org/r/1384852912-24791-1-git-send-email-alex.shi@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f7efc8604652..b122395bf5f4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -823,8 +823,6 @@ struct sched_domain {
 	unsigned int balance_interval;	/* initialise to 1. units in ms. */
 	unsigned int nr_balance_failed; /* initialise to 0 */
 
-	u64 last_update;
-
 	/* idle_balance() stats */
 	u64 max_newidle_lb_cost;
 	unsigned long next_decay_max_lb_cost;

commit f080480488028bcc25357f85e8ae54ccc3bb7173
Merge: eda670c626a4 e504c9098ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 13:51:36 2013 +0900

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Paolo Bonzini:
     "Here are the 3.13 KVM changes.  There was a lot of work on the PPC
      side: the HV and emulation flavors can now coexist in a single kernel
      is probably the most interesting change from a user point of view.
    
      On the x86 side there are nested virtualization improvements and a few
      bugfixes.
    
      ARM got transparent huge page support, improved overcommit, and
      support for big endian guests.
    
      Finally, there is a new interface to connect KVM with VFIO.  This
      helps with devices that use NoSnoop PCI transactions, letting the
      driver in the guest execute WBINVD instructions.  This includes some
      nVidia cards on Windows, that fail to start without these patches and
      the corresponding userspace changes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      kvm, vmx: Fix lazy FPU on nested guest
      arm/arm64: KVM: PSCI: propagate caller endianness to the incoming vcpu
      arm/arm64: KVM: MMIO support for BE guest
      kvm, cpuid: Fix sparse warning
      kvm: Delete prototype for non-existent function kvm_check_iopl
      kvm: Delete prototype for non-existent function complete_pio
      hung_task: add method to reset detector
      pvclock: detect watchdog reset at pvclock read
      kvm: optimize out smp_mb after srcu_read_unlock
      srcu: API for barrier after srcu read unlock
      KVM: remove vm mmap method
      KVM: IOMMU: hva align mapping page size
      KVM: x86: trace cpuid emulation when called from emulator
      KVM: emulator: cleanup decode_register_operand() a bit
      KVM: emulator: check rex prefix inside decode_register()
      KVM: x86: fix emulation of "movzbl %bpl, %eax"
      kvm_host: typo fix
      KVM: x86: emulate SAHF instruction
      MAINTAINERS: add tree for kvm.git
      Documentation/kvm: add a 00-INDEX file
      ...

commit 00d1a39e69d5afa7523dad515a05b21abd17c389
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 17 18:53:09 2013 +0000

    preempt: Make PREEMPT_ACTIVE generic
    
    No point in having this bit defined by architecture.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130917183629.090698799@linutronix.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 045b0d227846..55080df48b70 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -22,7 +22,7 @@ struct sched_param {
 #include <linux/errno.h>
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
-#include <linux/preempt.h>
+#include <linux/preempt_mask.h>
 
 #include <asm/page.h>
 #include <asm/ptrace.h>

commit d049f74f2dbe71354d43d393ac3a188947811348
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Nov 12 15:11:17 2013 -0800

    exec/ptrace: fix get_dumpable() incorrect tests
    
    The get_dumpable() return value is not boolean.  Most users of the
    function actually want to be testing for non-SUID_DUMP_USER(1) rather than
    SUID_DUMP_DISABLE(0).  The SUID_DUMP_ROOT(2) is also considered a
    protected state.  Almost all places did this correctly, excepting the two
    places fixed in this patch.
    
    Wrong logic:
        if (dumpable == SUID_DUMP_DISABLE) { /* be protective */ }
            or
        if (dumpable == 0) { /* be protective */ }
            or
        if (!dumpable) { /* be protective */ }
    
    Correct logic:
        if (dumpable != SUID_DUMP_USER) { /* be protective */ }
            or
        if (dumpable != 1) { /* be protective */ }
    
    Without this patch, if the system had set the sysctl fs/suid_dumpable=2, a
    user was able to ptrace attach to processes that had dropped privileges to
    that user.  (This may have been partially mitigated if Yama was enabled.)
    
    The macros have been moved into the file that declares get/set_dumpable(),
    which means things like the ia64 code can see them too.
    
    CVE-2013-2929
    
    Reported-by: Vasily Kulikov <segoon@openwall.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e226fe3e512..f7efc8604652 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -323,6 +323,10 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 
+#define SUID_DUMP_DISABLE	0	/* No setuid dumping */
+#define SUID_DUMP_USER		1	/* Dump as user of process */
+#define SUID_DUMP_ROOT		2	/* Dump as root */
+
 /* mm flags */
 /* dumpable bits */
 #define MMF_DUMPABLE      0  /* core dump is permitted */

commit 27f69e68a5e534412faebc53a4e04acc9ce7fd7e
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Tue Nov 12 15:08:47 2013 -0800

    sched: remove ARCH specific fpu_counter from task_struct
    
    fpu_counter in task_struct was used only by sh/x86.  Both of these now
    carry it in ARCH specific thread_struct, hence this can now be removed
    from generic task_struct, shrinking it slightly for other arches.
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Mundt <paul.mundt@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 045b0d227846..5e226fe3e512 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1062,15 +1062,6 @@ struct task_struct {
 	struct hlist_head preempt_notifiers;
 #endif
 
-	/*
-	 * fpu_counter contains the number of consecutive context switches
-	 * that the FPU is used. If this is over a threshold, the lazy fpu
-	 * saving becomes unlazy to save the trap. This is an unsigned char
-	 * so that after 256 times the counter wraps and the behavior turns
-	 * lazy again; this to deal with bursty apps that only use FPU for
-	 * a short time
-	 */
-	unsigned char fpu_counter;
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif

commit 8b414521bc5375ae8ba18c083af95d44b8da0d04
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Oct 11 21:39:26 2013 -0300

    hung_task: add method to reset detector
    
    In certain occasions it is possible for a hung task detector
    positive to be false: continuation from a paused VM, for example.
    
    Add a method to reset detection, similar as is done
    with other kernel watchdogs.
    
    Acked-by: Don Zickus <dzickus@redhat.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6682da36b293..7bb4b4a2a101 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -285,6 +285,14 @@ static inline void lockup_detector_init(void)
 }
 #endif
 
+#ifdef CONFIG_DETECT_HUNG_TASK
+void reset_hung_task_detector(void);
+#else
+static inline void reset_hung_task_detector(void)
+{
+}
+#endif
+
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 

commit fb10d5b7efbcc0aa9e46a9aa5ad86772c7bacb9a
Merge: f9f9ffc237dd 52469b4fcd4f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Nov 1 08:10:58 2013 +0100

    Merge branch 'linus' into sched/core
    
    Resolve cherry-picking conflicts:
    
    Conflicts:
            mm/huge_memory.c
            mm/memory.c
            mm/mprotect.c
    
    See this upstream merge commit for more details:
    
      52469b4fcd4f Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4942642080ea82d99ab5b653abb9a12b7ba31f4a
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Oct 16 13:46:59 2013 -0700

    mm: memcg: handle non-error OOM situations more gracefully
    
    Commit 3812c8c8f395 ("mm: memcg: do not trap chargers with full
    callstack on OOM") assumed that only a few places that can trigger a
    memcg OOM situation do not return VM_FAULT_OOM, like optional page cache
    readahead.  But there are many more and it's impractical to annotate
    them all.
    
    First of all, we don't want to invoke the OOM killer when the failed
    allocation is gracefully handled, so defer the actual kill to the end of
    the fault handling as well.  This simplifies the code quite a bit for
    added bonus.
    
    Second, since a failed allocation might not be the abrupt end of the
    fault, the memcg OOM handler needs to be re-entrant until the fault
    finishes for subsequent allocation attempts.  If an allocation is
    attempted after the task already OOMed, allow it to bypass the limit so
    that it can quickly finish the fault and invoke the OOM killer.
    
    Reported-by: azurIt <azurit@pobox.sk>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6682da36b293..e27baeeda3f4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1394,11 +1394,10 @@ struct task_struct {
 	} memcg_batch;
 	unsigned int memcg_kmem_skip_account;
 	struct memcg_oom_info {
+		struct mem_cgroup *memcg;
+		gfp_t gfp_mask;
+		int order;
 		unsigned int may_oom:1;
-		unsigned int in_memcg_oom:1;
-		unsigned int oom_locked:1;
-		int wakeups;
-		struct mem_cgroup *wait_on_memcg;
 	} memcg_oom;
 #endif
 #ifdef CONFIG_UPROBES

commit de1c9ce6f07fec0381a39a9d0b379ea35aa1167f
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:39 2013 +0100

    sched/numa: Skip some page migrations after a shared fault
    
    Shared faults can lead to lots of unnecessary page migrations,
    slowing down the system, and causing private faults to hit the
    per-pgdat migration ratelimit.
    
    This patch adds sysctl numa_balancing_migrate_deferred, which specifies
    how many shared page migrations to skip unconditionally, after each page
    migration that is skipped because it is a shared fault.
    
    This reduces the number of page migrations back and forth in
    shared fault situations. It also gives a strong preference to
    the tasks that are already running where most of the memory is,
    and to moving the other tasks to near the memory.
    
    Testing this with a much higher scan rate than the default
    still seems to result in fewer page migrations than before.
    
    Memory seems to be somewhat better consolidated than previously,
    with multi-instance specjbb runs on a 4 node system.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-62-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d24f70ffddee..833eed55cf43 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1342,6 +1342,8 @@ struct task_struct {
 	int numa_scan_seq;
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
+	int numa_preferred_nid;
+	int numa_migrate_deferred;
 	unsigned long numa_migrate_retry;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
@@ -1372,7 +1374,6 @@ struct task_struct {
 	 */
 	unsigned long numa_faults_locality[2];
 
-	int numa_preferred_nid;
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
@@ -1469,6 +1470,8 @@ extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 extern void task_numa_free(struct task_struct *p);
+
+extern unsigned int sysctl_numa_balancing_migrate_deferred;
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)

commit 1e3646ffc64b232cb14a5ef01d7b98997c1b73f9
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:38 2013 +0100

    mm: numa: Revert temporarily disabling of NUMA migration
    
    With the scan rate code working (at least for multi-instance specjbb),
    the large hammer that is "sched: Do not migrate memory immediately after
    switching node" can be replaced with something smarter. Revert temporarily
    migration disabling and all traces of numa_migrate_seq.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-61-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2292f6c1596f..d24f70ffddee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1340,7 +1340,6 @@ struct task_struct {
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	int numa_scan_seq;
-	int numa_migrate_seq;
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
 	unsigned long numa_migrate_retry;

commit 04bb2f9475054298f0c67a89ca92cade42d3fe5e
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:36 2013 +0100

    sched/numa: Adjust scan rate in task_numa_placement
    
    Adjust numa_scan_period in task_numa_placement, depending on how much
    useful work the numa code can do. The more local faults there are in a
    given scan window the longer the period (and hence the slower the scan rate)
    during the next window. If there are excessive shared faults then the scan
    period will decrease with the amount of scaling depending on whether the
    ratio of shared/private faults. If the preferred node changes then the
    scan rate is reset to recheck if the task is properly placed.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-59-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 59f953b2e413..2292f6c1596f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1365,6 +1365,14 @@ struct task_struct {
 	 */
 	unsigned long *numa_faults_buffer;
 
+	/*
+	 * numa_faults_locality tracks if faults recorded during the last
+	 * scan window were remote/local. The task scan period is adapted
+	 * based on the locality of the faults with different weights
+	 * depending on whether they were shared or private faults
+	 */
+	unsigned long numa_faults_locality[2];
+
 	int numa_preferred_nid;
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
@@ -1455,6 +1463,7 @@ struct task_struct {
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04
+#define TNF_FAULT_LOCAL	0x08
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);

commit dabe1d992414a6456e60e41f1d1ad8affc6d444d
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:34 2013 +0100

    sched/numa: Be more careful about joining numa groups
    
    Due to the way the pid is truncated, and tasks are moved between
    CPUs by the scheduler, it is possible for the current task_numa_fault
    to group together tasks that do not actually share memory together.
    
    This patch adds a few easy sanity checks to task_numa_fault, joining
    tasks together if they share the same tsk->mm, or if the fault was on
    a page with an elevated mapcount, in a shared VMA.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-57-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1127a46ac3d2..59f953b2e413 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1454,6 +1454,7 @@ struct task_struct {
 
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
+#define TNF_SHARED	0x04
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, int flags);

commit b32e86b4301e345611f0446265f782a229faadf6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 7 11:29:30 2013 +0100

    sched/numa: Add debugging
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-53-git-send-email-mgorman@suse.de

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f6385107c352..1127a46ac3d2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1366,6 +1366,7 @@ struct task_struct {
 	unsigned long *numa_faults_buffer;
 
 	int numa_preferred_nid;
+	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
 	struct rcu_head rcu;
@@ -2661,6 +2662,11 @@ static inline unsigned int task_cpu(const struct task_struct *p)
 	return task_thread_info(p)->cpu;
 }
 
+static inline int task_node(const struct task_struct *p)
+{
+	return cpu_to_node(task_cpu(p));
+}
+
 extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
 
 #else

commit 82727018b0d33d188e9916bcf76f18387484cb04
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:28 2013 +0100

    sched/numa: Call task_numa_free() from do_execve()
    
    It is possible for a task in a numa group to call exec, and
    have the new (unrelated) executable inherit the numa group
    association from its former self.
    
    This has the potential to break numa grouping, and is trivial
    to fix.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-51-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 724482200b83..f6385107c352 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1458,6 +1458,7 @@ struct task_struct {
 extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
+extern void task_numa_free(struct task_struct *p);
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   int flags)
@@ -1470,6 +1471,9 @@ static inline pid_t task_numa_group_id(struct task_struct *p)
 static inline void set_numabalancing_state(bool enabled)
 {
 }
+static inline void task_numa_free(struct task_struct *p)
+{
+}
 #endif
 
 static inline struct pid *task_pid(struct task_struct *task)

commit 83e1d2cd9eabec5164afea295ff06b941ae8e4a9
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:27 2013 +0100

    sched/numa: Use group fault statistics in numa placement
    
    This patch uses the fraction of faults on a particular node for both task
    and group, to figure out the best node to place a task.  If the task and
    group statistics disagree on what the preferred node should be then a full
    rescan will select the node with the best combined weight.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-50-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8563e3dd5c0f..724482200b83 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1356,6 +1356,7 @@ struct task_struct {
 	 * The values remain static for the duration of a PTE scan
 	 */
 	unsigned long *numa_faults;
+	unsigned long total_numa_faults;
 
 	/*
 	 * numa_faults_buffer records faults per node during the current

commit 5e1576ed0e54d419286a8096133029062b6ad456
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:26 2013 +0100

    sched/numa: Stay on the same node if CLONE_VM
    
    A newly spawned thread inside a process should stay on the same
    NUMA node as its parent. This prevents processes from being "torn"
    across multiple NUMA nodes every time they spawn a new thread.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-49-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff543851a18a..8563e3dd5c0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2021,7 +2021,7 @@ extern void wake_up_new_task(struct task_struct *tsk);
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void sched_fork(struct task_struct *p);
+extern void sched_fork(unsigned long clone_flags, struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);

commit 6688cc05473b36a0a3d3971e1adf1712919b32eb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:24 2013 +0100

    mm: numa: Do not group on RO pages
    
    And here's a little something to make sure not the whole world ends up
    in a single group.
    
    As while we don't migrate shared executable pages, we do scan/fault on
    them. And since everybody links to libc, everybody ends up in the same
    group.
    
    Suggested-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1381141781-10992-47-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0b343b1ba64..ff543851a18a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1450,13 +1450,16 @@ struct task_struct {
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
+#define TNF_MIGRATED	0x01
+#define TNF_NO_GROUP	0x02
+
 #ifdef CONFIG_NUMA_BALANCING
-extern void task_numa_fault(int last_node, int node, int pages, bool migrated);
+extern void task_numa_fault(int last_node, int node, int pages, int flags);
 extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
-				   bool migrated)
+				   int flags)
 {
 }
 static inline pid_t task_numa_group_id(struct task_struct *p)

commit e29cf08b05dc0b8151d65704d96d525a9e179a6b
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:22 2013 +0100

    sched/numa: Report a NUMA task group ID
    
    It is desirable to model from userspace how the scheduler groups tasks
    over time. This patch adds an ID to the numa_group and reports it via
    /proc/PID/status.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-45-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f587ded5c148..b0b343b1ba64 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1452,12 +1452,17 @@ struct task_struct {
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int last_node, int node, int pages, bool migrated);
+extern pid_t task_numa_group_id(struct task_struct *p);
 extern void set_numabalancing_state(bool enabled);
 #else
 static inline void task_numa_fault(int last_node, int node, int pages,
 				   bool migrated)
 {
 }
+static inline pid_t task_numa_group_id(struct task_struct *p)
+{
+	return 0;
+}
 static inline void set_numabalancing_state(bool enabled)
 {
 }

commit 8c8a743c5087bac9caac8155b8f3b367e75cdd0b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:21 2013 +0100

    sched/numa: Use {cpu, pid} to create task groups for shared faults
    
    While parallel applications tend to align their data on the cache
    boundary, they tend not to align on the page or THP boundary.
    Consequently tasks that partition their data can still "false-share"
    pages presenting a problem for optimal NUMA placement.
    
    This patch uses NUMA hinting faults to chain tasks together into
    numa_groups. As well as storing the NID a task was running on when
    accessing a page a truncated representation of the faulting PID is
    stored. If subsequent faults are from different PIDs it is reasonable
    to assume that those two tasks share a page and are candidates for
    being grouped together. Note that this patch makes no scheduling
    decisions based on the grouping information.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1381141781-10992-44-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b6619792bb13..f587ded5c148 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1347,6 +1347,9 @@ struct task_struct {
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
 
+	struct list_head numa_entry;
+	struct numa_group *numa_group;
+
 	/*
 	 * Exponential decaying average of faults on a per-node basis.
 	 * Scheduling placement decisions are made based on the these counts.

commit ac66f5477239ebd3c4e2cbf2f591ef387aa09884
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:16 2013 +0100

    sched/numa: Introduce migrate_swap()
    
    Use the new stop_two_cpus() to implement migrate_swap(), a function that
    flips two tasks between their respective cpus.
    
    I'm fairly sure there's a less crude way than employing the stop_two_cpus()
    method, but everything I tried either got horribly fragile and/or complex. So
    keep it simple for now.
    
    The notable detail is how we 'migrate' tasks that aren't runnable
    anymore. We'll make it appear like we migrated them before they went to
    sleep. The sole difference is the previous cpu in the wakeup path, so we
    override this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Link: http://lkml.kernel.org/r/1381141781-10992-39-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 14251a8ff2ea..b6619792bb13 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1043,6 +1043,8 @@ struct task_struct {
 	struct task_struct *last_wakee;
 	unsigned long wakee_flips;
 	unsigned long wakee_flip_decay_ts;
+
+	int wake_cpu;
 #endif
 	int on_rq;
 

commit 6b9a7460b6baf6c77fc3d23d927ddfc3f3f05bf3
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:11 2013 +0100

    sched/numa: Retry migration of tasks to CPU on a preferred node
    
    When a preferred node is selected for a tasks there is an attempt to migrate
    the task to a CPU there. This may fail in which case the task will only
    migrate if the active load balancer takes action. This may never happen if
    the conditions are not right. This patch will check at NUMA hinting fault
    time if another attempt should be made to migrate the task. It will only
    make an attempt once every five seconds.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-34-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d946195eec10..14251a8ff2ea 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1341,6 +1341,7 @@ struct task_struct {
 	int numa_migrate_seq;
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
+	unsigned long numa_migrate_retry;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
 

commit ac8e895bd260cb8bb19ade6a3abd44e7abe9a01d
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:03 2013 +0100

    sched/numa: Add infrastructure for split shared/private accounting of NUMA hinting faults
    
    Ideally it would be possible to distinguish between NUMA hinting faults
    that are private to a task and those that are shared.  This patch prepares
    infrastructure for separately accounting shared and private faults by
    allocating the necessary buffers and passing in relevant information. For
    now, all faults are treated as private and detection will be introduced
    later.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-26-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aecdc5a18773..d946195eec10 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1445,10 +1445,11 @@ struct task_struct {
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
 #ifdef CONFIG_NUMA_BALANCING
-extern void task_numa_fault(int node, int pages, bool migrated);
+extern void task_numa_fault(int last_node, int node, int pages, bool migrated);
 extern void set_numabalancing_state(bool enabled);
 #else
-static inline void task_numa_fault(int node, int pages, bool migrated)
+static inline void task_numa_fault(int last_node, int node, int pages,
+				   bool migrated)
 {
 }
 static inline void set_numabalancing_state(bool enabled)

commit 3a7053b3224f4a8b0e8184166190076593621617
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:00 2013 +0100

    sched/numa: Favour moving tasks towards the preferred node
    
    This patch favours moving tasks towards NUMA node that recorded a higher
    number of NUMA faults during active load balancing.  Ideally this is
    self-reinforcing as the longer the task runs on that node, the more faults
    it should incur causing task_numa_placement to keep the task running on that
    node. In reality a big weakness is that the nodes CPUs can be overloaded
    and it would be more efficient to queue tasks on an idle node and migrate
    to the new node. This would require additional smarts in the balancer so
    for now the balancer will simply prefer to place the task on the preferred
    node for a PTE scans which is controlled by the numa_balancing_settle_count
    sysctl. Once the settle_count number of scans has complete the schedule
    is free to place the task on an alternative node if the load is imbalanced.
    
    [srikar@linux.vnet.ibm.com: Fixed statistics]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    [ Tunable and use higher faults instead of preferred. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-23-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a463bc3ad437..aecdc5a18773 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -777,6 +777,7 @@ enum cpu_idle_type {
 #define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
 #define SD_OVERLAP		0x2000	/* sched_domains of this level overlap */
+#define SD_NUMA			0x4000	/* cross-node balancing */
 
 extern int __weak arch_sd_sibiling_asym_packing(void);
 

commit 745d61476ddb737aad3495fa6d9a8f8c2ee59f86
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:59 2013 +0100

    sched/numa: Update NUMA hinting faults once per scan
    
    NUMA hinting fault counts and placement decisions are both recorded in the
    same array which distorts the samples in an unpredictable fashion. The values
    linearly accumulate during the scan and then decay creating a sawtooth-like
    pattern in the per-node counts. It also means that placement decisions are
    time sensitive. At best it means that it is very difficult to state that
    the buffer holds a decaying average of past faulting behaviour. At worst,
    it can confuse the load balancer if it sees one node with an artifically high
    count due to very recent faulting activity and may create a bouncing effect.
    
    This patch adds a second array. numa_faults stores the historical data
    which is used for placement decisions. numa_faults_buffer holds the
    fault activity during the current scan window. When the scan completes,
    numa_faults decays and the values from numa_faults_buffer are copied
    across.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-22-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b1fc75e7187b..a463bc3ad437 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1343,7 +1343,20 @@ struct task_struct {
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
 
+	/*
+	 * Exponential decaying average of faults on a per-node basis.
+	 * Scheduling placement decisions are made based on the these counts.
+	 * The values remain static for the duration of a PTE scan
+	 */
 	unsigned long *numa_faults;
+
+	/*
+	 * numa_faults_buffer records faults per node during the current
+	 * scan window. When the scan completes, the counts in numa_faults
+	 * decay and these values are copied.
+	 */
+	unsigned long *numa_faults_buffer;
+
 	int numa_preferred_nid;
 #endif /* CONFIG_NUMA_BALANCING */
 

commit 688b7585d16ab57a17aa4422a3b290b3a55fa679
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:58 2013 +0100

    sched/numa: Select a preferred node with the most numa hinting faults
    
    This patch selects a preferred node for a task to run on based on the
    NUMA hinting faults. This information is later used to migrate tasks
    towards the node during balancing.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-21-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a810e95bca2b..b1fc75e7187b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1344,6 +1344,7 @@ struct task_struct {
 	struct callback_head numa_work;
 
 	unsigned long *numa_faults;
+	int numa_preferred_nid;
 #endif /* CONFIG_NUMA_BALANCING */
 
 	struct rcu_head rcu;

commit f809ca9a554dda49fb264c79e31c722e0b063ff8
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:57 2013 +0100

    sched/numa: Track NUMA hinting faults on per-node basis
    
    This patch tracks what nodes numa hinting faults were incurred on.
    This information is later used to schedule a task on the node storing
    the pages most frequently faulted by the task.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-20-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fdcb4c855072..a810e95bca2b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1342,6 +1342,8 @@ struct task_struct {
 	unsigned int numa_scan_period_max;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
+
+	unsigned long *numa_faults;
 #endif /* CONFIG_NUMA_BALANCING */
 
 	struct rcu_head rcu;

commit 598f0ec0bc996e90a806ee9564af919ea5aad401
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:55 2013 +0100

    sched/numa: Set the scan rate proportional to the memory usage of the task being scanned
    
    The NUMA PTE scan rate is controlled with a combination of the
    numa_balancing_scan_period_min, numa_balancing_scan_period_max and
    numa_balancing_scan_size. This scan rate is independent of the size
    of the task and as an aside it is further complicated by the fact that
    numa_balancing_scan_size controls how many pages are marked pte_numa and
    not how much virtual memory is scanned.
    
    In combination, it is almost impossible to meaningfully tune the min and
    max scan periods and reasoning about performance is complex when the time
    to complete a full scan is is partially a function of the tasks memory
    size. This patch alters the semantic of the min and max tunables to be
    about tuning the length time it takes to complete a scan of a tasks occupied
    virtual address space. Conceptually this is a lot easier to understand. There
    is a "sanity" check to ensure the scan rate is never extremely fast based on
    the amount of virtual memory that should be scanned in a second. The default
    of 2.5G seems arbitrary but it is to have the maximum scan rate after the
    patch roughly match the maximum scan rate before the patch was applied.
    
    On a similar note, numa_scan_period is in milliseconds and not
    jiffies. Properly placed pages slow the scanning rate but adding 10 jiffies
    to numa_scan_period means that the rate scanning slows depends on HZ which
    is confusing. Get rid of the jiffies_to_msec conversion and treat it as ms.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-18-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2ac5285db434..fdcb4c855072 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1339,6 +1339,7 @@ struct task_struct {
 	int numa_scan_seq;
 	int numa_migrate_seq;
 	unsigned int numa_scan_period;
+	unsigned int numa_scan_period_max;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
 #endif /* CONFIG_NUMA_BALANCING */

commit 75f93fed50c2abadbab6ef546b265f51ca975b27
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 27 17:30:03 2013 +0200

    sched: Revert need_resched() to look at TIF_NEED_RESCHED
    
    Yuanhan reported a serious throughput regression in his pigz
    benchmark. Using the ftrace patch I found that several idle
    paths need more TLC before we can switch the generic
    need_resched() over to preempt_need_resched.
    
    The preemption paths benefit most from preempt_need_resched and
    do indeed use it; all other need_resched() users don't really
    care that much so reverting need_resched() back to
    tif_need_resched() is the simple and safe solution.
    
    Reported-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: lkp@linux.intel.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20130927153003.GF15690@laptop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b09798b672f3..2ac5285db434 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2577,6 +2577,11 @@ static inline bool __must_check current_clr_polling_and_test(void)
 }
 #endif
 
+static __always_inline bool need_resched(void)
+{
+	return unlikely(tif_need_resched());
+}
+
 /*
  * Thread group CPU time accounting.
  */

commit a233f1120c37724938f7201fe2353b2577adaaf9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 23 19:04:26 2013 +0200

    sched: Prepare for per-cpu preempt_count
    
    When using per-cpu preempt_count variables we need to save/restore the
    preempt_count on context switch (into per task storage; for instance
    the old thread_info::preempt_count variable) because of
    PREEMPT_ACTIVE.
    
    However, this means that on fork() the preempt_count value of the last
    context switch gets copied and if we had a PREEMPT_ACTIVE switch right
    before cloning a child task the child task will now too have
    PREEMPT_ACTIVE set and start its life with an extra PREEMPT_ACTIVE
    count.
    
    Therefore we need to make init_task_preempt_count() unconditional;
    this resets whatever preempt_count we inherited from our parent
    process.
    
    Doing so for !per-cpu implementations is harmless.
    
    For !PREEMPT_COUNT kernels we need to be careful not to start life
    with an increased preempt_count.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-4k0b7oy1rcdyzochwiixuwi9@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 06ac17c7e639..b09798b672f3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -428,6 +428,14 @@ struct task_cputime {
 		.sum_exec_runtime = 0,				\
 	}
 
+#define PREEMPT_ENABLED		(PREEMPT_NEED_RESCHED)
+
+#ifdef CONFIG_PREEMPT_COUNT
+#define PREEMPT_DISABLED	(1 + PREEMPT_ENABLED)
+#else
+#define PREEMPT_DISABLED	PREEMPT_ENABLED
+#endif
+
 /*
  * Disable preemption until the scheduler is running.
  * Reset by start_kernel()->sched_init()->init_idle().
@@ -435,9 +443,7 @@ struct task_cputime {
  * We include PREEMPT_ACTIVE to avoid cond_resched() from working
  * before the scheduler is active -- see should_resched().
  */
-#define INIT_PREEMPT_COUNT	(1 + PREEMPT_ACTIVE + PREEMPT_NEED_RESCHED)
-#define PREEMPT_ENABLED		(PREEMPT_NEED_RESCHED)
-#define PREEMPT_DISABLED	(1 + PREEMPT_NEED_RESCHED)
+#define INIT_PREEMPT_COUNT	(PREEMPT_DISABLED + PREEMPT_ACTIVE)
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9fa151fb968e..06ac17c7e639 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2409,11 +2409,6 @@ static inline int signal_pending_state(long state, struct task_struct *p)
 	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
 }
 
-static inline int need_resched(void)
-{
-	return unlikely(test_preempt_need_resched());
-}
-
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return

commit f27dde8deef33c9e58027df11ceab2198601d6a6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 14 14:55:31 2013 +0200

    sched: Add NEED_RESCHED to the preempt_count
    
    In order to combine the preemption and need_resched test we need to
    fold the need_resched information into the preempt_count value.
    
    Since the NEED_RESCHED flag is set across CPUs this needs to be an
    atomic operation, however we very much want to avoid making
    preempt_count atomic, therefore we keep the existing TIF_NEED_RESCHED
    infrastructure in place but at 3 sites test it and fold its value into
    preempt_count; namely:
    
     - resched_task() when setting TIF_NEED_RESCHED on the current task
     - scheduler_ipi() when resched_task() sets TIF_NEED_RESCHED on a
                       remote task it follows it up with a reschedule IPI
                       and we can modify the cpu local preempt_count from
                       there.
     - cpu_idle_loop() for when resched_task() found tsk_is_polling().
    
    We use an inverted bitmask to indicate need_resched so that a 0 means
    both need_resched and !atomic.
    
    Also remove the barrier() in preempt_enable() between
    preempt_enable_no_resched() and preempt_check_resched() to avoid
    having to reload the preemption value and allow the compiler to use
    the flags of the previuos decrement. I couldn't come up with any sane
    reason for this barrier() to be there as preempt_enable_no_resched()
    already has a barrier() before doing the decrement.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-7a7m5qqbn5pmwnd4wko9u6da@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e783ec52295a..9fa151fb968e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -22,6 +22,7 @@ struct sched_param {
 #include <linux/errno.h>
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
+#include <linux/preempt.h>
 
 #include <asm/page.h>
 #include <asm/ptrace.h>
@@ -434,7 +435,9 @@ struct task_cputime {
  * We include PREEMPT_ACTIVE to avoid cond_resched() from working
  * before the scheduler is active -- see should_resched().
  */
-#define INIT_PREEMPT_COUNT	(1 + PREEMPT_ACTIVE)
+#define INIT_PREEMPT_COUNT	(1 + PREEMPT_ACTIVE + PREEMPT_NEED_RESCHED)
+#define PREEMPT_ENABLED		(PREEMPT_NEED_RESCHED)
+#define PREEMPT_DISABLED	(1 + PREEMPT_NEED_RESCHED)
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts
@@ -2408,7 +2411,7 @@ static inline int signal_pending_state(long state, struct task_struct *p)
 
 static inline int need_resched(void)
 {
-	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
+	return unlikely(test_preempt_need_resched());
 }
 
 /*

commit ea8117478918a4734586d35ff530721b682425be
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 11 12:43:13 2013 +0200

    sched, idle: Fix the idle polling state logic
    
    Mike reported that commit 7d1a9417 ("x86: Use generic idle loop")
    regressed several workloads and caused excessive reschedule
    interrupts.
    
    The patch in question failed to notice that the x86 code had an
    inverted sense of the polling state versus the new generic code (x86:
    default polling, generic: default !polling).
    
    Fix the two prominent x86 mwait based idle drivers and introduce a few
    new generic polling helpers (fixing the wrong smp_mb__after_clear_bit
    usage).
    
    Also switch the idle routines to using tif_need_resched() which is an
    immediate TIF_NEED_RESCHED test as opposed to need_resched which will
    end up being slightly different.
    
    Reported-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: lenb@kernel.org
    Cc: tglx@linutronix.de
    Link: http://lkml.kernel.org/n/tip-nc03imb0etuefmzybzj7sprf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b5344de1658b..e783ec52295a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2479,34 +2479,98 @@ static inline int tsk_is_polling(struct task_struct *p)
 {
 	return task_thread_info(p)->status & TS_POLLING;
 }
-static inline void current_set_polling(void)
+static inline void __current_set_polling(void)
 {
 	current_thread_info()->status |= TS_POLLING;
 }
 
-static inline void current_clr_polling(void)
+static inline bool __must_check current_set_polling_and_test(void)
+{
+	__current_set_polling();
+
+	/*
+	 * Polling state must be visible before we test NEED_RESCHED,
+	 * paired by resched_task()
+	 */
+	smp_mb();
+
+	return unlikely(tif_need_resched());
+}
+
+static inline void __current_clr_polling(void)
 {
 	current_thread_info()->status &= ~TS_POLLING;
-	smp_mb__after_clear_bit();
+}
+
+static inline bool __must_check current_clr_polling_and_test(void)
+{
+	__current_clr_polling();
+
+	/*
+	 * Polling state must be visible before we test NEED_RESCHED,
+	 * paired by resched_task()
+	 */
+	smp_mb();
+
+	return unlikely(tif_need_resched());
 }
 #elif defined(TIF_POLLING_NRFLAG)
 static inline int tsk_is_polling(struct task_struct *p)
 {
 	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
 }
-static inline void current_set_polling(void)
+
+static inline void __current_set_polling(void)
 {
 	set_thread_flag(TIF_POLLING_NRFLAG);
 }
 
-static inline void current_clr_polling(void)
+static inline bool __must_check current_set_polling_and_test(void)
+{
+	__current_set_polling();
+
+	/*
+	 * Polling state must be visible before we test NEED_RESCHED,
+	 * paired by resched_task()
+	 *
+	 * XXX: assumes set/clear bit are identical barrier wise.
+	 */
+	smp_mb__after_clear_bit();
+
+	return unlikely(tif_need_resched());
+}
+
+static inline void __current_clr_polling(void)
 {
 	clear_thread_flag(TIF_POLLING_NRFLAG);
 }
+
+static inline bool __must_check current_clr_polling_and_test(void)
+{
+	__current_clr_polling();
+
+	/*
+	 * Polling state must be visible before we test NEED_RESCHED,
+	 * paired by resched_task()
+	 */
+	smp_mb__after_clear_bit();
+
+	return unlikely(tif_need_resched());
+}
+
 #else
 static inline int tsk_is_polling(struct task_struct *p) { return 0; }
-static inline void current_set_polling(void) { }
-static inline void current_clr_polling(void) { }
+static inline void __current_set_polling(void) { }
+static inline void __current_clr_polling(void) { }
+
+static inline bool __must_check current_set_polling_and_test(void)
+{
+	return unlikely(tif_need_resched());
+}
+static inline bool __must_check current_clr_polling_and_test(void)
+{
+	return unlikely(tif_need_resched());
+}
 #endif
 
 /*

commit f48627e686a69f5215cb0761e731edb3d9859dd9
Author: Jason Low <jason.low2@hp.com>
Date:   Fri Sep 13 11:26:53 2013 -0700

    sched/balancing: Periodically decay max cost of idle balance
    
    This patch builds on patch 2 and periodically decays that max value to
    do idle balancing per sched domain by approximately 1% per second. Also
    decay the rq's max_idle_balance_cost value.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379096813-3032-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index be078ff9157f..b5344de1658b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -810,7 +810,10 @@ struct sched_domain {
 	unsigned int nr_balance_failed; /* initialise to 0 */
 
 	u64 last_update;
+
+	/* idle_balance() stats */
 	u64 max_newidle_lb_cost;
+	unsigned long next_decay_max_lb_cost;
 
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */

commit 9bd721c55c8a886b938a45198aab0ccb52f1f7fa
Author: Jason Low <jason.low2@hp.com>
Date:   Fri Sep 13 11:26:52 2013 -0700

    sched/balancing: Consider max cost of idle balance per sched domain
    
    In this patch, we keep track of the max cost we spend doing idle load balancing
    for each sched domain. If the avg time the CPU remains idle is less then the
    time we have already spent on idle balancing + the max cost of idle balancing
    in the sched domain, then we don't continue to attempt the balance. We also
    keep a per rq variable, max_idle_balance_cost, which keeps track of the max
    time spent on newidle load balances throughout all its domains so that we can
    determine the avg_idle's max value.
    
    By using the max, we avoid overrunning the average. This further reduces the
    chance we attempt balancing when the CPU is not idle for longer than the cost
    to balance.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379096813-3032-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6682da36b293..be078ff9157f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -810,6 +810,7 @@ struct sched_domain {
 	unsigned int nr_balance_failed; /* initialise to 0 */
 
 	u64 last_update;
+	u64 max_newidle_lb_cost;
 
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */

commit 3812c8c8f3953921ef18544110dafc3505c1ac62
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Sep 12 15:13:44 2013 -0700

    mm: memcg: do not trap chargers with full callstack on OOM
    
    The memcg OOM handling is incredibly fragile and can deadlock.  When a
    task fails to charge memory, it invokes the OOM killer and loops right
    there in the charge code until it succeeds.  Comparably, any other task
    that enters the charge path at this point will go to a waitqueue right
    then and there and sleep until the OOM situation is resolved.  The problem
    is that these tasks may hold filesystem locks and the mmap_sem; locks that
    the selected OOM victim may need to exit.
    
    For example, in one reported case, the task invoking the OOM killer was
    about to charge a page cache page during a write(), which holds the
    i_mutex.  The OOM killer selected a task that was just entering truncate()
    and trying to acquire the i_mutex:
    
    OOM invoking task:
      mem_cgroup_handle_oom+0x241/0x3b0
      mem_cgroup_cache_charge+0xbe/0xe0
      add_to_page_cache_locked+0x4c/0x140
      add_to_page_cache_lru+0x22/0x50
      grab_cache_page_write_begin+0x8b/0xe0
      ext3_write_begin+0x88/0x270
      generic_file_buffered_write+0x116/0x290
      __generic_file_aio_write+0x27c/0x480
      generic_file_aio_write+0x76/0xf0           # takes ->i_mutex
      do_sync_write+0xea/0x130
      vfs_write+0xf3/0x1f0
      sys_write+0x51/0x90
      system_call_fastpath+0x18/0x1d
    
    OOM kill victim:
      do_truncate+0x58/0xa0              # takes i_mutex
      do_last+0x250/0xa30
      path_openat+0xd7/0x440
      do_filp_open+0x49/0xa0
      do_sys_open+0x106/0x240
      sys_open+0x20/0x30
      system_call_fastpath+0x18/0x1d
    
    The OOM handling task will retry the charge indefinitely while the OOM
    killed task is not releasing any resources.
    
    A similar scenario can happen when the kernel OOM killer for a memcg is
    disabled and a userspace task is in charge of resolving OOM situations.
    In this case, ALL tasks that enter the OOM path will be made to sleep on
    the OOM waitqueue and wait for userspace to free resources or increase
    the group's limit.  But a userspace OOM handler is prone to deadlock
    itself on the locks held by the waiting tasks.  For example one of the
    sleeping tasks may be stuck in a brk() call with the mmap_sem held for
    writing but the userspace handler, in order to pick an optimal victim,
    may need to read files from /proc/<pid>, which tries to acquire the same
    mmap_sem for reading and deadlocks.
    
    This patch changes the way tasks behave after detecting a memcg OOM and
    makes sure nobody loops or sleeps with locks held:
    
    1. When OOMing in a user fault, invoke the OOM killer and restart the
       fault instead of looping on the charge attempt.  This way, the OOM
       victim can not get stuck on locks the looping task may hold.
    
    2. When OOMing in a user fault but somebody else is handling it
       (either the kernel OOM killer or a userspace handler), don't go to
       sleep in the charge context.  Instead, remember the OOMing memcg in
       the task struct and then fully unwind the page fault stack with
       -ENOMEM.  pagefault_out_of_memory() will then call back into the
       memcg code to check if the -ENOMEM came from the memcg, and then
       either put the task to sleep on the memcg's OOM waitqueue or just
       restart the fault.  The OOM victim can no longer get stuck on any
       lock a sleeping task may hold.
    
    Debugged by Michal Hocko.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: azurIt <azurit@pobox.sk>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9ce1fa53031f..6682da36b293 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1395,6 +1395,10 @@ struct task_struct {
 	unsigned int memcg_kmem_skip_account;
 	struct memcg_oom_info {
 		unsigned int may_oom:1;
+		unsigned int in_memcg_oom:1;
+		unsigned int oom_locked:1;
+		int wakeups;
+		struct mem_cgroup *wait_on_memcg;
 	} memcg_oom;
 #endif
 #ifdef CONFIG_UPROBES

commit 519e52473ebe9db5cdef44670d5a97f1fd53d721
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Sep 12 15:13:42 2013 -0700

    mm: memcg: enable memcg OOM killer only for user faults
    
    System calls and kernel faults (uaccess, gup) can handle an out of memory
    situation gracefully and just return -ENOMEM.
    
    Enable the memcg OOM killer only for user faults, where it's really the
    only option available.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: azurIt <azurit@pobox.sk>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 45f254dddafc..9ce1fa53031f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1393,6 +1393,9 @@ struct task_struct {
 		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
 	} memcg_batch;
 	unsigned int memcg_kmem_skip_account;
+	struct memcg_oom_info {
+		unsigned int may_oom:1;
+	} memcg_oom;
 #endif
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;

commit e1403b8edf669ff49bbdf602cc97fefa2760cb15
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:20:06 2013 -0700

    include/linux/sched.h: don't use task->pid/tgid in same_thread_group/has_group_leader_pid
    
    task_struct->pid/tgid should go away.
    
    1. Change same_thread_group() to use task->signal for comparison.
    
    2. Change has_group_leader_pid(task) to compare task_pid(task) with
       signal->leader_pid.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Sergey Dyasly <dserrg@gmail.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ce1e1c0aaa33..45f254dddafc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2169,15 +2169,15 @@ static inline bool thread_group_leader(struct task_struct *p)
  * all we care about is that we have a task with the appropriate
  * pid, we don't actually care if we have the right task.
  */
-static inline int has_group_leader_pid(struct task_struct *p)
+static inline bool has_group_leader_pid(struct task_struct *p)
 {
-	return p->pid == p->tgid;
+	return task_pid(p) == p->signal->leader_pid;
 }
 
 static inline
-int same_thread_group(struct task_struct *p1, struct task_struct *p2)
+bool same_thread_group(struct task_struct *p1, struct task_struct *p2)
 {
-	return p1->tgid == p2->tgid;
+	return p1->signal == p2->signal;
 }
 
 static inline struct task_struct *next_thread(const struct task_struct *p)

commit ae7a835cc546fc67df90edaaa0c48ae2b22a29fe
Merge: cf39c8e5352b 6b9e4fa07443
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 18:15:06 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Gleb Natapov:
     "The highlights of the release are nested EPT and pv-ticketlocks
      support (hypervisor part, guest part, which is most of the code, goes
      through tip tree).  Apart of that there are many fixes for all arches"
    
    Fix up semantic conflicts as discussed in the pull request thread..
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (88 commits)
      ARM: KVM: Add newlines to panic strings
      ARM: KVM: Work around older compiler bug
      ARM: KVM: Simplify tracepoint text
      ARM: KVM: Fix kvm_set_pte assignment
      ARM: KVM: vgic: Bump VGIC_NR_IRQS to 256
      ARM: KVM: Bugfix: vgic_bytemap_get_reg per cpu regs
      ARM: KVM: vgic: fix GICD_ICFGRn access
      ARM: KVM: vgic: simplify vgic_get_target_reg
      KVM: MMU: remove unused parameter
      KVM: PPC: Book3S PR: Rework kvmppc_mmu_book3s_64_xlate()
      KVM: PPC: Book3S PR: Make instruction fetch fallback work for system calls
      KVM: PPC: Book3S PR: Don't corrupt guest state when kernel uses VMX
      KVM: x86: update masterclock when kvmclock_offset is calculated (v2)
      KVM: PPC: Book3S: Fix compile error in XICS emulation
      KVM: PPC: Book3S PR: return appropriate error when allocation fails
      arch: powerpc: kvm: add signed type cast for comparation
      KVM: x86: add comments where MMIO does not return to the emulator
      KVM: vmx: count exits to userspace during invalid guest emulation
      KVM: rename __kvm_io_bus_sort_cmp to kvm_io_bus_cmp
      kvm: optimize away THP checks in kvm_is_mmio_pfn()
      ...

commit aee2bce3cfdcb9bf2c51c24496ee776e8202ed11
Merge: 5ec4c599a523 c95389b4cd6a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Aug 29 12:02:08 2013 +0200

    Merge branch 'linus' into perf/core
    
    Pick up the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5ea80f76a56605a190a7ea16846c82aa63dbd0aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 22 09:13:06 2013 -0700

    Revert "x86 get_unmapped_area(): use proper mmap base for bottom-up direction"
    
    This reverts commit df54d6fa54275ce59660453e29d1228c2b45a826.
    
    The commit isn't necessarily wrong, but because it recalculates the
    random mmap_base every time, it seems to confuse user memory allocators
    that expect contiguous mmap allocations even when the mmap address isn't
    specified.
    
    In particular, the MATLAB Java runtime seems to be unhappy. See
    
      https://bugzilla.kernel.org/show_bug.cgi?id=60774
    
    So we'll want to apply the random offset only once, and Radu has a patch
    for that.  Revert this older commit in order to apply the other one.
    
    Reported-by: Jeff Shorey <shoreyjeff@gmail.com>
    Cc: Radu Caragea <sinaelgl@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e9995eb5985c..078066daffd4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -314,7 +314,6 @@ struct nsproxy;
 struct user_namespace;
 
 #ifdef CONFIG_MMU
-extern unsigned long mmap_legacy_base(void);
 extern void arch_pick_mmap_layout(struct mm_struct *mm);
 extern unsigned long
 arch_get_unmapped_area(struct file *, unsigned long, unsigned long,

commit c9572f010d369d9905309f63e31180f291b66a8a
Merge: 58cea307432e d4e4ab86bcba
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Aug 15 10:00:09 2013 +0200

    Merge tag 'v3.11-rc5' into perf/core
    
    Merge Linux 3.11-rc5, to sync up with the latest upstream fixes since -rc1.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f1d6e17f540af37bb1891480143669ba7636c4cf
Merge: 28fbc8b6a29c 8c8296223f3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 14 10:04:43 2013 -0700

    Merge branch 'akpm' (patches from Andrew Morton)
    
    Merge a bunch of fixes from Andrew Morton.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>:
      fs/proc/task_mmu.c: fix buffer overflow in add_page_map()
      arch: *: Kconfig: add "kernel/Kconfig.freezer" to "arch/*/Kconfig"
      ocfs2: fix null pointer dereference in ocfs2_dir_foreach_blk_id()
      x86 get_unmapped_area(): use proper mmap base for bottom-up direction
      ocfs2: fix NULL pointer dereference in ocfs2_duplicate_clusters_by_page
      ocfs2: Revert 40bd62e to avoid regression in extended allocation
      drivers/rtc/rtc-stmp3xxx.c: provide timeout for potentially endless loop polling a HW bit
      hugetlb: fix lockdep splat caused by pmd sharing
      aoe: adjust ref of head for compound page tails
      microblaze: fix clone syscall
      mm: save soft-dirty bits on file pages
      mm: save soft-dirty bits on swapped pages
      memcg: don't initialize kmem-cache destroying work for root caches

commit df54d6fa54275ce59660453e29d1228c2b45a826
Author: Radu Caragea <sinaelgl@gmail.com>
Date:   Tue Aug 13 16:00:59 2013 -0700

    x86 get_unmapped_area(): use proper mmap base for bottom-up direction
    
    When the stack is set to unlimited, the bottomup direction is used for
    mmap-ings but the mmap_base is not used and thus effectively renders
    ASLR for mmapings along with PIE useless.
    
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Adrian Sendroiu <molecula2788@gmail.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d722490da030..923dd6ea4a0e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -314,6 +314,7 @@ struct nsproxy;
 struct user_namespace;
 
 #ifdef CONFIG_MMU
+extern unsigned long mmap_legacy_base(void);
 extern void arch_pick_mmap_layout(struct mm_struct *mm);
 extern unsigned long
 arch_get_unmapped_area(struct file *, unsigned long, unsigned long,

commit 28fbc8b6a29c849a3f03a6b05010d4b584055665
Merge: bfd36050874d bf0bd948d168
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 13 16:58:17 2013 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Docbook fixes that make 99% of the diffstat, plus a oneliner fix"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Ensure update_cfs_shares() is called for parents of continuously-running tasks
      sched: Fix some kernel-doc warnings

commit 2b44c4db2e2f1765d35163a861d301038e0c8a75
Author: Colin Cross <ccross@android.com>
Date:   Wed Jul 24 17:41:33 2013 -0700

    freezer: set PF_SUSPEND_TASK flag on tasks that call freeze_processes
    
    Calling freeze_processes sets a global flag that will cause any
    process that calls try_to_freeze to enter the refrigerator.  It
    skips sending a signal to the current task, but if the current
    task ever hits try_to_freeze, all threads will be frozen and the
    system will deadlock.
    
    Set a new flag, PF_SUSPEND_TASK, on the task that calls
    freeze_processes.  The flag notifies the freezer that the thread
    is involved in suspend and should not be frozen.  Also add a
    WARN_ON in thaw_processes if the caller does not have the
    PF_SUSPEND_TASK flag set to catch if a different task calls
    thaw_processes than the one that called freeze_processes, leaving
    a task with PF_SUSPEND_TASK permanently set on it.
    
    Threads that spawn off a task with PF_SUSPEND_TASK set (which
    swsusp does) will also have PF_SUSPEND_TASK set, preventing them
    from freezing while they are helping with suspend, but they need
    to be dead by the time suspend is triggered, otherwise they may
    run when userspace is expected to be frozen.  Add a WARN_ON in
    thaw_processes if more than one thread has the PF_SUSPEND_TASK
    flag set.
    
    Reported-and-tested-by: Michael Leun <lkml20130126@newton.leun.net>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 50d04b92ceda..d722490da030 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1628,6 +1628,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
+#define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 62470419e993f8d9d93db0effd3af4296ecb79a5
Author: Michael Wang <wangyun@linux.vnet.ibm.com>
Date:   Thu Jul 4 12:55:51 2013 +0800

    sched: Implement smarter wake-affine logic
    
    The wake-affine scheduler feature is currently always trying to pull
    the wakee close to the waker. In theory this should be beneficial if
    the waker's CPU caches hot data for the wakee, and it's also beneficial
    in the extreme ping-pong high context switch rate case.
    
    Testing shows it can benefit hackbench up to 15%.
    
    However, the feature is somewhat blind, from which some workloads
    such as pgbench suffer. It's also time-consuming algorithmically.
    
    Testing shows it can damage pgbench up to 50% - far more than the
    benefit it brings in the best case.
    
    So wake-affine should be smarter and it should realize when to
    stop its thankless effort at trying to find a suitable CPU to wake on.
    
    This patch introduces 'wakee_flips', which will be increased each
    time the task flips (switches) its wakee target.
    
    So a high 'wakee_flips' value means the task has more than one
    wakee, and the bigger the number, the higher the wakeup frequency.
    
    Now when making the decision on whether to pull or not, pay attention to
    the wakee with a high 'wakee_flips', pulling such a task may benefit
    the wakee. Also imply that the waker will face cruel competition later,
    it could be very cruel or very fast depends on the story behind
    'wakee_flips', waker therefore suffers.
    
    Furthermore, if waker also has a high 'wakee_flips', that implies that
    multiple tasks rely on it, then waker's higher latency will damage all
    of them, so pulling wakee seems to be a bad deal.
    
    Thus, when 'waker->wakee_flips / wakee->wakee_flips' becomes
    higher and higher, the cost of pulling seems to be worse and worse.
    
    The patch therefore helps the wake-affine feature to stop its pulling
    work when:
    
            wakee->wakee_flips > factor &&
            waker->wakee_flips > (factor * wakee->wakee_flips)
    
    The 'factor' here is the number of CPUs in the current CPU's NUMA node,
    so a bigger node will lead to more pulling since the trial becomes more
    severe.
    
    After applying the patch, pgbench shows up to 40% improvements and no regressions.
    
    Tested with 12 cpu x86 server and tip 3.10.0-rc7.
    
    The percentages in the final column highlight the areas with the biggest wins,
    all other areas improved as well:
    
            pgbench             base        smart
    
            | db_size | clients |  tps  |   |  tps  |
            +---------+---------+-------+   +-------+
            | 22 MB   |       1 | 10598 |   | 10796 |
            | 22 MB   |       2 | 21257 |   | 21336 |
            | 22 MB   |       4 | 41386 |   | 41622 |
            | 22 MB   |       8 | 51253 |   | 57932 |
            | 22 MB   |      12 | 48570 |   | 54000 |
            | 22 MB   |      16 | 46748 |   | 55982 | +19.75%
            | 22 MB   |      24 | 44346 |   | 55847 | +25.93%
            | 22 MB   |      32 | 43460 |   | 54614 | +25.66%
            | 7484 MB |       1 |  8951 |   |  9193 |
            | 7484 MB |       2 | 19233 |   | 19240 |
            | 7484 MB |       4 | 37239 |   | 37302 |
            | 7484 MB |       8 | 46087 |   | 50018 |
            | 7484 MB |      12 | 42054 |   | 48763 |
            | 7484 MB |      16 | 40765 |   | 51633 | +26.66%
            | 7484 MB |      24 | 37651 |   | 52377 | +39.11%
            | 7484 MB |      32 | 37056 |   | 51108 | +37.92%
            | 15 GB   |       1 |  8845 |   |  9104 |
            | 15 GB   |       2 | 19094 |   | 19162 |
            | 15 GB   |       4 | 36979 |   | 36983 |
            | 15 GB   |       8 | 46087 |   | 49977 |
            | 15 GB   |      12 | 41901 |   | 48591 |
            | 15 GB   |      16 | 40147 |   | 50651 | +26.16%
            | 15 GB   |      24 | 37250 |   | 52365 | +40.58%
            | 15 GB   |      32 | 36470 |   | 50015 | +37.14%
    
    Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51D50057.9000809@linux.vnet.ibm.com
    [ Improved the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 50d04b92ceda..4f163a8ffabf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1034,6 +1034,9 @@ struct task_struct {
 #ifdef CONFIG_SMP
 	struct llist_node wake_entry;
 	int on_cpu;
+	struct task_struct *last_wakee;
+	unsigned long wakee_flips;
+	unsigned long wakee_flip_decay_ts;
 #endif
 	int on_rq;
 

commit e04c5d76b0cfb66cadd900cf147526f2271884b8
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jul 10 22:21:57 2013 -0300

    remove sched notifier for cross-cpu migrations
    
    Linux as a guest on KVM hypervisor, the only user of the pvclock
    vsyscall interface, does not require notification on task migration
    because:
    
    1. cpu ID number maps 1:1 to per-CPU pvclock time info.
    2. per-CPU pvclock time info is updated if the
       underlying CPU changes.
    3. that version is increased whenever underlying CPU
       changes.
    
    Which is sufficient to guarantee nanoseconds counter
    is calculated properly.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 50d04b92ceda..bfc809d51745 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -107,14 +107,6 @@ extern unsigned long this_cpu_load(void);
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);
 
-/* Notifier for when a task gets migrated to a new CPU */
-struct task_migration_notifier {
-	struct task_struct *task;
-	int from_cpu;
-	int to_cpu;
-};
-extern void register_task_migration_notifier(struct notifier_block *n);
-
 extern unsigned long get_parent_ip(unsigned long addr);
 
 extern void dump_cpu_task(int cpu);

commit e69f61862ab833e9b8d3c15b6ce07fd69f3bfecc
Author: Yacine Belkadi <yacine.belkadi.1@gmail.com>
Date:   Fri Jul 12 20:45:47 2013 +0200

    sched: Fix some kernel-doc warnings
    
    When building the htmldocs (in verbose mode), scripts/kernel-doc
    reports the follwing type of warnings:
    
      Warning(kernel/sched/core.c:936): No description found for return value of 'task_curr'
      ...
    
    Fix those by:
    
     - adding the missing descriptions
     - using "Return" sections for the descriptions
    
    Signed-off-by: Yacine Belkadi <yacine.belkadi.1@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1373654747-2389-1-git-send-email-yacine.belkadi.1@gmail.com
    [ While at it, fix the cpupri_set() explanation. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 50d04b92ceda..82300247974c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1532,6 +1532,8 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
  * Test if a process is not yet dead (at most zombie state)
  * If pid_alive fails, then pointers within the task structure
  * can be stale and must not be dereferenced.
+ *
+ * Return: 1 if the process is alive. 0 otherwise.
  */
 static inline int pid_alive(struct task_struct *p)
 {
@@ -1543,6 +1545,8 @@ static inline int pid_alive(struct task_struct *p)
  * @tsk: Task structure to be checked.
  *
  * Check if a task structure is the first user space task the kernel created.
+ *
+ * Return: 1 if the task structure is init. 0 otherwise.
  */
 static inline int is_global_init(struct task_struct *tsk)
 {
@@ -1893,6 +1897,8 @@ extern struct task_struct *idle_task(int cpu);
 /**
  * is_idle_task - is the specified task an idle task?
  * @p: the task in question.
+ *
+ * Return: 1 if @p is an idle task. 0 otherwise.
  */
 static inline bool is_idle_task(const struct task_struct *p)
 {

commit 98d1e64f95b177d0f14efbdf695a1b28e1428035
Author: Michel Lespinasse <walken@google.com>
Date:   Wed Jul 10 16:05:12 2013 -0700

    mm: remove free_area_cache
    
    Since all architectures have been converted to use vm_unmapped_area(),
    there is no remaining use for the free_area_cache.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f99d57e0ae47..50d04b92ceda 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -322,8 +322,6 @@ extern unsigned long
 arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 			  unsigned long len, unsigned long pgoff,
 			  unsigned long flags);
-extern void arch_unmap_area(struct mm_struct *, unsigned long);
-extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #else
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif

commit 496322bc91e35007ed754184dcd447a02b6dd685
Merge: 2e17c5a97e23 56e0ef527b18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 9 18:24:39 2013 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "This is a re-do of the net-next pull request for the current merge
      window.  The only difference from the one I made the other day is that
      this has Eliezer's interface renames and the timeout handling changes
      made based upon your feedback, as well as a few bug fixes that have
      trickeled in.
    
      Highlights:
    
       1) Low latency device polling, eliminating the cost of interrupt
          handling and context switches.  Allows direct polling of a network
          device from socket operations, such as recvmsg() and poll().
    
          Currently ixgbe, mlx4, and bnx2x support this feature.
    
          Full high level description, performance numbers, and design in
          commit 0a4db187a999 ("Merge branch 'll_poll'")
    
          From Eliezer Tamir.
    
       2) With the routing cache removed, ip_check_mc_rcu() gets exercised
          more than ever before in the case where we have lots of multicast
          addresses.  Use a hash table instead of a simple linked list, from
          Eric Dumazet.
    
       3) Add driver for Atheros CQA98xx 802.11ac wireless devices, from
          Bartosz Markowski, Janusz Dziedzic, Kalle Valo, Marek Kwaczynski,
          Marek Puzyniak, Michal Kazior, and Sujith Manoharan.
    
       4) Support reporting the TUN device persist flag to userspace, from
          Pavel Emelyanov.
    
       5) Allow controlling network device VF link state using netlink, from
          Rony Efraim.
    
       6) Support GRE tunneling in openvswitch, from Pravin B Shelar.
    
       7) Adjust SOCK_MIN_RCVBUF and SOCK_MIN_SNDBUF for modern times, from
          Daniel Borkmann and Eric Dumazet.
    
       8) Allow controlling of TCP quickack behavior on a per-route basis,
          from Cong Wang.
    
       9) Several bug fixes and improvements to vxlan from Stephen
          Hemminger, Pravin B Shelar, and Mike Rapoport.  In particular,
          support receiving on multiple UDP ports.
    
      10) Major cleanups, particular in the area of debugging and cookie
          lifetime handline, to the SCTP protocol code.  From Daniel
          Borkmann.
    
      11) Allow packets to cross network namespaces when traversing tunnel
          devices.  From Nicolas Dichtel.
    
      12) Allow monitoring netlink traffic via AF_PACKET sockets, in a
          manner akin to how we monitor real network traffic via ptype_all.
          From Daniel Borkmann.
    
      13) Several bug fixes and improvements for the new alx device driver,
          from Johannes Berg.
    
      14) Fix scalability issues in the netem packet scheduler's time queue,
          by using an rbtree.  From Eric Dumazet.
    
      15) Several bug fixes in TCP loss recovery handling, from Yuchung
          Cheng.
    
      16) Add support for GSO segmentation of MPLS packets, from Simon
          Horman.
    
      17) Make network notifiers have a real data type for the opaque
          pointer that's passed into them.  Use this to properly handle
          network device flag changes in arp_netdev_event().  From Jiri
          Pirko and Timo TerÃ¤s.
    
      18) Convert several drivers over to module_pci_driver(), from Peter
          Huewe.
    
      19) tcp_fixup_rcvbuf() can loop 500 times over loopback, just use a
          O(1) calculation instead.  From Eric Dumazet.
    
      20) Support setting of explicit tunnel peer addresses in ipv6, just
          like ipv4.  From Nicolas Dichtel.
    
      21) Protect x86 BPF JIT against spraying attacks, from Eric Dumazet.
    
      22) Prevent a single high rate flow from overruning an individual cpu
          during RX packet processing via selective flow shedding.  From
          Willem de Bruijn.
    
      23) Don't use spinlocks in TCP md5 signing fast paths, from Eric
          Dumazet.
    
      24) Don't just drop GSO packets which are above the TBF scheduler's
          burst limit, chop them up so they are in-bounds instead.  Also
          from Eric Dumazet.
    
      25) VLAN offloads are missed when configured on top of a bridge, fix
          from Vlad Yasevich.
    
      26) Support IPV6 in ping sockets.  From Lorenzo Colitti.
    
      27) Receive flow steering targets should be updated at poll() time
          too, from David Majnemer.
    
      28) Fix several corner case regressions in PMTU/redirect handling due
          to the routing cache removal, from Timo TerÃ¤s.
    
      29) We have to be mindful of ipv4 mapped ipv6 sockets in
          upd_v6_push_pending_frames().  From Hannes Frederic Sowa.
    
      30) Fix L2TP sequence number handling bugs, from James Chapman."
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1214 commits)
      drivers/net: caif: fix wrong rtnl_is_locked() usage
      drivers/net: enic: release rtnl_lock on error-path
      vhost-net: fix use-after-free in vhost_net_flush
      net: mv643xx_eth: do not use port number as platform device id
      net: sctp: confirm route during forward progress
      virtio_net: fix race in RX VQ processing
      virtio: support unlocked queue poll
      net/cadence/macb: fix bug/typo in extracting gem_irq_read_clear bit
      Documentation: Fix references to defunct linux-net@vger.kernel.org
      net/fs: change busy poll time accounting
      net: rename low latency sockets functions to busy poll
      bridge: fix some kernel warning in multicast timer
      sfc: Fix memory leak when discarding scattered packets
      sit: fix tunnel update via netlink
      dt:net:stmmac: Add dt specific phy reset callback support.
      dt:net:stmmac: Add support to dwmac version 3.610 and 3.710
      dt:net:stmmac: Allocate platform data only if its NULL.
      net:stmmac: fix memleak in the open method
      ipv6: rt6_check_neigh should successfully verify neigh if no NUD information are available
      net: ipv6: fix wrong ping_v6_sendmsg return value
      ...

commit 7c8df28633bf0b7eb253f866029be0ac59ddb062
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 8 16:00:54 2013 -0700

    ptrace: revert "Prepare to fix racy accesses on task breakpoints"
    
    This reverts commit bf26c018490c ("Prepare to fix racy accesses on task
    breakpoints").
    
    The patch was fine but we can no longer race with SIGKILL after commit
    9899d11f6544 ("ptrace: ensure arch_ptrace/ptrace_request can never race
    with SIGKILL"), the __TASK_TRACED tracee can't be woken up and
    ->ptrace_bps[] can't go away.
    
    Now that ptrace_get_breakpoints/ptrace_put_breakpoints have no callers,
    we can kill them and remove task->ptrace_bp_refcnt.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cdd5407b37e2..75324d8157e3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1401,9 +1401,6 @@ struct task_struct {
 	} memcg_batch;
 	unsigned int memcg_kmem_skip_account;
 #endif
-#ifdef CONFIG_HAVE_HW_BREAKPOINT
-	atomic_t ptrace_bp_refcnt;
-#endif
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;
 #endif

commit 81dabb464139324c005159f5afba377104d8828d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:26 2013 -0700

    exit.c: unexport __set_special_pids()
    
    Move __set_special_pids() from exit.c to sys.c close to its single caller
    and make it static.
    
    And rename it to set_special_pids(), another helper with this name has
    gone away.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ec80684a0127..cdd5407b37e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1950,8 +1950,6 @@ extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);
 
-extern void __set_special_pids(struct pid *pid);
-
 /* per-UID process charging. */
 extern struct user_struct * alloc_uid(kuid_t);
 static inline struct user_struct *get_uid(struct user_struct *u)

commit 239003ea2e30374d1cdfee788867e497cca2366c
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Thu Jun 27 11:34:09 2013 +0530

    sched: Fix typo in struct sched_avg member description
    
    Remove extra 'for' from the description about member of
    struct sched_avg.
    
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Cc: pjt@google.com
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/20130627060409.GB18582@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0019befea59a..ec80684a0127 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -924,7 +924,7 @@ struct load_weight {
 struct sched_avg {
 	/*
 	 * These sums represent an infinite geometric series and so are bound
-	 * above by 1024/(1-y).  Thus we only need a u32 to store them for for all
+	 * above by 1024/(1-y).  Thus we only need a u32 to store them for all
 	 * choices of y < 1-2^(-32)*1024.
 	 */
 	u32 runnable_avg_sum, runnable_avg_period;

commit 141965c7494d984b2bf24efd361a3125278869c6
Author: Alex Shi <alex.shi@intel.com>
Date:   Wed Jun 26 13:05:39 2013 +0800

    Revert "sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking"
    
    Remove CONFIG_FAIR_GROUP_SCHED that covers the runnable info, then
    we can use runnable load variables.
    
    Also remove 2 CONFIG_FAIR_GROUP_SCHED setting which is not in reverted
    patch(introduced in 9ee474f), but also need to revert.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51CA76A3.3050207@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 178a8d909f14..0019befea59a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -994,12 +994,7 @@ struct sched_entity {
 	struct cfs_rq		*my_q;
 #endif
 
-/*
- * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
- * removed when useful for applications beyond shares distribution (e.g.
- * load-balance).
- */
-#if defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)
+#ifdef CONFIG_SMP
 	/* Per-entity load-tracking */
 	struct sched_avg	avg;
 #endif

commit f6f3c437d09e2f62533034e67bfb4385191e992c
Author: Simon Horman <horms@verge.net.au>
Date:   Wed May 22 14:50:31 2013 +0900

    sched: add cond_resched_rcu() helper
    
    This is intended for use in loops which read data protected by RCU and may
    have a large number of iterations.  Such an example is dumping the list of
    connections known to IPVS: ip_vs_conn_array() and ip_vs_conn_seq_next().
    
    The benefits are for CONFIG_PREEMPT_RCU=y where we save CPU cycles
    by moving rcu_read_lock and rcu_read_unlock out of large loops
    but still allowing the current task to be preempted after every
    loop iteration for the CONFIG_PREEMPT_RCU=n case.
    
    The call to cond_resched() is not needed when CONFIG_PREEMPT_RCU=y.
    Thanks to Paul E. McKenney for explaining this and for the
    final version that checks the context with CONFIG_DEBUG_ATOMIC_SLEEP=y
    for all possible configurations.
    
    The function can be empty in the CONFIG_PREEMPT_RCU case,
    rcu_read_lock and rcu_read_unlock are not needed in this case
    because the task can be preempted on indication from scheduler.
    Thanks to Peter Zijlstra for catching this and for his help
    in trying a solution that changes __might_sleep.
    
    Initial cond_resched_rcu_lock() function suggested by Eric Dumazet.
    
    Tested-by: Julian Anastasov <ja@ssi.bg>
    Signed-off-by: Julian Anastasov <ja@ssi.bg>
    Signed-off-by: Simon Horman <horms@verge.net.au>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 178a8d909f14..4ff8da189253 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2444,6 +2444,15 @@ extern int __cond_resched_softirq(void);
 	__cond_resched_softirq();					\
 })
 
+static inline void cond_resched_rcu(void)
+{
+#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)
+	rcu_read_unlock();
+	cond_resched();
+	rcu_read_lock();
+#endif
+}
+
 /*
  * Does a critical section need to be broken due to another
  * task waiting?: (technically does not depend on CONFIG_PREEMPT,

commit c4cc75c3321cad6f20d1e5325293890255c8a663
Merge: 2dbd3cac8725 2a0b4be6dd65
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 11 14:29:11 2013 -0700

    Merge git://git.infradead.org/users/eparis/audit
    
    Pull audit changes from Eric Paris:
     "Al used to send pull requests every couple of years but he told me to
      just start pushing them to you directly.
    
      Our touching outside of core audit code is pretty straight forward.  A
      couple of interface changes which hit net/.  A simple argument bug
      calling audit functions in namei.c and the removal of some assembly
      branch prediction code on ppc"
    
    * git://git.infradead.org/users/eparis/audit: (31 commits)
      audit: fix message spacing printing auid
      Revert "audit: move kaudit thread start from auditd registration to kaudit init"
      audit: vfs: fix audit_inode call in O_CREAT case of do_last
      audit: Make testing for a valid loginuid explicit.
      audit: fix event coverage of AUDIT_ANOM_LINK
      audit: use spin_lock in audit_receive_msg to process tty logging
      audit: do not needlessly take a lock in tty_audit_exit
      audit: do not needlessly take a spinlock in copy_signal
      audit: add an option to control logging of passwords with pam_tty_audit
      audit: use spin_lock_irqsave/restore in audit tty code
      helper for some session id stuff
      audit: use a consistent audit helper to log lsm information
      audit: push loginuid and sessionid processing down
      audit: stop pushing loginid, uid, sessionid as arguments
      audit: remove the old depricated kernel interface
      audit: make validity checking generic
      audit: allow checking the type of audit message in the user filter
      audit: fix build break when AUDIT_DEBUG == 2
      audit: remove duplicate export of audit_enabled
      Audit: do not print error when LSMs disabled
      ...

commit ebb37277796269da36a8bc5d72ed1e8e1fb7d34b
Merge: 4de13d7aa8f4 f50efd2fdbd9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 8 11:51:05 2013 -0700

    Merge branch 'for-3.10/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "It might look big in volume, but when categorized, not a lot of
      drivers are touched.  The pull request contains:
    
       - mtip32xx fixes from Micron.
    
       - A slew of drbd updates, this time in a nicer series.
    
       - bcache, a flash/ssd caching framework from Kent.
    
       - Fixes for cciss"
    
    * 'for-3.10/drivers' of git://git.kernel.dk/linux-block: (66 commits)
      bcache: Use bd_link_disk_holder()
      bcache: Allocator cleanup/fixes
      cciss: bug fix to prevent cciss from loading in kdump crash kernel
      cciss: add cciss_allow_hpsa module parameter
      drivers/block/mg_disk.c: add CONFIG_PM_SLEEP to suspend/resume functions
      mtip32xx: Workaround for unaligned writes
      bcache: Make sure blocksize isn't smaller than device blocksize
      bcache: Fix merge_bvec_fn usage for when it modifies the bvm
      bcache: Correctly check against BIO_MAX_PAGES
      bcache: Hack around stuff that clones up to bi_max_vecs
      bcache: Set ra_pages based on backing device's ra_pages
      bcache: Take data offset from the bdev superblock.
      mtip32xx: mtip32xx: Disable TRIM support
      mtip32xx: fix a smatch warning
      bcache: Disable broken btree fuzz tester
      bcache: Fix a format string overflow
      bcache: Fix a minor memory leak on device teardown
      bcache: Documentation updates
      bcache: Use WARN_ONCE() instead of __WARN()
      bcache: Add missing #include <linux/prefetch.h>
      ...

commit a27bb332c04cec8c4afd7912df0dc7890db27560
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 7 16:19:08 2013 -0700

    aio: don't include aio.h in sched.h
    
    Faster kernel compiles by way of fewer unnecessary includes.
    
    [akpm@linux-foundation.org: fix fallout]
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Zach Brown <zab@redhat.com>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Reviewed-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4800e9d1864c..022c085ac3c5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -313,8 +313,6 @@ extern void schedule_preempt_disabled(void);
 struct nsproxy;
 struct user_namespace;
 
-#include <linux/aio.h>
-
 #ifdef CONFIG_MMU
 extern void arch_pick_mmap_layout(struct mm_struct *mm);
 extern unsigned long

commit 534c97b0950b1967bca1c753aeaed32f5db40264
Merge: 64049d1973c1 265f22a975c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 13:23:27 2013 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'full dynticks' support from Ingo Molnar:
     "This tree from Frederic Weisbecker adds a new, (exciting! :-) core
      kernel feature to the timer and scheduler subsystems: 'full dynticks',
      or CONFIG_NO_HZ_FULL=y.
    
      This feature extends the nohz variable-size timer tick feature from
      idle to busy CPUs (running at most one task) as well, potentially
      reducing the number of timer interrupts significantly.
    
      This feature got motivated by real-time folks and the -rt tree, but
      the general utility and motivation of full-dynticks runs wider than
      that:
    
       - HPC workloads get faster: CPUs running a single task should be able
         to utilize a maximum amount of CPU power.  A periodic timer tick at
         HZ=1000 can cause a constant overhead of up to 1.0%.  This feature
         removes that overhead - and speeds up the system by 0.5%-1.0% on
         typical distro configs even on modern systems.
    
       - Real-time workload latency reduction: CPUs running critical tasks
         should experience as little jitter as possible.  The last remaining
         source of kernel-related jitter was the periodic timer tick.
    
       - A single task executing on a CPU is a pretty common situation,
         especially with an increasing number of cores/CPUs, so this feature
         helps desktop and mobile workloads as well.
    
      The cost of the feature is mainly related to increased timer
      reprogramming overhead when a CPU switches its tick period, and thus
      slightly longer to-idle and from-idle latency.
    
      Configuration-wise a third mode of operation is added to the existing
      two NOHZ kconfig modes:
    
       - CONFIG_HZ_PERIODIC: [formerly !CONFIG_NO_HZ], now explicitly named
         as a config option.  This is the traditional Linux periodic tick
         design: there's a HZ tick going on all the time, regardless of
         whether a CPU is idle or not.
    
       - CONFIG_NO_HZ_IDLE: [formerly CONFIG_NO_HZ=y], this turns off the
         periodic tick when a CPU enters idle mode.
    
       - CONFIG_NO_HZ_FULL: this new mode, in addition to turning off the
         tick when a CPU is idle, also slows the tick down to 1 Hz (one
         timer interrupt per second) when only a single task is running on a
         CPU.
    
      The .config behavior is compatible: existing !CONFIG_NO_HZ and
      CONFIG_NO_HZ=y settings get translated to the new values, without the
      user having to configure anything.  CONFIG_NO_HZ_FULL is turned off by
      default.
    
      This feature is based on a lot of infrastructure work that has been
      steadily going upstream in the last 2-3 cycles: related RCU support
      and non-periodic cputime support in particular is upstream already.
    
      This tree adds the final pieces and activates the feature.  The pull
      request is marked RFC because:
    
       - it's marked 64-bit only at the moment - the 32-bit support patch is
         small but did not get ready in time.
    
       - it has a number of fresh commits that came in after the merge
         window.  The overwhelming majority of commits are from before the
         merge window, but still some aspects of the tree are fresh and so I
         marked it RFC.
    
       - it's a pretty wide-reaching feature with lots of effects - and
         while the components have been in testing for some time, the full
         combination is still not very widely used.  That it's default-off
         should reduce its regression abilities and obviously there are no
         known regressions with CONFIG_NO_HZ_FULL=y enabled either.
    
       - the feature is not completely idempotent: there is no 100%
         equivalent replacement for a periodic scheduler/timer tick.  In
         particular there's ongoing work to map out and reduce its effects
         on scheduler load-balancing and statistics.  This should not impact
         correctness though, there are no known regressions related to this
         feature at this point.
    
       - it's a pretty ambitious feature that with time will likely be
         enabled by most Linux distros, and we'd like you to make input on
         its design/implementation, if you dislike some aspect we missed.
         Without flaming us to crisp! :-)
    
      Future plans:
    
       - there's ongoing work to reduce 1Hz to 0Hz, to essentially shut off
         the periodic tick altogether when there's a single busy task on a
         CPU.  We'd first like 1 Hz to be exposed more widely before we go
         for the 0 Hz target though.
    
       - once we reach 0 Hz we can remove the periodic tick assumption from
         nr_running>=2 as well, by essentially interrupting busy tasks only
         as frequently as the sched_latency constraints require us to do -
         once every 4-40 msecs, depending on nr_running.
    
      I am personally leaning towards biting the bullet and doing this in
      v3.10, like the -rt tree this effort has been going on for too long -
      but the final word is up to you as usual.
    
      More technical details can be found in Documentation/timers/NO_HZ.txt"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      sched: Keep at least 1 tick per second for active dynticks tasks
      rcu: Fix full dynticks' dependency on wide RCU nocb mode
      nohz: Protect smp_processor_id() in tick_nohz_task_switch()
      nohz_full: Add documentation.
      cputime_nsecs: use math64.h for nsec resolution conversion helpers
      nohz: Select VIRT_CPU_ACCOUNTING_GEN from full dynticks config
      nohz: Reduce overhead under high-freq idling patterns
      nohz: Remove full dynticks' superfluous dependency on RCU tree
      nohz: Fix unavailable tick_stop tracepoint in dynticks idle
      nohz: Add basic tracing
      nohz: Select wide RCU nocb for full dynticks
      nohz: Disable the tick when irq resume in full dynticks CPU
      nohz: Re-evaluate the tick for the new task after a context switch
      nohz: Prepare to stop the tick on irq exit
      nohz: Implement full dynticks kick
      nohz: Re-evaluate the tick from the scheduler IPI
      sched: New helper to prevent from stopping the tick in full dynticks
      sched: Kick full dynticks CPU that have more than one task enqueued.
      perf: New helper to prevent full dynticks CPUs from stopping tick
      perf: Kick full dynticks CPU if events rotation is needed
      ...

commit 265f22a975c1e4cc3a4d1f94a3ec53ffbb6f5b9f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri May 3 03:39:05 2013 +0200

    sched: Keep at least 1 tick per second for active dynticks tasks
    
    The scheduler doesn't yet fully support environments
    with a single task running without a periodic tick.
    
    In order to ensure we still maintain the duties of scheduler_tick(),
    keep at least 1 tick per second.
    
    This makes sure that we keep the progression of various scheduler
    accounting and background maintainance even with a very low granularity.
    Examples include cpu load, sched average, CFS entity vruntime,
    avenrun and events such as load balancing, amongst other details
    handled in sched_class::task_tick().
    
    This limitation will be removed in the future once we get
    these individual items to work in full dynticks CPUs.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ebf7095158a9..af008d7bad57 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1862,6 +1862,7 @@ static inline void wake_up_nohz_cpu(int cpu) { }
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(void);
+extern u64 scheduler_tick_max_deferment(void);
 #else
 static inline bool sched_can_stop_tick(void) { return false; }
 #endif

commit c032862fba51a3ca504752d3a25186b324c5ce83
Merge: fda76e074c77 8700c95adb03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 2 17:37:49 2013 +0200

    Merge commit '8700c95adb03' into timers/nohz
    
    The full dynticks tree needs the latest RCU and sched
    upstream updates in order to fix some dependencies.
    
    Merge a common upstream merge point that has these
    updates.
    
    Conflicts:
            include/linux/perf_event.h
            kernel/rcutree.h
            kernel/rcutree_plugin.h
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit e56fb2874015370e3b7f8d85051f6dce26051df9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Apr 30 15:28:20 2013 -0700

    exec: do not abuse ->cred_guard_mutex in threadgroup_lock()
    
    threadgroup_lock() takes signal->cred_guard_mutex to ensure that
    thread_group_leader() is stable.  This doesn't look nice, the scope of
    this lock in do_execve() is huge.
    
    And as Dave pointed out this can lead to deadlock, we have the
    following dependencies:
    
            do_execve:              cred_guard_mutex -> i_mutex
            cgroup_mount:           i_mutex -> cgroup_mutex
            attach_task_by_pid:     cgroup_mutex -> cred_guard_mutex
    
    Change de_thread() to take threadgroup_change_begin() around the
    switch-the-leader code and change threadgroup_lock() to avoid
    ->cred_guard_mutex.
    
    Note that de_thread() can't sleep with ->group_rwsem held, this can
    obviously deadlock with the exiting leader if the writer is active, so it
    does threadgroup_change_end() before schedule().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a22baf83c20c..6f950048b6e9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2249,27 +2249,18 @@ static inline void threadgroup_change_end(struct task_struct *tsk)
  *
  * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter
  * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or
- * perform exec.  This is useful for cases where the threadgroup needs to
- * stay stable across blockable operations.
+ * change ->group_leader/pid.  This is useful for cases where the threadgroup
+ * needs to stay stable across blockable operations.
  *
  * fork and exit paths explicitly call threadgroup_change_{begin|end}() for
  * synchronization.  While held, no new task will be added to threadgroup
  * and no existing live task will have its PF_EXITING set.
  *
- * During exec, a task goes and puts its thread group through unusual
- * changes.  After de-threading, exclusive access is assumed to resources
- * which are usually shared by tasks in the same group - e.g. sighand may
- * be replaced with a new one.  Also, the exec'ing task takes over group
- * leader role including its pid.  Exclude these changes while locked by
- * grabbing cred_guard_mutex which is used to synchronize exec path.
+ * de_thread() does threadgroup_change_{begin|end}() when a non-leader
+ * sub-thread becomes a new leader.
  */
 static inline void threadgroup_lock(struct task_struct *tsk)
 {
-	/*
-	 * exec uses exit for de-threading nesting group_rwsem inside
-	 * cred_guard_mutex. Grab cred_guard_mutex first.
-	 */
-	mutex_lock(&tsk->signal->cred_guard_mutex);
 	down_write(&tsk->signal->group_rwsem);
 }
 
@@ -2282,7 +2273,6 @@ static inline void threadgroup_lock(struct task_struct *tsk)
 static inline void threadgroup_unlock(struct task_struct *tsk)
 {
 	up_write(&tsk->signal->group_rwsem);
-	mutex_unlock(&tsk->signal->cred_guard_mutex);
 }
 #else
 static inline void threadgroup_change_begin(struct task_struct *tsk) {}

commit 403bad72b67d8b3f5a0240af5023adfa48132a65
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Apr 30 15:28:10 2013 -0700

    coredump: only SIGKILL should interrupt the coredumping task
    
    There are 2 well known and ancient problems with coredump/signals, and a
    lot of related bug reports:
    
    - do_coredump() clears TIF_SIGPENDING but of course this can't help
      if, say, SIGCHLD comes after that.
    
      In this case the coredump can fail unexpectedly. See for example
      wait_for_dump_helper()->signal_pending() check but there are other
      reasons.
    
    - At the same time, dumping a huge core on the slow media can take a
      lot of time/resources and there is no way to kill the coredumping
      task reliably. In particular this is not oom_kill-friendly.
    
    This patch tries to fix the 1st problem, and makes the preparation for the
    next changes.
    
    We add the new SIGNAL_GROUP_COREDUMP flag set by zap_threads() to indicate
    that this process dumps the core.  prepare_signal() checks this flag and
    nacks any signal except SIGKILL.
    
    Note that this check tries to be conservative, in the long term we should
    probably treat the SIGNAL_GROUP_EXIT case equally but this needs more
    discussion.  See marc.info/?l=linux-kernel&m=120508897917439
    
    Notes:
            - recalc_sigpending() doesn't check SIGNAL_GROUP_COREDUMP.
              The patch assumes that dump_write/etc paths should never
              call it, but we can change it as well.
    
            - There is another source of TIF_SIGPENDING, freezer. This
              will be addressed separately.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Mandeep Singh Baines <msb@chromium.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Neil Horman <nhorman@redhat.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 54ddcb82cddf..a22baf83c20c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -626,6 +626,7 @@ struct signal_struct {
 #define SIGNAL_STOP_STOPPED	0x00000001 /* job control stop in effect */
 #define SIGNAL_STOP_CONTINUED	0x00000002 /* SIGCONT since WCONTINUED reap */
 #define SIGNAL_GROUP_EXIT	0x00000004 /* group exit in progress */
+#define SIGNAL_GROUP_COREDUMP	0x00000008 /* coredump in progress */
 /*
  * Pending notifications to parent.
  */

commit 46e959ea2969cc1668d09b0dc55226946cf781f1
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Fri May 3 14:03:50 2013 -0400

    audit: add an option to control logging of passwords with pam_tty_audit
    
    Most commands are entered one line at a time and processed as complete lines
    in non-canonical mode.  Commands that interactively require a password, enter
    canonical mode to do this while shutting off echo.  This pair of features
    (icanon and !echo) can be used to avoid logging passwords by audit while still
    logging the rest of the command.
    
    Adding a member (log_passwd) to the struct audit_tty_status passed in by
    pam_tty_audit allows control of canonical mode without echo per task.
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2112477ff5e..c4689fe92864 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -641,6 +641,7 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_AUDIT
 	unsigned audit_tty;
+	unsigned audit_tty_log_passwd;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
 #ifdef CONFIG_CGROUPS

commit ab86e974f04b1cd827a9c7c35273834ebcd9ab38
Merge: 8700c95adb03 6f7a05d7018d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:15:40 2013 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core timer updates from Ingo Molnar:
     "The main changes in this cycle's merge are:
    
       - Implement shadow timekeeper to shorten in kernel reader side
         blocking, by Thomas Gleixner.
    
       - Posix timers enhancements by Pavel Emelyanov:
    
       - allocate timer ID per process, so that exact timer ID allocations
         can be re-created be checkpoint/restore code.
    
       - debuggability and tooling (/proc/PID/timers, etc.) improvements.
    
       - suspend/resume enhancements by Feng Tang: on certain new Intel Atom
         processors (Penwell and Cloverview), there is a feature that the
         TSC won't stop in S3 state, so the TSC value won't be reset to 0
         after resume.  This can be taken advantage of by the generic via
         the CLOCK_SOURCE_SUSPEND_NONSTOP flag: instead of using the RTC to
         recover/approximate sleep time, the main (and precise) clocksource
         can be used.
    
       - Fix /proc/timer_list for 4096 CPUs by Nathan Zimmer: on so many
         CPUs the file goes beyond 4MB of size and thus the current
         simplistic seqfile approach fails.  Convert /proc/timer_list to a
         proper seq_file with its own iterator.
    
       - Cleanups and refactorings of the core timekeeping code by John
         Stultz.
    
       - International Atomic Clock time is managed by the NTP code
         internally currently but not exposed externally.  Separate the TAI
         code out and add CLOCK_TAI support and TAI support to the hrtimer
         and posix-timer code, by John Stultz.
    
       - Add deep idle support enhacement to the broadcast clockevents core
         timer code, by Daniel Lezcano: add an opt-in CLOCK_EVT_FEAT_DYNIRQ
         clockevents feature (which will be utilized by future clockevents
         driver updates), which allows the use of IRQ affinities to avoid
         spurious wakeups of idle CPUs - the right CPU with an expiring
         timer will be woken.
    
       - Add new ARM bcm281xx clocksource driver, by Christian Daudt
    
       - ... various other fixes and cleanups"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (52 commits)
      clockevents: Set dummy handler on CPU_DEAD shutdown
      timekeeping: Update tk->cycle_last in resume
      posix-timers: Remove unused variable
      clockevents: Switch into oneshot mode even if broadcast registered late
      timer_list: Convert timer list to be a proper seq_file
      timer_list: Split timer_list_show_tickdevices
      posix-timers: Show sigevent info in proc file
      posix-timers: Introduce /proc/PID/timers file
      posix timers: Allocate timer id per process (v2)
      timekeeping: Make sure to notify hrtimers when TAI offset changes
      hrtimer: Fix ktime_add_ns() overflow on 32bit architectures
      hrtimer: Add expiry time overflow check in hrtimer_interrupt
      timekeeping: Shorten seq_count region
      timekeeping: Implement a shadow timekeeper
      timekeeping: Delay update of clock->cycle_last
      timekeeping: Store cycle_last value in timekeeper struct as well
      ntp: Remove ntp_lock, using the timekeeping locks to protect ntp state
      timekeeping: Simplify tai updating from do_adjtimex
      timekeeping: Hold timekeepering locks in do_adjtimex and hardpps
      timekeeping: Move ADJ_SETOFFSET to top level do_adjtimex()
      ...

commit 8700c95adb033843fc163d112b9d21d4fda78018
Merge: 16fa94b532b1 d190e8195b90
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 07:50:17 2013 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP/hotplug changes from Ingo Molnar:
     "This is a pretty large, multi-arch series unifying and generalizing
      the various disjunct pieces of idle routines that architectures have
      historically copied from each other and have grown in random, wildly
      inconsistent and sometimes buggy directions:
    
       101 files changed, 455 insertions(+), 1328 deletions(-)
    
      this went through a number of review and test iterations before it was
      committed, it was tested on various architectures, was exposed to
      linux-next for quite some time - nevertheless it might cause problems
      on architectures that don't read the mailing lists and don't regularly
      test linux-next.
    
      This cat herding excercise was motivated by the -rt kernel, and was
      brought to you by Thomas "the Whip" Gleixner."
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      idle: Remove GENERIC_IDLE_LOOP config switch
      um: Use generic idle loop
      ia64: Make sure interrupts enabled when we "safe_halt()"
      sparc: Use generic idle loop
      idle: Remove unused ARCH_HAS_DEFAULT_IDLE
      bfin: Fix typo in arch_cpu_idle()
      xtensa: Use generic idle loop
      x86: Use generic idle loop
      unicore: Use generic idle loop
      tile: Use generic idle loop
      tile: Enter idle with preemption disabled
      sh: Use generic idle loop
      score: Use generic idle loop
      s390: Use generic idle loop
      powerpc: Use generic idle loop
      parisc: Use generic idle loop
      openrisc: Use generic idle loop
      mn10300: Use generic idle loop
      mips: Use generic idle loop
      microblaze: Use generic idle loop
      ...

commit 16fa94b532b1958f508e07eca1a9256351241fbc
Merge: e0972916e8fe 25f55d9d01ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 07:43:28 2013 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this development cycle were:
    
       - full dynticks preparatory work by Frederic Weisbecker
    
       - factor out the cpu time accounting code better, by Li Zefan
    
       - multi-CPU load balancer cleanups and improvements by Joonsoo Kim
    
       - various smaller fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (45 commits)
      sched: Fix init NOHZ_IDLE flag
      sched: Prevent to re-select dst-cpu in load_balance()
      sched: Rename load_balance_tmpmask to load_balance_mask
      sched: Move up affinity check to mitigate useless redoing overhead
      sched: Don't consider other cpus in our group in case of NEWLY_IDLE
      sched: Explicitly cpu_idle_type checking in rebalance_domains()
      sched: Change position of resched_cpu() in load_balance()
      sched: Fix wrong rq's runnable_avg update with rt tasks
      sched: Document task_struct::personality field
      sched/cpuacct/UML: Fix header file dependency bug on the UML build
      cgroup: Kill subsys.active flag
      sched/cpuacct: No need to check subsys active state
      sched/cpuacct: Initialize cpuacct subsystem earlier
      sched/cpuacct: Initialize root cpuacct earlier
      sched/cpuacct: Allocate per_cpu cpuusage for root cpuacct statically
      sched/cpuacct: Clean up cpuacct.h
      sched/cpuacct: Remove redundant NULL checks in cpuacct_acount_field()
      sched/cpuacct: Remove redundant NULL checks in cpuacct_charge()
      sched/cpuacct: Add cpuacct_acount_field()
      sched/cpuacct: Add cpuacct_init()
      ...

commit 46d9be3e5eb01f71fc02653755d970247174b400
Merge: ce8aa4892944 cece95dfe5aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 19:07:40 2013 -0700

    Merge branch 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "A lot of activities on workqueue side this time.  The changes achieve
      the followings.
    
       - WQ_UNBOUND workqueues - the workqueues which are per-cpu - are
         updated to be able to interface with multiple backend worker pools.
         This involved a lot of churning but the end result seems actually
         neater as unbound workqueues are now a lot closer to per-cpu ones.
    
       - The ability to interface with multiple backend worker pools are
         used to implement unbound workqueues with custom attributes.
         Currently the supported attributes are the nice level and CPU
         affinity.  It may be expanded to include cgroup association in
         future.  The attributes can be specified either by calling
         apply_workqueue_attrs() or through /sys/bus/workqueue/WQ_NAME/* if
         the workqueue in question is exported through sysfs.
    
         The backend worker pools are keyed by the actual attributes and
         shared by any workqueues which share the same attributes.  When
         attributes of a workqueue are changed, the workqueue binds to the
         worker pool with the specified attributes while leaving the work
         items which are already executing in its previous worker pools
         alone.
    
         This allows converting custom worker pool implementations which
         want worker attribute tuning to use workqueues.  The writeback pool
         is already converted in block tree and there are a couple others
         are likely to follow including btrfs io workers.
    
       - WQ_UNBOUND's ability to bind to multiple worker pools is also used
         to make it NUMA-aware.  Because there's no association between work
         item issuer and the specific worker assigned to execute it, before
         this change, using unbound workqueue led to unnecessary cross-node
         bouncing and it couldn't be helped by autonuma as it requires tasks
         to have implicit node affinity and workers are assigned randomly.
    
         After these changes, an unbound workqueue now binds to multiple
         NUMA-affine worker pools so that queued work items are executed in
         the same node.  This is turned on by default but can be disabled
         system-wide or for individual workqueues.
    
         Crypto was requesting NUMA affinity as encrypting data across
         different nodes can contribute noticeable overhead and doing it
         per-cpu was too limiting for certain cases and IO throughput could
         be bottlenecked by one CPU being fully occupied while others have
         idle cycles.
    
      While the new features required a lot of changes including
      restructuring locking, it didn't complicate the execution paths much.
      The unbound workqueue handling is now closer to per-cpu ones and the
      new features are implemented by simply associating a workqueue with
      different sets of backend worker pools without changing queue,
      execution or flush paths.
    
      As such, even though the amount of change is very high, I feel
      relatively safe in that it isn't likely to cause subtle issues with
      basic correctness of work item execution and handling.  If something
      is wrong, it's likely to show up as being associated with worker pools
      with the wrong attributes or OOPS while workqueue attributes are being
      changed or during CPU hotplug.
    
      While this creates more backend worker pools, it doesn't add too many
      more workers unless, of course, there are many workqueues with unique
      combinations of attributes.  Assuming everything else is the same,
      NUMA awareness costs an extra worker pool per NUMA node with online
      CPUs.
    
      There are also a couple things which are being routed outside the
      workqueue tree.
    
       - block tree pulled in workqueue for-3.10 so that writeback worker
         pool can be converted to unbound workqueue with sysfs control
         exposed.  This simplifies the code, makes writeback workers
         NUMA-aware and allows tuning nice level and CPU affinity via sysfs.
    
       - The conversion to workqueue means that there's no 1:1 association
         between a specific worker, which makes writeback folks unhappy as
         they want to be able to tell which filesystem caused a problem from
         backtrace on systems with many filesystems mounted.  This is
         resolved by allowing work items to set debug info string which is
         printed when the task is dumped.  As this change involves unifying
         implementations of dump_stack() and friends in arch codes, it's
         being routed through Andrew's -mm tree."
    
    * 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (84 commits)
      workqueue: use kmem_cache_free() instead of kfree()
      workqueue: avoid false negative WARN_ON() in destroy_workqueue()
      workqueue: update sysfs interface to reflect NUMA awareness and a kernel param to disable NUMA affinity
      workqueue: implement NUMA affinity for unbound workqueues
      workqueue: introduce put_pwq_unlocked()
      workqueue: introduce numa_pwq_tbl_install()
      workqueue: use NUMA-aware allocation for pool_workqueues
      workqueue: break init_and_link_pwq() into two functions and introduce alloc_unbound_pwq()
      workqueue: map an unbound workqueues to multiple per-node pool_workqueues
      workqueue: move hot fields of workqueue_struct to the end
      workqueue: make workqueue->name[] fixed len
      workqueue: add workqueue->unbound_attrs
      workqueue: determine NUMA node of workers accourding to the allowed cpumask
      workqueue: drop 'H' from kworker names of unbound worker pools
      workqueue: add wq_numa_tbl_len and wq_numa_possible_cpumask[]
      workqueue: move pwq_pool_locking outside of get/put_unbound_pool()
      workqueue: fix memory leak in apply_workqueue_attrs()
      workqueue: fix unbound workqueue attrs hashing / comparison
      workqueue: fix race condition in unbound workqueue free path
      workqueue: remove pwq_lock which is no longer used
      ...

commit 916bb6d76dfa49b540baa3f7262792d1de7f1c24
Merge: d0b8883800c9 2c522836627c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 08:21:37 2013 -0700

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking changes from Ingo Molnar:
     "The most noticeable change are mutex speedups from Waiman Long, for
      higher loads.  These scalability changes should be most noticeable on
      larger server systems.
    
      There are also cleanups, fixes and debuggability improvements."
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      lockdep: Consolidate bug messages into a single print_lockdep_off() function
      lockdep: Print out additional debugging advice when we hit lockdep BUGs
      mutex: Back out architecture specific check for negative mutex count
      mutex: Queue mutex spinners with MCS lock to reduce cacheline contention
      mutex: Make more scalable by doing less atomic operations
      mutex: Move mutex spinning code from sched/core.c back to mutex.c
      locking/rtmutex/tester: Set correct permissions on sysfs files
      lockdep: Remove unnecessary 'hlock_next' variable

commit 25f55d9d01ad7a7ad248fd5af1d22675ffd202c5
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Apr 23 16:59:02 2013 +0200

    sched: Fix init NOHZ_IDLE flag
    
    On my SMP platform which is made of 5 cores in 2 clusters, I
    have the nr_busy_cpu field of sched_group_power struct that is
    not null when the platform is fully idle - which makes the
    scheduler unhappy.
    
    The root cause is:
    
    During the boot sequence, some CPUs reach the idle loop and set
    their NOHZ_IDLE flag while waiting for others CPUs to boot. But
    the nr_busy_cpus field is initialized later with the assumption
    that all CPUs are in the busy state whereas some CPUs have
    already set their NOHZ_IDLE flag.
    
    More generally, the NOHZ_IDLE flag must be initialized when new
    sched_domains are created in order to ensure that NOHZ_IDLE and
    nr_busy_cpus are aligned.
    
    This condition can be ensured by adding a synchronize_rcu()
    between the destruction of old sched_domains and the creation of
    new ones so the NOHZ_IDLE flag will not be updated with old
    sched_domain once it has been initialized. But this solution
    introduces a additionnal latency in the rebuild sequence that is
    called during cpu hotplug.
    
    As suggested by Frederic Weisbecker, another solution is to have
    the same rcu lifecycle for both NOHZ_IDLE and sched_domain
    struct. A new nohz_idle field is added to sched_domain so both
    status and sched_domain will share the same RCU lifecycle and
    will be always synchronized. In addition, there is no more need
    to protect nohz_idle against concurrent access as it is only
    modified by 2 exclusive functions called by local cpu.
    
    This solution has been prefered to the creation of a new struct
    with an extra pointer indirection for sched_domain.
    
    The synchronization is done at the cost of :
    
     - An additional indirection and a rcu_dereference for accessing nohz_idle.
     - We use only the nohz_idle field of the top sched_domain.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: linaro-kernel@lists.linaro.org
    Cc: peterz@infradead.org
    Cc: fweisbec@gmail.com
    Cc: pjt@google.com
    Cc: rostedt@goodmis.org
    Cc: efault@gmx.de
    Link: http://lkml.kernel.org/r/1366729142-14662-1-git-send-email-vincent.guittot@linaro.org
    [ Fixed !NO_HZ build bug. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6bdaa73ede13..a25168f4ab86 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -808,6 +808,8 @@ struct sched_domain {
 	unsigned int wake_idx;
 	unsigned int forkexec_idx;
 	unsigned int smt_gain;
+
+	int nohz_idle;			/* NOHZ IDLE status */
 	int flags;			/* See SD_* */
 	int level;
 

commit 6402c7dc2a19c19bd8cdc7d80878b850da418942
Merge: 77c675ba1883 60d509fa6a9c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 24 20:33:46 2013 +0200

    Merge branch 'linus' into timers/core
    Reason: Get upstream fixes before adding conflicting code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit ce831b38ca4920739a7a5b0c73b921da41f03718
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 15:15:35 2013 +0200

    sched: New helper to prevent from stopping the tick in full dynticks
    
    Provide a new helper to be called from the full dynticks engine
    before stopping the tick in order to make sure we don't stop
    it when there is more than one task running on the CPU.
    
    This way we make sure that the tick stays alive to maintain
    fairness.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1ff9e0a5de27..a74adedcdd10 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1856,6 +1856,12 @@ extern void wake_up_nohz_cpu(int cpu);
 static inline void wake_up_nohz_cpu(int cpu) { }
 #endif
 
+#ifdef CONFIG_NO_HZ_FULL
+extern bool sched_can_stop_tick(void);
+#else
+static inline bool sched_can_stop_tick(void) { return false; }
+#endif
+
 #ifdef CONFIG_SCHED_AUTOGROUP
 extern void sched_autogroup_create_attach(struct task_struct *p);
 extern void sched_autogroup_detach(struct task_struct *p);

commit 41fcb9f230bf773656d1768b73000ef720bf00c3
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Wed Apr 17 15:23:11 2013 -0400

    mutex: Move mutex spinning code from sched/core.c back to mutex.c
    
    As mentioned by Ingo, the SCHED_FEAT_OWNER_SPIN scheduler
    feature bit was really just an early hack to make with/without
    mutex-spinning testable. So it is no longer necessary.
    
    This patch removes the SCHED_FEAT_OWNER_SPIN feature bit and
    move the mutex spinning code from kernel/sched/core.c back to
    kernel/mutex.c which is where they should belong.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chandramouleeswaran Aswin <aswin@hp.com>
    Cc: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Cc: Norton Scott J <scott.norton@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1366226594-5506-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..aefe45d79f53 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -320,7 +320,6 @@ extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
-extern int mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner);
 
 struct nsproxy;
 struct user_namespace;

commit 5ed67f05f66c41e39880a6d61358438a25f9fee5
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Mon Mar 11 13:12:21 2013 +0400

    posix timers: Allocate timer id per process (v2)
    
    Currently kernel generates IDs for posix timers in a global manner --
    there's a kernel-wide IDR tree from which IDs are created. This makes
    it impossible to recreate a timer with a desired ID (in particular
    this is done by the CRIU checkpoint-restore project) -- since these
    IDs are global it may happen, that at the time we recreate a timer, the
    ID we want for it is already busy by some other timer.
    
    In order to address this, replace the IDR tree with a global hash
    table for timers and makes timer IDs unique per signal_struct (to
    which timers are linked anyway). With this, two timers belonging to
    different processes may have equal IDs and we can recreate either of
    them with the ID we want.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Matthew Helsley <matt.helsley@gmail.com>
    Link: http://lkml.kernel.org/r/513D9FF5.9010004@parallels.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..d13341b55096 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -526,7 +526,8 @@ struct signal_struct {
 	unsigned int		has_child_subreaper:1;
 
 	/* POSIX.1b Interval Timers */
-	struct list_head posix_timers;
+	int			posix_timer_id;
+	struct list_head	posix_timers;
 
 	/* ITIMER_REAL timer for the process */
 	struct hrtimer real_timer;

commit f2530dc71cf0822f90bb63ea4600caaef33a66bb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 9 09:33:34 2013 +0200

    kthread: Prevent unpark race which puts threads on the wrong cpu
    
    The smpboot threads rely on the park/unpark mechanism which binds per
    cpu threads on a particular core. Though the functionality is racy:
    
    CPU0                    CPU1                CPU2
    unpark(T)                                   wake_up_process(T)
      clear(SHOULD_PARK)    T runs
                            leave parkme() due to !SHOULD_PARK
      bind_to(CPU2)         BUG_ON(wrong CPU)
    
    We cannot let the tasks move themself to the target CPU as one of
    those tasks is actually the migration thread itself, which requires
    that it starts running on the target cpu right away.
    
    The solution to this problem is to prevent wakeups in park mode which
    are not from unpark(). That way we can guarantee that the association
    of the task to the target cpu is working correctly.
    
    Add a new task state (TASK_PARKED) which prevents other wakeups and
    use this state explicitly for the unpark wakeup.
    
    Peter noticed: Also, since the task state is visible to userspace and
    all the parked tasks are still in the PID space, its a good hint in ps
    and friends that these tasks aren't really there for the moment.
    
    The migration thread has another related issue.
    
    CPU0                     CPU1
    Bring up CPU2
    create_thread(T)
    park(T)
     wait_for_completion()
                             parkme()
                             complete()
    sched_set_stop_task()
                             schedule(TASK_PARKED)
    
    The sched_set_stop_task() call is issued while the task is on the
    runqueue of CPU1 and that confuses the hell out of the stop_task class
    on that cpu. So we need the same synchronizaion before
    sched_set_stop_task().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Dave Hansen <dave@sr71.net>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Peter Ziljstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: dhillf@gmail.com
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304091635430.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..e692a022527b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -163,9 +163,10 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define TASK_DEAD		64
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
-#define TASK_STATE_MAX		512
+#define TASK_PARKED		512
+#define TASK_STATE_MAX		1024
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZXxKW"
+#define TASK_STATE_TO_CHAR_STR "RSDTtZXxKWP"
 
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];

commit 9b89f6ba2ab56e4d9c00e7e591d6bc333137895e
Author: Andrei Epure <epure.andrei@gmail.com>
Date:   Thu Apr 11 20:30:29 2013 +0300

    sched: Document task_struct::personality field
    
    Signed-off-by: Andrei Epure <epure.andrei@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1365701429-4721-1-git-send-email-epure.andrei@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9004f6e19eac..6bdaa73ede13 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1105,8 +1105,10 @@ struct task_struct {
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
 	unsigned int jobctl;	/* JOBCTL_*, siglock protected */
-	/* ??? */
+
+	/* Used for emulating ABI behavior of previous Linux versions */
 	unsigned int personality;
+
 	unsigned did_exec:1;
 	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
 				 * execve */

commit 3a98f871ecaf44806e188184332c3fec27c8f08c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:33 2013 +0100

    idle: Implement set/clr functions for need_resched poll
    
    Implement set/clear functions for the idle need_resched poll
    implementation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215233.518839807@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6709a5813f27..21fe9a142e51 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2632,13 +2632,34 @@ static inline int tsk_is_polling(struct task_struct *p)
 {
 	return task_thread_info(p)->status & TS_POLLING;
 }
+static inline void current_set_polling(void)
+{
+	current_thread_info()->status |= TS_POLLING;
+}
+
+static inline void current_clr_polling(void)
+{
+	current_thread_info()->status &= ~TS_POLLING;
+	smp_mb__after_clear_bit();
+}
 #elif defined(TIF_POLLING_NRFLAG)
 static inline int tsk_is_polling(struct task_struct *p)
 {
 	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
 }
+static inline void current_set_polling(void)
+{
+	set_thread_flag(TIF_POLLING_NRFLAG);
+}
+
+static inline void current_clr_polling(void)
+{
+	clear_thread_flag(TIF_POLLING_NRFLAG);
+}
 #else
 static inline int tsk_is_polling(struct task_struct *p) { return 0; }
+static inline void current_set_polling(void) { }
+static inline void current_clr_polling(void) { }
 #endif
 
 /*

commit ee761f629d598579594d7e1eb8c552f3c5f71e4d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:32 2013 +0100

    arch: Consolidate tsk_is_polling()
    
    Move it to a common place. Preparatory patch for implementing
    set/clear for the idle need_resched poll implementation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130321215233.446034505@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..6709a5813f27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2621,6 +2621,26 @@ static inline int spin_needbreak(spinlock_t *lock)
 #endif
 }
 
+/*
+ * Idle thread specific functions to determine the need_resched
+ * polling state. We have two versions, one based on TS_POLLING in
+ * thread_info.status and one based on TIF_POLLING_NRFLAG in
+ * thread_info.flags
+ */
+#ifdef TS_POLLING
+static inline int tsk_is_polling(struct task_struct *p)
+{
+	return task_thread_info(p)->status & TS_POLLING;
+}
+#elif defined(TIF_POLLING_NRFLAG)
+static inline int tsk_is_polling(struct task_struct *p)
+{
+	return test_tsk_thread_flag(p, TIF_POLLING_NRFLAG);
+}
+#else
+static inline int tsk_is_polling(struct task_struct *p) { return 0; }
+#endif
+
 /*
  * Thread group CPU time accounting.
  */

commit 3451d0243c3cdfd729b36f9684a14659d4895ca3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 10 23:21:01 2011 +0200

    nohz: Rename CONFIG_NO_HZ to CONFIG_NO_HZ_COMMON
    
    We are planning to convert the dynticks Kconfig options layout
    into a choice menu. The user must be able to easily pick
    any of the following implementations: constant periodic tick,
    idle dynticks, full dynticks.
    
    As this implies a mutual exclusion, the two dynticks implementions
    need to converge on the selection of a common Kconfig option in order
    to ease the sharing of a common infrastructure.
    
    It would thus seem pretty natural to reuse CONFIG_NO_HZ to
    that end. It already implements all the idle dynticks code
    and the full dynticks depends on all that code for now.
    So ideally the choice menu would propose CONFIG_NO_HZ_IDLE and
    CONFIG_NO_HZ_EXTENDED then both would select CONFIG_NO_HZ.
    
    On the other hand we want to stay backward compatible: if
    CONFIG_NO_HZ is set in an older config file, we want to
    enable CONFIG_NO_HZ_IDLE by default.
    
    But we can't afford both at the same time or we run into
    a circular dependency:
    
    1) CONFIG_NO_HZ_IDLE and CONFIG_NO_HZ_EXTENDED both select
       CONFIG_NO_HZ
    2) If CONFIG_NO_HZ is set, we default to CONFIG_NO_HZ_IDLE
    
    We might be able to support that from Kconfig/Kbuild but it
    may not be wise to introduce such a confusing behaviour.
    
    So to solve this, create a new CONFIG_NO_HZ_COMMON option
    which gathers the common code between idle and full dynticks
    (that common code for now is simply the idle dynticks code)
    and select it from their referring Kconfig.
    
    Then we'll later create CONFIG_NO_HZ_IDLE and map CONFIG_NO_HZ
    to it for backward compatibility.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 10626e2ee688..1ff9e0a5de27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -230,7 +230,7 @@ extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern int runqueue_is_locked(int cpu);
 
-#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
+#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)
 extern void nohz_balance_enter_idle(int cpu);
 extern void set_cpu_sd_state_idle(void);
 extern int get_nohz_timer_target(void);
@@ -1758,13 +1758,13 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 }
 #endif
 
-#ifdef CONFIG_NO_HZ
+#ifdef CONFIG_NO_HZ_COMMON
 void calc_load_enter_idle(void);
 void calc_load_exit_idle(void);
 #else
 static inline void calc_load_enter_idle(void) { }
 static inline void calc_load_exit_idle(void) { }
-#endif /* CONFIG_NO_HZ */
+#endif /* CONFIG_NO_HZ_COMMON */
 
 #ifndef CONFIG_CPUMASK_OFFSTACK
 static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
@@ -1850,7 +1850,7 @@ extern void idle_task_exit(void);
 static inline void idle_task_exit(void) {}
 #endif
 
-#if defined(CONFIG_NO_HZ) && defined(CONFIG_SMP)
+#if defined(CONFIG_NO_HZ_COMMON) && defined(CONFIG_SMP)
 extern void wake_up_nohz_cpu(int cpu);
 #else
 static inline void wake_up_nohz_cpu(int cpu) { }

commit cafe563591446cf80bfbc2fe3bc72a2e36cf1060
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat Mar 23 16:11:31 2013 -0700

    bcache: A block layer cache
    
    Does writethrough and writeback caching, handles unclean shutdown, and
    has a bunch of other nifty features motivated by real world usage.
    
    See the wiki at http://bcache.evilpiepirate.org for more.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..a8482d063bc3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1576,6 +1576,10 @@ struct task_struct {
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;
 #endif
+#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
+	unsigned int	sequential_io;
+	unsigned int	sequential_io_avg;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 1c20091e77fc5a9b7d7d905176443b4822a23cdb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 10 23:21:01 2011 +0200

    nohz: Wake up full dynticks CPUs when a timer gets enqueued
    
    Wake up a CPU when a timer list timer is enqueued there and
    the target is part of the full dynticks range. Sending an IPI
    to it makes it reconsidering the next timer to program on top
    of recent updates.
    
    This may later be improved by checking if the tick is really
    stopped on the target. This would need some careful
    synchronization though. So deal with such optimization later
    and start simple.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9004f6e19eac..10626e2ee688 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1851,9 +1851,9 @@ static inline void idle_task_exit(void) {}
 #endif
 
 #if defined(CONFIG_NO_HZ) && defined(CONFIG_SMP)
-extern void wake_up_idle_cpu(int cpu);
+extern void wake_up_nohz_cpu(int cpu);
 #else
-static inline void wake_up_idle_cpu(int cpu) { }
+static inline void wake_up_nohz_cpu(int cpu) { }
 #endif
 
 #ifdef CONFIG_SCHED_AUTOGROUP

commit 14a40ffccd6163bbcd1d6f32b28a88ffe6149fc6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 19 13:45:20 2013 -0700

    sched: replace PF_THREAD_BOUND with PF_NO_SETAFFINITY
    
    PF_THREAD_BOUND was originally used to mark kernel threads which were
    bound to a specific CPU using kthread_bind() and a task with the flag
    set allows cpus_allowed modifications only to itself.  Workqueue is
    currently abusing it to prevent userland from meddling with
    cpus_allowed of workqueue workers.
    
    What we need is a flag to prevent userland from messing with
    cpus_allowed of certain kernel tasks.  In kernel, anyone can
    (incorrectly) squash the flag, and, for worker-type usages,
    restricting cpus_allowed modification to the task itself doesn't
    provide meaningful extra proection as other tasks can inject work
    items to the task anyway.
    
    This patch replaces PF_THREAD_BOUND with PF_NO_SETAFFINITY.
    sched_setaffinity() checks the flag and return -EINVAL if set.
    set_cpus_allowed_ptr() is no longer affected by the flag.
    
    This will allow simplifying workqueue worker CPU affinity management.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..e5c64f7b8c1d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1793,7 +1793,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
 #define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
-#define PF_THREAD_BOUND	0x04000000	/* Thread bound to specific cpu */
+#define PF_NO_SETAFFINITY 0x04000000	/* Userland is not allowed to meddle with cpus_allowed */
 #define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */

commit 4e3da46797f8e4d8217d2e3d6857444391b306da
Merge: 27b4b9319a3c 8b43876643a7
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Mar 8 16:41:22 2013 +0100

    Merge branch 'sched/cputime' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into sched/core
    
    Pull cputime changes from Frederic Weisbecker:
    
      * Generalize exception handling
    
      * Fix race in context tracking state restore on return from exception
        and irq exit kernel preemption
    
      * Fix cputime scaling in full dynticks accounting dynamic off-case
    
      * Fix default Kconfig value
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9fbc42eac1f6917081dc3b39922b2f1c57fdff28
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Feb 25 17:25:39 2013 +0100

    cputime: Dynamically scale cputime for full dynticks accounting
    
    The full dynticks cputime accounting is able to account either
    using the tick or the context tracking subsystem. This way
    the housekeeping CPU can keep the low overhead tick based
    solution.
    
    This latter mode has a low jiffies resolution granularity and
    need to be scaled against CFS precise runtime accounting to
    improve its result. We are doing this for CONFIG_TICK_CPU_ACCOUNTING,
    now we also need to expand it to full dynticks accounting dynamic
    off-case as well.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Mats Liljegren <mats.liljegren@enea.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..8d1b6034d80b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -570,7 +570,7 @@ struct signal_struct {
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 	struct cputime prev_cputime;
 #endif
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
@@ -1327,7 +1327,7 @@ struct task_struct {
 
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 	struct cputime prev_cputime;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN

commit 25cc7da7e6336d3bb6a5bad3d3fa96fce9a81d5b
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:07:33 2013 +0800

    sched: Move group scheduling functions out of include/linux/sched.h
    
    - Make sched_group_{set_,}runtime(), sched_group_{set_,}period()
    and sched_rt_can_attach() static.
    
    - Move sched_{create,destroy,online,offline}_group() to
    kernel/sched/sched.h.
    
    - Remove declaration of sched_group_shares().
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A7C5.3000708@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eadd113e1eb2..fc039ceccbea 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2512,28 +2512,7 @@ extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
 #ifdef CONFIG_CGROUP_SCHED
-
 extern struct task_group root_task_group;
-
-extern struct task_group *sched_create_group(struct task_group *parent);
-extern void sched_online_group(struct task_group *tg,
-			       struct task_group *parent);
-extern void sched_destroy_group(struct task_group *tg);
-extern void sched_offline_group(struct task_group *tg);
-extern void sched_move_task(struct task_struct *tsk);
-#ifdef CONFIG_FAIR_GROUP_SCHED
-extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
-extern unsigned long sched_group_shares(struct task_group *tg);
-#endif
-#ifdef CONFIG_RT_GROUP_SCHED
-extern int sched_group_set_rt_runtime(struct task_group *tg,
-				      long rt_runtime_us);
-extern long sched_group_rt_runtime(struct task_group *tg);
-extern int sched_group_set_rt_period(struct task_group *tg,
-				      long rt_period_us);
-extern long sched_group_rt_period(struct task_group *tg);
-extern int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk);
-#endif
 #endif /* CONFIG_CGROUP_SCHED */
 
 extern int task_can_switch_user(struct user_struct *up,

commit 15f803c94bd92b17708aad9e74226fd0b2c9130c
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:07:11 2013 +0800

    sched: Make default_scale_freq_power() static
    
    As default_scale_{freq,smt}_power() and update_rt_power() are
    used in kernel/sched/fair.c only, annotate them as static
    functions.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A7AF.8010900@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04b834fa14bc..eadd113e1eb2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -880,9 +880,6 @@ extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 cpumask_var_t *alloc_sched_domains(unsigned int ndoms);
 void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
 
-unsigned long default_scale_freq_power(struct sched_domain *sd, int cpu);
-unsigned long default_scale_smt_power(struct sched_domain *sd, int cpu);
-
 bool cpus_share_cache(int this_cpu, int that_cpu);
 
 #else /* CONFIG_SMP */

commit c82ba9fa7588dfd02d4dc99ad1af486304bc424c
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:55 2013 +0800

    sched: Move struct sched_class to kernel/sched/sched.h
    
    It's used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A79F.8090502@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 863b505ac48e..04b834fa14bc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -917,65 +917,6 @@ struct mempolicy;
 struct pipe_inode_info;
 struct uts_namespace;
 
-struct rq;
-struct sched_domain;
-
-#define ENQUEUE_WAKEUP		1
-#define ENQUEUE_HEAD		2
-#ifdef CONFIG_SMP
-#define ENQUEUE_WAKING		4	/* sched_class::task_waking was called */
-#else
-#define ENQUEUE_WAKING		0
-#endif
-
-#define DEQUEUE_SLEEP		1
-
-struct sched_class {
-	const struct sched_class *next;
-
-	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
-	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
-	void (*yield_task) (struct rq *rq);
-	bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
-
-	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
-
-	struct task_struct * (*pick_next_task) (struct rq *rq);
-	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
-
-#ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
-	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
-
-	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
-	void (*post_schedule) (struct rq *this_rq);
-	void (*task_waking) (struct task_struct *task);
-	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
-
-	void (*set_cpus_allowed)(struct task_struct *p,
-				 const struct cpumask *newmask);
-
-	void (*rq_online)(struct rq *rq);
-	void (*rq_offline)(struct rq *rq);
-#endif
-
-	void (*set_curr_task) (struct rq *rq);
-	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
-	void (*task_fork) (struct task_struct *p);
-
-	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
-	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
-	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
-			     int oldprio);
-
-	unsigned int (*get_rr_interval) (struct rq *rq,
-					 struct task_struct *task);
-
-#ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*task_move_group) (struct task_struct *p, int on_rq);
-#endif
-};
-
 struct load_weight {
 	unsigned long weight, inv_weight;
 };

commit b13095f07f25464de65f5ce5ea94e16813d67488
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:38 2013 +0800

    sched: Move wake flags to kernel/sched/sched.h
    
    They are used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A78E.7040609@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0d641304c0ff..863b505ac48e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -920,13 +920,6 @@ struct uts_namespace;
 struct rq;
 struct sched_domain;
 
-/*
- * wake flags
- */
-#define WF_SYNC		0x01		/* waker goes to sleep after wakup */
-#define WF_FORK		0x02		/* child wakeup after fork */
-#define WF_MIGRATED	0x04		/* internal use, task got migrated */
-
 #define ENQUEUE_WAKEUP		1
 #define ENQUEUE_HEAD		2
 #ifdef CONFIG_SMP

commit 5e6521eaa1ee581a13b904f35b80c5efeb2baccb
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:23 2013 +0800

    sched: Move struct sched_group to kernel/sched/sched.h
    
    Move struct sched_group_power and sched_group and related inline
    functions to kernel/sched/sched.h, as they are used internally
    only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A77F.2010705@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f8826d04fb12..0d641304c0ff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -780,62 +780,6 @@ enum cpu_idle_type {
 
 extern int __weak arch_sd_sibiling_asym_packing(void);
 
-struct sched_group_power {
-	atomic_t ref;
-	/*
-	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
-	 * single CPU.
-	 */
-	unsigned int power, power_orig;
-	unsigned long next_update;
-	/*
-	 * Number of busy cpus in this group.
-	 */
-	atomic_t nr_busy_cpus;
-
-	unsigned long cpumask[0]; /* iteration mask */
-};
-
-struct sched_group {
-	struct sched_group *next;	/* Must be a circular list */
-	atomic_t ref;
-
-	unsigned int group_weight;
-	struct sched_group_power *sgp;
-
-	/*
-	 * The CPUs this group covers.
-	 *
-	 * NOTE: this field is variable length. (Allocated dynamically
-	 * by attaching extra space to the end of the structure,
-	 * depending on how many CPUs the kernel has booted up with)
-	 */
-	unsigned long cpumask[0];
-};
-
-static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
-{
-	return to_cpumask(sg->cpumask);
-}
-
-/*
- * cpumask masking which cpus in the group are allowed to iterate up the domain
- * tree.
- */
-static inline struct cpumask *sched_group_mask(struct sched_group *sg)
-{
-	return to_cpumask(sg->sgp->cpumask);
-}
-
-/**
- * group_first_cpu - Returns the first cpu in the cpumask of a sched_group.
- * @group: The group whose first cpu is to be returned.
- */
-static inline unsigned int group_first_cpu(struct sched_group *group)
-{
-	return cpumask_first(sched_group_cpus(group));
-}
-
 struct sched_domain_attr {
 	int relax_domain_level;
 };
@@ -846,6 +790,8 @@ struct sched_domain_attr {
 
 extern int sched_domain_level_max;
 
+struct sched_group;
+
 struct sched_domain {
 	/* These fields must be setup */
 	struct sched_domain *parent;	/* top domain must be null terminated */

commit cc1f4b1f3faed9f2040eff2a75f510b424b3cf18
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:09 2013 +0800

    sched: Move SCHED_LOAD_SHIFT macros to kernel/sched/sched.h
    
    They are used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A771.4070104@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e880d7d115ef..f8826d04fb12 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -755,31 +755,6 @@ enum cpu_idle_type {
 	CPU_MAX_IDLE_TYPES
 };
 
-/*
- * Increase resolution of nice-level calculations for 64-bit architectures.
- * The extra resolution improves shares distribution and load balancing of
- * low-weight task groups (eg. nice +19 on an autogroup), deeper taskgroup
- * hierarchies, especially on larger systems. This is not a user-visible change
- * and does not change the user-interface for setting shares/weights.
- *
- * We increase resolution only if we have enough bits to allow this increased
- * resolution (i.e. BITS_PER_LONG > 32). The costs for increasing resolution
- * when BITS_PER_LONG <= 32 are pretty high and the returns do not justify the
- * increased costs.
- */
-#if 0 /* BITS_PER_LONG > 32 -- currently broken: it increases power usage under light load  */
-# define SCHED_LOAD_RESOLUTION	10
-# define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
-# define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)
-#else
-# define SCHED_LOAD_RESOLUTION	0
-# define scale_load(w)		(w)
-# define scale_load_down(w)	(w)
-#endif
-
-#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)
-#define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
-
 /*
  * Increase resolution of cpu_power calculations
  */

commit 090b582f27ac7b6714661020033160130e5297bd
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:05:51 2013 +0800

    sched: Remove test_sd_parent()
    
    It's unused.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A75F.4070202@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2715fbb9ea85..e880d7d115ef 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -959,15 +959,6 @@ extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 cpumask_var_t *alloc_sched_domains(unsigned int ndoms);
 void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
 
-/* Test a flag in parent sched domain */
-static inline int test_sd_parent(struct sched_domain *sd, int flag)
-{
-	if (sd->parent && (sd->parent->flags & flag))
-		return 1;
-
-	return 0;
-}
-
 unsigned long default_scale_freq_power(struct sched_domain *sd, int cpu);
 unsigned long default_scale_smt_power(struct sched_domain *sd, int cpu);
 

commit 19a37d1cd5465c10d669a296a2ea24b4c985363b
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:05:28 2013 +0800

    sched: Remove some dummy functions
    
    No one will call those functions if CONFIG_SCHED_DEBUG=n.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A748.3050206@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d35d2b6ddbfb..2715fbb9ea85 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -127,18 +127,6 @@ extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
 extern void proc_sched_set_task(struct task_struct *p);
 extern void
 print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
-#else
-static inline void
-proc_sched_show_task(struct task_struct *p, struct seq_file *m)
-{
-}
-static inline void proc_sched_set_task(struct task_struct *p)
-{
-}
-static inline void
-print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
-{
-}
 #endif
 
 /*

commit e579d2c259be42b6f29458327e5153b22414b031
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 27 17:03:15 2013 -0800

    coredump: remove redundant defines for dumpable states
    
    The existing SUID_DUMP_* defines duplicate the newer SUID_DUMPABLE_*
    defines introduced in 54b501992dd2 ("coredump: warn about unsafe
    suid_dumpable / core_pattern combo").  Remove the new ones, and use the
    prior values instead.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reported-by: Chen Gang <gang.chen@asianux.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6853bf947fde..d35d2b6ddbfb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -346,11 +346,6 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 
-/* get/set_dumpable() values */
-#define SUID_DUMPABLE_DISABLED	0
-#define SUID_DUMPABLE_ENABLED	1
-#define SUID_DUMPABLE_SAFE	2
-
 /* mm flags */
 /* dumpable bits */
 #define MMF_DUMPABLE      0  /* core dump is permitted */

commit dcad0fceae528e8007610308bad7e5a3370e5c39
Merge: f8ef15d6b9d8 7f6575f1fb96
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 19:42:08 2013 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cputime: Use local_clock() for full dynticks cputime accounting
      cputime: Constify timeval_to_cputime(timeval) argument
      sched: Move RR_TIMESLICE from sysctl.h to rt.h
      sched: Fix /proc/sched_debug failure on very very large systems
      sched: Fix /proc/sched_stat failure on very very large systems
      sched/core: Remove the obsolete and unused nr_uninterruptible() function

commit 9e2d59ad580d590134285f361a0e80f0e98c0207
Merge: 5ce1a70e2f00 235b80226b98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 23 18:50:11 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull signal handling cleanups from Al Viro:
     "This is the first pile; another one will come a bit later and will
      contain SYSCALL_DEFINE-related patches.
    
       - a bunch of signal-related syscalls (both native and compat)
         unified.
    
       - a bunch of compat syscalls switched to COMPAT_SYSCALL_DEFINE
         (fixing several potential problems with missing argument
         validation, while we are at it)
    
       - a lot of now-pointless wrappers killed
    
       - a couple of architectures (cris and hexagon) forgot to save
         altstack settings into sigframe, even though they used the
         (uninitialized) values in sigreturn; fixed.
    
       - microblaze fixes for delivery of multiple signals arriving at once
    
       - saner set of helpers for signal delivery introduced, several
         architectures switched to using those."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (143 commits)
      x86: convert to ksignal
      sparc: convert to ksignal
      arm: switch to struct ksignal * passing
      alpha: pass k_sigaction and siginfo_t using ksignal pointer
      burying unused conditionals
      make do_sigaltstack() static
      arm64: switch to generic old sigaction() (compat-only)
      arm64: switch to generic compat rt_sigaction()
      arm64: switch compat to generic old sigsuspend
      arm64: switch to generic compat rt_sigqueueinfo()
      arm64: switch to generic compat rt_sigpending()
      arm64: switch to generic compat rt_sigprocmask()
      arm64: switch to generic sigaltstack
      sparc: switch to generic old sigsuspend
      sparc: COMPAT_SYSCALL_DEFINE does all sign-extension as well as SYSCALL_DEFINE
      sparc: kill sign-extending wrappers for native syscalls
      kill sparc32_open()
      sparc: switch to use of generic old sigaction
      sparc: switch sys_compat_rt_sigaction() to COMPAT_SYSCALL_DEFINE
      mips: switch to generic sys_fork() and sys_clone()
      ...

commit 21caf2fc1931b485483ddd254b634fa8f0099963
Author: Ming Lei <ming.lei@canonical.com>
Date:   Fri Feb 22 16:34:08 2013 -0800

    mm: teach mm by current context info to not do I/O during memory allocation
    
    This patch introduces PF_MEMALLOC_NOIO on process flag('flags' field of
    'struct task_struct'), so that the flag can be set by one task to avoid
    doing I/O inside memory allocation in the task's context.
    
    The patch trys to solve one deadlock problem caused by block device, and
    the problem may happen at least in the below situations:
    
    - during block device runtime resume, if memory allocation with
      GFP_KERNEL is called inside runtime resume callback of any one of its
      ancestors(or the block device itself), the deadlock may be triggered
      inside the memory allocation since it might not complete until the block
      device becomes active and the involed page I/O finishes.  The situation
      is pointed out first by Alan Stern.  It is not a good approach to
      convert all GFP_KERNEL[1] in the path into GFP_NOIO because several
      subsystems may be involved(for example, PCI, USB and SCSI may be
      involved for usb mass stoarage device, network devices involved too in
      the iSCSI case)
    
    - during block device runtime suspend, because runtime resume need to
      wait for completion of concurrent runtime suspend.
    
    - during error handling of usb mass storage deivce, USB bus reset will
      be put on the device, so there shouldn't have any memory allocation with
      GFP_KERNEL during USB bus reset, otherwise the deadlock similar with
      above may be triggered.  Unfortunately, any usb device may include one
      mass storage interface in theory, so it requires all usb interface
      drivers to handle the situation.  In fact, most usb drivers don't know
      how to handle bus reset on the device and don't provide .pre_set() and
      .post_reset() callback at all, so USB core has to unbind and bind driver
      for these devices.  So it is still not practical to resort to GFP_NOIO
      for solving the problem.
    
    Also the introduced solution can be used by block subsystem or block
    drivers too, for example, set the PF_MEMALLOC_NOIO flag before doing
    actual I/O transfer.
    
    It is not a good idea to convert all these GFP_KERNEL in the affected
    path into GFP_NOIO because these functions doing that may be implemented
    as library and will be called in many other contexts.
    
    In fact, memalloc_noio_flags() can convert some of current static
    GFP_NOIO allocation into GFP_KERNEL back in other non-affected contexts,
    at least almost all GFP_NOIO in USB subsystem can be converted into
    GFP_KERNEL after applying the approach and make allocation with GFP_NOIO
    only happen in runtime resume/bus reset/block I/O transfer contexts
    generally.
    
    [1], several GFP_KERNEL allocation examples in runtime resume path
    
    - pci subsystem
    acpi_os_allocate
            <-acpi_ut_allocate
                    <-ACPI_ALLOCATE_ZEROED
                            <-acpi_evaluate_object
                                    <-__acpi_bus_set_power
                                            <-acpi_bus_set_power
                                                    <-acpi_pci_set_power_state
                                                            <-platform_pci_set_power_state
                                                                    <-pci_platform_power_transition
                                                                            <-__pci_complete_power_transition
                                                                                    <-pci_set_power_state
                                                                                            <-pci_restore_standard_config
                                                                                                    <-pci_pm_runtime_resume
    - usb subsystem
    usb_get_status
            <-finish_port_resume
                    <-usb_port_resume
                            <-generic_resume
                                    <-usb_resume_device
                                            <-usb_resume_both
                                                    <-usb_runtime_resume
    
    - some individual usb drivers
    usblp, uvc, gspca, most of dvb-usb-v2 media drivers, cpia2, az6007, ....
    
    That is just what I have found.  Unfortunately, this allocation can only
    be found by human being now, and there should be many not found since
    any function in the resume path(call tree) may allocate memory with
    GFP_KERNEL.
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Oliver Neukum <oneukum@suse.de>
    Cc: Jiri Kosina <jiri.kosina@suse.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Greg KH <greg@kroah.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Decotigny <david.decotigny@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e4112aad2964..c2182b53dace 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -51,6 +51,7 @@ struct sched_param {
 #include <linux/cred.h>
 #include <linux/llist.h>
 #include <linux/uidgid.h>
+#include <linux/gfp.h>
 
 #include <asm/processor.h>
 
@@ -1791,6 +1792,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
 #define PF_KSWAPD	0x00040000	/* I am kswapd */
+#define PF_MEMALLOC_NOIO 0x00080000	/* Allocating memory without IO involved */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
 #define PF_KTHREAD	0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */
@@ -1828,6 +1830,26 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
+/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags */
+static inline gfp_t memalloc_noio_flags(gfp_t flags)
+{
+	if (unlikely(current->flags & PF_MEMALLOC_NOIO))
+		flags &= ~__GFP_IO;
+	return flags;
+}
+
+static inline unsigned int memalloc_noio_save(void)
+{
+	unsigned int flags = current->flags & PF_MEMALLOC_NOIO;
+	current->flags |= PF_MEMALLOC_NOIO;
+	return flags;
+}
+
+static inline void memalloc_noio_restore(unsigned int flags)
+{
+	current->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;
+}
+
 /*
  * task->jobctl flags
  */

commit 502b24c23b44fbaa01cc2cbd86d8035845b7811f
Merge: ece8e0b2f9c9 f169007b2773
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 09:16:21 2013 -0800

    Merge branch 'for-3.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup changes from Tejun Heo:
     "Nothing too drastic.
    
       - Removal of synchronize_rcu() from userland visible paths.
    
       - Various fixes and cleanups from Li.
    
       - cgroup_rightmost_descendant() added which will be used by cpuset
         changes (it will be a separate pull request)."
    
    * 'for-3.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: fail if monitored file and event_control are in different cgroup
      cgroup: fix cgroup_rmdir() vs close(eventfd) race
      cpuset: fix cpuset_print_task_mems_allowed() vs rename() race
      cgroup: fix exit() vs rmdir() race
      cgroup: remove bogus comments in cgroup_diput()
      cgroup: remove synchronize_rcu() from cgroup_diput()
      cgroup: remove duplicate RCU free on struct cgroup
      sched: remove redundant NULL cgroup check in task_group_path()
      sched: split out css_online/css_offline from tg creation/destruction
      cgroup: initialize cgrp->dentry before css_alloc()
      cgroup: remove a NULL check in cgroup_exit()
      cgroup: fix bogus kernel warnings when cgroup_create() failed
      cgroup: remove synchronize_rcu() from rebind_subsystems()
      cgroup: remove synchronize_rcu() from cgroup_attach_{task|proc}()
      cgroup: use new hashtable implementation
      cgroups: fix cgroup_event_listener error handling
      cgroups: move cgroup_event_listener.c to tools/cgroup
      cgroup: implement cgroup_rightmost_descendant()
      cgroup: remove unused dummy cgroup_fork_callbacks()

commit 1c3e826482ab698e418c7a894440e62c76aac893
Author: Sha Zhengju <handai.szj@taobao.com>
Date:   Wed Feb 20 17:14:38 2013 +0800

    sched/core: Remove the obsolete and unused nr_uninterruptible() function
    
    Signed-off-by: Sha Zhengju <handai.szj@taobao.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1361351678-8065-1-git-send-email-handai.szj@taobao.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 33cc42130371..f9ca237df7e8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -98,7 +98,6 @@ extern int nr_threads;
 DECLARE_PER_CPU(unsigned long, process_counts);
 extern int nr_processes(void);
 extern unsigned long nr_running(void);
-extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
 extern unsigned long this_cpu_load(void);

commit e9b04b5b67ec628a5e9a312e14b6864f8f73ba12
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 20 11:14:10 2012 -0500

    make do_sigaltstack() static
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8f983293b403..6dd06494997a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2259,7 +2259,6 @@ extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
-extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
 
 static inline void restore_saved_sigmask(void)
 {

commit 8bd75c77b7c6a3954140dd2e20346aef3efe4a35
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:47:07 2013 -0600

    sched/rt: Move rt specific bits into new header file
    
    Move rt scheduler definitions out of include/linux/sched.h into
    new file include/linux/sched/rt.h
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094707.7b9f825f@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8fc9b2710a80..33cc42130371 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1164,6 +1164,7 @@ struct sched_entity {
 	/* rq "owned" by this entity/group: */
 	struct cfs_rq		*my_q;
 #endif
+
 /*
  * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
  * removed when useful for applications beyond shares distribution (e.g.
@@ -1191,6 +1192,7 @@ struct sched_rt_entity {
 #endif
 };
 
+
 struct rcu_node;
 
 enum perf_event_task_context {
@@ -1596,37 +1598,6 @@ static inline void set_numabalancing_state(bool enabled)
 }
 #endif
 
-/*
- * Priority of a process goes from 0..MAX_PRIO-1, valid RT
- * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
- * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
- * values are inverted: lower p->prio value means higher priority.
- *
- * The MAX_USER_RT_PRIO value allows the actual maximum
- * RT priority to be separate from the value exported to
- * user-space.  This allows kernel threads to set their
- * priority to a value higher than any user task. Note:
- * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
- */
-
-#define MAX_USER_RT_PRIO	100
-#define MAX_RT_PRIO		MAX_USER_RT_PRIO
-
-#define MAX_PRIO		(MAX_RT_PRIO + 40)
-#define DEFAULT_PRIO		(MAX_RT_PRIO + 20)
-
-static inline int rt_prio(int prio)
-{
-	if (unlikely(prio < MAX_RT_PRIO))
-		return 1;
-	return 0;
-}
-
-static inline int rt_task(struct task_struct *p)
-{
-	return rt_prio(p->prio);
-}
-
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -2054,26 +2025,6 @@ static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 #endif
 
-#ifdef CONFIG_RT_MUTEXES
-extern int rt_mutex_getprio(struct task_struct *p);
-extern void rt_mutex_setprio(struct task_struct *p, int prio);
-extern void rt_mutex_adjust_pi(struct task_struct *p);
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return tsk->pi_blocked_on != NULL;
-}
-#else
-static inline int rt_mutex_getprio(struct task_struct *p)
-{
-	return p->normal_prio;
-}
-# define rt_mutex_adjust_pi(p)		do { } while (0)
-static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
-{
-	return false;
-}
-#endif
-
 extern bool yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
@@ -2703,8 +2654,6 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
-extern void normalize_rt_tasks(void);
-
 #ifdef CONFIG_CGROUP_SCHED
 
 extern struct task_group root_task_group;

commit cf4aebc292fac7f34f8345664320e9d4a42ca76c
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:46:59 2013 -0600

    sched: Move sched.h sysctl bits into separate header
    
    Move the sysctl-related bits from include/linux/sched.h into
    a new file: include/linux/sched/sysctl.h. Then update source
    files requiring access to those bits by including the new
    header file.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094659.06dced96@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 719ee0815e3a..8fc9b2710a80 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -304,19 +304,6 @@ static inline void lockup_detector_init(void)
 }
 #endif
 
-#ifdef CONFIG_DETECT_HUNG_TASK
-extern unsigned int  sysctl_hung_task_panic;
-extern unsigned long sysctl_hung_task_check_count;
-extern unsigned long sysctl_hung_task_timeout_secs;
-extern unsigned long sysctl_hung_task_warnings;
-extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
-					 void __user *buffer,
-					 size_t *lenp, loff_t *ppos);
-#else
-/* Avoid need for ifdefs elsewhere in the code */
-enum { sysctl_hung_task_timeout_secs = 0 };
-#endif
-
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 
@@ -338,23 +325,6 @@ extern int mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner);
 struct nsproxy;
 struct user_namespace;
 
-/*
- * Default maximum number of active map areas, this limits the number of vmas
- * per mm struct. Users can overwrite this number by sysctl but there is a
- * problem.
- *
- * When a program's coredump is generated as ELF format, a section is created
- * per a vma. In ELF, the number of sections is represented in unsigned short.
- * This means the number of sections should be smaller than 65535 at coredump.
- * Because the kernel adds some informative sections to a image of program at
- * generating coredump, we need some margin. The number of extra sections is
- * 1-3 now and depends on arch. We use "5" as safe margin, here.
- */
-#define MAPCOUNT_ELF_CORE_MARGIN	(5)
-#define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
-
-extern int sysctl_max_map_count;
-
 #include <linux/aio.h>
 
 #ifdef CONFIG_MMU
@@ -1221,12 +1191,6 @@ struct sched_rt_entity {
 #endif
 };
 
-/*
- * default timeslice is 100 msecs (used only for SCHED_RR tasks).
- * Timeslices get refilled after they expire.
- */
-#define RR_TIMESLICE		(100 * HZ / 1000)
-
 struct rcu_node;
 
 enum perf_event_task_context {
@@ -2074,58 +2038,7 @@ extern void wake_up_idle_cpu(int cpu);
 static inline void wake_up_idle_cpu(int cpu) { }
 #endif
 
-extern unsigned int sysctl_sched_latency;
-extern unsigned int sysctl_sched_min_granularity;
-extern unsigned int sysctl_sched_wakeup_granularity;
-extern unsigned int sysctl_sched_child_runs_first;
-
-enum sched_tunable_scaling {
-	SCHED_TUNABLESCALING_NONE,
-	SCHED_TUNABLESCALING_LOG,
-	SCHED_TUNABLESCALING_LINEAR,
-	SCHED_TUNABLESCALING_END,
-};
-extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
-
-extern unsigned int sysctl_numa_balancing_scan_delay;
-extern unsigned int sysctl_numa_balancing_scan_period_min;
-extern unsigned int sysctl_numa_balancing_scan_period_max;
-extern unsigned int sysctl_numa_balancing_scan_period_reset;
-extern unsigned int sysctl_numa_balancing_scan_size;
-extern unsigned int sysctl_numa_balancing_settle_count;
-
-#ifdef CONFIG_SCHED_DEBUG
-extern unsigned int sysctl_sched_migration_cost;
-extern unsigned int sysctl_sched_nr_migrate;
-extern unsigned int sysctl_sched_time_avg;
-extern unsigned int sysctl_timer_migration;
-extern unsigned int sysctl_sched_shares_window;
-
-int sched_proc_update_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *length,
-		loff_t *ppos);
-#endif
-#ifdef CONFIG_SCHED_DEBUG
-static inline unsigned int get_sysctl_timer_migration(void)
-{
-	return sysctl_timer_migration;
-}
-#else
-static inline unsigned int get_sysctl_timer_migration(void)
-{
-	return 1;
-}
-#endif
-extern unsigned int sysctl_sched_rt_period;
-extern int sysctl_sched_rt_runtime;
-
-int sched_rt_handler(struct ctl_table *table, int write,
-		void __user *buffer, size_t *lenp,
-		loff_t *ppos);
-
 #ifdef CONFIG_SCHED_AUTOGROUP
-extern unsigned int sysctl_sched_autogroup_enabled;
-
 extern void sched_autogroup_create_attach(struct task_struct *p);
 extern void sched_autogroup_detach(struct task_struct *p);
 extern void sched_autogroup_fork(struct signal_struct *sig);
@@ -2141,10 +2054,6 @@ static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 #endif
 
-#ifdef CONFIG_CFS_BANDWIDTH
-extern unsigned int sysctl_sched_cfs_bandwidth_slice;
-#endif
-
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);

commit b2c77a57e4a0a7877e357dead7ee8acc19944f3e
Merge: c3c186403c6a 6a61671bb2f3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 5 13:10:33 2013 +0100

    Merge tag 'full-dynticks-cputime-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into sched/core
    
    Pull full-dynticks (user-space execution is undisturbed and
    receives no timer IRQs) preparation changes that convert the
    cputime accounting code to be full-dynticks ready,
    from Frederic Weisbecker:
    
     "This implements the cputime accounting on full dynticks CPUs.
    
      Typical cputime stats infrastructure relies on the timer tick and
      its periodic polling on the CPU to account the amount of time
      spent by the CPUs and the tasks per high level domains such as
      userspace, kernelspace, guest, ...
    
      Now we are preparing to implement full dynticks capability on
      Linux for Real Time and HPC users who want full CPU isolation.
      This feature requires a cputime accounting that doesn't depend
      on the timer tick.
    
      To implement it, this new cputime infrastructure plugs into
      kernel/user/guest boundaries to take snapshots of cputime and
      flush these to the stats when needed. This performs pretty
      much like CONFIG_VIRT_CPU_ACCOUNTING except that context location
      and cputime snaphots are synchronized between write and read
      side such that the latter can safely retrieve the pending tickless
      cputime of a task and add it to its latest cputime snapshot to
      return the correct result to the user."
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5a1b98d3096f1d780045f9be812335ad77aed93d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 6 13:28:21 2012 -0500

    new helper: sigsp()
    
    Normal logics for altstack handling in sigframe allocation
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6fc8f45de4e9..8f983293b403 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2305,6 +2305,17 @@ static inline int sas_ss_flags(unsigned long sp)
 		: on_sig_stack(sp) ? SS_ONSTACK : 0);
 }
 
+static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
+{
+	if (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))
+#ifdef CONFIG_STACK_GROWSUP
+		return current->sas_ss_sp;
+#else
+		return current->sas_ss_sp + current->sas_ss_size;
+#endif
+	return sp;
+}
+
 /*
  * Routines for handling mm_structs
  */

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a9c608b6154e..a9fa5145e1a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1367,6 +1367,15 @@ struct task_struct {
 	cputime_t gtime;
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
 	struct cputime prev_cputime;
+#endif
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+	seqlock_t vtime_seqlock;
+	unsigned long long vtime_snap;
+	enum {
+		VTIME_SLEEPING = 0,
+		VTIME_USER,
+		VTIME_SYS,
+	} vtime_snap_whence;
 #endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
@@ -1792,11 +1801,13 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
-static inline cputime_t task_gtime(struct task_struct *t)
-{
-	return t->gtime;
-}
-
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+extern void task_cputime(struct task_struct *t,
+			 cputime_t *utime, cputime_t *stime);
+extern void task_cputime_scaled(struct task_struct *t,
+				cputime_t *utimescaled, cputime_t *stimescaled);
+extern cputime_t task_gtime(struct task_struct *t);
+#else
 static inline void task_cputime(struct task_struct *t,
 				cputime_t *utime, cputime_t *stime)
 {
@@ -1815,6 +1826,12 @@ static inline void task_cputime_scaled(struct task_struct *t,
 	if (stimescaled)
 		*stimescaled = t->stimescaled;
 }
+
+static inline cputime_t task_gtime(struct task_struct *t)
+{
+	return t->gtime;
+}
+#endif
 extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 

commit 6fac4829ce0ef9b7f24369086ce5f0e9f38d37bc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 14:20:55 2012 +0100

    cputime: Use accessors to read task cputime stats
    
    This is in preparation for the full dynticks feature. While
    remotely reading the cputime of a task running in a full
    dynticks CPU, we'll need to do some extra-computation. This
    way we can account the time it spent tickless in userspace
    since its last cputime snapshot.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6fc8f45de4e9..a9c608b6154e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1792,6 +1792,29 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
+static inline cputime_t task_gtime(struct task_struct *t)
+{
+	return t->gtime;
+}
+
+static inline void task_cputime(struct task_struct *t,
+				cputime_t *utime, cputime_t *stime)
+{
+	if (utime)
+		*utime = t->utime;
+	if (stime)
+		*stime = t->stime;
+}
+
+static inline void task_cputime_scaled(struct task_struct *t,
+				       cputime_t *utimescaled,
+				       cputime_t *stimescaled)
+{
+	if (utimescaled)
+		*utimescaled = t->utimescaled;
+	if (stimescaled)
+		*stimescaled = t->stimescaled;
+}
 extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 

commit 57d2aa00dcec67afa52478730f2b524521af14fb
Author: Ying Xue <ying.xue@windriver.com>
Date:   Tue Jul 17 15:03:43 2012 +0800

    sched/rt: Avoid updating RT entry timeout twice within one tick period
    
    The issue below was found in 2.6.34-rt rather than mainline rt
    kernel, but the issue still exists upstream as well.
    
    So please let me describe how it was noticed on 2.6.34-rt:
    
    On this version, each softirq has its own thread, it means there
    is at least one RT FIFO task per cpu. The priority of these
    tasks is set to 49 by default. If user launches an RT FIFO task
    with priority lower than 49 of softirq RT tasks, it's possible
    there are two RT FIFO tasks enqueued one cpu runqueue at one
    moment. By current strategy of balancing RT tasks, when it comes
    to RT tasks, we really need to put them off to a CPU that they
    can run on as soon as possible. Even if it means a bit of cache
    line flushing, we want RT tasks to be run with the least latency.
    
    When the user RT FIFO task which just launched before is
    running, the sched timer tick of the current cpu happens. In this
    tick period, the timeout value of the user RT task will be
    updated once. Subsequently, we try to wake up one softirq RT
    task on its local cpu. As the priority of current user RT task
    is lower than the softirq RT task, the current task will be
    preempted by the higher priority softirq RT task. Before
    preemption, we check to see if current can readily move to a
    different cpu. If so, we will reschedule to allow the RT push logic
    to try to move current somewhere else. Whenever the woken
    softirq RT task runs, it first tries to migrate the user FIFO RT
    task over to a cpu that is running a task of lesser priority. If
    migration is done, it will send a reschedule request to the found
    cpu by IPI interrupt. Once the target cpu responds the IPI
    interrupt, it will pick the migrated user RT task to preempt its
    current task. When the user RT task is running on the new cpu,
    the sched timer tick of the cpu fires. So it will tick the user
    RT task again. This also means the RT task timeout value will be
    updated again. As the migration may be done in one tick period,
    it means the user RT task timeout value will be updated twice
    within one tick.
    
    If we set a limit on the amount of cpu time for the user RT task
    by setrlimit(RLIMIT_RTTIME), the SIGXCPU signal should be posted
    upon reaching the soft limit.
    
    But exactly when the SIGXCPU signal should be sent depends on the
    RT task timeout value. In fact the timeout mechanism of sending
    the SIGXCPU signal assumes the RT task timeout is increased once
    every tick.
    
    However, currently the timeout value may be added twice per
    tick. So it results in the SIGXCPU signal being sent earlier
    than expected.
    
    To solve this issue, we prevent the timeout value from increasing
    twice within one tick time by remembering the jiffies value of
    last updating the timeout. As long as the RT task's jiffies is
    different with the global jiffies value, we allow its timeout to
    be updated.
    
    Signed-off-by: Ying Xue <ying.xue@windriver.com>
    Signed-off-by: Fan Du <fan.du@windriver.com>
    Reviewed-by: Yong Zhang <yong.zhang0@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1342508623-2887-1-git-send-email-ying.xue@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d2112477ff5e..924e42a8df58 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1208,6 +1208,7 @@ struct sched_entity {
 struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned long timeout;
+	unsigned long watchdog_stamp;
 	unsigned int time_slice;
 
 	struct sched_rt_entity *back;

commit ace783b9bbfa2182b4a561498db3f09a0c56bc79
Author: Li Zefan <lizefan@huawei.com>
Date:   Thu Jan 24 14:30:48 2013 +0800

    sched: split out css_online/css_offline from tg creation/destruction
    
    This is a preparaton for later patches.
    
    - What do we gain from cpu_cgroup_css_online():
    
    After ss->css_alloc() and before ss->css_online(), there's a small
    window that tg->css.cgroup is NULL. With this change, tg won't be seen
    before ss->css_online(), where it's added to the global list, so we're
    guaranteed we'll never see NULL tg->css.cgroup.
    
    - What do we gain from cpu_cgroup_css_offline():
    
    tg is freed via RCU, so is cgroup. Without this change, This is how
    synchronization works:
    
    cgroup_rmdir()
      no ss->css_offline()
    diput()
      syncornize_rcu()
      ss->css_free()       <-- unregister tg, and free it via call_rcu()
      kfree_rcu(cgroup)    <-- wait possible refs to cgroup, and free cgroup
    
    We can't just kfree(cgroup), because tg might access tg->css.cgroup.
    
    With this change:
    
    cgroup_rmdir()
      ss->css_offline()    <-- unregister tg
    diput()
      synchronize_rcu()    <-- wait possible refs to tg and cgroup
      ss->css_free()       <-- free tg
      kfree_rcu(cgroup)    <-- free cgroup
    
    As you see, kfree_rcu() is redundant now.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 206bb089c06b..577eb973de7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2750,7 +2750,10 @@ extern void normalize_rt_tasks(void);
 extern struct task_group root_task_group;
 
 extern struct task_group *sched_create_group(struct task_group *parent);
+extern void sched_online_group(struct task_group *tg,
+			       struct task_group *parent);
 extern void sched_destroy_group(struct task_group *tg);
+extern void sched_offline_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);

commit 910ffdb18a6408e14febbb6e4b6840fd2c928c82
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 21 20:47:41 2013 +0100

    ptrace: introduce signal_wake_up_state() and ptrace_signal_wake_up()
    
    Cleanup and preparation for the next change.
    
    signal_wake_up(resume => true) is overused. None of ptrace/jctl callers
    actually want to wakeup a TASK_WAKEKILL task, but they can't specify the
    necessary mask.
    
    Turn signal_wake_up() into signal_wake_up_state(state), reintroduce
    signal_wake_up() as a trivial helper, and add ptrace_signal_wake_up()
    which adds __TASK_TRACED.
    
    This way ptrace_signal_wake_up() can work "inside" ptrace_request()
    even if the tracee doesn't have the TASK_WAKEKILL bit set.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6fc8f45de4e9..d2112477ff5e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2714,7 +2714,16 @@ static inline void thread_group_cputime_init(struct signal_struct *sig)
 extern void recalc_sigpending_and_wake(struct task_struct *t);
 extern void recalc_sigpending(void);
 
-extern void signal_wake_up(struct task_struct *t, int resume_stopped);
+extern void signal_wake_up_state(struct task_struct *t, unsigned int state);
+
+static inline void signal_wake_up(struct task_struct *t, bool resume)
+{
+	signal_wake_up_state(t, resume ? TASK_WAKEKILL : 0);
+}
+static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)
+{
+	signal_wake_up_state(t, resume ? __TASK_TRACED : 0);
+}
 
 /*
  * Wrappers for p->thread_info->cpu access. No-op on UP.

commit 774a1221e862b343388347bac9b318767336b20b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 15 18:52:51 2013 -0800

    module, async: async_synchronize_full() on module init iff async is used
    
    If the default iosched is built as module, the kernel may deadlock
    while trying to load the iosched module on device probe if the probing
    was running off async.  This is because async_synchronize_full() at
    the end of module init ends up waiting for the async job which
    initiated the module loading.
    
     async A                                modprobe
    
     1. finds a device
     2. registers the block device
     3. request_module(default iosched)
                                            4. modprobe in userland
                                            5. load and init module
                                            6. async_synchronize_full()
    
    Async A waits for modprobe to finish in request_module() and modprobe
    waits for async A to finish in async_synchronize_full().
    
    Because there's no easy to track dependency once control goes out to
    userland, implementing properly nested flushing is difficult.  For
    now, make module init perform async_synchronize_full() iff module init
    has queued async jobs as suggested by Linus.
    
    This avoids the described deadlock because iosched module doesn't use
    async and thus wouldn't invoke async_synchronize_full().  This is
    hacky and incomplete.  It will deadlock if async module loading nests;
    however, this works around the known problem case and seems to be the
    best of bad options.
    
    For more details, please refer to the following thread.
    
      http://thread.gmane.org/gmane.linux.kernel/1420814
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Alex Riesen <raa.lkml@gmail.com>
    Tested-by: Ming Lei <ming.lei@canonical.com>
    Tested-by: Alex Riesen <raa.lkml@gmail.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 206bb089c06b..6fc8f45de4e9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1810,6 +1810,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_NPROC_EXCEEDED 0x00001000	/* set_user noticed that RLIMIT_NPROC was exceeded */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
+#define PF_USED_ASYNC	0x00004000	/* used async_schedule*(), used by module init */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */

commit 54d46ea993744c5408e39ce0cb4851e13cbea716
Merge: f59dc2bb5a50 50ececcfa7d1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 20 18:05:28 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull signal handling cleanups from Al Viro:
     "sigaltstack infrastructure + conversion for x86, alpha and um,
      COMPAT_SYSCALL_DEFINE infrastructure.
    
      Note that there are several conflicts between "unify
      SS_ONSTACK/SS_DISABLE definitions" and UAPI patches in mainline;
      resolution is trivial - just remove definitions of SS_ONSTACK and
      SS_DISABLED from arch/*/uapi/asm/signal.h; they are all identical and
      include/uapi/linux/signal.h contains the unified variant."
    
    Fixed up conflicts as per Al.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      alpha: switch to generic sigaltstack
      new helpers: __save_altstack/__compat_save_altstack, switch x86 and um to those
      generic compat_sys_sigaltstack()
      introduce generic sys_sigaltstack(), switch x86 and um to it
      new helper: compat_user_stack_pointer()
      new helper: restore_altstack()
      unify SS_ONSTACK/SS_DISABLE definitions
      new helper: current_user_stack_pointer()
      missing user_stack_pointer() instances
      Bury the conditionals from kernel_thread/kernel_execve series
      COMPAT_SYSCALL_DEFINE: infrastructure

commit ae903caae267154de7cf8576b130ff474630596b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Dec 14 12:44:11 2012 -0500

    Bury the conditionals from kernel_thread/kernel_execve series
    
    All architectures have
            CONFIG_GENERIC_KERNEL_THREAD
            CONFIG_GENERIC_KERNEL_EXECVE
            __ARCH_WANT_SYS_EXECVE
    None of them have __ARCH_WANT_KERNEL_EXECVE and there are only two callers
    of kernel_execve() (which is a trivial wrapper for do_execve() now) left.
    Kill the conditionals and make both callers use do_execve().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1162258bcaf0..9e5a54e3d84f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2291,9 +2291,7 @@ extern int do_execve(const char *,
 		     const char __user * const __user *);
 extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
-#ifdef CONFIG_GENERIC_KERNEL_THREAD
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
-#endif
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);

commit 0e9d92f2d02d8c8320f0502307c688d07bdac2b3
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:42 2012 -0800

    memcg: skip memcg kmem allocations in specified code regions
    
    Create a mechanism that skip memcg allocations during certain pieces of
    our core code.  It basically works in the same way as
    preempt_disable()/preempt_enable(): By marking a region under which all
    allocations will be accounted to the root memcg.
    
    We need this to prevent races in early cache creation, when we
    allocate data using caches that are not necessarily created already.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    yCc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9914c662ed7b..f712465b05c5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1597,6 +1597,7 @@ struct task_struct {
 		unsigned long nr_pages;	/* uncharged usage */
 		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
 	} memcg_batch;
+	unsigned int memcg_kmem_skip_account;
 #endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;

commit a5ba911ec3792168530d35e16a8ec3b6fc60bcb5
Author: Gao feng <gaofeng@cn.fujitsu.com>
Date:   Mon Dec 17 16:03:22 2012 -0800

    pidns: remove unused is_container_init()
    
    Since commit 1cdcbec1a337 ("CRED: Neuter sys_capset()")
    is_container_init() has no callers.
    
    Signed-off-by: Gao feng <gaofeng@cn.fujitsu.com>
    Cc: David Howells <dhowells@redhat.com>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b089c92c609b..9914c662ed7b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1778,12 +1778,6 @@ static inline int is_global_init(struct task_struct *tsk)
 	return tsk->pid == 1;
 }
 
-/*
- * is_container_init:
- * check whether in the task is init in its own pid namespace.
- */
-extern int is_container_init(struct task_struct *tsk);
-
 extern struct pid *cad_pid;
 
 extern void free_task(struct task_struct *tsk);

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 66cdd0ceaf65a18996f561b770eedde1d123b019
Merge: 896ea17d3da5 58b7825bc324
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 15:31:08 2012 -0800

    Merge tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "Considerable KVM/PPC work, x86 kvmclock vsyscall support,
      IA32_TSC_ADJUST MSR emulation, amongst others."
    
    Fix up trivial conflict in kernel/sched/core.c due to cross-cpu
    migration notifier added next to rq migration call-back.
    
    * tag 'kvm-3.8-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (156 commits)
      KVM: emulator: fix real mode segment checks in address linearization
      VMX: remove unneeded enable_unrestricted_guest check
      KVM: VMX: fix DPL during entry to protected mode
      x86/kexec: crash_vmclear_local_vmcss needs __rcu
      kvm: Fix irqfd resampler list walk
      KVM: VMX: provide the vmclear function and a bitmap to support VMCLEAR in kdump
      x86/kexec: VMCLEAR VMCSs loaded on all cpus if necessary
      KVM: MMU: optimize for set_spte
      KVM: PPC: booke: Get/set guest EPCR register using ONE_REG interface
      KVM: PPC: bookehv: Add EPCR support in mtspr/mfspr emulation
      KVM: PPC: bookehv: Add guest computation mode for irq delivery
      KVM: PPC: Make EPCR a valid field for booke64 and bookehv
      KVM: PPC: booke: Extend MAS2 EPN mask for 64-bit
      KVM: PPC: e500: Mask MAS2 EPN high 32-bits in 32/64 tlbwe emulation
      KVM: PPC: Mask ea's high 32-bits in 32/64 instr emulation
      KVM: PPC: e500: Add emulation helper for getting instruction ea
      KVM: PPC: bookehv64: Add support for interrupt handling
      KVM: PPC: bookehv: Remove GET_VCPU macro from exception handler
      KVM: PPC: booke: Fix get_tb() compile error on 64-bit
      KVM: PPC: e500: Silence bogus GCC warning in tlb code
      ...

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit f57d54bab696133fae569c5f01352249c36fc74f
Merge: da830e589a45 c1ad41f1f727
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 18:21:38 2012 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest change affects group scheduling: we now track the runnable
      average on a per-task entity basis, allowing a smoother, exponential
      decay average based load/weight estimation instead of the previous
      binary on-the-runqueue/off-the-runqueue load weight method.
    
      This will inevitably disturb workloads that were in some sort of
      borderline balancing state or unstable equilibrium, so an eye has to
      be kept on regressions.
    
      For that reason the new load average is only limited to group
      scheduling (shares distribution) at the moment (which was also hurting
      the most from the prior, crude weight calculation and whose scheduling
      quality wins most from this change) - but we plan to extend this to
      regular SMP balancing as well in the future, which will simplify and
      speed up things a bit.
    
      Other changes involve ongoing preparatory work to extend NOHZ to the
      scheduler as well, eventually allowing completely irq-free user-space
      execution."
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      Revert "sched/autogroup: Fix crash on reboot when autogroup is disabled"
      cputime: Comment cputime's adjusting code
      cputime: Consolidate cputime adjustment code
      cputime: Rename thread_group_times to thread_group_cputime_adjusted
      cputime: Move thread_group_cputime() to sched code
      vtime: Warn if irqs aren't disabled on system time accounting APIs
      vtime: No need to disable irqs on vtime_account()
      vtime: Consolidate a bit the ctx switch code
      vtime: Explicitly account pending user time on process tick
      vtime: Remove the underscore prefix invasion
      sched/autogroup: Fix crash on reboot when autogroup is disabled
      cputime: Separate irqtime accounting from generic vtime
      cputime: Specialize irq vtime hooks
      kvm: Directly account vtime to system on guest switch
      vtime: Make vtime_account_system() irqsafe
      vtime: Gather vtime declarations to their own header file
      sched: Describe CFS load-balancer
      sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking
      sched: Make __update_entity_runnable_avg() fast
      sched: Update_cfs_shares at period edge
      ...

commit 37ea95a959d4a49846ecbf2dd45326b6b34bf049
Merge: de0c276b3153 630e1e0bcddf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 18:10:49 2012 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU update from Ingo Molnar:
     "The major features of this tree are:
    
         1. A first version of no-callbacks CPUs.  This version prohibits
            offlining CPU 0, but only when enabled via CONFIG_RCU_NOCB_CPU=y.
            Relaxing this constraint is in progress, but not yet ready
            for prime time.  These commits were posted to LKML at
            https://lkml.org/lkml/2012/10/30/724.
    
         2. Changes to SRCU that allows statically initialized srcu_struct
            structures.  These commits were posted to LKML at
            https://lkml.org/lkml/2012/10/30/296.
    
         3. Restructuring of RCU's debugfs output.  These commits were posted
            to LKML at https://lkml.org/lkml/2012/10/30/341.
    
         4. Additional CPU-hotplug/RCU improvements, posted to LKML at
            https://lkml.org/lkml/2012/10/30/327.
            Note that the commit eliminating __stop_machine() was judged to
            be too-high of risk, so is deferred to 3.9.
    
         5. Changes to RCU's idle interface, most notably a new module
            parameter that redirects normal grace-period operations to
            their expedited equivalents.  These were posted to LKML at
            https://lkml.org/lkml/2012/10/30/739.
    
         6. Additional diagnostics for RCU's CPU stall warning facility,
            posted to LKML at https://lkml.org/lkml/2012/10/30/315.
            The most notable change reduces the
            default RCU CPU stall-warning time from 60 seconds to 21 seconds,
            so that it once again happens sooner than the softlockup timeout.
    
         7. Documentation updates, which were posted to LKML at
            https://lkml.org/lkml/2012/10/30/280.
            A couple of late-breaking changes were posted at
            https://lkml.org/lkml/2012/11/16/634 and
            https://lkml.org/lkml/2012/11/16/547.
    
         8. Miscellaneous fixes, which were posted to LKML at
            https://lkml.org/lkml/2012/10/30/309.
    
         9. Finally, a fix for an lockdep-RCU splat was posted to LKML
            at https://lkml.org/lkml/2012/11/7/486."
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (49 commits)
      context_tracking: New context tracking susbsystem
      sched: Mark RCU reader in sched_show_task()
      rcu: Separate accounting of callbacks from callback-free CPUs
      rcu: Add callback-free CPUs
      rcu: Add documentation for the new rcuexp debugfs trace file
      rcu: Update documentation for TREE_RCU debugfs tracing
      rcu: Reduce default RCU CPU stall warning timeout
      rcu: Fix TINY_RCU rcu_is_cpu_rrupt_from_idle check
      rcu: Clarify memory-ordering properties of grace-period primitives
      rcu: Add new rcutorture module parameters to start/end test messages
      rcu: Remove list_for_each_continue_rcu()
      rcu: Fix batch-limit size problem
      rcu: Add tracing for synchronize_sched_expedited()
      rcu: Remove old debugfs interfaces and also RCU flavor name
      rcu: split 'rcuhier' to each flavor
      rcu: split 'rcugp' to each flavor
      rcu: split 'rcuboost' to each flavor
      rcu: split 'rcubarrier' to each flavor
      rcu: Fix tracing formatting
      rcu: Remove the interface "rcudata.csv"
      ...

commit e1e12d2f3104be886073ac6c5c4678f30b1b9e51
Author: David Rientjes <rientjes@google.com>
Date:   Tue Dec 11 16:02:56 2012 -0800

    mm, oom: fix race when specifying a thread as the oom origin
    
    test_set_oom_score_adj() and compare_swap_oom_score_adj() are used to
    specify that current should be killed first if an oom condition occurs in
    between the two calls.
    
    The usage is
    
            short oom_score_adj = test_set_oom_score_adj(OOM_SCORE_ADJ_MAX);
            ...
            compare_swap_oom_score_adj(OOM_SCORE_ADJ_MAX, oom_score_adj);
    
    to store the thread's oom_score_adj, temporarily change it to the maximum
    score possible, and then restore the old value if it is still the same.
    
    This happens to still be racy, however, if the user writes
    OOM_SCORE_ADJ_MAX to /proc/pid/oom_score_adj in between the two calls.
    The compare_swap_oom_score_adj() will then incorrectly reset the old value
    prior to the write of OOM_SCORE_ADJ_MAX.
    
    To fix this, introduce a new oom_flags_t member in struct signal_struct
    that will be used for per-thread oom killer flags.  KSM and swapoff can
    now use a bit in this member to specify that threads should be killed
    first in oom conditions without playing around with oom_score_adj.
    
    This also allows the correct oom_score_adj to always be shown when reading
    /proc/pid/oom_score.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ed30456152da..3e387df065fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -631,6 +631,7 @@ struct signal_struct {
 	struct rw_semaphore group_rwsem;
 #endif
 
+	oom_flags_t oom_flags;
 	short oom_score_adj;		/* OOM kill score adjustment */
 	short oom_score_adj_min;	/* OOM kill score adjustment min value.
 					 * Only settable by CAP_SYS_RESOURCE. */

commit a9c58b907dbc6821533dfc295b63caf111ff1f16
Author: David Rientjes <rientjes@google.com>
Date:   Tue Dec 11 16:02:54 2012 -0800

    mm, oom: change type of oom_score_adj to short
    
    The maximum oom_score_adj is 1000 and the minimum oom_score_adj is -1000,
    so this range can be represented by the signed short type with no
    functional change.  The extra space this frees up in struct signal_struct
    will be used for per-thread oom kill flags in the next patch.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..ed30456152da 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -631,9 +631,9 @@ struct signal_struct {
 	struct rw_semaphore group_rwsem;
 #endif
 
-	int oom_score_adj;	/* OOM kill score adjustment */
-	int oom_score_adj_min;	/* OOM kill score adjustment minimum value.
-				 * Only settable by CAP_SYS_RESOURCE. */
+	short oom_score_adj;		/* OOM kill score adjustment */
+	short oom_score_adj_min;	/* OOM kill score adjustment min value.
+					 * Only settable by CAP_SYS_RESOURCE. */
 
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations

commit 1a687c2e9a99335c9e77392f050fe607fa18a652
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Nov 22 11:16:36 2012 +0000

    mm: sched: numa: Control enabling and disabling of NUMA balancing
    
    This patch adds Kconfig options and kernel parameters to allow the
    enabling and disabling of automatic NUMA balancing. The existance
    of such a switch was and is very important when debugging problems
    related to transparent hugepages and we should have the same for
    automatic NUMA placement.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0f4ff2bd03f6..b1e619f9ff1a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1563,10 +1563,14 @@ struct task_struct {
 
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int node, int pages, bool migrated);
+extern void set_numabalancing_state(bool enabled);
 #else
 static inline void task_numa_fault(int node, int pages, bool migrated)
 {
 }
+static inline void set_numabalancing_state(bool enabled)
+{
+}
 #endif
 
 /*

commit b8593bfda1652755136333cdd362de125b283a9c
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Nov 21 01:18:23 2012 +0000

    mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
    
    The PTE scanning rate and fault rates are two of the biggest sources of
    system CPU overhead with automatic NUMA placement.  Ideally a proper policy
    would detect if a workload was properly placed, schedule and adjust the
    PTE scanning rate accordingly. We do not track the necessary information
    to do that but we at least know if we migrated or not.
    
    This patch scans slower if a page was not migrated as the result of a
    NUMA hinting fault up to sysctl_numa_balancing_scan_period_max which is
    now higher than the previous default. Once every minute it will reset
    the scanner in case of phase changes.
    
    This is hilariously crude and the numbers are arbitrary. Workloads will
    converge quite slowly in comparison to what a proper policy should be able
    to do. On the plus side, we will chew up less CPU for workloads that have
    no need for automatic balancing.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d95a232b5b9..0f4ff2bd03f6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1562,9 +1562,9 @@ struct task_struct {
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
 #ifdef CONFIG_NUMA_BALANCING
-extern void task_numa_fault(int node, int pages);
+extern void task_numa_fault(int node, int pages, bool migrated);
 #else
-static inline void task_numa_fault(int node, int pages)
+static inline void task_numa_fault(int node, int pages, bool migrated)
 {
 }
 #endif
@@ -2009,6 +2009,7 @@ extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
+extern unsigned int sysctl_numa_balancing_scan_period_reset;
 extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_settle_count;
 

commit 4b96a29ba891dd59734cb7be80a900fe93aa2d9f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 25 14:16:47 2012 +0200

    mm: sched: numa: Implement slow start for working set sampling
    
    Add a 1 second delay before starting to scan the working set of
    a task and starting to balance it amongst nodes.
    
    [ note that before the constant per task WSS sampling rate patch
      the initial scan would happen much later still, in effect that
      patch caused this regression. ]
    
    The theory is that short-run tasks benefit very little from NUMA
    placement: they come and go, and they better stick to the node
    they were started on. As tasks mature and rebalance to other CPUs
    and nodes, so does their NUMA placement have to change and so
    does it start to matter more and more.
    
    In practice this change fixes an observable kbuild regression:
    
       # [ a perf stat --null --repeat 10 test of ten bzImage builds to /dev/shm ]
    
       !NUMA:
       45.291088843 seconds time elapsed                                          ( +-  0.40% )
       45.154231752 seconds time elapsed                                          ( +-  0.36% )
    
       +NUMA, no slow start:
       46.172308123 seconds time elapsed                                          ( +-  0.30% )
       46.343168745 seconds time elapsed                                          ( +-  0.25% )
    
       +NUMA, 1 sec slow start:
       45.224189155 seconds time elapsed                                          ( +-  0.25% )
       45.160866532 seconds time elapsed                                          ( +-  0.17% )
    
    and it also fixes an observable perf bench (hackbench) regression:
    
       # perf stat --null --repeat 10 perf bench sched messaging
    
       -NUMA:
    
       -NUMA:                  0.246225691 seconds time elapsed                   ( +-  1.31% )
       +NUMA no slow start:    0.252620063 seconds time elapsed                   ( +-  1.13% )
    
       +NUMA 1sec delay:       0.248076230 seconds time elapsed                   ( +-  1.35% )
    
    The implementation is simple and straightforward, most of the patch
    deals with adding the /proc/sys/kernel/numa_balancing_scan_delay_ms tunable
    knob.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    [ Wrote the changelog, ran measurements, tuned the default. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 37841958d234..7d95a232b5b9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2006,6 +2006,7 @@ enum sched_tunable_scaling {
 };
 extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 
+extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
 extern unsigned int sysctl_numa_balancing_scan_size;

commit 6e5fb223e89dbe5cb5c563f8d4a4a0a7d62455a8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 25 14:16:45 2012 +0200

    mm: sched: numa: Implement constant, per task Working Set Sampling (WSS) rate
    
    Previously, to probe the working set of a task, we'd use
    a very simple and crude method: mark all of its address
    space PROT_NONE.
    
    That method has various (obvious) disadvantages:
    
     - it samples the working set at dissimilar rates,
       giving some tasks a sampling quality advantage
       over others.
    
     - creates performance problems for tasks with very
       large working sets
    
     - over-samples processes with large address spaces but
       which only very rarely execute
    
    Improve that method by keeping a rotating offset into the
    address space that marks the current position of the scan,
    and advance it by a constant rate (in a CPU cycles execution
    proportional manner). If the offset reaches the last mapped
    address of the mm then it then it starts over at the first
    address.
    
    The per-task nature of the working set sampling functionality in this tree
    allows such constant rate, per task, execution-weight proportional sampling
    of the working set, with an adaptive sampling interval/frequency that
    goes from once per 100ms up to just once per 8 seconds.  The current
    sampling volume is 256 MB per interval.
    
    As tasks mature and converge their working set, so does the
    sampling rate slow down to just a trickle, 256 MB per 8
    seconds of CPU time executed.
    
    This, beyond being adaptive, also rate-limits rarely
    executing systems and does not over-sample on overloaded
    systems.
    
    [ In AutoNUMA speak, this patch deals with the effective sampling
      rate of the 'hinting page fault'. AutoNUMA's scanning is
      currently rate-limited, but it is also fundamentally
      single-threaded, executing in the knuma_scand kernel thread,
      so the limit in AutoNUMA is global and does not scale up with
      the number of CPUs, nor does it scan tasks in an execution
      proportional manner.
    
      So the idea of rate-limiting the scanning was first implemented
      in the AutoNUMA tree via a global rate limit. This patch goes
      beyond that by implementing an execution rate proportional
      working set sampling rate that is not implemented via a single
      global scanning daemon. ]
    
    [ Dan Carpenter pointed out a possible NULL pointer dereference in the
      first version of this patch. ]
    
    Based-on-idea-by: Andrea Arcangeli <aarcange@redhat.com>
    Bug-Found-By: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    [ Wrote changelog and fixed bug. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 844af5b12cb2..37841958d234 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2008,6 +2008,7 @@ extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
+extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_settle_count;
 
 #ifdef CONFIG_SCHED_DEBUG

commit cbee9f88ec1b8dd6b58f25f54e4f52c82ed77690
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 25 14:16:43 2012 +0200

    mm: numa: Add fault driven placement and migration
    
    NOTE: This patch is based on "sched, numa, mm: Add fault driven
            placement and migration policy" but as it throws away all the policy
            to just leave a basic foundation I had to drop the signed-offs-by.
    
    This patch creates a bare-bones method for setting PTEs pte_numa in the
    context of the scheduler that when faulted later will be faulted onto the
    node the CPU is running on.  In itself this does nothing useful but any
    placement policy will fundamentally depend on receiving hints on placement
    from fault context and doing something intelligent about it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..844af5b12cb2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1479,6 +1479,14 @@ struct task_struct {
 	short il_next;
 	short pref_node_fork;
 #endif
+#ifdef CONFIG_NUMA_BALANCING
+	int numa_scan_seq;
+	int numa_migrate_seq;
+	unsigned int numa_scan_period;
+	u64 node_stamp;			/* migration stamp  */
+	struct callback_head numa_work;
+#endif /* CONFIG_NUMA_BALANCING */
+
 	struct rcu_head rcu;
 
 	/*
@@ -1553,6 +1561,14 @@ struct task_struct {
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
+#ifdef CONFIG_NUMA_BALANCING
+extern void task_numa_fault(int node, int pages);
+#else
+static inline void task_numa_fault(int node, int pages)
+{
+}
+#endif
+
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
  * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
@@ -1990,6 +2006,10 @@ enum sched_tunable_scaling {
 };
 extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 
+extern unsigned int sysctl_numa_balancing_scan_period_min;
+extern unsigned int sysctl_numa_balancing_scan_period_max;
+extern unsigned int sysctl_numa_balancing_settle_count;
+
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;

commit e80d6661c3a5caa0cebec0853c6cb0db090fb506
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 23:10:08 2012 -0400

    flagday: kill pt_regs argument of do_fork()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78a2ae3470df..1162258bcaf0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2289,7 +2289,7 @@ extern int disallow_signal(int);
 extern int do_execve(const char *,
 		     const char __user * const __user *,
 		     const char __user * const __user *);
-extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
+extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
 #ifdef CONFIG_GENERIC_KERNEL_THREAD
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);

commit afa86fc426ff7e7f5477f15da9c405d08d5cf790
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:51:14 2012 -0400

    flagday: don't pass regs to copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c57249782e48..78a2ae3470df 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2271,7 +2271,7 @@ extern void mm_release(struct task_struct *, struct mm_struct *);
 extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
 extern int copy_thread(unsigned long, unsigned long, unsigned long,
-			struct task_struct *, struct pt_regs *);
+			struct task_struct *);
 extern void flush_thread(void);
 extern void exit_thread(void);
 

commit da3d4c5fa56236dd924d77ffc4f982356816b93b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Oct 20 21:49:33 2012 -0400

    get rid of pt_regs argument of do_execve()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a0166481eb20..c57249782e48 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2288,7 +2288,7 @@ extern int disallow_signal(int);
 
 extern int do_execve(const char *,
 		     const char __user * const __user *,
-		     const char __user * const __user *, struct pt_regs *);
+		     const char __user * const __user *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
 #ifdef CONFIG_GENERIC_KERNEL_THREAD

commit c4144670fd9b34d6eae22c9f83751745898e8243
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Oct 2 16:34:38 2012 -0400

    kill daemonize()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..a0166481eb20 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2283,7 +2283,6 @@ extern void flush_itimer_signals(void);
 
 extern void do_group_exit(int);
 
-extern void daemonize(const char *, ...);
 extern int allow_signal(int);
 extern int disallow_signal(int);
 

commit d37f761dbd276790f70dcf73a287fde2c3464482
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 22 00:58:35 2012 +0100

    cputime: Consolidate cputime adjustment code
    
    task_cputime_adjusted() and thread_group_cputime_adjusted()
    essentially share the same code. They just don't use the same
    source:
    
    * The first function uses the cputime in the task struct and the
    previous adjusted snapshot that ensures monotonicity.
    
    * The second adds the cputime of all tasks in the group and the
    previous adjusted snapshot of the whole group from the signal
    structure.
    
    Just consolidate the common code that does the adjustment. These
    functions just need to fetch the values from the appropriate
    source.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e75cab5820ab..5dafac366811 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -433,14 +433,29 @@ struct cpu_itimer {
 	u32 incr_error;
 };
 
+/**
+ * struct cputime - snaphsot of system and user cputime
+ * @utime: time spent in user mode
+ * @stime: time spent in system mode
+ *
+ * Gathers a generic snapshot of user and system time.
+ */
+struct cputime {
+	cputime_t utime;
+	cputime_t stime;
+};
+
 /**
  * struct task_cputime - collected CPU time counts
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
  *
- * This structure groups together three kinds of CPU time that are
- * tracked for threads and thread groups.  Most things considering
+ * This is an extension of struct cputime that includes the total runtime
+ * spent by the task from the scheduler point of view.
+ *
+ * As a result, this structure groups together three kinds of CPU time
+ * that are tracked for threads and thread groups.  Most things considering
  * CPU time want to group these counts together and treat all three
  * of them in parallel.
  */
@@ -581,7 +596,7 @@ struct signal_struct {
 	cputime_t gtime;
 	cputime_t cgtime;
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
-	cputime_t prev_utime, prev_stime;
+	struct cputime prev_cputime;
 #endif
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
@@ -1340,7 +1355,7 @@ struct task_struct {
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
-	cputime_t prev_utime, prev_stime;
+	struct cputime prev_cputime;
 #endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */

commit e80d0a1ae8bb8fee0edd37427836f108b30f596b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 21 16:26:44 2012 +0100

    cputime: Rename thread_group_times to thread_group_cputime_adjusted
    
    We have thread_group_cputime() and thread_group_times(). The naming
    doesn't provide enough information about the difference between
    these two APIs.
    
    To lower the confusion, rename thread_group_times() to
    thread_group_cputime_adjusted(). This name better suggests that
    it's a version of thread_group_cputime() that does some stabilization
    on the raw cputime values. ie here: scale on top of CFS runtime
    stats and bound lower value for monotonicity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e1581a029e3d..e75cab5820ab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1751,8 +1751,8 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
-extern void task_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
-extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
+extern void task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
+extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st);
 
 /*
  * Per process flags

commit 582b336ec2c0f0076f5650a029fcc9abd4a906f7
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:28:54 2012 -0200

    sched: add notifier for cross-cpu migrations
    
    Originally from Jeremy Fitzhardinge.
    
    Acked-by: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..6eb2ed819e36 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -107,6 +107,14 @@ extern unsigned long this_cpu_load(void);
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);
 
+/* Notifier for when a task gets migrated to a new CPU */
+struct task_migration_notifier {
+	struct task_struct *task;
+	int from_cpu;
+	int to_cpu;
+};
+extern void register_task_migration_notifier(struct notifier_block *n);
+
 extern unsigned long get_parent_ip(unsigned long addr);
 
 struct seq_file;

commit aac1cda34b84a9411d6b8d18c3658f094c834911
Merge: 2c5594df344c d484a215139c 351573a86d0e cda4dc813071 c896054f75f9 7bd8f2a74bcb af71befa282d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 16 09:59:58 2012 -0800

    Merge branches 'urgent.2012.10.27a', 'doc.2012.11.16a', 'fixes.2012.11.13a', 'srcu.2012.10.27a', 'stall.2012.11.13a', 'tracing.2012.11.08a' and 'idle.2012.10.24a' into HEAD
    
    urgent.2012.10.27a: Fix for RCU user-mode transition (already in -tip).
    
    doc.2012.11.08a: Documentation updates, most notably codifying the
            memory-barrier guarantees inherent to grace periods.
    
    fixes.2012.11.13a: Miscellaneous fixes.
    
    srcu.2012.10.27a: Allow statically allocated and initialized srcu_struct
            structures (courtesy of Lai Jiangshan).
    
    stall.2012.11.13a: Add more diagnostic information to RCU CPU stall
            warnings, also decrease from 60 seconds to 21 seconds.
    
    hotplug.2012.11.08a: Minor updates to CPU hotplug handling.
    
    tracing.2012.11.08a: Improved debugfs tracing, courtesy of Michael Wang.
    
    idle.2012.10.24a: Updates to RCU idle/adaptive-idle handling, including
            a boot parameter that maps normal grace periods to expedited.
    
    Resolved conflict in kernel/rcutree.c due to side-by-side change.

commit f4e26b120b9de84cb627bc7361ba43cfdc51341f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:32 2012 +0200

    sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking
    
    While per-entity load-tracking is generally useful, beyond computing shares
    distribution, e.g. runnable based load-balance (in progress), governors,
    power-management, etc.
    
    These facilities are not yet consumers of this data.  This may be trivially
    reverted when the information is required; but avoid paying the overhead for
    calculations we will not use until then.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.422162369@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e483ccb08ce6..e1581a029e3d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1168,7 +1168,13 @@ struct sched_entity {
 	/* rq "owned" by this entity/group: */
 	struct cfs_rq		*my_q;
 #endif
-#ifdef CONFIG_SMP
+/*
+ * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
+ * removed when useful for applications beyond shares distribution (e.g.
+ * load-balance).
+ */
+#if defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)
+	/* Per-entity load-tracking */
 	struct sched_avg	avg;
 #endif
 };

commit 0a74bef8bed18dc6889e9bc37ea1050a50c86c89
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Add an rq migration call-back to sched_class
    
    Since we are now doing bottom up load accumulation we need explicit
    notification when a task has been re-parented so that the old hierarchy can be
    updated.
    
    Adds: migrate_task_rq(struct task_struct *p, int next_cpu)
    
    (The alternative is to do this out of __set_task_cpu, but it was suggested that
    this would be a cleaner encapsulation.)
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.660023400@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b1831accfd89..e483ccb08ce6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1061,6 +1061,7 @@ struct sched_class {
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
+	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
 
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);

commit 9ee474f55664ff63111c843099d365e7ecffb56f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Maintain the load contribution of blocked entities
    
    We are currently maintaining:
    
      runnable_load(cfs_rq) = \Sum task_load(t)
    
    For all running children t of cfs_rq.  While this can be naturally updated for
    tasks in a runnable state (as they are scheduled); this does not account for
    the load contributed by blocked task entities.
    
    This can be solved by introducing a separate accounting for blocked load:
    
      blocked_load(cfs_rq) = \Sum runnable(b) * weight(b)
    
    Obviously we do not want to iterate over all blocked entities to account for
    their decay, we instead observe that:
    
      runnable_load(t) = \Sum p_i*y^i
    
    and that to account for an additional idle period we only need to compute:
    
      y*runnable_load(t).
    
    This means that we can compute all blocked entities at once by evaluating:
    
      blocked_load(cfs_rq)` = y * blocked_load(cfs_rq)
    
    Finally we maintain a decay counter so that when a sleeping entity re-awakens
    we can determine how much of its load should be removed from the blocked sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.585389902@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81d8b1ba4100..b1831accfd89 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1103,6 +1103,7 @@ struct sched_avg {
 	 */
 	u32 runnable_avg_sum, runnable_avg_period;
 	u64 last_runnable_update;
+	s64 decay_count;
 	unsigned long load_avg_contrib;
 };
 

commit 2dac754e10a5d41d94d2d2365c0345d4f215a266
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate load contributed by task entities on parenting cfs_rq
    
    For a given task t, we can compute its contribution to load as:
    
      task_load(t) = runnable_avg(t) * weight(t)
    
    On a parenting cfs_rq we can then aggregate:
    
      runnable_load(cfs_rq) = \Sum task_load(t), for all runnable children t
    
    Maintain this bottom up, with task entities adding their contributed load to
    the parenting cfs_rq sum.  When a task entity's load changes we add the same
    delta to the maintained sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.514678907@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 418fc6d8a4da..81d8b1ba4100 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1103,6 +1103,7 @@ struct sched_avg {
 	 */
 	u32 runnable_avg_sum, runnable_avg_period;
 	u64 last_runnable_update;
+	unsigned long load_avg_contrib;
 };
 
 #ifdef CONFIG_SCHEDSTATS

commit 9d85f21c94f7f7a84d0ba686c58aa6d9da58fdbb
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:29 2012 +0200

    sched: Track the runnable average on a per-task entity basis
    
    Instead of tracking averaging the load parented by a cfs_rq, we can track
    entity load directly. With the load for a given cfs_rq then being the sum
    of its children.
    
    To do this we represent the historical contribution to runnable average
    within each trailing 1024us of execution as the coefficients of a
    geometric series.
    
    We can express this for a given task t as:
    
      runnable_sum(t) = \Sum u_i * y^i, runnable_avg_period(t) = \Sum 1024 * y^i
      load(t) = weight_t * runnable_sum(t) / runnable_avg_period(t)
    
    Where: u_i is the usage in the last i`th 1024us period (approximately 1ms)
    ~ms and y is chosen such that y^k = 1/2.  We currently choose k to be 32 which
    roughly translates to about a sched period.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.372695337@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..418fc6d8a4da 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1095,6 +1095,16 @@ struct load_weight {
 	unsigned long weight, inv_weight;
 };
 
+struct sched_avg {
+	/*
+	 * These sums represent an infinite geometric series and so are bound
+	 * above by 1024/(1-y).  Thus we only need a u32 to store them for for all
+	 * choices of y < 1-2^(-32)*1024.
+	 */
+	u32 runnable_avg_sum, runnable_avg_period;
+	u64 last_runnable_update;
+};
+
 #ifdef CONFIG_SCHEDSTATS
 struct sched_statistics {
 	u64			wait_start;
@@ -1155,6 +1165,9 @@ struct sched_entity {
 	/* rq "owned" by this entity/group: */
 	struct cfs_rq		*my_q;
 #endif
+#ifdef CONFIG_SMP
+	struct sched_avg	avg;
+#endif
 };
 
 struct sched_rt_entity {

commit b637a328bd4f43a0e146d1eef0142b650ba0d644
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Sep 19 16:58:38 2012 -0700

    rcu: Print remote CPU's stacks in stall warnings
    
    The RCU CPU stall warnings rely on trigger_all_cpu_backtrace() to
    do NMI-based dump of the stack traces of all CPUs.  Unfortunately, a
    number of architectures do not implement trigger_all_cpu_backtrace(), in
    which case RCU falls back to just dumping the stack of the running CPU.
    This is unhelpful in the case where the running CPU has detected that
    some other CPU has stalled.
    
    This commit therefore makes the running CPU dump the stacks of the
    tasks running on the stalled CPUs.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..ba69b5adea30 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -109,6 +109,8 @@ extern void update_cpu_load_nohz(void);
 
 extern unsigned long get_parent_ip(unsigned long addr);
 
+extern void dump_cpu_task(int cpu);
+
 struct seq_file;
 struct cfs_rq;
 struct task_group;

commit 4d9a5d4319e22670ec6d6227e12b54f361c46d0f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Oct 11 01:47:16 2012 +0200

    rcu: Remove rcu_switch()
    
    It's only there to call rcu_user_hooks_switch(). Let's
    just call rcu_user_hooks_switch() directly, we don't need this
    function in the middle.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0dd42a02df2e..432cc5e1bbee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1844,14 +1844,6 @@ static inline void rcu_copy_process(struct task_struct *p)
 
 #endif
 
-static inline void rcu_switch(struct task_struct *prev,
-			      struct task_struct *next)
-{
-#ifdef CONFIG_RCU_USER_QS
-	rcu_user_hooks_switch(prev, next);
-#endif
-}
-
 static inline void tsk_restore_flags(struct task_struct *task,
 				unsigned long orig_flags, unsigned long flags)
 {

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a83ca5816ecb..0dd42a02df2e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,48 +1,8 @@
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
-/*
- * cloning flags:
- */
-#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
-#define CLONE_VM	0x00000100	/* set if VM shared between processes */
-#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
-#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
-#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
-#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
-#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
-#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
-#define CLONE_THREAD	0x00010000	/* Same thread group? */
-#define CLONE_NEWNS	0x00020000	/* New namespace group? */
-#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
-#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
-#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
-#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
-#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
-#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
-#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
-/* 0x02000000 was previously the unused CLONE_STOPPED (Start in stopped state)
-   and is now available for re-use. */
-#define CLONE_NEWUTS		0x04000000	/* New utsname group? */
-#define CLONE_NEWIPC		0x08000000	/* New ipcs */
-#define CLONE_NEWUSER		0x10000000	/* New user namespace */
-#define CLONE_NEWPID		0x20000000	/* New pid namespace */
-#define CLONE_NEWNET		0x40000000	/* New network namespace */
-#define CLONE_IO		0x80000000	/* Clone io context */
-
-/*
- * Scheduling policies
- */
-#define SCHED_NORMAL		0
-#define SCHED_FIFO		1
-#define SCHED_RR		2
-#define SCHED_BATCH		3
-/* SCHED_ISO: reserved but not implemented yet */
-#define SCHED_IDLE		5
-/* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
-#define SCHED_RESET_ON_FORK     0x40000000
+#include <uapi/linux/sched.h>
 
-#ifdef __KERNEL__
 
 struct sched_param {
 	int sched_priority;
@@ -2828,6 +2788,4 @@ static inline unsigned long rlimit_max(unsigned int limit)
 	return task_rlimit_max(current, limit);
 }
 
-#endif /* __KERNEL__ */
-
 #endif

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit 01dc52ebdf472f77cca623ca693ca24cfc0f1bbe
Author: Davidlohr Bueso <dave@gnu.org>
Date:   Mon Oct 8 16:29:30 2012 -0700

    oom: remove deprecated oom_adj
    
    The deprecated /proc/<pid>/oom_adj is scheduled for removal this month.
    
    Signed-off-by: Davidlohr Bueso <dave@gnu.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c5612f0374b..c2070e92a9d6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -671,7 +671,6 @@ struct signal_struct {
 	struct rw_semaphore group_rwsem;
 #endif
 
-	int oom_adj;		/* OOM kill score adjustment (bit shift) */
 	int oom_score_adj;	/* OOM kill score adjustment */
 	int oom_score_adj_min;	/* OOM kill score adjustment minimum value.
 				 * Only settable by CAP_SYS_RESOURCE. */

commit 179899fd5dc780fe3bcd44d0eb7823e3d855c855
Author: Alex Kelly <alex.page.kelly@gmail.com>
Date:   Thu Oct 4 17:15:24 2012 -0700

    coredump: update coredump-related headers
    
    Create a new header file, fs/coredump.h, which contains functions only
    used by the new coredump.c.  It also moves do_coredump to the
    include/linux/coredump.h header file, for consistency.
    
    Signed-off-by: Alex Kelly <alex.page.kelly@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9d51e260bde0..9c5612f0374b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -405,7 +405,6 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
-extern int __get_dumpable(unsigned long mm_flags);
 
 /* get/set_dumpable() values */
 #define SUID_DUMPABLE_DISABLED	0

commit aab174f0df5d72d31caccf281af5f614fa254578
Merge: ca41cc96b281 2bd2c1941f14
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 20:25:04 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs update from Al Viro:
    
     - big one - consolidation of descriptor-related logics; almost all of
       that is moved to fs/file.c
    
       (BTW, I'm seriously tempted to rename the result to fd.c.  As it is,
       we have a situation when file_table.c is about handling of struct
       file and file.c is about handling of descriptor tables; the reasons
       are historical - file_table.c used to be about a static array of
       struct file we used to have way back).
    
       A lot of stray ends got cleaned up and converted to saner primitives,
       disgusting mess in android/binder.c is still disgusting, but at least
       doesn't poke so much in descriptor table guts anymore.  A bunch of
       relatively minor races got fixed in process, plus an ext4 struct file
       leak.
    
     - related thing - fget_light() partially unuglified; see fdget() in
       there (and yes, it generates the code as good as we used to have).
    
     - also related - bits of Cyrill's procfs stuff that got entangled into
       that work; _not_ all of it, just the initial move to fs/proc/fd.c and
       switch of fdinfo to seq_file.
    
     - Alex's fs/coredump.c spiltoff - the same story, had been easier to
       take that commit than mess with conflicts.  The rest is a separate
       pile, this was just a mechanical code movement.
    
     - a few misc patches all over the place.  Not all for this cycle,
       there'll be more (and quite a few currently sit in akpm's tree)."
    
    Fix up trivial conflicts in the android binder driver, and some fairly
    simple conflicts due to two different changes to the sock_alloc_file()
    interface ("take descriptor handling from sock_alloc_file() to callers"
    vs "net: Providing protocol type via system.sockprotoname xattr of
    /proc/PID/fd entries" adding a dentry name to the socket)
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (72 commits)
      MAX_LFS_FILESIZE should be a loff_t
      compat: fs: Generic compat_sys_sendfile implementation
      fs: push rcu_barrier() from deactivate_locked_super() to filesystems
      btrfs: reada_extent doesn't need kref for refcount
      coredump: move core dump functionality into its own file
      coredump: prevent double-free on an error path in core dumper
      usb/gadget: fix misannotations
      fcntl: fix misannotations
      ceph: don't abuse d_delete() on failure exits
      hypfs: ->d_parent is never NULL or negative
      vfs: delete surplus inode NULL check
      switch simple cases of fget_light to fdget
      new helpers: fdget()/fdput()
      switch o2hb_region_dev_write() to fget_light()
      proc_map_files_readdir(): don't bother with grabbing files
      make get_file() return its argument
      vhost_set_vring(): turn pollstart/pollstop into bool
      switch prctl_set_mm_exe_file() to fget_light()
      switch xfs_find_handle() to fget_light()
      switch xfs_swapext() to fget_light()
      ...

commit 10c28d937e2cca577c2d804106b50dd0562fb062
Author: Alex Kelly <alex.page.kelly@gmail.com>
Date:   Wed Sep 26 21:52:08 2012 -0400

    coredump: move core dump functionality into its own file
    
    This prepares for making core dump functionality optional.
    
    The variable "suid_dumpable" and associated functions are left in fs/exec.c
    because they're used elsewhere, such as in ptrace.
    
    Signed-off-by: Alex Kelly <alex.page.kelly@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 23bddac4bad8..78041f4c7584 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -405,6 +405,7 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
+extern int __get_dumpable(unsigned long mm_flags);
 
 /* get/set_dumpable() values */
 #define SUID_DUMPABLE_DISABLED	0

commit aecdc33e111b2c447b622e287c6003726daa1426
Merge: a20acf99f75e a3a6cab5ea10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 13:38:27 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking changes from David Miller:
    
     1) GRE now works over ipv6, from Dmitry Kozlov.
    
     2) Make SCTP more network namespace aware, from Eric Biederman.
    
     3) TEAM driver now works with non-ethernet devices, from Jiri Pirko.
    
     4) Make openvswitch network namespace aware, from Pravin B Shelar.
    
     5) IPV6 NAT implementation, from Patrick McHardy.
    
     6) Server side support for TCP Fast Open, from Jerry Chu and others.
    
     7) Packet BPF filter supports MOD and XOR, from Eric Dumazet and Daniel
        Borkmann.
    
     8) Increate the loopback default MTU to 64K, from Eric Dumazet.
    
     9) Use a per-task rather than per-socket page fragment allocator for
        outgoing networking traffic.  This benefits processes that have very
        many mostly idle sockets, which is quite common.
    
        From Eric Dumazet.
    
    10) Use up to 32K for page fragment allocations, with fallbacks to
        smaller sizes when higher order page allocations fail.  Benefits are
        a) less segments for driver to process b) less calls to page
        allocator c) less waste of space.
    
        From Eric Dumazet.
    
    11) Allow GRO to be used on GRE tunnels, from Eric Dumazet.
    
    12) VXLAN device driver, one way to handle VLAN issues such as the
        limitation of 4096 VLAN IDs yet still have some level of isolation.
        From Stephen Hemminger.
    
    13) As usual there is a large boatload of driver changes, with the scale
        perhaps tilted towards the wireless side this time around.
    
    Fix up various fairly trivial conflicts, mostly caused by the user
    namespace changes.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1012 commits)
      hyperv: Add buffer for extended info after the RNDIS response message.
      hyperv: Report actual status in receive completion packet
      hyperv: Remove extra allocated space for recv_pkt_list elements
      hyperv: Fix page buffer handling in rndis_filter_send_request()
      hyperv: Fix the missing return value in rndis_filter_set_packet_filter()
      hyperv: Fix the max_xfer_size in RNDIS initialization
      vxlan: put UDP socket in correct namespace
      vxlan: Depend on CONFIG_INET
      sfc: Fix the reported priorities of different filter types
      sfc: Remove EFX_FILTER_FLAG_RX_OVERRIDE_IP
      sfc: Fix loopback self-test with separate_tx_channels=1
      sfc: Fix MCDI structure field lookup
      sfc: Add parentheses around use of bitfield macro arguments
      sfc: Fix null function pointer in efx_sriov_channel_type
      vxlan: virtual extensible lan
      igmp: export symbol ip_mc_leave_group
      netlink: add attributes to fdb interface
      tg3: unconditionally select HWMON support when tg3 is enabled.
      Revert "net: ti cpsw ethernet: allow reading phy interface mode from DT"
      gre: fix sparse warning
      ...

commit 437589a74b6a590d175f86cf9f7b2efcee7765e7
Merge: 68d47a137c3b 72235465864d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 11:11:09 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace changes from Eric Biederman:
     "This is a mostly modest set of changes to enable basic user namespace
      support.  This allows the code to code to compile with user namespaces
      enabled and removes the assumption there is only the initial user
      namespace.  Everything is converted except for the most complex of the
      filesystems: autofs4, 9p, afs, ceph, cifs, coda, fuse, gfs2, ncpfs,
      nfs, ocfs2 and xfs as those patches need a bit more review.
    
      The strategy is to push kuid_t and kgid_t values are far down into
      subsystems and filesystems as reasonable.  Leaving the make_kuid and
      from_kuid operations to happen at the edge of userspace, as the values
      come off the disk, and as the values come in from the network.
      Letting compile type incompatible compile errors (present when user
      namespaces are enabled) guide me to find the issues.
    
      The most tricky areas have been the places where we had an implicit
      union of uid and gid values and were storing them in an unsigned int.
      Those places were converted into explicit unions.  I made certain to
      handle those places with simple trivial patches.
    
      Out of that work I discovered we have generic interfaces for storing
      quota by projid.  I had never heard of the project identifiers before.
      Adding full user namespace support for project identifiers accounts
      for most of the code size growth in my git tree.
    
      Ultimately there will be work to relax privlige checks from
      "capable(FOO)" to "ns_capable(user_ns, FOO)" where it is safe allowing
      root in a user names to do those things that today we only forbid to
      non-root users because it will confuse suid root applications.
    
      While I was pushing kuid_t and kgid_t changes deep into the audit code
      I made a few other cleanups.  I capitalized on the fact we process
      netlink messages in the context of the message sender.  I removed
      usage of NETLINK_CRED, and started directly using current->tty.
    
      Some of these patches have also made it into maintainer trees, with no
      problems from identical code from different trees showing up in
      linux-next.
    
      After reading through all of this code I feel like I might be able to
      win a game of kernel trivial pursuit."
    
    Fix up some fairly trivial conflicts in netfilter uid/git logging code.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (107 commits)
      userns: Convert the ufs filesystem to use kuid/kgid where appropriate
      userns: Convert the udf filesystem to use kuid/kgid where appropriate
      userns: Convert ubifs to use kuid/kgid
      userns: Convert squashfs to use kuid/kgid where appropriate
      userns: Convert reiserfs to use kuid and kgid where appropriate
      userns: Convert jfs to use kuid/kgid where appropriate
      userns: Convert jffs2 to use kuid and kgid where appropriate
      userns: Convert hpfs to use kuid and kgid where appropriate
      userns: Convert btrfs to use kuid/kgid where appropriate
      userns: Convert bfs to use kuid/kgid where appropriate
      userns: Convert affs to use kuid/kgid wherwe appropriate
      userns: On alpha modify linux_to_osf_stat to use convert from kuids and kgids
      userns: On ia64 deal with current_uid and current_gid being kuid and kgid
      userns: On ppc convert current_uid from a kuid before printing.
      userns: Convert s390 getting uid and gid system calls to use kuid and kgid
      userns: Convert s390 hypfs to use kuid and kgid where appropriate
      userns: Convert binder ipc to use kuids
      userns: Teach security_path_chown to take kuids and kgids
      userns: Add user namespace support to IMA
      userns: Convert EVM to deal with kuids and kgids in it's hmac computation
      ...

commit 0b981cb94bc63a2d0e5eccccdca75fe57643ffce
Merge: 4cba3335826c fdf9c356502a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:43:39 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Continued quest to clean up and enhance the cputime code by Frederic
      Weisbecker, in preparation for future tickless kernel features.
    
      Other than that, smallish changes."
    
    Fix up trivial conflicts due to additions next to each other in arch/{x86/}Kconfig
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      cputime: Make finegrained irqtime accounting generally available
      cputime: Gather time/stats accounting config options into a single menu
      ia64: Reuse system and user vtime accounting functions on task switch
      ia64: Consolidate user vtime accounting
      vtime: Consolidate system/idle context detection
      cputime: Use a proper subsystem naming for vtime related APIs
      sched: cpu_power: enable ARCH_POWER
      sched/nohz: Clean up select_nohz_load_balancer()
      sched: Fix load avg vs. cpu-hotplug
      sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: Fix nohz_idle_balance()
      sched: Remove useless code in yield_to()
      sched: Add time unit suffix to sched sysctl knobs
      sched/debug: Limit sd->*_idx range on sysctl
      sched: Remove AFFINE_WAKEUPS feature flag
      s390: Remove leftover account_tick_vtime() header
      cputime: Consolidate vtime handling on context switch
      sched: Move cputime code to its own file
      cputime: Generalize CONFIG_VIRT_CPU_ACCOUNTING
      tile: Remove SD_PREFER_LOCAL leftover
      ...

commit 7e92daaefa68e5ef1e1732e45231e73adbb724e7
Merge: 7a68294278ae 1d787d37c8ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:28:49 2012 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf update from Ingo Molnar:
     "Lots of changes in this cycle as well, with hundreds of commits from
      over 30 contributors.  Most of the activity was on the tooling side.
    
      Higher level changes:
    
       - New 'perf kvm' analysis tool, from Xiao Guangrong.
    
       - New 'perf trace' system-wide tracing tool
    
       - uprobes fixes + cleanups from Oleg Nesterov.
    
       - Lots of patches to make perf build on Android out of box, from
         Irina Tirdea
    
       - Extend ftrace function tracing utility to be more dynamic for its
         users.  It allows for data passing to the callback functions, as
         well as reading regs as if a breakpoint were to trigger at function
         entry.
    
         The main goal of this patch series was to allow kprobes to use
         ftrace as an optimized probe point when a probe is placed on an
         ftrace nop.  With lots of help from Masami Hiramatsu, and going
         through lots of iterations, we finally came up with a good
         solution.
    
       - Add cpumask for uncore pmu, use it in 'stat', from Yan, Zheng.
    
       - Various tracing updates from Steve Rostedt
    
       - Clean up and improve 'perf sched' performance by elliminating lots
         of needless calls to libtraceevent.
    
       - Event group parsing support, from Jiri Olsa
    
       - UI/gtk refactorings and improvements from Namhyung Kim
    
       - Add support for non-tracepoint events in perf script python, from
         Feng Tang
    
       - Add --symbols to 'script', similar to the one in 'report', from
         Feng Tang.
    
      Infrastructure enhancements and fixes:
    
       - Convert the trace builtins to use the growing evsel/evlist
         tracepoint infrastructure, removing several open coded constructs
         like switch like series of strcmp to dispatch events, etc.
         Basically what had already been showcased in 'perf sched'.
    
       - Add evsel constructor for tracepoints, that uses libtraceevent just
         to parse the /format events file, use it in a new 'perf test' to
         make sure the libtraceevent format parsing regressions can be more
         readily caught.
    
       - Some strange errors were happening in some builds, but not on the
         next, reported by several people, problem was some parser related
         files, generated during the build, didn't had proper make deps, fix
         from Eric Sandeen.
    
       - Introduce struct and cache information about the environment where
         a perf.data file was captured, from Namhyung Kim.
    
       - Fix handling of unresolved samples when --symbols is used in
         'report', from Feng Tang.
    
       - Add union member access support to 'probe', from Hyeoncheol Lee.
    
       - Fixups to die() removal, from Namhyung Kim.
    
       - Render fixes for the TUI, from Namhyung Kim.
    
       - Don't enable annotation in non symbolic view, from Namhyung Kim.
    
       - Fix pipe mode in 'report', from Namhyung Kim.
    
       - Move related stats code from stat to util/, will be used by the
         'stat' kvm tool, from Xiao Guangrong.
    
       - Remove die()/exit() calls from several tools.
    
       - Resolve vdso callchains, from Jiri Olsa
    
       - Don't pass const char pointers to basename, so that we can
         unconditionally use libgen.h and thus avoid ifdef BIONIC lines,
         from David Ahern
    
       - Refactor hist formatting so that it can be reused with the GTK
         browser, From Namhyung Kim
    
       - Fix build for another rbtree.c change, from Adrian Hunter.
    
       - Make 'perf diff' command work with evsel hists, from Jiri Olsa.
    
       - Use the only field_sep var that is set up: symbol_conf.field_sep,
         fix from Jiri Olsa.
    
       - .gitignore compiled python binaries, from Namhyung Kim.
    
       - Get rid of die() in more libtraceevent places, from Namhyung Kim.
    
       - Rename libtraceevent 'private' struct member to 'priv' so that it
         works in C++, from Steven Rostedt
    
       - Remove lots of exit()/die() calls from tools so that the main perf
         exit routine can take place, from David Ahern
    
       - Fix x86 build on x86-64, from David Ahern.
    
       - {int,str,rb}list fixes from Suzuki K Poulose
    
       - perf.data header fixes from Namhyung Kim
    
       - Allow user to indicate objdump path, needed in cross environments,
         from Maciek Borzecki
    
       - Fix hardware cache event name generation, fix from Jiri Olsa
    
       - Add round trip test for sw, hw and cache event names, catching the
         problem Jiri fixed, after Jiri's patch, the test passes
         successfully.
    
       - Clean target should do clean for lib/traceevent too, fix from David
         Ahern
    
       - Check the right variable for allocation failure, fix from Namhyung
         Kim
    
       - Set up evsel->tp_format regardless of evsel->name being set
         already, fix from Namhyung Kim
    
       - Oprofile fixes from Robert Richter.
    
       - Remove perf_event_attr needless version inflation, from Jiri Olsa
    
       - Introduce libtraceevent strerror like error reporting facility,
         from Namhyung Kim
    
       - Add pmu mappings to perf.data header and use event names from cmd
         line, from Robert Richter
    
       - Fix include order for bison/flex-generated C files, from Ben
         Hutchings
    
       - Build fixes and documentation corrections from David Ahern
    
       - Assorted cleanups from Robert Richter
    
       - Let O= makes handle relative paths, from Steven Rostedt
    
       - perf script python fixes, from Feng Tang.
    
       - Initial bash completion support, from Frederic Weisbecker
    
       - Allow building without libelf, from Namhyung Kim.
    
       - Support DWARF CFI based unwind to have callchains when %bp based
         unwinding is not possible, from Jiri Olsa.
    
       - Symbol resolution fixes, while fixing support PPC64 files with an
         .opt ELF section was the end goal, several fixes for code that
         handles all architectures and cleanups are included, from Cody
         Schafer.
    
       - Assorted fixes for Documentation and build in 32 bit, from Robert
         Richter
    
       - Cache the libtraceevent event_format associated to each evsel
         early, so that we avoid relookups, i.e.  calling pevent_find_event
         repeatedly when processing tracepoint events.
    
         [ This is to reduce the surface contact with libtraceevents and
            make clear what is that the perf tools needs from that lib: so
            far parsing the common and per event fields.  ]
    
       - Don't stop the build if the audit libraries are not installed, fix
         from Namhyung Kim.
    
       - Fix bfd.h/libbfd detection with recent binutils, from Markus
         Trippelsdorf.
    
       - Improve warning message when libunwind devel packages not present,
         from Jiri Olsa"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (282 commits)
      perf trace: Add aliases for some syscalls
      perf probe: Print an enum type variable in "enum variable-name" format when showing accessible variables
      perf tools: Check libaudit availability for perf-trace builtin
      perf hists: Add missing period_* fields when collapsing a hist entry
      perf trace: New tool
      perf evsel: Export the event_format constructor
      perf evsel: Introduce rawptr() method
      perf tools: Use perf_evsel__newtp in the event parser
      perf evsel: The tracepoint constructor should store sys:name
      perf evlist: Introduce set_filter() method
      perf evlist: Renane set_filters method to apply_filters
      perf test: Add test to check we correctly parse and match syscall open parms
      perf evsel: Handle endianity in intval method
      perf evsel: Know if byte swap is needed
      perf tools: Allow handling a NULL cpu_map as meaning "all cpus"
      perf evsel: Improve tracepoint constructor setup
      tools lib traceevent: Fix error path on pevent_parse_event
      perf test: Fix build failure
      trace: Move trace event enable from fs_initcall to core_initcall
      tracing: Add an option for disabling markers
      ...

commit 2aa3a7f8660355c3dddead17e224545c1a3d5a5f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Sep 21 19:55:31 2012 -0400

    preparation for generic kernel_thread()
    
    Let architectures select GENERIC_KERNEL_THREAD and have their copy_thread()
    treat NULL regs as "it came from kernel_thread(), sp argument contains
    the function new thread will be calling and stack_size - the argument for
    that function".  Switching the architectures begins shortly...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 23bddac4bad8..34da9340c6a4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2325,6 +2325,9 @@ extern int do_execve(const char *,
 		     const char __user * const __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
+#ifdef CONFIG_GENERIC_KERNEL_THREAD
+extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
+#endif
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);

commit 6a06e5e1bb217be077e1f8ee2745b4c5b1aa02db
Merge: d9f72f359e00 6672d90fe779
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 28 14:40:49 2012 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/team/team.c
            drivers/net/usb/qmi_wwan.c
            net/batman-adv/bat_iv_ogm.c
            net/ipv4/fib_frontend.c
            net/ipv4/route.c
            net/l2tp/l2tp_netlink.c
    
    The team, fib_frontend, route, and l2tp_netlink conflicts were simply
    overlapping changes.
    
    qmi_wwan and bat_iv_ogm were of the "use HEAD" variety.
    
    With help from Antonio Quartulli.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 04e7e951532b390b16feb070be9972b8fad2fc57
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jul 16 15:06:40 2012 -0700

    rcu: Switch task's syscall hooks on context switch
    
    Clear the syscalls hook of a task when it's scheduled out so that if
    the task migrates, it doesn't run the syscall slow path on a CPU
    that might not need it.
    
    Also set the syscalls hook on the next task if needed.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Kevin Hilman <khilman@ti.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sven-Thorsten Dietrich <thebigcorporation@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 23bddac4bad8..335720a1fc33 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1885,6 +1885,14 @@ static inline void rcu_copy_process(struct task_struct *p)
 
 #endif
 
+static inline void rcu_switch(struct task_struct *prev,
+			      struct task_struct *next)
+{
+#ifdef CONFIG_RCU_USER_QS
+	rcu_user_hooks_switch(prev, next);
+#endif
+}
+
 static inline void tsk_restore_flags(struct task_struct *task,
 				unsigned long orig_flags, unsigned long flags)
 {

commit 5640f7685831e088fe6c2e1f863a6805962f8e81
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Sep 23 23:04:42 2012 +0000

    net: use a per task frag allocator
    
    We currently use a per socket order-0 page cache for tcp_sendmsg()
    operations.
    
    This page is used to build fragments for skbs.
    
    Its done to increase probability of coalescing small write() into
    single segments in skbs still in write queue (not yet sent)
    
    But it wastes a lot of memory for applications handling many mostly
    idle sockets, since each socket holds one page in sk->sk_sndmsg_page
    
    Its also quite inefficient to build TSO 64KB packets, because we need
    about 16 pages per skb on arches where PAGE_SIZE = 4096, so we hit
    page allocator more than wanted.
    
    This patch adds a per task frag allocator and uses bigger pages,
    if available. An automatic fallback is done in case of memory pressure.
    
    (up to 32768 bytes per frag, thats order-3 pages on x86)
    
    This increases TCP stream performance by 20% on loopback device,
    but also benefits on other network devices, since 8x less frags are
    mapped on transmit and unmapped on tx completion. Alexander Duyck
    mentioned a probable performance win on systems with IOMMU enabled.
    
    Its possible some SG enabled hardware cant cope with bigger fragments,
    but their ndo_start_xmit() should already handle this, splitting a
    fragment in sub fragments, since some arches have PAGE_SIZE=65536
    
    Successfully tested on various ethernet devices.
    (ixgbe, igb, bnx2x, tg3, mellanox mlx4)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Vijay Subramanian <subramanian.vijay@gmail.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b8c86648a2f9..a8e2413f6bc3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1530,6 +1530,9 @@ struct task_struct {
 	 * cache last used pipe for splice
 	 */
 	struct pipe_inode_info *splice_pipe;
+
+	struct page_frag task_frag;
+
 #ifdef	CONFIG_TASK_DELAY_ACCT
 	struct task_delay_info *delays;
 #endif

commit e1760bd5ffae8cb98cffb030ee8e631eba28f3d8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Sep 10 22:39:43 2012 -0700

    userns: Convert the audit loginuid  to be a kuid
    
    Always store audit loginuids in type kuid_t.
    
    Print loginuids by converting them into uids in the appropriate user
    namespace, and then printing the resulting uid.
    
    Modify audit_get_loginuid to return a kuid_t.
    
    Modify audit_set_loginuid to take a kuid_t.
    
    Modify /proc/<pid>/loginuid on read to convert the loginuid into the
    user namespace of the opener of the file.
    
    Modify /proc/<pid>/loginud on write to convert the loginuid
    rom the user namespace of the opener of the file.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Paul Moore <paul@paul-moore.com> ?
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c147e7024f11..f64d092f2bed 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1426,7 +1426,7 @@ struct task_struct {
 
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL
-	uid_t loginuid;
+	kuid_t loginuid;
 	unsigned int sessionid;
 #endif
 	struct seccomp seccomp;

commit 37407ea7f93864c2cfc03edf8f37872ec539ea2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 16 12:29:43 2012 -0700

    Revert "sched: Improve scalability via 'CPU buddies', which withstand random perturbations"
    
    This reverts commit 970e178985cadbca660feb02f4d2ee3a09f7fdda.
    
    Nikolay Ulyanitsky reported thatthe 3.6-rc5 kernel has a 15-20%
    performance drop on PostgreSQL 9.2 on his machine (running "pgbench").
    
    Borislav Petkov was able to reproduce this, and bisected it to this
    commit 970e178985ca ("sched: Improve scalability via 'CPU buddies' ...")
    apparently because the new single-idle-buddy model simply doesn't find
    idle CPU's to reschedule on aggressively enough.
    
    Mike Galbraith suspects that it is likely due to the user-mode spinlocks
    in PostgreSQL not reacting well to preemption, but we don't really know
    the details - I'll just revert the commit for now.
    
    There are hopefully other approaches to improve scheduler scalability
    without it causing these kinds of downsides.
    
    Reported-by: Nikolay Ulyanitsky <lystor@gmail.com>
    Bisected-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Mike Galbraith <efault@gmx.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b8c86648a2f9..23bddac4bad8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -954,7 +954,6 @@ struct sched_domain {
 	unsigned int smt_gain;
 	int flags;			/* See SD_* */
 	int level;
-	int idle_buddy;			/* cpu assigned to select_idle_sibling() */
 
 	/* Runtime fields. */
 	unsigned long last_balance;	/* init to jiffies. units in jiffies */

commit 9f68f672c47b9bd4cfe0a667ecb0b1382c61e2de
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Aug 19 16:15:09 2012 +0200

    uprobes: Introduce MMF_RECALC_UPROBES
    
    Add the new MMF_RECALC_UPROBES flag, it means that MMF_HAS_UPROBES
    can be false positive after remove_breakpoint() or uprobe_munmap().
    It is also set by uprobe_dup_mmap(), this is not optimal but simple.
    We could add the new hook, uprobe_dup_vma(), to set MMF_HAS_UPROBES
    only if the new mm actually has uprobes, but I don't think this
    makes sense.
    
    The next patch will use this flag to clear MMF_HAS_UPROBES.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3667c332e61d..255661d48834 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -446,7 +446,8 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
 #define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
 
-#define MMF_HAS_UPROBES		19	/* might have uprobes */
+#define MMF_HAS_UPROBES		19	/* has uprobes */
+#define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit c1cc017c59c44d9ede7003631c43adc0cfdce2f9
Author: Alex Shi <alex.shi@intel.com>
Date:   Mon Sep 10 15:10:58 2012 +0800

    sched/nohz: Clean up select_nohz_load_balancer()
    
    There is no load_balancer to be selected now. It just sets the
    state of the nohz tick to stop.
    
    So rename the function, pass the 'cpu' as a parameter and then
    remove the useless call from tick_nohz_restart_sched_tick().
    
    [ s/set_nohz_tick_stopped/nohz_balance_enter_idle/g
      s/clear_nohz_tick_stopped/nohz_balance_exit_idle/g ]
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1347261059-24747-1-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 60e5e38eee2a..8c38df07ac3a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -273,11 +273,11 @@ extern void init_idle_bootup_task(struct task_struct *idle);
 extern int runqueue_is_locked(int cpu);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
-extern void select_nohz_load_balancer(int stop_tick);
+extern void nohz_balance_enter_idle(int cpu);
 extern void set_cpu_sd_state_idle(void);
 extern int get_nohz_timer_target(void);
 #else
-static inline void select_nohz_load_balancer(int stop_tick) { }
+static inline void nohz_balance_enter_idle(int cpu) { }
 static inline void set_cpu_sd_state_idle(void) { }
 #endif
 

commit f3e947867478af9a12b9956bcd000ac7613a8a95
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 12 11:22:00 2012 +0200

    sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
    
    Now that the last architecture to use this has stopped doing so (ARM,
    thanks Catalin!) we can remove this complexity from the scheduler
    core.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/n/tip-g9p2a1w81xxbrze25v9zpzbf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f3eebc121ebc..60e5e38eee2a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -678,11 +678,6 @@ struct signal_struct {
 					 * (notably. ptrace) */
 };
 
-/* Context switch must be unlocked if interrupts are to be enabled */
-#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
-# define __ARCH_WANT_UNLOCKED_CTXSW
-#endif
-
 /*
  * Bits in flags field of signal_struct.
  */

commit f8ac4ec9c064b330dcc49e03c450fe74298c4622
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Aug 8 17:11:42 2012 +0200

    uprobes: Introduce MMF_HAS_UPROBES
    
    Add the new MMF_HAS_UPROBES flag. It is set by install_breakpoint()
    and it is copied by dup_mmap(), uprobe_pre_sstep_notifier() checks
    it to avoid the slow path if the task was never probed. Perhaps it
    makes sense to check it in valid_vma(is_register => false) as well.
    
    This needs the new dup_mmap()->uprobe_dup_mmap() hook. We can't use
    uprobe_reset_state() or put MMF_HAS_UPROBES into MMF_INIT_MASK, we
    need oldmm->mmap_sem to avoid the race with uprobe_register() or
    mmap() from another thread.
    
    Currently we never clear this bit, it can be false-positive after
    uprobe_unregister() or uprobe_munmap() or if dup_mmap() hits the
    probed VM_DONTCOPY vma. But this is fine correctness-wise and has
    no effect unless the task hits the non-uprobe breakpoint.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b8c86648a2f9..3667c332e61d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -446,6 +446,8 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
 #define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
 
+#define MMF_HAS_UPROBES		19	/* might have uprobes */
+
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
 struct sighand_struct {

commit f03542a7019c600163ac4441d8a826c92c1bd510
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jul 26 08:55:34 2012 +0800

    sched: recover SD_WAKE_AFFINE in select_task_rq_fair and code clean up
    
    Since power saving code was removed from sched now, the implement
    code is out of service in this function, and even pollute other logical.
    like, 'want_sd' never has chance to be set '0', that remove the effect
    of SD_WAKE_AFFINE here.
    
    So, clean up the obsolete code, includes SD_PREFER_LOCAL.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/5028F431.6000306@intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b8c86648a2f9..f3eebc121ebc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -860,7 +860,6 @@ enum cpu_idle_type {
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-#define SD_PREFER_LOCAL		0x0040  /* Prefer to keep tasks local to this domain */
 #define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */

commit 300d3739e873d50d4c6e3656f89007a217fb1d29
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Aug 7 13:50:22 2012 +0200

    Revert "NMI watchdog: fix for lockup detector breakage on resume"
    
    Revert commit 45226e9 (NMI watchdog: fix for lockup detector breakage
    on resume) which breaks resume from system suspend on my SH7372
    Mackerel board (by causing a NULL pointer dereference to happen) and
    is generally wrong, because it abuses the CPU hotplug functionality
    in a shamelessly blatant way.
    
    The original issue should be addressed through appropriate syscore
    resume callback instead.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c147e7024f11..b8c86648a2f9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -334,14 +334,6 @@ static inline void lockup_detector_init(void)
 }
 #endif
 
-#if defined(CONFIG_LOCKUP_DETECTOR) && defined(CONFIG_SUSPEND)
-void lockup_detector_bootcpu_resume(void);
-#else
-static inline void lockup_detector_bootcpu_resume(void)
-{
-}
-#endif
-
 #ifdef CONFIG_DETECT_HUNG_TASK
 extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_check_count;

commit 907aed48f65efeecf91575397e3d79335d93a466
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:07 2012 -0700

    mm: allow PF_MEMALLOC from softirq context
    
    This is needed to allow network softirq packet processing to make use of
    PF_MEMALLOC.
    
    Currently softirq context cannot use PF_MEMALLOC due to it not being
    associated with a task, and therefore not having task flags to fiddle with
    - thus the gfp to alloc flag mapping ignores the task flags when in
    interrupts (hard or soft) context.
    
    Allowing softirqs to make use of PF_MEMALLOC therefore requires some
    trickery.  This patch borrows the task flags from whatever process happens
    to be preempted by the softirq.  It then modifies the gfp to alloc flags
    mapping to not exclude task flags in softirq context, and modify the
    softirq code to save, clear and restore the PF_MEMALLOC flag.
    
    The save and clear, ensures the preempted task's PF_MEMALLOC flag doesn't
    leak into the softirq.  The restore ensures a softirq's PF_MEMALLOC flag
    cannot leak back into the preempted process.  This should be safe due to
    the following reasons
    
    Softirqs can run on multiple CPUs sure but the same task should not be
            executing the same softirq code. Neither should the softirq
            handler be preempted by any other softirq handler so the flags
            should not leak to an unrelated softirq.
    
    Softirqs re-enable hardware interrupts in __do_softirq() so can be
            preempted by hardware interrupts so PF_MEMALLOC is inherited
            by the hard IRQ. However, this is similar to a process in
            reclaim being preempted by a hardirq. While PF_MEMALLOC is
            set, gfp_to_alloc_flags() distinguishes between hard and
            soft irqs and avoids giving a hardirq the ALLOC_NO_WATERMARKS
            flag.
    
    If the softirq is deferred to ksoftirq then its flags may be used
            instead of a normal tasks but as the softirq cannot be preempted,
            the PF_MEMALLOC flag does not leak to other code by accident.
    
    [davem@davemloft.net: Document why PF_MEMALLOC is safe]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 865725adb9d3..c147e7024f11 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1894,6 +1894,13 @@ static inline void rcu_copy_process(struct task_struct *p)
 
 #endif
 
+static inline void tsk_restore_flags(struct task_struct *task,
+				unsigned long orig_flags, unsigned long flags)
+{
+	task->flags &= ~flags;
+	task->flags |= orig_flags & flags;
+}
+
 #ifdef CONFIG_SMP
 extern void do_set_cpus_allowed(struct task_struct *p,
 			       const struct cpumask *new_mask);

commit c255a458055e459f65eb7b7f51dc5dbdd0caf1d8
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Jul 31 16:43:02 2012 -0700

    memcg: rename config variables
    
    Sanity:
    
    CONFIG_CGROUP_MEM_RES_CTLR -> CONFIG_MEMCG
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP -> CONFIG_MEMCG_SWAP
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP_ENABLED -> CONFIG_MEMCG_SWAP_ENABLED
    CONFIG_CGROUP_MEM_RES_CTLR_KMEM -> CONFIG_MEMCG_KMEM
    
    [mhocko@suse.cz: fix missed bits]
    Cc: Glauber Costa <glommer@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68dcffaa62a0..865725adb9d3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1584,7 +1584,7 @@ struct task_struct {
 	/* bitmask and counter of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */
+#ifdef CONFIG_MEMCG /* memcg uses this to do batch job */
 	struct memcg_batch_info {
 		int do_batch;	/* incremented when batch uncharge started */
 		struct mem_cgroup *memcg; /* target memcg of uncharge */

commit 45226e944ce071d0231949f2fea90969437cd2dc
Author: Sameer Nanda <snanda@chromium.org>
Date:   Mon Jul 30 14:40:00 2012 -0700

    NMI watchdog: fix for lockup detector breakage on resume
    
    On the suspend/resume path the boot CPU does not go though an
    offline->online transition.  This breaks the NMI detector post-resume
    since it depends on PMU state that is lost when the system gets
    suspended.
    
    Fix this by forcing a CPU offline->online transition for the lockup
    detector on the boot CPU during resume.
    
    To provide more context, we enable NMI watchdog on Chrome OS.  We have
    seen several reports of systems freezing up completely which indicated
    that the NMI watchdog was not firing for some reason.
    
    Debugging further, we found a simple way of repro'ing system freezes --
    issuing the command 'tasket 1 sh -c "echo nmilockup > /proc/breakme"'
    after the system has been suspended/resumed one or more times.
    
    With this patch in place, the system freeze result in panics, as
    expected.
    
    These panics provide a nice stack trace for us to debug the actual issue
    causing the freeze.
    
    [akpm@linux-foundation.org: fiddle with code comment]
    [akpm@linux-foundation.org: make lockup_detector_bootcpu_resume() conditional on CONFIG_SUSPEND]
    [akpm@linux-foundation.org: fix section errors]
    Signed-off-by: Sameer Nanda <snanda@chromium.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e26a5e45aa6..68dcffaa62a0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -334,6 +334,14 @@ static inline void lockup_detector_init(void)
 }
 #endif
 
+#if defined(CONFIG_LOCKUP_DETECTOR) && defined(CONFIG_SUSPEND)
+void lockup_detector_bootcpu_resume(void);
+#else
+static inline void lockup_detector_bootcpu_resume(void)
+{
+}
+#endif
+
 #ifdef CONFIG_DETECT_HUNG_TASK
 extern unsigned int  sysctl_hung_task_panic;
 extern unsigned long sysctl_hung_task_check_count;

commit 54b501992dd2a839e94e76aa392c392b55080ce8
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Jul 30 14:39:18 2012 -0700

    coredump: warn about unsafe suid_dumpable / core_pattern combo
    
    When suid_dumpable=2, detect unsafe core_pattern settings and warn when
    they are seen.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a721cef7e2d4..1e26a5e45aa6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -406,6 +406,11 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 
+/* get/set_dumpable() values */
+#define SUID_DUMPABLE_DISABLED	0
+#define SUID_DUMPABLE_ENABLED	1
+#define SUID_DUMPABLE_SAFE	2
+
 /* mm flags */
 /* dumpable bits */
 #define MMF_DUMPABLE      0  /* core dump is permitted */

commit 79071638ce655c1f78a50d05c7dae0ad04a3e92a
Merge: 44a6b8442190 8323f26ce342
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 26 13:08:01 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is a performance improvement on SMP systems:
    
      | 4 socket 40 core + SMT Westmere box, single 30 sec tbench
      | runs, higher is better:
      |
      | clients     1       2       4        8       16       32       64      128
      |..........................................................................
      | pre        30      41     118      645     3769     6214    12233    14312
      | post      299     603    1211     2418     4697     6847    11606    14557
      |
      | A nice increase in performance.
    
      which speedup is particularly noticeable on heavily interacting
      few-tasks workloads, so the changes should help desktop-style Xorg
      workloads and interactivity as well, on multi-core CPUs.
    
      There are also cpuset suspend behavior fixes/restructuring and various
      smaller tweaks."
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix race in task_group()
      sched: Improve balance_cpu() to consider other cpus in its group as target of (pinned) task
      sched: Reset loop counters if all tasks are pinned and we need to redo load balance
      sched: Reorder 'struct lb_env' members to reduce its size
      sched: Improve scalability via 'CPU buddies', which withstand random perturbations
      cpusets: Remove/update outdated comments
      cpusets, hotplug: Restructure functions that are invoked during hotplug
      cpusets, hotplug: Implement cpuset tree traversal in a helper function
      CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
      sched/x86: Remove broken power estimation

commit 8323f26ce3425460769605a6aece7a174edaa7d1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jun 22 13:36:05 2012 +0200

    sched: Fix race in task_group()
    
    Stefan reported a crash on a kernel before a3e5d1091c1 ("sched:
    Don't call task_group() too many times in set_task_rq()"), he
    found the reason to be that the multiple task_group()
    invocations in set_task_rq() returned different values.
    
    Looking at all that I found a lack of serialization and plain
    wrong comments.
    
    The below tries to fix it using an extra pointer which is
    updated under the appropriate scheduler locks. Its not pretty,
    but I can't really see another way given how all the cgroup
    stuff works.
    
    Reported-and-tested-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340364965.18025.71.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bc9952991710..fd9436a3a545 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1245,6 +1245,9 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+#ifdef CONFIG_CGROUP_SCHED
+	struct task_group *sched_task_group;
+#endif
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -2724,7 +2727,7 @@ extern int sched_group_set_rt_period(struct task_group *tg,
 extern long sched_group_rt_period(struct task_group *tg);
 extern int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk);
 #endif
-#endif
+#endif /* CONFIG_CGROUP_SCHED */
 
 extern int task_can_switch_user(struct user_struct *up,
 					struct task_struct *tsk);

commit 970e178985cadbca660feb02f4d2ee3a09f7fdda
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Jun 12 05:18:32 2012 +0200

    sched: Improve scalability via 'CPU buddies', which withstand random perturbations
    
    Traversing an entire package is not only expensive, it also leads to tasks
    bouncing all over a partially idle and possible quite large package.  Fix
    that up by assigning a 'buddy' CPU to try to motivate.  Each buddy may try
    to motivate that one other CPU, if it's busy, tough, it may then try its
    SMT sibling, but that's all this optimization is allowed to cost.
    
    Sibling cache buddies are cross-wired to prevent bouncing.
    
    4 socket 40 core + SMT Westmere box, single 30 sec tbench runs, higher is better:
    
     clients     1       2       4        8       16       32       64      128
     ..........................................................................
     pre        30      41     118      645     3769     6214    12233    14312
     post      299     603    1211     2418     4697     6847    11606    14557
    
    A nice increase in performance.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1339471112.7352.32.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a1f493e0fef..bc9952991710 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -949,6 +949,7 @@ struct sched_domain {
 	unsigned int smt_gain;
 	int flags;			/* See SD_* */
 	int level;
+	int idle_buddy;			/* cpu assigned to select_idle_sibling() */
 
 	/* Runtime fields. */
 	unsigned long last_balance;	/* init to jiffies. units in jiffies */

commit a66d2c8f7ec1284206ca7c14569e2a607583f1e3
Merge: a6be1fcbc57f 8cae6f7158ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 23 12:27:27 2012 -0700

    Merge branch 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull the big VFS changes from Al Viro:
     "This one is *big* and changes quite a few things around VFS.  What's in there:
    
       - the first of two really major architecture changes - death to open
         intents.
    
         The former is finally there; it was very long in making, but with
         Miklos getting through really hard and messy final push in
         fs/namei.c, we finally have it.  Unlike his variant, this one
         doesn't introduce struct opendata; what we have instead is
         ->atomic_open() taking preallocated struct file * and passing
         everything via its fields.
    
         Instead of returning struct file *, it returns -E...  on error, 0
         on success and 1 in "deal with it yourself" case (e.g.  symlink
         found on server, etc.).
    
         See comments before fs/namei.c:atomic_open().  That made a lot of
         goodies finally possible and quite a few are in that pile:
         ->lookup(), ->d_revalidate() and ->create() do not get struct
         nameidata * anymore; ->lookup() and ->d_revalidate() get lookup
         flags instead, ->create() gets "do we want it exclusive" flag.
    
         With the introduction of new helper (kern_path_locked()) we are rid
         of all struct nameidata instances outside of fs/namei.c; it's still
         visible in namei.h, but not for long.  Come the next cycle,
         declaration will move either to fs/internal.h or to fs/namei.c
         itself.  [me, miklos, hch]
    
       - The second major change: behaviour of final fput().  Now we have
         __fput() done without any locks held by caller *and* not from deep
         in call stack.
    
         That obviously lifts a lot of constraints on the locking in there.
         Moreover, it's legal now to call fput() from atomic contexts (which
         has immediately simplified life for aio.c).  We also don't need
         anti-recursion logics in __scm_destroy() anymore.
    
         There is a price, though - the damn thing has become partially
         asynchronous.  For fput() from normal process we are guaranteed
         that pending __fput() will be done before the caller returns to
         userland, exits or gets stopped for ptrace.
    
         For kernel threads and atomic contexts it's done via
         schedule_work(), so theoretically we might need a way to make sure
         it's finished; so far only one such place had been found, but there
         might be more.
    
         There's flush_delayed_fput() (do all pending __fput()) and there's
         __fput_sync() (fput() analog doing __fput() immediately).  I hope
         we won't need them often; see warnings in fs/file_table.c for
         details.  [me, based on task_work series from Oleg merged last
         cycle]
    
       - sync series from Jan
    
       - large part of "death to sync_supers()" work from Artem; the only
         bits missing here are exofs and ext4 ones.  As far as I understand,
         those are going via the exofs and ext4 trees resp.; once they are
         in, we can put ->write_super() to the rest, along with the thread
         calling it.
    
       - preparatory bits from unionmount series (from dhowells).
    
       - assorted cleanups and fixes all over the place, as usual.
    
      This is not the last pile for this cycle; there's at least jlayton's
      ESTALE work and fsfreeze series (the latter - in dire need of fixes,
      so I'm not sure it'll make the cut this cycle).  I'll probably throw
      symlink/hardlink restrictions stuff from Kees into the next pile, too.
      Plus there's a lot of misc patches I hadn't thrown into that one -
      it's large enough as it is..."
    
    * 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (127 commits)
      ext4: switch EXT4_IOC_RESIZE_FS to mnt_want_write_file()
      btrfs: switch btrfs_ioctl_balance() to mnt_want_write_file()
      switch dentry_open() to struct path, make it grab references itself
      spufs: shift dget/mntget towards dentry_open()
      zoran: don't bother with struct file * in zoran_map
      ecryptfs: don't reinvent the wheels, please - use struct completion
      don't expose I_NEW inodes via dentry->d_inode
      tidy up namei.c a bit
      unobfuscate follow_up() a bit
      ext3: pass custom EOF to generic_file_llseek_size()
      ext4: use core vfs llseek code for dir seeks
      vfs: allow custom EOF in generic_file_llseek code
      vfs: Avoid unnecessary WB_SYNC_NONE writeback during sys_sync and reorder sync passes
      vfs: Remove unnecessary flushing of block devices
      vfs: Make sys_sync writeout also block device inodes
      vfs: Create function for iterating over block devices
      vfs: Reorder operations during sys_sync
      quota: Move quota syncing to ->sync_fs method
      quota: Split dquot_quota_sync() to writeback and cache flushing part
      vfs: Move noop_backing_dev_info check from sync into writeback
      ...

commit 6120d3dbb1220792ebea88cd475e1ec8f8620a93
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Jun 24 10:03:05 2012 +0400

    get rid of ->scm_work_list
    
    recursion in __scm_destroy() will be cut by delaying final fput()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af3555cc760f..598ba2da7865 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1546,7 +1546,6 @@ struct task_struct {
 	unsigned long timer_slack_ns;
 	unsigned long default_timer_slack_ns;
 
-	struct list_head	*scm_work_list;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored address in ret_stack */
 	int curr_ret_stack;

commit 67d1214551e800f9fe7dc7c47a346d2df0fafed5
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 27 11:07:19 2012 +0400

    merge task_work and rcu_head, get rid of separate allocation for keyring case
    
    task_work and rcu_head are identical now; merge them (calling the result
    struct callback_head, rcu_head #define'd to it), kill separate allocation
    in security/keys since we can just use cred->rcu now.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b9216ebc2789..af3555cc760f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1405,7 +1405,7 @@ struct task_struct {
 	int (*notifier)(void *priv);
 	void *notifier_data;
 	sigset_t *notifier_mask;
-	void *task_works;
+	struct callback_head *task_works;
 
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL

commit 158e1645e07f3e9f7e4962d7a0997f5c3b98311b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 27 09:24:13 2012 +0400

    trim task_work: get rid of hlist
    
    layout based on Oleg's suggestion; single-linked list,
    task->task_works points to the last element, forward pointer
    from said last element points to head.  I'd still prefer
    much more regular scheme with two pointers in task_work,
    but...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4059c0f33f07..b9216ebc2789 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1405,7 +1405,7 @@ struct task_struct {
 	int (*notifier)(void *priv);
 	void *notifier_data;
 	sigset_t *notifier_mask;
-	struct hlist_head task_works;
+	void *task_works;
 
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL

commit a2fe194723f6e4990d01d8c208c7b138fd410522
Merge: c3b7cdf18009 a018540141a9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jul 18 11:17:17 2012 +0200

    Merge branch 'linus' into perf/core
    
    Pick up the latest ring-buffer fixes, before applying a new fix.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ab93eb82164f9fc426eae7b286510cfd94738b8e
Merge: fdb1335a82ef 40b3c43f042c 25c037d64e7a 95c0d71dcb85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 14 11:16:24 2012 -0700

    Merge branches 'core-urgent-for-linus', 'perf-urgent-for-linus' and 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU, perf, and scheduler fixes from Ingo Molnar.
    
    The RCU fix is a revert for an optimization that could cause deadlocks.
    
    One of the scheduler commits (164c33c6adee "sched: Fix fork() error path
    to not crash") is correct but not complete (some architectures like Tile
    are not covered yet) - the resulting additional fixes are still WIP and
    Ingo did not want to delay these pending fixes.  See this thread on
    lkml:
    
      [PATCH] fork: fix error handling in dup_task()
    
    The perf fixes are just trivial oneliners.
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      Revert "rcu: Move PREEMPT_RCU preemption to switch_to() invocation"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf kvm: Fix segfault with report and mixed guestmount use
      perf kvm: Fix regression with guest machine creation
      perf script: Fix format regression due to libtraceevent merge
      ring-buffer: Fix accounting of entries when removing pages
      ring-buffer: Fix crash due to uninitialized new_pages list head
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      MAINTAINERS/sched: Update scheduler file pattern
      sched/nohz: Rewrite and fix load-avg computation -- again
      sched: Fix fork() error path to not crash

commit 5167e8d5417bf5c322a703d2927daec727ea40dd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 22 15:52:09 2012 +0200

    sched/nohz: Rewrite and fix load-avg computation -- again
    
    Thanks to Charles Wang for spotting the defects in the current code:
    
     - If we go idle during the sample window -- after sampling, we get a
       negative bias because we can negate our own sample.
    
     - If we wake up during the sample window we get a positive bias
       because we push the sample to a known active period.
    
    So rewrite the entire nohz load-avg muck once again, now adding
    copious documentation to the code.
    
    Reported-and-tested-by: Doug Smythies <dsmythies@telus.net>
    Reported-and-tested-by: Charles Wang <muming.wq@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/1340373782.18025.74.camel@twins
    [ minor edits ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4059c0f33f07..20cb7497c59c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1909,6 +1909,14 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 }
 #endif
 
+#ifdef CONFIG_NO_HZ
+void calc_load_enter_idle(void);
+void calc_load_exit_idle(void);
+#else
+static inline void calc_load_enter_idle(void) { }
+static inline void calc_load_exit_idle(void) { }
+#endif /* CONFIG_NO_HZ */
+
 #ifndef CONFIG_CPUMASK_OFFSTACK
 static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 {

commit cba6d0d64ee53772b285d0c0c288deefbeaf7775
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 07:08:42 2012 -0700

    Revert "rcu: Move PREEMPT_RCU preemption to switch_to() invocation"
    
    This reverts commit 616c310e83b872024271c915c1b9ab505b9efad9.
    (Move PREEMPT_RCU preemption to switch_to() invocation).
    Testing by Sasha Levin <levinsasha928@gmail.com> showed that this
    can result in deadlock due to invoking the scheduler when one of
    the runqueue locks is held.  Because this commit was simply a
    performance optimization, revert it.
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4059c0f33f07..06a4c5f4f55c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1871,22 +1871,12 @@ static inline void rcu_copy_process(struct task_struct *p)
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 
-static inline void rcu_switch_from(struct task_struct *prev)
-{
-	if (prev->rcu_read_lock_nesting != 0)
-		rcu_preempt_note_context_switch();
-}
-
 #else
 
 static inline void rcu_copy_process(struct task_struct *p)
 {
 }
 
-static inline void rcu_switch_from(struct task_struct *prev)
-{
-}
-
 #endif
 
 #ifdef CONFIG_SMP

commit c3e228d59bd2054fd57f7f146ef0f6fb0e1996b7
Merge: 7eb9ba5ed312 cfaf025112d3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 11 10:51:35 2012 +0200

    Merge tag 'v3.5-rc2' into perf/core
    
    Merge in Linux 3.5-rc2 - to pick up fixes.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 72494504498ff5ac2f086a83473d4dd1ca490bd3
Merge: cd96891d48a9 a841f8cef4bb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 8 14:59:29 2012 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix the relax_domain_level boot parameter
      sched: Validate assumptions in sched_init_numa()
      sched: Always initialize cpu-power
      sched: Fix domain iteration
      sched/rt: Fix lockdep annotation within find_lock_lowest_rq()
      sched/numa: Load balance between remote nodes
      sched/x86: Calculate booted cores after construction of sibling_mask

commit bafb282df29c1524b1617019adebd6d0c3eb7a47
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu Jun 7 14:21:11 2012 -0700

    c/r: prctl: update prctl_set_mm_exe_file() after mm->num_exe_file_vmas removal
    
    A fix for commit b32dfe377102 ("c/r: prctl: add ability to set new
    mm_struct::exe_file").
    
    After removing mm->num_exe_file_vmas kernel keeps mm->exe_file until
    final mmput(), it never becomes NULL while task is alive.
    
    We can check for other mapped files in mm instead of checking
    mm->num_exe_file_vmas, and mark mm with flag MMF_EXE_FILE_CHANGED in
    order to forbid second changing of mm->exe_file.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Reviewed-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6029d8c54476..c688d4cc2e40 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -439,6 +439,7 @@ extern int get_dumpable(struct mm_struct *mm);
 					/* leave room for more dump flags */
 #define MMF_VM_MERGEABLE	16	/* KSM may merge identical pages */
 #define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
+#define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit 778b032d96909690c19d84f8d17c13be65ed6f8e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue May 29 21:30:08 2012 +0200

    uprobes: Kill uprobes_srcu/uprobe_srcu_id
    
    Kill the no longer needed uprobes_srcu/uprobe_srcu_id code.
    
    It doesn't really work anyway. synchronize_srcu() can only
    synchronize with the code "inside" the
    srcu_read_lock/srcu_read_unlock section, while
    uprobe_pre_sstep_notifier() does srcu_read_lock() _after_ we
    already hit the breakpoint.
    
    I guess this probably works "in practice". synchronize_srcu() is
    slow and it implies synchronize_sched(), and the probed task
    enters the non- preemptible section at the start of exception
    handler. Still this is not right at least in theory, and
    task->uprobe_srcu_id blows task_struct.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anton Arapov <anton@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120529193008.GG8057@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6029d8c54476..6bd19655c1a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1569,7 +1569,6 @@ struct task_struct {
 #endif
 #ifdef CONFIG_UPROBES
 	struct uprobe_task *utask;
-	int uprobe_srcu_id;
 #endif
 };
 

commit c1174876874dcf8986806e4dad3d7d07af20b439
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 31 14:47:33 2012 +0200

    sched: Fix domain iteration
    
    Weird topologies can lead to asymmetric domain setups. This needs
    further consideration since these setups are typically non-minimal
    too.
    
    For now, make it work by adding an extra mask selecting which CPUs
    are allowed to iterate up.
    
    The topology that triggered it is the one from David Rientjes:
    
            10 20 20 30
            20 10 20 20
            20 20 10 20
            30 20 20 10
    
    resulting in boxes that wouldn't even boot.
    
    Reported-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-3p86l9cuaqnxz7uxsojmz5rm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6029d8c54476..ac321d753470 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -876,6 +876,8 @@ struct sched_group_power {
 	 * Number of busy cpus in this group.
 	 */
 	atomic_t nr_busy_cpus;
+
+	unsigned long cpumask[0]; /* iteration mask */
 };
 
 struct sched_group {
@@ -900,6 +902,15 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 	return to_cpumask(sg->cpumask);
 }
 
+/*
+ * cpumask masking which cpus in the group are allowed to iterate up the domain
+ * tree.
+ */
+static inline struct cpumask *sched_group_mask(struct sched_group *sg)
+{
+	return to_cpumask(sg->sgp->cpumask);
+}
+
 /**
  * group_first_cpu - Returns the first cpu in the cpumask of a sched_group.
  * @group: The group whose first cpu is to be returned.

commit 0b3e9f3f21c42d064f5f4088df4088e3d55755eb
Merge: 99becf1328d8 6a4c96eef42f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 5 09:47:15 2012 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Remove NULL assignment of dattr_cur
      sched: Remove the last NULL entry from sched_feat_names
      sched: Make sched_feat_names const
      sched/rt: Fix SCHED_RR across cgroups
      sched: Move nr_cpus_allowed out of 'struct sched_rt_entity'
      sched: Make sure to not re-read variables after validation
      sched: Fix SD_OVERLAP
      sched: Don't try allocating memory from offline nodes
      sched/nohz: Fix rq->cpu_load calculations some more
      sched/x86: Use cpu_llc_shared_mask(cpu) for coregroup_mask

commit 77097ae503b170120ab66dd1d547f8577193f91f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Apr 27 13:58:59 2012 -0400

    most of set_current_blocked() callers want SIGKILL/SIGSTOP removed from set
    
    Only 3 out of 63 do not.  Renamed the current variant to __set_current_blocked(),
    added set_current_blocked() that will exclude unblockable signals, switched
    open-coded instances to it.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ded3fb63fb06..f34437e835a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2210,7 +2210,7 @@ extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned lon
 static inline void restore_saved_sigmask(void)
 {
 	if (test_and_clear_restore_sigmask())
-		set_current_blocked(&current->saved_sigmask);
+		__set_current_blocked(&current->saved_sigmask);
 }
 
 static inline sigset_t *sigmask_to_save(void)

commit b7f9a11a6cf1ea9ee6be3eb2b90d91327a09ad14
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed May 2 09:59:21 2012 -0400

    new helper: sigmask_to_save()
    
    replace boilerplate "should we use ->saved_sigmask or ->blocked?"
    with calls of obvious inlined helper...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f1b46b88f6f5..ded3fb63fb06 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2213,6 +2213,14 @@ static inline void restore_saved_sigmask(void)
 		set_current_blocked(&current->saved_sigmask);
 }
 
+static inline sigset_t *sigmask_to_save(void)
+{
+	sigset_t *res = &current->blocked;
+	if (unlikely(test_restore_sigmask()))
+		res = &current->saved_sigmask;
+	return res;
+}
+
 static inline int kill_cad_pid(int sig, int priv)
 {
 	return kill_pid(cad_pid, sig, priv);

commit 51a7b448d4134e3e8eec633435e3e8faee14a828
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon May 21 23:33:55 2012 -0400

    new helper: restore_saved_sigmask()
    
    first fruits of ..._restore_sigmask() helpers: now we can take
    boilerplate "signal didn't have a handler, clear RESTORE_SIGMASK
    and restore the blocked mask from ->saved_mask" into a common
    helper.  Open-coded instances switched...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 660c8ae93471..f1b46b88f6f5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2207,6 +2207,12 @@ extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
 
+static inline void restore_saved_sigmask(void)
+{
+	if (test_and_clear_restore_sigmask())
+		set_current_blocked(&current->saved_sigmask);
+}
+
 static inline int kill_cad_pid(int sig, int priv)
 {
 	return kill_pid(cad_pid, sig, priv);

commit fb21affa49204acd409328415b49bfe90136653c
Merge: a00b6151a2ae f23ca335462e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 31 18:47:30 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull second pile of signal handling patches from Al Viro:
     "This one is just task_work_add() series + remaining prereqs for it.
    
      There probably will be another pull request from that tree this
      cycle - at least for helpers, to get them out of the way for per-arch
      fixes remaining in the tree."
    
    Fix trivial conflict in kernel/irq/manage.c: the merge of Andrew's pile
    had brought in commit 97fd75b7b8e0 ("kernel/irq/manage.c: use the
    pr_foo() infrastructure to prefix printks") which changed one of the
    pr_err() calls that this merge moves around.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      keys: kill task_struct->replacement_session_keyring
      keys: kill the dummy key_replace_session_keyring()
      keys: change keyctl_session_to_parent() to use task_work_add()
      genirq: reimplement exit_irq_thread() hook via task_work_add()
      task_work_add: generic process-context callbacks
      avr32: missed _TIF_NOTIFY_RESUME on one of do_notify_resume callers
      parisc: need to check NOTIFY_RESUME when exiting from syscall
      move key_repace_session_keyring() into tracehook_notify_resume()
      TIF_NOTIFY_RESUME is defined on all targets now

commit 29baa7478ba47d746e3625c91d3b2afbf46b4312
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Apr 23 12:11:21 2012 +0200

    sched: Move nr_cpus_allowed out of 'struct sched_rt_entity'
    
    Since nr_cpus_allowed is used outside of sched/rt.c and wants to be
    used outside of there more, move it to a more natural site.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-kr61f02y9brwzkh6x53pdptm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d61e5977e517..0f50e78f7f44 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1188,7 +1188,6 @@ struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned long timeout;
 	unsigned int time_slice;
-	int nr_cpus_allowed;
 
 	struct sched_rt_entity *back;
 #ifdef CONFIG_RT_GROUP_SCHED
@@ -1253,6 +1252,7 @@ struct task_struct {
 #endif
 
 	unsigned int policy;
+	int nr_cpus_allowed;
 	cpumask_t cpus_allowed;
 
 #ifdef CONFIG_PREEMPT_RCU

commit 5aaa0b7a2ed5b12692c9ffb5222182bd558d3146
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 17 17:15:29 2012 +0200

    sched/nohz: Fix rq->cpu_load calculations some more
    
    Follow up on commit 556061b00 ("sched/nohz: Fix rq->cpu_load[]
    calculations") since while that fixed the busy case it regressed the
    mostly idle case.
    
    Add a callback from the nohz exit to also age the rq->cpu_load[]
    array. This closes the hole where either there was no nohz load
    balance pass during the nohz, or there was a 'significant' amount of
    idle time between the last nohz balance and the nohz exit.
    
    So we'll update unconditionally from the tick to not insert any
    accidental 0 load periods while busy, and we try and catch up from
    nohz idle balance and nohz exit. Both these are still prone to missing
    a jiffy, but that has always been the case.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: pjt@google.com
    Cc: Venkatesh Pallipadi <venki@google.com>
    Link: http://lkml.kernel.org/n/tip-kt0trz0apodbf84ucjfdbr1a@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f45c0b280b5d..d61e5977e517 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -145,6 +145,7 @@ extern unsigned long this_cpu_load(void);
 
 
 extern void calc_global_load(unsigned long ticks);
+extern void update_cpu_load_nohz(void);
 
 extern unsigned long get_parent_ip(unsigned long addr);
 

commit 654443e20dfc0617231f28a07c96a979ee1a0239
Merge: 2c01e7bc46f1 9cba26e66d09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 11:39:34 2012 -0700

    Merge branch 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull user-space probe instrumentation from Ingo Molnar:
     "The uprobes code originates from SystemTap and has been used for years
      in Fedora and RHEL kernels.  This version is much rewritten, reviews
      from PeterZ, Oleg and myself shaped the end result.
    
      This tree includes uprobes support in 'perf probe' - but SystemTap
      (and other tools) can take advantage of user probe points as well.
    
      Sample usage of uprobes via perf, for example to profile malloc()
      calls without modifying user-space binaries.
    
      First boot a new kernel with CONFIG_UPROBE_EVENT=y enabled.
    
      If you don't know which function you want to probe you can pick one
      from 'perf top' or can get a list all functions that can be probed
      within libc (binaries can be specified as well):
    
            $ perf probe -F -x /lib/libc.so.6
    
      To probe libc's malloc():
    
            $ perf probe -x /lib64/libc.so.6 malloc
            Added new event:
            probe_libc:malloc    (on 0x7eac0)
    
      You can now use it in all perf tools, such as:
    
            perf record -e probe_libc:malloc -aR sleep 1
    
      Make use of it to create a call graph (as the flat profile is going to
      look very boring):
    
            $ perf record -e probe_libc:malloc -gR make
            [ perf record: Woken up 173 times to write data ]
            [ perf record: Captured and wrote 44.190 MB perf.data (~1930712
    
            $ perf report | less
    
              32.03%            git  libc-2.15.so   [.] malloc
                                |
                                --- malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                                |
                                --- malloc
                                   |
                                   |--0.95%-- 0x208eb1000000000
                                   |
                                   |--0.63%-- htab_traverse_noresize
    
              11.04%             as  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               7.15%             ld  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               5.07%             sh  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
               4.99%  python-config  libc-2.15.so   [.] malloc
                      |
                      --- malloc
                         |
               4.54%           make  libc-2.15.so   [.] malloc
                               |
                               --- malloc
                                  |
                                  |--7.34%-- glob
                                  |          |
                                  |          |--93.18%-- 0x41588f
                                  |          |
                                  |           --6.82%-- glob
                                  |                     0x41588f
    
               ...
    
      Or:
    
            $ perf report -g flat | less
    
            # Overhead        Command  Shared Object      Symbol
            # ........  .............  .............  ..........
            #
              32.03%            git  libc-2.15.so   [.] malloc
                      27.19%
                          malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                      24.77%
                          malloc
    
              11.04%             as  libc-2.15.so   [.] malloc
                      11.02%
                          malloc
    
               7.15%             ld  libc-2.15.so   [.] malloc
                       6.57%
                          malloc
    
             ...
    
      The core uprobes design is fairly straightforward: uprobes probe
      points register themselves at (inode:offset) addresses of
      libraries/binaries, after which all existing (or new) vmas that map
      that address will have a software breakpoint injected at that address.
      vmas are COW-ed to preserve original content.  The probe points are
      kept in an rbtree.
    
      If user-space executes the probed inode:offset instruction address
      then an event is generated which can be recovered from the regular
      perf event channels and mmap-ed ring-buffer.
    
      Multiple probes at the same address are supported, they create a
      dynamic callback list of event consumers.
    
      The basic model is further complicated by the XOL speedup: the
      original instruction that is probed is copied (in an architecture
      specific fashion) and executed out of line when the probe triggers.
      The XOL area is a single vma per process, with a fixed number of
      entries (which limits probe execution parallelism).
    
      The API: uprobes are installed/removed via
      /sys/kernel/debug/tracing/uprobe_events, the API is integrated to
      align with the kprobes interface as much as possible, but is separate
      to it.
    
      Injecting a probe point is privileged operation, which can be relaxed
      by setting perf_paranoid to -1.
    
      You can use multiple probes as well and mix them with kprobes and
      regular PMU events or tracepoints, when instrumenting a task."
    
    Fix up trivial conflicts in mm/memory.c due to previous cleanup of
    unmap_single_vma().
    
    * 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      perf probe: Detect probe target when m/x options are absent
      perf probe: Provide perf interface for uprobes
      tracing: Fix kconfig warning due to a typo
      tracing: Provide trace events interface for uprobes
      tracing: Extract out common code for kprobes/uprobes trace events
      tracing: Modify is_delete, is_return from int to bool
      uprobes/core: Decrement uprobe count before the pages are unmapped
      uprobes/core: Make background page replacement logic account for rss_stat counters
      uprobes/core: Optimize probe hits with the help of a counter
      uprobes/core: Allocate XOL slots for uprobes use
      uprobes/core: Handle breakpoint and singlestep exceptions
      uprobes/core: Rename bkpt to swbp
      uprobes/core: Make order of function parameters consistent across functions
      uprobes/core: Make macro names consistent
      uprobes: Update copyright notices
      uprobes/core: Move insn to arch specific structure
      uprobes/core: Remove uprobe_opcode_sz
      uprobes/core: Make instruction tables volatile
      uprobes: Move to kernel/events/
      uprobes/core: Clean up, refactor and improve the code
      ...

commit f23ca335462e3c84f13270b9e65f83936068ec2c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:09 2012 +1000

    keys: kill task_struct->replacement_session_keyring
    
    Kill the no longer used task_struct->replacement_session_keyring, update
    copy_creds() and exit_creds().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index da013853a622..17c6c929ee94 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1357,8 +1357,6 @@ struct task_struct {
 					 * credentials (COW) */
 	const struct cred __rcu *cred;	/* effective (overridable) subjective task
 					 * credentials (COW) */
-	struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */
-
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
 				       it with task_lock())

commit 4d1d61a6b203d957777d73fcebf19d90b038b5b2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:08 2012 +1000

    genirq: reimplement exit_irq_thread() hook via task_work_add()
    
    exit_irq_thread() and task->irq_thread are needed to handle the unexpected
    (and unlikely) exit of irq-thread.
    
    We can use task_work instead and make this all private to
    kernel/irq/manage.c, cleanup plus micro-optimization.
    
    1. rename exit_irq_thread() to irq_thread_dtor(), make it
       static, and move it up before irq_thread().
    
    2. change irq_thread() to do task_work_add(irq_thread_dtor)
       at the start and task_work_cancel() before return.
    
       tracehook_notify_resume() can never play with kthreads,
       only do_exit()->exit_task_work() can call the callback
       and this is what we want.
    
    3. remove task_struct->irq_thread and the special hook
       in do_exit().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7930131abc1a..da013853a622 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1301,11 +1301,6 @@ struct task_struct {
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 
-#ifdef CONFIG_GENERIC_HARDIRQS
-	/* IRQ handler threads */
-	unsigned irq_thread:1;
-#endif
-
 	pid_t pid;
 	pid_t tgid;
 
@@ -1313,10 +1308,9 @@ struct task_struct {
 	/* Canary value for the -fstack-protector gcc feature */
 	unsigned long stack_canary;
 #endif
-
-	/* 
+	/*
 	 * pointers to (original) parent process, youngest child, younger sibling,
-	 * older sibling, respectively.  (p->father can be replaced with 
+	 * older sibling, respectively.  (p->father can be replaced with
 	 * p->real_parent->pid)
 	 */
 	struct task_struct __rcu *real_parent; /* real parent process */

commit e73f8959af0439d114847eab5a8a5ce48f1217c4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:07 2012 +1000

    task_work_add: generic process-context callbacks
    
    Provide a simple mechanism that allows running code in the (nonatomic)
    context of the arbitrary task.
    
    The caller does task_work_add(task, task_work) and this task executes
    task_work->func() either from do_notify_resume() or from do_exit().  The
    callback can rely on PF_EXITING to detect the latter case.
    
    "struct task_work" can be embedded in another struct, still it has "void
    *data" to handle the most common/simple case.
    
    This allows us to kill the ->replacement_session_keyring hack, and
    potentially this can have more users.
    
    Performance-wise, this adds 2 "unlikely(!hlist_empty())" checks into
    tracehook_notify_resume() and do_exit().  But at the same time we can
    remove the "replacement_session_keyring != NULL" checks from
    arch/*/signal.c and exit_creds().
    
    Note: task_work_add/task_work_run abuses ->pi_lock.  This is only because
    this lock is already used by lookup_pi_state() to synchronize with
    do_exit() setting PF_EXITING.  Fortunately the scope of this lock in
    task_work.c is really tiny, and the code is unlikely anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5ea8baea9387..7930131abc1a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1400,6 +1400,8 @@ struct task_struct {
 	int (*notifier)(void *priv);
 	void *notifier_data;
 	sigset_t *notifier_mask;
+	struct hlist_head task_works;
+
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL
 	uid_t loginuid;

commit 644473e9c60c1ff4f6351fed637a6e5551e3dce7
Merge: fb827ec68446 4b06a81f1dae
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 17:42:39 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace enhancements from Eric Biederman:
     "This is a course correction for the user namespace, so that we can
      reach an inexpensive, maintainable, and reasonably complete
      implementation.
    
      Highlights:
       - Config guards make it impossible to enable the user namespace and
         code that has not been converted to be user namespace safe.
    
       - Use of the new kuid_t type ensures the if you somehow get past the
         config guards the kernel will encounter type errors if you enable
         user namespaces and attempt to compile in code whose permission
         checks have not been updated to be user namespace safe.
    
       - All uids from child user namespaces are mapped into the initial
         user namespace before they are processed.  Removing the need to add
         an additional check to see if the user namespace of the compared
         uids remains the same.
    
       - With the user namespaces compiled out the performance is as good or
         better than it is today.
    
       - For most operations absolutely nothing changes performance or
         operationally with the user namespace enabled.
    
       - The worst case performance I could come up with was timing 1
         billion cache cold stat operations with the user namespace code
         enabled.  This went from 156s to 164s on my laptop (or 156ns to
         164ns per stat operation).
    
       - (uid_t)-1 and (gid_t)-1 are reserved as an internal error value.
         Most uid/gid setting system calls treat these value specially
         anyway so attempting to use -1 as a uid would likely cause
         entertaining failures in userspace.
    
       - If setuid is called with a uid that can not be mapped setuid fails.
         I have looked at sendmail, login, ssh and every other program I
         could think of that would call setuid and they all check for and
         handle the case where setuid fails.
    
       - If stat or a similar system call is called from a context in which
         we can not map a uid we lie and return overflowuid.  The LFS
         experience suggests not lying and returning an error code might be
         better, but the historical precedent with uids is different and I
         can not think of anything that would break by lying about a uid we
         can't map.
    
       - Capabilities are localized to the current user namespace making it
         safe to give the initial user in a user namespace all capabilities.
    
      My git tree covers all of the modifications needed to convert the core
      kernel and enough changes to make a system bootable to runlevel 1."
    
    Fix up trivial conflicts due to nearby independent changes in fs/stat.c
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (46 commits)
      userns:  Silence silly gcc warning.
      cred: use correct cred accessor with regards to rcu read lock
      userns: Convert the move_pages, and migrate_pages permission checks to use uid_eq
      userns: Convert cgroup permission checks to use uid_eq
      userns: Convert tmpfs to use kuid and kgid where appropriate
      userns: Convert sysfs to use kgid/kuid where appropriate
      userns: Convert sysctl permission checks to use kuid and kgids.
      userns: Convert proc to use kuid/kgid where appropriate
      userns: Convert ext4 to user kuid/kgid where appropriate
      userns: Convert ext3 to use kuid/kgid where appropriate
      userns: Convert ext2 to use kuid/kgid where appropriate.
      userns: Convert devpts to use kuid/kgid where appropriate
      userns: Convert binary formats to use kuid/kgid where appropriate
      userns: Add negative depends on entries to avoid building code that is userns unsafe
      userns: signal remove unnecessary map_cred_ns
      userns: Teach inode_capable to understand inodes whose uids map to other namespaces.
      userns: Fail exec for suid and sgid binaries with ids outside our user namespace.
      userns: Convert stat to return values mapped from kuids and kgids
      userns: Convert user specfied uids and gids in chown into kuids and kgid
      userns: Use uid_eq gid_eq helpers when comparing kuids and kgids in the vfs
      ...

commit d79ee93de909dfb252279b9a95978bbda9a814a9
Merge: 2ff2b289a695 1c2927f18576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 18:27:32 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is the cleanup/simplification of the load-balancer:
      instead of the current practice of architectures twiddling scheduler
      internal data structures and providing the scheduler domains in
      colorfully inconsistent ways, we now have generic scheduler code in
      kernel/sched/core.c:sched_init_numa() that looks at the architecture's
      node_distance() parameters and (while not fully trusting it) deducts a
      NUMA topology from it.
    
      This inevitably changes balancing behavior - hopefully for the better.
    
      There are various smaller optimizations, cleanups and fixlets as well"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Taint kernel with TAINT_WARN after sleep-in-atomic bug
      sched: Remove stale power aware scheduling remnants and dysfunctional knobs
      sched/debug: Fix printing large integers on 32-bit platforms
      sched/fair: Improve the ->group_imb logic
      sched/nohz: Fix rq->cpu_load[] calculations
      sched/numa: Don't scale the imbalance
      sched/fair: Revert sched-domain iteration breakage
      sched/x86: Rewrite set_cpu_sibling_map()
      sched/numa: Fix the new NUMA topology bits
      sched/numa: Rewrite the CONFIG_NUMA sched domain support
      sched/fair: Propagate 'struct lb_env' usage into find_busiest_group
      sched/fair: Add some serialization to the sched_domain load-balance walk
      sched/fair: Let minimally loaded cpu balance the group
      sched: Change rq->nr_running to unsigned int
      x86/numa: Check for nonsensical topologies on real hw as well
      x86/numa: Hard partition cpu topology masks on node boundaries
      x86/numa: Allow specifying node_distance() for numa=fake
      x86/sched: Make mwait_usable() heed to "idle=" kernel parameters properly
      sched: Update documentation and comments
      sched_rt: Avoid unnecessary dequeue and enqueue of pushable tasks in set_cpus_allowed_rt()

commit cb60e3e65c1b96a4d6444a7a13dc7dd48bc15a2b
Merge: 99262a3dafa3 ff2bb047c4bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 21 20:27:36 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "New notable features:
       - The seccomp work from Will Drewry
       - PR_{GET,SET}_NO_NEW_PRIVS from Andy Lutomirski
       - Longer security labels for Smack from Casey Schaufler
       - Additional ptrace restriction modes for Yama by Kees Cook"
    
    Fix up trivial context conflicts in arch/x86/Kconfig and include/linux/filter.h
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (65 commits)
      apparmor: fix long path failure due to disconnected path
      apparmor: fix profile lookup for unconfined
      ima: fix filename hint to reflect script interpreter name
      KEYS: Don't check for NULL key pointer in key_validate()
      Smack: allow for significantly longer Smack labels v4
      gfp flags for security_inode_alloc()?
      Smack: recursive tramsmute
      Yama: replace capable() with ns_capable()
      TOMOYO: Accept manager programs which do not start with / .
      KEYS: Add invalidation support
      KEYS: Do LRU discard in full keyrings
      KEYS: Permit in-place link replacement in keyring list
      KEYS: Perform RCU synchronisation on keys prior to key destruction
      KEYS: Announce key type (un)registration
      KEYS: Reorganise keys Makefile
      KEYS: Move the key config into security/keys/Kconfig
      KEYS: Use the compat keyctl() syscall wrapper on Sparc64 for Sparc32 compat
      Yama: remove an unused variable
      samples/seccomp: fix dependencies on arch macros
      Yama: add additional ptrace scopes
      ...

commit 8e7fbcbc22c12414bcc9dfdd683637f58fb32759
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 9 11:28:35 2012 +0100

    sched: Remove stale power aware scheduling remnants and dysfunctional knobs
    
    It's been broken forever (i.e. it's not scheduling in a power
    aware fashion), as reported by Suresh and others sending
    patches, and nobody cares enough to fix it properly ...
    so remove it to make space free for something better.
    
    There's various problems with the code as it stands today, first
    and foremost the user interface which is bound to topology
    levels and has multiple values per level. This results in a
    state explosion which the administrator or distro needs to
    master and almost nobody does.
    
    Furthermore large configuration state spaces aren't good, it
    means the thing doesn't just work right because it's either
    under so many impossibe to meet constraints, or even if
    there's an achievable state workloads have to be aware of
    it precisely and can never meet it for dynamic workloads.
    
    So pushing this kind of decision to user-space was a bad idea
    even with a single knob - it's exponentially worse with knobs
    on every node of the topology.
    
    There is a proposal to replace the user interface with a single
    3 state knob:
    
     sched_balance_policy := { performance, power, auto }
    
    where 'auto' would be the preferred default which looks at things
    like Battery/AC mode and possible cpufreq state or whatever the hw
    exposes to show us power use expectations - but there's been no
    progress on it in the past many months.
    
    Aside from that, the actual implementation of the various knobs
    is known to be broken. There have been sporadic attempts at
    fixing things but these always stop short of reaching a mergable
    state.
    
    Therefore this wholesale removal with the hopes of spurring
    people who care to come forward once again and work on a
    coherent replacement.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1326104915.2442.53.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a559bf0622f..3d644809c9db 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -855,61 +855,14 @@ enum cpu_idle_type {
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
 #define SD_PREFER_LOCAL		0x0040  /* Prefer to keep tasks local to this domain */
 #define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
-#define SD_POWERSAVINGS_BALANCE	0x0100	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
 #define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
 #define SD_OVERLAP		0x2000	/* sched_domains of this level overlap */
 
-enum powersavings_balance_level {
-	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */
-	POWERSAVINGS_BALANCE_BASIC,	/* Fill one thread/core/package
-					 * first for long running threads
-					 */
-	POWERSAVINGS_BALANCE_WAKEUP,	/* Also bias task wakeups to semi-idle
-					 * cpu package for power savings
-					 */
-	MAX_POWERSAVINGS_BALANCE_LEVELS
-};
-
-extern int sched_mc_power_savings, sched_smt_power_savings;
-
-static inline int sd_balance_for_mc_power(void)
-{
-	if (sched_smt_power_savings)
-		return SD_POWERSAVINGS_BALANCE;
-
-	if (!sched_mc_power_savings)
-		return SD_PREFER_SIBLING;
-
-	return 0;
-}
-
-static inline int sd_balance_for_package_power(void)
-{
-	if (sched_mc_power_savings | sched_smt_power_savings)
-		return SD_POWERSAVINGS_BALANCE;
-
-	return SD_PREFER_SIBLING;
-}
-
 extern int __weak arch_sd_sibiling_asym_packing(void);
 
-/*
- * Optimise SD flags for power savings:
- * SD_BALANCE_NEWIDLE helps aggressive task consolidation and power savings.
- * Keep default SD flags if sched_{smt,mc}_power_saving=0
- */
-
-static inline int sd_power_saving_flags(void)
-{
-	if (sched_mc_power_savings | sched_smt_power_savings)
-		return SD_BALANCE_NEWIDLE;
-
-	return 0;
-}
-
 struct sched_group_power {
 	atomic_t ref;
 	/*

commit 04f733b4afac5dc93ae9b0a8703c60b87def491e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 00:12:02 2012 +0200

    sched/fair: Revert sched-domain iteration breakage
    
    Patches c22402a2f ("sched/fair: Let minimally loaded cpu balance the
    group") and 0ce90475 ("sched/fair: Add some serialization to the
    sched_domain load-balance walk") are horribly broken so revert them.
    
    The problem is that while it sounds good to have the minimally loaded
    cpu do the pulling of more load, the way we walk the domains there is
    absolutely no guarantee this cpu will actually get to the domain. In
    fact its very likely it wont. Therefore the higher up the tree we get,
    the less likely it is we'll balance at all.
    
    The first of mask always walks up, while sucky in that it accumulates
    load on the first cpu and needs extra passes to spread it out at least
    guarantees a cpu gets up that far and load-balancing happens at all.
    
    Since its now always the first and idle cpus should always be able to
    balance so they get a task as fast as possible we can also do away
    with the added serialization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-rpuhs5s56aiv1aw7khv9zkw6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3cbfb55bde25..4a559bf0622f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -927,7 +927,6 @@ struct sched_group_power {
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
 	atomic_t ref;
-	int balance_cpu;
 
 	unsigned int group_weight;
 	struct sched_group_power *sgp;

commit 0ce90475dcdbe90affc218e9688c8401e468e84d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Apr 25 00:30:36 2012 +0200

    sched/fair: Add some serialization to the sched_domain load-balance walk
    
    Since the sched_domain walk is completely unserialized (!SD_SERIALIZE)
    it is possible that multiple cpus in the group get elected to do the
    next level. Avoid this by adding some serialization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-vqh9ai6s0ewmeakjz80w4qz6@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a559bf0622f..3cbfb55bde25 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -927,6 +927,7 @@ struct sched_group_power {
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
 	atomic_t ref;
+	int balance_cpu;
 
 	unsigned int group_weight;
 	struct sched_group_power *sgp;

commit 489a71b029cd94e3b0132795146e8be3a87bf3fa
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Apr 2 17:00:44 2012 +0900

    sched: Update documentation and comments
    
    Change sched_*.c to sched/*.c in documentation and comments.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F795CAC.9080206@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81a173c0897d..4a559bf0622f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1950,7 +1950,7 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
  */
 extern unsigned long long notrace sched_clock(void);
 /*
- * See the comment in kernel/sched_clock.c
+ * See the comment in kernel/sched/clock.c
  */
 extern u64 cpu_clock(int cpu);
 extern u64 local_clock(void);

commit 616c310e83b872024271c915c1b9ab505b9efad9
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Mar 27 16:02:08 2012 -0700

    rcu: Move PREEMPT_RCU preemption to switch_to() invocation
    
    Currently, PREEMPT_RCU readers are enqueued upon entry to the scheduler.
    This is inefficient because enqueuing is required only if there is a
    context switch, and entry to the scheduler does not guarantee a context
    switch.
    
    The commit therefore moves the enqueuing to immediately precede the
    call to switch_to() from the scheduler.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81a173c0897d..8f3fd945070f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1905,12 +1905,22 @@ static inline void rcu_copy_process(struct task_struct *p)
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 
+static inline void rcu_switch_from(struct task_struct *prev)
+{
+	if (prev->rcu_read_lock_nesting != 0)
+		rcu_preempt_note_context_switch();
+}
+
 #else
 
 static inline void rcu_copy_process(struct task_struct *p)
 {
 }
 
+static inline void rcu_switch_from(struct task_struct *prev)
+{
+}
+
 #endif
 
 #ifdef CONFIG_SMP

commit 6ac1ef482d7ae0c690f1640bf6eb818ff9a2d91e
Merge: 682968e0c425 a385ec4f11bd
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Apr 14 13:18:27 2012 +0200

    Merge branch 'perf/core' into perf/uprobes
    
    Merge in latest upstream (and the latest perf development tree),
    to prepare for tooling changes, and also to pick up v3.4 MM
    changes that the uprobes code needs to take care of.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 932ecebb0405b9a41cd18946e6cff8a17d434e23
Author: Will Drewry <wad@chromium.org>
Date:   Thu Apr 12 16:47:54 2012 -0500

    seccomp: kill the seccomp_t typedef
    
    Replaces the seccomp_t typedef with struct seccomp to match modern
    kernel style.
    
    Signed-off-by: Will Drewry <wad@chromium.org>
    Reviewed-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Acked-by: Eric Paris <eparis@redhat.com>
    
    v18: rebase
    ...
    v14: rebase/nochanges
    v13: rebase on to 88ebdda6159ffc15699f204c33feb3e431bf9bdc
    v12: rebase on to linux-next
    v8-v11: no changes
    v7: struct seccomp_struct -> struct seccomp
    v6: original inclusion in this series.
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ba60897bb447..cad15023f458 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1452,7 +1452,7 @@ struct task_struct {
 	uid_t loginuid;
 	unsigned int sessionid;
 #endif
-	seccomp_t seccomp;
+	struct seccomp seccomp;
 
 /* Thread group tracking */
    	u32 parent_exec_id;

commit 259e5e6c75a910f3b5e656151dc602f53f9d7548
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Apr 12 16:47:50 2012 -0500

    Add PR_{GET,SET}_NO_NEW_PRIVS to prevent execve from granting privs
    
    With this change, calling
      prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0)
    disables privilege granting operations at execve-time.  For example, a
    process will not be able to execute a setuid binary to change their uid
    or gid if this bit is set.  The same is true for file capabilities.
    
    Additionally, LSM_UNSAFE_NO_NEW_PRIVS is defined to ensure that
    LSMs respect the requested behavior.
    
    To determine if the NO_NEW_PRIVS bit is set, a task may call
      prctl(PR_GET_NO_NEW_PRIVS, 0, 0, 0, 0);
    It returns 1 if set and 0 if it is not set. If any of the arguments are
    non-zero, it will return -1 and set errno to -EINVAL.
    (PR_SET_NO_NEW_PRIVS behaves similarly.)
    
    This functionality is desired for the proposed seccomp filter patch
    series.  By using PR_SET_NO_NEW_PRIVS, it allows a task to modify the
    system call behavior for itself and its child tasks without being
    able to impact the behavior of a more privileged task.
    
    Another potential use is making certain privileged operations
    unprivileged.  For example, chroot may be considered "safe" if it cannot
    affect privileged tasks.
    
    Note, this patch causes execve to fail when PR_SET_NO_NEW_PRIVS is
    set and AppArmor is in use.  It is fixed in a subsequent patch.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Will Drewry <wad@chromium.org>
    Acked-by: Eric Paris <eparis@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    
    v18: updated change desc
    v17: using new define values as per 3.4
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81a173c0897d..ba60897bb447 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1341,6 +1341,8 @@ struct task_struct {
 				 * execve */
 	unsigned in_iowait:1;
 
+	/* task may not gain privileges */
+	unsigned no_new_privs:1;
 
 	/* Revert to default priority/policy when forking */
 	unsigned sched_reset_on_fork:1;

commit 7b44ab978b77a91b327058a0f4db7e6fcdb90b92
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Nov 16 23:20:58 2011 -0800

    userns: Disassociate user_struct from the user_namespace.
    
    Modify alloc_uid to take a kuid and make the user hash table global.
    Stop holding a reference to the user namespace in struct user_struct.
    
    This simplifies the code and makes the per user accounting not
    care about which user namespace a uid happens to appear in.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6867ae9bc8a0..5fdc1ebbcbc4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -90,6 +90,7 @@ struct sched_param {
 #include <linux/latencytop.h>
 #include <linux/cred.h>
 #include <linux/llist.h>
+#include <linux/uidgid.h>
 
 #include <asm/processor.h>
 
@@ -728,8 +729,7 @@ struct user_struct {
 
 	/* Hash table maintenance information */
 	struct hlist_node uidhash_node;
-	uid_t uid;
-	struct user_namespace *_user_ns; /* Don't use will be removed soon */
+	kuid_t uid;
 
 #ifdef CONFIG_PERF_EVENTS
 	atomic_long_t locked_vm;
@@ -738,7 +738,7 @@ struct user_struct {
 
 extern int uids_sysfs_init(void);
 
-extern struct user_struct *find_user(uid_t);
+extern struct user_struct *find_user(kuid_t);
 
 extern struct user_struct root_user;
 #define INIT_USER (&root_user)
@@ -2177,7 +2177,7 @@ extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 extern void __set_special_pids(struct pid *pid);
 
 /* per-UID process charging. */
-extern struct user_struct * alloc_uid(struct user_namespace *, uid_t);
+extern struct user_struct * alloc_uid(kuid_t);
 static inline struct user_struct *get_uid(struct user_struct *u)
 {
 	atomic_inc(&u->__count);

commit d0bd6594e286bd6145e04e19e8d3fa2e902cb800
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Nov 16 23:20:58 2011 -0800

    userns: Deprecate and rename the user_namespace reference in the user_struct
    
    With a user_ns reference in struct cred the only user of the user namespace
    reference in struct user_struct is to keep the uid hash table alive.
    
    The user_namespace reference in struct user_struct will be going away soon, and
    I have removed all of the references.  Rename the field from user_ns to _user_ns
    so that the compiler can verify nothing follows the user struct to the user
    namespace anymore.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 720ce8d98a7d..6867ae9bc8a0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -729,7 +729,7 @@ struct user_struct {
 	/* Hash table maintenance information */
 	struct hlist_node uidhash_node;
 	uid_t uid;
-	struct user_namespace *user_ns;
+	struct user_namespace *_user_ns; /* Don't use will be removed soon */
 
 #ifdef CONFIG_PERF_EVENTS
 	atomic_long_t locked_vm;

commit 57a39aa3e3ca00e371cec37be4f7c2e950eb1f1f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Nov 16 22:06:16 2011 -0800

    userns: Kill bogus declaration of function release_uids
    
    There is no release_uids function remove the declaration from sched.h
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81a173c0897d..720ce8d98a7d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2184,7 +2184,6 @@ static inline struct user_struct *get_uid(struct user_struct *u)
 	return u;
 }
 extern void free_uid(struct user_struct *);
-extern void release_uids(struct user_namespace *ns);
 
 #include <asm/current.h>
 

commit 0195c00244dc2e9f522475868fa278c473ba7339
Merge: f21ce8f8447c 141124c02059
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:58:21 2012 -0700

    Merge tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system
    
    Pull "Disintegrate and delete asm/system.h" from David Howells:
     "Here are a bunch of patches to disintegrate asm/system.h into a set of
      separate bits to relieve the problem of circular inclusion
      dependencies.
    
      I've built all the working defconfigs from all the arches that I can
      and made sure that they don't break.
    
      The reason for these patches is that I recently encountered a circular
      dependency problem that came about when I produced some patches to
      optimise get_order() by rewriting it to use ilog2().
    
      This uses bitops - and on the SH arch asm/bitops.h drags in
      asm-generic/get_order.h by a circuituous route involving asm/system.h.
    
      The main difficulty seems to be asm/system.h.  It holds a number of
      low level bits with no/few dependencies that are commonly used (eg.
      memory barriers) and a number of bits with more dependencies that
      aren't used in many places (eg.  switch_to()).
    
      These patches break asm/system.h up into the following core pieces:
    
        (1) asm/barrier.h
    
            Move memory barriers here.  This already done for MIPS and Alpha.
    
        (2) asm/switch_to.h
    
            Move switch_to() and related stuff here.
    
        (3) asm/exec.h
    
            Move arch_align_stack() here.  Other process execution related bits
            could perhaps go here from asm/processor.h.
    
        (4) asm/cmpxchg.h
    
            Move xchg() and cmpxchg() here as they're full word atomic ops and
            frequently used by atomic_xchg() and atomic_cmpxchg().
    
        (5) asm/bug.h
    
            Move die() and related bits.
    
        (6) asm/auxvec.h
    
            Move AT_VECTOR_SIZE_ARCH here.
    
      Other arch headers are created as needed on a per-arch basis."
    
    Fixed up some conflicts from other header file cleanups and moving code
    around that has happened in the meantime, so David's testing is somewhat
    weakened by that.  We'll find out anything that got broken and fix it..
    
    * tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system: (38 commits)
      Delete all instances of asm/system.h
      Remove all #inclusions of asm/system.h
      Add #includes needed to permit the removal of asm/system.h
      Move all declarations of free_initmem() to linux/mm.h
      Disintegrate asm/system.h for OpenRISC
      Split arch_align_stack() out from asm-generic/system.h
      Split the switch_to() wrapper out of asm-generic/system.h
      Move the asm-generic/system.h xchg() implementation to asm-generic/cmpxchg.h
      Create asm-generic/barrier.h
      Make asm-generic/cmpxchg.h #include asm-generic/cmpxchg-local.h
      Disintegrate asm/system.h for Xtensa
      Disintegrate asm/system.h for Unicore32 [based on ver #3, changed by gxt]
      Disintegrate asm/system.h for Tile
      Disintegrate asm/system.h for Sparc
      Disintegrate asm/system.h for SH
      Disintegrate asm/system.h for Score
      Disintegrate asm/system.h for S390
      Disintegrate asm/system.h for PowerPC
      Disintegrate asm/system.h for PA-RISC
      Disintegrate asm/system.h for MN10300
      ...

commit 9ffc93f203c18a70623f21950f1dd473c9ec48cd
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Remove all #inclusions of asm/system.h
    
    Remove all #inclusions of asm/system.h preparatory to splitting and killing
    it.  Performed with the following command:
    
    perl -p -i -e 's!^#\s*include\s*<asm/system[.]h>.*\n!!' `grep -Irl '^#\s*include\s*<asm/system[.]h>' *`
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0c147a4260a5..704464d71a96 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -63,7 +63,6 @@ struct sched_param {
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
 
-#include <asm/system.h>
 #include <asm/page.h>
 #include <asm/ptrace.h>
 #include <asm/cputime.h>

commit ebec18a6d3aa1e7d84aab16225e87fd25170ec2b
Author: Lennart Poettering <lennart@poettering.net>
Date:   Fri Mar 23 15:01:54 2012 -0700

    prctl: add PR_{SET,GET}_CHILD_SUBREAPER to allow simple process supervision
    
    Userspace service managers/supervisors need to track their started
    services.  Many services daemonize by double-forking and get implicitly
    re-parented to PID 1.  The service manager will no longer be able to
    receive the SIGCHLD signals for them, and is no longer in charge of
    reaping the children with wait().  All information about the children is
    lost at the moment PID 1 cleans up the re-parented processes.
    
    With this prctl, a service manager process can mark itself as a sort of
    'sub-init', able to stay as the parent for all orphaned processes
    created by the started services.  All SIGCHLD signals will be delivered
    to the service manager.
    
    Receiving SIGCHLD and doing wait() is in cases of a service-manager much
    preferred over any possible asynchronous notification about specific
    PIDs, because the service manager has full access to the child process
    data in /proc and the PID can not be re-used until the wait(), the
    service-manager itself is in charge of, has happened.
    
    As a side effect, the relevant parent PID information does not get lost
    by a double-fork, which results in a more elaborate process tree and
    'ps' output:
    
    before:
      # ps afx
      253 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      294 ?        Sl     0:00 /usr/libexec/polkit-1/polkitd
      328 ?        S      0:00 /usr/sbin/modem-manager
      608 ?        Sl     0:00 /usr/libexec/colord
      658 ?        Sl     0:00 /usr/libexec/upowerd
      819 ?        Sl     0:00 /usr/libexec/imsettings-daemon
      916 ?        Sl     0:00 /usr/libexec/udisks-daemon
      917 ?        S      0:00  \_ udisks-daemon: not polling any devices
    
    after:
      # ps afx
      294 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      426 ?        Sl     0:00  \_ /usr/libexec/polkit-1/polkitd
      449 ?        S      0:00  \_ /usr/sbin/modem-manager
      635 ?        Sl     0:00  \_ /usr/libexec/colord
      705 ?        Sl     0:00  \_ /usr/libexec/upowerd
      959 ?        Sl     0:00  \_ /usr/libexec/udisks-daemon
      960 ?        S      0:00  |   \_ udisks-daemon: not polling any devices
      977 ?        Sl     0:00  \_ /usr/libexec/packagekitd
    
    This prctl is orthogonal to PID namespaces.  PID namespaces are isolated
    from each other, while a service management process usually requires the
    services to live in the same namespace, to be able to talk to each
    other.
    
    Users of this will be the systemd per-user instance, which provides
    init-like functionality for the user's login session and D-Bus, which
    activates bus services on-demand.  Both need init-like capabilities to
    be able to properly keep track of the services they start.
    
    Many thanks to Oleg for several rounds of review and insights.
    
    [akpm@linux-foundation.org: fix comment layout and spelling]
    [akpm@linux-foundation.org: add lengthy code comment from Oleg]
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Lennart Poettering <lennart@poettering.net>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Acked-by: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0c147a4260a5..0c3854b0d4b1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -553,6 +553,18 @@ struct signal_struct {
 	int			group_stop_count;
 	unsigned int		flags; /* see SIGNAL_* flags below */
 
+	/*
+	 * PR_SET_CHILD_SUBREAPER marks a process, like a service
+	 * manager, to re-parent orphan (double-forking) child processes
+	 * to this process instead of 'init'. The service manager is
+	 * able to receive SIGCHLD signals and is able to investigate
+	 * the process until it calls wait(). All children of this
+	 * process will inherit a flag if they should look for a
+	 * child_subreaper process at exit.
+	 */
+	unsigned int		is_child_subreaper:1;
+	unsigned int		has_child_subreaper:1;
+
 	/* POSIX.1b Interval Timers */
 	struct list_head posix_timers;
 

commit cc9a6c8776615f9c194ccf0b63a0aa5628235545
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Mar 21 16:34:11 2012 -0700

    cpuset: mm: reduce large amounts of memory barrier related damage v3
    
    Commit c0ff7453bb5c ("cpuset,mm: fix no node to alloc memory when
    changing cpuset's mems") wins a super prize for the largest number of
    memory barriers entered into fast paths for one commit.
    
    [get|put]_mems_allowed is incredibly heavy with pairs of full memory
    barriers inserted into a number of hot paths.  This was detected while
    investigating at large page allocator slowdown introduced some time
    after 2.6.32.  The largest portion of this overhead was shown by
    oprofile to be at an mfence introduced by this commit into the page
    allocator hot path.
    
    For extra style points, the commit introduced the use of yield() in an
    implementation of what looks like a spinning mutex.
    
    This patch replaces the full memory barriers on both read and write
    sides with a sequence counter with just read barriers on the fast path
    side.  This is much cheaper on some architectures, including x86.  The
    main bulk of the patch is the retry logic if the nodemask changes in a
    manner that can cause a false failure.
    
    While updating the nodemask, a check is made to see if a false failure
    is a risk.  If it is, the sequence number gets bumped and parallel
    allocators will briefly stall while the nodemask update takes place.
    
    In a page fault test microbenchmark, oprofile samples from
    __alloc_pages_nodemask went from 4.53% of all samples to 1.15%.  The
    actual results were
    
                                 3.3.0-rc3          3.3.0-rc3
                                 rc3-vanilla        nobarrier-v2r1
        Clients   1 UserTime       0.07 (  0.00%)   0.08 (-14.19%)
        Clients   2 UserTime       0.07 (  0.00%)   0.07 (  2.72%)
        Clients   4 UserTime       0.08 (  0.00%)   0.07 (  3.29%)
        Clients   1 SysTime        0.70 (  0.00%)   0.65 (  6.65%)
        Clients   2 SysTime        0.85 (  0.00%)   0.82 (  3.65%)
        Clients   4 SysTime        1.41 (  0.00%)   1.41 (  0.32%)
        Clients   1 WallTime       0.77 (  0.00%)   0.74 (  4.19%)
        Clients   2 WallTime       0.47 (  0.00%)   0.45 (  3.73%)
        Clients   4 WallTime       0.38 (  0.00%)   0.37 (  1.58%)
        Clients   1 Flt/sec/cpu  497620.28 (  0.00%) 520294.53 (  4.56%)
        Clients   2 Flt/sec/cpu  414639.05 (  0.00%) 429882.01 (  3.68%)
        Clients   4 Flt/sec/cpu  257959.16 (  0.00%) 258761.48 (  0.31%)
        Clients   1 Flt/sec      495161.39 (  0.00%) 517292.87 (  4.47%)
        Clients   2 Flt/sec      820325.95 (  0.00%) 850289.77 (  3.65%)
        Clients   4 Flt/sec      1020068.93 (  0.00%) 1022674.06 (  0.26%)
        MMTests Statistics: duration
        Sys Time Running Test (seconds)             135.68    132.17
        User+Sys Time Running Test (seconds)         164.2    160.13
        Total Elapsed Time (seconds)                123.46    120.87
    
    The overall improvement is small but the System CPU time is much
    improved and roughly in correlation to what oprofile reported (these
    performance figures are without profiling so skew is expected).  The
    actual number of page faults is noticeably improved.
    
    For benchmarks like kernel builds, the overall benefit is marginal but
    the system CPU time is slightly reduced.
    
    To test the actual bug the commit fixed I opened two terminals.  The
    first ran within a cpuset and continually ran a small program that
    faulted 100M of anonymous data.  In a second window, the nodemask of the
    cpuset was continually randomised in a loop.
    
    Without the commit, the program would fail every so often (usually
    within 10 seconds) and obviously with the commit everything worked fine.
    With this patch applied, it also worked fine so the fix should be
    functionally equivalent.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e074e1e54f85..0c147a4260a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1514,7 +1514,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_CPUSETS
 	nodemask_t mems_allowed;	/* Protected by alloc_lock */
-	int mems_allowed_change_disable;
+	seqcount_t mems_allowed_seq;	/* Seqence no to catch updates */
 	int cpuset_mem_spread_rotor;
 	int cpuset_slab_spread_rotor;
 #endif

commit 2ba68940c893c8f0bfc8573c041254251bb6aeab
Merge: 9c2b957db177 600e14588280
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:31:44 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes for v3.4 from Ingo Molnar
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      printk: Make it compile with !CONFIG_PRINTK
      sched/x86: Fix overflow in cyc2ns_offset
      sched: Fix nohz load accounting -- again!
      sched: Update yield() docs
      printk/sched: Introduce special printk_sched() for those awkward moments
      sched/nohz: Correctly initialize 'next_balance' in 'nohz' idle balancer
      sched: Cleanup cpu_active madness
      sched: Fix load-balance wreckage
      sched: Clean up parameter passing of proc_sched_autogroup_set_nice()
      sched: Ditch per cgroup task lists for load-balancing
      sched: Rename load-balancing fields
      sched: Move load-balancing arguments into helper struct
      sched/rt: Do not submit new work when PI-blocked
      sched/rt: Prevent idle task boosting
      sched/wait: Add __wake_up_all_locked() API
      sched/rt: Document scheduler related skip-resched-check sites
      sched/rt: Use schedule_preempt_disabled()
      sched/rt: Add schedule_preempt_disabled()
      sched/rt: Do not throttle when PI boosting
      sched/rt: Keep period timer ticking when rt throttling is active
      ...

commit 0bbfcaff9b2a69c71a95e6902253487ab30cb498
Merge: 5928a2b60cfd e04268b0effc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:28:56 2012 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq/core changes for v3.4 from Ingo Molnar
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      genirq: Remove paranoid warnons and bogus fixups
      genirq: Flush the irq thread on synchronization
      genirq: Get rid of unnecessary IRQTF_DIED flag
      genirq: No need to check IRQTF_DIED before stopping a thread handler
      genirq: Get rid of unnecessary irqaction field in task_struct
      genirq: Fix incorrect check for forced IRQ thread handler
      softirq: Reduce invoke_softirq() code duplication
      genirq: Fix long-term regression in genirq irq_set_irq_type() handling
      x86-32/irq: Don't switch to irq stack for a user-mode irq

commit 5928a2b60cfdbad730f93696acab142d0b607280
Merge: 5ed59af85077 bdd4431c8d07
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 19 17:12:34 2012 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes for v3.4 from Ingo Molnar.  The major features of this
    series are:
    
     - making RCU more aggressive about entering dyntick-idle mode in order
       to improve energy efficiency
    
     - converting a few more call_rcu()s to kfree_rcu()s
    
     - applying a number of rcutree fixes and cleanups to rcutiny
    
     - removing CONFIG_SMP #ifdefs from treercu
    
     - allowing RCU CPU stall times to be set via sysfs
    
     - adding CPU-stall capability to rcutorture
    
     - adding more RCU-abuse diagnostics
    
     - updating documentation
    
     - fixing yet more issues located by the still-ongoing top-to-bottom
       inspection of RCU, this time with a special focus on the CPU-hotplug
       code path.
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      rcu: Stop spurious warnings from synchronize_sched_expedited
      rcu: Hold off RCU_FAST_NO_HZ after timer posted
      rcu: Eliminate softirq-mediated RCU_FAST_NO_HZ idle-entry loop
      rcu: Add RCU_NONIDLE() for idle-loop RCU read-side critical sections
      rcu: Allow nesting of rcu_idle_enter() and rcu_idle_exit()
      rcu: Remove redundant check for rcu_head misalignment
      PTR_ERR should be called before its argument is cleared.
      rcu: Convert WARN_ON_ONCE() in rcu_lock_acquire() to lockdep
      rcu: Trace only after NULL-pointer check
      rcu: Call out dangers of expedited RCU primitives
      rcu: Rework detection of use of RCU by offline CPUs
      lockdep: Add CPU-idle/offline warning to lockdep-RCU splat
      rcu: No interrupt disabling for rcu_prepare_for_idle()
      rcu: Move synchronize_sched_expedited() to rcutree.c
      rcu: Check for illegal use of RCU from offlined CPUs
      rcu: Update stall-warning documentation
      rcu: Add CPU-stall capability to rcutorture
      rcu: Make documentation give more realistic rcutorture duration
      rcutorture: Permit holding off CPU-hotplug operations during boot
      rcu: Print scheduling-clock information on RCU CPU stall-warning messages
      ...

commit 0326f5a94ddea33fa331b2519f4172f4fb387baa
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Mar 13 23:30:11 2012 +0530

    uprobes/core: Handle breakpoint and singlestep exceptions
    
    Uprobes uses exception notifiers to get to know if a thread hit
    a breakpoint or a singlestep exception.
    
    When a thread hits a uprobe or is singlestepping post a uprobe
    hit, the uprobe exception notifier sets its TIF_UPROBE bit,
    which will then be checked on its return to userspace path
    (do_notify_resume() ->uprobe_notify_resume()), where the
    consumers handlers are run (in task context) based on the
    defined filters.
    
    Uprobe hits are thread specific and hence we need to maintain
    information about if a task hit a uprobe, what uprobe was hit,
    the slot where the original instruction was copied for xol so
    that it can be singlestepped with appropriate fixups.
    
    In some cases, special care is needed for instructions that are
    executed out of line (xol). These are architecture specific
    artefacts, such as handling RIP relative instructions on x86_64.
    
    Since the instruction at which the uprobe was inserted is
    executed out of line, architecture specific fixups are added so
    that the thread continues normal execution in the presence of a
    uprobe.
    
    Postpone the signals until we execute the probed insn.
    post_xol() path does a recalc_sigpending() before return to
    user-mode, this ensures the signal can't be lost.
    
    Uprobes relies on DIE_DEBUG notification to notify if a
    singlestep is complete.
    
    Adds x86 specific uprobe exception notifiers and appropriate
    hooks needed to determine a uprobe hit and subsequent post
    processing.
    
    Add requisite x86 fixups for xol for uprobes. Specific cases
    needing fixups include relative jumps (x86_64), calls, etc.
    
    Where possible, we check and skip singlestepping the
    breakpointed instructions. For now we skip single byte as well
    as few multibyte nop instructions. However this can be extended
    to other instructions too.
    
    Credits to Oleg Nesterov for suggestions/patches related to
    signal, breakpoint, singlestep handling code.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120313180011.29771.89027.sendpatchset@srdronam.in.ibm.com
    [ Performed various cleanliness edits ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d379a6bfd88..8379e3771690 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1590,6 +1590,10 @@ struct task_struct {
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;
 #endif
+#ifdef CONFIG_UPROBES
+	struct uprobe_task *utask;
+	int uprobe_srcu_id;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit df8d291f28aa1e8437c8f7816328a6516379c71b
Merge: 5234ffb9f74c fde7d9049e55
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 13 16:34:48 2012 +0100

    Merge branch 'linus' into irq/core
    
    Reason: Get upstream fixes integrated before further modifications.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 47258cf3c4aa5d56e678bafe0dd0d03ddd980b88
Merge: c308b56b5398 fde7d9049e55
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 13 16:26:52 2012 +0100

    Merge tag 'v3.3-rc7' into sched/core
    
    Merge reason: merge back final fixes, prepare for the merge window.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4bcdf1d0b652bc33d52f2322b77463e4dc58abf8
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Fri Mar 9 14:59:26 2012 +0100

    genirq: Get rid of unnecessary irqaction field in task_struct
    
    When a new thread handler is created, an irqaction is passed to it as
    data. Not only that irqaction is stored in task_struct by the handler
    for later use, but also a structure associated with the kernel thread
    keeps this value as long as the thread exists.
    
    This fix kicks irqaction out off task_struct. Yes, I introduce new bit
    field. But it allows not only to eliminate the duplicate, but also
    shortens size of task_struct.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Link: http://lkml.kernel.org/r/20120309135925.GB2114@dhcp-26-207.brq.redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d379a6bfd88..07f537a371ce 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1319,6 +1319,11 @@ struct task_struct {
 	unsigned sched_reset_on_fork:1;
 	unsigned sched_contributes_to_load:1;
 
+#ifdef CONFIG_GENERIC_HARDIRQS
+	/* IRQ handler threads */
+	unsigned irq_thread:1;
+#endif
+
 	pid_t pid;
 	pid_t tgid;
 
@@ -1427,11 +1432,6 @@ struct task_struct {
  * mempolicy */
 	spinlock_t alloc_lock;
 
-#ifdef CONFIG_GENERIC_HARDIRQS
-	/* IRQ handler threads */
-	struct irqaction *irqaction;
-#endif
-
 	/* Protection of the PI data structures: */
 	raw_spinlock_t pi_lock;
 

commit 6e27f63edbd7ab893258e16500171dd1270a1369
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:14 2012 -0800

    vfork: kill PF_STARTING
    
    Previously it was (ab)used by utrace.  Then it was wrongly used by the
    scheduler code.
    
    Currently it is not used, kill it before it finds the new erroneous user.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11fcafaf4ae4..0657368bd78f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1777,7 +1777,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * Per process flags
  */
-#define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */

commit 57b59c4a1400fa6c34764eab2e35a8762dc05a09
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    coredump_wait: don't call complete_vfork_done()
    
    Now that CLONE_VFORK is killable, coredump_wait() no longer needs
    complete_vfork_done().  zap_threads() should find and kill all tasks with
    the same ->mm, this includes our parent if ->vfork_done is set.
    
    mm_release() becomes the only caller, unexport complete_vfork_done().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b6467711f12e..11fcafaf4ae4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2291,7 +2291,6 @@ extern int do_execve(const char *,
 		     const char __user * const __user *,
 		     const char __user * const __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
-extern void complete_vfork_done(struct task_struct *tsk);
 struct task_struct *fork_idle(int);
 
 extern void set_task_comm(struct task_struct *tsk, char *from);

commit d68b46fe16ad59b3a5f51ec73daaa5dc06753798
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    vfork: make it killable
    
    Make vfork() killable.
    
    Change do_fork(CLONE_VFORK) to do wait_for_completion_killable().  If it
    fails we do not return to the user-mode and never touch the memory shared
    with our child.
    
    However, in this case we should clear child->vfork_done before return, we
    use task_lock() in do_fork()->wait_for_vfork_done() and
    complete_vfork_done() to serialize with each other.
    
    Note: now that we use task_lock() we don't really need completion, we
    could turn task->vfork_done into "task_struct *wake_up_me" but this needs
    some complications.
    
    NOTE: this and the next patches do not affect in-kernel users of
    CLONE_VFORK, kernel threads run with all signals ignored including
    SIGKILL/SIGSTOP.
    
    However this is obviously the user-visible change.  Not only a fatal
    signal can kill the vforking parent, a sub-thread can do execve or
    exit_group() and kill the thread sleeping in vfork().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1b25a37f2aee..b6467711f12e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2372,7 +2372,7 @@ static inline int thread_group_empty(struct task_struct *p)
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
  * pins the final release of task.io_context.  Also protects ->cpuset and
- * ->cgroup.subsys[].
+ * ->cgroup.subsys[]. And ->vfork_done.
  *
  * Nests both inside and outside of read_lock(&tasklist_lock).
  * It must not be nested with write_lock_irq(&tasklist_lock),

commit c415c3b47ea2754659d915cca387a20999044163
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    vfork: introduce complete_vfork_done()
    
    No functional changes.
    
    Move the clear-and-complete-vfork_done code into the new trivial helper,
    complete_vfork_done().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d379a6bfd88..1b25a37f2aee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2291,6 +2291,7 @@ extern int do_execve(const char *,
 		     const char __user * const __user *,
 		     const char __user * const __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
+extern void complete_vfork_done(struct task_struct *tsk);
 struct task_struct *fork_idle(int);
 
 extern void set_task_comm(struct task_struct *tsk, char *from);

commit 2e5b5b3a1b7768c89fbfeca18e75f8ee377e924c
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Thu Feb 23 17:41:27 2012 +0900

    sched: Clean up parameter passing of proc_sched_autogroup_set_nice()
    
    Pass nice as a value to proc_sched_autogroup_set_nice().
    
    No side effect is expected, and the variable err will be overwritten with
    the return value.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F45FBB7.5090607@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c628a9151437..c298fb9cf5ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2065,7 +2065,7 @@ extern void sched_autogroup_fork(struct signal_struct *sig);
 extern void sched_autogroup_exit(struct signal_struct *sig);
 #ifdef CONFIG_PROC_FS
 extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
-extern int proc_sched_autogroup_set_nice(struct task_struct *p, int *nice);
+extern int proc_sched_autogroup_set_nice(struct task_struct *p, int nice);
 #endif
 #else
 static inline void sched_autogroup_create_attach(struct task_struct *p) { }

commit 3c7d51843b03a6839e9ec7cda724e54d2319a63a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 20:46:52 2011 +0200

    sched/rt: Do not submit new work when PI-blocked
    
    When we are PI-blocked then we want to get things done ASAP.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-vw8et3445km5b8mpihf4trae@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 03dd224d0667..c628a9151437 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2082,12 +2082,20 @@ extern unsigned int sysctl_sched_cfs_bandwidth_slice;
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
+static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
+{
+	return tsk->pi_blocked_on != NULL;
+}
 #else
 static inline int rt_mutex_getprio(struct task_struct *p)
 {
 	return p->normal_prio;
 }
 # define rt_mutex_adjust_pi(p)		do { } while (0)
+static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
+{
+	return false;
+}
 #endif
 
 extern bool yield_to(struct task_struct *p, bool preempt);

commit c5491ea779793f977d282754db478157cc409d82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 21 12:09:35 2011 +0100

    sched/rt: Add schedule_preempt_disabled()
    
    Add helper to get rid of the ever repeating:
    
        preempt_enable_no_resched();
        schedule();
        preempt_disable();
    
    patterns.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-wxx7btox7coby6ifv5vzhzgp@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1a6398424fab..03dd224d0667 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -361,6 +361,7 @@ extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
+extern void schedule_preempt_disabled(void);
 extern int mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner);
 
 struct nsproxy;

commit 7e4d960993331e92567f0180e45322a93e6780ba
Merge: de5bdff7a72a 164974a8f2a4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 1 10:26:41 2012 +0100

    Merge branch 'linus' into sched/core
    
    Merge reason: we'll queue up dependent patches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit de5bdff7a72acc281219be2b8edeeca1fd81c542
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Thu Feb 16 14:52:21 2012 +0900

    sched: Make initial SCHED_RR timeslace DEF_TIMESLICE
    
    Current the initial SCHED_RR timeslice of init_task is HZ, which means
    1s, and is not same as the default SCHED_RR timeslice DEF_TIMESLICE.
    
    Change that initial timeslice to the DEF_TIMESLICE.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    [ s/DEF_TIMESLICE/RR_TIMESLICE/g ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F3C9995.3010800@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5dba2ad52431..eb5de466f099 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1234,6 +1234,12 @@ struct sched_rt_entity {
 #endif
 };
 
+/*
+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).
+ * Timeslices get refilled after they expire.
+ */
+#define RR_TIMESLICE		(100 * HZ / 1000)
+
 struct rcu_node;
 
 enum perf_event_task_context {

commit 1aa03f1188f7b0b85df2de602b33ee7b6fab8e00
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Jan 11 17:25:17 2012 -0800

    rcu: Simplify unboosting checks
    
    This is a port of commit #82e78d80 from TREE_PREEMPT_RCU to
    TINY_PREEMPT_RCU.
    
    This commit uses the fact that current->rcu_boost_mutex is set
    any time that the RCU_READ_UNLOCK_BOOSTED flag is set in the
    current->rcu_read_unlock_special bitmask.  This allows tests of
    the bit to be changed to tests of the pointer, which in turn allows
    the RCU_READ_UNLOCK_BOOSTED flag to be eliminated.
    
    Please note that the check of current->rcu_read_unlock_special need not
    change because any time that RCU_READ_UNLOCK_BOOSTED was set, so was
    RCU_READ_UNLOCK_BLOCKED.  Therefore, __rcu_read_unlock() can continue
    testing current->rcu_read_unlock_special for non-zero, as before.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7d379a6bfd88..e692abaf915a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1864,8 +1864,7 @@ extern void task_clear_jobctl_pending(struct task_struct *task,
 #ifdef CONFIG_PREEMPT_RCU
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
-#define RCU_READ_UNLOCK_BOOSTED (1 << 1) /* boosted while in RCU read-side. */
-#define RCU_READ_UNLOCK_NEED_QS (1 << 2) /* RCU core needs CPU response. */
+#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
 
 static inline void rcu_copy_process(struct task_struct *p)
 {

commit 9388dc3047a88bedfd867e9ba3e1980c815ac524
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu Feb 9 20:45:19 2012 +0400

    sched: Turn lock_task_sighand() into a static inline
    
    It appears that sparse tool understands static inline functions
    for context balance checking, so let's turn the macros into an
    inline func.
    
    This makes the code a little bit more robust.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Arve <arve@android.com>
    Cc: San Mehat <san@google.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: kernel-team@android.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120209164519.GA10266@oksana.dev.rtsoft.ru
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 92313a3f6f77..5dba2ad52431 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2393,12 +2393,15 @@ static inline void task_unlock(struct task_struct *p)
 extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 							unsigned long *flags);
 
-#define lock_task_sighand(tsk, flags)					\
-({	struct sighand_struct *__ss;					\
-	__cond_lock(&(tsk)->sighand->siglock,				\
-		    (__ss = __lock_task_sighand(tsk, flags)));		\
-	__ss;								\
-})									\
+static inline struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
+						       unsigned long *flags)
+{
+	struct sighand_struct *ret;
+
+	ret = __lock_task_sighand(tsk, flags);
+	(void)__cond_lock(&tsk->sighand->siglock, ret);
+	return ret;
+}
 
 static inline void unlock_task_sighand(struct task_struct *tsk,
 						unsigned long *flags)

commit 8cdb878dcb359fd1137e9abdee9322f5e9bcfdf8
Author: Christopher Yeoh <cyeoh@au1.ibm.com>
Date:   Thu Feb 2 11:34:09 2012 +1030

    Fix race in process_vm_rw_core
    
    This fixes the race in process_vm_core found by Oleg (see
    
      http://article.gmane.org/gmane.linux.kernel/1235667/
    
    for details).
    
    This has been updated since I last sent it as the creation of the new
    mm_access() function did almost exactly the same thing as parts of the
    previous version of this patch did.
    
    In order to use mm_access() even when /proc isn't enabled, we move it to
    kernel/fork.c where other related process mm access functions already
    are.
    
    Signed-off-by: Chris Yeoh <yeohc@au1.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2234985a5e65..7d379a6bfd88 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2259,6 +2259,12 @@ static inline void mmdrop(struct mm_struct * mm)
 extern void mmput(struct mm_struct *);
 /* Grab a reference to a task's mm, if it is not already going away */
 extern struct mm_struct *get_task_mm(struct task_struct *task);
+/*
+ * Grab a reference to a task's mm, if it is not already going away
+ * and ptrace_may_access with the mode parameter passed to it
+ * succeeds.
+ */
+extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
 /* Allocate a new mm structure and copy contents from tsk->mm */

commit 4ec4412e1e91f44a3dcb97b6c9172a13fc78bac9
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Dec 12 20:21:08 2011 +0100

    sched: Ensure cpu_power periodic update
    
    With a lot of small tasks, the softirq sched is nearly never called
    when no_hz is enabled. In this case load_balance() is mainly called
    with the newly_idle mode which doesn't update the cpu_power.
    
    Add a next_update field which ensure a maximum update period when
    there is short activity.
    
    Having stale cpu_power information can skew the load-balancing
    decisions, this is cured by the guaranteed update.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1323717668-2143-1-git-send-email-vincent.guittot@linaro.org

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0e1959568836..92313a3f6f77 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -905,6 +905,7 @@ struct sched_group_power {
 	 * single CPU.
 	 */
 	unsigned int power, power_orig;
+	unsigned long next_update;
 	/*
 	 * Number of busy cpus in this group.
 	 */

commit 39be350127ec60a078edffe5b4915dafba4ba514
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 26 12:44:34 2012 +0100

    sched, block: Unify cache detection
    
    The block layer has some code trying to determine if two CPUs share a
    cache, the scheduler has a similar function. Expose the function used
    by the scheduler and make the block layer use it, thereby removing the
    block layers usage of CONFIG_SCHED* and topology bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Link: http://lkml.kernel.org/r/1327579450.2446.95.camel@twins

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 513f52459872..0e1959568836 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1052,6 +1052,8 @@ static inline int test_sd_parent(struct sched_domain *sd, int flag)
 unsigned long default_scale_freq_power(struct sched_domain *sd, int cpu);
 unsigned long default_scale_smt_power(struct sched_domain *sd, int cpu);
 
+bool cpus_share_cache(int this_cpu, int that_cpu);
+
 #else /* CONFIG_SMP */
 
 struct sched_domain_attr;
@@ -1061,6 +1063,12 @@ partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 			struct sched_domain_attr *dattr_new)
 {
 }
+
+static inline bool cpus_share_cache(int this_cpu, int that_cpu)
+{
+	return true;
+}
+
 #endif	/* !CONFIG_SMP */
 
 

commit 2437dcbf555bff04e4ee8b8dba4587f946c1cd3d
Merge: 0dbfe8ddaaab b64b223aed5f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 26 12:45:41 2012 -0800

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      rcu: Add missing __cpuinit annotation in rcutorture code
      sched: Add "const" to is_idle_task() parameter
      rcu: Make rcutorture bool parameters really bool (core code)
      memblock: Fix alloc failure due to dumb underflow protection in memblock_find_in_range_node()

commit fa757281a08799fd6c0f7ec6f111d1cd66afc97b
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Sat Jan 21 11:03:13 2012 -0800

    kernel-doc: fix kernel-doc warnings in sched
    
    Fix new kernel-doc notation warnings:
    
    Warning(include/linux/sched.h:2094): No description found for parameter 'p'
    Warning(include/linux/sched.h:2094): Excess function parameter 'tsk' description in 'is_idle_task'
    Warning(kernel/sched/cpupri.c:139): No description found for parameter 'newpri'
    Warning(kernel/sched/cpupri.c:139): Excess function parameter 'pri' description in 'cpupri_set'
    Warning(kernel/sched/cpupri.c:208): Excess function parameter 'bootmem' description in 'cpupri_init'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Cc:     Ingo Molnar <mingo@elte.hu>
    Cc:     Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4032ec1cf836..513f52459872 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2088,7 +2088,7 @@ extern int sched_setscheduler_nocheck(struct task_struct *, int,
 extern struct task_struct *idle_task(int cpu);
 /**
  * is_idle_task - is the specified task an idle task?
- * @tsk: the task in question.
+ * @p: the task in question.
  */
 static inline bool is_idle_task(struct task_struct *p)
 {

commit b64b223aed5f8aeeb6c046f1b050a8f976b87de0
Merge: 5d53cb27d849 4410030646be
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 17 09:44:17 2012 +0100

    Merge branch 'rcu/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/urgent

commit 7061ca3b6c99fc78115560b9a10227c8c5fafc45
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 20 08:20:46 2011 -0800

    sched: Add "const" to is_idle_task() parameter
    
    This patch fixes a build warning in -next due to a const pointer being
    passed to is_idle_task().  Because is_idle_task() does not modify anything,
    this commit adds the "const" to is_idle_task()'s argument declaration.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4a7e4d333a27..56fa25a5b1eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2074,7 +2074,7 @@ extern struct task_struct *idle_task(int cpu);
  * is_idle_task - is the specified task an idle task?
  * @tsk: the task in question.
  */
-static inline bool is_idle_task(struct task_struct *p)
+static inline bool is_idle_task(const struct task_struct *p)
 {
 	return p->pid == 0;
 }

commit 9402c95f34a66e81eba473a2f7267bbae5a1dee2
Author: Joe Perches <joe@perches.com>
Date:   Thu Jan 12 17:17:17 2012 -0800

    treewide: remove useless NORET_TYPE macro and uses
    
    It's a very old and now unused prototype marking so just delete it.
    
    Neaten panic pointer argument style to keep checkpatch quiet.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 21cd0303af51..4032ec1cf836 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2275,7 +2275,7 @@ extern void __cleanup_sighand(struct sighand_struct *);
 extern void exit_itimers(struct signal_struct *);
 extern void flush_itimer_signals(void);
 
-extern NORET_TYPE void do_group_exit(int);
+extern void do_group_exit(int);
 
 extern void daemonize(const char *, ...);
 extern int allow_signal(int);

commit 001a541ea9163ace5e8243ee0e907ad80a4c0ec2
Merge: 40ba587923ae bc31b86a5923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 10 16:59:59 2012 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
      writeback: balanced_rate cannot exceed write bandwidth
      writeback: do strict bdi dirty_exceeded
      writeback: avoid tiny dirty poll intervals
      writeback: max, min and target dirty pause time
      writeback: dirty ratelimit - think time compensation
      btrfs: fix dirtied pages accounting on sub-page writes
      writeback: fix dirtied pages accounting on redirty
      writeback: fix dirtied pages accounting on sub-page writes
      writeback: charge leaked page dirties to active tasks
      writeback: Include all dirty inodes in background writeback

commit db0c2bf69aa095d4a6de7b1145f29fe9a7c0f6a3
Merge: ac69e0928054 0d19ea866562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 9 12:59:24 2012 -0800

    Merge branch 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    * 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (21 commits)
      cgroup: fix to allow mounting a hierarchy by name
      cgroup: move assignement out of condition in cgroup_attach_proc()
      cgroup: Remove task_lock() from cgroup_post_fork()
      cgroup: add sparse annotation to cgroup_iter_start() and cgroup_iter_end()
      cgroup: mark cgroup_rmdir_waitq and cgroup_attach_proc() as static
      cgroup: only need to check oldcgrp==newgrp once
      cgroup: remove redundant get/put of task struct
      cgroup: remove redundant get/put of old css_set from migrate
      cgroup: Remove unnecessary task_lock before fetching css_set on migration
      cgroup: Drop task_lock(parent) on cgroup_fork()
      cgroups: remove redundant get/put of css_set from css_set_check_fetched()
      resource cgroups: remove bogus cast
      cgroup: kill subsys->can_attach_task(), pre_attach() and attach_task()
      cgroup, cpuset: don't use ss->pre_attach()
      cgroup: don't use subsys->can_attach_task() or ->attach_task()
      cgroup: introduce cgroup_taskset and use it in subsys->can_attach(), cancel_attach() and attach()
      cgroup: improve old cgroup handling in cgroup_attach_proc()
      cgroup: always lock threadgroup during migration
      threadgroup: extend threadgroup_lock() to cover exit and exec
      threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
      ...
    
    Fix up conflict in kernel/cgroup.c due to commit e0197aae59e5: "cgroups:
    fix a css_set not found bug in cgroup_attach_proc" that already
    mentioned that the bug is fixed (differently) in Tejun's cgroup
    patchset. This one, in other words.

commit eb59c505f8a5906ad2e053d14fab50eb8574fd6f
Merge: 1619ed8f6095 c233523b3d39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 8 13:10:57 2012 -0800

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (76 commits)
      PM / Hibernate: Implement compat_ioctl for /dev/snapshot
      PM / Freezer: fix return value of freezable_schedule_timeout_killable()
      PM / shmobile: Allow the A4R domain to be turned off at run time
      PM / input / touchscreen: Make st1232 use device PM QoS constraints
      PM / QoS: Introduce dev_pm_qos_add_ancestor_request()
      PM / shmobile: Remove the stay_on flag from SH7372's PM domains
      PM / shmobile: Don't include SH7372's INTCS in syscore suspend/resume
      PM / shmobile: Add support for the sh7372 A4S power domain / sleep mode
      PM: Drop generic_subsys_pm_ops
      PM / Sleep: Remove forward-only callbacks from AMBA bus type
      PM / Sleep: Remove forward-only callbacks from platform bus type
      PM: Run the driver callback directly if the subsystem one is not there
      PM / Sleep: Make pm_op() and pm_noirq_op() return callback pointers
      PM/Devfreq: Add Exynos4-bus device DVFS driver for Exynos4210/4212/4412.
      PM / Sleep: Merge internal functions in generic_ops.c
      PM / Sleep: Simplify generic system suspend callbacks
      PM / Hibernate: Remove deprecated hibernation snapshot ioctls
      PM / Sleep: Fix freezer failures due to racy usermodehelper_is_disabled()
      ARM: S3C64XX: Implement basic power domain support
      PM / shmobile: Use common always on power domain governor
      ...
    
    Fix up trivial conflict in fs/xfs/xfs_buf.c due to removal of unused
    XBT_FORCE_SLEEP bit

commit 0db49b72bce26341274b74fd968501489a361ae3
Merge: 35b740e4662e 1ac9bc6943ed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 6 08:33:28 2012 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      sched/tracing: Add a new tracepoint for sleeptime
      sched: Disable scheduler warnings during oopses
      sched: Fix cgroup movement of waking process
      sched: Fix cgroup movement of newly created process
      sched: Fix cgroup movement of forking process
      sched: Remove cfs bandwidth period check in tg_set_cfs_period()
      sched: Fix load-balance lock-breaking
      sched: Replace all_pinned with a generic flags field
      sched: Only queue remote wakeups when crossing cache boundaries
      sched: Add missing rcu_dereference() around ->real_parent usage
      [S390] fix cputime overflow in uptime_proc_show
      [S390] cputime: add sparse checking and cleanup
      sched: Mark parent and real_parent as __rcu
      sched, nohz: Fix missing RCU read lock
      sched, nohz: Set the NOHZ_BALANCE_KICK flag for idle load balancer
      sched, nohz: Fix the idle cpu check in nohz_idle_balance
      sched: Use jump_labels for sched_feat
      sched/accounting: Fix parameter passing in task_group_account_field
      sched/accounting: Fix user/system tick double accounting
      sched/accounting: Re-use scheduler statistics for the root cgroup
      ...
    
    Fix up conflicts in
     - arch/ia64/include/asm/cputime.h, include/asm-generic/cputime.h
            usecs_to_cputime64() vs the sparse cleanups
     - kernel/sched/fair.c, kernel/time/tick-sched.c
            scheduler changes in multiple branches

commit 612ef28a045efadb3a98d4492ead7806a146485d
Merge: c3e0ef9a298e 07cde2608a3b
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Dec 19 19:23:15 2011 +0100

    Merge branch 'sched/core' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into cputime-tip
    
    Conflicts:
            drivers/cpufreq/cpufreq_conservative.c
            drivers/cpufreq/cpufreq_ondemand.c
            drivers/macintosh/rack-meter.c
            fs/proc/stat.c
            fs/proc/uptime.c
            kernel/sched/core.c

commit 83712358ba0a1497ce59a4f84ce4dd0f803fe6fc
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Jun 11 19:25:42 2011 -0600

    writeback: dirty ratelimit - think time compensation
    
    Compensate the task's think time when computing the final pause time,
    so that ->dirty_ratelimit can be executed accurately.
    
            think time := time spend outside of balance_dirty_pages()
    
    In the rare case that the task slept longer than the 200ms period time
    (result in negative pause time), the sleep time will be compensated in
    the following periods, too, if it's less than 1 second.
    
    Accumulated errors are carefully avoided as long as the max pause area
    is not hitted.
    
    Pseudo code:
    
            period = pages_dirtied / task_ratelimit;
            think = jiffies - dirty_paused_when;
            pause = period - think;
    
    1) normal case: period > think
    
            pause = period - think
            dirty_paused_when = jiffies + pause
            nr_dirtied = 0
    
                                 period time
                  |===============================>|
                      think time      pause time
                  |===============>|==============>|
            ------|----------------|---------------|------------------------
            dirty_paused_when   jiffies
    
    2) no pause case: period <= think
    
            don't pause; reduce future pause time by:
            dirty_paused_when += period
            nr_dirtied = 0
    
                               period time
                  |===============================>|
                                      think time
                  |===================================================>|
            ------|--------------------------------+-------------------|----
            dirty_paused_when                                       jiffies
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c4f3e9b9bc5..984c3b295978 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1527,6 +1527,7 @@ struct task_struct {
 	 */
 	int nr_dirtied;
 	int nr_dirtied_pause;
+	unsigned long dirty_paused_when; /* start of a write-and-pause period */
 
 #ifdef CONFIG_LATENCYTOP
 	int latency_record_count;

commit 648616343cdbe904c585a6c12e323d3b3c72e46f
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Dec 15 14:56:09 2011 +0100

    [S390] cputime: add sparse checking and cleanup
    
    Make cputime_t and cputime64_t nocast to enable sparse checking to
    detect incorrect use of cputime. Drop the cputime macros for simple
    scalar operations. The conversion macros are still needed.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c4f3e9b9bc5..5649032d73fe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -483,8 +483,8 @@ struct task_cputime {
 
 #define INIT_CPUTIME	\
 	(struct task_cputime) {					\
-		.utime = cputime_zero,				\
-		.stime = cputime_zero,				\
+		.utime = 0,					\
+		.stime = 0,					\
 		.sum_exec_runtime = 0,				\
 	}
 

commit abd63bc3a0f65ae9d85bc3b1bb067d3e3c2b2cc2
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Dec 14 14:39:26 2011 -0800

    sched: Mark parent and real_parent as __rcu
    
    The parent and real_parent pointers should be considered __rcu,
    since they should be held under either tasklist_lock or
    rcu_read_lock.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Link: http://lkml.kernel.org/r/20111214223925.GA27578@www.outflux.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cc8c6206657f..5ef09012a629 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1330,8 +1330,8 @@ struct task_struct {
 	 * older sibling, respectively.  (p->father can be replaced with 
 	 * p->real_parent->pid)
 	 */
-	struct task_struct *real_parent; /* real parent process */
-	struct task_struct *parent; /* recipient of SIGCHLD, wait4() reports */
+	struct task_struct __rcu *real_parent; /* real parent process */
+	struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
 	/*
 	 * children/sibling forms the list of my natural children
 	 */

commit 6a54aebf6978e9f296a4d3da3e40af425163c22e
Merge: 067491b7313c dc47ce90c3a8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 15 08:21:21 2011 +0100

    Merge commit 'v3.2-rc5' into sched/core
    
    Merge reason: Pick up the latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 77e4ef99d1c596a31747668e5fd837f77b6349b6
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 12 18:12:21 2011 -0800

    threadgroup: extend threadgroup_lock() to cover exit and exec
    
    threadgroup_lock() protected only protected against new addition to
    the threadgroup, which was inherently somewhat incomplete and
    problematic for its only user cgroup.  On-going migration could race
    against exec and exit leading to interesting problems - the symmetry
    between various attach methods, task exiting during method execution,
    ->exit() racing against attach methods, migrating task switching basic
    properties during exec and so on.
    
    This patch extends threadgroup_lock() such that it protects against
    all three threadgroup altering operations - fork, exit and exec.  For
    exit, threadgroup_change_begin/end() calls are added to exit_signals
    around assertion of PF_EXITING.  For exec, threadgroup_[un]lock() are
    updated to also grab and release cred_guard_mutex.
    
    With this change, threadgroup_lock() guarantees that the target
    threadgroup will remain stable - no new task will be added, no new
    PF_EXITING will be set and exec won't happen.
    
    The next patch will update cgroup so that it can take full advantage
    of this change.
    
    -v2: beefed up comment as suggested by Frederic.
    
    -v3: narrowed scope of protection in exit path as suggested by
         Frederic.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Menage <paul@paulmenage.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8cd523202a3b..c0c5876c52c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -635,11 +635,13 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_CGROUPS
 	/*
-	 * The group_rwsem prevents threads from forking with
-	 * CLONE_THREAD while held for writing. Use this for fork-sensitive
-	 * threadgroup-wide operations. It's taken for reading in fork.c in
-	 * copy_process().
-	 * Currently only needed write-side by cgroups.
+	 * group_rwsem prevents new tasks from entering the threadgroup and
+	 * member tasks from exiting,a more specifically, setting of
+	 * PF_EXITING.  fork and exit paths are protected with this rwsem
+	 * using threadgroup_change_begin/end().  Users which require
+	 * threadgroup to remain stable should use threadgroup_[un]lock()
+	 * which also takes care of exec path.  Currently, cgroup is the
+	 * only user.
 	 */
 	struct rw_semaphore group_rwsem;
 #endif
@@ -2371,7 +2373,6 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 }
 
-/* See the declaration of group_rwsem in signal_struct. */
 #ifdef CONFIG_CGROUPS
 static inline void threadgroup_change_begin(struct task_struct *tsk)
 {
@@ -2381,13 +2382,47 @@ static inline void threadgroup_change_end(struct task_struct *tsk)
 {
 	up_read(&tsk->signal->group_rwsem);
 }
+
+/**
+ * threadgroup_lock - lock threadgroup
+ * @tsk: member task of the threadgroup to lock
+ *
+ * Lock the threadgroup @tsk belongs to.  No new task is allowed to enter
+ * and member tasks aren't allowed to exit (as indicated by PF_EXITING) or
+ * perform exec.  This is useful for cases where the threadgroup needs to
+ * stay stable across blockable operations.
+ *
+ * fork and exit paths explicitly call threadgroup_change_{begin|end}() for
+ * synchronization.  While held, no new task will be added to threadgroup
+ * and no existing live task will have its PF_EXITING set.
+ *
+ * During exec, a task goes and puts its thread group through unusual
+ * changes.  After de-threading, exclusive access is assumed to resources
+ * which are usually shared by tasks in the same group - e.g. sighand may
+ * be replaced with a new one.  Also, the exec'ing task takes over group
+ * leader role including its pid.  Exclude these changes while locked by
+ * grabbing cred_guard_mutex which is used to synchronize exec path.
+ */
 static inline void threadgroup_lock(struct task_struct *tsk)
 {
+	/*
+	 * exec uses exit for de-threading nesting group_rwsem inside
+	 * cred_guard_mutex. Grab cred_guard_mutex first.
+	 */
+	mutex_lock(&tsk->signal->cred_guard_mutex);
 	down_write(&tsk->signal->group_rwsem);
 }
+
+/**
+ * threadgroup_unlock - unlock threadgroup
+ * @tsk: member task of the threadgroup to unlock
+ *
+ * Reverse threadgroup_lock().
+ */
 static inline void threadgroup_unlock(struct task_struct *tsk)
 {
 	up_write(&tsk->signal->group_rwsem);
+	mutex_unlock(&tsk->signal->cred_guard_mutex);
 }
 #else
 static inline void threadgroup_change_begin(struct task_struct *tsk) {}

commit 257058ae2b971646b96ab3a15605ac69186e562a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 12 18:12:21 2011 -0800

    threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
    
    Make the following renames to prepare for extension of threadgroup
    locking.
    
    * s/signal->threadgroup_fork_lock/signal->group_rwsem/
    * s/threadgroup_fork_read_lock()/threadgroup_change_begin()/
    * s/threadgroup_fork_read_unlock()/threadgroup_change_end()/
    * s/threadgroup_fork_write_lock()/threadgroup_lock()/
    * s/threadgroup_fork_write_unlock()/threadgroup_unlock()/
    
    This patch doesn't cause any behavior change.
    
    -v2: Rename threadgroup_change_done() to threadgroup_change_end() per
         KAMEZAWA's suggestion.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Menage <paul@paulmenage.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d81cce933869..8cd523202a3b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -635,13 +635,13 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_CGROUPS
 	/*
-	 * The threadgroup_fork_lock prevents threads from forking with
+	 * The group_rwsem prevents threads from forking with
 	 * CLONE_THREAD while held for writing. Use this for fork-sensitive
 	 * threadgroup-wide operations. It's taken for reading in fork.c in
 	 * copy_process().
 	 * Currently only needed write-side by cgroups.
 	 */
-	struct rw_semaphore threadgroup_fork_lock;
+	struct rw_semaphore group_rwsem;
 #endif
 
 	int oom_adj;		/* OOM kill score adjustment (bit shift) */
@@ -2371,29 +2371,29 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 }
 
-/* See the declaration of threadgroup_fork_lock in signal_struct. */
+/* See the declaration of group_rwsem in signal_struct. */
 #ifdef CONFIG_CGROUPS
-static inline void threadgroup_fork_read_lock(struct task_struct *tsk)
+static inline void threadgroup_change_begin(struct task_struct *tsk)
 {
-	down_read(&tsk->signal->threadgroup_fork_lock);
+	down_read(&tsk->signal->group_rwsem);
 }
-static inline void threadgroup_fork_read_unlock(struct task_struct *tsk)
+static inline void threadgroup_change_end(struct task_struct *tsk)
 {
-	up_read(&tsk->signal->threadgroup_fork_lock);
+	up_read(&tsk->signal->group_rwsem);
 }
-static inline void threadgroup_fork_write_lock(struct task_struct *tsk)
+static inline void threadgroup_lock(struct task_struct *tsk)
 {
-	down_write(&tsk->signal->threadgroup_fork_lock);
+	down_write(&tsk->signal->group_rwsem);
 }
-static inline void threadgroup_fork_write_unlock(struct task_struct *tsk)
+static inline void threadgroup_unlock(struct task_struct *tsk)
 {
-	up_write(&tsk->signal->threadgroup_fork_lock);
+	up_write(&tsk->signal->group_rwsem);
 }
 #else
-static inline void threadgroup_fork_read_lock(struct task_struct *tsk) {}
-static inline void threadgroup_fork_read_unlock(struct task_struct *tsk) {}
-static inline void threadgroup_fork_write_lock(struct task_struct *tsk) {}
-static inline void threadgroup_fork_write_unlock(struct task_struct *tsk) {}
+static inline void threadgroup_change_begin(struct task_struct *tsk) {}
+static inline void threadgroup_change_end(struct task_struct *tsk) {}
+static inline void threadgroup_lock(struct task_struct *tsk) {}
+static inline void threadgroup_unlock(struct task_struct *tsk) {}
 #endif
 
 #ifndef __HAVE_THREAD_FUNCTIONS

commit c4f3060843506ba6d473ab9a0afe5bd5dc93a00d
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Nov 10 12:41:56 2011 -0800

    sched: Add is_idle_task() to handle invalidated uses of idle_cpu()
    
    Commit 908a3283 (Fix idle_cpu()) invalidated some uses of idle_cpu(),
    which used to say whether or not the CPU was running the idle task,
    but now instead says whether or not the CPU is running the idle task
    in the absence of pending wakeups.  Although this new implementation
    gives a better answer to the question "is this CPU idle?", it also
    invalidates other uses that were made of idle_cpu().
    
    This commit therefore introduces a new is_idle_task() API member
    that determines whether or not the specified task is one of the
    idle tasks, allowing open-coded "->pid == 0" sequences to be replaced
    by something more meaningful.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c4f3e9b9bc5..4a7e4d333a27 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2070,6 +2070,14 @@ extern int sched_setscheduler(struct task_struct *, int,
 extern int sched_setscheduler_nocheck(struct task_struct *, int,
 				      const struct sched_param *);
 extern struct task_struct *idle_task(int cpu);
+/**
+ * is_idle_task - is the specified task an idle task?
+ * @tsk: the task in question.
+ */
+static inline bool is_idle_task(struct task_struct *p)
+{
+	return p->pid == 0;
+}
 extern struct task_struct *curr_task(int cpu);
 extern void set_curr_task(int cpu, struct task_struct *p);
 

commit fdaabd800bdd60652a448994eeb77442180db6c0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Dec 6 12:47:55 2011 +0100

    sched: Fix compile error for UP,!NOHZ
    
    Commit 69e1e811 ("sched, nohz: Track nr_busy_cpus in the
    sched_group_power") messed up the static inline function definition.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/n/tip-abjah8ctq5qrjjtdiabe8lph@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 295666cb5b86..64527c499624 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -277,7 +277,7 @@ extern void set_cpu_sd_state_idle(void);
 extern int get_nohz_timer_target(void);
 #else
 static inline void select_nohz_load_balancer(int stop_tick) { }
-static inline void set_cpu_sd_state_idle(void);
+static inline void set_cpu_sd_state_idle(void) { }
 #endif
 
 /*

commit 69e1e811dcc436a6b129dbef273ad9ec22d095ce
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Dec 1 17:07:33 2011 -0800

    sched, nohz: Track nr_busy_cpus in the sched_group_power
    
    Introduce nr_busy_cpus in the struct sched_group_power [Not in sched_group
    because sched groups are duplicated for the SD_OVERLAP scheduler domain]
    and for each cpu that enters and exits idle, this parameter will
    be updated in each scheduler group of the scheduler domain that this cpu
    belongs to.
    
    To avoid the frequent update of this state as the cpu enters
    and exits idle, the update of the stat during idle exit is
    delayed to the first timer tick that happens after the cpu becomes busy.
    This is done using NOHZ_IDLE flag in the struct rq's nohz_flags.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20111202010832.555984323@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8db17b7622ec..295666cb5b86 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -273,9 +273,11 @@ extern int runqueue_is_locked(int cpu);
 
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern void select_nohz_load_balancer(int stop_tick);
+extern void set_cpu_sd_state_idle(void);
 extern int get_nohz_timer_target(void);
 #else
 static inline void select_nohz_load_balancer(int stop_tick) { }
+static inline void set_cpu_sd_state_idle(void);
 #endif
 
 /*
@@ -901,6 +903,10 @@ struct sched_group_power {
 	 * single CPU.
 	 */
 	unsigned int power, power_orig;
+	/*
+	 * Number of busy cpus in this group.
+	 */
+	atomic_t nr_busy_cpus;
 };
 
 struct sched_group {

commit 986b11c3ee9e0eace25fe74a502205f7fe8c179b
Merge: bb58dd5d1ffa 24b7ead3fb0b
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Nov 23 21:09:02 2011 +0100

    Merge branch 'pm-freezer' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into pm-freezer
    
    * 'pm-freezer' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc: (24 commits)
      freezer: fix wait_event_freezable/__thaw_task races
      freezer: kill unused set_freezable_with_signal()
      dmatest: don't use set_freezable_with_signal()
      usb_storage: don't use set_freezable_with_signal()
      freezer: remove unused @sig_only from freeze_task()
      freezer: use lock_task_sighand() in fake_signal_wake_up()
      freezer: restructure __refrigerator()
      freezer: fix set_freezable[_with_signal]() race
      freezer: remove should_send_signal() and update frozen()
      freezer: remove now unused TIF_FREEZE
      freezer: make freezing() test freeze conditions in effect instead of TIF_FREEZE
      cgroup_freezer: prepare for removal of TIF_FREEZE
      freezer: clean up freeze_processes() failure path
      freezer: kill PF_FREEZING
      freezer: test freezable conditions while holding freezer_lock
      freezer: make freezing indicate freeze condition in effect
      freezer: use dedicated lock instead of task_lock() + memory barrier
      freezer: don't distinguish nosig tasks on thaw
      freezer: remove racy clear_freeze_flag() and set PF_NOFREEZE on dead tasks
      freezer: rename thaw_process() to __thaw_task() and simplify the implementation
      ...

commit 34b087e48367c252e343c2f8de65676a78af1e4a
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Nov 23 09:28:17 2011 -0800

    freezer: kill unused set_freezable_with_signal()
    
    There's no in-kernel user of set_freezable_with_signal() left.  Mixing
    TIF_SIGPENDING with kernel threads can lead to nasty corner cases as
    kernel threads never travel signal delivery path on their own.
    
    e.g. the current implementation is buggy in the cancelation path of
    __thaw_task().  It calls recalc_sigpending_and_wake() in an attempt to
    clear TIF_SIGPENDING but the function never clears it regardless of
    sigpending state.  This means that signallable freezable kthreads may
    continue executing with !freezing() && stuck TIF_SIGPENDING, which can
    be troublesome.
    
    This patch removes set_freezable_with_signal() along with
    PF_FREEZER_NOSIG and recalc_sigpending*() calls in freezer.  User
    tasks get TIF_SIGPENDING, kernel tasks get woken up and the spurious
    sigpending is dealt with in the usual signal delivery path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d12bd03b688f..2f90470ad843 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1788,7 +1788,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
-#define PF_FREEZER_NOSIG 0x80000000	/* Freezer won't send signals to it */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 376fede80e74d98b49d1ba9ac18f23c9fd026ddd
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 21 12:32:24 2011 -0800

    freezer: kill PF_FREEZING
    
    With the previous changes, there's no meaningful difference between
    PF_FREEZING and PF_FROZEN.  Remove PF_FREEZING and use PF_FROZEN
    instead in task_contributes_to_load().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68daf4f27e2c..d12bd03b688f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -220,7 +220,7 @@ extern char ___assert_task_state[1 - 2*!!(
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
-				 (task->flags & PF_FREEZING) == 0)
+				 (task->flags & PF_FROZEN) == 0)
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
@@ -1773,7 +1773,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_NPROC_EXCEEDED 0x00001000	/* set_user noticed that RLIMIT_NPROC was exceeded */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
-#define PF_FREEZING	0x00004000	/* freeze in progress. do not account to load */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */

commit 468e6a20afaccb67e2a7d7f60d301f90e1c6f301
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Sep 7 10:41:32 2011 -0600

    writeback: remove vm_dirties and task->dirties
    
    They are not used any more.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68daf4f27e2c..1c4f3e9b9bc5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1521,7 +1521,6 @@ struct task_struct {
 #ifdef CONFIG_FAULT_INJECTION
 	int make_it_fail;
 #endif
-	struct prop_local_single dirties;
 	/*
 	 * when (nr_dirtied >= nr_dirtied_pause), it's time to call
 	 * balance_dirty_pages() for some dirty throttling pause

commit 029632fbb7b7c9d85063cc9eb470de6c54873df3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 25 10:00:11 2011 +0200

    sched: Make separate sched*.c translation units
    
    Since once needs to do something at conferences and fixing compile
    warnings doesn't actually require much if any attention I decided
    to break up the sched.c #include "*.c" fest.
    
    This further modularizes the scheduler code.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-x0fcd3mnp8f9c99grcpewmhi@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 68daf4f27e2c..8db17b7622ec 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -925,6 +925,15 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 	return to_cpumask(sg->cpumask);
 }
 
+/**
+ * group_first_cpu - Returns the first cpu in the cpumask of a sched_group.
+ * @group: The group whose first cpu is to be returned.
+ */
+static inline unsigned int group_first_cpu(struct sched_group *group)
+{
+	return cpumask_first(sched_group_cpus(group));
+}
+
 struct sched_domain_attr {
 	int relax_domain_level;
 };

commit 208bca0860406d16398145ddd950036a737c3c9d
Merge: 6aad3738f6a7 0e175a1835ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:02:23 2011 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: Add a 'reason' to wb_writeback_work
      writeback: send work item to queue_io, move_expired_inodes
      writeback: trace event balance_dirty_pages
      writeback: trace event bdi_dirty_ratelimit
      writeback: fix ppc compile warnings on do_div(long long, unsigned long)
      writeback: per-bdi background threshold
      writeback: dirty position control - bdi reserve area
      writeback: control dirty pause time
      writeback: limit max dirty pause time
      writeback: IO-less balance_dirty_pages()
      writeback: per task dirty rate limit
      writeback: stabilize bdi->dirty_ratelimit
      writeback: dirty rate control
      writeback: add bg_threshold parameter to __bdi_update_bandwidth()
      writeback: dirty position control
      writeback: account per-bdi accumulated dirtied pages

commit 8a4a8918ed6e4a361f4df19f199bbc2d0a89a46c
Merge: 8686a0e20041 540f41edc154
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 17:08:43 2011 +0200

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      llist: Add back llist_add_batch() and llist_del_first() prototypes
      sched: Don't use tasklist_lock for debug prints
      sched: Warn on rt throttling
      sched: Unify the ->cpus_allowed mask copy
      sched: Wrap scheduler p->cpus_allowed access
      sched: Request for idle balance during nohz idle load balance
      sched: Use resched IPI to kick off the nohz idle balance
      sched: Fix idle_cpu()
      llist: Remove cpu_relax() usage in cmpxchg loops
      sched: Convert to struct llist
      llist: Add llist_next()
      irq_work: Use llist in the struct irq_work logic
      llist: Return whether list is empty before adding in llist_add()
      llist: Move cpu_relax() to after the cmpxchg()
      llist: Remove the platform-dependent NMI checks
      llist: Make some llist functions inline
      sched, tracing: Show PREEMPT_ACTIVE state in trace_sched_switch
      sched: Remove redundant test in check_preempt_tick()
      sched: Add documentation for bandwidth control
      sched: Return unused runtime on group dequeue
      ...

commit 19b4a8d520a6e0176dd52aaa429261ad4fcaa545
Merge: 3cfef9524677 048b71802903
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 16:26:53 2011 +0200

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (45 commits)
      rcu: Move propagation of ->completed from rcu_start_gp() to rcu_report_qs_rsp()
      rcu: Remove rcu_needs_cpu_flush() to avoid false quiescent states
      rcu: Wire up RCU_BOOST_PRIO for rcutree
      rcu: Make rcu_torture_boost() exit loops at end of test
      rcu: Make rcu_torture_fqs() exit loops at end of test
      rcu: Permit rt_mutex_unlock() with irqs disabled
      rcu: Avoid having just-onlined CPU resched itself when RCU is idle
      rcu: Suppress NMI backtraces when stall ends before dump
      rcu: Prohibit grace periods during early boot
      rcu: Simplify unboosting checks
      rcu: Prevent early boot set_need_resched() from __rcu_pending()
      rcu: Dump local stack if cannot dump all CPUs' stacks
      rcu: Move __rcu_read_unlock()'s barrier() within if-statement
      rcu: Improve rcu_assign_pointer() and RCU_INIT_POINTER() documentation
      rcu: Make rcu_assign_pointer() unconditionally insert a memory barrier
      rcu: Make rcu_implicit_dynticks_qs() locals be correct size
      rcu: Eliminate in_irq() checks in rcu_enter_nohz()
      nohz: Remove nohz_cpu_mask
      rcu: Document interpretation of RCU-lockdep splats
      rcu: Allow rcutorture's stat_interval parameter to be changed at runtime
      ...

commit 3cfef9524677a4ecb392d6fbffe6ebce6302f1d4
Merge: 982653009b88 68cc3990a545
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 16:17:32 2011 +0200

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      rtmutex: Add missing rcu_read_unlock() in debug_rt_mutex_print_deadlock()
      lockdep: Comment all warnings
      lib: atomic64: Change the type of local lock to raw_spinlock_t
      locking, lib/atomic64: Annotate atomic64_lock::lock as raw
      locking, x86, iommu: Annotate qi->q_lock as raw
      locking, x86, iommu: Annotate irq_2_ir_lock as raw
      locking, x86, iommu: Annotate iommu->register_lock as raw
      locking, dma, ipu: Annotate bank_lock as raw
      locking, ARM: Annotate low level hw locks as raw
      locking, drivers/dca: Annotate dca_lock as raw
      locking, powerpc: Annotate uic->lock as raw
      locking, x86: mce: Annotate cmci_discover_lock as raw
      locking, ACPI: Annotate c3_lock as raw
      locking, oprofile: Annotate oprofilefs lock as raw
      locking, video: Annotate vga console lock as raw
      locking, latencytop: Annotate latency_lock as raw
      locking, timer_stats: Annotate table_lock as raw
      locking, rwsem: Annotate inner lock as raw
      locking, semaphores: Annotate inner lock as raw
      locking, sched: Annotate thread_group_cputimer as raw
      ...
    
    Fix up conflicts in kernel/posix-cpu-timers.c manually: making
    cputimer->cputime a raw lock conflicted with the ABBA fix in commit
    bcd5cff7216f ("cputimer: Cure lock inversion").

commit 1be025d3cb40cd295123af2c394f7229ef9b30ca
Merge: 2d03423b2319 a2c76b83fdd7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 25 12:23:15 2011 +0200

    Merge branch 'usb-next' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/usb
    
    * 'usb-next' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/usb: (260 commits)
      usb: renesas_usbhs: fixup inconsistent return from usbhs_pkt_push()
      usb/isp1760: Allow to optionally trigger low-level chip reset via GPIOLIB.
      USB: gadget: midi: memory leak in f_midi_bind_config()
      USB: gadget: midi: fix range check in f_midi_out_open()
      QE/FHCI: fixed the CONTROL bug
      usb: renesas_usbhs: tidyup for smatch warnings
      USB: Fix USB Kconfig dependency problem on 85xx/QoirQ platforms
      EHCI: workaround for MosChip controller bug
      usb: gadget: file_storage: fix race on unloading
      USB: ftdi_sio.c: Use ftdi async_icount structure for TIOCMIWAIT, as in other drivers
      USB: ftdi_sio.c:Fill MSR fields of the ftdi async_icount structure
      USB: ftdi_sio.c: Fill LSR fields of the ftdi async_icount structure
      USB: ftdi_sio.c:Fill TX field of the ftdi async_icount structure
      USB: ftdi_sio.c: Fill the RX field of the ftdi async_icount structure
      USB: ftdi_sio.c: Basic icount infrastructure for ftdi_sio
      usb/isp1760: Let OF bindings depend on general CONFIG_OF instead of PPC_OF .
      USB: ftdi_sio: Support TI/Luminary Micro Stellaris BD-ICDI Board
      USB: Fix runtime wakeup on OHCI
      xHCI/USB: Make xHCI driver have a BOS descriptor.
      usb: gadget: add new usb gadget for ACM and mass storage
      ...

commit fa14ff4accfb24e59d2473f3d864d6648d80563b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 12 13:06:17 2011 +0200

    sched: Convert to struct llist
    
    Use the generic llist primitives.
    
    We had a private lockless list implementation in the scheduler in the wake-list
    code, now that we have a generic llist implementation that provides all required
    operations, switch to it.
    
    This patch is not expected to change any behavior.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1315836353.26517.42.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9fda2888a6ab..fc3e8911818a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -90,6 +90,7 @@ struct sched_param {
 #include <linux/task_io_accounting.h>
 #include <linux/latencytop.h>
 #include <linux/cred.h>
+#include <linux/llist.h>
 
 #include <asm/processor.h>
 
@@ -1225,7 +1226,7 @@ struct task_struct {
 	unsigned int ptrace;
 
 #ifdef CONFIG_SMP
-	struct task_struct *wake_entry;
+	struct llist_node wake_entry;
 	int on_cpu;
 #endif
 	int on_rq;

commit 22f92bacbeea24b20e447444c28e7cad9f1ac3f8
Merge: 557ab425429a 0f86267b79bc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Oct 4 11:08:16 2011 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: pick up the latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9d823e8f6b1b7b39f952d7d1795f29162143a433
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Jun 11 18:10:12 2011 -0600

    writeback: per task dirty rate limit
    
    Add two fields to task_struct.
    
    1) account dirtied pages in the individual tasks, for accuracy
    2) per-task balance_dirty_pages() call intervals, for flexibility
    
    The balance_dirty_pages() call interval (ie. nr_dirtied_pause) will
    scale near-sqrt to the safety gap between dirty pages and threshold.
    
    The main problem of per-task nr_dirtied is, if 1k+ tasks start dirtying
    pages at exactly the same time, each task will be assigned a large
    initial nr_dirtied_pause, so that the dirty threshold will be exceeded
    long before each task reached its nr_dirtied_pause and hence call
    balance_dirty_pages().
    
    The solution is to watch for the number of pages dirtied on each CPU in
    between the calls into balance_dirty_pages(). If it exceeds ratelimit_pages
    (3% dirty threshold), force call balance_dirty_pages() for a chance to
    set bdi->dirty_exceeded. In normal situations, this safeguarding
    condition is not expected to trigger at all.
    
    On the sqrt in dirty_poll_interval():
    
    It will serve as an initial guess when dirty pages are still in the
    freerun area.
    
    When dirty pages are floating inside the dirty control scope [freerun,
    limit], a followup patch will use some refined dirty poll interval to
    get the desired pause time.
    
       thresh-dirty (MB)    sqrt
                       1      16
                       2      22
                       4      32
                       8      45
                      16      64
                      32      90
                      64     128
                     128     181
                     256     256
                     512     362
                    1024     512
    
    The above table means, given 1MB (or 1GB) gap and the dd tasks polling
    balance_dirty_pages() on every 16 (or 512) pages, the dirty limit won't
    be exceeded as long as there are less than 16 (or 512) concurrent dd's.
    
    So sqrt naturally leads to less overheads and more safe concurrent tasks
    for large memory servers, which have large (thresh-freerun) gaps.
    
    peter: keep the per-CPU ratelimit for safeguarding the 1k+ tasks case
    
    CC: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Andrea Righi <andrea@betterlinux.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 41d0237fd449..a4a5582dc618 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1525,6 +1525,13 @@ struct task_struct {
 	int make_it_fail;
 #endif
 	struct prop_local_single dirties;
+	/*
+	 * when (nr_dirtied >= nr_dirtied_pause), it's time to call
+	 * balance_dirty_pages() for some dirty throttling pause
+	 */
+	int nr_dirtied;
+	int nr_dirtied_pause;
+
 #ifdef CONFIG_LATENCYTOP
 	int latency_record_count;
 	struct latency_record latency_record[LT_SAVECOUNT];

commit d670ec13178d0fd8680e6742a2bc6e04f28f87d8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 1 12:42:04 2011 +0200

    posix-cpu-timers: Cure SMP wobbles
    
    David reported:
    
      Attached below is a watered-down version of rt/tst-cpuclock2.c from
      GLIBC.  Just build it with "gcc -o test test.c -lpthread -lrt" or
      similar.
    
      Run it several times, and you will see cases where the main thread
      will measure a process clock difference before and after the nanosleep
      which is smaller than the cpu-burner thread's individual thread clock
      difference.  This doesn't make any sense since the cpu-burner thread
      is part of the top-level process's thread group.
    
      I've reproduced this on both x86-64 and sparc64 (using both 32-bit and
      64-bit binaries).
    
      For example:
    
      [davem@boricha build-x86_64-linux]$ ./test
      process: before(0.001221967) after(0.498624371) diff(497402404)
      thread:  before(0.000081692) after(0.498316431) diff(498234739)
      self:    before(0.001223521) after(0.001240219) diff(16698)
      [davem@boricha build-x86_64-linux]$
    
      The diff of 'process' should always be >= the diff of 'thread'.
    
      I make sure to wrap the 'thread' clock measurements the most tightly
      around the nanosleep() call, and that the 'process' clock measurements
      are the outer-most ones.
    
      ---
      #include <unistd.h>
      #include <stdio.h>
      #include <stdlib.h>
      #include <time.h>
      #include <fcntl.h>
      #include <string.h>
      #include <errno.h>
      #include <pthread.h>
    
      static pthread_barrier_t barrier;
    
      static void *chew_cpu(void *arg)
      {
              pthread_barrier_wait(&barrier);
              while (1)
                      __asm__ __volatile__("" : : : "memory");
              return NULL;
      }
    
      int main(void)
      {
              clockid_t process_clock, my_thread_clock, th_clock;
              struct timespec process_before, process_after;
              struct timespec me_before, me_after;
              struct timespec th_before, th_after;
              struct timespec sleeptime;
              unsigned long diff;
              pthread_t th;
              int err;
    
              err = clock_getcpuclockid(0, &process_clock);
              if (err)
                      return 1;
    
              err = pthread_getcpuclockid(pthread_self(), &my_thread_clock);
              if (err)
                      return 1;
    
              pthread_barrier_init(&barrier, NULL, 2);
              err = pthread_create(&th, NULL, chew_cpu, NULL);
              if (err)
                      return 1;
    
              err = pthread_getcpuclockid(th, &th_clock);
              if (err)
                      return 1;
    
              pthread_barrier_wait(&barrier);
    
              err = clock_gettime(process_clock, &process_before);
              if (err)
                      return 1;
    
              err = clock_gettime(my_thread_clock, &me_before);
              if (err)
                      return 1;
    
              err = clock_gettime(th_clock, &th_before);
              if (err)
                      return 1;
    
              sleeptime.tv_sec = 0;
              sleeptime.tv_nsec = 500000000;
              nanosleep(&sleeptime, NULL);
    
              err = clock_gettime(th_clock, &th_after);
              if (err)
                      return 1;
    
              err = clock_gettime(my_thread_clock, &me_after);
              if (err)
                      return 1;
    
              err = clock_gettime(process_clock, &process_after);
              if (err)
                      return 1;
    
              diff = process_after.tv_nsec - process_before.tv_nsec;
              printf("process: before(%lu.%.9lu) after(%lu.%.9lu) diff(%lu)\n",
                     process_before.tv_sec, process_before.tv_nsec,
                     process_after.tv_sec, process_after.tv_nsec, diff);
              diff = th_after.tv_nsec - th_before.tv_nsec;
              printf("thread:  before(%lu.%.9lu) after(%lu.%.9lu) diff(%lu)\n",
                     th_before.tv_sec, th_before.tv_nsec,
                     th_after.tv_sec, th_after.tv_nsec, diff);
              diff = me_after.tv_nsec - me_before.tv_nsec;
              printf("self:    before(%lu.%.9lu) after(%lu.%.9lu) diff(%lu)\n",
                     me_before.tv_sec, me_before.tv_nsec,
                     me_after.tv_sec, me_after.tv_nsec, diff);
    
              return 0;
      }
    
    This is due to us using p->se.sum_exec_runtime in
    thread_group_cputime() where we iterate the thread group and sum all
    data. This does not take time since the last schedule operation (tick
    or otherwise) into account. We can cure this by using
    task_sched_runtime() at the cost of having to take locks.
    
    This also means we can (and must) do away with
    thread_group_sched_runtime() since the modified thread_group_cputime()
    is now more accurate and would deadlock when called from
    thread_group_sched_runtime().
    
    Aside of that it makes the function safe on 32 bit systems. The old
    code added t->se.sum_exec_runtime unprotected. sum_exec_runtime is a
    64bit value and could be changed on another cpu at the same time.
    
    Reported-by: David Miller <davem@davemloft.net>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/1314874459.7945.22.camel@twins
    Tested-by: David Miller <davem@davemloft.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ac2c0578e0f..41d0237fd449 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1956,7 +1956,6 @@ static inline void disable_sched_clock_irqtime(void) {}
 
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
-extern unsigned long long thread_group_sched_runtime(struct task_struct *task);
 
 /* sched_exec is called by processes performing an exec */
 #ifdef CONFIG_SMP

commit d178bc3a708f39cbfefc3fab37032d3f2511b4ec
Author: Serge Hallyn <serge.hallyn@canonical.com>
Date:   Mon Sep 26 10:45:18 2011 -0500

    user namespace: usb: make usb urbs user namespace aware (v2)
    
    Add to the dev_state and alloc_async structures the user namespace
    corresponding to the uid and euid.  Pass these to kill_pid_info_as_uid(),
    which can then implement a proper, user-namespace-aware uid check.
    
    Changelog:
    Sep 20: Per Oleg's suggestion: Instead of caching and passing user namespace,
            uid, and euid each separately, pass a struct cred.
    Sep 26: Address Alan Stern's comments: don't define a struct cred at
            usbdev_open(), and take and put a cred at async_completed() to
            ensure it lasts for the duration of kill_pid_info_as_cred().
    
    Signed-off-by: Serge Hallyn <serge.hallyn@canonical.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ac2c0578e0f..57ddb5283d9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2166,7 +2166,8 @@ extern int force_sigsegv(int, struct task_struct *);
 extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
-extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_t, u32);
+extern int kill_pid_info_as_cred(int, struct siginfo *, struct pid *,
+				const struct cred *, u32);
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int kill_proc_info(int, struct siginfo *, pid_t);

commit 82e78d80fc392ac7e98326bc8beeb8a679913ffd
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Aug 4 07:55:34 2011 -0700

    rcu: Simplify unboosting checks
    
    Commit 7765be (Fix RCU_BOOST race handling current->rcu_read_unlock_special)
    introduced a new ->rcu_boosted field in the task structure.  This is
    redundant because the existing ->rcu_boost_mutex will be non-NULL at
    any time that ->rcu_boosted is nonzero.  Therefore, this commit removes
    ->rcu_boosted and tests ->rcu_boost_mutex instead.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6ee91e20353b..acca43560805 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1259,9 +1259,6 @@ struct task_struct {
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
-#if defined(CONFIG_RCU_BOOST) && defined(CONFIG_TREE_PREEMPT_RCU)
-	int rcu_boosted;
-#endif /* #if defined(CONFIG_RCU_BOOST) && defined(CONFIG_TREE_PREEMPT_RCU) */
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TREE_PREEMPT_RCU

commit fc0763f53e3ff6a6bfa66934662a3446b9ca6f16
Author: Shi, Alex <alex.shi@intel.com>
Date:   Thu Jul 28 14:56:12 2011 +0800

    nohz: Remove nohz_cpu_mask
    
    RCU no longer uses this global variable, nor does anyone else.  This
    commit therefore removes this variable.  This reduces memory footprint
    and also removes some atomic instructions and memory barriers from
    the dyntick-idle path.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ac2c0578e0f..6ee91e20353b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -270,7 +270,6 @@ extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern int runqueue_is_locked(int cpu);
 
-extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern void select_nohz_load_balancer(int stop_tick);
 extern int get_nohz_timer_target(void);

commit ee30a7b2fc072f139dac44826860d2c1f422137c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 18:56:56 2009 +0200

    locking, sched: Annotate thread_group_cputimer as raw
    
    The thread_group_cputimer lock can be taken in atomic context and therefore
    cannot be preempted on -rt - annotate it.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ac2c0578e0f..e672236c08e0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -510,7 +510,7 @@ struct task_cputime {
 struct thread_group_cputimer {
 	struct task_cputime cputime;
 	int running;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 };
 
 #include <linux/rwsem.h>
@@ -2566,7 +2566,7 @@ void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
 static inline void thread_group_cputime_init(struct signal_struct *sig)
 {
-	spin_lock_init(&sig->cputimer.lock);
+	raw_spin_lock_init(&sig->cputimer.lock);
 }
 
 /*

commit ec12cb7f31e28854efae7dd6f9544e0a66379040
Author: Paul Turner <pjt@google.com>
Date:   Thu Jul 21 09:43:30 2011 -0700

    sched: Accumulate per-cfs_rq cpu usage and charge against bandwidth
    
    Account bandwidth usage on the cfs_rq level versus the task_groups to which
    they belong.  Whether we are tracking bandwidth on a given cfs_rq is maintained
    under cfs_rq->runtime_enabled.
    
    cfs_rq's which belong to a bandwidth constrained task_group have their runtime
    accounted via the update_curr() path, which withdraws bandwidth from the global
    pool as desired.  Updates involving the global pool are currently protected
    under cfs_bandwidth->lock, local runtime is protected by rq->lock.
    
    This patch only assigns and tracks quota, no action is taken in the case that
    cfs_rq->runtime_used exceeds cfs_rq->runtime_assigned.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Signed-off-by: Nikhil Rao <ncrao@google.com>
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Reviewed-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110721184757.179386821@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ac2c0578e0f..bc6f5f2e24fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2040,6 +2040,10 @@ static inline void sched_autogroup_fork(struct signal_struct *sig) { }
 static inline void sched_autogroup_exit(struct signal_struct *sig) { }
 #endif
 
+#ifdef CONFIG_CFS_BANDWIDTH
+extern unsigned int sysctl_sched_cfs_bandwidth_slice;
+#endif
+
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);

commit 72fa59970f8698023045ab0713d66f3f4f96945c
Author: Vasiliy Kulikov <segoon@openwall.com>
Date:   Mon Aug 8 19:02:04 2011 +0400

    move RLIMIT_NPROC check from set_user() to do_execve_common()
    
    The patch http://lkml.org/lkml/2003/7/13/226 introduced an RLIMIT_NPROC
    check in set_user() to check for NPROC exceeding via setuid() and
    similar functions.
    
    Before the check there was a possibility to greatly exceed the allowed
    number of processes by an unprivileged user if the program relied on
    rlimit only.  But the check created new security threat: many poorly
    written programs simply don't check setuid() return code and believe it
    cannot fail if executed with root privileges.  So, the check is removed
    in this patch because of too often privilege escalations related to
    buggy programs.
    
    The NPROC can still be enforced in the common code flow of daemons
    spawning user processes.  Most of daemons do fork()+setuid()+execve().
    The check introduced in execve() (1) enforces the same limit as in
    setuid() and (2) doesn't create similar security issues.
    
    Neil Brown suggested to track what specific process has exceeded the
    limit by setting PF_NPROC_EXCEEDED process flag.  With the change only
    this process would fail on execve(), and other processes' execve()
    behaviour is not changed.
    
    Solar Designer suggested to re-check whether NPROC limit is still
    exceeded at the moment of execve().  If the process was sleeping for
    days between set*uid() and execve(), and the NPROC counter step down
    under the limit, the defered execve() failure because NPROC limit was
    exceeded days ago would be unexpected.  If the limit is not exceeded
    anymore, we clear the flag on successful calls to execve() and fork().
    
    The flag is also cleared on successful calls to set_user() as the limit
    was exceeded for the previous user, not the current one.
    
    Similar check was introduced in -ow patches (without the process flag).
    
    v3 - clear PF_NPROC_EXCEEDED on successful calls to set_user().
    
    Reviewed-by: James Morris <jmorris@namei.org>
    Signed-off-by: Vasiliy Kulikov <segoon@openwall.com>
    Acked-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20b03bf94748..4ac2c0578e0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1767,6 +1767,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_DUMPCORE	0x00000200	/* dumped core */
 #define PF_SIGNALED	0x00000400	/* killed by a signal */
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
+#define PF_NPROC_EXCEEDED 0x00001000	/* set_user noticed that RLIMIT_NPROC was exceeded */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
 #define PF_FREEZING	0x00004000	/* freeze in progress. do not account to load */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */

commit 096a705bbc080a4041636d07514560da8d78acbe
Merge: fea80311a939 5757a6d76cdf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 10:33:36 2011 -0700

    Merge branch 'for-3.1/core' of git://git.kernel.dk/linux-block
    
    * 'for-3.1/core' of git://git.kernel.dk/linux-block: (24 commits)
      block: strict rq_affinity
      backing-dev: use synchronize_rcu_expedited instead of synchronize_rcu
      block: fix patch import error in max_discard_sectors check
      block: reorder request_queue to remove 64 bit alignment padding
      CFQ: add think time check for group
      CFQ: add think time check for service tree
      CFQ: move think time check variables to a separate struct
      fixlet: Remove fs_excl from struct task.
      cfq: Remove special treatment for metadata rqs.
      block: document blk_plug list access
      block: avoid building too big plug list
      compat_ioctl: fix make headers_check regression
      block: eliminate potential for infinite loop in blkdev_issue_discard
      compat_ioctl: fix warning caused by qemu
      block: flush MEDIA_CHANGE from drivers on close(2)
      blk-throttle: Make total_nr_queued unsigned
      block: Add __attribute__((format(printf...) and fix fallout
      fs/partitions/check.c: make local symbols static
      block:remove some spare spaces in genhd.c
      block:fix the comment error in blkdev.h
      ...

commit bdc7ccfc0631797636837b10df7f87bc1e2e4ae3
Merge: 4d4abdcb1dee 0f3171438fc9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 16:45:02 2011 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (24 commits)
      sched: Cleanup duplicate local variable in [enqueue|dequeue]_task_fair
      sched: Replace use of entity_key()
      sched: Separate group-scheduling code more clearly
      sched: Reorder root_domain to remove 64 bit alignment padding
      sched: Do not attempt to destroy uninitialized rt_bandwidth
      sched: Remove unused function cpu_cfs_rq()
      sched: Fix (harmless) typo 'CONFG_FAIR_GROUP_SCHED'
      sched, cgroup: Optimize load_balance_fair()
      sched: Don't update shares twice on on_rq parent
      sched: update correct entity's runtime in check_preempt_wakeup()
      xtensa: Use generic config PREEMPT definition
      h8300: Use generic config PREEMPT definition
      m32r: Use generic PREEMPT config
      sched: Skip autogroup when looking for all rt sched groups
      sched: Simplify mutex_spin_on_owner()
      sched: Remove rcu_read_lock() from wake_affine()
      sched: Generalize sleep inside spinlock detection
      sched: Make sleeping inside spinlock detection working in !CONFIG_PREEMPT
      sched: Isolate preempt counting in its own config option
      sched: Remove pointless in_atomic() definition check
      ...

commit 8209f53d79444747782a28520187abaf689761f2
Merge: 22a3b9771117 eac1b5e57d7a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 15:06:50 2011 -0700

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc
    
    * 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc: (39 commits)
      ptrace: do_wait(traced_leader_killed_by_mt_exec) can block forever
      ptrace: fix ptrace_signal() && STOP_DEQUEUED interaction
      connector: add an event for monitoring process tracers
      ptrace: dont send SIGSTOP on auto-attach if PT_SEIZED
      ptrace: mv send-SIGSTOP from do_fork() to ptrace_init_task()
      ptrace_init_task: initialize child->jobctl explicitly
      has_stopped_jobs: s/task_is_stopped/SIGNAL_STOP_STOPPED/
      ptrace: make former thread ID available via PTRACE_GETEVENTMSG after PTRACE_EVENT_EXEC stop
      ptrace: wait_consider_task: s/same_thread_group/ptrace_reparented/
      ptrace: kill real_parent_is_ptracer() in in favor of ptrace_reparented()
      ptrace: ptrace_reparented() should check same_thread_group()
      redefine thread_group_leader() as exit_signal >= 0
      do not change dead_task->exit_signal
      kill task_detached()
      reparent_leader: check EXIT_DEAD instead of task_detached()
      make do_notify_parent() __must_check, update the callers
      __ptrace_detach: avoid task_detached(), check do_notify_parent()
      kill tracehook_notify_death()
      make do_notify_parent() return bool
      ptrace: s/tracehook_tracer_task()/ptrace_parent()/
      ...

commit 994bf1c92270e3d7731ea08f1d1bd7a668314e60
Merge: bd96efe17d94 cf6ace16a3cd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 21 17:59:54 2011 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: pick up the latest scheduler fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit cf6ace16a3cd8b728fb0afa68368fd40bbeae19f
Merge: acc11eab7059 d1e9ae47a028
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 20 15:56:25 2011 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      signal: align __lock_task_sighand() irq disabling and RCU
      softirq,rcu: Inform RCU of irq_exit() activity
      sched: Add irq_{enter,exit}() to scheduler_ipi()
      rcu: protect __rcu_read_unlock() against scheduler-using irq handlers
      rcu: Streamline code produced by __rcu_read_unlock()
      rcu: Fix RCU_BOOST race handling current->rcu_read_unlock_special
      rcu: decrease rcu_report_exp_rnp coupling with scheduler

commit e3589f6c81e4764d32a25d2a2a0afe54fa344f5c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 15 10:35:52 2011 +0200

    sched: Allow for overlapping sched_domain spans
    
    Allow for sched_domain spans that overlap by giving such domains their
    own sched_group list instead of sharing the sched_groups amongst
    each-other.
    
    This is needed for machines with more than 16 nodes, because
    sched_domain_node_span() will generate a node mask from the
    16 nearest nodes without regard if these masks have any overlap.
    
    Currently sched_domains have a sched_group that maps to their child
    sched_domain span, and since there is no overlap we share the
    sched_group between the sched_domains of the various CPUs. If however
    there is overlap, we would need to link the sched_group list in
    different ways for each cpu, and hence sharing isn't possible.
    
    In order to solve this, allocate private sched_groups for each CPU's
    sched_domain but have the sched_groups share a sched_group_power
    structure such that we can uniquely track the power.
    
    Reported-and-tested-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-08bxqw9wis3qti9u5inifh3y@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e5b3c8e2d3e..bde99d5358dc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -844,6 +844,7 @@ enum cpu_idle_type {
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
 #define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
+#define SD_OVERLAP		0x2000	/* sched_domains of this level overlap */
 
 enum powersavings_balance_level {
 	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */
@@ -894,6 +895,7 @@ static inline int sd_power_saving_flags(void)
 }
 
 struct sched_group_power {
+	atomic_t ref;
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
 	 * single CPU.

commit 9c3f75cbd144014bea6af866a154cc2e73ab2287
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 14 13:00:06 2011 +0200

    sched: Break out cpu_power from the sched_group structure
    
    In order to prepare for non-unique sched_groups per domain, we need to
    carry the cpu_power elsewhere, so put a level of indirection in.
    
    Reported-and-tested-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-qkho2byuhe4482fuknss40ad@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 496770a96487..2e5b3c8e2d3e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -893,16 +893,20 @@ static inline int sd_power_saving_flags(void)
 	return 0;
 }
 
-struct sched_group {
-	struct sched_group *next;	/* Must be a circular list */
-	atomic_t ref;
-
+struct sched_group_power {
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
 	 * single CPU.
 	 */
-	unsigned int cpu_power, cpu_power_orig;
+	unsigned int power, power_orig;
+};
+
+struct sched_group {
+	struct sched_group *next;	/* Must be a circular list */
+	atomic_t ref;
+
 	unsigned int group_weight;
+	struct sched_group_power *sgp;
 
 	/*
 	 * The CPUs this group covers.

commit 7765be2fec0f476fcd61812d5f9406b04c765020
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jul 14 12:24:11 2011 -0700

    rcu: Fix RCU_BOOST race handling current->rcu_read_unlock_special
    
    The RCU_BOOST commits for TREE_PREEMPT_RCU introduced an other-task
    write to a new RCU_READ_UNLOCK_BOOSTED bit in the task_struct structure's
    ->rcu_read_unlock_special field, but, as noted by Steven Rostedt, without
    correctly synchronizing all accesses to ->rcu_read_unlock_special.
    This could result in bits in ->rcu_read_unlock_special being spuriously
    set and cleared due to conflicting accesses, which in turn could result
    in deadlocks between the rcu_node structure's ->lock and the scheduler's
    rq and pi locks.  These deadlocks would result from RCU incorrectly
    believing that the just-ended RCU read-side critical section had been
    preempted and/or boosted.  If that RCU read-side critical section was
    executed with either rq or pi locks held, RCU's ensuing (incorrect)
    calls to the scheduler would cause the scheduler to attempt to once
    again acquire the rq and pi locks, resulting in deadlock.  More complex
    deadlock cycles are also possible, involving multiple rq and pi locks
    as well as locks from multiple rcu_node structures.
    
    This commit fixes synchronization by creating ->rcu_boosted field in
    task_struct that is accessed and modified only when holding the ->lock
    in the rcu_node structure on which the task is queued (on that rcu_node
    structure's ->blkd_tasks list).  This results in tasks accessing only
    their own current->rcu_read_unlock_special fields, making unsynchronized
    access once again legal, and keeping the rcu_read_unlock() fastpath free
    of atomic instructions and memory barriers.
    
    The reason that the rcu_read_unlock() fastpath does not need to access
    the new current->rcu_boosted field is that this new field cannot
    be non-zero unless the RCU_READ_UNLOCK_BLOCKED bit is set in the
    current->rcu_read_unlock_special field.  Therefore, rcu_read_unlock()
    need only test current->rcu_read_unlock_special: if that is zero, then
    current->rcu_boosted must also be zero.
    
    This bug does not affect TINY_PREEMPT_RCU because this implementation
    of RCU accesses current->rcu_read_unlock_special with irqs disabled,
    thus preventing races on the !SMP systems that TINY_PREEMPT_RCU runs on.
    
    Maybe-reported-by: Dave Jones <davej@redhat.com>
    Maybe-reported-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 496770a96487..76676a407e4a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1254,6 +1254,9 @@ struct task_struct {
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
+#if defined(CONFIG_RCU_BOOST) && defined(CONFIG_TREE_PREEMPT_RCU)
+	int rcu_boosted;
+#endif /* #if defined(CONFIG_RCU_BOOST) && defined(CONFIG_TREE_PREEMPT_RCU) */
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TREE_PREEMPT_RCU

commit 4aede84b33d6beb401136a3deca0651ae07c5e99
Author: Justin TerAvest <teravest@google.com>
Date:   Tue Jul 12 08:31:45 2011 +0200

    fixlet: Remove fs_excl from struct task.
    
    fs_excl is a poor man's priority inheritance for filesystems to hint to
    the block layer that an operation is important. It was never clearly
    specified, not widely adopted, and will not prevent starvation in many
    cases (like across cgroups).
    
    fs_excl was introduced with the time sliced CFQ IO scheduler, to
    indicate when a process held FS exclusive resources and thus needed
    a boost.
    
    It doesn't cover all file systems, and it was never fully complete.
    Lets kill it.
    
    Signed-off-by: Justin TerAvest <teravest@google.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a837b20ba190..22f54249cde1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1503,7 +1503,6 @@ struct task_struct {
 	short il_next;
 	short pref_node_fork;
 #endif
-	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;
 
 	/*

commit e4c2fb0d5776b58049d2556b456144a4db3fe5a9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jul 5 10:56:32 2011 +0200

    sched: Disable (revert) SCHED_LOAD_SCALE increase
    
    Alex reported that commit c8b281161df ("sched: Increase
    SCHED_LOAD_SCALE resolution") caused a power usage regression
    under light load as it increases the number of load-balance
    operations and keeps idle cpus from staying idle.
    
    Time has run out to find the root cause for this release so
    disable the feature for v3.0 until we can figure out what
    causes the problem.
    
    Reported-by: "Alex, Shi" <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Nikhil Rao <ncrao@google.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-m4onxn0sxnyn5iz9o88eskc3@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a837b20ba190..496770a96487 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -808,7 +808,7 @@ enum cpu_idle_type {
  * when BITS_PER_LONG <= 32 are pretty high and the returns do not justify the
  * increased costs.
  */
-#if BITS_PER_LONG > 32
+#if 0 /* BITS_PER_LONG > 32 -- currently broken: it increases power usage under light load  */
 # define SCHED_LOAD_RESOLUTION	10
 # define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
 # define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)

commit 1ecc818c51b1f6886825dae3885792d5e49ec798
Merge: 1c09ab0d2573 d902db1eb603
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 1 13:20:51 2011 +0200

    Merge branch 'sched/core-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into sched/core

commit 087806b1281563e4ae7a5bce3155f894af5f4118
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:10:26 2011 +0200

    redefine thread_group_leader() as exit_signal >= 0
    
    Change de_thread() to set old_leader->exit_signal = -1. This is
    good for the consistency, it is no longer the leader and all
    sub-threads have exit_signal = -1 set by copy_process(CLONE_THREAD).
    
    And this allows us to micro-optimize thread_group_leader(), it can
    simply check exit_signal >= 0. This also makes sense because we
    should move ->group_leader from task_struct to signal_struct.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 39acee2c8929..b38ed51d5c64 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2284,8 +2284,10 @@ static inline int get_nr_threads(struct task_struct *tsk)
 	return tsk->signal->nr_threads;
 }
 
-/* de_thread depends on thread_group_leader not being a pid based check */
-#define thread_group_leader(p)	(p == p->group_leader)
+static inline bool thread_group_leader(struct task_struct *p)
+{
+	return p->exit_signal >= 0;
+}
 
 /* Do to the insanities of de_thread it is possible for a process
  * to have the pid of the thread group leader without actually being

commit e550f14dc6322e794d4e70825f63c9c99177ae8b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:09:54 2011 +0200

    kill task_detached()
    
    Upadate the last user of task_detached(), wait_task_zombie(), to
    use thread_group_leader() and kill task_detached().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0cb4f097f76c..39acee2c8929 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2318,11 +2318,6 @@ static inline int thread_group_empty(struct task_struct *p)
 #define delay_group_leader(p) \
 		(thread_group_leader(p) && !thread_group_empty(p))
 
-static inline int task_detached(struct task_struct *p)
-{
-	return p->exit_signal == -1;
-}
-
 /*
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also

commit 8677347378044ab564470bced2275520efb3670d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:09:09 2011 +0200

    make do_notify_parent() __must_check, update the callers
    
    Change other callers of do_notify_parent() to check the value it
    returns, this makes the subsequent task_detached() unnecessary.
    Mark do_notify_parent() as __must_check.
    
    Use thread_group_leader() instead of !task_detached() to check
    if we need to notify the real parent in wait_task_zombie().
    
    Remove the stale comment in release_task(). "just for sanity" is
    no longer true, we have to set EXIT_DEAD to avoid the races with
    do_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0df7231d9ee0..0cb4f097f76c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2160,7 +2160,7 @@ extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern bool do_notify_parent(struct task_struct *, int);
+extern __must_check bool do_notify_parent(struct task_struct *, int);
 extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 extern void force_sig(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);

commit 53c8f9f199b239668e6b1a907735ee323a0d1ccd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:08:18 2011 +0200

    make do_notify_parent() return bool
    
    - change do_notify_parent() to return a boolean, true if the task should
      be reaped because its parent ignores SIGCHLD.
    
    - update the only caller which checks the returned value, exit_notify().
    
    This temporary uglifies exit_notify() even more, will be cleanuped by
    the next change.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 87f7ca7ed6f6..0df7231d9ee0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2145,7 +2145,7 @@ static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, s
 	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 
 	return ret;
-}	
+}
 
 extern void block_all_signals(int (*notifier)(void *priv), void *priv,
 			      sigset_t *mask);
@@ -2160,7 +2160,7 @@ extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern int do_notify_parent(struct task_struct *, int);
+extern bool do_notify_parent(struct task_struct *, int);
 extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 extern void force_sig(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);

commit 544b2c91a9f14f9565af1972203438b7f49afd48
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 14 11:20:18 2011 +0200

    ptrace: implement PTRACE_LISTEN
    
    The previous patch implemented async notification for ptrace but it
    only worked while trace is running.  This patch introduces
    PTRACE_LISTEN which is suggested by Oleg Nestrov.
    
    It's allowed iff tracee is in STOP trap and puts tracee into
    quasi-running state - tracee never really runs but wait(2) and
    ptrace(2) consider it to be running.  While ptracer is listening,
    tracee is allowed to re-enter STOP to notify an async event.
    Listening state is cleared on the first notification.  Ptracer can
    also clear it by issuing INTERRUPT - tracee will re-trap into STOP
    with listening state cleared.
    
    This allows ptracer to monitor group stop state without running tracee
    - use INTERRUPT to put tracee into STOP trap, issue LISTEN and then
    wait(2) to wait for the next group stop event.  When it happens,
    PTRACE_GETSIGINFO provides information to determine the current state.
    
    Test program follows.
    
      #define PTRACE_SEIZE          0x4206
      #define PTRACE_INTERRUPT      0x4207
      #define PTRACE_LISTEN         0x4208
    
      #define PTRACE_SEIZE_DEVEL    0x80000000
    
      static const struct timespec ts1s = { .tv_sec = 1 };
    
      int main(int argc, char **argv)
      {
              pid_t tracee, tracer;
              int i;
    
              tracee = fork();
              if (!tracee)
                      while (1)
                              pause();
    
              tracer = fork();
              if (!tracer) {
                      siginfo_t si;
    
                      ptrace(PTRACE_SEIZE, tracee, NULL,
                             (void *)(unsigned long)PTRACE_SEIZE_DEVEL);
                      ptrace(PTRACE_INTERRUPT, tracee, NULL, NULL);
              repeat:
                      waitid(P_PID, tracee, NULL, WSTOPPED);
    
                      ptrace(PTRACE_GETSIGINFO, tracee, NULL, &si);
                      if (!si.si_code) {
                              printf("tracer: SIG %d\n", si.si_signo);
                              ptrace(PTRACE_CONT, tracee, NULL,
                                     (void *)(unsigned long)si.si_signo);
                              goto repeat;
                      }
                      printf("tracer: stopped=%d signo=%d\n",
                             si.si_signo != SIGTRAP, si.si_signo);
                      if (si.si_signo != SIGTRAP)
                              ptrace(PTRACE_LISTEN, tracee, NULL, NULL);
                      else
                              ptrace(PTRACE_CONT, tracee, NULL, NULL);
                      goto repeat;
              }
    
              for (i = 0; i < 3; i++) {
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGSTOP\n");
                      kill(tracee, SIGSTOP);
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGCONT\n");
                      kill(tracee, SIGCONT);
              }
              nanosleep(&ts1s, NULL);
    
              kill(tracer, SIGKILL);
              kill(tracee, SIGKILL);
              return 0;
      }
    
    This is identical to the program to test TRAP_NOTIFY except that
    tracee is PTRACE_LISTEN'd instead of PTRACE_CONT'd when group stopped.
    This allows ptracer to monitor when group stop ends without running
    tracee.
    
      # ./test-listen
      tracer: stopped=0 signo=5
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
    
    -v2: Moved JOBCTL_LISTENING check in wait_task_stopped() into
         task_stopped_code() as suggested by Oleg.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1854def284f5..87f7ca7ed6f6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1813,6 +1813,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
 #define JOBCTL_TRAP_NOTIFY_BIT	20	/* trap for NOTIFY */
 #define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
+#define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
 
 #define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
 #define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
@@ -1820,6 +1821,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
 #define JOBCTL_TRAP_NOTIFY	(1 << JOBCTL_TRAP_NOTIFY_BIT)
 #define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
+#define JOBCTL_LISTENING	(1 << JOBCTL_LISTENING_BIT)
 
 #define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 #define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)

commit fb1d910c178ba0c5bc32d3e5a9e82e05b7aad3cd
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 14 11:20:17 2011 +0200

    ptrace: implement TRAP_NOTIFY and use it for group stop events
    
    Currently there's no way for ptracer to find out whether group stop
    finished other than polling with INTERRUPT - GETSIGINFO - CONT
    sequence.  This patch implements group stop notification for ptracer
    using STOP traps.
    
    When group stop state of a seized tracee changes, JOBCTL_TRAP_NOTIFY
    is set, which schedules a STOP trap which is sticky - it isn't cleared
    by other traps and at least one STOP trap will happen eventually.
    STOP trap is synchronization point for event notification and the
    tracer can determine the current group stop state by looking at the
    signal number portion of exit code (si_status from waitid(2) or
    si_code from PTRACE_GETSIGINFO).
    
    Notifications are generated both on start and end of group stops but,
    because group stop participation always happens before STOP trap, this
    doesn't cause an extra trap while tracee is participating in group
    stop.  The symmetry will be useful later.
    
    Note that this notification works iff tracee is not trapped.
    Currently there is no way to be notified of group stop state changes
    while tracee is trapped.  This will be addressed by a later patch.
    
    An example program follows.
    
      #define PTRACE_SEIZE          0x4206
      #define PTRACE_INTERRUPT      0x4207
    
      #define PTRACE_SEIZE_DEVEL    0x80000000
    
      static const struct timespec ts1s = { .tv_sec = 1 };
    
      int main(int argc, char **argv)
      {
              pid_t tracee, tracer;
              int i;
    
              tracee = fork();
              if (!tracee)
                      while (1)
                              pause();
    
              tracer = fork();
              if (!tracer) {
                      siginfo_t si;
    
                      ptrace(PTRACE_SEIZE, tracee, NULL,
                             (void *)(unsigned long)PTRACE_SEIZE_DEVEL);
                      ptrace(PTRACE_INTERRUPT, tracee, NULL, NULL);
              repeat:
                      waitid(P_PID, tracee, NULL, WSTOPPED);
    
                      ptrace(PTRACE_GETSIGINFO, tracee, NULL, &si);
                      if (!si.si_code) {
                              printf("tracer: SIG %d\n", si.si_signo);
                              ptrace(PTRACE_CONT, tracee, NULL,
                                     (void *)(unsigned long)si.si_signo);
                              goto repeat;
                      }
                      printf("tracer: stopped=%d signo=%d\n",
                             si.si_signo != SIGTRAP, si.si_signo);
                      ptrace(PTRACE_CONT, tracee, NULL, NULL);
                      goto repeat;
              }
    
              for (i = 0; i < 3; i++) {
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGSTOP\n");
                      kill(tracee, SIGSTOP);
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGCONT\n");
                      kill(tracee, SIGCONT);
              }
              nanosleep(&ts1s, NULL);
    
              kill(tracer, SIGKILL);
              kill(tracee, SIGKILL);
              return 0;
      }
    
    In the above program, tracer keeps tracee running and gets
    notification of each group stop state changes.
    
      # ./test-notify
      tracer: stopped=0 signo=5
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8bd84b83a35b..1854def284f5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1811,15 +1811,17 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
 #define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
 #define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
+#define JOBCTL_TRAP_NOTIFY_BIT	20	/* trap for NOTIFY */
 #define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 
 #define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
 #define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
 #define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
 #define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
+#define JOBCTL_TRAP_NOTIFY	(1 << JOBCTL_TRAP_NOTIFY_BIT)
 #define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
 
-#define JOBCTL_TRAP_MASK	JOBCTL_TRAP_STOP
+#define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 #define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 
 extern bool task_set_jobctl_pending(struct task_struct *task,

commit 73ddff2bee159ffb580bd24faf625cd5e628f5ec
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 14 11:20:14 2011 +0200

    job control: introduce JOBCTL_TRAP_STOP and use it for group stop trap
    
    do_signal_stop() implemented both normal group stop and trap for group
    stop while ptraced.  This approach has been enough but scheduled
    changes require trap mechanism which can be used in more generic
    manner and using group stop trap for generic trap site simplifies both
    userland visible interface and implementation.
    
    This patch adds a new jobctl flag - JOBCTL_TRAP_STOP.  When set, it
    triggers a trap site, which behaves like group stop trap, in
    get_signal_to_deliver() after checking for pending signals.  While
    ptraced, do_signal_stop() doesn't stop itself.  It initiates group
    stop if requested and schedules JOBCTL_TRAP_STOP and returns.  The
    caller - get_signal_to_deliver() - is responsible for checking whether
    TRAP_STOP is pending afterwards and handling it.
    
    ptrace_attach() is updated to use JOBCTL_TRAP_STOP instead of
    JOBCTL_STOP_PENDING and __ptrace_unlink() to clear all pending trap
    bits and TRAPPING so that TRAP_STOP and future trap bits don't linger
    after detach.
    
    While at it, add proper function comment to do_signal_stop() and make
    it return bool.
    
    -v2: __ptrace_unlink() updated to clear JOBCTL_TRAP_MASK and TRAPPING
         instead of JOBCTL_PENDING_MASK.  This avoids accidentally
         clearing JOBCTL_STOP_CONSUME.  Spotted by Oleg.
    
    -v3: do_signal_stop() updated to return %false without dropping
         siglock while ptraced and TRAP_STOP check moved inside for(;;)
         loop after group stop participation.  This avoids unnecessary
         relocking and also will help avoiding unnecessary traps by
         consuming group stop before handling pending traps.
    
    -v4: Jobctl trap handling moved into a separate function -
         do_jobctl_trap().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5157bd9eee37..8bd84b83a35b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1810,17 +1810,21 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define JOBCTL_STOP_DEQUEUED_BIT 16	/* stop signal dequeued */
 #define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
 #define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
+#define JOBCTL_TRAP_STOP_BIT	19	/* trap for STOP */
 #define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 
 #define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
 #define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
 #define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
+#define JOBCTL_TRAP_STOP	(1 << JOBCTL_TRAP_STOP_BIT)
 #define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
 
-#define JOBCTL_PENDING_MASK	JOBCTL_STOP_PENDING
+#define JOBCTL_TRAP_MASK	JOBCTL_TRAP_STOP
+#define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 
 extern bool task_set_jobctl_pending(struct task_struct *task,
 				    unsigned int mask);
+extern void task_clear_jobctl_trapping(struct task_struct *task);
 extern void task_clear_jobctl_pending(struct task_struct *task,
 				      unsigned int mask);
 

commit bdd4e85dc36cdbcfc1608a5b2a17c80a9db8986a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 8 01:13:27 2011 +0200

    sched: Isolate preempt counting in its own config option
    
    Create a new CONFIG_PREEMPT_COUNT that handles the inc/dec
    of preempt count offset independently. So that the offset
    can be updated by preempt_disable() and preempt_enable()
    even without the need for CONFIG_PREEMPT beeing set.
    
    This prepares to make CONFIG_DEBUG_SPINLOCK_SLEEP working
    with !CONFIG_PREEMPT where it currently doesn't detect
    code that sleeps inside explicit preemption disabled
    sections.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 483c1ed5bc4d..4ecd5cbe7e24 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2502,7 +2502,7 @@ extern int _cond_resched(void);
 
 extern int __cond_resched_lock(spinlock_t *lock);
 
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPT_COUNT
 #define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
 #else
 #define PREEMPT_LOCK_OFFSET	0

commit 6715a52a581c891e9a2034abe1c81ddb482d70b3
Merge: ef2398019b30 6c6c54e1807f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 7 19:20:28 2011 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: Fix/clarify set_task_cpu() locking rules
      lockdep: Fix lock_is_held() on recursion
      sched: Fix schedstat.nr_wakeups_migrate
      sched: Fix cross-cpu clock sync on remote wakeups

commit 7dd3db54e77d21eb95e145f19ba53f68250d0e73
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 2 11:14:00 2011 +0200

    job control: introduce task_set_jobctl_pending()
    
    task->jobctl currently hosts JOBCTL_STOP_PENDING and will host TRAP
    pending bits too.  Setting pending conditions on a dying task may make
    the task unkillable.  Currently, each setting site is responsible for
    checking for the condition but with to-be-added job control traps this
    becomes too fragile.
    
    This patch adds task_set_jobctl_pending() which should be used when
    setting task->jobctl bits to schedule a stop or trap.  The function
    performs the followings to ease setting pending bits.
    
    * Sanity checks.
    
    * If fatal signal is pending or PF_EXITING is set, no bit is set.
    
    * STOP_SIGMASK is automatically cleared if new value is being set.
    
    do_signal_stop() and ptrace_attach() are updated to use
    task_set_jobctl_pending() instead of setting STOP_PENDING explicitly.
    The surrounding structures around setting are changed to fit
    task_set_jobctl_pending() better but there should be no userland
    visible behavior difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a958b17f9fe..5157bd9eee37 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1819,6 +1819,8 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 
 #define JOBCTL_PENDING_MASK	JOBCTL_STOP_PENDING
 
+extern bool task_set_jobctl_pending(struct task_struct *task,
+				    unsigned int mask);
 extern void task_clear_jobctl_pending(struct task_struct *task,
 				      unsigned int mask);
 

commit 3759a0d94c18764247b66511d1038f2b93aa95de
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 2 11:14:00 2011 +0200

    job control: introduce JOBCTL_PENDING_MASK and task_clear_jobctl_pending()
    
    This patch introduces JOBCTL_PENDING_MASK and replaces
    task_clear_jobctl_stop_pending() with task_clear_jobctl_pending()
    which takes an extra @mask argument.
    
    JOBCTL_PENDING_MASK is currently equal to JOBCTL_STOP_PENDING but
    future patches will add more bits.  recalc_sigpending_tsk() is updated
    to use JOBCTL_PENDING_MASK instead.
    
    task_clear_jobctl_pending() takes @mask which in subset of
    JOBCTL_PENDING_MASK and clears the relevant jobctl bits.  If
    JOBCTL_STOP_PENDING is set, other STOP bits are cleared together.  All
    task_clear_jobctl_stop_pending() users are updated to call
    task_clear_jobctl_pending() with JOBCTL_STOP_PENDING which is
    functionally identical to task_clear_jobctl_stop_pending().
    
    This patch doesn't cause any functional change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0dd064eb4fc..5a958b17f9fe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1817,7 +1817,10 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
 #define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
 
-extern void task_clear_jobctl_stop_pending(struct task_struct *task);
+#define JOBCTL_PENDING_MASK	JOBCTL_STOP_PENDING
+
+extern void task_clear_jobctl_pending(struct task_struct *task,
+				      unsigned int mask);
 
 #ifdef CONFIG_PREEMPT_RCU
 

commit a8f072c1d624a627b67f2ace2f0c25d856ef4e54
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jun 2 11:13:59 2011 +0200

    job control: rename signal->group_stop and flags to jobctl and update them
    
    signal->group_stop currently hosts mostly group stop related flags;
    however, it's gonna be used for wider purposes and the GROUP_STOP_
    flag prefix becomes confusing.  Rename signal->group_stop to
    signal->jobctl and rename all GROUP_STOP_* flags to JOBCTL_*.
    
    Bit position macros JOBCTL_*_BIT are defined and JOBCTL_* flags are
    defined in terms of them to allow using bitops later.
    
    While at it, reassign JOBCTL_TRAPPING to bit 22 to better accomodate
    future additions.
    
    This doesn't cause any functional change.
    
    -v2: JOBCTL_*_BIT macros added as suggested by Linus.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2a8621c4be1e..b0dd064eb4fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1282,7 +1282,7 @@ struct task_struct {
 	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
-	unsigned int group_stop;	/* GROUP_STOP_*, siglock protected */
+	unsigned int jobctl;	/* JOBCTL_*, siglock protected */
 	/* ??? */
 	unsigned int personality;
 	unsigned did_exec:1;
@@ -1803,15 +1803,21 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define used_math() tsk_used_math(current)
 
 /*
- * task->group_stop flags
+ * task->jobctl flags
  */
-#define GROUP_STOP_SIGMASK	0xffff    /* signr of the last group stop */
-#define GROUP_STOP_PENDING	(1 << 16) /* task should stop for group stop */
-#define GROUP_STOP_CONSUME	(1 << 17) /* consume group stop count */
-#define GROUP_STOP_TRAPPING	(1 << 18) /* switching from STOPPED to TRACED */
-#define GROUP_STOP_DEQUEUED	(1 << 19) /* stop signal dequeued */
+#define JOBCTL_STOP_SIGMASK	0xffff	/* signr of the last group stop */
 
-extern void task_clear_group_stop_pending(struct task_struct *task);
+#define JOBCTL_STOP_DEQUEUED_BIT 16	/* stop signal dequeued */
+#define JOBCTL_STOP_PENDING_BIT	17	/* task should stop for group stop */
+#define JOBCTL_STOP_CONSUME_BIT	18	/* consume group stop count */
+#define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
+
+#define JOBCTL_STOP_DEQUEUED	(1 << JOBCTL_STOP_DEQUEUED_BIT)
+#define JOBCTL_STOP_PENDING	(1 << JOBCTL_STOP_PENDING_BIT)
+#define JOBCTL_STOP_CONSUME	(1 << JOBCTL_STOP_CONSUME_BIT)
+#define JOBCTL_TRAPPING		(1 << JOBCTL_TRAPPING_BIT)
+
+extern void task_clear_jobctl_stop_pending(struct task_struct *task);
 
 #ifdef CONFIG_PREEMPT_RCU
 

commit f339b9dc1f03591761d5d930800db24bc0eda1e1
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 31 10:49:20 2011 +0200

    sched: Fix schedstat.nr_wakeups_migrate
    
    While looking over the code I found that with the ttwu rework the
    nr_wakeups_migrate test broke since we now switch cpus prior to
    calling ttwu_stat(), hence the test is always true.
    
    Cure this by passing the migration state in wake_flags. Also move the
    whole test under CONFIG_SMP, its hard to migrate tasks on UP :-)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-pwwxl7gdqs5676f1d4cx6pj7@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8da84b7bc1b8..483c1ed5bc4d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1063,6 +1063,7 @@ struct sched_domain;
  */
 #define WF_SYNC		0x01		/* waker goes to sleep after wakup */
 #define WF_FORK		0x02		/* child wakeup after fork */
+#define WF_MIGRATED	0x04		/* internal use, task got migrated */
 
 #define ENQUEUE_WAKEUP		1
 #define ENQUEUE_HEAD		2

commit 6345d24daf0c1fffe6642081d783cdf653ebaa5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:32:28 2011 -0700

    mm: Fix boot crash in mm_alloc()
    
    Thomas Gleixner reports that we now have a boot crash triggered by
    CONFIG_CPUMASK_OFFSTACK=y:
    
        BUG: unable to handle kernel NULL pointer dereference at   (null)
        IP: [<c11ae035>] find_next_bit+0x55/0xb0
        Call Trace:
         [<c11addda>] cpumask_any_but+0x2a/0x70
         [<c102396b>] flush_tlb_mm+0x2b/0x80
         [<c1022705>] pud_populate+0x35/0x50
         [<c10227ba>] pgd_alloc+0x9a/0xf0
         [<c103a3fc>] mm_init+0xec/0x120
         [<c103a7a3>] mm_alloc+0x53/0xd0
    
    which was introduced by commit de03c72cfce5 ("mm: convert
    mm->cpu_vm_cpumask into cpumask_var_t"), and is due to wrong ordering of
    mm_init() vs mm_init_cpumask
    
    Thomas wrote a patch to just fix the ordering of initialization, but I
    hate the new double allocation in the fork path, so I ended up instead
    doing some more radical surgery to clean it all up.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bcddd0138105..2a8621c4be1e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2194,7 +2194,6 @@ static inline void mmdrop(struct mm_struct * mm)
 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 		__mmdrop(mm);
 }
-extern int mm_init_cpumask(struct mm_struct *mm, struct mm_struct *oldmm);
 
 /* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);

commit 08a8b79600101fd6e13dcf05409b330e7f5b0478
Merge: 1ba4b8cb94e5 1e1b6c511d1b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 28 12:56:46 2011 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      cpuset: Fix cpuset_cpus_allowed_fallback(), don't update tsk->rt.nr_cpus_allowed
      sched: Fix ->min_vruntime calculation in dequeue_entity()
      sched: Fix ttwu() for __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: More sched_domain iterations fixes

commit c4a227d89f758e582fd167bb15245f2704de99ef
Merge: 87367a0b71a5 f506b3dc0ec4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 28 12:55:55 2011 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (25 commits)
      perf: Fix SIGIO handling
      perf top: Don't stop if no kernel symtab is found
      perf top: Handle kptr_restrict
      perf top: Remove unused macro
      perf events: initialize fd array to -1 instead of 0
      perf tools: Make sure kptr_restrict warnings fit 80 col terms
      perf tools: Fix build on older systems
      perf symbols: Handle /proc/sys/kernel/kptr_restrict
      perf: Remove duplicate headers
      ftrace: Add internal recursive checks
      tracing: Update btrfs's tracepoints to use u64 interface
      tracing: Add __print_symbolic_u64 to avoid warnings on 32bit machine
      ftrace: Set ops->flag to enabled even on static function tracing
      tracing: Have event with function tracer check error return
      ftrace: Have ftrace_startup() return failure code
      jump_label: Check entries limit in __jump_label_update
      ftrace/recordmcount: Avoid STT_FUNC symbols as base on ARM
      scripts/tags.sh: Add magic for trace-events for etags too
      scripts/tags.sh: Fix ctags for DEFINE_EVENT()
      x86/ftrace: Fix compiler warning in ftrace.c
      ...

commit 1e1b6c511d1b23cb7c3b619d82fc7bd9f620565d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu May 19 15:08:58 2011 +0900

    cpuset: Fix cpuset_cpus_allowed_fallback(), don't update tsk->rt.nr_cpus_allowed
    
    The rule is, we have to update tsk->rt.nr_cpus_allowed if we change
    tsk->cpus_allowed. Otherwise RT scheduler may confuse.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4DD4B3FA.5060901@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc8871295a5a..8da84b7bc1b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1841,9 +1841,16 @@ static inline void rcu_copy_process(struct task_struct *p)
 #endif
 
 #ifdef CONFIG_SMP
+extern void do_set_cpus_allowed(struct task_struct *p,
+			       const struct cpumask *new_mask);
+
 extern int set_cpus_allowed_ptr(struct task_struct *p,
 				const struct cpumask *new_mask);
 #else
+static inline void do_set_cpus_allowed(struct task_struct *p,
+				      const struct cpumask *new_mask)
+{
+}
 static inline int set_cpus_allowed_ptr(struct task_struct *p,
 				       const struct cpumask *new_mask)
 {

commit d6a72fe465f4c54654a1d5488daeb820b4ecf275
Merge: b1d2dc3c06d8 b1cff0ad1062
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri May 27 14:28:09 2011 +0200

    Merge branch 'tip/perf/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/urgent

commit 4714d1d32d97239fb5ae3e10521d3f133a899b66
Author: Ben Blum <bblum@andrew.cmu.edu>
Date:   Thu May 26 16:25:18 2011 -0700

    cgroups: read-write lock CLONE_THREAD forking per threadgroup
    
    Adds functionality to read/write lock CLONE_THREAD fork()ing per-threadgroup
    
    Add an rwsem that lives in a threadgroup's signal_struct that's taken for
    reading in the fork path, under CONFIG_CGROUPS.  If another part of the
    kernel later wants to use such a locking mechanism, the CONFIG_CGROUPS
    ifdefs should be changed to a higher-up flag that CGROUPS and the other
    system would both depend on.
    
    This is a pre-patch for cgroup-procs-write.patch.
    
    Signed-off-by: Ben Blum <bblum@andrew.cmu.edu>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f18300eddfcb..dc8871295a5a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -513,6 +513,7 @@ struct thread_group_cputimer {
 	spinlock_t lock;
 };
 
+#include <linux/rwsem.h>
 struct autogroup;
 
 /*
@@ -632,6 +633,16 @@ struct signal_struct {
 	unsigned audit_tty;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
+#ifdef CONFIG_CGROUPS
+	/*
+	 * The threadgroup_fork_lock prevents threads from forking with
+	 * CLONE_THREAD while held for writing. Use this for fork-sensitive
+	 * threadgroup-wide operations. It's taken for reading in fork.c in
+	 * copy_process().
+	 * Currently only needed write-side by cgroups.
+	 */
+	struct rw_semaphore threadgroup_fork_lock;
+#endif
 
 	int oom_adj;		/* OOM kill score adjustment (bit shift) */
 	int oom_score_adj;	/* OOM kill score adjustment */
@@ -2323,6 +2334,31 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
 }
 
+/* See the declaration of threadgroup_fork_lock in signal_struct. */
+#ifdef CONFIG_CGROUPS
+static inline void threadgroup_fork_read_lock(struct task_struct *tsk)
+{
+	down_read(&tsk->signal->threadgroup_fork_lock);
+}
+static inline void threadgroup_fork_read_unlock(struct task_struct *tsk)
+{
+	up_read(&tsk->signal->threadgroup_fork_lock);
+}
+static inline void threadgroup_fork_write_lock(struct task_struct *tsk)
+{
+	down_write(&tsk->signal->threadgroup_fork_lock);
+}
+static inline void threadgroup_fork_write_unlock(struct task_struct *tsk)
+{
+	up_write(&tsk->signal->threadgroup_fork_lock);
+}
+#else
+static inline void threadgroup_fork_read_lock(struct task_struct *tsk) {}
+static inline void threadgroup_fork_read_unlock(struct task_struct *tsk) {}
+static inline void threadgroup_fork_write_lock(struct task_struct *tsk) {}
+static inline void threadgroup_fork_write_unlock(struct task_struct *tsk) {}
+#endif
+
 #ifndef __HAVE_THREAD_FUNCTIONS
 
 #define task_thread_info(task)	((struct thread_info *)(task)->stack)

commit b1cff0ad1062621ae63cb6c5dc4165191fe2e9f1
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed May 25 14:27:43 2011 -0400

    ftrace: Add internal recursive checks
    
    Witold reported a reboot caused by the selftests of the dynamic function
    tracer. He sent me a config and I used ktest to do a config_bisect on it
    (as my config did not cause the crash). It pointed out that the problem
    config was CONFIG_PROVE_RCU.
    
    What happened was that if multiple callbacks are attached to the
    function tracer, we iterate a list of callbacks. Because the list is
    managed by synchronize_sched() and preempt_disable, the access to the
    pointers uses rcu_dereference_raw().
    
    When PROVE_RCU is enabled, the rcu_dereference_raw() calls some
    debugging functions, which happen to be traced. The tracing of the debug
    function would then call rcu_dereference_raw() which would then call the
    debug function and then... well you get the idea.
    
    I first wrote two different patches to solve this bug.
    
    1) add a __rcu_dereference_raw() that would not do any checks.
    2) add notrace to the offending debug functions.
    
    Both of these patches worked.
    
    Talking with Paul McKenney on IRC, he suggested to add recursion
    detection instead. This seemed to be a better solution, so I decided to
    implement it. As the task_struct already has a trace_recursion to detect
    recursion in the ring buffer, and that has a very small number it
    allows, I decided to use that same variable to add flags that can detect
    the recursion inside the infrastructure of the function tracer.
    
    I plan to change it so that the task struct bit can be checked in
    mcount, but as that requires changes to all archs, I will hold that off
    to the next merge window.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1306348063.1465.116.camel@gandalf.stny.rr.com
    Reported-by: Witold Baryluk <baryluk@smp.if.uj.edu.pl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d8b2d0bec0d8..7b78d9cad471 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1513,7 +1513,7 @@ struct task_struct {
 #ifdef CONFIG_TRACING
 	/* state flags for use by tracers */
 	unsigned long trace;
-	/* bitmask of trace recursion */
+	/* bitmask and counter of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
 #ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */

commit de03c72cfce5b263a674d04348b58475ec50163c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue May 24 17:12:15 2011 -0700

    mm: convert mm->cpu_vm_cpumask into cpumask_var_t
    
    cpumask_t is very big struct and cpu_vm_mask is placed wrong position.
    It might lead to reduce cache hit ratio.
    
    This patch has two change.
    1) Move the place of cpumask into last of mm_struct. Because usually cpumask
       is accessed only front bits when the system has cpu-hotplug capability
    2) Convert cpu_vm_mask into cpumask_var_t. It may help to reduce memory
       footprint if cpumask_size() will use nr_cpumask_bits properly in future.
    
    In addition, this patch change the name of cpu_vm_mask with cpu_vm_mask_var.
    It may help to detect out of tree cpu_vm_mask users.
    
    This patch has no functional change.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 44b8faaac7c0..f18300eddfcb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2176,6 +2176,7 @@ static inline void mmdrop(struct mm_struct * mm)
 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 		__mmdrop(mm);
 }
+extern int mm_init_cpumask(struct mm_struct *mm, struct mm_struct *oldmm);
 
 /* mmput gets rid of the mappings and all user-space */
 extern void mmput(struct mm_struct *);

commit 72788c385604523422592249c19cba0187021e9b
Author: David Rientjes <rientjes@google.com>
Date:   Tue May 24 17:11:40 2011 -0700

    oom: replace PF_OOM_ORIGIN with toggling oom_score_adj
    
    There's a kernel-wide shortage of per-process flags, so it's always
    helpful to trim one when possible without incurring a significant penalty.
     It's even more important when you're planning on adding a per- process
    flag yourself, which I plan to do shortly for transparent hugepages.
    
    PF_OOM_ORIGIN is used by ksm and swapoff to prefer current since it has a
    tendency to allocate large amounts of memory and should be preferred for
    killing over other tasks.  We'd rather immediately kill the task making
    the errant syscall rather than penalizing an innocent task.
    
    This patch removes PF_OOM_ORIGIN since its behavior is equivalent to
    setting the process's oom_score_adj to OOM_SCORE_ADJ_MAX.
    
    The process's old oom_score_adj is stored and then set to
    OOM_SCORE_ADJ_MAX during the time it used to have PF_OOM_ORIGIN.  The old
    value is then reinstated when the process should no longer be considered a
    high priority for oom killing.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Izik Eidus <ieidus@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aaf71e08222c..44b8faaac7c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1753,7 +1753,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
 #define PF_KSWAPD	0x00040000	/* I am kswapd */
-#define PF_OOM_ORIGIN	0x00080000	/* Allocating much memory to others */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
 #define PF_KTHREAD	0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */

commit 15a3d11b0f2ebdfb3591e411e268aa81998d4723
Merge: 1f3a8e093f47 c8b281161dfa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 12:53:48 2011 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: Increase SCHED_LOAD_SCALE resolution
      sched: Introduce SCHED_POWER_SCALE to scale cpu_power calculations
      sched: Cleanup set_load_weight()

commit 19504828b4bee5e471bcd35e214bc6fd0d380692
Merge: 57d19e80f459 3cb6d1540880
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 09:25:52 2011 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      perf tools: Fix sample size bit operations
      perf tools: Fix ommitted mmap data update on remap
      watchdog: Change the default timeout and configure nmi watchdog period based on watchdog_thresh
      watchdog: Disable watchdog when thresh is zero
      watchdog: Only disable/enable watchdog if neccessary
      watchdog: Fix rounding bug in get_sample_period()
      perf tools: Propagate event parse error handling
      perf tools: Robustify dynamic sample content fetch
      perf tools: Pre-check sample size before parsing
      perf tools: Move evlist sample helpers to evlist area
      perf tools: Remove junk code in mmap size handling
      perf tools: Check we are able to read the event size on mmap

commit 586692a5a5fc5740c8a46abc0f2365495c2d7c5f
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Sun May 22 22:10:22 2011 -0700

    watchdog: Disable watchdog when thresh is zero
    
    This restores the previous behavior of softlock_thresh.
    
    Currently, setting watchdog_thresh to zero causes the watchdog
    kthreads to consume a lot of CPU.
    
    In addition, the logic of proc_dowatchdog_thresh and
    proc_dowatchdog_enabled has been factored into proc_dowatchdog.
    
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Cc: Marcin Slusarz <marcin.slusarz@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1306127423-3347-3-git-send-email-msb@chromium.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <20110517071018.GE22305@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 12211e1666e2..d8b2d0bec0d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -315,7 +315,6 @@ extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
 				  void __user *buffer,
 				  size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
-extern int softlockup_thresh;
 void lockup_detector_init(void);
 #else
 static inline void touch_softlockup_watchdog(void)

commit 3ed4c0583daa34dedb568b26ff99e5a7b58db612
Merge: ad9471752eba bd715d9a4f13
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 13:33:21 2011 -0700

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc
    
    * 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc: (41 commits)
      signal: trivial, fix the "timespec declared inside parameter list" warning
      job control: reorganize wait_task_stopped()
      ptrace: fix signal->wait_chldexit usage in task_clear_group_stop_trapping()
      signal: sys_sigprocmask() needs retarget_shared_pending()
      signal: cleanup sys_sigprocmask()
      signal: rename signandsets() to sigandnsets()
      signal: do_sigtimedwait() needs retarget_shared_pending()
      signal: introduce do_sigtimedwait() to factor out compat/native code
      signal: sys_rt_sigtimedwait: simplify the timeout logic
      signal: cleanup sys_rt_sigprocmask()
      x86: signal: sys_rt_sigreturn() should use set_current_blocked()
      x86: signal: handle_signal() should use set_current_blocked()
      signal: sigprocmask() should do retarget_shared_pending()
      signal: sigprocmask: narrow the scope of ->siglock
      signal: retarget_shared_pending: optimize while_each_thread() loop
      signal: retarget_shared_pending: consider shared/unblocked signals only
      signal: introduce retarget_shared_pending()
      ptrace: ptrace_check_attach() should not do s/STOPPED/TRACED/
      signal: Turn SIGNAL_STOP_DEQUEUED into GROUP_STOP_DEQUEUED
      signal: do_signal_stop: Remove the unneeded task_clear_group_stop_pending()
      ...

commit c8b281161dfa4bb5d5be63fb036ce19347b88c63
Author: Nikhil Rao <ncrao@google.com>
Date:   Wed May 18 14:37:48 2011 -0700

    sched: Increase SCHED_LOAD_SCALE resolution
    
    Introduce SCHED_LOAD_RESOLUTION, which scales is added to
    SCHED_LOAD_SHIFT and increases the resolution of
    SCHED_LOAD_SCALE. This patch sets the value of
    SCHED_LOAD_RESOLUTION to 10, scaling up the weights for all
    sched entities by a factor of 1024. With this extra resolution,
    we can handle deeper cgroup hiearchies and the scheduler can do
    better shares distribution and load load balancing on larger
    systems (especially for low weight task groups).
    
    This does not change the existing user interface, the scaled
    weights are only used internally. We do not modify
    prio_to_weight values or inverses, but use the original weights
    when calculating the inverse which is used to scale execution
    time delta in calc_delta_mine(). This ensures we do not lose
    accuracy when accounting time to the sched entities. Thanks to
    Nikunj Dadhania for fixing an bug in c_d_m() that broken fairness.
    
    Below is some analysis of the performance costs/improvements of
    this patch.
    
    1. Micro-arch performance costs:
    
    Experiment was to run Ingo's pipe_test_100k 200 times with the
    task pinned to one cpu. I measured instruction, cycles and
    stalled-cycles for the runs. See:
    
       http://thread.gmane.org/gmane.linux.kernel/1129232/focus=1129389
    
    for more info.
    
    -tip (baseline):
    
     Performance counter stats for '/root/load-scale/pipe-test-100k' (200 runs):
    
           964,991,769 instructions             #    0.82  insns per cycle
                                                #    0.33  stalled cycles per insn
                                                #    ( +-  0.05% )
         1,171,186,635 cycles                   #    0.000 GHz                      ( +-  0.08% )
           306,373,664 stalled-cycles-backend   #   26.16% backend  cycles idle     ( +-  0.28% )
           314,933,621 stalled-cycles-frontend  #   26.89% frontend cycles idle     ( +-  0.34% )
    
            1.122405684  seconds time elapsed  ( +-  0.05% )
    
    -tip+patches:
    
     Performance counter stats for './load-scale/pipe-test-100k' (200 runs):
    
           963,624,821 instructions             #    0.82  insns per cycle
                                                #    0.33  stalled cycles per insn
                                                #    ( +-  0.04% )
         1,175,215,649 cycles                   #    0.000 GHz                      ( +-  0.08% )
           315,321,126 stalled-cycles-backend   #   26.83% backend  cycles idle     ( +-  0.28% )
           316,835,873 stalled-cycles-frontend  #   26.96% frontend cycles idle     ( +-  0.29% )
    
            1.122238659  seconds time elapsed  ( +-  0.06% )
    
    With this patch, instructions decrease by ~0.10% and cycles
    increase by 0.27%. This doesn't look statistically significant.
    The number of stalled cycles in the backend increased from
    26.16% to 26.83%. This can be attributed to the shifts we do in
    c_d_m() and other places. The fraction of stalled cycles in the
    frontend remains about the same, at 26.96% compared to 26.89% in -tip.
    
    2. Balancing low-weight task groups
    
    Test setup: run 50 tasks with random sleep/busy times (biased
    around 100ms) in a low weight container (with cpu.shares = 2).
    Measure %idle as reported by mpstat over a 10s window.
    
    -tip (baseline):
    
    06:47:48 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle    intr/s
    06:47:49 PM  all   94.32    0.00    0.06    0.00    0.00    0.00    0.00    0.00    5.62  15888.00
    06:47:50 PM  all   94.57    0.00    0.62    0.00    0.00    0.00    0.00    0.00    4.81  16180.00
    06:47:51 PM  all   94.69    0.00    0.06    0.00    0.00    0.00    0.00    0.00    5.25  15966.00
    06:47:52 PM  all   95.81    0.00    0.00    0.00    0.00    0.00    0.00    0.00    4.19  16053.00
    06:47:53 PM  all   94.88    0.06    0.00    0.00    0.00    0.00    0.00    0.00    5.06  15984.00
    06:47:54 PM  all   93.31    0.00    0.00    0.00    0.00    0.00    0.00    0.00    6.69  15806.00
    06:47:55 PM  all   94.19    0.00    0.06    0.00    0.00    0.00    0.00    0.00    5.75  15896.00
    06:47:56 PM  all   92.87    0.00    0.00    0.00    0.00    0.00    0.00    0.00    7.13  15716.00
    06:47:57 PM  all   94.88    0.00    0.00    0.00    0.00    0.00    0.00    0.00    5.12  15982.00
    06:47:58 PM  all   95.44    0.00    0.00    0.00    0.00    0.00    0.00    0.00    4.56  16075.00
    Average:     all   94.49    0.01    0.08    0.00    0.00    0.00    0.00    0.00    5.42  15954.60
    
    -tip+patches:
    
    06:47:03 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle    intr/s
    06:47:04 PM  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  16630.00
    06:47:05 PM  all   99.69    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.31  16580.20
    06:47:06 PM  all   99.69    0.00    0.06    0.00    0.00    0.00    0.00    0.00    0.25  16596.00
    06:47:07 PM  all   99.20    0.00    0.74    0.00    0.00    0.06    0.00    0.00    0.00  17838.61
    06:47:08 PM  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  16540.00
    06:47:09 PM  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  16575.00
    06:47:10 PM  all  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  16614.00
    06:47:11 PM  all   99.94    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.06  16588.00
    06:47:12 PM  all   99.94    0.00    0.06    0.00    0.00    0.00    0.00    0.00    0.00  16593.00
    06:47:13 PM  all   99.94    0.00    0.06    0.00    0.00    0.00    0.00    0.00    0.00  16551.00
    Average:     all   99.84    0.00    0.09    0.00    0.00    0.01    0.00    0.00    0.06  16711.58
    
    We see an improvement in idle% on the system (drops from 5.42% on -tip to 0.06%
    with the patches).
    
    We see an improvement in idle% on the system (drops from 5.42%
    on -tip to 0.06% with the patches).
    
    Signed-off-by: Nikhil Rao <ncrao@google.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Nikunj A. Dadhania <nikunj@linux.vnet.ibm.com>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Stephan Barwolf <stephan.baerwolf@tu-ilmenau.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1305754668-18792-1-git-send-email-ncrao@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f2f440221b70..c34a718e20dd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -788,9 +788,28 @@ enum cpu_idle_type {
 };
 
 /*
- * Increase resolution of nice-level calculations:
+ * Increase resolution of nice-level calculations for 64-bit architectures.
+ * The extra resolution improves shares distribution and load balancing of
+ * low-weight task groups (eg. nice +19 on an autogroup), deeper taskgroup
+ * hierarchies, especially on larger systems. This is not a user-visible change
+ * and does not change the user-interface for setting shares/weights.
+ *
+ * We increase resolution only if we have enough bits to allow this increased
+ * resolution (i.e. BITS_PER_LONG > 32). The costs for increasing resolution
+ * when BITS_PER_LONG <= 32 are pretty high and the returns do not justify the
+ * increased costs.
  */
-#define SCHED_LOAD_SHIFT	10
+#if BITS_PER_LONG > 32
+# define SCHED_LOAD_RESOLUTION	10
+# define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
+# define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)
+#else
+# define SCHED_LOAD_RESOLUTION	0
+# define scale_load(w)		(w)
+# define scale_load_down(w)	(w)
+#endif
+
+#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)
 #define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
 
 /*

commit 1399fa7807a1a5998bbf147e80668e9950661dfa
Author: Nikhil Rao <ncrao@google.com>
Date:   Wed May 18 10:09:39 2011 -0700

    sched: Introduce SCHED_POWER_SCALE to scale cpu_power calculations
    
    SCHED_LOAD_SCALE is used to increase nice resolution and to
    scale cpu_power calculations in the scheduler. This patch
    introduces SCHED_POWER_SCALE and converts all uses of
    SCHED_LOAD_SCALE for scaling cpu_power to use SCHED_POWER_SCALE
    instead.
    
    This is a preparatory patch for increasing the resolution of
    SCHED_LOAD_SCALE, and there is no need to increase resolution
    for cpu_power calculations.
    
    Signed-off-by: Nikhil Rao <ncrao@google.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Nikunj A. Dadhania <nikunj@linux.vnet.ibm.com>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Stephan Barwolf <stephan.baerwolf@tu-ilmenau.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/1305738580-9924-3-git-send-email-ncrao@google.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 12211e1666e2..f2f440221b70 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -787,18 +787,21 @@ enum cpu_idle_type {
 	CPU_MAX_IDLE_TYPES
 };
 
-/*
- * sched-domains (multiprocessor balancing) declarations:
- */
-
 /*
  * Increase resolution of nice-level calculations:
  */
 #define SCHED_LOAD_SHIFT	10
 #define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
 
-#define SCHED_LOAD_SCALE_FUZZ	SCHED_LOAD_SCALE
+/*
+ * Increase resolution of cpu_power calculations
+ */
+#define SCHED_POWER_SHIFT	10
+#define SCHED_POWER_SCALE	(1L << SCHED_POWER_SHIFT)
 
+/*
+ * sched-domains (multiprocessor balancing) declarations:
+ */
 #ifdef CONFIG_SMP
 #define SD_LOAD_BALANCE		0x0001	/* Do load balancing on this domain. */
 #define SD_BALANCE_NEWIDLE	0x0002	/* Balance when about to become idle */

commit 3e51e3edfd81bfd9853ad7de91167e4ce33d0fe7
Author: Samir Bellabes <sam@synack.fr>
Date:   Wed May 11 18:18:05 2011 +0200

    sched: Remove unused parameters from sched_fork() and wake_up_new_task()
    
    sched_fork() and wake_up_new_task() are defined with a parameter
    'unsigned long clone_flags', which is unused.
    
    This patch removes the parameters.
    
    Signed-off-by: Samir Bellabes <sam@synack.fr>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1305130685-1047-1-git-send-email-sam@synack.fr
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6b4280b23ee6..12211e1666e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2051,14 +2051,13 @@ extern void xtime_update(unsigned long ticks);
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
-extern void wake_up_new_task(struct task_struct *tsk,
-				unsigned long clone_flags);
+extern void wake_up_new_task(struct task_struct *tsk);
 #ifdef CONFIG_SMP
  extern void kick_process(struct task_struct *tsk);
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void sched_fork(struct task_struct *p, int clone_flags);
+extern void sched_fork(struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);

commit 9cb5baba5e3acba0994ad899ee908799104c9965
Merge: 7142d17e8f93 693d92a1bbc9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 12 09:36:18 2011 +0200

    Merge commit 'v2.6.39-rc7' into sched/core

commit bf26c018490c2fce7fe9b629083b96ce0e6ad019
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Apr 7 16:53:20 2011 +0200

    ptrace: Prepare to fix racy accesses on task breakpoints
    
    When a task is traced and is in a stopped state, the tracer
    may execute a ptrace request to examine the tracee state and
    get its task struct. Right after, the tracee can be killed
    and thus its breakpoints released.
    This can happen concurrently when the tracer is in the middle
    of reading or modifying these breakpoints, leading to dereferencing
    a freed pointer.
    
    Hence, to prepare the fix, create a generic breakpoint reference
    holding API. When a reference on the breakpoints of a task is
    held, the breakpoints won't be released until the last reference
    is dropped. After that, no more ptrace request on the task's
    breakpoints can be serviced for the tracer.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: v2.6.33.. <stable@kernel.org>
    Link: http://lkml.kernel.org/r/1302284067-7860-2-git-send-email-fweisbec@gmail.com

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 18d63cea2848..781abd137673 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1537,6 +1537,9 @@ struct task_struct {
 		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
 	} memcg_batch;
 #endif
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	atomic_t ptrace_bp_refcnt;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 625f2a378e5a10f45fdc37932fc9f8a21676de9e
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Fri Apr 22 11:19:10 2011 -0600

    sched: Get rid of lock_depth
    
    Neil Brown pointed out that lock_depth somehow escaped the BKL
    removal work.  Let's get rid of it now.
    
    Note that the perf scripting utilities still have a bunch of
    code for dealing with common_lock_depth in tracepoints; I have
    left that in place in case anybody wants to use that code with
    older kernels.
    
    Suggested-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110422111910.456c0e84@bike.lwn.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 171ba24b08a7..013314a56105 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -731,10 +731,6 @@ struct sched_info {
 	/* timestamps */
 	unsigned long long last_arrival,/* when we last ran on a cpu */
 			   last_queued;	/* when we were last queued to run */
-#ifdef CONFIG_SCHEDSTATS
-	/* BKL stats */
-	unsigned int bkl_count;
-#endif
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 
@@ -1190,8 +1186,6 @@ struct task_struct {
 	unsigned int flags;	/* per process flags, defined below */
 	unsigned int ptrace;
 
-	int lock_depth;		/* BKL lock depth */
-
 #ifdef CONFIG_SMP
 	struct task_struct *wake_entry;
 	int on_cpu;

commit 42ac9e87fdd89b77fa2ca0a5226023c1c2d83226
Merge: 057f3fadb347 f0e615c3cb72
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Apr 21 11:39:21 2011 +0200

    Merge commit 'v2.6.39-rc4' into sched/core
    
    Merge reason: Pick up upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6ddafdaab3f809b110ada253d2f2d4910ebd3ac5
Merge: 3905c54f2bd2 bd8e7dded88a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 18 14:53:18 2011 +0200

    Merge branch 'sched/locking' into sched/core
    
    Merge reason: the rq locking changes are stable,
                  propagate them into the .40 queue.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4471a675dfc7ca676c165079e91c712b09dc9ce4
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Thu Apr 14 15:22:09 2011 -0700

    brk: COMPAT_BRK: fix detection of randomized brk
    
    5520e89 ("brk: fix min_brk lower bound computation for COMPAT_BRK")
    tried to get the whole logic of brk randomization for legacy
    (libc5-based) applications finally right.
    
    It turns out that the way to detect whether brk has actually been
    randomized in the end or not introduced by that patch still doesn't work
    for those binaries, as reported by Geert:
    
    : /sbin/init from my old m68k ramdisk exists prematurely.
    :
    : Before the patch:
    :
    : | brk(0x80005c8e)                         = 0x80006000
    :
    : After the patch:
    :
    : | brk(0x80005c8e)                         = 0x80005c8e
    :
    : Old libc5 considers brk() to have failed if the return value is not
    : identical to the requested value.
    
    I don't like it, but currently see no better option than a bit flag in
    task_struct to catch the CONFIG_COMPAT_BRK && randomize_va_space == 2
    case.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Tested-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ec2c027e92c..18d63cea2848 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1254,6 +1254,9 @@ struct task_struct {
 #endif
 
 	struct mm_struct *mm, *active_mm;
+#ifdef CONFIG_COMPAT_BRK
+	unsigned brk_randomized:1;
+#endif
 #if defined(SPLIT_RSS_COUNTING)
 	struct task_rss_stat	rss_stat;
 #endif

commit 317f394160e9beb97d19a84c39b7e5eb3d7815a8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:58 2011 +0200

    sched: Move the second half of ttwu() to the remote cpu
    
    Now that we've removed the rq->lock requirement from the first part of
    ttwu() and can compute placement without holding any rq->lock, ensure
    we execute the second half of ttwu() on the actual cpu we want the
    task to run on.
    
    This avoids having to take rq->lock and doing the task enqueue
    remotely, saving lots on cacheline transfers.
    
    As measured using: http://oss.oracle.com/~mason/sembench.c
    
      $ for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ; do echo performance > $i; done
      $ echo 4096 32000 64 128 > /proc/sys/kernel/sem
      $ ./sembench -t 2048 -w 1900 -o 0
    
      unpatched: run time 30 seconds 647278 worker burns per second
      patched:   run time 30 seconds 816715 worker burns per second
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152729.515897185@chello.nl

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 25c50317ddc1..e09dafa6e149 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1203,6 +1203,7 @@ struct task_struct {
 	int lock_depth;		/* BKL lock depth */
 
 #ifdef CONFIG_SMP
+	struct task_struct *wake_entry;
 	int on_cpu;
 #endif
 	int on_rq;
@@ -2192,7 +2193,7 @@ extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
-static inline void scheduler_ipi(void) { }
+void scheduler_ipi(void);
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
 static inline void scheduler_ipi(void) { }

commit a8e4f2eaecc9bfa4954adf79a04f4f22fddd829c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:49 2011 +0200

    sched: Delay task_contributes_to_load()
    
    In prepratation of having to call task_contributes_to_load() without
    holding rq->lock, we need to store the result until we do and can
    update the rq accounting accordingly.
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152729.151523907@chello.nl

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7f5732f8c618..25c50317ddc1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1273,6 +1273,7 @@ struct task_struct {
 
 	/* Revert to default priority/policy when forking */
 	unsigned sched_reset_on_fork:1;
+	unsigned sched_contributes_to_load:1;
 
 	pid_t pid;
 	pid_t tgid;

commit 74f8e4b2335de45485b8d5b31a504747f13c8070
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:47 2011 +0200

    sched: Remove rq argument to sched_class::task_waking()
    
    In preparation of calling this without rq->lock held, remove the
    dependency on the rq argument.
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110405152729.071474242@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff4e2f9c24a7..7f5732f8c618 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1048,8 +1048,12 @@ struct sched_domain;
 #define WF_FORK		0x02		/* child wakeup after fork */
 
 #define ENQUEUE_WAKEUP		1
-#define ENQUEUE_WAKING		2
-#define ENQUEUE_HEAD		4
+#define ENQUEUE_HEAD		2
+#ifdef CONFIG_SMP
+#define ENQUEUE_WAKING		4	/* sched_class::task_waking was called */
+#else
+#define ENQUEUE_WAKING		0
+#endif
 
 #define DEQUEUE_SLEEP		1
 
@@ -1071,7 +1075,7 @@ struct sched_class {
 
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
-	void (*task_waking) (struct rq *this_rq, struct task_struct *task);
+	void (*task_waking) (struct task_struct *task);
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,

commit 7608dec2ce2004c234339bef8c8074e5e601d0e9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:46 2011 +0200

    sched: Drop the rq argument to sched_class::select_task_rq()
    
    In preparation of calling select_task_rq() without rq->lock held, drop
    the dependency on the rq argument.
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110405152729.031077745@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b33a700652dc..ff4e2f9c24a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1067,8 +1067,7 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct rq *rq, struct task_struct *p,
-			       int sd_flag, int flags);
+	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
 
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);

commit fd2f4419b4cbe8fe90796df9617c355762afd6a4
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:44 2011 +0200

    sched: Provide p->on_rq
    
    Provide a generic p->on_rq because the p->se.on_rq semantics are
    unfavourable for lockless wakeups but needed for sched_fair.
    
    In particular, p->on_rq is only cleared when we actually dequeue the
    task in schedule() and not on any random dequeue as done by things
    like __migrate_task() and __sched_setscheduler().
    
    This also allows us to remove p->se usage from !sched_fair code.
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152728.949545047@chello.nl

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 173850479e2c..b33a700652dc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1202,6 +1202,7 @@ struct task_struct {
 #ifdef CONFIG_SMP
 	int on_cpu;
 #endif
+	int on_rq;
 
 	int prio, static_prio, normal_prio;
 	unsigned int rt_priority;

commit c6eb3dda25892f1f974f5420f63e6721aab02f6f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:41 2011 +0200

    mutex: Use p->on_cpu for the adaptive spin
    
    Since we now have p->on_cpu unconditionally available, use it to
    re-implement mutex_spin_on_owner.
    
    Requested-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152728.826338173@chello.nl

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3435837e89ff..173850479e2c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -360,7 +360,7 @@ extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
-extern int mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner);
+extern int mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner);
 
 struct nsproxy;
 struct user_namespace;

commit 3ca7a440da394808571dad32d33d3bc0389982e6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:40 2011 +0200

    sched: Always provide p->on_cpu
    
    Always provide p->on_cpu so that we can determine if its on a cpu
    without having to lock the rq.
    
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110405152728.785452014@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 758e27afcda5..3435837e89ff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1200,9 +1200,7 @@ struct task_struct {
 	int lock_depth;		/* BKL lock depth */
 
 #ifdef CONFIG_SMP
-#ifdef __ARCH_WANT_UNLOCKED_CTXSW
-	int oncpu;
-#endif
+	int on_cpu;
 #endif
 
 	int prio, static_prio, normal_prio;

commit 184748cc50b2dceb8287f9fb657eda48ff8fcfe7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Apr 5 17:23:39 2011 +0200

    sched: Provide scheduler_ipi() callback in response to smp_send_reschedule()
    
    For future rework of try_to_wake_up() we'd like to push part of that
    function onto the CPU the task is actually going to run on.
    
    In order to do so we need a generic callback from the existing scheduler IPI.
    
    This patch introduces such a generic callback: scheduler_ipi() and
    implements it as a NOP.
    
    BenH notes: PowerPC might use this IPI on offline CPUs under rare conditions!
    
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110405152728.744338123@chello.nl

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ec2c027e92c..758e27afcda5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2189,8 +2189,10 @@ extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
+static inline void scheduler_ipi(void) { }
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
+static inline void scheduler_ipi(void) { }
 static inline unsigned long wait_task_inactive(struct task_struct *p,
 					       long match_state)
 {

commit 60495e7760d8ee364695006af37309b0755e0e17
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 7 14:10:04 2011 +0200

    sched: Dynamic sched_domain::level
    
    Remove the SD_LV_ enum and use dynamic level assignments.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110407122942.969433965@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 09d9e02f2b61..e43e5b0ab0b5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -892,25 +892,6 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 	return to_cpumask(sg->cpumask);
 }
 
-enum sched_domain_level {
-	SD_LV_NONE = 0,
-#ifdef CONFIG_SCHED_SMT
-	SD_LV_SIBLING,
-#endif
-#ifdef CONFIG_SCHED_MC
-	SD_LV_MC,
-#endif
-#ifdef CONFIG_SCHED_BOOK
-	SD_LV_BOOK,
-#endif
-	SD_LV_CPU,
-#ifdef CONFIG_NUMA
-	SD_LV_NODE,
-	SD_LV_ALLNODES,
-#endif
-	SD_LV_MAX
-};
-
 struct sched_domain_attr {
 	int relax_domain_level;
 };
@@ -919,6 +900,8 @@ struct sched_domain_attr {
 	.relax_domain_level = -1,			\
 }
 
+extern int sched_domain_level_max;
+
 struct sched_domain {
 	/* These fields must be setup */
 	struct sched_domain *parent;	/* top domain must be null terminated */
@@ -936,7 +919,7 @@ struct sched_domain {
 	unsigned int forkexec_idx;
 	unsigned int smt_gain;
 	int flags;			/* See SD_* */
-	enum sched_domain_level level;
+	int level;
 
 	/* Runtime fields. */
 	unsigned long last_balance;	/* init to jiffies. units in jiffies */

commit 7dd04b730749f957c116f363524fd622b05e5141
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 7 14:09:56 2011 +0200

    sched: Remove some dead code
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110407122942.553814623@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a9168b01db8..09d9e02f2b61 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -883,9 +883,6 @@ struct sched_group {
 	 * NOTE: this field is variable length. (Allocated dynamically
 	 * by attaching extra space to the end of the structure,
 	 * depending on how many CPUs the kernel has booted up with)
-	 *
-	 * It is also be embedded into static data structures at build
-	 * time. (See 'struct static_sched_group' in kernel/sched.c)
 	 */
 	unsigned long cpumask[0];
 };
@@ -994,9 +991,6 @@ struct sched_domain {
 	 * NOTE: this field is variable length. (Allocated dynamically
 	 * by attaching extra space to the end of the structure,
 	 * depending on how many CPUs the kernel has booted up with)
-	 *
-	 * It is also be embedded into static data structures at build
-	 * time. (See 'struct static_sched_domain' in kernel/sched.c)
 	 */
 	unsigned long span[0];
 };

commit 3859173d43658d51a749bc0201b943922577d39c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 7 14:09:53 2011 +0200

    sched: Reduce some allocation pressure
    
    Since we now allocate SD_LV_MAX * nr_cpu_ids sched_domain/sched_group
    structures when rebuilding the scheduler toplogy it might make sense
    to shrink that depending on the CONFIG_ options.
    
    This is only needed until we get rid of SD_LV_* alltogether and
    provide a full dynamic topology interface.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110407122942.406226449@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 020b79d6c486..5a9168b01db8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -897,12 +897,20 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 
 enum sched_domain_level {
 	SD_LV_NONE = 0,
+#ifdef CONFIG_SCHED_SMT
 	SD_LV_SIBLING,
+#endif
+#ifdef CONFIG_SCHED_MC
 	SD_LV_MC,
+#endif
+#ifdef CONFIG_SCHED_BOOK
 	SD_LV_BOOK,
+#endif
 	SD_LV_CPU,
+#ifdef CONFIG_NUMA
 	SD_LV_NODE,
 	SD_LV_ALLNODES,
+#endif
 	SD_LV_MAX
 };
 

commit dce840a08702bd13a9a186e07e63d1ef82256b5e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 7 14:09:50 2011 +0200

    sched: Dynamically allocate sched_domain/sched_group data-structures
    
    Instead of relying on static allocations for the sched_domain and
    sched_group trees, dynamically allocate and RCU free them.
    
    Allocating this dynamically also allows for some build_sched_groups()
    simplification since we can now (like with other simplifications) rely
    on the sched_domain tree instead of hard-coded knowledge.
    
    One tricky to note is that detach_destroy_domains() needs to hold
    rcu_read_lock() over the entire tear-down, per-cpu is not sufficient
    since that can lead to partial sched_group existance (could possibly
    be solved by doing the tear-down backwards but this is much more
    robust).
    
    A concequence of the above is that we can no longer print the
    sched_domain debug stuff from cpu_attach_domain() since that might now
    run with preemption disabled (due to classic RCU etc.) and
    sched_domain_debug() does some GFP_KERNEL allocations.
    
    Another thing to note is that we now fully rely on normal RCU and not
    RCU-sched, this is because with the new and exiting RCU flavours we
    grew over the years BH doesn't necessarily hold off RCU-sched grace
    periods (-rt is known to break this). This would in fact already cause
    us grief since we do sched_domain/sched_group iterations from softirq
    context.
    
    This patch is somewhat larger than I would like it to be, but I didn't
    find any means of shrinking/splitting this.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110407122942.245307941@chello.nl
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ec2c027e92c..020b79d6c486 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -868,6 +868,7 @@ static inline int sd_power_saving_flags(void)
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
+	atomic_t ref;
 
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
@@ -973,6 +974,10 @@ struct sched_domain {
 #ifdef CONFIG_SCHED_DEBUG
 	char *name;
 #endif
+	union {
+		void *private;		/* used during construction */
+		struct rcu_head rcu;	/* used during destruction */
+	};
 
 	unsigned int span_weight;
 	/*

commit e46bc9b6fd65bc9f406a4211fbf95683cc9c2937
Merge: 2b9accbee563 321fb561971b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 7 20:44:11 2011 +0200

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into ptrace

commit ee77f075921730b2b465880f9fd4367003bdab39
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Apr 1 20:12:38 2011 +0200

    signal: Turn SIGNAL_STOP_DEQUEUED into GROUP_STOP_DEQUEUED
    
    This patch moves SIGNAL_STOP_DEQUEUED from signal_struct->flags to
    task_struct->group_stop, and thus makes it per-thread.
    
    Like SIGNAL_STOP_DEQUEUED, GROUP_STOP_DEQUEUED can be false-positive
    after return from get_signal_to_deliver(), this is fine. The only
    purpose of this bit is: we can drop ->siglock after __dequeue_signal()
    returns the sig_kernel_stop() signal and before we call
    do_signal_stop(), in this case we must not miss SIGCONT if it comes in
    between.
    
    But, unlike SIGNAL_STOP_DEQUEUED, GROUP_STOP_DEQUEUED can not be
    false-positive in do_signal_stop() if multiple threads dequeue the
    sig_kernel_stop() signal at the same time.
    
    Consider two threads T1 and T2, SIGTTIN has a hanlder.
    
            - T1 dequeues SIGTSTP and sets SIGNAL_STOP_DEQUEUED, then
              it drops ->siglock
    
            - SIGCONT comes and clears SIGNAL_STOP_DEQUEUED, SIGTSTP
              should be cancelled.
    
            - T2 dequeues SIGTTIN and sets SIGNAL_STOP_DEQUEUED again.
              Since we have a handler we should not stop, T2 returns
              to usermode to run the handler.
    
            - T1 continues, calls do_signal_stop() and wrongly starts
              the group stop because SIGNAL_STOP_DEQUEUED was restored
              in between.
    
    With or without this change:
    
            - we need to do something with ptrace_signal() which can
              return SIGSTOP, but this needs another discussion
    
            - SIGSTOP can be lost if it races with the mt exec, will
              be fixed later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 456d80ed3b78..8cef82d4cf77 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -652,9 +652,8 @@ struct signal_struct {
  * Bits in flags field of signal_struct.
  */
 #define SIGNAL_STOP_STOPPED	0x00000001 /* job control stop in effect */
-#define SIGNAL_STOP_DEQUEUED	0x00000002 /* stop signal dequeued */
-#define SIGNAL_STOP_CONTINUED	0x00000004 /* SIGCONT since WCONTINUED reap */
-#define SIGNAL_GROUP_EXIT	0x00000008 /* group exit in progress */
+#define SIGNAL_STOP_CONTINUED	0x00000002 /* SIGCONT since WCONTINUED reap */
+#define SIGNAL_GROUP_EXIT	0x00000004 /* group exit in progress */
 /*
  * Pending notifications to parent.
  */
@@ -1779,6 +1778,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define GROUP_STOP_PENDING	(1 << 16) /* task should stop for group stop */
 #define GROUP_STOP_CONSUME	(1 << 17) /* consume group stop count */
 #define GROUP_STOP_TRAPPING	(1 << 18) /* switching from STOPPED to TRACED */
+#define GROUP_STOP_DEQUEUED	(1 << 19) /* stop signal dequeued */
 
 extern void task_clear_group_stop_pending(struct task_struct *task);
 

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 83bd2e2982fc..4ec2c027e92c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -854,7 +854,7 @@ extern int __weak arch_sd_sibiling_asym_packing(void);
 
 /*
  * Optimise SD flags for power savings:
- * SD_BALANCE_NEWIDLE helps agressive task consolidation and power savings.
+ * SD_BALANCE_NEWIDLE helps aggressive task consolidation and power savings.
  * Keep default SD flags if sched_{smt,mc}_power_saving=0
  */
 

commit 8dd90265ac0754da0df47d9c597f25187bb1c947
Merge: 2a20b02c055a 1232d6132a98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 25 17:59:38 2011 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched, doc: Update sched-design-CFS.txt
      sched: Remove unused 'rq' variable and cpu_rq() call from alloc_fair_sched_group()
      sched.h: Fix a typo ("its")
      sched: Fix yield_to kernel-doc

commit 6c5103890057b1bb781b26b7aae38d33e4c517d8
Merge: 3dab04e6978e 9d2e157d970a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:16:26 2011 -0700

    Merge branch 'for-2.6.39/core' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.39/core' of git://git.kernel.dk/linux-2.6-block: (65 commits)
      Documentation/iostats.txt: bit-size reference etc.
      cfq-iosched: removing unnecessary think time checking
      cfq-iosched: Don't clear queue stats when preempt.
      blk-throttle: Reset group slice when limits are changed
      blk-cgroup: Only give unaccounted_time under debug
      cfq-iosched: Don't set active queue in preempt
      block: fix non-atomic access to genhd inflight structures
      block: attempt to merge with existing requests on plug flush
      block: NULL dereference on error path in __blkdev_get()
      cfq-iosched: Don't update group weights when on service tree
      fs: assign sb->s_bdi to default_backing_dev_info if the bdi is going away
      block: Require subsystems to explicitly allocate bio_set integrity mempool
      jbd2: finish conversion from WRITE_SYNC_PLUG to WRITE_SYNC and explicit plugging
      jbd: finish conversion from WRITE_SYNC_PLUG to WRITE_SYNC and explicit plugging
      fs: make fsync_buffers_list() plug
      mm: make generic_writepages() use plugging
      blk-cgroup: Add unaccounted time to timeslice_used.
      block: fixup plugging stubs for !CONFIG_BLOCK
      block: remove obsolete comments for blkdev_issue_zeroout.
      blktrace: Use rq->cmd_flags directly in blk_add_trace_rq.
      ...
    
    Fix up conflicts in fs/{aio.c,super.c}

commit 7ffd4ca7a2cdd7a18f0b499a4e9e0e7cf36ba018
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Mar 23 16:42:35 2011 -0700

    memcg: convert uncharge batching from bytes to page granularity
    
    We never uncharge subpage quantities.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b601be3dace..98fc7ed4b191 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1524,8 +1524,8 @@ struct task_struct {
 	struct memcg_batch_info {
 		int do_batch;	/* incremented when batch uncharge started */
 		struct mem_cgroup *memcg; /* target memcg of uncharge */
-		unsigned long bytes; 		/* uncharged usage */
-		unsigned long memsw_bytes; /* uncharged mem+swap usage */
+		unsigned long nr_pages;	/* uncharged usage */
+		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
 	} memcg_batch;
 #endif
 };

commit e815f0a84fc9a98e5cc3ef0b520122e5e18520e7
Author: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
Date:   Mon Mar 21 20:24:47 2011 +0100

    sched.h: Fix a typo ("its")
    
    The sentence uses the possessive pronoun, which is spelled
    without an apostrophe.
    
    Signed-off-by: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1300735487-2406-1-git-send-email-j.neuschaefer@gmx.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c15936fe998b..e89f1292c301 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -516,7 +516,7 @@ struct thread_group_cputimer {
 struct autogroup;
 
 /*
- * NOTE! "signal_struct" does not have it's own
+ * NOTE! "signal_struct" does not have its own
  * locking, because a shared signal_struct always
  * implies a shared sighand_struct, so locking
  * sighand_struct is always a proper superset of

commit d79fdd6d96f46fabb779d86332e3677c6f5c2a4f
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:00 2011 +0100

    ptrace: Clean transitions between TASK_STOPPED and TRACED
    
    Currently, if the task is STOPPED on ptrace attach, it's left alone
    and the state is silently changed to TRACED on the next ptrace call.
    The behavior breaks the assumption that arch_ptrace_stop() is called
    before any task is poked by ptrace and is ugly in that a task
    manipulates the state of another task directly.
    
    With GROUP_STOP_PENDING, the transitions between TASK_STOPPED and
    TRACED can be made clean.  The tracer can use the flag to tell the
    tracee to retry stop on attach and detach.  On retry, the tracee will
    enter the desired state in the correct way.  The lower 16bits of
    task->group_stop is used to remember the signal number which caused
    the last group stop.  This is used while retrying for ptrace attach as
    the original group_exit_code could have been consumed with wait(2) by
    then.
    
    As the real parent may wait(2) and consume the group_exit_code
    anytime, the group_exit_code needs to be saved separately so that it
    can be used when switching from regular sleep to ptrace_stop().  This
    is recorded in the lower 16bits of task->group_stop.
    
    If a task is already stopped and there's no intervening SIGCONT, a
    ptrace request immediately following a successful PTRACE_ATTACH should
    always succeed even if the tracer doesn't wait(2) for attach
    completion; however, with this change, the tracee might still be
    TASK_RUNNING trying to enter TASK_TRACED which would cause the
    following request to fail with -ESRCH.
    
    This intermediate state is hidden from the ptracer by setting
    GROUP_STOP_TRAPPING on attach and making ptrace_check_attach() wait
    for it to clear on its signal->wait_chldexit.  Completing the
    transition or getting killed clears TRAPPING and wakes up the tracer.
    
    Note that the STOPPED -> RUNNING -> TRACED transition is still visible
    to other threads which are in the same group as the ptracer and the
    reverse transition is visible to all.  Please read the comments for
    details.
    
    Oleg:
    
    * Spotted a race condition where a task may retry group stop without
      proper bookkeeping.  Fixed by redoing bookkeeping on retry.
    
    * Spotted that the transition is visible to userland in several
      different ways.  Most are fixed with GROUP_STOP_TRAPPING.  Unhandled
      corner case is documented.
    
    * Pointed out not setting GROUP_STOP_SIGMASK on an already stopped
      task would result in more consistent behavior.
    
    * Pointed out that calling ptrace_stop() from do_signal_stop() in
      TASK_STOPPED can race with group stop start logic and then confuse
      the TRAPPING wait in ptrace_check_attach().  ptrace_stop() is now
      called with TASK_RUNNING.
    
    * Suggested using signal->wait_chldexit instead of bit wait.
    
    * Spotted a race condition between TRACED transition and clearing of
      TRAPPING.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b2a17dfbdbad..456d80ed3b78 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1775,8 +1775,10 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * task->group_stop flags
  */
+#define GROUP_STOP_SIGMASK	0xffff    /* signr of the last group stop */
 #define GROUP_STOP_PENDING	(1 << 16) /* task should stop for group stop */
 #define GROUP_STOP_CONSUME	(1 << 17) /* consume group stop count */
+#define GROUP_STOP_TRAPPING	(1 << 18) /* switching from STOPPED to TRACED */
 
 extern void task_clear_group_stop_pending(struct task_struct *task);
 

commit 39efa3ef3a376a4e53de2f82fc91182459d34200
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:00 2011 +0100

    signal: Use GROUP_STOP_PENDING to stop once for a single group stop
    
    Currently task->signal->group_stop_count is used to decide whether to
    stop for group stop.  However, if there is a task in the group which
    is taking a long time to stop, other tasks which are continued by
    ptrace would repeatedly stop for the same group stop until the group
    stop is complete.
    
    Conversely, if a ptraced task is in TASK_TRACED state, the debugger
    won't get notified of group stops which is inconsistent compared to
    the ptraced task in any other state.
    
    This patch introduces GROUP_STOP_PENDING which tracks whether a task
    is yet to stop for the group stop in progress.  The flag is set when a
    group stop starts and cleared when the task stops the first time for
    the group stop, and consulted whenever whether the task should
    participate in a group stop needs to be determined.  Note that now
    tasks in TASK_TRACED also participate in group stop.
    
    This results in the following behavior changes.
    
    * For a single group stop, a ptracer would see at most one stop
      reported.
    
    * A ptracee in TASK_TRACED now also participates in group stop and the
      tracer would get the notification.  However, as a ptraced task could
      be in TASK_STOPPED state or any ptrace trap could consume group
      stop, the notification may still be missing.  These will be
      addressed with further patches.
    
    * A ptracee may start a group stop while one is still in progress if
      the tracer let it continue with stop signal delivery.  Group stop
      code handles this correctly.
    
    Oleg:
    
    * Spotted that a task might skip signal check even when its
      GROUP_STOP_PENDING is set.  Fixed by updating
      recalc_sigpending_tsk() to check GROUP_STOP_PENDING instead of
      group_stop_count.
    
    * Pointed out that task->group_stop should be cleared whenever
      task->signal->group_stop_count is cleared.  Fixed accordingly.
    
    * Pointed out the behavior inconsistency between TASK_TRACED and
      RUNNING and the last behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 85f51042c2b8..b2a17dfbdbad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1775,8 +1775,11 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * task->group_stop flags
  */
+#define GROUP_STOP_PENDING	(1 << 16) /* task should stop for group stop */
 #define GROUP_STOP_CONSUME	(1 << 17) /* consume group stop count */
 
+extern void task_clear_group_stop_pending(struct task_struct *task);
+
 #ifdef CONFIG_PREEMPT_RCU
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */

commit e5c1902e9260a0075ea52cb5ef627a8d9aaede89
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:00 2011 +0100

    signal: Fix premature completion of group stop when interfered by ptrace
    
    task->signal->group_stop_count is used to track the progress of group
    stop.  It's initialized to the number of tasks which need to stop for
    group stop to finish and each stopping or trapping task decrements.
    However, each task doesn't keep track of whether it decremented the
    counter or not and if woken up before the group stop is complete and
    stops again, it can decrement the counter multiple times.
    
    Please consider the following example code.
    
     static void *worker(void *arg)
     {
             while (1) ;
             return NULL;
     }
    
     int main(void)
     {
             pthread_t thread;
             pid_t pid;
             int i;
    
             pid = fork();
             if (!pid) {
                     for (i = 0; i < 5; i++)
                             pthread_create(&thread, NULL, worker, NULL);
                     while (1) ;
                     return 0;
             }
    
             ptrace(PTRACE_ATTACH, pid, NULL, NULL);
             while (1) {
                     waitid(P_PID, pid, NULL, WSTOPPED);
                     ptrace(PTRACE_SINGLESTEP, pid, NULL, (void *)(long)SIGSTOP);
             }
             return 0;
     }
    
    The child creates five threads and the parent continuously traps the
    first thread and whenever the child gets a signal, SIGSTOP is
    delivered.  If an external process sends SIGSTOP to the child, all
    other threads in the process should reliably stop.  However, due to
    the above bug, the first thread will often end up consuming
    group_stop_count multiple times and SIGSTOP often ends up stopping
    none or part of the other four threads.
    
    This patch adds a new field task->group_stop which is protected by
    siglock and uses GROUP_STOP_CONSUME flag to track which task is still
    to consume group_stop_count to fix this bug.
    
    task_clear_group_stop_pending() and task_participate_group_stop() are
    added to help manipulating group stop states.  As ptrace_stop() now
    also uses task_participate_group_stop(), it will set
    SIGNAL_STOP_STOPPED if it completes a group stop.
    
    There still are many issues regarding the interaction between group
    stop and ptrace.  Patches to address them will follow.
    
    - Oleg spotted duplicate GROUP_STOP_CONSUME.  Dropped.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b601be3dace..85f51042c2b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1260,6 +1260,7 @@ struct task_struct {
 	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
+	unsigned int group_stop;	/* GROUP_STOP_*, siglock protected */
 	/* ??? */
 	unsigned int personality;
 	unsigned did_exec:1;
@@ -1771,6 +1772,11 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
+/*
+ * task->group_stop flags
+ */
+#define GROUP_STOP_CONSUME	(1 << 17) /* consume group stop count */
+
 #ifdef CONFIG_PREEMPT_RCU
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */

commit 207205a2ba2655652fe46a60b49838af6c16a919
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:44 2011 -0700

    kthread: NUMA aware kthread_create_on_node()
    
    All kthreads being created from a single helper task, they all use memory
    from a single node for their kernel stack and task struct.
    
    This patch suite creates kthread_create_on_node(), adding a 'cpu' parameter
    to parameters already used by kthread_create().
    
    This parameter serves in allocating memory for the new kthread on its
    memory node if possible.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c15936fe998b..4b601be3dace 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1471,6 +1471,7 @@ struct task_struct {
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
 	short il_next;
+	short pref_node_fork;
 #endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;

commit 420c1c572d4ceaa2f37b6311b7017ac6cf049fe2
Merge: 9620639b7ea3 6e6823d17b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 18:53:35 2011 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (62 commits)
      posix-clocks: Check write permissions in posix syscalls
      hrtimer: Remove empty hrtimer_init_hres_timer()
      hrtimer: Update hrtimer->state documentation
      hrtimer: Update base[CLOCK_BOOTTIME].offset correctly
      timers: Export CLOCK_BOOTTIME via the posix timers interface
      timers: Add CLOCK_BOOTTIME hrtimer base
      time: Extend get_xtime_and_monotonic_offset() to also return sleep
      time: Introduce get_monotonic_boottime and ktime_get_boottime
      hrtimers: extend hrtimer base code to handle more then 2 clockids
      ntp: Remove redundant and incorrect parameter check
      mn10300: Switch do_timer() to xtimer_update()
      posix clocks: Introduce dynamic clocks
      posix-timers: Cleanup namespace
      posix-timers: Add support for fd based clocks
      x86: Add clock_adjtime for x86
      posix-timers: Introduce a syscall for clock tuning.
      time: Splitout compat timex accessors
      ntp: Add ADJ_SETOFFSET mode bit
      time: Introduce timekeeping_inject_offset
      posix-timer: Update comment
      ...
    
    Fix up new system-call-related conflicts in
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/syscall_table_32.S
    (name_to_handle_at()/open_by_handle_at() vs clock_adjtime()), and some
    due to movement of get_jiffies_64() in:
            kernel/time.c

commit 9620639b7ea3843983f4ced8b4c81eb4d8974838
Merge: a926021cb1f8 6d1cafd8b56e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 18:37:30 2011 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (26 commits)
      sched: Resched proper CPU on yield_to()
      sched: Allow users with sufficient RLIMIT_NICE to change from SCHED_IDLE policy
      sched: Allow SCHED_BATCH to preempt SCHED_IDLE tasks
      sched: Clean up the IRQ_TIME_ACCOUNTING code
      sched: Add #ifdef around irq time accounting functions
      sched, autogroup: Stop claiming ownership of the root task group
      sched, autogroup: Stop going ahead if autogroup is disabled
      sched, autogroup, sysctl: Use proc_dointvec_minmax() instead
      sched: Fix the group_imb logic
      sched: Clean up some f_b_g() comments
      sched: Clean up remnants of sd_idle
      sched: Wholesale removal of sd_idle logic
      sched: Add yield_to(task, preempt) functionality
      sched: Use a buddy to implement yield_task_fair()
      sched: Limit the scope of clear_buddies
      sched: Check the right ->nr_running in yield_task_fair()
      sched: Avoid expensive initial update_cfs_load(), on UP too
      sched: Fix switch_from_fair()
      sched: Simplify the idle scheduling class
      softirqs: Account ksoftirqd time as cpustat softirq
      ...

commit 73c101011926c5832e6e141682180c4debe2cf45
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Mar 8 13:19:51 2011 +0100

    block: initial patch for on-stack per-task plugging
    
    This patch adds support for creating a queuing context outside
    of the queue itself. This enables us to batch up pieces of IO
    before grabbing the block device queue lock and submitting them to
    the IO scheduler.
    
    The context is created on the stack of the process and assigned in
    the task structure, so that we can auto-unplug it if we hit a schedule
    event.
    
    The current queue plugging happens implicitly if IO is submitted to
    an empty device, yet callers have to remember to unplug that IO when
    they are going to wait for it. This is an ugly API and has caused bugs
    in the past. Additionally, it requires hacks in the vm (->sync_page()
    callback) to handle that logic. By switching to an explicit plugging
    scheme we make the API a lot nicer and can get rid of the ->sync_page()
    hack in the vm.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 777d8a5ed06b..96ac22643742 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -99,6 +99,7 @@ struct robust_list_head;
 struct bio_list;
 struct fs_struct;
 struct perf_event_context;
+struct blk_plug;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1429,6 +1430,11 @@ struct task_struct {
 /* stacked block device info */
 	struct bio_list *bio_list;
 
+#ifdef CONFIG_BLOCK
+/* stack plugging */
+	struct blk_plug *plug;
+#endif
+
 /* VM state */
 	struct reclaim_state *reclaim_state;
 

commit 888a8a3e9d79cbb9d83e53955f684998248580ec
Merge: cfff2d909cbd b06b3d49699a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 4 10:40:22 2011 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: Pick up updates before queueing up dependent patches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d927dc937910ad8c7350266cac70e42a5f0b48cf
Merge: 46e49b3836c7 f5412be59960
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 23 11:31:34 2011 +0100

    Merge commit 'v2.6.38-rc6' into sched/core
    
    Merge reason: Pick up the latest fixes before queueing up new changes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 58a69cb47ec6991bf006a3e5d202e8571b0327a4
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 16 09:25:31 2011 +0100

    workqueue, freezer: unify spelling of 'freeze' + 'able' to 'freezable'
    
    There are two spellings in use for 'freeze' + 'able' - 'freezable' and
    'freezeable'.  The former is the more prominent one.  The latter is
    mostly used by workqueue and in a few other odd places.  Unify the
    spelling to 'freezable'.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Dmitry Torokhov <dtor@mail.ru>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Alex Dubov <oakad@yahoo.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Steven Whitehouse <swhiteho@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d747f948b34e..777d8a5ed06b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1744,7 +1744,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
-#define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezeable */
+#define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
 #define PF_FREEZER_NOSIG 0x80000000	/* Freezer won't send signals to it */
 
 /*

commit d95f412200652694e63e64bfd49f0ae274a54479
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Feb 1 09:50:51 2011 -0500

    sched: Add yield_to(task, preempt) functionality
    
    Currently only implemented for fair class tasks.
    
    Add a yield_to_task method() to the fair scheduling class. allowing the
    caller of yield_to() to accelerate another thread in it's thread group,
    task group.
    
    Implemented via a scheduler hint, using cfs_rq->next to encourage the
    target being selected.  We can rely on pick_next_entity to keep things
    fair, so noone can accelerate a thread that has already used its fair
    share of CPU time.
    
    This also means callers should only call yield_to when they really
    mean it.  Calling it too often can result in the scheduler just
    ignoring the hint.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110201095051.4ddb7738@annuminas.surriel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4e9fad271c30..c88b3bfbd09e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1058,6 +1058,7 @@ struct sched_class {
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*yield_task) (struct rq *rq);
+	bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
 
@@ -1972,6 +1973,7 @@ static inline int rt_mutex_getprio(struct task_struct *p)
 # define rt_mutex_adjust_pi(p)		do { } while (0)
 #endif
 
+extern bool yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
 extern int task_nice(const struct task_struct *p);

commit ac53db596cc08ecb8040cfb6f71ae40c6f2041c4
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 1 09:51:03 2011 -0500

    sched: Use a buddy to implement yield_task_fair()
    
    Use the buddy mechanism to implement yield_task_fair.  This
    allows us to skip onto the next highest priority se at every
    level in the CFS tree, unless doing so would introduce gross
    unfairness in CPU time distribution.
    
    We order the buddy selection in pick_next_entity to check
    yield first, then last, then next.  We need next to be able
    to override yield, because it is possible for the "next" and
    "yield" task to be different processen in the same sub-tree
    of the CFS tree.  When they are, we need to go into that
    sub-tree regardless of the "yield" hint, and pick the correct
    entity once we get to the right level.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110201095103.3a79e92a@annuminas.surriel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0542774914d4..4e9fad271c30 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1942,8 +1942,6 @@ int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
-extern unsigned int sysctl_sched_compat_yield;
-
 #ifdef CONFIG_SCHED_AUTOGROUP
 extern unsigned int sysctl_sched_autogroup_enabled;
 

commit fe4b04fa31a6dcf4358aa84cf81e5a7fd079469b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 2 13:19:09 2011 +0100

    perf: Cure task_oncpu_function_call() races
    
    Oleg reported that on architectures with
    __ARCH_WANT_INTERRUPTS_ON_CTXSW the IPI from
    task_oncpu_function_call() can land before perf_event_task_sched_in()
    and cause interesting situations for eg. perf_install_in_context().
    
    This patch reworks the task_oncpu_function_call() interface to give a
    more usable primitive as well as rework all its users to hopefully be
    more obvious as well as remove the races.
    
    While looking at the code I also found a number of races against
    perf_event_task_sched_out() which can flip contexts between tasks so
    plug those too.
    
    Reported-and-reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d747f948b34e..0b40ee3f6d7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2578,13 +2578,6 @@ static inline void inc_syscw(struct task_struct *tsk)
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
-/*
- * Call the function if the target task is executing on a CPU right now:
- */
-extern void task_oncpu_function_call(struct task_struct *p,
-				     void (*func) (void *info), void *info);
-
-
 #ifdef CONFIG_MM_OWNER
 extern void mm_update_next_owner(struct mm_struct *mm);
 extern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);

commit e2830b5c1b2b2217894370a3b95af87d4a958401
Author: Torben Hohn <torbenh@gmx.de>
Date:   Thu Jan 27 16:00:32 2011 +0100

    time: Make do_timer() and xtime_lock local to kernel/time/
    
    All callers of do_timer() are converted to xtime_update(). The only
    users of xtime_lock are in kernel/time/. Make both local to
    kernel/time/ and remove them from the global header files.
    
    [ tglx: Reuse tick-internal.h instead of creating another local header
            file. Massaged changelog ]
    
    Signed-off-by: Torben Hohn <torbenh@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: johnstul@us.ibm.com
    Cc: yong.zhang0@gmail.com
    Cc: hch@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9d9a0787eed3..cdef640aa446 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2049,7 +2049,6 @@ extern void release_uids(struct user_namespace *ns);
 
 #include <asm/current.h>
 
-extern void do_timer(unsigned long ticks);
 extern void xtime_update(unsigned long ticks);
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);

commit f0af911a9dec9de702645182c8d269449e24d24b
Author: Torben Hohn <torbenh@gmx.de>
Date:   Thu Jan 27 15:59:10 2011 +0100

    time: Provide xtime_update()
    
    xtime_update() takes xtime_lock write locked and calls
    do_timer(). Provided to replace the do_timer() calls in the
    architecture code.
    
    Signed-off-by: Torben Hohn <torbenh@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: johnstul@us.ibm.com
    Cc: yong.zhang0@gmail.com
    Cc: hch@infradead.org
    LKML-Reference: <20110127145910.23248.21379.stgit@localhost>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d747f948b34e..9d9a0787eed3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2050,6 +2050,7 @@ extern void release_uids(struct user_namespace *ns);
 #include <asm/current.h>
 
 extern void do_timer(unsigned long ticks);
+extern void xtime_update(unsigned long ticks);
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);

commit da7a735e51f9622eb3e1672594d4a41da01d7e4f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jan 17 17:03:27 2011 +0100

    sched: Fix switch_from_fair()
    
    When a task is taken out of the fair class we must ensure the vruntime
    is properly normalized because when we put it back in it will assume
    to be normalized.
    
    The case that goes wrong is when changing away from the fair class
    while sleeping. Sleeping tasks have non-normalized vruntime in order
    to make sleeper-fairness work. So treat the switch away from fair as a
    wakeup and preserve the relative vruntime.
    
    Also update sysrq-n to call the ->switch_{to,from} methods.
    
    Reported-by: Onkalo Samu <samu.p.onkalo@nokia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af6e15fbfb78..0542774914d4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1084,12 +1084,10 @@ struct sched_class {
 	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
 	void (*task_fork) (struct task_struct *p);
 
-	void (*switched_from) (struct rq *this_rq, struct task_struct *task,
-			       int running);
-	void (*switched_to) (struct rq *this_rq, struct task_struct *task,
-			     int running);
+	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
+	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
-			     int oldprio, int running);
+			     int oldprio);
 
 	unsigned int (*get_rr_interval) (struct rq *rq,
 					 struct task_struct *task);

commit 4dd53d891ca46dcc1fde0376a33540d3fd83cb9a
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Tue Dec 21 17:09:00 2010 -0800

    softirqs: Free up pf flag PF_KSOFTIRQD
    
    Cleanup patch, freeing up PF_KSOFTIRQD and use per_cpu ksoftirqd pointer
    instead, as suggested by Eric Dumazet.
    
    Tested-by: Shaun Ruffell <sruffell@digium.com>
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1292980144-28796-2-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d747f948b34e..af6e15fbfb78 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1715,7 +1715,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * Per process flags
  */
-#define PF_KSOFTIRQD	0x00000001	/* I am ksoftirqd */
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */

commit ba76149f47d8c939efa0acc07a191237af900471
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:46:58 2011 -0800

    thp: khugepaged
    
    Add khugepaged to relocate fragmented pages into hugepages if new
    hugepages become available.  (this is indipendent of the defrag logic that
    will have to make new hugepages available)
    
    The fundamental reason why khugepaged is unavoidable, is that some memory
    can be fragmented and not everything can be relocated.  So when a virtual
    machine quits and releases gigabytes of hugepages, we want to use those
    freely available hugepages to create huge-pmd in the other virtual
    machines that may be running on fragmented memory, to maximize the CPU
    efficiency at all times.  The scan is slow, it takes nearly zero cpu time,
    except when it copies data (in which case it means we definitely want to
    pay for that cpu time) so it seems a good tradeoff.
    
    In addition to the hugepages being released by other process releasing
    memory, we have the strong suspicion that the performance impact of
    potentially defragmenting hugepages during or before each page fault could
    lead to more performance inconsistency than allocating small pages at
    first and having them collapsed into large pages later...  if they prove
    themselfs to be long lived mappings (khugepaged scan is slow so short
    lived mappings have low probability to run into khugepaged if compared to
    long lived mappings).
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f23b5bb6f52e..d747f948b34e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -434,6 +434,7 @@ extern int get_dumpable(struct mm_struct *mm);
 #endif
 					/* leave room for more dump flags */
 #define MMF_VM_MERGEABLE	16	/* KSM may merge identical pages */
+#define MMF_VM_HUGEPAGE		17	/* set when VM_HUGEPAGE is set on vma */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 

commit dabb16f639820267b3850d804571c70bd93d4e07
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Thu Jan 13 15:46:05 2011 -0800

    oom: allow a non-CAP_SYS_RESOURCE proces to oom_score_adj down
    
    We'd like to be able to oom_score_adj a process up/down as it
    enters/leaves the foreground.  Currently, it is not possible to oom_adj
    down without CAP_SYS_RESOURCE.  This patch allows a task to decrease its
    oom_score_adj back to the value that a CAP_SYS_RESOURCE thread set it to
    or its inherited value at fork.  Assuming the thread that has forked it
    has oom_score_adj of 0, each process could decrease it back from 0 upon
    activation unless a CAP_SYS_RESOURCE thread elevated it to something
    higher.
    
    Alternative considered:
    
    * a setuid binary
    * a daemon with CAP_SYS_RESOURCE
    
    Since you don't wan't all processes to be able to reduce their oom_adj, a
    setuid or daemon implementation would be complex.  The alternatives also
    have much higher overhead.
    
    This patch updated from original patch based on feedback from David
    Rientjes.
    
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ying Han <yinghan@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 07402530fc70..f23b5bb6f52e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -634,6 +634,8 @@ struct signal_struct {
 
 	int oom_adj;		/* OOM kill score adjustment (bit shift) */
 	int oom_score_adj;	/* OOM kill score adjustment */
+	int oom_score_adj_min;	/* OOM kill score adjustment minimum value.
+				 * Only settable by CAP_SYS_RESOURCE. */
 
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations

commit 43bb40c9e3aa51a3b038c9df2c9afb4d4685614d
Author: Dave Jones <davej@redhat.com>
Date:   Thu Jan 13 15:45:40 2011 -0800

    sched: remove long deprecated CLONE_STOPPED flag
    
    This warning was added in commit bdff746a3915 ("clone: prepare to recycle
    CLONE_STOPPED") three years ago.  2.6.26 came and went.  As far as I know,
    no-one is actually using CLONE_STOPPED.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 96e23215e276..07402530fc70 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -21,7 +21,8 @@
 #define CLONE_DETACHED		0x00400000	/* Unused, ignored */
 #define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
 #define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
-#define CLONE_STOPPED		0x02000000	/* Start in stopped state */
+/* 0x02000000 was previously the unused CLONE_STOPPED (Start in stopped state)
+   and is now available for re-use. */
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 #define CLONE_NEWIPC		0x08000000	/* New ipcs */
 #define CLONE_NEWUSER		0x10000000	/* New user namespace */

commit 52bd19f7691b2ea6433aef0ef94c08c57efd7e79
Author: Robin Holt <holt@sgi.com>
Date:   Wed Jan 12 17:00:01 2011 -0800

    epoll: convert max_user_watches to long
    
    On a 16TB machine, max_user_watches has an integer overflow.  Convert it
    to use a long and handle the associated fallout.
    
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Davide Libenzi <davidel@xmailserver.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index abc527aa8550..96e23215e276 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -683,7 +683,7 @@ struct user_struct {
 	atomic_t fanotify_listeners;
 #endif
 #ifdef CONFIG_EPOLL
-	atomic_t epoll_watches;	/* The number of file descriptors currently watched */
+	atomic_long_t epoll_watches; /* The number of file descriptors currently watched */
 #endif
 #ifdef CONFIG_POSIX_MQUEUE
 	/* protected by mq_lock	*/

commit 57cc7215b70856dc6bae8e55b00ecd7b1d7429b1
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jan 10 08:18:25 2011 +0200

    headers: kobject.h redux
    
    Remove kobject.h from files which don't need it, notably,
    sched.h and fs.h.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c118a7f203aa..abc527aa8550 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -87,7 +87,6 @@ struct sched_param {
 #include <linux/timer.h>
 #include <linux/hrtimer.h>
 #include <linux/task_io_accounting.h>
-#include <linux/kobject.h>
 #include <linux/latencytop.h>
 #include <linux/cred.h>
 

commit 37721e1b0cf98cb65895f234d8c500d270546529
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jan 10 08:17:10 2011 +0200

    headers: path.h redux
    
    Remove path.h from sched.h and other files.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 341acbbc434a..c118a7f203aa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -70,7 +70,6 @@ struct sched_param {
 #include <linux/smp.h>
 #include <linux/sem.h>
 #include <linux/signal.h>
-#include <linux/path.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
 #include <linux/pid.h>

commit 07e06b011db2b3300f6c975ebf293fc4c8c59942
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Fri Jan 7 15:17:36 2011 +0800

    sched: Consolidate the name of root_task_group and init_task_group
    
    root_task_group is the leftover of USER_SCHED, now it's always
    same to init_task_group.
    But as Mike suggested, root_task_group is maybe the suitable name
    to keep for a tree.
    So in this patch:
      init_task_group      --> root_task_group
      init_task_group_load --> root_task_group_load
      INIT_TASK_GROUP_LOAD --> ROOT_TASK_GROUP_LOAD
    
    Suggested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110107071736.GA32635@windriver.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 777cd01e240e..341acbbc434a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2511,7 +2511,7 @@ extern void normalize_rt_tasks(void);
 
 #ifdef CONFIG_CGROUP_SCHED
 
-extern struct task_group init_task_group;
+extern struct task_group root_task_group;
 
 extern struct task_group *sched_create_group(struct task_group *parent);
 extern void sched_destroy_group(struct task_group *tg);

commit 65b2074f84be2287e020839e93b4cdaaf60eb37c
Merge: 28d9bfc37c86 6bf4123760a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 6 10:23:33 2011 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (30 commits)
      sched: Change wait_for_completion_*_timeout() to return a signed long
      sched, autogroup: Fix reference leak
      sched, autogroup: Fix potential access to freed memory
      sched: Remove redundant CONFIG_CGROUP_SCHED ifdef
      sched: Fix interactivity bug by charging unaccounted run-time on entity re-weight
      sched: Move periodic share updates to entity_tick()
      printk: Use this_cpu_{read|write} api on printk_pending
      sched: Make pushable_tasks CONFIG_SMP dependant
      sched: Add 'autogroup' scheduling feature: automated per session task groups
      sched: Fix unregister_fair_sched_group()
      sched: Remove unused argument dest_cpu to migrate_task()
      mutexes, sched: Introduce arch_mutex_cpu_relax()
      sched: Add some clock info to sched_debug
      cpu: Remove incorrect BUG_ON
      cpu: Remove unused variable
      sched: Fix UP build breakage
      sched: Make task dump print all 15 chars of proc comm
      sched: Update tg->shares after cpu.shares write
      sched: Allow update_cfs_load() to update global load
      sched: Implement demand based update_cfs_load()
      ...

commit 28d9bfc37c861aa9c8386dff1ac7e9a10e5c5162
Merge: f3b0cfa9b017 4b95f135f606
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 6 10:17:26 2011 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (146 commits)
      tools, perf: Documentation for the power events API
      perf: Add calls to suspend trace point
      perf script: Make some lists static
      perf script: Use the default lost event handler
      perf session: Warn about errors when processing pipe events too
      perf tools: Fix perf_event.h header usage
      perf test: Clarify some error reports in the open syscall test
      x86, NMI: Add touch_nmi_watchdog to io_check_error delay
      x86: Avoid calling arch_trigger_all_cpu_backtrace() at the same time
      x86: Only call smp_processor_id in non-preempt cases
      perf timechart: Adjust perf timechart to the new power events
      perf: Clean up power events by introducing new, more generic ones
      perf: Do not export power_frequency, but power_start event
      perf test: Add test for counting open syscalls
      perf evsel: Auto allocate resources needed for some methods
      perf evsel: Use {cpu,thread}_map to shorten list of parameters
      perf tools: Refactor all_tids to hold nr and the map
      perf tools: Refactor cpumap to hold nr and the map
      perf evsel: Introduce per cpu and per thread open helpers
      perf evsel: Steal the counter reading routines from stat
      ...

commit 27066fd484a32c80630136aa2b91c980f3198f9d
Merge: 101e5f77bf35 3c0eee3fe6a3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 5 14:14:42 2011 +0100

    Merge commit 'v2.6.37' into sched/core
    
    Merge reason: Merge the final .37 tree.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 394f4528c523d88daabd50f883a8d6b164075555
Merge: 90a8a73c06cc 3c2dcf2aed5e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 23 12:57:04 2010 +0100

    Merge branch 'rcu/next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-2.6-rcu into core/rcu

commit 6c529a266bdc590a870ee2d2092ff6527eff427b
Merge: 7639dae0ca11 90a8a73c06cc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 22 11:53:20 2010 +0100

    Merge commit 'v2.6.37-rc7' into perf/core
    
    Merge reason: Pick up the latest -rc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 806c09a7db457be3758e14b1f152761135d89af5
Author: Dario Faggioli <raistlin@linux.it>
Date:   Tue Nov 30 19:51:33 2010 +0100

    sched: Make pushable_tasks CONFIG_SMP dependant
    
    As noted by Peter Zijlstra at https://lkml.org/lkml/2010/11/10/391
    (while reviewing other stuff, though), tracking pushable tasks
    only makes sense on SMP systems.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1291143093.2697.298.camel@Palantir>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c2d46da486e..4f92a239c14d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1240,7 +1240,9 @@ struct task_struct {
 #endif
 
 	struct list_head tasks;
+#ifdef CONFIG_SMP
 	struct plist_node pushable_tasks;
+#endif
 
 	struct mm_struct *mm, *active_mm;
 #if defined(SPLIT_RSS_COUNTING)

commit 0f004f5a696a9434b7214d0d3cbd0525ee77d428
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Nov 30 19:48:45 2010 +0100

    sched: Cure more NO_HZ load average woes
    
    There's a long-running regression that proved difficult to fix and
    which is hitting certain people and is rather annoying in its effects.
    
    Damien reported that after 74f5187ac8 (sched: Cure load average vs
    NO_HZ woes) his load average is unnaturally high, he also noted that
    even with that patch reverted the load avgerage numbers are not
    correct.
    
    The problem is that the previous patch only solved half the NO_HZ
    problem, it addressed the part of going into NO_HZ mode, not of
    comming out of NO_HZ mode. This patch implements that missing half.
    
    When comming out of NO_HZ mode there are two important things to take
    care of:
    
     - Folding the pending idle delta into the global active count.
     - Correctly aging the averages for the idle-duration.
    
    So with this patch the NO_HZ interaction should be complete and
    behaviour between CONFIG_NO_HZ=[yn] should be equivalent.
    
    Furthermore, this patch slightly changes the load average computation
    by adding a rounding term to the fixed point multiplication.
    
    Reported-by: Damien Wyart <damien.wyart@free.fr>
    Reported-by: Tim McGrath <tmhikaru@gmail.com>
    Tested-by: Damien Wyart <damien.wyart@free.fr>
    Tested-by: Orion Poplawski <orion@cora.nwra.com>
    Tested-by: Kyle McMartin <kyle@mcmartin.ca>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: stable@kernel.org
    Cc: Chase Douglas <chase.douglas@canonical.com>
    LKML-Reference: <1291129145.32004.874.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c79e921a68b..223874538b33 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -143,7 +143,7 @@ extern unsigned long nr_iowait_cpu(int cpu);
 extern unsigned long this_cpu_load(void);
 
 
-extern void calc_global_load(void);
+extern void calc_global_load(unsigned long ticks);
 
 extern unsigned long get_parent_ip(unsigned long addr);
 

commit 5091faa449ee0b7d73bc296a93bca9540fc51d0a
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Nov 30 14:18:03 2010 +0100

    sched: Add 'autogroup' scheduling feature: automated per session task groups
    
    A recurring complaint from CFS users is that parallel kbuild has
    a negative impact on desktop interactivity.  This patch
    implements an idea from Linus, to automatically create task
    groups.  Currently, only per session autogroups are implemented,
    but the patch leaves the way open for enhancement.
    
    Implementation: each task's signal struct contains an inherited
    pointer to a refcounted autogroup struct containing a task group
    pointer, the default for all tasks pointing to the
    init_task_group.  When a task calls setsid(), a new task group
    is created, the process is moved into the new task group, and a
    reference to the preveious task group is dropped.  Child
    processes inherit this task group thereafter, and increase it's
    refcount.  When the last thread of a process exits, the
    process's reference is dropped, such that when the last process
    referencing an autogroup exits, the autogroup is destroyed.
    
    At runqueue selection time, IFF a task has no cgroup assignment,
    its current autogroup is used.
    
    Autogroup bandwidth is controllable via setting it's nice level
    through the proc filesystem:
    
      cat /proc/<pid>/autogroup
    
    Displays the task's group and the group's nice level.
    
      echo <nice level> > /proc/<pid>/autogroup
    
    Sets the task group's shares to the weight of nice <level> task.
    Setting nice level is rate limited for !admin users due to the
    abuse risk of task group locking.
    
    The feature is enabled from boot by default if
    CONFIG_SCHED_AUTOGROUP=y is selected, but can be disabled via
    the boot option noautogroup, and can also be turned on/off on
    the fly via:
    
      echo [01] > /proc/sys/kernel/sched_autogroup_enabled
    
    ... which will automatically move tasks to/from the root task group.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    [ Removed the task_group_path() debug code, and fixed !EVENTFD build failure. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <1290281700.28711.9.camel@maggy.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a5b92c70c737..9c2d46da486e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -509,6 +509,8 @@ struct thread_group_cputimer {
 	spinlock_t lock;
 };
 
+struct autogroup;
+
 /*
  * NOTE! "signal_struct" does not have it's own
  * locking, because a shared signal_struct always
@@ -576,6 +578,9 @@ struct signal_struct {
 
 	struct tty_struct *tty; /* NULL if no tty */
 
+#ifdef CONFIG_SCHED_AUTOGROUP
+	struct autogroup *autogroup;
+#endif
 	/*
 	 * Cumulative resource counters for dead threads in the group,
 	 * and for reaped dead child processes forked by this group.
@@ -1927,6 +1932,24 @@ int sched_rt_handler(struct ctl_table *table, int write,
 
 extern unsigned int sysctl_sched_compat_yield;
 
+#ifdef CONFIG_SCHED_AUTOGROUP
+extern unsigned int sysctl_sched_autogroup_enabled;
+
+extern void sched_autogroup_create_attach(struct task_struct *p);
+extern void sched_autogroup_detach(struct task_struct *p);
+extern void sched_autogroup_fork(struct signal_struct *sig);
+extern void sched_autogroup_exit(struct signal_struct *sig);
+#ifdef CONFIG_PROC_FS
+extern void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m);
+extern int proc_sched_autogroup_set_nice(struct task_struct *p, int *nice);
+#endif
+#else
+static inline void sched_autogroup_create_attach(struct task_struct *p) { }
+static inline void sched_autogroup_detach(struct task_struct *p) { }
+static inline void sched_autogroup_fork(struct signal_struct *sig) { }
+static inline void sched_autogroup_exit(struct signal_struct *sig) { }
+#endif
+
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);

commit 24278d148316d2180be6df40e06db013d8b232b8
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Sep 27 17:25:23 2010 -0700

    rcu: priority boosting for TINY_PREEMPT_RCU
    
    Add priority boosting, but only for TINY_PREEMPT_RCU.  This is enabled
    by the default-off RCU_BOOST kernel parameter.  The priority to which to
    boost preempted RCU readers is controlled by the RCU_BOOST_PRIO kernel
    parameter (defaulting to real-time priority 1) and the time to wait
    before boosting the readers blocking a given grace period is controlled
    by the RCU_BOOST_DELAY kernel parameter (defaulting to 500 milliseconds).
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e18473f0eb78..ed1a9bc52b2f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1210,6 +1210,9 @@ struct task_struct {
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+#ifdef CONFIG_RCU_BOOST
+	struct rt_mutex *rcu_boost_mutex;
+#endif /* #ifdef CONFIG_RCU_BOOST */
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
@@ -1745,7 +1748,8 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #ifdef CONFIG_PREEMPT_RCU
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
-#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
+#define RCU_READ_UNLOCK_BOOSTED (1 << 1) /* boosted while in RCU read-side. */
+#define RCU_READ_UNLOCK_NEED_QS (1 << 2) /* RCU core needs CPU response. */
 
 static inline void rcu_copy_process(struct task_struct *p)
 {
@@ -1753,7 +1757,10 @@ static inline void rcu_copy_process(struct task_struct *p)
 	p->rcu_read_unlock_special = 0;
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	p->rcu_blocked_node = NULL;
-#endif
+#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+#ifdef CONFIG_RCU_BOOST
+	p->rcu_boost_mutex = NULL;
+#endif /* #ifdef CONFIG_RCU_BOOST */
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 

commit 004417a6d468e24399e383645c068b498eed84ad
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Nov 25 18:38:29 2010 +0100

    perf, arch: Cleanup perf-pmu init vs lockup-detector
    
    The perf hardware pmu got initialized at various points in the boot,
    some before early_initcall() some after (notably arch_initcall).
    
    The problem is that the NMI lockup detector is ran from early_initcall()
    and expects the hardware pmu to be present.
    
    Sanitize this by moving all architecture hardware pmu implementations to
    initialize at early_initcall() and move the lockup detector to an explicit
    initcall right after that.
    
    Cc: paulus <paulus@samba.org>
    Cc: davem <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1290707759.2145.119.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c79e921a68b..d2e63d1e725c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -316,6 +316,7 @@ extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
 				  size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
 extern int softlockup_thresh;
+void lockup_detector_init(void);
 #else
 static inline void touch_softlockup_watchdog(void)
 {
@@ -326,6 +327,9 @@ static inline void touch_softlockup_watchdog_sync(void)
 static inline void touch_all_softlockup_watchdogs(void)
 {
 }
+static inline void lockup_detector_init(void)
+{
+}
 #endif
 
 #ifdef CONFIG_DETECT_HUNG_TASK

commit 22a867d81707b0a2720bb5f65255265b95d30526
Merge: 5bb6b1ea67a7 3561d43fd289
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Nov 26 15:03:27 2010 +0100

    Merge commit 'v2.6.37-rc3' into sched/core
    
    Merge reason: Pick up latest fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a7a4f8a752ec734b2eab904fc863d5dc873de338
Author: Paul Turner <pjt@google.com>
Date:   Mon Nov 15 15:47:06 2010 -0800

    sched: Add sysctl_sched_shares_window
    
    Introduce a new sysctl for the shares window and disambiguate it from
    sched_time_avg.
    
    A 10ms window appears to be a good compromise between accuracy and performance.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101115234938.112173964@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8abb8aa59664..840f1277492f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1900,6 +1900,7 @@ extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;
 extern unsigned int sysctl_timer_migration;
+extern unsigned int sysctl_sched_shares_window;
 
 int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,

commit 2069dd75c7d0f49355939e5586daf5a9ab216db7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 15 15:47:00 2010 -0800

    sched: Rewrite tg_shares_up)
    
    By tracking a per-cpu load-avg for each cfs_rq and folding it into a
    global task_group load on each tick we can rework tg_shares_up to be
    strictly per-cpu.
    
    This should improve cpu-cgroup performance for smp systems
    significantly.
    
    [ Paul: changed to use queueing cfs_rq + bug fixes ]
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20101115234937.580480400@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 29d953abb5ad..8abb8aa59664 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1885,8 +1885,6 @@ static inline void wake_up_idle_cpu(int cpu) { }
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
-extern unsigned int sysctl_sched_shares_ratelimit;
-extern unsigned int sysctl_sched_shares_thresh;
 extern unsigned int sysctl_sched_child_runs_first;
 
 enum sched_tunable_scaling {

commit 48c5ccae88dcd989d9de507e8510313c6cbd352b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Nov 13 19:32:29 2010 +0100

    sched: Simplify cpu-hot-unplug task migration
    
    While discussing the need for sched_idle_next(), Oleg remarked that
    since try_to_wake_up() ensures sleeping tasks will end up running on a
    sane cpu, we can do away with migrate_live_tasks().
    
    If we then extend the existing hack of migrating current from
    CPU_DYING to migrating the full rq worth of tasks from CPU_DYING, the
    need for the sched_idle_next() abomination disappears as well, since
    idle will be the only possible thread left after the migration thread
    stops.
    
    This greatly simplifies the hot-unplug task migration path, as can be
    seen from the resulting code reduction (and about half the new lines
    are comments).
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1289851597.2109.547.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3cd70cf91fde..29d953abb5ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1871,14 +1871,11 @@ extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 
 #ifdef CONFIG_HOTPLUG_CPU
-extern void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p);
 extern void idle_task_exit(void);
 #else
 static inline void idle_task_exit(void) {}
 #endif
 
-extern void sched_idle_next(void);
-
 #if defined(CONFIG_NO_HZ) && defined(CONFIG_SMP)
 extern void wake_up_idle_cpu(int cpu);
 #else

commit 92fd4d4d67b945c0766416284d4ab236b31542c4
Merge: fe7de49f9d4e e53beacd23d9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Nov 18 13:22:14 2010 +0100

    Merge commit 'v2.6.37-rc2' into sched/core
    
    Merge reason: Move to a .37-rc base.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit aae6d3ddd8b90f5b2c8d79a2b914d1706d124193
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Sep 17 15:02:32 2010 -0700

    sched: Use group weight, idle cpu metrics to fix imbalances during idle
    
    Currently we consider a sched domain to be well balanced when the imbalance
    is less than the domain's imablance_pct. As the number of cores and threads
    are increasing, current values of imbalance_pct (for example 25% for a
    NUMA domain) are not enough to detect imbalances like:
    
    a) On a WSM-EP system (two sockets, each having 6 cores and 12 logical threads),
    24 cpu-hogging tasks get scheduled as 13 on one socket and 11 on another
    socket. Leading to an idle HT cpu.
    
    b) On a hypothetial 2 socket NHM-EX system (each socket having 8 cores and
    16 logical threads), 16 cpu-hogging tasks can get scheduled as 9 on one
    socket and 7 on another socket. Leaving one core in a socket idle
    whereas in another socket we have a core having both its HT siblings busy.
    
    While this issue can be fixed by decreasing the domain's imbalance_pct
    (by making it a function of number of logical cpus in the domain), it
    can potentially cause more task migrations across sched groups in an
    overloaded case.
    
    Fix this by using imbalance_pct only during newly_idle and busy
    load balancing. And during idle load balancing, check if there
    is an imbalance in number of idle cpu's across the busiest and this
    sched_group or if the busiest group has more tasks than its weight that
    the idle cpu in this_group can pull.
    
    Reported-by: Nikhil Rao <ncrao@google.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1284760952.2676.11.camel@sbsiddha-MOBL3.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d0036e52a24a..2c79e921a68b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -862,6 +862,7 @@ struct sched_group {
 	 * single CPU.
 	 */
 	unsigned int cpu_power, cpu_power_orig;
+	unsigned int group_weight;
 
 	/*
 	 * The CPUs this group covers.

commit 1792f17b7210280a3d7ff29da9614ba779cfcedb
Merge: f02a38d86a14 6bff7eccb0d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 30 11:50:37 2010 -0700

    Merge branch 'for-linus' of git://git.infradead.org/users/eparis/notify
    
    * 'for-linus' of git://git.infradead.org/users/eparis/notify: (22 commits)
      Ensure FMODE_NONOTIFY is not set by userspace
      make fanotify_read() restartable across signals
      fsnotify: remove alignment padding from fsnotify_mark on 64 bit builds
      fs/notify/fanotify/fanotify_user.c: fix warnings
      fanotify: Fix FAN_CLOSE comments
      fanotify: do not recalculate the mask if the ignored mask changed
      fanotify: ignore events on directories unless specifically requested
      fsnotify: rename FS_IN_ISDIR to FS_ISDIR
      fanotify: do not send events for irregular files
      fanotify: limit number of listeners per user
      fanotify: allow userspace to override max marks
      fanotify: limit the number of marks in a single fanotify group
      fanotify: allow userspace to override max queue depth
      fsnotify: implement a default maximum queue depth
      fanotify: ignore fanotify ignore marks if open writers
      fanotify: allow userspace to flush all marks
      fsnotify: call fsnotify_parent in perm events
      fsnotify: correctly handle return codes from listeners
      fanotify: use __aligned_u64 in fanotify userspace metadata
      fanotify: implement fanotify listener ordering
      ...

commit 37542b6a7e73e81f8c066a48e6911e476ee3b22f
Merge: c07724e5b868 d4a6f3c32c39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 29 08:05:33 2010 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched_stat: Update sched_info_queue/dequeue() code comments
      sched, cgroup: Fixup broken cgroup movement

commit 4afeff8505cb8a38e36c1ef2bd3447c4b8f87367
Author: Eric Paris <eparis@redhat.com>
Date:   Thu Oct 28 17:21:58 2010 -0400

    fanotify: limit number of listeners per user
    
    fanotify currently has no limit on the number of listeners a given user can
    have open.  This patch limits the total number of listeners per user to
    128.  This is the same as the inotify default limit.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index be7adb7588e5..6f420baf37ca 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -672,6 +672,9 @@ struct user_struct {
 	atomic_t inotify_watches; /* How many inotify watches does this user have? */
 	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
 #endif
+#ifdef CONFIG_FANOTIFY
+	atomic_t fanotify_listeners;
+#endif
 #ifdef CONFIG_EPOLL
 	atomic_t epoll_watches;	/* The number of file descriptors currently watched */
 #endif

commit 9b1bf12d5d51bca178dea21b04a0805e29d60cf1
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 27 15:34:08 2010 -0700

    signals: move cred_guard_mutex from task_struct to signal_struct
    
    Oleg Nesterov pointed out we have to prevent multiple-threads-inside-exec
    itself and we can reuse ->cred_guard_mutex for it.  Yes, concurrent
    execve() has no worth.
    
    Let's move ->cred_guard_mutex from task_struct to signal_struct.  It
    naturally prevent multiple-threads-inside-exec.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3ff5c8519abd..be7adb7588e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -626,6 +626,10 @@ struct signal_struct {
 
 	int oom_adj;		/* OOM kill score adjustment (bit shift) */
 	int oom_score_adj;	/* OOM kill score adjustment */
+
+	struct mutex cred_guard_mutex;	/* guard against foreign influences on
+					 * credential calculations
+					 * (notably. ptrace) */
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */
@@ -1305,9 +1309,6 @@ struct task_struct {
 					 * credentials (COW) */
 	const struct cred __rcu *cred;	/* effective (overridable) subjective task
 					 * credentials (COW) */
-	struct mutex cred_guard_mutex;	/* guard against foreign influences on
-					 * credential calculations
-					 * (notably. ptrace) */
 	struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path

commit b8ed374e202e23caaf9bd77dcadc9de6447faaa8
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Wed Oct 27 15:34:06 2010 -0700

    signals: annotate lock_task_sighand()
    
    lock_task_sighand() grabs sighand->siglock in case of returning non-NULL
    but unlock_task_sighand() releases it unconditionally.  This leads sparse
    to complain about the lock context imbalance.  Rename and wrap
    lock_task_sighand() using __cond_lock() macro to make sparse happy.
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 393ce94e54b7..3ff5c8519abd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2236,9 +2236,16 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
-extern struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
+extern struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 							unsigned long *flags);
 
+#define lock_task_sighand(tsk, flags)					\
+({	struct sighand_struct *__ss;					\
+	__cond_lock(&(tsk)->sighand->siglock,				\
+		    (__ss = __lock_task_sighand(tsk, flags)));		\
+	__ss;								\
+})									\
+
 static inline void unlock_task_sighand(struct task_struct *tsk,
 						unsigned long *flags)
 {

commit 766f9164193f6dda1497bbf3861060198421fb92
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 26 14:22:45 2010 -0700

    kernel: remove PF_FLUSHER
    
    PF_FLUSHER is only ever set, not tested, remove it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 56154bbb8da9..393ce94e54b7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1706,7 +1706,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_DUMPCORE	0x00000200	/* dumped core */
 #define PF_SIGNALED	0x00000400	/* killed by a signal */
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
-#define PF_FLUSHER	0x00001000	/* responsible for disk writeback */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
 #define PF_FREEZING	0x00004000	/* freeze in progress. do not account to load */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */

commit fe7de49f9d4e53f24ec9ef762a503f70b562341c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 20 16:01:12 2010 -0700

    sched: Make sched_param argument static in sched_setscheduler() callers
    
    Andrew Morton pointed out almost all sched_setscheduler() callers are
    using fixed parameters and can be converted to static.  It reduces runtime
    memory use a little.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: James Morris <jmorris@namei.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0383601a927c..849c8670583d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1942,9 +1942,10 @@ extern int task_nice(const struct task_struct *p);
 extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
-extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
+extern int sched_setscheduler(struct task_struct *, int,
+			      const struct sched_param *);
 extern int sched_setscheduler_nocheck(struct task_struct *, int,
-				      struct sched_param *);
+				      const struct sched_param *);
 extern struct task_struct *idle_task(int cpu);
 extern struct task_struct *curr_task(int cpu);
 extern void set_curr_task(int cpu, struct task_struct *p);

commit e9dd2b6837e26fe202708cce5ea4bb4ee3e3482e
Merge: 4f3a29dadaf9 b4627321e185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 22 17:00:32 2010 -0700

    Merge branch 'for-2.6.37/core' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.37/core' of git://git.kernel.dk/linux-2.6-block: (39 commits)
      cfq-iosched: Fix a gcc 4.5 warning and put some comments
      block: Turn bvec_k{un,}map_irq() into static inline functions
      block: fix accounting bug on cross partition merges
      block: Make the integrity mapped property a bio flag
      block: Fix double free in blk_integrity_unregister
      block: Ensure physical block size is unsigned int
      blkio-throttle: Fix possible multiplication overflow in iops calculations
      blkio-throttle: limit max iops value to UINT_MAX
      blkio-throttle: There is no need to convert jiffies to milli seconds
      blkio-throttle: Fix link failure failure on i386
      blkio: Recalculate the throttled bio dispatch time upon throttle limit change
      blkio: Add root group to td->tg_list
      blkio: deletion of a cgroup was causes oops
      blkio: Do not export throttle files if CONFIG_BLK_DEV_THROTTLING=n
      block: set the bounce_pfn to the actual DMA limit rather than to max memory
      block: revert bad fix for memory hotplug causing bounces
      Fix compile error in blk-exec.c for !CONFIG_DETECT_HUNG_TASK
      block: set the bounce_pfn to the actual DMA limit rather than to max memory
      block: Prevent hang_check firing during long I/O
      cfq: improve fsync performance for small files
      ...
    
    Fix up trivial conflicts due to __rcu sparse annotation in include/linux/genhd.h

commit b2b5ce022acf5e9f52f7b78c5579994fdde191d4
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Oct 15 15:24:15 2010 +0200

    sched, cgroup: Fixup broken cgroup movement
    
    Dima noticed that we fail to correct the ->vruntime of sleeping tasks
    when we move them between cgroups.
    
    Reported-by: Dima Zavin <dima@android.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <1287150604.29097.1513.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2cca9a92f5e5..be312c129787 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1073,7 +1073,7 @@ struct sched_class {
 					 struct task_struct *task);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*moved_group) (struct task_struct *p, int on_rq);
+	void (*task_move_group) (struct task_struct *p, int on_rq);
 #endif
 };
 

commit bc4016f48161454a9a8e5eb209b0693c6cde9f62
Merge: 5d70f79b5ef6 b7dadc387975
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 12:55:43 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (29 commits)
      sched: Export account_system_vtime()
      sched: Call tick_check_idle before __irq_enter
      sched: Remove irq time from available CPU power
      sched: Do not account irq time to current task
      x86: Add IRQ_TIME_ACCOUNTING
      sched: Add IRQ_TIME_ACCOUNTING, finer accounting of irq time
      sched: Add a PF flag for ksoftirqd identification
      sched: Consolidate account_system_vtime extern declaration
      sched: Fix softirq time accounting
      sched: Drop group_capacity to 1 only if local group has extra capacity
      sched: Force balancing on newidle balance if local group has capacity
      sched: Set group_imb only a task can be pulled from the busiest cpu
      sched: Do not consider SCHED_IDLE tasks to be cache hot
      sched: Drop all load weight manipulation for RT tasks
      sched: Create special class for stop/migrate work
      sched: Unindent labels
      sched: Comment updates: fix default latency and granularity numbers
      tracing/sched: Add sched_pi_setprio tracepoint
      sched: Give CPU bound RT tasks preference
      sched: Try not to migrate higher priority RT tasks
      ...

commit 5d70f79b5ef6ea2de4f72a37b2d96e2601e40a22
Merge: 888a6f77e041 750ed158bf6c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 12:54:49 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (163 commits)
      tracing: Fix compile issue for trace_sched_wakeup.c
      [S390] hardirq: remove pointless header file includes
      [IA64] Move local_softirq_pending() definition
      perf, powerpc: Fix power_pmu_event_init to not use event->ctx
      ftrace: Remove recursion between recordmcount and scripts/mod/empty
      jump_label: Add COND_STMT(), reducer wrappery
      perf: Optimize sw events
      perf: Use jump_labels to optimize the scheduler hooks
      jump_label: Add atomic_t interface
      jump_label: Use more consistent naming
      perf, hw_breakpoint: Fix crash in hw_breakpoint creation
      perf: Find task before event alloc
      perf: Fix task refcount bugs
      perf: Fix group moving
      irq_work: Add generic hardirq context callbacks
      perf_events: Fix transaction recovery in group_sched_in()
      perf_events: Fix bogus AMD64 generic TLB events
      perf_events: Fix bogus context time tracking
      tracing: Remove parent recording in latency tracer graph options
      tracing: Use one prologue for the preempt irqs off tracer function tracers
      ...

commit b52bfee445d315549d41eacf2fa7c156e7d153d5
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:19 2010 -0700

    sched: Add IRQ_TIME_ACCOUNTING, finer accounting of irq time
    
    s390/powerpc/ia64 have support for CONFIG_VIRT_CPU_ACCOUNTING which does
    the fine granularity accounting of user, system, hardirq, softirq times.
    Adding that option on archs like x86 will be challenging however, given the
    state of TSC reliability on various platforms and also the overhead it will
    add in syscall entry exit.
    
    Instead, add a lighter variant that only does finer accounting of
    hardirq and softirq times, providing precise irq times (instead of timer tick
    based samples). This accounting is added with a new config option
    CONFIG_IRQ_TIME_ACCOUNTING so that there won't be any overhead for users not
    interested in paying the perf penalty.
    
    This accounting is based on sched_clock, with the code being generic.
    So, other archs may find it useful as well.
    
    This patch just adds the core logic and does not enable this logic yet.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-5-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aca0ce675939..2cca9a92f5e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1826,6 +1826,19 @@ extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 #endif
 
+#ifdef CONFIG_IRQ_TIME_ACCOUNTING
+/*
+ * An i/f to runtime opt-in for irq time accounting based off of sched_clock.
+ * The reason for this explicit opt-in is not to have perf penalty with
+ * slow sched_clocks.
+ */
+extern void enable_sched_clock_irqtime(void);
+extern void disable_sched_clock_irqtime(void);
+#else
+static inline void enable_sched_clock_irqtime(void) {}
+static inline void disable_sched_clock_irqtime(void) {}
+#endif
+
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
 extern unsigned long long thread_group_sched_runtime(struct task_struct *task);

commit 6cdd5199daf0cb7b0fcc8dca941af08492612887
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:18 2010 -0700

    sched: Add a PF flag for ksoftirqd identification
    
    To account softirq time cleanly in scheduler, we need to identify whether
    softirq is invoked in ksoftirqd context or softirq at hardirq tail context.
    Add PF_KSOFTIRQD for that purpose.
    
    As all PF flag bits are currently taken, create space by moving one of the
    infrequently used bits (PF_THREAD_BOUND) down in task_struct to be along
    with some other state fields.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-4-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8744e50cb083..aca0ce675939 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1682,6 +1682,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * Per process flags
  */
+#define PF_KSOFTIRQD	0x00000001	/* I am ksoftirqd */
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */

commit 75e1056f5c57050415b64cb761a3acc35d91f013
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:16 2010 -0700

    sched: Fix softirq time accounting
    
    Peter Zijlstra found a bug in the way softirq time is accounted in
    VIRT_CPU_ACCOUNTING on this thread:
    
       http://lkml.indiana.edu/hypermail//linux/kernel/1009.2/01366.html
    
    The problem is, softirq processing uses local_bh_disable internally. There
    is no way, later in the flow, to differentiate between whether softirq is
    being processed or is it just that bh has been disabled. So, a hardirq when bh
    is disabled results in time being wrongly accounted as softirq.
    
    Looking at the code a bit more, the problem exists in !VIRT_CPU_ACCOUNTING
    as well. As account_system_time() in normal tick based accouting also uses
    softirq_count, which will be set even when not in softirq with bh disabled.
    
    Peter also suggested solution of using 2*SOFTIRQ_OFFSET as irq count
    for local_bh_{disable,enable} and using just SOFTIRQ_OFFSET while softirq
    processing. The patch below does that and adds API in_serving_softirq() which
    returns whether we are currently processing softirq or not.
    
    Also changes one of the usages of softirq_count in net/sched/cls_cgroup.c
    to in_serving_softirq.
    
    Looks like many usages of in_softirq really want in_serving_softirq. Those
    changes can be made individually on a case by case basis.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-2-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cdf56693ecbf..8744e50cb083 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2366,9 +2366,9 @@ extern int __cond_resched_lock(spinlock_t *lock);
 
 extern int __cond_resched_softirq(void);
 
-#define cond_resched_softirq() ({				\
-	__might_sleep(__FILE__, __LINE__, SOFTIRQ_OFFSET);	\
-	__cond_resched_softirq();				\
+#define cond_resched_softirq() ({					\
+	__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
+	__cond_resched_softirq();					\
 })
 
 /*

commit e4ecda1b60bfd2333c12bbe71b153d3b6bdc831a
Author: Mark Lord <mlord@pobox.com>
Date:   Sat Sep 25 11:17:22 2010 +0200

    Fix compile error in blk-exec.c for !CONFIG_DETECT_HUNG_TASK
    
    Ensure that 'sysctl_hung_task_timeout_secs' is defined
    even when CONFIG_DETECT_HUNG_TASK is not set.
    This way we can safely reference it without need for
    ifdefs in the code elsewhere.  eg. in block/blk-exec.c
    
    Signed-off-by: Mark Lord <mlord@pobox.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e2a6db2d7dd..dbafa9e34a2d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -336,6 +336,9 @@ extern unsigned long sysctl_hung_task_warnings;
 extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 					 void __user *buffer,
 					 size_t *lenp, loff_t *ppos);
+#else
+/* Avoid need for ifdefs elsewhere in the code */
+enum { sysctl_hung_task_timeout_secs = 0 };
 #endif
 
 /* Attach to any functions which should be ignored in wchan output. */

commit 637bbdc5b83615ef9f45f50399d1c7f27473c713
Author: Dave Young <hidave.darkstar@gmail.com>
Date:   Mon Sep 13 20:19:03 2010 +0800

    sched: Remove unused PF_ALIGNWARN flag
    
    PF_ALIGNWARN is not implemented and it is for 486 as the
    comment.
    
    It is not likely someone will implement this flag feature.
    So here remove this flag and leave the valuable 0x00000001 for
    future use.
    
    Signed-off-by: Dave Young <hidave.darkstar@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <20100913121903.GB22238@darkstar>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b51c53c285b8..cdf56693ecbf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1682,8 +1682,6 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 /*
  * Per process flags
  */
-#define PF_ALIGNWARN	0x00000001	/* Print alignment warning msgs */
-					/* Not implemented yet, only for 486*/
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */

commit 89a1e18731959e9953fae15ddc1a983eb15a4f19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 7 17:34:50 2010 +0200

    perf: Provide a separate task context for swevents
    
    Since software events are always schedulable, mixing them up with
    hardware events (who are not) can lead to funny scheduling oddities.
    
    Giving them their own context solves this.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89d6023c6f82..eb3c1ceec06e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1163,6 +1163,7 @@ struct rcu_node;
 enum perf_event_task_context {
 	perf_invalid_context = -1,
 	perf_hw_context = 0,
+	perf_sw_context,
 	perf_nr_task_contexts,
 };
 

commit 8dc85d547285668e509f86c177bcd4ea055bcaaf
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 2 16:50:03 2010 +0200

    perf: Multiple task contexts
    
    Provide the infrastructure for multiple task contexts.
    
    A more flexible approach would have resulted in more pointer chases
    in the scheduling hot-paths. This approach has the limitation of a
    static number of task contexts.
    
    Since I expect most external PMUs to be system wide, or at least node
    wide (as per the intel uncore unit) they won't actually need a task
    context.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e2a6db2d7dd..89d6023c6f82 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1160,6 +1160,12 @@ struct sched_rt_entity {
 
 struct rcu_node;
 
+enum perf_event_task_context {
+	perf_invalid_context = -1,
+	perf_hw_context = 0,
+	perf_nr_task_contexts,
+};
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -1431,7 +1437,7 @@ struct task_struct {
 	struct futex_pi_state *pi_state_cache;
 #endif
 #ifdef CONFIG_PERF_EVENTS
-	struct perf_event_context *perf_event_ctxp;
+	struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];
 	struct mutex perf_event_mutex;
 	struct list_head perf_event_list;
 #endif

commit 01a08546af311c065f34727787dd0cc8dc0c216f
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Aug 31 10:28:16 2010 +0200

    sched: Add book scheduling domain
    
    On top of the SMT and MC scheduling domains this adds the BOOK scheduling
    domain. This is useful for NUMA like machines which do not have an interface
    which tells which piece of memory is attached to which node or where the
    hardware performs striping.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100831082844.253053798@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e2a6db2d7dd..b51c53c285b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -875,6 +875,7 @@ enum sched_domain_level {
 	SD_LV_NONE = 0,
 	SD_LV_SIBLING,
 	SD_LV_MC,
+	SD_LV_BOOK,
 	SD_LV_CPU,
 	SD_LV_NODE,
 	SD_LV_ALLNODES,

commit a57eb940d130477a799dfb24a570ee04979c0f7f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 29 16:49:16 2010 -0700

    rcu: Add a TINY_PREEMPT_RCU
    
    Implement a small-memory-footprint uniprocessor-only implementation of
    preemptible RCU.  This implementation uses but a single blocked-tasks
    list rather than the combinatorial number used per leaf rcu_node by
    TREE_PREEMPT_RCU, which reduces memory consumption and greatly simplifies
    processing.  This version also takes advantage of uniprocessor execution
    to accelerate grace periods in the case where there are no readers.
    
    The general design is otherwise broadly similar to that of TREE_PREEMPT_RCU.
    
    This implementation is a step towards having RCU implementation driven
    off of the SMP and PREEMPT kernel configuration variables, which can
    happen once this implementation has accumulated sufficient experience.
    
    Removed ACCESS_ONCE() from __rcu_read_unlock() and added barrier() as
    suggested by Steve Rostedt in order to avoid the compiler-reordering
    issue noted by Mathieu Desnoyers (http://lkml.org/lkml/2010/8/16/183).
    
    As can be seen below, CONFIG_TINY_PREEMPT_RCU represents almost 5Kbyte
    savings compared to CONFIG_TREE_PREEMPT_RCU.  Of course, for non-real-time
    workloads, CONFIG_TINY_RCU is even better.
    
            CONFIG_TREE_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               6170     825      28    7023    kernel/rcutree.o
                                       ----
                                       7026    Total
    
            CONFIG_TINY_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               2081      81       8    2170    kernel/rcutiny.o
                                       ----
                                       2183    Total
    
            CONFIG_TINY_RCU (non-preemptible)
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
                719      25       0     744    kernel/rcutiny.o
                                        ---
                                        757    Total
    
    Requested-by: LoÃ¯c Minier <loic.minier@canonical.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c756666c111..e18473f0eb78 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1202,11 +1202,13 @@ struct task_struct {
 	unsigned int policy;
 	cpumask_t cpus_allowed;
 
-#ifdef CONFIG_TREE_PREEMPT_RCU
+#ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
-	struct rcu_node *rcu_blocked_node;
 	struct list_head rcu_node_entry;
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
+#ifdef CONFIG_TREE_PREEMPT_RCU
+	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
@@ -1740,7 +1742,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
-#ifdef CONFIG_TREE_PREEMPT_RCU
+#ifdef CONFIG_PREEMPT_RCU
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
 #define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
@@ -1749,7 +1751,9 @@ static inline void rcu_copy_process(struct task_struct *p)
 {
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_read_unlock_special = 0;
+#ifdef CONFIG_TREE_PREEMPT_RCU
 	p->rcu_blocked_node = NULL;
+#endif
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 

commit 1b0ba1c9037b2265d6e5d0165d31e4c0269b603b
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 24 19:45:09 2010 +0100

    credentials: rcu annotation
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bbffd087476c..2c756666c111 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1288,9 +1288,9 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
-	const struct cred *real_cred;	/* objective and real subjective task
+	const struct cred __rcu *real_cred; /* objective and real subjective task
 					 * credentials (COW) */
-	const struct cred *cred;	/* effective (overridable) subjective task
+	const struct cred __rcu *cred;	/* effective (overridable) subjective task
 					 * credentials (COW) */
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations

commit 2c392b8c3450ceb69ba1b93cb0cddb3998fb8cdc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Feb 24 19:41:39 2010 +0100

    cgroups: __rcu annotations
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Paul Menage <menage@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e2a6db2d7dd..bbffd087476c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1418,7 +1418,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_CGROUPS
 	/* Control Group info protected by css_set_lock */
-	struct css_set *cgroups;
+	struct css_set __rcu *cgroups;
 	/* cg_list protected by css_set_lock and tsk->alloc_lock */
 	struct list_head cg_list;
 #endif

commit d7627467b7a8dd6944885290a03a07ceb28c10eb
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 17 23:52:56 2010 +0100

    Make do_execve() take a const filename pointer
    
    Make do_execve() take a const filename pointer so that kernel_execve() compiles
    correctly on ARM:
    
    arch/arm/kernel/sys_arm.c:88: warning: passing argument 1 of 'do_execve' discards qualifiers from pointer target type
    
    This also requires the argv and envp arguments to be consted twice, once for
    the pointer array and once for the strings the array points to.  This is
    because do_execve() passes a pointer to the filename (now const) to
    copy_strings_kernel().  A simpler alternative would be to cast the filename
    pointer in do_execve() when it's passed to copy_strings_kernel().
    
    do_execve() may not change any of the strings it is passed as part of the argv
    or envp lists as they are some of them in .rodata, so marking these strings as
    const should be fine.
    
    Further kernel_execve() and sys_execve() need to be changed to match.
    
    This has been test built on x86_64, frv, arm and mips.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ce160d68f5e7..1e2a6db2d7dd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2109,7 +2109,9 @@ extern void daemonize(const char *, ...);
 extern int allow_signal(int);
 extern int disallow_signal(int);
 
-extern int do_execve(char *, char __user * __user *, char __user * __user *, struct pt_regs *);
+extern int do_execve(const char *,
+		     const char __user * const __user *,
+		     const char __user * const __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
 struct task_struct *fork_idle(int);
 

commit a63d83f427fbce97a6cea0db2e64b0eb8435cd10
Author: David Rientjes <rientjes@google.com>
Date:   Mon Aug 9 17:19:46 2010 -0700

    oom: badness heuristic rewrite
    
    This a complete rewrite of the oom killer's badness() heuristic which is
    used to determine which task to kill in oom conditions.  The goal is to
    make it as simple and predictable as possible so the results are better
    understood and we end up killing the task which will lead to the most
    memory freeing while still respecting the fine-tuning from userspace.
    
    Instead of basing the heuristic on mm->total_vm for each task, the task's
    rss and swap space is used instead.  This is a better indication of the
    amount of memory that will be freeable if the oom killed task is chosen
    and subsequently exits.  This helps specifically in cases where KDE or
    GNOME is chosen for oom kill on desktop systems instead of a memory
    hogging task.
    
    The baseline for the heuristic is a proportion of memory that each task is
    currently using in memory plus swap compared to the amount of "allowable"
    memory.  "Allowable," in this sense, means the system-wide resources for
    unconstrained oom conditions, the set of mempolicy nodes, the mems
    attached to current's cpuset, or a memory controller's limit.  The
    proportion is given on a scale of 0 (never kill) to 1000 (always kill),
    roughly meaning that if a task has a badness() score of 500 that the task
    consumes approximately 50% of allowable memory resident in RAM or in swap
    space.
    
    The proportion is always relative to the amount of "allowable" memory and
    not the total amount of RAM systemwide so that mempolicies and cpusets may
    operate in isolation; they shall not need to know the true size of the
    machine on which they are running if they are bound to a specific set of
    nodes or mems, respectively.
    
    Root tasks are given 3% extra memory just like __vm_enough_memory()
    provides in LSMs.  In the event of two tasks consuming similar amounts of
    memory, it is generally better to save root's task.
    
    Because of the change in the badness() heuristic's baseline, it is also
    necessary to introduce a new user interface to tune it.  It's not possible
    to redefine the meaning of /proc/pid/oom_adj with a new scale since the
    ABI cannot be changed for backward compatability.  Instead, a new tunable,
    /proc/pid/oom_score_adj, is added that ranges from -1000 to +1000.  It may
    be used to polarize the heuristic such that certain tasks are never
    considered for oom kill while others may always be considered.  The value
    is added directly into the badness() score so a value of -500, for
    example, means to discount 50% of its memory consumption in comparison to
    other tasks either on the system, bound to the mempolicy, in the cpuset,
    or sharing the same memory controller.
    
    /proc/pid/oom_adj is changed so that its meaning is rescaled into the
    units used by /proc/pid/oom_score_adj, and vice versa.  Changing one of
    these per-task tunables will rescale the value of the other to an
    equivalent meaning.  Although /proc/pid/oom_adj was originally defined as
    a bitshift on the badness score, it now shares the same linear growth as
    /proc/pid/oom_score_adj but with different granularity.  This is required
    so the ABI is not broken with userspace applications and allows oom_adj to
    be deprecated for future removal.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9591907c4f79..ce160d68f5e7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -621,7 +621,8 @@ struct signal_struct {
 	struct tty_audit_buf *tty_audit_buf;
 #endif
 
-	int oom_adj;	/* OOM kill score adjustment (bit shift) */
+	int oom_adj;		/* OOM kill score adjustment (bit shift) */
+	int oom_score_adj;	/* OOM kill score adjustment */
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */

commit c4efd6b569b2646e1346a08a4c40286f8bcb5f11
Merge: 4aed2fd8e318 0bcfe7580794
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:39:22 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (27 commits)
      sched: Use correct macro to display sched_child_runs_first in /proc/sched_debug
      sched: No need for bootmem special cases
      sched: Revert nohz_ratelimit() for now
      sched: Reduce update_group_power() calls
      sched: Update rq->clock for nohz balanced cpus
      sched: Fix spelling of sibling
      sched, cpuset: Drop __cpuexit from cpu hotplug callbacks
      sched: Fix the racy usage of thread_group_cputimer() in fastpath_timer_check()
      sched: run_posix_cpu_timers: Don't check ->exit_state, use lock_task_sighand()
      sched: thread_group_cputime: Simplify, document the "alive" check
      sched: Remove the obsolete exit_state/signal hacks
      sched: task_tick_rt: Remove the obsolete ->signal != NULL check
      sched: __sched_setscheduler: Read the RLIMIT_RTPRIO value lockless
      sched: Fix comments to make them DocBook happy
      sched: Fix fix_small_capacity
      powerpc: Exclude arch_sd_sibiling_asym_packing() on UP
      powerpc: Enable asymmetric SMT scheduling on POWER7
      sched: Add asymmetric group packing option for sibling domain
      sched: Fix capacity calculations for SMT4
      sched: Change nohz idle load balancing logic to push model
      ...

commit 0bcfe75807944106a3aa655a54bb610d62f3a7f5
Merge: eebef74695e1 396e894d289d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 5 09:46:29 2010 +0200

    Merge branch 'sched/urgent' into sched/core
    
    Conflicts:
            include/linux/sched.h
    
    Merge reason: Add the leftover .35 urgent bits, fix the conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 61be7fdec2f51b99570cd5dcc30c7848c8e56513
Merge: 12a81c8df13c eb703f98191a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 5 08:45:05 2010 +0200

    Merge branch 'perf/nmi' into perf/core
    
    Conflicts:
            kernel/Makefile
    
    Merge reason: Add the now complete topic, fix the conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3772b734720e1a3f2dc1d95cfdfaa5332f4ccf01
Merge: 9fc3af467d07 9fe6206f4006
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 2 08:29:56 2010 +0200

    Merge commit 'v2.6.35' into perf/core
    
    Conflicts:
            tools/perf/Makefile
            tools/perf/util/hist.c
    
    Merge reason: Resolve the conflicts and update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8f92054e7ca1d3a3ae50fb42d2253ac8730d9b2a
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jul 29 12:45:55 2010 +0100

    CRED: Fix __task_cred()'s lockdep check and banner comment
    
    Fix __task_cred()'s lockdep check by removing the following validation
    condition:
    
            lockdep_tasklist_lock_is_held()
    
    as commit_creds() does not take the tasklist_lock, and nor do most of the
    functions that call it, so this check is pointless and it can prevent
    detection of the RCU lock not being held if the tasklist_lock is held.
    
    Instead, add the following validation condition:
    
            task->exit_state >= 0
    
    to permit the access if the target task is dead and therefore unable to change
    its own credentials.
    
    Fix __task_cred()'s comment to:
    
     (1) discard the bit that says that the caller must prevent the target task
         from being deleted.  That shouldn't need saying.
    
     (2) Add a comment indicating the result of __task_cred() should not be passed
         directly to get_cred(), but rather than get_task_cred() should be used
         instead.
    
    Also put a note into the documentation to enforce this point there too.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 747fcaedddb7..0478888c6899 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -214,6 +214,7 @@ extern char ___assert_task_state[1 - 2*!!(
 
 #define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
 #define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
+#define task_is_dead(task)	((task)->exit_state != 0)
 #define task_is_stopped_or_traced(task)	\
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\

commit dca45ad8af54963c005393a484ad117b8ba6150f
Merge: 68c38fc3cb4e cd5b8f8755a8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 21 21:45:02 2010 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: Move from the -rc3 to the almost-rc6 base.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit eb7beb5c09af75494234ea6acd09d0a647cf7338
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 16 00:50:03 2010 +0200

    tracing: Remove special traces
    
    Special traces type was only used by sysprof. Lets remove it now
    that sysprof ftrace plugin has been dropped.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Soeren Sandmann <sandmann@daimi.au.dk>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 747fcaedddb7..f751ea9dcb7b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2434,18 +2434,6 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 
 #endif /* CONFIG_SMP */
 
-#ifdef CONFIG_TRACING
-extern void
-__trace_special(void *__tr, void *__data,
-		unsigned long arg1, unsigned long arg2, unsigned long arg3);
-#else
-static inline void
-__trace_special(void *__tr, void *__data,
-		unsigned long arg1, unsigned long arg2, unsigned long arg3)
-{
-}
-#endif
-
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 

commit 396e894d289d69bacf5acd983c97cd6e21a14c08
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 9 15:12:27 2010 +0200

    sched: Revert nohz_ratelimit() for now
    
    Norbert reported that nohz_ratelimit() causes his laptop to burn about
    4W (40%) extra. For now back out the change and see if we can adjust
    the power management code to make better decisions.
    
    Reported-by: Norbert Preining <preining@logic.at>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Mike Galbraith <efault@gmx.de>
    Cc: Arjan van de Ven <arjan@infradead.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 747fcaedddb7..6e0bb86de990 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -273,17 +273,11 @@ extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern int select_nohz_load_balancer(int cpu);
 extern int get_nohz_load_balancer(void);
-extern int nohz_ratelimit(int cpu);
 #else
 static inline int select_nohz_load_balancer(int cpu)
 {
 	return 0;
 }
-
-static inline int nohz_ratelimit(int cpu)
-{
-	return 0;
-}
 #endif
 
 /*

commit 8c215bd3890c347dfb6a2db4779755f8b9c298a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 1 09:07:17 2010 +0200

    sched: Cure nr_iowait_cpu() users
    
    Commit 0224cf4c5e (sched: Intoduce get_cpu_iowait_time_us())
    broke things by not making sure preemption was indeed disabled
    by the callers of nr_iowait_cpu() which took the iowait value of
    the current cpu.
    
    This resulted in a heap of preempt warnings. Cure this by making
    nr_iowait_cpu() take a cpu number and fix up the callers to pass
    in the right number.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: linux-pm@lists.linux-foundation.org
    LKML-Reference: <1277968037.1868.120.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f118809c953f..747fcaedddb7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -139,7 +139,7 @@ extern int nr_processes(void);
 extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_iowait(void);
-extern unsigned long nr_iowait_cpu(void);
+extern unsigned long nr_iowait_cpu(int cpu);
 extern unsigned long this_cpu_load(void);
 
 

commit 532cb4c401e225b084c14d6bd6a2f8ee561de2f1
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 8 14:57:02 2010 +1000

    sched: Add asymmetric group packing option for sibling domain
    
    Check to see if the group is packed in a sched doman.
    
    This is primarily intended to used at the sibling level.  Some cores
    like POWER7 prefer to use lower numbered SMT threads.  In the case of
    POWER7, it can move to lower SMT modes only when higher threads are
    idle.  When in lower SMT modes, the threads will perform better since
    they share less core resources.  Hence when we have idle threads, we
    want them to be the higher ones.
    
    This adds a hook into f_b_g() called check_asym_packing() to check the
    packing.  This packing function is run on idle threads.  It checks to
    see if the busiest CPU in this domain (core in the P7 case) has a
    higher CPU number than what where the packing function is being run
    on.  If it is, calculate the imbalance and return the higher busier
    thread as the busiest group to f_b_g().  Here we are assuming a lower
    CPU number will be equivalent to a lower SMT thread number.
    
    It also creates a new SD_ASYM_PACKING flag to enable this feature at
    any scheduler domain level.
    
    It also creates an arch hook to enable this feature at the sibling
    level.  The default function doesn't enable this feature.
    
    Based heavily on patch from Peter Zijlstra.
    Fixes from Srivatsa Vaddagiri.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <20100608045702.2936CCC897@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c731296e5e93..ff154e10752b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -801,7 +801,7 @@ enum cpu_idle_type {
 #define SD_POWERSAVINGS_BALANCE	0x0100	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
-
+#define SD_ASYM_PACKING		0x0800  /* Place busy groups earlier in the domain */
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
 
 enum powersavings_balance_level {
@@ -836,6 +836,8 @@ static inline int sd_balance_for_package_power(void)
 	return SD_PREFER_SIBLING;
 }
 
+extern int __weak arch_sd_sibiling_asym_packing(void);
+
 /*
  * Optimise SD flags for power savings:
  * SD_BALANCE_NEWIDLE helps agressive task consolidation and power savings.

commit 9d5efe05eb0c904545a28b19c18b949f23334de0
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Tue Jun 8 14:57:02 2010 +1000

    sched: Fix capacity calculations for SMT4
    
    Handle cpu capacity being reported as 0 on cores with more number of
    hardware threads. For example on a Power7 core with 4 hardware
    threads, core power is 1177 and thus power of each hardware thread is
    1177/4 = 294. This low power can lead to capacity for each hardware
    thread being calculated as 0, which leads to tasks bouncing within the
    core madly!
    
    Fix this by reporting capacity for hardware threads as 1, provided
    their power is not scaled down significantly because of frequency
    scaling or real-time tasks usage of cpu.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    LKML-Reference: <20100608045702.21D03CC895@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a3e5b1cd0438..c731296e5e93 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -857,7 +857,7 @@ struct sched_group {
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
 	 * single CPU.
 	 */
-	unsigned int cpu_power;
+	unsigned int cpu_power, cpu_power_orig;
 
 	/*
 	 * The CPUs this group covers.

commit 83cd4fe27ad8446619b2e030b171b858501de87d
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Fri May 21 17:09:41 2010 -0700

    sched: Change nohz idle load balancing logic to push model
    
    In the new push model, all idle CPUs indeed go into nohz mode. There is
    still the concept of idle load balancer (performing the load balancing
    on behalf of all the idle cpu's in the system). Busy CPU kicks the nohz
    balancer when any of the nohz CPUs need idle load balancing.
    The kickee CPU does the idle load balancing on behalf of all idle CPUs
    instead of the normal idle balance.
    
    This addresses the below two problems with the current nohz ilb logic:
    * the idle load balancer continued to have periodic ticks during idle and
      wokeup frequently, even though it did not have any rebalancing to do on
      behalf of any of the idle CPUs.
    * On x86 and CPUs that have APIC timer stoppage on idle CPUs, this
      periodic wakeup can result in a periodic additional interrupt on a CPU
      doing the timer broadcast.
    
    Also currently we are migrating the unpinned timers from an idle to the cpu
    doing idle load balancing (when all the cpus in the system are idle,
    there is no idle load balancing cpu and timers get added to the same idle cpu
    where the request was made. So the existing optimization works only on semi idle
    system).
    
    And In semi idle system, we no longer have periodic ticks on the idle load
    balancer CPU. Using that cpu will add more delays to the timers than intended
    (as that cpu's timer base may not be uptodate wrt jiffies etc). This was
    causing mysterious slowdowns during boot etc.
    
    For now, in the semi idle case, use the nearest busy cpu for migrating timers
    from an idle cpu.  This is good for power-savings anyway.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <1274486981.2840.46.camel@sbs-t61.sc.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c2d4316a04bb..a3e5b1cd0438 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -271,14 +271,11 @@ extern int runqueue_is_locked(int cpu);
 
 extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
-extern int select_nohz_load_balancer(int cpu);
-extern int get_nohz_load_balancer(void);
+extern void select_nohz_load_balancer(int stop_tick);
+extern int get_nohz_timer_target(void);
 extern int nohz_ratelimit(int cpu);
 #else
-static inline int select_nohz_load_balancer(int cpu)
-{
-	return 0;
-}
+static inline void select_nohz_load_balancer(int stop_tick) { }
 
 static inline int nohz_ratelimit(int cpu)
 {

commit c676329abb2b8359d9a5d734dec0c81779823fd6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 25 10:48:51 2010 +0200

    sched_clock: Add local_clock() API and improve documentation
    
    For people who otherwise get to write: cpu_clock(smp_processor_id()),
    there is now: local_clock().
    
    Also, as per suggestion from Andrew, provide some documentation on
    the various clock interfaces, and minimize the unsigned long long vs
    u64 mess.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    LKML-Reference: <1275052414.1645.52.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edc3dd168d87..c2d4316a04bb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1791,20 +1791,23 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 #endif
 
 /*
- * Architectures can set this to 1 if they have specified
- * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
- * but then during bootup it turns out that sched_clock()
- * is reliable after all:
+ * Do not use outside of architecture code which knows its limitations.
+ *
+ * sched_clock() has no promise of monotonicity or bounded drift between
+ * CPUs, use (which you should not) requires disabling IRQs.
+ *
+ * Please use one of the three interfaces below.
  */
-#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
-extern int sched_clock_stable;
-#endif
-
-/* ftrace calls sched_clock() directly */
 extern unsigned long long notrace sched_clock(void);
+/*
+ * See the comment in kernel/sched_clock.c
+ */
+extern u64 cpu_clock(int cpu);
+extern u64 local_clock(void);
+extern u64 sched_clock_cpu(int cpu);
+
 
 extern void sched_clock_init(void);
-extern u64 sched_clock_cpu(int cpu);
 
 #ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
 static inline void sched_clock_tick(void)
@@ -1819,17 +1822,19 @@ static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
 #else
+/*
+ * Architectures can set this to 1 if they have specified
+ * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
+ * but then during bootup it turns out that sched_clock()
+ * is reliable after all:
+ */
+extern int sched_clock_stable;
+
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 #endif
 
-/*
- * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
- * clock constructed from sched_clock():
- */
-extern unsigned long long cpu_clock(int cpu);
-
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
 extern unsigned long long thread_group_sched_runtime(struct task_struct *task);

commit 21aa9af03d06cb1d19a3738e5cf12acff984e69b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 8 21:40:37 2010 +0200

    sched: add hooks for workqueue
    
    Concurrency managed workqueue needs to know when workers are going to
    sleep and waking up.  Using these two hooks, cmwq keeps track of the
    current concurrency level and throttles execution of new works if it's
    too high and wakes up another worker from the sleep hook if it becomes
    too low.
    
    This patch introduces PF_WQ_WORKER to identify workqueue workers and
    adds the following two hooks.
    
    * wq_worker_waking_up(): called when a worker is woken up.
    
    * wq_worker_sleeping(): called when a worker is going to sleep and may
      return a pointer to a local task which should be woken up.  The
      returned task is woken up using try_to_wake_up_local() which is
      simplified ttwu which is called under rq lock and can only wake up
      local tasks.
    
    Both hooks are currently defined as noop in kernel/workqueue_sched.h.
    Later cmwq implementation will replace them with proper
    implementation.
    
    These hooks are hard coded as they'll always be enabled.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f118809c953f..edc3dd168d87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1696,6 +1696,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
+#define PF_WQ_WORKER	0x00000020	/* I'm a workqueue worker */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */

commit b3ac022cb9dc5883505a88b159d1b240ad1ef405
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:24 2010 -0700

    proc: turn signal_struct->count into "int nr_threads"
    
    No functional changes, just s/atomic_t count/int nr_threads/.
    
    With the recent changes this counter has a single user, get_nr_threads()
    And, none of its callers need the really accurate number of threads, not
    to mention each caller obviously races with fork/exit.  It is only used to
    report this value to the user-space, except first_tid() uses it to avoid
    the unnecessary while_each_thread() loop in the unlikely case.
    
    It is a bit sad we need a word in struct signal_struct for this, perhaps
    we can change get_nr_threads() to approximate the number of threads using
    signal->live and kill ->nr_threads later.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ccd2d1500720..f118809c953f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -527,8 +527,8 @@ struct thread_group_cputimer {
  */
 struct signal_struct {
 	atomic_t		sigcnt;
-	atomic_t		count;
 	atomic_t		live;
+	int			nr_threads;
 
 	wait_queue_head_t	wait_chldexit;	/* for wait4() */
 
@@ -2149,7 +2149,7 @@ extern bool current_is_single_threaded(void);
 
 static inline int get_nr_threads(struct task_struct *tsk)
 {
-	return atomic_read(&tsk->signal->count);
+	return tsk->signal->nr_threads;
 }
 
 /* de_thread depends on thread_group_leader not being a pid based check */

commit 7e49827cc937a742ae02078b483e3eb78f791a2a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:22 2010 -0700

    proc: get_nr_threads() doesn't need ->siglock any longer
    
    Now that task->signal can't go away get_nr_threads() doesn't need
    ->siglock to read signal->count.
    
    Also, make it inline, move into sched.h, and convert 2 other proc users of
    signal->count to use this (now trivial) helper.
    
    Henceforth get_nr_threads() is the only valid user of signal->count, we
    are ready to turn it into "int nr_threads" or, perhaps, kill it.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dd597d8013a8..ccd2d1500720 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2147,6 +2147,11 @@ extern bool current_is_single_threaded(void);
 #define while_each_thread(g, t) \
 	while ((t = next_thread(t)) != g)
 
+static inline int get_nr_threads(struct task_struct *tsk)
+{
+	return atomic_read(&tsk->signal->count);
+}
+
 /* de_thread depends on thread_group_leader not being a pid based check */
 #define thread_group_leader(p)	(p == p->group_leader)
 

commit a705be6b5e8b05f2ae51536ec709de921960326c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:19 2010 -0700

    kill the obsolete thread_group_cputime_free() helper
    
    Kill the empty thread_group_cputime_free() helper.  It was needed to free
    the per-cpu data which we no longer have.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2d1e1a1228ef..dd597d8013a8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2393,10 +2393,6 @@ static inline void thread_group_cputime_init(struct signal_struct *sig)
 	spin_lock_init(&sig->cputimer.lock);
 }
 
-static inline void thread_group_cputime_free(struct signal_struct *sig)
-{
-}
-
 /*
  * Reevaluate whether the task has signals pending delivery.
  * Wake the task if so.

commit b7b8ff6373d4b910af081f76888395e6df53249d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:18 2010 -0700

    signals: kill the awful task_rq_unlock_wait() hack
    
    Now that task->signal can't go away we can revert the horrible hack added
    by ad474caca3e2a0550b7ce0706527ad5ab389a4d4 ("fix for
    account_group_exec_runtime(), make sure ->signal can't be freed under
    rq->lock").
    
    And we can do more cleanups sched_stats.h/posix-cpu-timers.c later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 32e309df408c..2d1e1a1228ef 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -268,7 +268,6 @@ extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern int runqueue_is_locked(int cpu);
-extern void task_rq_unlock_wait(struct task_struct *p);
 
 extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)

commit ea6d290ca34c4fd91b7348338c0cc7bdeff94a35
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:16 2010 -0700

    signals: make task_struct->signal immutable/refcountable
    
    We have a lot of problems with accessing task_struct->signal, it can
    "disappear" at any moment.  Even current can't use its ->signal safely
    after exit_notify().  ->siglock helps, but it is not convenient, not
    always possible, and sometimes it makes sense to use task->signal even
    after this task has already dead.
    
    This patch adds the reference counter, sigcnt, into signal_struct.  This
    reference is owned by task_struct and it is dropped in
    __put_task_struct().  Perhaps it makes sense to export
    get/put_signal_struct() later, but currently I don't see the immediate
    reason.
    
    Rename __cleanup_signal() to free_signal_struct() and unexport it.  With
    the previous changes it does nothing except kmem_cache_free().
    
    Change __exit_signal() to not clear/free ->signal, it will be freed when
    the last reference to any thread in the thread group goes away.
    
    Note:
            - when the last thead exits signal->tty can point to nowhere, see
              the next patch.
    
            - with or without this patch signal_struct->count should go away,
              or at least it should be "int nr_threads" for fs/proc. This will
              be addressed later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a95a2455cebe..32e309df408c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -527,6 +527,7 @@ struct thread_group_cputimer {
  * the locking of signal_struct.
  */
 struct signal_struct {
+	atomic_t		sigcnt;
 	atomic_t		count;
 	atomic_t		live;
 
@@ -2101,7 +2102,6 @@ extern void flush_thread(void);
 extern void exit_thread(void);
 
 extern void exit_files(struct task_struct *);
-extern void __cleanup_signal(struct signal_struct *);
 extern void __cleanup_sighand(struct sighand_struct *);
 
 extern void exit_itimers(struct signal_struct *);

commit 09faef11df8c559a23e2405d123cb2683733a79a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:11 2010 -0700

    exit: change zap_other_threads() to count sub-threads
    
    Change zap_other_threads() to return the number of other sub-threads found
    on ->thread_group list.
    
    Other changes are cosmetic:
    
            - change the code to use while_each_thread() helper
    
            - remove the obsolete comment about SIGKILL/SIGSTOP
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4f31a166b1a1..a95a2455cebe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2036,7 +2036,7 @@ extern int do_notify_parent(struct task_struct *, int);
 extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 extern void force_sig(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
-extern void zap_other_threads(struct task_struct *p);
+extern int zap_other_threads(struct task_struct *p);
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);

commit 6adef3ebe570bcde67fd6c16101451ddde5712b5
Author: Jack Steiner <steiner@sgi.com>
Date:   Wed May 26 14:42:49 2010 -0700

    cpusets: new round-robin rotor for SLAB allocations
    
    We have observed several workloads running on multi-node systems where
    memory is assigned unevenly across the nodes in the system.  There are
    numerous reasons for this but one is the round-robin rotor in
    cpuset_mem_spread_node().
    
    For example, a simple test that writes a multi-page file will allocate
    pages on nodes 0 2 4 6 ...  Odd nodes are skipped.  (Sometimes it
    allocates on odd nodes & skips even nodes).
    
    An example is shown below.  The program "lfile" writes a file consisting
    of 10 pages.  The program then mmaps the file & uses get_mempolicy(...,
    MPOL_F_NODE) to determine the nodes where the file pages were allocated.
    The output is shown below:
    
            # ./lfile
             allocated on nodes: 2 4 6 0 1 2 6 0 2
    
    There is a single rotor that is used for allocating both file pages & slab
    pages.  Writing the file allocates both a data page & a slab page
    (buffer_head).  This advances the RR rotor 2 nodes for each page
    allocated.
    
    A quick confirmation seems to confirm this is the cause of the uneven
    allocation:
    
            # echo 0 >/dev/cpuset/memory_spread_slab
            # ./lfile
             allocated on nodes: 6 7 8 9 0 1 2 3 4 5
    
    This patch introduces a second rotor that is used for slab allocations.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Paul Menage <menage@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c0151ffd3541..4f31a166b1a1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1423,6 +1423,7 @@ struct task_struct {
 	nodemask_t mems_allowed;	/* Protected by alloc_lock */
 	int mems_allowed_change_disable;
 	int cpuset_mem_spread_rotor;
+	int cpuset_slab_spread_rotor;
 #endif
 #ifdef CONFIG_CGROUPS
 	/* Control Group info protected by css_set_lock */

commit 4be929be34f9bdeffa40d815d32d7d60d2c7f03b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 24 14:33:03 2010 -0700

    kernel-wide: replace USHORT_MAX, SHORT_MAX and SHORT_MIN with USHRT_MAX, SHRT_MAX and SHRT_MIN
    
    - C99 knows about USHRT_MAX/SHRT_MAX/SHRT_MIN, not
      USHORT_MAX/SHORT_MAX/SHORT_MIN.
    
    - Make SHRT_MIN of type s16, not int, for consistency.
    
    [akpm@linux-foundation.org: fix drivers/dma/timb_dma.c]
    [akpm@linux-foundation.org: fix security/keys/keyring.c]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: WANG Cong <xiyou.wangcong@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 415b8f8a3f45..c0151ffd3541 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -384,7 +384,7 @@ struct user_namespace;
  * 1-3 now and depends on arch. We use "5" as safe margin, here.
  */
 #define MAPCOUNT_ELF_CORE_MARGIN	(5)
-#define DEFAULT_MAX_MAP_COUNT	(USHORT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
+#define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
 

commit c0ff7453bb5c7c98e0885fb94279f2571946f280
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon May 24 14:32:08 2010 -0700

    cpuset,mm: fix no node to alloc memory when changing cpuset's mems
    
    Before applying this patch, cpuset updates task->mems_allowed and
    mempolicy by setting all new bits in the nodemask first, and clearing all
    old unallowed bits later.  But in the way, the allocator may find that
    there is no node to alloc memory.
    
    The reason is that cpuset rebinds the task's mempolicy, it cleans the
    nodes which the allocater can alloc pages on, for example:
    
    (mpol: mempolicy)
            task1                   task1's mpol    task2
            alloc page              1
              alloc on node0? NO    1
                                    1               change mems from 1 to 0
                                    1               rebind task1's mpol
                                    0-1               set new bits
                                    0                 clear disallowed bits
              alloc on node1? NO    0
              ...
            can't alloc page
              goto oom
    
    This patch fixes this problem by expanding the nodes range first(set newly
    allowed bits) and shrink it lazily(clear newly disallowed bits).  So we
    use a variable to tell the write-side task that read-side task is reading
    nodemask, and the write-side task clears newly disallowed nodes after
    read-side task ends the current memory allocation.
    
    [akpm@linux-foundation.org: fix spello]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Paul Menage <menage@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b55e988988b5..415b8f8a3f45 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1421,6 +1421,7 @@ struct task_struct {
 #endif
 #ifdef CONFIG_CPUSETS
 	nodemask_t mems_allowed;	/* Protected by alloc_lock */
+	int mems_allowed_change_disable;
 	int cpuset_mem_spread_rotor;
 #endif
 #ifdef CONFIG_CGROUPS

commit b8ae30ee26d379db436b0b8c8c3ff1b52f69e5d1
Merge: 4d7b4ac22fbe 9c6f7e43b4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:27:54 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (49 commits)
      stop_machine: Move local variable closer to the usage site in cpu_stop_cpu_callback()
      sched, wait: Use wrapper functions
      sched: Remove a stale comment
      ondemand: Make the iowait-is-busy time a sysfs tunable
      ondemand: Solve a big performance issue by counting IOWAIT time as busy
      sched: Intoduce get_cpu_iowait_time_us()
      sched: Eliminate the ts->idle_lastupdate field
      sched: Fold updating of the last_update_time_info into update_ts_time_stats()
      sched: Update the idle statistics in get_cpu_idle_time_us()
      sched: Introduce a function to update the idle statistics
      sched: Add a comment to get_cpu_idle_time_us()
      cpu_stop: add dummy implementation for UP
      sched: Remove rq argument to the tracepoints
      rcu: need barrier() in UP synchronize_sched_expedited()
      sched: correctly place paranioa memory barriers in synchronize_sched_expedited()
      sched: kill paranoia check in synchronize_sched_expedited()
      sched: replace migration_thread with cpu_stop
      stop_machine: reimplement using cpu_stop
      cpu_stop: implement stop_cpu[s]()
      sched: Fix select_idle_sibling() logic in select_task_rq_fair()
      ...

commit 4d7b4ac22fbec1a03206c6cde353f2fd6942f828
Merge: 3aaf51ace597 94f3ca95787a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:19:03 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (311 commits)
      perf tools: Add mode to build without newt support
      perf symbols: symbol inconsistency message should be done only at verbose=1
      perf tui: Add explicit -lslang option
      perf options: Type check all the remaining OPT_ variants
      perf options: Type check OPT_BOOLEAN and fix the offenders
      perf options: Check v type in OPT_U?INTEGER
      perf options: Introduce OPT_UINTEGER
      perf tui: Add workaround for slang < 2.1.4
      perf record: Fix bug mismatch with -c option definition
      perf options: Introduce OPT_U64
      perf tui: Add help window to show key associations
      perf tui: Make <- exit menus too
      perf newt: Add single key shortcuts for zoom into DSO and threads
      perf newt: Exit browser unconditionally when CTRL+C, q or Q is pressed
      perf newt: Fix the 'A'/'a' shortcut for annotate
      perf newt: Make <- exit the ui_browser
      x86, perf: P4 PMU - fix counters management logic
      perf newt: Make <- zoom out filters
      perf report: Report number of events, not samples
      perf hist: Clarify events_stats fields usage
      ...
    
    Fix up trivial conflicts in kernel/fork.c and tools/perf/builtin-record.c

commit 19cc36c0f0457e5c6629ec24036fbbe8255c88ec
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 13 02:30:49 2010 +0200

    lockup_detector: Fix forgotten config conversion
    
    Fix forgotten CONFIG_DETECT_SOFTLOCKUP -> CONFIG_LOCKUP_DETECTOR
    in sched.h
    
    Fixes:
            arch/x86/built-in.o: In function `touch_nmi_watchdog':
            (.text+0x1bd59): undefined reference to `touch_softlockup_watchdog'
            kernel/built-in.o: In function `show_state_filter':
            (.text+0x10d01): undefined reference to `touch_all_softlockup_watchdogs'
            kernel/built-in.o: In function `sched_clock_idle_wakeup_event':
            (.text+0x362f9): undefined reference to `touch_softlockup_watchdog'
            kernel/built-in.o: In function `timekeeping_resume':
            timekeeping.c:(.text+0x38757): undefined reference to `touch_softlockup_watchdog'
            kernel/built-in.o: In function `tick_nohz_handler':
            tick-sched.c:(.text+0x3e5b9): undefined reference to `touch_softlockup_watchdog'
            kernel/built-in.o: In function `tick_sched_timer':
            tick-sched.c:(.text+0x3e671): undefined reference to `touch_softlockup_watchdog'
            kernel/built-in.o: In function `tick_check_idle':
            (.text+0x3e90b): undefined reference to `touch_softlockup_watchdog'
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 33f9b2ad0bbb..3958e0cd24f7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -311,7 +311,7 @@ extern void scheduler_tick(void);
 
 extern void sched_show_task(struct task_struct *p);
 
-#ifdef CONFIG_DETECT_SOFTLOCKUP
+#ifdef CONFIG_LOCKUP_DETECTOR
 extern void touch_softlockup_watchdog(void);
 extern void touch_softlockup_watchdog_sync(void);
 extern void touch_all_softlockup_watchdogs(void);

commit 332fbdbca3f7716c5620970755ae054d213bcc4e
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 7 17:11:45 2010 -0400

    lockup_detector: Touch_softlockup cleanups and softlockup_tick removal
    
    Just some code cleanup to make touch_softlockup clearer and remove the
    softlockup_tick function as it is no longer needed.
    
    Also remove the /proc softlockup_thres call as it has been changed to
    watchdog_thres.
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    LKML-Reference: <1273266711-18706-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 37efe8fa5306..33f9b2ad0bbb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -312,19 +312,15 @@ extern void scheduler_tick(void);
 extern void sched_show_task(struct task_struct *p);
 
 #ifdef CONFIG_DETECT_SOFTLOCKUP
-extern void softlockup_tick(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_softlockup_watchdog_sync(void);
 extern void touch_all_softlockup_watchdogs(void);
-extern int proc_dosoftlockup_thresh(struct ctl_table *table, int write,
-				    void __user *buffer,
-				    size_t *lenp, loff_t *ppos);
+extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
+				  void __user *buffer,
+				  size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
 extern int softlockup_thresh;
 #else
-static inline void softlockup_tick(void)
-{
-}
 static inline void touch_softlockup_watchdog(void)
 {
 }
@@ -346,12 +342,6 @@ extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 					 size_t *lenp, loff_t *ppos);
 #endif
 
-#ifdef CONFIG_LOCKUP_DETECTOR
-extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
-				  void __user *buffer,
-				  size_t *lenp, loff_t *ppos);
-#endif
-
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 

commit 58687acba59266735adb8ccd9b5b9aa2c7cd205b
Author: Don Zickus <dzickus@redhat.com>
Date:   Fri May 7 17:11:44 2010 -0400

    lockup_detector: Combine nmi_watchdog and softlockup detector
    
    The new nmi_watchdog (which uses the perf event subsystem) is very
    similar in structure to the softlockup detector.  Using Ingo's
    suggestion, I combined the two functionalities into one file:
    kernel/watchdog.c.
    
    Now both the nmi_watchdog (or hardlockup detector) and softlockup
    detector sit on top of the perf event subsystem, which is run every
    60 seconds or so to see if there are any lockups.
    
    To detect hardlockups, cpus not responding to interrupts, I
    implemented an hrtimer that runs 5 times for every perf event
    overflow event.  If that stops counting on a cpu, then the cpu is
    most likely in trouble.
    
    To detect softlockups, tasks not yielding to the scheduler, I used the
    previous kthread idea that now gets kicked every time the hrtimer fires.
    If the kthread isn't being scheduled neither is anyone else and the
    warning is printed to the console.
    
    I tested this on x86_64 and both the softlockup and hardlockup paths
    work.
    
    V2:
    - cleaned up the Kconfig and softlockup combination
    - surrounded hardlockup cases with #ifdef CONFIG_PERF_EVENTS_NMI
    - seperated out the softlockup case from perf event subsystem
    - re-arranged the enabling/disabling nmi watchdog from proc space
    - added cpumasks for hardlockup failure cases
    - removed fallback to soft events if no PMU exists for hard events
    
    V3:
    - comment cleanups
    - drop support for older softlockup code
    - per_cpu cleanups
    - completely remove software clock base hardlockup detector
    - use per_cpu masking on hard/soft lockup detection
    - #ifdef cleanups
    - rename config option NMI_WATCHDOG to LOCKUP_DETECTOR
    - documentation additions
    
    V4:
    - documentation fixes
    - convert per_cpu to __get_cpu_var
    - powerpc compile fixes
    
    V5:
    - split apart warn flags for hard and soft lockups
    
    TODO:
    - figure out how to make an arch-agnostic clock2cycles call
      (if possible) to feed into perf events as a sample period
    
    [fweisbec: merged conflict patch]
    
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    LKML-Reference: <1273266711-18706-2-git-send-email-dzickus@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dad7f668ebf7..37efe8fa5306 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -346,6 +346,12 @@ extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
 					 size_t *lenp, loff_t *ppos);
 #endif
 
+#ifdef CONFIG_LOCKUP_DETECTOR
+extern int proc_dowatchdog_thresh(struct ctl_table *table, int write,
+				  void __user *buffer,
+				  size_t *lenp, loff_t *ppos);
+#endif
+
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 

commit 34441427aab4bdb3069a4ffcda69a99357abcb2e
Author: Robin Holt <holt@sgi.com>
Date:   Tue May 11 14:06:46 2010 -0700

    revert "procfs: provide stack information for threads" and its fixup commits
    
    Originally, commit d899bf7b ("procfs: provide stack information for
    threads") attempted to introduce a new feature for showing where the
    threadstack was located and how many pages are being utilized by the
    stack.
    
    Commit c44972f1 ("procfs: disable per-task stack usage on NOMMU") was
    applied to fix the NO_MMU case.
    
    Commit 89240ba0 ("x86, fs: Fix x86 procfs stack information for threads on
    64-bit") was applied to fix a bug in ia32 executables being loaded.
    
    Commit 9ebd4eba7 ("procfs: fix /proc/<pid>/stat stack pointer for kernel
    threads") was applied to fix a bug which had kernel threads printing a
    userland stack address.
    
    Commit 1306d603f ('proc: partially revert "procfs: provide stack
    information for threads"') was then applied to revert the stack pages
    being used to solve a significant performance regression.
    
    This patch nearly undoes the effect of all these patches.
    
    The reason for reverting these is it provides an unusable value in
    field 28.  For x86_64, a fork will result in the task->stack_start
    value being updated to the current user top of stack and not the stack
    start address.  This unpredictability of the stack_start value makes
    it worthless.  That includes the intended use of showing how much stack
    space a thread has.
    
    Other architectures will get different values.  As an example, ia64
    gets 0.  The do_fork() and copy_process() functions appear to treat the
    stack_start and stack_size parameters as architecture specific.
    
    I only partially reverted c44972f1 ("procfs: disable per-task stack usage
    on NOMMU") .  If I had completely reverted it, I would have had to change
    mm/Makefile only build pagewalk.o when CONFIG_PROC_PAGE_MONITOR is
    configured.  Since I could not test the builds without significant effort,
    I decided to not change mm/Makefile.
    
    I only partially reverted 89240ba0 ("x86, fs: Fix x86 procfs stack
    information for threads on 64-bit") .  I left the KSTK_ESP() change in
    place as that seemed worthwhile.
    
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: Stefani Seibold <stefani@seibold.net>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dad7f668ebf7..2b7b81df78b3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1497,7 +1497,6 @@ struct task_struct {
 	/* bitmask of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
-	unsigned long stack_start;
 #ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */
 	struct memcg_batch_info {
 		int do_batch;	/* incremented when batch uncharge started */

commit 669c55e9f99b90e46eaa0f98a67ec53d46dc969a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Apr 16 14:59:29 2010 +0200

    sched: Pre-compute cpumask_weight(sched_domain_span(sd))
    
    Dave reported that his large SPARC machines spend lots of time in
    hweight64(), try and optimize some of those needless cpumask_weight()
    invocations (esp. with the large offstack cpumasks these are very
    expensive indeed).
    
    Reported-by: David Miller <davem@davemloft.net>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e3e900f318d7..dfea40574b2a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -960,6 +960,7 @@ struct sched_domain {
 	char *name;
 #endif
 
+	unsigned int span_weight;
 	/*
 	 * Span of all CPUs in this domain.
 	 *

commit 371fd7e7a56a5c136d31aa980011bd2f131c3ef5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 24 16:38:48 2010 +0100

    sched: Add enqueue/dequeue flags
    
    In order to reduce the dependency on TASK_WAKING rework the enqueue
    interface to support a proper flags field.
    
    Replace the int wakeup, bool head arguments with an int flags argument
    and create the following flags:
    
      ENQUEUE_WAKEUP - the enqueue is a wakeup of a sleeping task,
      ENQUEUE_WAKING - the enqueue has relative vruntime due to
                       having sched_class::task_waking() called,
      ENQUEUE_HEAD - the waking task should be places on the head
                     of the priority queue (where appropriate).
    
    For symmetry also convert sched_class::dequeue() to a flags scheme.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fb6c18843ee8..e3e900f318d7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1032,12 +1032,17 @@ struct sched_domain;
 #define WF_SYNC		0x01		/* waker goes to sleep after wakup */
 #define WF_FORK		0x02		/* child wakeup after fork */
 
+#define ENQUEUE_WAKEUP		1
+#define ENQUEUE_WAKING		2
+#define ENQUEUE_HEAD		4
+
+#define DEQUEUE_SLEEP		1
+
 struct sched_class {
 	const struct sched_class *next;
 
-	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup,
-			      bool head);
-	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
+	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
+	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*yield_task) (struct rq *rq);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);

commit 0017d735092844118bef006696a750a0e4ef6ebd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Mar 24 18:34:10 2010 +0100

    sched: Fix TASK_WAKING vs fork deadlock
    
    Oleg noticed a few races with the TASK_WAKING usage on fork.
    
     - since TASK_WAKING is basically a spinlock, it should be IRQ safe
     - since we set TASK_WAKING (*) without holding rq->lock it could
       be there still is a rq->lock holder, thereby not actually
       providing full serialization.
    
    (*) in fact we clear PF_STARTING, which in effect enables TASK_WAKING.
    
    Cure the second issue by not setting TASK_WAKING in sched_fork(), but
    only temporarily in wake_up_new_task() while calling select_task_rq().
    
    Cure the first by holding rq->lock around the select_task_rq() call,
    this will disable IRQs, this however requires that we push down the
    rq->lock release into select_task_rq_fair()'s cgroup stuff.
    
    Because select_task_rq_fair() still needs to drop the rq->lock we
    cannot fully get rid of TASK_WAKING.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8bea40725c76..fb6c18843ee8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1046,7 +1046,8 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
+	int  (*select_task_rq)(struct rq *rq, struct task_struct *p,
+			       int sd_flag, int flags);
 
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);

commit 6a1bdc1b577ebcb65f6603c57f8347309bc4ab13
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 15 10:10:23 2010 +0100

    sched: _cpu_down(): Don't play with current->cpus_allowed
    
    _cpu_down() changes the current task's affinity and then recovers it at
    the end. The problems are well known: we can't restore old_allowed if it
    was bound to the now-dead-cpu, and we can race with the userspace which
    can change cpu-affinity during unplug.
    
    _cpu_down() should not play with current->cpus_allowed at all. Instead,
    take_cpu_down() can migrate the caller of _cpu_down() after __cpu_disable()
    removes the dying cpu from cpu_online_mask.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100315091023.GA9148@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43c945152732..8bea40725c76 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1843,6 +1843,7 @@ extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 
 #ifdef CONFIG_HOTPLUG_CPU
+extern void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p);
 extern void idle_task_exit(void);
 #else
 static inline void idle_task_exit(void) {}

commit c9494727cf293ae2ec66af57547a3e79c724fec2
Merge: 6427462bfa50 42be79e37e26
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 2 20:02:55 2010 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: update to latest upstream
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dad7f668ebf7..e0447c64af6a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -99,7 +99,6 @@ struct futex_pi_state;
 struct robust_list_head;
 struct bio_list;
 struct fs_struct;
-struct bts_context;
 struct perf_event_context;
 
 /*
@@ -1272,12 +1271,6 @@ struct task_struct {
 	struct list_head ptraced;
 	struct list_head ptrace_entry;
 
-	/*
-	 * This is the tracer handle for the ptrace BTS extension.
-	 * This field actually belongs to the ptracer task.
-	 */
-	struct bts_context *bts;
-
 	/* PID/PID hash table linkage. */
 	struct pid_link pids[PIDTYPE_MAX];
 	struct list_head thread_group;
@@ -2123,10 +2116,8 @@ extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
-extern void wait_task_context_switch(struct task_struct *p);
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
-static inline void wait_task_context_switch(struct task_struct *p) {}
 static inline unsigned long wait_task_inactive(struct task_struct *p,
 					       long match_state)
 {

commit 4e3eaddd142e2142c048c5052a0a9d2604fccfc6
Merge: 8655e7e3ddec b97c4bc16734
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 13 14:43:01 2010 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      locking: Make sparse work with inline spinlocks and rwlocks
      x86/mce: Fix RCU lockdep splats
      rcu: Increase RCU CPU stall timeouts if PROVE_RCU
      ftrace: Replace read_barrier_depends() with rcu_dereference_raw()
      rcu: Suppress RCU lockdep warnings during early boot
      rcu, ftrace: Fix RCU lockdep splat in ftrace_perf_buf_prepare()
      rcu: Suppress __mpol_dup() false positive from RCU lockdep
      rcu: Make rcu_read_lock_sched_held() handle !PREEMPT
      rcu: Add control variables to lockdep_rcu_dereference() diagnostics
      rcu, cgroup: Relax the check in task_subsys_state() as early boot is now handled by lockdep-RCU
      rcu: Use wrapper function instead of exporting tasklist_lock
      sched, rcu: Fix rcu_dereference() for RCU-lockdep
      rcu: Make task_subsys_state() RCU-lockdep checks handle boot-time use
      rcu: Fix holdoff for accelerated GPs for last non-dynticked CPU
      x86/gart: Unexport gart_iommu_aperture
    
    Fix trivial conflicts in kernel/trace/ftrace.c

commit c32da02342b7521df25fefc2ef20aee0e61cf887
Merge: dca1d9f6d7ae 318ae2edc3b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 12 16:04:50 2010 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (56 commits)
      doc: fix typo in comment explaining rb_tree usage
      Remove fs/ntfs/ChangeLog
      doc: fix console doc typo
      doc: cpuset: Update the cpuset flag file
      Fix of spelling in arch/sparc/kernel/leon_kernel.c no longer needed
      Remove drivers/parport/ChangeLog
      Remove drivers/char/ChangeLog
      doc: typo - Table 1-2 should refer to "status", not "statm"
      tree-wide: fix typos "ass?o[sc]iac?te" -> "associate" in comments
      No need to patch AMD-provided drivers/gpu/drm/radeon/atombios.h
      devres/irq: Fix devm_irq_match comment
      Remove reference to kthread_create_on_cpu
      tree-wide: Assorted spelling fixes
      tree-wide: fix 'lenght' typo in comments and code
      drm/kms: fix spelling in error message
      doc: capitalization and other minor fixes in pnp doc
      devres: typo fix s/dev/devm/
      Remove redundant trailing semicolons from macros
      fix typo "definetly" -> "definitely" in comment
      tree-wide: s/widht/width/g typo in comments
      ...
    
    Fix trivial conflict in Documentation/laptops/00-INDEX

commit 93c59907c6f247d09239135caecf294a106a2ae0
Author: Veaceslav Falico <vfalico@redhat.com>
Date:   Wed Mar 10 15:23:03 2010 -0800

    copy_signal() cleanup: clean thread_group_cputime_init()
    
    Remove unneeded initializations in thread_group_cputime_init() and in
    posix_cpu_timers_init_group().  They are useless after kmem_cache_zalloc()
    was used in copy_signal().
    
    Signed-off-by: Veaceslav Falico <vfalico@redhat.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 46c6f8d5dc06..ca635c128482 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2391,9 +2391,7 @@ void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
 static inline void thread_group_cputime_init(struct signal_struct *sig)
 {
-	sig->cputimer.cputime = INIT_CPUTIME;
 	spin_lock_init(&sig->cputimer.lock);
-	sig->cputimer.running = 0;
 }
 
 static inline void thread_group_cputime_free(struct signal_struct *sig)

commit e12f31d3e5d36328c7fbd0fce40a95e70b59152c
Author: Mike Galbraith <efault@gmx.de>
Date:   Thu Mar 11 17:15:51 2010 +0100

    sched: Remove avg_overlap
    
    Both avg_overlap and avg_wakeup had an inherent problem in that their accuracy
    was detrimentally affected by cross-cpu wakeups, this because we are missing
    the necessary call to update_curr().  This can't be fixed without increasing
    overhead in our already too fat fastpath.
    
    Additionally, with recent load balancing changes making us prefer to place tasks
    in an idle cache domain (which is good for compute bound loads), communicating
    tasks suffer when a sync wakeup, which would enable affine placement, is turned
    into a non-sync wakeup by SYNC_LESS.  With one task on the runqueue, wake_affine()
    rejects the affine wakeup request, leaving the unfortunate where placed, taking
    frequent cache misses.
    
    Remove it, and recover some fastpath cycles.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1268301121.6785.30.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 70c560f5ada0..8604884cee87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1180,9 +1180,6 @@ struct sched_entity {
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
 
-	u64			last_wakeup;
-	u64			avg_overlap;
-
 	u64			nr_migrations;
 
 #ifdef CONFIG_SCHEDSTATS

commit b42e0c41a422a212ddea0666d5a3a0e3c35206db
Author: Mike Galbraith <efault@gmx.de>
Date:   Thu Mar 11 17:15:38 2010 +0100

    sched: Remove avg_wakeup
    
    Testing the load which led to this heuristic (nfs4 kbuild) shows that it has
    outlived it's usefullness.  With intervening load balancing changes, I cannot
    see any difference with/without, so recover there fastpath cycles.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1268301062.6785.29.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 13efe7dac5fa..70c560f5ada0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1185,9 +1185,6 @@ struct sched_entity {
 
 	u64			nr_migrations;
 
-	u64			start_runtime;
-	u64			avg_wakeup;
-
 #ifdef CONFIG_SCHEDSTATS
 	struct sched_statistics statistics;
 #endif

commit 39c0cbe2150cbd848a25ba6cdb271d1ad46818ad
Author: Mike Galbraith <efault@gmx.de>
Date:   Thu Mar 11 17:17:13 2010 +0100

    sched: Rate-limit nohz
    
    Entering nohz code on every micro-idle is costing ~10% throughput for netperf
    TCP_RR when scheduling cross-cpu.  Rate limiting entry fixes this, but raises
    ticks a bit.  On my Q6600, an idle box goes from ~85 interrupts/sec to 128.
    
    The higher the context switch rate, the more nohz entry costs.  With this patch
    and some cycle recovery patches in my tree, max cross cpu context switch rate is
    improved by ~16%, a large portion of which of which is this ratelimiting.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1268301003.6785.28.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8cc863d66477..13efe7dac5fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -271,11 +271,17 @@ extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern int select_nohz_load_balancer(int cpu);
 extern int get_nohz_load_balancer(void);
+extern int nohz_ratelimit(int cpu);
 #else
 static inline int select_nohz_load_balancer(int cpu)
 {
 	return 0;
 }
+
+static inline int nohz_ratelimit(int cpu)
+{
+	return 0;
+}
 #endif
 
 /*

commit 41acab8851a0408c1d5ad6c21a07456f88b54d40
Author: Lucas De Marchi <lucas.de.marchi@gmail.com>
Date:   Wed Mar 10 23:37:45 2010 -0300

    sched: Implement group scheduler statistics in one struct
    
    Put all statistic fields of sched_entity in one struct, sched_statistics,
    and embed it into sched_entity.
    
    This change allows to memset the sched_statistics to 0 when needed (for
    instance when forking), avoiding bugs of non initialized fields.
    
    Signed-off-by: Lucas De Marchi <lucas.de.marchi@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1268275065-18542-1-git-send-email-lucas.de.marchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b1753f7e48e..8cc863d66477 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1127,36 +1127,8 @@ struct load_weight {
 	unsigned long weight, inv_weight;
 };
 
-/*
- * CFS stats for a schedulable entity (task, task-group etc)
- *
- * Current field usage histogram:
- *
- *     4 se->block_start
- *     4 se->run_node
- *     4 se->sleep_start
- *     6 se->load.weight
- */
-struct sched_entity {
-	struct load_weight	load;		/* for load-balancing */
-	struct rb_node		run_node;
-	struct list_head	group_node;
-	unsigned int		on_rq;
-
-	u64			exec_start;
-	u64			sum_exec_runtime;
-	u64			vruntime;
-	u64			prev_sum_exec_runtime;
-
-	u64			last_wakeup;
-	u64			avg_overlap;
-
-	u64			nr_migrations;
-
-	u64			start_runtime;
-	u64			avg_wakeup;
-
 #ifdef CONFIG_SCHEDSTATS
+struct sched_statistics {
 	u64			wait_start;
 	u64			wait_max;
 	u64			wait_count;
@@ -1188,6 +1160,30 @@ struct sched_entity {
 	u64			nr_wakeups_affine_attempts;
 	u64			nr_wakeups_passive;
 	u64			nr_wakeups_idle;
+};
+#endif
+
+struct sched_entity {
+	struct load_weight	load;		/* for load-balancing */
+	struct rb_node		run_node;
+	struct list_head	group_node;
+	unsigned int		on_rq;
+
+	u64			exec_start;
+	u64			sum_exec_runtime;
+	u64			vruntime;
+	u64			prev_sum_exec_runtime;
+
+	u64			last_wakeup;
+	u64			avg_overlap;
+
+	u64			nr_migrations;
+
+	u64			start_runtime;
+	u64			avg_wakeup;
+
+#ifdef CONFIG_SCHEDSTATS
+	struct sched_statistics statistics;
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit 318ae2edc3b29216abd8a2510f3f80b764f06858
Merge: 25cf84cf377c 3e58974027b0
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Mar 8 16:55:37 2010 +0100

    Merge branch 'for-next' into for-linus
    
    Conflicts:
            Documentation/filesystems/proc.txt
            arch/arm/mach-u300/include/mach/debug-macro.S
            drivers/net/qlge/qlge_ethtool.c
            drivers/net/qlge/qlge_main.c
            drivers/net/typhoon.c

commit 34e55232e59f7b19050267a05ff1226e5cd122a5
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Fri Mar 5 13:41:40 2010 -0800

    mm: avoid false sharing of mm_counter
    
    Considering the nature of per mm stats, it's the shared object among
    threads and can be a cache-miss point in the page fault path.
    
    This patch adds per-thread cache for mm_counter.  RSS value will be
    counted into a struct in task_struct and synchronized with mm's one at
    events.
    
    Now, in this patch, the event is the number of calls to handle_mm_fault.
    Per-thread value is added to mm at each 64 calls.
    
     rough estimation with small benchmark on parallel thread (2threads) shows
     [before]
         4.5 cache-miss/faults
     [after]
         4.0 cache-miss/faults
     Anyway, the most contended object is mmap_sem if the number of threads grows.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cbeafa49a53b..46c6f8d5dc06 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1220,7 +1220,9 @@ struct task_struct {
 	struct plist_node pushable_tasks;
 
 	struct mm_struct *mm, *active_mm;
-
+#if defined(SPLIT_RSS_COUNTING)
+	struct task_rss_stat	rss_stat;
+#endif
 /* task state */
 	int exit_state;
 	int exit_code, exit_signal;

commit d559db086ff5be9bcc259e5aa50bf3d881eaf1d1
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Fri Mar 5 13:41:39 2010 -0800

    mm: clean up mm_counter
    
    Presently, per-mm statistics counter is defined by macro in sched.h
    
    This patch modifies it to
      - defined in mm.h as inlinf functions
      - use array instead of macro's name creation.
    
    This patch is for reducing patch size in future patch to modify
    implementation of per-mm counter.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b1753f7e48e..cbeafa49a53b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -396,60 +396,6 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-#if USE_SPLIT_PTLOCKS
-/*
- * The mm counters are not protected by its page_table_lock,
- * so must be incremented atomically.
- */
-#define set_mm_counter(mm, member, value) atomic_long_set(&(mm)->_##member, value)
-#define get_mm_counter(mm, member) ((unsigned long)atomic_long_read(&(mm)->_##member))
-#define add_mm_counter(mm, member, value) atomic_long_add(value, &(mm)->_##member)
-#define inc_mm_counter(mm, member) atomic_long_inc(&(mm)->_##member)
-#define dec_mm_counter(mm, member) atomic_long_dec(&(mm)->_##member)
-
-#else  /* !USE_SPLIT_PTLOCKS */
-/*
- * The mm counters are protected by its page_table_lock,
- * so can be incremented directly.
- */
-#define set_mm_counter(mm, member, value) (mm)->_##member = (value)
-#define get_mm_counter(mm, member) ((mm)->_##member)
-#define add_mm_counter(mm, member, value) (mm)->_##member += (value)
-#define inc_mm_counter(mm, member) (mm)->_##member++
-#define dec_mm_counter(mm, member) (mm)->_##member--
-
-#endif /* !USE_SPLIT_PTLOCKS */
-
-#define get_mm_rss(mm)					\
-	(get_mm_counter(mm, file_rss) + get_mm_counter(mm, anon_rss))
-#define update_hiwater_rss(mm)	do {			\
-	unsigned long _rss = get_mm_rss(mm);		\
-	if ((mm)->hiwater_rss < _rss)			\
-		(mm)->hiwater_rss = _rss;		\
-} while (0)
-#define update_hiwater_vm(mm)	do {			\
-	if ((mm)->hiwater_vm < (mm)->total_vm)		\
-		(mm)->hiwater_vm = (mm)->total_vm;	\
-} while (0)
-
-static inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)
-{
-	return max(mm->hiwater_rss, get_mm_rss(mm));
-}
-
-static inline void setmax_mm_hiwater_rss(unsigned long *maxrss,
-					 struct mm_struct *mm)
-{
-	unsigned long hiwater_rss = get_mm_hiwater_rss(mm);
-
-	if (*maxrss < hiwater_rss)
-		*maxrss = hiwater_rss;
-}
-
-static inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)
-{
-	return max(mm->hiwater_vm, mm->total_vm);
-}
 
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);

commit db1466b3e1bd1727375cdbfcbea4bcce2f860f61
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 3 07:46:56 2010 -0800

    rcu: Use wrapper function instead of exporting tasklist_lock
    
    Lockdep-RCU commit d11c563d exported tasklist_lock, which is not
    a good thing.  This patch instead exports a function that uses
    lockdep to check whether tasklist_lock is held.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    Cc: Christoph Hellwig <hch@lst.de>
    LKML-Reference: <1267631219-8713-1-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0eef87b58ea5..a47af2064dcc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -258,6 +258,10 @@ extern spinlock_t mmlist_lock;
 
 struct task_struct;
 
+#ifdef CONFIG_PROVE_RCU
+extern int lockdep_tasklist_lock_is_held(void);
+#endif /* #ifdef CONFIG_PROVE_RCU */
+
 extern void sched_init(void);
 extern void sched_init_smp(void);
 extern asmlinkage void schedule_tail(struct task_struct *prev);

commit b1bf9368407ae7e89d8a005bb40beb70a41df539
Merge: 524df5572521 4671a1322052
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 1 09:00:29 2010 -0800

    Merge branch 'for-2.6.34' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.34' of git://git.kernel.dk/linux-2.6-block: (38 commits)
      block: don't access jiffies when initialising io_context
      cfq: remove 8 bytes of padding from cfq_rb_root on 64 bit builds
      block: fix for "Consolidate phys_segment and hw_segment limits"
      cfq-iosched: quantum check tweak
      blktrace: perform cleanup after setup error
      blkdev: fix merge_bvec_fn return value checks
      cfq-iosched: requests "in flight" vs "in driver" clarification
      cciss: Fix problem with scatter gather elements in the scsi half of the driver
      cciss: eliminate unnecessary pointer use in cciss scsi code
      cciss: do not use void pointer for scsi hba data
      cciss: factor out scatter gather chain block mapping code
      cciss: fix scatter gather chain block dma direction kludge
      cciss: simplify scatter gather code
      cciss: factor out scatter gather chain block allocation and freeing
      cciss: detect bad alignment of scsi commands at build time
      cciss: clarify command list padding calculation
      cfq-iosched: rethink seeky detection for SSDs
      cfq-iosched: rework seeky detection
      block: remove padding from io_context on 64bit builds
      block: Consolidate phys_segment and hw_segment limits
      ...

commit bddd87c7e622ea681c665049027ed84cdcafcb09
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Tue Feb 23 08:55:42 2010 +0100

    blk-core: use BIO list management functions
    
    Now that the bio list management stuff is generic, convert
    generic_make_request to use bio lists instead of its own private bio
    list implementation.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78efe7c485ac..7eb82975e1ee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -97,7 +97,7 @@ struct sched_param {
 struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
-struct bio;
+struct bio_list;
 struct fs_struct;
 struct bts_context;
 struct perf_event_context;
@@ -1466,7 +1466,7 @@ struct task_struct {
 	void *journal_info;
 
 /* stacked block device info */
-	struct bio *bio_list, **bio_tail;
+	struct bio_list *bio_list;
 
 /* VM state */
 	struct reclaim_state *reclaim_state;

commit 6e40f5bbbc734231bc5809d3eb785e3c21f275d7
Merge: 301ba0457f1e 0970d2992dfd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 16 16:48:56 2010 +0100

    Merge branch 'sched/urgent' into sched/core
    
    Conflicts: kernel/sched.c
    
    Necessary due to the urgent fixes which conflict with the code move
    from sched.c to sched_fair.c
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 28f5318167adf23b16c844b9c2253f355cb21796
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Mon Feb 8 15:35:55 2010 +0530

    sched: Fix sched_mv_power_savings for !SMT
    
    Fix for sched_mc_powersavigs for pre-Nehalem platforms.
    Child sched domain should clear SD_PREFER_SIBLING if parent will have
    SD_POWERSAVINGS_BALANCE because they are contradicting.
    
    Sets the flags correctly based on sched_mc_power_savings.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100208100555.GD2931@dirshya.in.ibm.com>
    Cc: stable@kernel.org [2.6.32.x]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78efe7c485ac..1f5fa53b46b1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -878,7 +878,10 @@ static inline int sd_balance_for_mc_power(void)
 	if (sched_smt_power_savings)
 		return SD_POWERSAVINGS_BALANCE;
 
-	return SD_PREFER_SIBLING;
+	if (!sched_mc_power_savings)
+		return SD_PREFER_SIBLING;
+
+	return 0;
 }
 
 static inline int sd_balance_for_package_power(void)

commit 3ad2f3fbb961429d2aa627465ae4829758bc7e07
Author: Daniel Mack <daniel@caiaq.de>
Date:   Wed Feb 3 08:01:28 2010 +0800

    tree-wide: Assorted spelling fixes
    
    In particular, several occurances of funny versions of 'success',
    'unknown', 'therefore', 'acknowledge', 'argument', 'achieve', 'address',
    'beginning', 'desirable', 'separate' and 'necessary' are fixed.
    
    Signed-off-by: Daniel Mack <daniel@caiaq.de>
    Cc: Joe Perches <joe@perches.com>
    Cc: Junio C Hamano <gitster@pobox.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index abdfacc58653..a70957b138ed 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1533,7 +1533,7 @@ struct task_struct {
 
 	struct list_head	*scm_work_list;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	/* Index of current stored adress in ret_stack */
+	/* Index of current stored address in ret_stack */
 	int curr_ret_stack;
 	/* Stack of return addresses for return function tracing */
 	struct ftrace_ret_stack	*ret_stack;

commit 6d3e0907b8b239d16720d144e2675ecf10d3bc3b
Merge: 23577256953c 50200df46202
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 8 08:55:43 2010 +0100

    Merge branch 'sched/urgent' into sched/core
    
    Merge reason: Merge dependent fix, update to latest -rc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit aa16cd8d12352ccb5b921995ab3901110779f24a
Merge: a3a71ca9a715 59647b6ac305
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 4 16:07:41 2010 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      futex: Handle futex value corruption gracefully
      futex: Handle user space corruption gracefully
      futex_lock_pi() key refcnt fix
      softlockup: Add sched_clock_tick() to avoid kernel warning on kgdb resume

commit d6ad3e286d2c075a60b9f11075a2c55aeeeca2ad
Author: Jason Wessel <jason.wessel@windriver.com>
Date:   Wed Jan 27 16:25:22 2010 -0600

    softlockup: Add sched_clock_tick() to avoid kernel warning on kgdb resume
    
    When CONFIG_HAVE_UNSTABLE_SCHED_CLOCK is set, sched_clock() gets
    the time from hardware such as the TSC on x86. In this
    configuration kgdb will report a softlock warning message on
    resuming or detaching from a debug session.
    
    Sequence of events in the problem case:
    
     1) "cpu sched clock" and "hardware time" are at 100 sec prior
        to a call to kgdb_handle_exception()
    
     2) Debugger waits in kgdb_handle_exception() for 80 sec and on
        exit the following is called ...  touch_softlockup_watchdog() -->
        __raw_get_cpu_var(touch_timestamp) = 0;
    
     3) "cpu sched clock" = 100s (it was not updated, because the
        interrupt was disabled in kgdb) but the "hardware time" = 180 sec
    
     4) The first timer interrupt after resuming from
        kgdb_handle_exception updates the watchdog from the "cpu sched clock"
    
    update_process_times() { ...  run_local_timers() -->
    softlockup_tick() --> check (touch_timestamp == 0) (it is "YES"
    here, we have set "touch_timestamp = 0" at kgdb) -->
    __touch_softlockup_watchdog() ***(A)--> reset "touch_timestamp"
    to "get_timestamp()" (Here, the "touch_timestamp" will still be
    set to 100s.)  ...
    
        scheduler_tick() ***(B)--> sched_clock_tick() (update "cpu sched
        clock" to "hardware time" = 180s) ...  }
    
     5) The Second timer interrupt handler appears to have a large
        jump and trips the softlockup warning.
    
    update_process_times() { ...  run_local_timers() -->
    softlockup_tick() --> "cpu sched clock" - "touch_timestamp" =
    180s-100s > 60s --> printk "soft lockup error messages" ...  }
    
    note: ***(A) reset "touch_timestamp" to
    "get_timestamp(this_cpu)"
    
    Why is "touch_timestamp" 100 sec, instead of 180 sec?
    
    When CONFIG_HAVE_UNSTABLE_SCHED_CLOCK is set, the call trace of
    get_timestamp() is:
    
    get_timestamp(this_cpu)
     -->cpu_clock(this_cpu)
     -->sched_clock_cpu(this_cpu)
     -->__update_sched_clock(sched_clock_data, now)
    
    The __update_sched_clock() function uses the GTOD tick value to
    create a window to normalize the "now" values.  So if "now"
    value is too big for sched_clock_data, it will be ignored.
    
    The fix is to invoke sched_clock_tick() to update "cpu sched
    clock" in order to recover from this state.  This is done by
    introducing the function touch_softlockup_watchdog_sync(). This
    allows kgdb to request that the sched clock is updated when the
    watchdog thread runs the first time after a resume from kgdb.
    
    [yong.zhang0@gmail.com: Use per cpu instead of an array]
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>
    Signed-off-by: Dongdong Deng <Dongdong.Deng@windriver.com>
    Cc: kgdb-bugreport@lists.sourceforge.net
    Cc: peterz@infradead.org
    LKML-Reference: <1264631124-4837-2-git-send-email-jason.wessel@windriver.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6f7bba93929b..89232151a9d0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -310,6 +310,7 @@ extern void sched_show_task(struct task_struct *p);
 #ifdef CONFIG_DETECT_SOFTLOCKUP
 extern void softlockup_tick(void);
 extern void touch_softlockup_watchdog(void);
+extern void touch_softlockup_watchdog_sync(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern int proc_dosoftlockup_thresh(struct ctl_table *table, int write,
 				    void __user *buffer,
@@ -323,6 +324,9 @@ static inline void softlockup_tick(void)
 static inline void touch_softlockup_watchdog(void)
 {
 }
+static inline void touch_softlockup_watchdog_sync(void)
+{
+}
 static inline void touch_all_softlockup_watchdogs(void)
 {
 }

commit 221af7f87b97431e3ee21ce4b0e77d5411cf1549
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 28 22:14:42 2010 -0800

    Split 'flush_old_exec' into two functions
    
    'flush_old_exec()' is the point of no return when doing an execve(), and
    it is pretty badly misnamed.  It doesn't just flush the old executable
    environment, it also starts up the new one.
    
    Which is very inconvenient for things like setting up the new
    personality, because we want the new personality to affect the starting
    of the new environment, but at the same time we do _not_ want the new
    personality to take effect if flushing the old one fails.
    
    As a result, the x86-64 '32-bit' personality is actually done using this
    insane "I'm going to change the ABI, but I haven't done it yet" bit
    (TIF_ABI_PENDING), with SET_PERSONALITY() not actually setting the
    personality, but just the "pending" bit, so that "flush_thread()" can do
    the actual personality magic.
    
    This patch in no way changes any of that insanity, but it does split the
    'flush_old_exec()' function up into a preparatory part that can fail
    (still called flush_old_exec()), and a new part that will actually set
    up the new exec environment (setup_new_exec()).  All callers are changed
    to trivially comply with the new world order.
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6f7bba93929b..abdfacc58653 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1369,7 +1369,7 @@ struct task_struct {
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
 				       it with task_lock())
-				     - initialized normally by flush_old_exec */
+				     - initialized normally by setup_new_exec */
 /* file system info */
 	int link_count, total_link_count;
 #ifdef CONFIG_SYSVIPC

commit ea87bb7853168434f4a82426dd1ea8421f9e604d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 20 20:58:57 2010 +0000

    sched: Extend enqueue_task to allow head queueing
    
    The ability of enqueueing a task to the head of a SCHED_FIFO priority
    list is required to fix some violations of POSIX scheduling policy.
    
    Extend the related functions with a "head" argument.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Tested-by: Carsten Emde <cbe@osadl.org>
    Tested-by: Mathias Weber <mathias.weber.mw1@roche.com>
    LKML-Reference: <20100120171629.734886007@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8b079735ae5f..b35c0c7130c8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1067,7 +1067,8 @@ struct sched_domain;
 struct sched_class {
 	const struct sched_class *next;
 
-	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
+	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup,
+			      bool head);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
 	void (*yield_task) (struct rq *rq);
 

commit 7c9414385ebfdd87cc542d4e7e3bb0dbb2d3ce25
Author: Dhaval Giani <dhaval.giani@gmail.com>
Date:   Wed Jan 20 13:26:18 2010 +0100

    sched: Remove USER_SCHED
    
    Remove the USER_SCHED feature. It has been scheduled to be removed in
    2.6.34 as per http://marc.info/?l=linux-kernel&m=125728479022976&w=2
    
    Signed-off-by: Dhaval Giani <dhaval.giani@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1263990378.24844.3.camel@localhost>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 50d685cde70e..8b079735ae5f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -731,14 +731,6 @@ struct user_struct {
 	uid_t uid;
 	struct user_namespace *user_ns;
 
-#ifdef CONFIG_USER_SCHED
-	struct task_group *tg;
-#ifdef CONFIG_SYSFS
-	struct kobject kobj;
-	struct delayed_work work;
-#endif
-#endif
-
 #ifdef CONFIG_PERF_EVENTS
 	atomic_long_t locked_vm;
 #endif
@@ -2502,13 +2494,9 @@ extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
 extern void normalize_rt_tasks(void);
 
-#ifdef CONFIG_GROUP_SCHED
+#ifdef CONFIG_CGROUP_SCHED
 
 extern struct task_group init_task_group;
-#ifdef CONFIG_USER_SCHED
-extern struct task_group root_task_group;
-extern void set_tg_uid(struct user_struct *user);
-#endif
 
 extern struct task_group *sched_create_group(struct task_group *parent);
 extern void sched_destroy_group(struct task_group *tg);

commit 3d45fd804a95055ecab5b3eed81f5ab2dbb047a2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Dec 17 17:12:46 2009 +0100

    sched: Remove the sched_class load_balance methods
    
    Take out the sched_class methods for load-balancing.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f2f842db03ce..50d685cde70e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1087,14 +1087,6 @@ struct sched_class {
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
 
-	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
-			struct rq *busiest, unsigned long max_load_move,
-			struct sched_domain *sd, enum cpu_idle_type idle,
-			int *all_pinned, int *this_best_prio);
-
-	int (*move_one_task) (struct rq *this_rq, int this_cpu,
-			      struct rq *busiest, struct sched_domain *sd,
-			      enum cpu_idle_type idle);
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_waking) (struct rq *this_rq, struct task_struct *task);

commit efc1a3b16930c41d64ffefde16b87d82f603a8a0
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jan 15 17:01:35 2010 -0800

    nommu: don't need get_unmapped_area() for NOMMU
    
    get_unmapped_area() is unnecessary for NOMMU as no-one calls it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Greg Ungerer <gerg@snapgear.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8d4991be9d53..6f7bba93929b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -377,6 +377,8 @@ extern int sysctl_max_map_count;
 
 #include <linux/aio.h>
 
+#ifdef CONFIG_MMU
+extern void arch_pick_mmap_layout(struct mm_struct *mm);
 extern unsigned long
 arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
 		       unsigned long, unsigned long);
@@ -386,6 +388,9 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 			  unsigned long flags);
 extern void arch_unmap_area(struct mm_struct *, unsigned long);
 extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
+#else
+static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
+#endif
 
 #if USE_SPLIT_PTLOCKS
 /*
@@ -2491,8 +2496,6 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 
 #endif /* CONFIG_SMP */
 
-extern void arch_pick_mmap_layout(struct mm_struct *mm);
-
 #ifdef CONFIG_TRACING
 extern void
 __trace_special(void *__tr, void *__data,

commit 3e10e716abf3c71bdb5d86b8f507f9e72236c9cd
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Thu Nov 19 17:16:37 2009 +0100

    resource: add helpers for fetching rlimits
    
    We want to be sure that compiler fetches the limit variable only
    once, so add helpers for fetching current and maximal resource
    limits which do that.
    
    Add them to sched.h (instead of resource.h) due to circular dependency
     sched.h->resource.h->task_struct
    Alternative would be to create a separate res_access.h or similar.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: James Morris <jmorris@namei.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f2f842db03ce..8d4991be9d53 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2601,6 +2601,28 @@ static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 }
 #endif /* CONFIG_MM_OWNER */
 
+static inline unsigned long task_rlimit(const struct task_struct *tsk,
+		unsigned int limit)
+{
+	return ACCESS_ONCE(tsk->signal->rlim[limit].rlim_cur);
+}
+
+static inline unsigned long task_rlimit_max(const struct task_struct *tsk,
+		unsigned int limit)
+{
+	return ACCESS_ONCE(tsk->signal->rlim[limit].rlim_max);
+}
+
+static inline unsigned long rlimit(unsigned int limit)
+{
+	return task_rlimit(current, limit);
+}
+
+static inline unsigned long rlimit_max(unsigned int limit)
+{
+	return task_rlimit_max(current, limit);
+}
+
 #endif /* __KERNEL__ */
 
 #endif

commit aac3d39693529ca538e37ebdb6ed5d6432a697c7
Merge: 10e5453ffa0d 077614ee1e93
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 19 09:47:49 2009 -0800

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (25 commits)
      sched: Fix broken assertion
      sched: Assert task state bits at build time
      sched: Update task_state_arraypwith new states
      sched: Add missing state chars to TASK_STATE_TO_CHAR_STR
      sched: Move TASK_STATE_TO_CHAR_STR near the TASK_state bits
      sched: Teach might_sleep() about preemptible RCU
      sched: Make warning less noisy
      sched: Simplify set_task_cpu()
      sched: Remove the cfs_rq dependency from set_task_cpu()
      sched: Add pre and post wakeup hooks
      sched: Move kthread_bind() back to kthread.c
      sched: Fix select_task_rq() vs hotplug issues
      sched: Fix sched_exec() balancing
      sched: Ensure set_task_cpu() is never called on blocked tasks
      sched: Use TASK_WAKING for fork wakups
      sched: Select_task_rq_fair() must honour SD_LOAD_BALANCE
      sched: Fix task_hot() test order
      sched: Fix set_cpu_active() in cpu_down()
      sched: Mark boot-cpu active before smp_init()
      sched: Fix cpu_clock() in NMIs, on !CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
      ...

commit 55db493b65c7b6bb5d7bd3dd3c8a2fe13f5dc09c
Merge: efc8e7f4c83d a4636818f8e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 17 17:00:20 2009 -0800

    Merge branch 'cpumask-cleanups' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux-2.6-for-linus
    
    * 'cpumask-cleanups' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux-2.6-for-linus:
      cpumask: rename tsk_cpumask to tsk_cpus_allowed
      cpumask: don't recommend set_cpus_allowed hack in Documentation/cpu-hotplug.txt
      cpumask: avoid dereferencing struct cpumask
      cpumask: convert drivers/idle/i7300_idle.c to cpumask_var_t
      cpumask: use modern cpumask style in drivers/scsi/fcoe/fcoe.c
      cpumask: avoid deprecated function in mm/slab.c
      cpumask: use cpu_online in kernel/perf_event.c

commit b6e3224fb20954f155e41ec5709b2ab70b50ae2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 17 13:23:24 2009 -0800

    Revert "task_struct: make journal_info conditional"
    
    This reverts commit e4c570c4cb7a95dbfafa3d016d2739bf3fdfe319, as
    requested by Alexey:
    
     "I think I gave a good enough arguments to not merge it.
      To iterate:
       * patch makes impossible to start using ext3 on EXT3_FS=n kernels
         without reboot.
       * this is done only for one pointer on task_struct"
    
      None of config options which define task_struct are tristate directly
      or effectively."
    
    Requested-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 244c287a5ac1..211ed32befbd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1446,10 +1446,8 @@ struct task_struct {
 	gfp_t lockdep_reclaim_gfp;
 #endif
 
-#ifdef CONFIG_FS_JOURNAL_INFO
 /* journalling filesystem info */
 	void *journal_info;
-#endif
 
 /* stacked block device info */
 	struct bio *bio_list, **bio_tail;

commit e1781538cf5c870ab696e9b8f0a5c498d3900f2f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Dec 17 13:16:30 2009 +0100

    sched: Assert task state bits at build time
    
    Since everybody is lazy and prone to forgetting things, make the
    compiler help us a bit.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20091217121830.060186433@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 94858df38a87..37543876ddf5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -192,9 +192,13 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define TASK_DEAD		64
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
+#define TASK_STATE_MAX		512
 
 #define TASK_STATE_TO_CHAR_STR "RSDTtZXxKW"
 
+extern char ___assert_task_state[1 - 2*!!(
+		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];
+
 /* Convenience macros for the sake of set_task_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)

commit 44d90df6b757c59651ddd55f1a84f28132b50d29
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Dec 17 13:16:28 2009 +0100

    sched: Add missing state chars to TASK_STATE_TO_CHAR_STR
    
    We grew 3 new task states since the last time someone touched
    it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20091217121829.892737686@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c28ed1b1d7c2..94858df38a87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -193,7 +193,7 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+#define TASK_STATE_TO_CHAR_STR "RSDTtZXxKW"
 
 /* Convenience macros for the sake of set_task_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)

commit 733421516b42c44b9e21f1793c430cc801ef8324
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Dec 17 13:16:27 2009 +0100

    sched: Move TASK_STATE_TO_CHAR_STR near the TASK_state bits
    
    So that we don't keep forgetting about it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20091217121829.815779372@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 973b2b89f86d..c28ed1b1d7c2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -193,6 +193,8 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
 
+#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+
 /* Convenience macros for the sake of set_task_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
@@ -2595,8 +2597,6 @@ static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 }
 #endif /* CONFIG_MM_OWNER */
 
-#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
-
 #endif /* __KERNEL__ */
 
 #endif

commit a4636818f8e0991f32d9528f39cf4f3d6a7d30a3
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Dec 17 11:43:29 2009 -0600

    cpumask: rename tsk_cpumask to tsk_cpus_allowed
    
    Noone uses this wrapper yet, and Ingo asked that it be kept consistent
    with current task_struct usage.
    
    (One user crept in via linux-next: fixed)
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au.
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 244c287a5ac1..4d7adb282bdd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1555,7 +1555,7 @@ struct task_struct {
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
-#define tsk_cpumask(tsk) (&(tsk)->cpus_allowed)
+#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT

commit 88ec22d3edb72b261f8628226cd543589a6d5e1b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 16 18:04:41 2009 +0100

    sched: Remove the cfs_rq dependency from set_task_cpu()
    
    In order to remove the cfs_rq dependency from set_task_cpu() we
    need to ensure the task is cfs_rq invariant for all callsites.
    
    The simple approach is to substract cfs_rq->min_vruntime from
    se->vruntime on dequeue, and add cfs_rq->min_vruntime on
    enqueue.
    
    However, this has the downside of breaking FAIR_SLEEPERS since
    we loose the old vruntime as we only maintain the relative
    position.
    
    To solve this, we observe that we only migrate runnable tasks,
    we do this using deactivate_task(.sleep=0) and
    activate_task(.wakeup=0), therefore we can restrain the
    min_vruntime invariance to that state.
    
    The only other case is wakeup balancing, since we want to
    maintain the old vruntime we cannot make it relative on dequeue,
    but since we don't migrate inactive tasks, we can do so right
    before we activate it again.
    
    This is where we need the new pre-wakeup hook, we need to call
    this while still holding the old rq->lock. We could fold it into
    ->select_task_rq(), but since that has multiple callsites and
    would obfuscate the locking requirements, that seems like a
    fudge.
    
    This leaves the fork() case, simply make sure that ->task_fork()
    leaves the ->vruntime in a relative state.
    
    This covers all cases where set_task_cpu() gets called, and
    ensures it sees a relative vruntime.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <20091216170518.191697025@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c9fa1ccebff..973b2b89f86d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1116,7 +1116,7 @@ struct sched_class {
 					 struct task_struct *task);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*moved_group) (struct task_struct *p);
+	void (*moved_group) (struct task_struct *p, int on_rq);
 #endif
 };
 

commit efbbd05a595343a413964ad85a2ad359b7b7efbd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 16 18:04:40 2009 +0100

    sched: Add pre and post wakeup hooks
    
    As will be apparent in the next patch, we need a pre wakeup hook
    for sched_fair task migration, hence rename the post wakeup hook
    and one pre wakeup.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <20091216170518.114746117@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c858f38e81a..2c9fa1ccebff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1091,7 +1091,8 @@ struct sched_class {
 			      enum cpu_idle_type idle);
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
-	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
+	void (*task_waking) (struct rq *this_rq, struct task_struct *task);
+	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
 				 const struct cpumask *newmask);

commit ad09750b51150ca87531b8790a379214a974c167
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Dec 15 16:47:25 2009 -0800

    signals: kill force_sig_specific()
    
    Kill force_sig_specific(), this trivial wrapper has no callers.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 57b3516f055b..244c287a5ac1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2083,7 +2083,6 @@ extern int kill_proc_info(int, struct siginfo *, pid_t);
 extern int do_notify_parent(struct task_struct *, int);
 extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 extern void force_sig(int, struct task_struct *);
-extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
 extern void zap_other_threads(struct task_struct *p);
 extern struct sigqueue *sigqueue_alloc(void);

commit 614c517d7c00af1b26ded20646b329397d6f51a1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Dec 15 16:47:22 2009 -0800

    signals: SEND_SIG_NOINFO should be considered as SI_FROMUSER()
    
    No changes in compiled code. The patch adds the new helper, si_fromuser()
    and changes check_kill_permission() to use this helper.
    
    The real effect of this patch is that from now we "officially" consider
    SEND_SIG_NOINFO signal as "from user-space" signals. This is already true
    if we look at the code which uses SEND_SIG_NOINFO, except __send_signal()
    has another opinion - see the next patch.
    
    The naming of these special SEND_SIG_XXX siginfo's is really bad
    imho.  From __send_signal()'s pov they mean
    
            SEND_SIG_NOINFO         from user
            SEND_SIG_PRIV           from kernel
            SEND_SIG_FORCED         no info
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Reviewed-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f4c145410a8d..57b3516f055b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2102,11 +2102,6 @@ static inline int kill_cad_pid(int sig, int priv)
 #define SEND_SIG_PRIV	((struct siginfo *) 1)
 #define SEND_SIG_FORCED	((struct siginfo *) 2)
 
-static inline int is_si_special(const struct siginfo *info)
-{
-	return info <= SEND_SIG_FORCED;
-}
-
 /*
  * True if we are on the alternate signal stack.
  */

commit 569b846df54ffb2827b83ce3244c5f032394cba4
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Dec 15 16:47:03 2009 -0800

    memcg: coalesce uncharge during unmap/truncate
    
    In massive parallel enviroment, res_counter can be a performance
    bottleneck.  One strong techinque to reduce lock contention is reducing
    calls by coalescing some amount of calls into one.
    
    Considering charge/uncharge chatacteristic,
            - charge is done one by one via demand-paging.
            - uncharge is done by
                    - in chunk at munmap, truncate, exit, execve...
                    - one by one via vmscan/paging.
    
    It seems we have a chance to coalesce uncharges for improving scalability
    at unmap/truncation.
    
    This patch is a for coalescing uncharge.  For avoiding scattering memcg's
    structure to functions under /mm, this patch adds memcg batch uncharge
    information to the task.  A reason for per-task batching is for making use
    of caller's context information.  We do batched uncharge (deleyed
    uncharge) when truncation/unmap occurs but do direct uncharge when
    uncharge is called by memory reclaim (vmscan.c).
    
    The degree of coalescing depends on callers
      - at invalidate/trucate... pagevec size
      - at unmap ....ZAP_BLOCK_SIZE
    (memory itself will be freed in this degree.)
    Then, we'll not coalescing too much.
    
    On x86-64 8cpu server, I tested overheads of memcg at page fault by
    running a program which does map/fault/unmap in a loop. Running
    a task per a cpu by taskset and see sum of the number of page faults
    in 60secs.
    
    [without memcg config]
      40156968  page-faults              #      0.085 M/sec   ( +-   0.046% )
      27.67 cache-miss/faults
    [root cgroup]
      36659599  page-faults              #      0.077 M/sec   ( +-   0.247% )
      31.58 miss/faults
    [in a child cgroup]
      18444157  page-faults              #      0.039 M/sec   ( +-   0.133% )
      69.96 miss/faults
    [child with this patch]
      27133719  page-faults              #      0.057 M/sec   ( +-   0.155% )
      47.16 miss/faults
    
    We can see some amounts of improvement.
    (root cgroup doesn't affected by this patch)
    Another patch for "charge" will follow this and above will be improved more.
    
    Changelog(since 2009/10/02):
     - renamed filed of memcg_batch (as pages to bytes, memsw to memsw_bytes)
     - some clean up and commentary/description updates.
     - added initialize code to copy_process(). (possible bug fix)
    
    Changelog(old):
     - fixed !CONFIG_MEM_CGROUP case.
     - rebased onto the latest mmotm + softlimit fix patches.
     - unified patch for callers
     - added commetns.
     - make ->do_batch as bool.
     - removed css_get() at el. We don't need it.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c858f38e81a..f4c145410a8d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1544,6 +1544,14 @@ struct task_struct {
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
 	unsigned long stack_start;
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR /* memcg uses this to do batch job */
+	struct memcg_batch_info {
+		int do_batch;	/* incremented when batch uncharge started */
+		struct mem_cgroup *memcg; /* target memcg of uncharge */
+		unsigned long bytes; 		/* uncharged usage */
+		unsigned long memsw_bytes; /* uncharged mem+swap usage */
+	} memcg_batch;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 8f0ddf91f2aeb09602373e400cf8b403e9017210
Merge: 050cbb09dac0 b5f91da0a697
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 15 09:02:01 2009 -0800

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (26 commits)
      clockevents: Convert to raw_spinlock
      clockevents: Make tick_device_lock static
      debugobjects: Convert to raw_spinlocks
      perf_event: Convert to raw_spinlock
      hrtimers: Convert to raw_spinlocks
      genirq: Convert irq_desc.lock to raw_spinlock
      smp: Convert smplocks to raw_spinlocks
      rtmutes: Convert rtmutex.lock to raw_spinlock
      sched: Convert pi_lock to raw_spinlock
      sched: Convert cpupri lock to raw_spinlock
      sched: Convert rt_runtime_lock to raw_spinlock
      sched: Convert rq->lock to raw_spinlock
      plist: Make plist debugging raw_spinlock aware
      bkl: Fixup core_lock fallout
      locking: Cleanup the name space completely
      locking: Further name space cleanups
      alpha: Fix fallout from locking changes
      locking: Implement new raw_spinlock
      locking: Convert raw_rwlock functions to arch_rwlock
      locking: Convert raw_rwlock to arch_rwlock
      ...

commit e4c570c4cb7a95dbfafa3d016d2739bf3fdfe319
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Dec 14 18:00:26 2009 -0800

    task_struct: make journal_info conditional
    
    journal_info in task_struct is used in journaling file system only.  So
    introduce CONFIG_FS_JOURNAL_INFO and make it conditional.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: KONISHI Ryusuke <konishi.ryusuke@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 294eb2f80144..7d388494f45d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1446,8 +1446,10 @@ struct task_struct {
 	gfp_t lockdep_reclaim_gfp;
 #endif
 
+#ifdef CONFIG_FS_JOURNAL_INFO
 /* journalling filesystem info */
 	void *journal_info;
+#endif
 
 /* stacked block device info */
 	struct bio *bio_list, **bio_tail;

commit 1d615482547584b9a8bb6316a58fed6ce90dd9ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 14:54:03 2009 +0100

    sched: Convert pi_lock to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 294eb2f80144..41a9ea322dce 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1409,7 +1409,7 @@ struct task_struct {
 #endif
 
 	/* Protection of the PI data structures: */
-	spinlock_t pi_lock;
+	raw_spinlock_t pi_lock;
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */

commit 702a7c7609bec3a940b6a46b0d6ab9ce45274580
Merge: 053fe57ac249 b9889ed1ddec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 12 11:34:10 2009 -0800

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (21 commits)
      sched: Remove forced2_migrations stats
      sched: Fix memory leak in two error corner cases
      sched: Fix build warning in get_update_sysctl_factor()
      sched: Update normalized values on user updates via proc
      sched: Make tunable scaling style configurable
      sched: Fix missing sched tunable recalculation on cpu add/remove
      sched: Fix task priority bug
      sched: cgroup: Implement different treatment for idle shares
      sched: Remove unnecessary RCU exclusion
      sched: Discard some old bits
      sched: Clean up check_preempt_wakeup()
      sched: Move update_curr() in check_preempt_wakeup() to avoid redundant call
      sched: Sanitize fork() handling
      sched: Clean up ttwu() rq locking
      sched: Remove rq->clock coupling from set_task_cpu()
      sched: Consolidate select_task_rq() callers
      sched: Remove sysctl.sched_features
      sched: Protect sched_rr_get_param() access to task->sched_class
      sched: Protect task->cpus_allowed access in sched_getaffinity()
      sched: Fix balance vs hotplug race
      ...
    
    Fixed up conflicts in kernel/sysctl.c (due to sysctl cleanup)

commit 6f696eb17be741668810fe1f798135c7cf6733e2
Merge: c4e194e3b71f 125580380f41
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 11 20:47:30 2009 -0800

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (57 commits)
      x86, perf events: Check if we have APIC enabled
      perf_event: Fix variable initialization in other codepaths
      perf kmem: Fix unused argument build warning
      perf symbols: perf_header__read_build_ids() offset'n'size should be u64
      perf symbols: dsos__read_build_ids() should read both user and kernel buildids
      perf tools: Align long options which have no short forms
      perf kmem: Show usage if no option is specified
      sched: Mark sched_clock() as notrace
      perf sched: Add max delay time snapshot
      perf tools: Correct size given to memset
      perf_event: Fix perf_swevent_hrtimer() variable initialization
      perf sched: Fix for getting task's execution time
      tracing/kprobes: Fix field creation's bad error handling
      perf_event: Cleanup for cpu_clock_perf_event_update()
      perf_event: Allocate children's perf_event_ctxp at the right time
      perf_event: Clean up __perf_event_init_context()
      hw-breakpoints: Modify breakpoints without unregistering them
      perf probe: Update perf-probe document
      perf probe: Support --del option
      trace-kprobe: Support delete probe syntax
      ...

commit b9889ed1ddeca5a3f3569c8de7354e9e97d803ae
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 10 20:32:39 2009 +0100

    sched: Remove forced2_migrations stats
    
    This build warning:
    
     kernel/sched.c: In function 'set_task_cpu':
     kernel/sched.c:2070: warning: unused variable 'old_rq'
    
    Made me realize that the forced2_migrations stat looks pretty
    pointless (and a misnomer) - remove it.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ee9f200d12d3..87b89a827f0c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1174,7 +1174,6 @@ struct sched_entity {
 	u64			nr_failed_migrations_running;
 	u64			nr_failed_migrations_hot;
 	u64			nr_forced_migrations;
-	u64			nr_forced2_migrations;
 
 	u64			nr_wakeups;
 	u64			nr_wakeups_sync;

commit 1bbfa6f25673019dc0acc9308b667c96f6cda8bf
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Wed Dec 9 20:07:03 2009 -0500

    sched: Mark sched_clock() as notrace
    
    The core ftrace code (trace_clock_local) calls sched_clock()
    directly, so we don't want to recurisvely trigger the ftrace
    code.  Rather than update every sched_clock() definition, tag
    the prototype for everyone as notrace.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1260407223-10900-1-git-send-email-vapier@gentoo.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75e6e60bf583..576d838adf68 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1836,7 +1836,8 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 extern int sched_clock_stable;
 #endif
 
-extern unsigned long long sched_clock(void);
+/* ftrace calls sched_clock() directly */
+extern unsigned long long notrace sched_clock(void);
 
 extern void sched_clock_init(void);
 extern u64 sched_clock_cpu(int cpu);

commit 1983a922a1bc843806b9a36cf3a370b242783140
Author: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
Date:   Mon Nov 30 12:16:47 2009 +0100

    sched: Make tunable scaling style configurable
    
    As scaling now takes place on all kind of cpu add/remove events a user
    that configures values via proc should be able to configure if his set
    values are still rescaled or kept whatever happens.
    
    As the comments state that log2 was just a second guess that worked the
    interface is not just designed for on/off, but to choose a scaling type.
    Currently this allows none, log and linear, but more important it allwos
    us to keep the interface even if someone has an even better idea how to
    scale the values.
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1259579808-11357-3-git-send-email-ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b1ebd3280c6..ee9f200d12d3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1902,13 +1902,22 @@ extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_shares_ratelimit;
 extern unsigned int sysctl_sched_shares_thresh;
 extern unsigned int sysctl_sched_child_runs_first;
+
+enum sched_tunable_scaling {
+	SCHED_TUNABLESCALING_NONE,
+	SCHED_TUNABLESCALING_LOG,
+	SCHED_TUNABLESCALING_LINEAR,
+	SCHED_TUNABLESCALING_END,
+};
+extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
+
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;
 extern unsigned int sysctl_timer_migration;
 
-int sched_nr_latency_handler(struct ctl_table *table, int write,
+int sched_proc_update_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *length,
 		loff_t *ppos);
 #endif

commit 6cecd084d0fd27bb1e498e2829fd45846d806856
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Nov 30 13:00:37 2009 +0100

    sched: Discard some old bits
    
    WAKEUP_RUNNING was an experiment, not sure why that ever ended up being
    merged...
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 31d9dec78675..4b1ebd3280c6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1152,8 +1152,6 @@ struct sched_entity {
 	u64			start_runtime;
 	u64			avg_wakeup;
 
-	u64			avg_running;
-
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;

commit cd29fe6f2637cc2ccbda5ac65f5332d6bf5fa3c6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 27 17:32:46 2009 +0100

    sched: Sanitize fork() handling
    
    Currently we try to do task placement in wake_up_new_task() after we do
    the load-balance pass in sched_fork(). This yields complicated semantics
    in that we have to deal with tasks on different RQs and the
    set_task_cpu() calls in copy_process() and sched_fork()
    
    Rename ->task_new() to ->task_fork() and call it from sched_fork()
    before the balancing, this gives the policy a clear point to place the
    task.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ca72ed42ac34..31d9dec78675 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1102,7 +1102,7 @@ struct sched_class {
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
-	void (*task_new) (struct rq *rq, struct task_struct *p);
+	void (*task_fork) (struct task_struct *p);
 
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task,
 			       int running);

commit 6b314d0e11924c803bf8cd944e87fd58cdb5088c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 2 18:58:05 2009 +0100

    sched: Remove sysctl.sched_features
    
    Since we've had a much saner debugfs interface to this, remove the
    sysctl one.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    [ v2: build fix ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9b2402725088..ca72ed42ac34 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1905,7 +1905,6 @@ extern unsigned int sysctl_sched_shares_ratelimit;
 extern unsigned int sysctl_sched_shares_thresh;
 extern unsigned int sysctl_sched_child_runs_first;
 #ifdef CONFIG_SCHED_DEBUG
-extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;

commit dba091b9e3522b9d32fc9975e48d3b69633b45f0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 9 09:32:03 2009 +0100

    sched: Protect sched_rr_get_param() access to task->sched_class
    
    sched_rr_get_param calls
    task->sched_class->get_rr_interval(task) without protection
    against a concurrent sched_setscheduler() call which modifies
    task->sched_class.
    
    Serialize the access with task_rq_lock(task) and hand the rq
    pointer into get_rr_interval() as it's needed at least in the
    sched_fair implementation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <alpine.LFD.2.00.0912090930120.3089@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89115ec7d43f..9b2402725088 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1111,7 +1111,8 @@ struct sched_class {
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
 			     int oldprio, int running);
 
-	unsigned int (*get_rr_interval) (struct task_struct *task);
+	unsigned int (*get_rr_interval) (struct rq *rq,
+					 struct task_struct *task);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	void (*moved_group) (struct task_struct *p);

commit 897e81bea1fcfcd2c5cdb720c9efdb25da9ff374
Merge: c3fa27d1367f 0cf55e1ec08b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 15:30:49 2009 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (35 commits)
      sched, cputime: Introduce thread_group_times()
      sched, cputime: Cleanups related to task_times()
      Revert "sched, x86: Optimize branch hint in __switch_to()"
      sched: Fix isolcpus boot option
      sched: Revert 498657a478c60be092208422fefa9c7b248729c2
      sched, time: Define nsecs_to_jiffies()
      sched: Remove task_{u,s,g}time()
      sched: Introduce task_times() to replace task_{u,s}time() pair
      sched: Limit the number of scheduler debug messages
      sched.c: Call debug_show_all_locks() when dumping all tasks
      sched, x86: Optimize branch hint in __switch_to()
      sched: Optimize branch hint in context_switch()
      sched: Optimize branch hint in pick_next_task_fair()
      sched_feat_write(): Update ppos instead of file->f_pos
      sched: Sched_rt_periodic_timer vs cpu hotplug
      sched, kvm: Fix race condition involving sched_in_preempt_notifers
      sched: More generic WAKE_AFFINE vs select_idle_sibling()
      sched: Cleanup select_task_rq_fair()
      sched: Fix granularity of task_u/stime()
      sched: Fix/add missing update_rq_clock() calls
      ...

commit 69f061e0c2ed47304b3eeac7fb7bd5268652dc50
Merge: 607781762e7a f84d49b218b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 09:52:33 2009 -0800

    Merge branch 'core-signal-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-signal-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      signal: Print warning message when dropping signals
      signal: Fix alternate signal stack check

commit 0cf55e1ec08bb5a22e068309e2d8ba1180ab4239
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 2 17:28:07 2009 +0900

    sched, cputime: Introduce thread_group_times()
    
    This is a real fix for problem of utime/stime values decreasing
    described in the thread:
    
       http://lkml.org/lkml/2009/11/3/522
    
    Now cputime is accounted in the following way:
    
     - {u,s}time in task_struct are increased every time when the thread
       is interrupted by a tick (timer interrupt).
    
     - When a thread exits, its {u,s}time are added to signal->{u,s}time,
       after adjusted by task_times().
    
     - When all threads in a thread_group exits, accumulated {u,s}time
       (and also c{u,s}time) in signal struct are added to c{u,s}time
       in signal struct of the group's parent.
    
    So {u,s}time in task struct are "raw" tick count, while
    {u,s}time and c{u,s}time in signal struct are "adjusted" values.
    
    And accounted values are used by:
    
     - task_times(), to get cputime of a thread:
       This function returns adjusted values that originates from raw
       {u,s}time and scaled by sum_exec_runtime that accounted by CFS.
    
     - thread_group_cputime(), to get cputime of a thread group:
       This function returns sum of all {u,s}time of living threads in
       the group, plus {u,s}time in the signal struct that is sum of
       adjusted cputimes of all exited threads belonged to the group.
    
    The problem is the return value of thread_group_cputime(),
    because it is mixed sum of "raw" value and "adjusted" value:
    
      group's {u,s}time = foreach(thread){{u,s}time} + exited({u,s}time)
    
    This misbehavior can break {u,s}time monotonicity.
    Assume that if there is a thread that have raw values greater
    than adjusted values (e.g. interrupted by 1000Hz ticks 50 times
    but only runs 45ms) and if it exits, cputime will decrease (e.g.
    -5ms).
    
    To fix this, we could do:
    
      group's {u,s}time = foreach(t){task_times(t)} + exited({u,s}time)
    
    But task_times() contains hard divisions, so applying it for
    every thread should be avoided.
    
    This patch fixes the above problem in the following way:
    
     - Modify thread's exit (= __exit_signal()) not to use task_times().
       It means {u,s}time in signal struct accumulates raw values instead
       of adjusted values.  As the result it makes thread_group_cputime()
       to return pure sum of "raw" values.
    
     - Introduce a new function thread_group_times(*task, *utime, *stime)
       that converts "raw" values of thread_group_cputime() to "adjusted"
       values, in same calculation procedure as task_times().
    
     - Modify group's exit (= wait_task_zombie()) to use this introduced
       thread_group_times().  It make c{u,s}time in signal struct to
       have adjusted values like before this patch.
    
     - Replace some thread_group_cputime() by thread_group_times().
       This replacements are only applied where conveys the "adjusted"
       cputime to users, and where already uses task_times() near by it.
       (i.e. sys_times(), getrusage(), and /proc/<PID>/stat.)
    
    This patch have a positive side effect:
    
     - Before this patch, if a group contains many short-life threads
       (e.g. runs 0.9ms and not interrupted by ticks), the group's
       cputime could be invisible since thread's cputime was accumulated
       after adjusted: imagine adjustment function as adj(ticks, runtime),
         {adj(0, 0.9) + adj(0, 0.9) + ....} = {0 + 0 + ....} = 0.
       After this patch it will not happen because the adjustment is
       applied after accumulated.
    
    v2:
     - remove if()s, put new variables into signal_struct.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    LKML-Reference: <4B162517.8040909@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dff85e58264e..34238bd10ebf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -624,6 +624,9 @@ struct signal_struct {
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+	cputime_t prev_utime, prev_stime;
+#endif
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
@@ -1723,6 +1726,7 @@ static inline void put_task_struct(struct task_struct *t)
 }
 
 extern void task_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
+extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
 
 /*
  * Per process flags

commit d99ca3b977fc5a93141304f571475c2af9e6c1c5
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 2 17:26:47 2009 +0900

    sched, cputime: Cleanups related to task_times()
    
    - Remove if({u,s}t)s because no one call it with NULL now.
    - Use cputime_{add,sub}().
    - Add ifndef-endif for prev_{u,s}time since they are used
      only when !VIRT_CPU_ACCOUNTING.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    LKML-Reference: <4B1624C7.7040302@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0395b0f4df3a..dff85e58264e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1331,7 +1331,9 @@ struct task_struct {
 
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING
 	cputime_t prev_utime, prev_stime;
+#endif
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
 	struct timespec real_start_time;	/* boot based time */

commit fa1452e808732ae10e8b1267fd75fc2d028d634b
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Mon Nov 30 14:59:44 2009 +0900

    locking, task_struct: Reduce size on TRACE_IRQFLAGS and 64bit
    
    Reorder task_struct field for TRACE_IRQFLAGS to remove padding
    on 64-bit.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4B135F50.8070302@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75e6e60bf583..49be8f7c05f6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1421,17 +1421,17 @@ struct task_struct {
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int irq_events;
-	int hardirqs_enabled;
 	unsigned long hardirq_enable_ip;
-	unsigned int hardirq_enable_event;
 	unsigned long hardirq_disable_ip;
+	unsigned int hardirq_enable_event;
 	unsigned int hardirq_disable_event;
-	int softirqs_enabled;
+	int hardirqs_enabled;
+	int hardirq_context;
 	unsigned long softirq_disable_ip;
-	unsigned int softirq_disable_event;
 	unsigned long softirq_enable_ip;
+	unsigned int softirq_disable_event;
 	unsigned int softirq_enable_event;
-	int hardirq_context;
+	int softirqs_enabled;
 	int softirq_context;
 #endif
 #ifdef CONFIG_LOCKDEP

commit d5b7c78e975302a1bab28263266c39ecb71acad4
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Thu Nov 26 14:49:05 2009 +0900

    sched: Remove task_{u,s,g}time()
    
    Now all task_{u,s}time() pairs are replaced by task_times().
    And task_gtime() is too simple to be an inline function.
    
    Cleanup them all.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    LKML-Reference: <4B0E16D1.70902@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fe6ae1516640..0395b0f4df3a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1720,9 +1720,6 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
-extern cputime_t task_utime(struct task_struct *p);
-extern cputime_t task_stime(struct task_struct *p);
-extern cputime_t task_gtime(struct task_struct *p);
 extern void task_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
 
 /*

commit d180c5bccec02612256fd8076ff3c1fac3429553
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Thu Nov 26 14:48:30 2009 +0900

    sched: Introduce task_times() to replace task_{u,s}time() pair
    
    Functions task_{u,s}time() are called in pair in almost all
    cases.  However task_stime() is implemented to call task_utime()
    from its inside, so such paired calls run task_utime() twice.
    
    It means we do heavy divisions (div_u64 + do_div) twice to get
    utime and stime which can be obtained at same time by one set
    of divisions.
    
    This patch introduces a function task_times(*tsk, *utime,
    *stime) to retrieve utime and stime at once in better, optimized
    way.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    LKML-Reference: <4B0E16AE.906@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78ba664474f3..fe6ae1516640 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1723,6 +1723,7 @@ static inline void put_task_struct(struct task_struct *t)
 extern cputime_t task_utime(struct task_struct *p);
 extern cputime_t task_stime(struct task_struct *p);
 extern cputime_t task_gtime(struct task_struct *p);
+extern void task_times(struct task_struct *p, cputime_t *ut, cputime_t *st);
 
 /*
  * Per process flags

commit 2a855dd01bc1539111adb7233f587c5c468732ac
Author: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
Date:   Sun Oct 25 15:37:58 2009 +0100

    signal: Fix alternate signal stack check
    
    All architectures in the kernel increment/decrement the stack pointer
    before storing values on the stack.
    
    On architectures which have the stack grow down sas_ss_sp == sp is not
    on the alternate signal stack while sas_ss_sp + sas_ss_size == sp is
    on the alternate signal stack.
    
    On architectures which have the stack grow up sas_ss_sp == sp is on
    the alternate signal stack while sas_ss_sp + sas_ss_size == sp is not
    on the alternate signal stack.
    
    The current implementation fails for architectures which have the
    stack grow down on the corner case where sas_ss_sp == sp.This was
    reported as Debian bug #544905 on AMD64.
    Simplified test case: http://download.breakpoint.cc/tc-sig-stack.c
    
    The test case creates the following stack scenario:
       0xn0300      stack top
       0xn0200      alt stack pointer top (when switching to alt stack)
       0xn01ff      alt stack end
       0xn0100      alt stack start == stack pointer
    
    If the signal is sent the stack pointer is pointing to the base
    address of the alt stack and the kernel erroneously decides that it
    has already switched to the alternate stack because of the current
    check for "sp - sas_ss_sp < sas_ss_size"
    
    On parisc (stack grows up) the scenario would be:
       0xn0200      stack pointer
       0xn01ff      alt stack end
       0xn0100      alt stack start = alt stack pointer base
                                      (when switching to alt stack)
       0xn0000      stack base
    
    This is handled correctly by the current implementation.
    
    [ tglx: Modified for archs which have the stack grow up (parisc) which
            would fail with the correct implementation for stack grows
            down. Added a check for sp >= current->sas_ss_sp which is
            strictly not necessary but makes the code symetric for both
            variants ]
    
    Signed-off-by: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: stable@kernel.org
    LKML-Reference: <20091025143758.GA6653@Chamillionaire.breakpoint.cc>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75e6e60bf583..0f67914a43c9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2086,11 +2086,18 @@ static inline int is_si_special(const struct siginfo *info)
 	return info <= SEND_SIG_FORCED;
 }
 
-/* True if we are on the alternate signal stack.  */
-
+/*
+ * True if we are on the alternate signal stack.
+ */
 static inline int on_sig_stack(unsigned long sp)
 {
-	return (sp - current->sas_ss_sp < current->sas_ss_size);
+#ifdef CONFIG_STACK_GROWSUP
+	return sp >= current->sas_ss_sp &&
+		sp - current->sas_ss_sp < current->sas_ss_size;
+#else
+	return sp > current->sas_ss_sp &&
+		sp - current->sas_ss_sp <= current->sas_ss_size;
+#endif
 }
 
 static inline int sas_ss_flags(unsigned long sp)

commit acc3f5d7cabbfd6cec71f0c1f9900621fa2d6ae7
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 3 14:53:40 2009 +1030

    cpumask: Partition_sched_domains takes array of cpumask_var_t
    
    Currently partition_sched_domains() takes a 'struct cpumask
    *doms_new' which is a kmalloc'ed array of cpumask_t.  You can't
    have such an array if 'struct cpumask' is undefined, as we plan
    for CONFIG_CPUMASK_OFFSTACK=y.
    
    So, we make this an array of cpumask_var_t instead: this is the
    same for the CONFIG_CPUMASK_OFFSTACK=n case, but requires
    multiple allocations for the CONFIG_CPUMASK_OFFSTACK=y case.
    Hence we add alloc_sched_domains() and free_sched_domains()
    functions.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <200911031453.40668.rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dfc21fb76bf1..78ba664474f3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1009,9 +1009,13 @@ static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 	return to_cpumask(sd->span);
 }
 
-extern void partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
+extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 				    struct sched_domain_attr *dattr_new);
 
+/* Allocate an array of sched domains, for partition_sched_domains(). */
+cpumask_var_t *alloc_sched_domains(unsigned int ndoms);
+void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);
+
 /* Test a flag in parent sched domain */
 static inline int test_sd_parent(struct sched_domain *sd, int flag)
 {
@@ -1029,7 +1033,7 @@ unsigned long default_scale_smt_power(struct sched_domain *sd, int cpu);
 struct sched_domain_attr;
 
 static inline void
-partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
+partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 			struct sched_domain_attr *dattr_new)
 {
 }

commit 9824a2b728b63e7ff586b9fd9293c819be79f0f3
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Nov 4 16:16:54 2009 +0900

    sched: Remove unused cpu_nr_migrations()
    
    cpu_nr_migrations() is not used, remove it.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4AF12A66.6020609@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 754b3deed02b..dfc21fb76bf1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -145,7 +145,6 @@ extern unsigned long this_cpu_load(void);
 
 
 extern void calc_global_load(void);
-extern u64 cpu_nr_migrations(int cpu);
 
 extern unsigned long get_parent_ip(unsigned long addr);
 

commit 2a2bb3142d326bb28b03875cabfc49baaac9a14a
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Nov 4 16:16:10 2009 +0900

    sched: Remove unused time_sync_thresh declaration
    
    time_sync_thresh had been removed.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4AF12A3A.5050200@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f18102c4d0b8..754b3deed02b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -171,8 +171,6 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 }
 #endif
 
-extern unsigned long long time_sync_thresh;
-
 /*
  * Task state bitmask. NOTE! These bits are also
  * encoded in fs/proc/array.c: get_task_state().

commit 1477b6a7edd9ffa7bba4f9779ce9a76ce92761ed
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Nov 4 16:14:16 2009 +0900

    sched: Remove unused __schedule() declaration
    
    __schedule() had been removed.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4AF129C8.3030008@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75e6e60bf583..f18102c4d0b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -349,7 +349,6 @@ extern signed long schedule_timeout(signed long timeout);
 extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
-asmlinkage void __schedule(void);
 asmlinkage void schedule(void);
 extern int mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner);
 

commit db16826367fefcb0ddb93d76b66adc52eb4e6339
Merge: cd6045138ed1 465fdd97cbe1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 24 07:53:22 2009 -0700

    Merge branch 'hwpoison' of git://git.kernel.org/pub/scm/linux/kernel/git/ak/linux-mce-2.6
    
    * 'hwpoison' of git://git.kernel.org/pub/scm/linux/kernel/git/ak/linux-mce-2.6: (21 commits)
      HWPOISON: Enable error_remove_page on btrfs
      HWPOISON: Add simple debugfs interface to inject hwpoison on arbitary PFNs
      HWPOISON: Add madvise() based injector for hardware poisoned pages v4
      HWPOISON: Enable error_remove_page for NFS
      HWPOISON: Enable .remove_error_page for migration aware file systems
      HWPOISON: The high level memory error handler in the VM v7
      HWPOISON: Add PR_MCE_KILL prctl to control early kill behaviour per process
      HWPOISON: shmem: call set_page_dirty() with locked page
      HWPOISON: Define a new error_remove_page address space op for async truncation
      HWPOISON: Add invalidate_inode_page
      HWPOISON: Refactor truncate to allow direct truncating of page v2
      HWPOISON: check and isolate corrupted free pages v2
      HWPOISON: Handle hardware poisoned pages in try_to_unmap
      HWPOISON: Use bitmask/action code for try_to_unmap behaviour
      HWPOISON: x86: Add VM_FAULT_HWPOISON handling to x86 page fault handler v2
      HWPOISON: Add poison check to page fault handling
      HWPOISON: Add basic support for poisoned pages in fault handler v3
      HWPOISON: Add new SIGBUS error codes for hardware poison signals
      HWPOISON: Add support for poison swap entries v2
      HWPOISON: Export some rmap vma locking to outside world
      ...

commit 801460d0cf5c5288153b722565773059b0f44348
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Sep 23 15:57:41 2009 -0700

    task_struct cleanup: move binfmt field to mm_struct
    
    Because the binfmt is not different between threads in the same process,
    it can be moved from task_struct to mm_struct.  And binfmt moudle is
    handled per mm_struct instead of task_struct.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 811cd96524d7..8a16f6d11dcd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1271,7 +1271,6 @@ struct task_struct {
 	struct mm_struct *mm, *active_mm;
 
 /* task state */
-	struct linux_binfmt *binfmt;
 	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e951bd2eb9fc..811cd96524d7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -309,7 +309,7 @@ extern void softlockup_tick(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern int proc_dosoftlockup_thresh(struct ctl_table *table, int write,
-				    struct file *filp, void __user *buffer,
+				    void __user *buffer,
 				    size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
 extern int softlockup_thresh;
@@ -331,7 +331,7 @@ extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;
 extern unsigned long sysctl_hung_task_warnings;
 extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
-					 struct file *filp, void __user *buffer,
+					 void __user *buffer,
 					 size_t *lenp, loff_t *ppos);
 #endif
 
@@ -1906,7 +1906,7 @@ extern unsigned int sysctl_sched_time_avg;
 extern unsigned int sysctl_timer_migration;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
-		struct file *file, void __user *buffer, size_t *length,
+		void __user *buffer, size_t *length,
 		loff_t *ppos);
 #endif
 #ifdef CONFIG_SCHED_DEBUG
@@ -1924,7 +1924,7 @@ extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
 int sched_rt_handler(struct ctl_table *table, int write,
-		struct file *filp, void __user *buffer, size_t *lenp,
+		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
 extern unsigned int sysctl_sched_compat_yield;

commit d9588725e52650e82989707f8fd2feb67ad2dc8e
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Sep 23 15:57:04 2009 -0700

    signals: inline __fatal_signal_pending
    
    __fatal_signal_pending inlines to one instruction on x86, probably two
    instructions on other machines.  It takes two longer x86 instructions just
    to call it and test its return value, not to mention the function itself.
    
    On my random x86_64 config, this saved 70 bytes of text (59 of those being
    __fatal_signal_pending itself).
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9e5a88afe6be..e951bd2eb9fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2337,7 +2337,10 @@ static inline int signal_pending(struct task_struct *p)
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
 }
 
-extern int __fatal_signal_pending(struct task_struct *p);
+static inline int __fatal_signal_pending(struct task_struct *p)
+{
+	return unlikely(sigismember(&p->pending.signal, SIGKILL));
+}
 
 static inline int fatal_signal_pending(struct task_struct *p)
 {

commit a7f0765edfd53aed09cb7b0e15863688b39447de
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:44 2009 -0700

    ptrace: __ptrace_detach: do __wake_up_parent() if we reap the tracee
    
    The bug is old, it wasn't cause by recent changes.
    
    Test case:
    
            static void *tfunc(void *arg)
            {
                    int pid = (long)arg;
    
                    assert(ptrace(PTRACE_ATTACH, pid, NULL, NULL) == 0);
                    kill(pid, SIGKILL);
    
                    sleep(1);
                    return NULL;
            }
    
            int main(void)
            {
                    pthread_t th;
                    long pid = fork();
    
                    if (!pid)
                            pause();
    
                    signal(SIGCHLD, SIG_IGN);
                    assert(pthread_create(&th, NULL, tfunc, (void*)pid) == 0);
    
                    int r = waitpid(-1, NULL, __WNOTHREAD);
                    printf("waitpid: %d %m\n", r);
    
                    return 0;
            }
    
    Before the patch this program hangs, after this patch waitpid() correctly
    fails with errno == -ECHILD.
    
    The problem is, __ptrace_detach() reaps the EXIT_ZOMBIE tracee if its
    ->real_parent is our sub-thread and we ignore SIGCHLD.  But in this case
    we should wake up other threads which can sleep in do_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 848d1f20086e..9e5a88afe6be 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2059,6 +2059,7 @@ extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
 extern int do_notify_parent(struct task_struct *, int);
+extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);

commit e0ad955680878998ff7dc51ce06ddad12260423a
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:38 2009 -0600

    cpumask: don't define set_cpus_allowed() if CONFIG_CPUMASK_OFFSTACK=y
    
    You're not supposed to pass cpumasks on the stack in that case.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cbf2a3b46280..848d1f20086e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1817,10 +1817,13 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 	return 0;
 }
 #endif
+
+#ifndef CONFIG_CPUMASK_OFFSTACK
 static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 {
 	return set_cpus_allowed_ptr(p, &new_mask);
 }
+#endif
 
 /*
  * Architectures can set this to 1 if they have specified

commit 31bbb9b58d1e8ebcf2b28c95c2250a9f8e31e397
Merge: ff830b8e5f99 3f0a525ebf4b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 23 09:46:15 2009 -0700

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      itimers: Add tracepoints for itimer
      hrtimer: Add tracepoint for hrtimers
      timers: Add tracepoints for timer_list timers
      cputime: Optimize jiffies_to_cputime(1)
      itimers: Simplify arm_timer() code a bit
      itimers: Fix periodic tics precision
      itimers: Merge ITIMER_VIRT and ITIMER_PROF
    
    Trivial header file include conflicts in kernel/fork.c

commit d899bf7b55f503ba7d3d07ed27c3a37e270fa7db
Author: Stefani Seibold <stefani@seibold.net>
Date:   Tue Sep 22 16:45:40 2009 -0700

    procfs: provide stack information for threads
    
    A patch to give a better overview of the userland application stack usage,
    especially for embedded linux.
    
    Currently you are only able to dump the main process/thread stack usage
    which is showed in /proc/pid/status by the "VmStk" Value.  But you get no
    information about the consumed stack memory of the the threads.
    
    There is an enhancement in the /proc/<pid>/{task/*,}/*maps and which marks
    the vm mapping where the thread stack pointer reside with "[thread stack
    xxxxxxxx]".  xxxxxxxx is the maximum size of stack.  This is a value
    information, because libpthread doesn't set the start of the stack to the
    top of the mapped area, depending of the pthread usage.
    
    A sample output of /proc/<pid>/task/<tid>/maps looks like:
    
    08048000-08049000 r-xp 00000000 03:00 8312       /opt/z
    08049000-0804a000 rw-p 00001000 03:00 8312       /opt/z
    0804a000-0806b000 rw-p 00000000 00:00 0          [heap]
    a7d12000-a7d13000 ---p 00000000 00:00 0
    a7d13000-a7f13000 rw-p 00000000 00:00 0          [thread stack: 001ff4b4]
    a7f13000-a7f14000 ---p 00000000 00:00 0
    a7f14000-a7f36000 rw-p 00000000 00:00 0
    a7f36000-a8069000 r-xp 00000000 03:00 4222       /lib/libc.so.6
    a8069000-a806b000 r--p 00133000 03:00 4222       /lib/libc.so.6
    a806b000-a806c000 rw-p 00135000 03:00 4222       /lib/libc.so.6
    a806c000-a806f000 rw-p 00000000 00:00 0
    a806f000-a8083000 r-xp 00000000 03:00 14462      /lib/libpthread.so.0
    a8083000-a8084000 r--p 00013000 03:00 14462      /lib/libpthread.so.0
    a8084000-a8085000 rw-p 00014000 03:00 14462      /lib/libpthread.so.0
    a8085000-a8088000 rw-p 00000000 00:00 0
    a8088000-a80a4000 r-xp 00000000 03:00 8317       /lib/ld-linux.so.2
    a80a4000-a80a5000 r--p 0001b000 03:00 8317       /lib/ld-linux.so.2
    a80a5000-a80a6000 rw-p 0001c000 03:00 8317       /lib/ld-linux.so.2
    afaf5000-afb0a000 rw-p 00000000 00:00 0          [stack]
    ffffe000-fffff000 r-xp 00000000 00:00 0          [vdso]
    
    Also there is a new entry "stack usage" in /proc/<pid>/{task/*,}/status
    which will you give the current stack usage in kb.
    
    A sample output of /proc/self/status looks like:
    
    Name:   cat
    State:  R (running)
    Tgid:   507
    Pid:    507
    .
    .
    .
    CapBnd: fffffffffffffeff
    voluntary_ctxt_switches:        0
    nonvoluntary_ctxt_switches:     0
    Stack usage:    12 kB
    
    I also fixed stack base address in /proc/<pid>/{task/*,}/stat to the base
    address of the associated thread stack and not the one of the main
    process.  This makes more sense.
    
    [akpm@linux-foundation.org: fs/proc/array.c now needs walk_page_range()]
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6448bbc6406b..3cbc6c0be666 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1529,6 +1529,7 @@ struct task_struct {
 	/* bitmask of trace recursion */
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
+	unsigned long stack_start;
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 1f10206cf8e945220f7220a809d8bfc15c21f9a5
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Tue Sep 22 16:44:10 2009 -0700

    getrusage: fill ru_maxrss value
    
    Make ->ru_maxrss value in struct rusage filled accordingly to rss hiwater
    mark.  This struct is filled as a parameter to getrusage syscall.
    ->ru_maxrss value is set to KBs which is the way it is done in BSD
    systems.  /usr/bin/time (gnu time) application converts ->ru_maxrss to KBs
    which seems to be incorrect behavior.  Maintainer of this util was
    notified by me with the patch which corrects it and cc'ed.
    
    To make this happen we extend struct signal_struct by two fields.  The
    first one is ->maxrss which we use to store rss hiwater of the task.  The
    second one is ->cmaxrss which we use to store highest rss hiwater of all
    task childs.  These values are used in k_getrusage() to actually fill
    ->ru_maxrss.  k_getrusage() uses current rss hiwater value directly if mm
    struct exists.
    
    Note:
    exec() clear mm->hiwater_rss, but doesn't clear sig->maxrss.
    it is intetionally behavior. *BSD getrusage have exec() inheriting.
    
    test programs
    ========================================================
    
    getrusage.c
    ===========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
    
     #define err(str) perror(str), exit(1)
    
    int main(int argc, char** argv)
    {
            int status;
    
            printf("allocate 100MB\n");
            consume(100);
    
            printf("testcase1: fork inherit? \n");
            printf("  expect: initial.self ~= child.self\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase2: fork inherit? (cont.) \n");
            printf("  expect: initial.children ~= 100MB, but child.children = 0\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase3: fork + malloc \n");
            printf("  expect: child.self ~= initial.self + 50MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    printf("allocate +50MB\n");
                    consume(50);
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase4: grandchild maxrss\n");
            printf("  expect: post_wait.children ~= 300MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 0 -g 300");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase5: zombie\n");
            printf("  expect: pre_wait ~= initial, IOW the zombie process is not accounted.\n");
            printf("          post_wait ~= 400MB, IOW wait() collect child's max_rss. \n");
            show_rusage("initial");
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("pre_wait");
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 400");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase6: SIG_IGN\n");
            printf("  expect: initial ~= after_zombie (child's 500MB alloc should be ignored).\n");
            show_rusage("initial");
            signal(SIGCHLD, SIG_IGN);
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("after_zombie");
            } else {
                    system("./child -n 500");
                    _exit(0);
            }
            printf("\n");
            signal(SIGCHLD, SIG_DFL);
    
            printf("testcase7: exec (without fork) \n");
            printf("  expect: initial ~= exec \n");
            show_rusage("initial");
            execl("./child", "child", "-v", NULL);
    
            return 0;
    }
    
    child.c
    =======
     #include <sys/types.h>
     #include <unistd.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
    
     #include "common.h"
    
    int main(int argc, char** argv)
    {
            int status;
            int c;
            long consume_size = 0;
            long grandchild_consume_size = 0;
            int show = 0;
    
            while ((c = getopt(argc, argv, "n:g:v")) != -1) {
                    switch (c) {
                    case 'n':
                            consume_size = atol(optarg);
                            break;
                    case 'v':
                            show = 1;
                            break;
                    case 'g':
    
                            grandchild_consume_size = atol(optarg);
                            break;
                    default:
                            break;
                    }
            }
    
            if (show)
                    show_rusage("exec");
    
            if (consume_size) {
                    printf("child alloc %ldMB\n", consume_size);
                    consume(consume_size);
            }
    
            if (grandchild_consume_size) {
                    if (fork()) {
                            wait(&status);
                    } else {
                            printf("grandchild alloc %ldMB\n", grandchild_consume_size);
                            consume(grandchild_consume_size);
    
                            exit(0);
                    }
            }
    
            return 0;
    }
    
    common.c
    ========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
     #define err(str) perror(str), exit(1)
    
    void show_rusage(char *prefix)
    {
            int err, err2;
            struct rusage rusage_self;
            struct rusage rusage_children;
    
            printf("%s: ", prefix);
            err = getrusage(RUSAGE_SELF, &rusage_self);
            if (!err)
                    printf("self %ld ", rusage_self.ru_maxrss);
            err2 = getrusage(RUSAGE_CHILDREN, &rusage_children);
            if (!err2)
                    printf("children %ld ", rusage_children.ru_maxrss);
    
            printf("\n");
    }
    
    /* Some buggy OS need this worthless CPU waste. */
    void make_pagefault(void)
    {
            void *addr;
            int size = getpagesize();
            int i;
    
            for (i=0; i<1000; i++) {
                    addr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANON, -1, 0);
                    if (addr == MAP_FAILED)
                            err("make_pagefault");
                    memset(addr, 0, size);
                    munmap(addr, size);
            }
    }
    
    void consume(int mega)
    {
            size_t sz = mega * 1024 * 1024;
            void *ptr;
    
            ptr = malloc(sz);
            memset(ptr, 0, sz);
            make_pagefault();
    }
    
    pid_t __fork(void)
    {
            pid_t pid;
    
            pid = fork();
            make_pagefault();
    
            return pid;
    }
    
    common.h
    ========
    void show_rusage(char *prefix);
    void make_pagefault(void);
    void consume(int mega);
    pid_t __fork(void);
    
    FreeBSD result (expected result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 103492 children 0
    fork child: self 103540 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 103540 children 103540
    child: self 103564 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 103564 children 103564
    allocate +50MB
    fork child: self 154860 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 103564 children 154860
    grandchild alloc 300MB
    post_wait: self 103564 children 308720
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 103564 children 308720
    child alloc 400MB
    pre_wait: self 103564 children 308720
    post_wait: self 103564 children 411312
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 103564 children 411312
    child alloc 500MB
    after_zombie: self 103624 children 411312
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 103624 children 411312
    exec: self 103624 children 411312
    
    Linux result (actual test result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 102848 children 0
    fork child: self 102572 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 102876 children 102644
    child: self 102572 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 102876 children 102644
    allocate +50MB
    fork child: self 153804 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 102876 children 153864
    grandchild alloc 300MB
    post_wait: self 102876 children 307536
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 102876 children 307536
    child alloc 400MB
    pre_wait: self 102876 children 307536
    post_wait: self 102876 children 410076
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 102876 children 410076
    child alloc 500MB
    after_zombie: self 102880 children 410076
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 102880 children 410076
    exec: self 102880 children 410076
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 97b10da0a3ea..6448bbc6406b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -426,6 +426,15 @@ static inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)
 	return max(mm->hiwater_rss, get_mm_rss(mm));
 }
 
+static inline void setmax_mm_hiwater_rss(unsigned long *maxrss,
+					 struct mm_struct *mm)
+{
+	unsigned long hiwater_rss = get_mm_hiwater_rss(mm);
+
+	if (*maxrss < hiwater_rss)
+		*maxrss = hiwater_rss;
+}
+
 static inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)
 {
 	return max(mm->hiwater_vm, mm->total_vm);
@@ -612,6 +621,7 @@ struct signal_struct {
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
+	unsigned long maxrss, cmaxrss;
 	struct task_io_accounting ioac;
 
 	/*

commit 69d25870f20c4b2563304f2b79c5300dd60a067e
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Mon Sep 21 17:04:08 2009 -0700

    cpuidle: fix the menu governor to boost IO performance
    
    Fix the menu idle governor which balances power savings, energy efficiency
    and performance impact.
    
    The reason for a reworked governor is that there have been serious
    performance issues reported with the existing code on Nehalem server
    systems.
    
    To show this I'm sure Andrew wants to see benchmark results:
    (benchmark is "fio", "no cstates" is using "idle=poll")
    
                    no cstates      current linux   new algorithm
    1 disk          107 Mb/s        85 Mb/s         105 Mb/s
    2 disks         215 Mb/s        123 Mb/s        209 Mb/s
    12 disks        590 Mb/s        320 Mb/s        585 Mb/s
    
    In various power benchmark measurements, no degredation was found by our
    measurement&diagnostics team.  Obviously a small percentage more power was
    used in the "fio" benchmark, due to the much higher performance.
    
    While it would be a novel idea to describe the new algorithm in this
    commit message, I cheaped out and described it in comments in the code
    instead.
    
    [changes since first post: spelling fixes from akpm, review feedback,
    folded menu-tng into menu.c]
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17e9a8e9a51d..97b10da0a3ea 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -140,6 +140,10 @@ extern int nr_processes(void);
 extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_iowait(void);
+extern unsigned long nr_iowait_cpu(void);
+extern unsigned long this_cpu_load(void);
+
+
 extern void calc_global_load(void);
 extern u64 cpu_nr_migrations(int cpu);
 

commit 28b83c5193e7ab951e402252278f2cc79dc4d298
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:03:13 2009 -0700

    oom: move oom_adj value from task_struct to signal_struct
    
    Currently, OOM logic callflow is here.
    
        __out_of_memory()
            select_bad_process()            for each task
                badness()                   calculate badness of one task
                    oom_kill_process()      search child
                        oom_kill_task()     kill target task and mm shared tasks with it
    
    example, process-A have two thread, thread-A and thread-B and it have very
    fat memory and each thread have following oom_adj and oom_score.
    
         thread-A: oom_adj = OOM_DISABLE, oom_score = 0
         thread-B: oom_adj = 0,           oom_score = very-high
    
    Then, select_bad_process() select thread-B, but oom_kill_task() refuse
    kill the task because thread-A have OOM_DISABLE.  Thus __out_of_memory()
    call select_bad_process() again.  but select_bad_process() select the same
    task.  It mean kernel fall in livelock.
    
    The fact is, select_bad_process() must select killable task.  otherwise
    OOM logic go into livelock.
    
    And root cause is, oom_adj shouldn't be per-thread value.  it should be
    per-process value because OOM-killer kill a process, not thread.  Thus
    This patch moves oomkilladj (now more appropriately named oom_adj) from
    struct task_struct to struct signal_struct.  it naturally prevent
    select_bad_process() choose wrong task.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 899d7304d594..17e9a8e9a51d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -639,6 +639,8 @@ struct signal_struct {
 	unsigned audit_tty;
 	struct tty_audit_buf *tty_audit_buf;
 #endif
+
+	int oom_adj;	/* OOM kill score adjustment (bit shift) */
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */
@@ -1221,7 +1223,6 @@ struct task_struct {
 	 * a short time
 	 */
 	unsigned char fpu_counter;
-	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif

commit 35451beecbd7c86ce3249d543594517a5fe9a0cd
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:02:27 2009 -0700

    ksm: unmerge is an origin of OOMs
    
    Just as the swapoff system call allocates many pages of RAM to various
    processes, perhaps triggering OOM, so "echo 2 >/sys/kernel/mm/ksm/run"
    (unmerge) is liable to allocate many pages of RAM to various processes,
    perhaps triggering OOM; and each is normally run from a modest admin
    process (swapoff or shell), easily repeated until it succeeds.
    
    So treat unmerge_and_remove_all_rmap_items() in the same way that we treat
    try_to_unuse(): generalize PF_SWAPOFF to PF_OOM_ORIGIN, and bracket both
    with that, to ask the OOM killer to kill them first, to prevent them from
    spawning more and more OOM kills.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Acked-by: Izik Eidus <ieidus@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8f3e63cb33a6..899d7304d594 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1720,7 +1720,7 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
 #define PF_KSWAPD	0x00040000	/* I am kswapd */
-#define PF_SWAPOFF	0x00080000	/* I am in swapoff */
+#define PF_OOM_ORIGIN	0x00080000	/* Allocating much memory to others */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
 #define PF_KTHREAD	0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */

commit f8af4da3b4c14e7267c4ffb952079af3912c51c5
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:01:57 2009 -0700

    ksm: the mm interface to ksm
    
    This patch presents the mm interface to a dummy version of ksm.c, for
    better scrutiny of that interface: the real ksm.c follows later.
    
    When CONFIG_KSM is not set, madvise(2) reject MADV_MERGEABLE and
    MADV_UNMERGEABLE with EINVAL, since that seems more helpful than
    pretending that they can be serviced.  But when CONFIG_KSM=y, accept them
    even if KSM is not currently running, and even on areas which KSM will not
    touch (e.g.  hugetlb or shared file or special driver mappings).
    
    Like other madvices, report ENOMEM despite success if any area in the
    range is unmapped, and use EAGAIN to report out of memory.
    
    Define vma flag VM_MERGEABLE to identify an area on which KSM may try
    merging pages: leave it to ksm_madvise() to decide whether to set it.
    Define mm flag MMF_VM_MERGEABLE to identify an mm which might contain
    VM_MERGEABLE areas, to minimize callouts when forking or exiting.
    
    Based upon earlier patches by Chris Wright and Izik Eidus.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Chris Wright <chrisw@redhat.com>
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8fe351c3914a..8f3e63cb33a6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -434,7 +434,9 @@ extern int get_dumpable(struct mm_struct *mm);
 /* dumpable bits */
 #define MMF_DUMPABLE      0  /* core dump is permitted */
 #define MMF_DUMP_SECURELY 1  /* core file is readable only by root */
+
 #define MMF_DUMPABLE_BITS 2
+#define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
 
 /* coredump filter bits */
 #define MMF_DUMP_ANON_PRIVATE	2
@@ -444,6 +446,7 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_DUMP_ELF_HEADERS	6
 #define MMF_DUMP_HUGETLB_PRIVATE 7
 #define MMF_DUMP_HUGETLB_SHARED  8
+
 #define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
 #define MMF_DUMP_FILTER_BITS	7
 #define MMF_DUMP_FILTER_MASK \
@@ -457,6 +460,10 @@ extern int get_dumpable(struct mm_struct *mm);
 #else
 # define MMF_DUMP_MASK_DEFAULT_ELF	0
 #endif
+					/* leave room for more dump flags */
+#define MMF_VM_MERGEABLE	16	/* KSM may merge identical pages */
+
+#define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
 struct sighand_struct {
 	atomic_t		count;

commit 43c1266ce4dc06bfd236cec31e11e9ecd69c0bef
Merge: b8c7f1dc5ca4 57c0c15b5244
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 21 09:15:07 2009 -0700

    Merge branch 'perfcounters-rename-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perfcounters-rename-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      perf: Tidy up after the big rename
      perf: Do the big rename: Performance Counters -> Performance Events
      perf_counter: Rename 'event' to event_id/hw_event
      perf_counter: Rename list_entry -> group_entry, counter_list -> group_list
    
    Manually resolved some fairly trivial conflicts with the tracing tree in
    include/trace/ftrace.h and kernel/trace/trace_syscalls.c.

commit b8c7f1dc5ca4e0d10709182233cdab932cef593d
Merge: f4eccb6d979e a71fca58b7f4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 21 09:06:52 2009 -0700

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      rcu: Fix whitespace inconsistencies
      rcu: Fix thinko, actually initialize full tree
      rcu: Apply results of code inspection of kernel/rcutree_plugin.h
      rcu: Add WARN_ON_ONCE() consistency checks covering state transitions
      rcu: Fix synchronize_rcu() for TREE_PREEMPT_RCU
      rcu: Simplify rcu_read_unlock_special() quiescent-state accounting
      rcu: Add debug checks to TREE_PREEMPT_RCU for premature grace periods
      rcu: Kconfig help needs to say that TREE_PREEMPT_RCU scales down
      rcutorture: Occasionally delay readers enough to make RCU force_quiescent_state
      rcu: Initialize multi-level RCU grace periods holding locks
      rcu: Need to update rnp->gpnum if preemptable RCU is to be reliable

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8af3d249170e..8b265a8986d0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -100,7 +100,7 @@ struct robust_list_head;
 struct bio;
 struct fs_struct;
 struct bts_context;
-struct perf_counter_context;
+struct perf_event_context;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -701,7 +701,7 @@ struct user_struct {
 #endif
 #endif
 
-#ifdef CONFIG_PERF_COUNTERS
+#ifdef CONFIG_PERF_EVENTS
 	atomic_long_t locked_vm;
 #endif
 };
@@ -1449,10 +1449,10 @@ struct task_struct {
 	struct list_head pi_state_list;
 	struct futex_pi_state *pi_state_cache;
 #endif
-#ifdef CONFIG_PERF_COUNTERS
-	struct perf_counter_context *perf_counter_ctxp;
-	struct mutex perf_counter_mutex;
-	struct list_head perf_counter_list;
+#ifdef CONFIG_PERF_EVENTS
+	struct perf_event_context *perf_event_ctxp;
+	struct mutex perf_event_mutex;
+	struct list_head perf_event_list;
 #endif
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;	/* Protected by alloc_lock */

commit 0d721ceadbeaa24d7f9dd41b3e5e29912327a7e1
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Mon Sep 21 01:31:53 2009 +0000

    sched: Simplify sys_sched_rr_get_interval() system call
    
    By removing the need for it to know details of scheduling classes.
    
    This allows PlugSched to define orthogonal scheduling classes.
    
    Signed-off-by: Peter Williams <pwil3058@bigpond.net.au>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <06d1b89ee15a0eef82d7.1253496713@mudlark.pw.nest>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cc37a3fa5065..239c8e0dba9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1075,6 +1075,8 @@ struct sched_class {
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
 			     int oldprio, int running);
 
+	unsigned int (*get_rr_interval) (struct task_struct *task);
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	void (*moved_group) (struct task_struct *p);
 #endif

commit 89f19f04dc72363d912fd007a399cb10310eff6e
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Sat Sep 19 11:55:44 2009 -0700

    sched: Fix raciness in runqueue_is_locked()
    
    runqueue_is_locked() is unavoidably racy due to a poor interface design.
    It does
    
            cpu = get_cpu()
            ret = some_perpcu_thing(cpu);
            put_cpu(cpu);
            return ret;
    
    Its return value is unreliable.
    
    Fix.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <200909191855.n8JItiko022148@imap1.linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8af3d249170e..cc37a3fa5065 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -257,7 +257,7 @@ extern asmlinkage void schedule_tail(struct task_struct *prev);
 extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 
-extern int runqueue_is_locked(void);
+extern int runqueue_is_locked(int cpu);
 extern void task_rq_unlock_wait(struct task_struct *p);
 
 extern cpumask_var_t nohz_cpu_mask;

commit c3422bea5f09b0e85704f51f2b01271630b8940b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Sep 13 09:15:10 2009 -0700

    rcu: Simplify rcu_read_unlock_special() quiescent-state accounting
    
    The earlier approach required two scheduling-clock ticks to note an
    preemptable-RCU quiescent state in the situation in which the
    scheduling-clock interrupt is unlucky enough to always interrupt an
    RCU read-side critical section.
    
    With this change, the quiescent state is instead noted by the
    outermost rcu_read_unlock() immediately following the first
    scheduling-clock tick, or, alternatively, by the first subsequent
    context switch.  Therefore, this change also speeds up grace
    periods.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    LKML-Reference: <12528585111945-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f3d74bd04d18..c62a9f84d614 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1740,7 +1740,6 @@ extern cputime_t task_gtime(struct task_struct *p);
 
 #define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
 #define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
-#define RCU_READ_UNLOCK_GOT_QS  (1 << 2) /* CPU has responded to RCU core. */
 
 static inline void rcu_copy_process(struct task_struct *p)
 {

commit ad4b78bbcbab66998b05d422ac6106b645796e54
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 16 12:31:31 2009 +0200

    sched: Add new wakeup preemption mode: WAKEUP_RUNNING
    
    Create a new wakeup preemption mode, preempt towards tasks that run
    shorter on avg. It sets next buddy to be sure we actually run the task
    we preempted for.
    
    Test results:
    
     root@twins:~# while :; do :; done &
     [1] 6537
     root@twins:~# while :; do :; done &
     [2] 6538
     root@twins:~# while :; do :; done &
     [3] 6539
     root@twins:~# while :; do :; done &
     [4] 6540
    
     root@twins:/home/peter# ./latt -c4 sleep 4
     Entries: 48 (clients=4)
    
     Averages:
     ------------------------------
            Max          4750 usec
            Avg           497 usec
            Stdev         737 usec
    
     root@twins:/home/peter# echo WAKEUP_RUNNING > /debug/sched_features
    
     root@twins:/home/peter# ./latt -c4 sleep 4
     Entries: 48 (clients=4)
    
     Averages:
     ------------------------------
            Max            14 usec
            Avg             5 usec
            Stdev           3 usec
    
    Disabled by default - needs more testing.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <new-submission>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4a39bb2b4a4..8af3d249170e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1113,6 +1113,8 @@ struct sched_entity {
 	u64			start_runtime;
 	u64			avg_wakeup;
 
+	u64			avg_running;
+
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;

commit 4db96cf077aa938b11fe7ac79ecc9b29ec00fbab
Author: Andi Kleen <andi@firstfloor.org>
Date:   Wed Sep 16 11:50:14 2009 +0200

    HWPOISON: Add PR_MCE_KILL prctl to control early kill behaviour per process
    
    This allows processes to override their early/late kill
    behaviour on hardware memory errors.
    
    Typically applications which are memory error aware is
    better of with early kill (see the error as soon
    as possible), all others with late kill (only
    see the error when the error is really impacting execution)
    
    There's a global sysctl, but this way an application
    can set its specific policy.
    
    We're using two bits, one to signify that the process
    stated its intention and that
    
    I also made the prctl future proof by enforcing
    the unused arguments are 0.
    
    The state is inherited to children.
    
    Note this makes us officially run out of process flags
    on 32bit, but the next patch can easily add another field.
    
    Manpage patch will be supplied separately.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f3d74bd04d18..29eae73c951d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1687,6 +1687,7 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
+#define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */
 #define PF_SIGNALED	0x00000400	/* killed by a signal */
@@ -1706,6 +1707,7 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_THREAD_BOUND	0x04000000	/* Thread bound to specific cpu */
+#define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezeable */

commit 59abf02644c45f1591e1374ee7bb45dc757fcb88
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 16 08:28:30 2009 +0200

    sched: Add SD_PREFER_LOCAL
    
    And turn it on for NUMA and MC domains. This improves
    locality in balancing decisions by keeping up to
    capacity amount of tasks local before looking for idle
    CPUs. (and twice the capacity if SD_POWERSAVINGS_BALANCE
    is set.)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ee1f88993097..b4a39bb2b4a4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -805,7 +805,7 @@ enum cpu_idle_type {
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
 #define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-
+#define SD_PREFER_LOCAL		0x0040  /* Prefer to keep tasks local to this domain */
 #define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
 #define SD_POWERSAVINGS_BALANCE	0x0100	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */

commit a7558e01056f5191ff2ecff53b075dcb9e484188
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 14 20:02:34 2009 +0200

    sched: Add WF_FORK
    
    Avoid the cache buddies from biasing the time distribution away
    from fork()ers. Normally the next buddy will be the preferred
    scheduling target, but this makes fork()s prefer to run the new
    child, whereas we prefer to run the parent, since that will
    generate more work.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b07168b6f03..ee1f88993097 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1028,6 +1028,7 @@ struct sched_domain;
  * wake flags
  */
 #define WF_SYNC		0x01		/* waker goes to sleep after wakup */
+#define WF_FORK		0x02		/* child wakeup after fork */
 
 struct sched_class {
 	const struct sched_class *next;

commit 7d47872146398dbede13223299fe1cb368ebc781
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 14 19:55:44 2009 +0200

    sched: Rename sync arguments
    
    In order to extend the functions to have more than 1 flag (sync),
    rename the argument to flags, and explicitly define a WF_ space for
    individual flags.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c116f03d74c..3b07168b6f03 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1024,6 +1024,11 @@ struct uts_namespace;
 struct rq;
 struct sched_domain;
 
+/*
+ * wake flags
+ */
+#define WF_SYNC		0x01		/* waker goes to sleep after wakup */
+
 struct sched_class {
 	const struct sched_class *next;
 
@@ -1031,13 +1036,13 @@ struct sched_class {
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
 	void (*yield_task) (struct rq *rq);
 
-	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int sync);
+	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
 
 	struct task_struct * (*pick_next_task) (struct rq *rq);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int sync);
+	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest, unsigned long max_load_move,

commit 0763a660a84220cc3900fd32abdd7ad109e2278d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 14 19:37:39 2009 +0200

    sched: Rename select_task_rq() argument
    
    In order to be able to rename the sync argument, we need to rename
    the current flag argument.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fc4c0f9393d2..5c116f03d74c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1037,7 +1037,7 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int flag, int sync);
+	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int sync);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest, unsigned long max_load_move,

commit 47fe38fcff0517e67d395c039d2e26d2de688a60
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 2 13:49:18 2009 +0200

    x86: sched: Provide arch implementations using aperf/mperf
    
    APERF/MPERF support for cpu_power.
    
    APERF/MPERF is arch defined to be a relative scale of work capacity
    per logical cpu, this is assumed to include SMT and Turbo mode.
    
    APERF/MPERF are specified to both reset to 0 when either counter
    wraps, which is highly inconvenient, since that'll give a blimp
    when that happens. The manual specifies writing 0 to the counters
    after each read, but that's 1) too expensive, and 2) destroys the
    possibility of sharing these counters with other users, so we live
    with the blimp - the other existing user does too.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c30bf3d516d1..fc4c0f9393d2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -992,6 +992,9 @@ static inline int test_sd_parent(struct sched_domain *sd, int flag)
 	return 0;
 }
 
+unsigned long default_scale_freq_power(struct sched_domain *sd, int cpu);
+unsigned long default_scale_smt_power(struct sched_domain *sd, int cpu);
+
 #else /* CONFIG_SMP */
 
 struct sched_domain_attr;
@@ -1003,6 +1006,7 @@ partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
 }
 #endif	/* !CONFIG_SMP */
 
+
 struct io_context;			/* See blkdev.h */
 
 

commit c88d5910890ad35af283344417891344604f0438
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 10 13:50:02 2009 +0200

    sched: Merge select_task_rq_fair() and sched_balance_self()
    
    The problem with wake_idle() is that is doesn't respect things like
    cpu_power, which means it doesn't deal well with SMT nor the recent
    RT interaction.
    
    To cure this, it needs to do what sched_balance_self() does, which
    leads to the possibility of merging select_task_rq_fair() and
    sched_balance_self().
    
    Modify sched_balance_self() to:
    
      - update_shares() when walking up the domain tree,
        (it only called it for the top domain, but it should
         have done this anyway), which allows us to remove
        this ugly bit from try_to_wake_up().
    
      - do wake_affine() on the smallest domain that contains
        both this (the waking) and the prev (the wakee) cpu for
        WAKE invocations.
    
    Then use the top-down balance steps it had to replace wake_idle().
    
    This leads to the dissapearance of SD_WAKE_BALANCE and
    SD_WAKE_IDLE_FAR, with SD_WAKE_IDLE replaced with SD_BALANCE_WAKE.
    
    SD_WAKE_AFFINE needs SD_BALANCE_WAKE to be effective.
    
    Touch all topology bits to replace the old with new SD flags --
    platforms might need re-tuning, enabling SD_BALANCE_WAKE
    conditionally on a NUMA distance seems like a good additional
    feature, magny-core and small nehalem systems would want this
    enabled, systems with slow interconnects would not.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b0ca66bd6ce..c30bf3d516d1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -803,16 +803,15 @@ enum cpu_idle_type {
 #define SD_BALANCE_NEWIDLE	0x0002	/* Balance when about to become idle */
 #define SD_BALANCE_EXEC		0x0004	/* Balance on exec */
 #define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
-#define SD_WAKE_IDLE		0x0010	/* Wake to idle CPU on task wakeup */
+#define SD_BALANCE_WAKE		0x0010  /* Balance on wakeup */
 #define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
-#define SD_WAKE_BALANCE		0x0040	/* Perform balancing at task wakeup */
+
 #define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
 #define SD_POWERSAVINGS_BALANCE	0x0100	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
-#define SD_WAKE_IDLE_FAR	0x0800	/* Gain latency sacrificing cache hit */
+
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
-#define SD_BALANCE_WAKE		0x2000  /* Balance on wakeup */
 
 enum powersavings_balance_level {
 	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */

commit e9c8431185d6c406887190519f6dbdd112641686
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 15 14:43:03 2009 +0200

    sched: Add TASK_WAKING
    
    We're going to want to drop rq->lock in try_to_wake_up() for a
    longer period of time, however we also want to deal with concurrent
    waking of the same task, which is currently handled by holding
    rq->lock.
    
    So introduce a new TASK state, namely TASK_WAKING, which indicates
    someone is already waking the task (other wakers will fail p->state
    & state).
    
    We also keep preemption disabled over the whole ttwu().
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5d3c9900943e..3b0ca66bd6ce 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -190,6 +190,7 @@ extern unsigned long long time_sync_thresh;
 /* in tsk->state again */
 #define TASK_DEAD		64
 #define TASK_WAKEKILL		128
+#define TASK_WAKING		256
 
 /* Convenience macros for the sake of set_task_state */
 #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)

commit 5f3edc1b1ead6d9bd45a85c551f44eff8fe76b9f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 10 13:42:00 2009 +0200

    sched: Hook sched_balance_self() into sched_class::select_task_rq()
    
    Rather ugly patch to fully place the sched_balance_self() code
    inside the fair class.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f3d74bd04d18..5d3c9900943e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -811,6 +811,7 @@ enum cpu_idle_type {
 #define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
 #define SD_WAKE_IDLE_FAR	0x0800	/* Gain latency sacrificing cache hit */
 #define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
+#define SD_BALANCE_WAKE		0x2000  /* Balance on wakeup */
 
 enum powersavings_balance_level {
 	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */
@@ -1032,7 +1033,7 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int sync);
+	int  (*select_task_rq)(struct task_struct *p, int flag, int sync);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest, unsigned long max_load_move,

commit 774a694f8cd08115d130a290d73c6d8563f26b1b
Merge: 4f0ac8541678 e1f8450854d6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 11 13:23:18 2009 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (64 commits)
      sched: Fix sched::sched_stat_wait tracepoint field
      sched: Disable NEW_FAIR_SLEEPERS for now
      sched: Keep kthreads at default priority
      sched: Re-tune the scheduler latency defaults to decrease worst-case latencies
      sched: Turn off child_runs_first
      sched: Ensure that a child can't gain time over it's parent after fork()
      sched: enable SD_WAKE_IDLE
      sched: Deal with low-load in wake_affine()
      sched: Remove short cut from select_task_rq_fair()
      sched: Turn on SD_BALANCE_NEWIDLE
      sched: Clean up topology.h
      sched: Fix dynamic power-balancing crash
      sched: Remove reciprocal for cpu_power
      sched: Try to deal with low capacity, fix update_sd_power_savings_stats()
      sched: Try to deal with low capacity
      sched: Scale down cpu_power due to RT tasks
      sched: Implement dynamic cpu_power
      sched: Add smt_gain
      sched: Update the cpu_power sum during load-balance
      sched: Add SD_PREFER_SIBLING
      ...

commit eee2775d9924b22643bd89b2e568cc5eed7e8a04
Merge: 53e16fbd3000 7db905e636f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 11 13:20:18 2009 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (28 commits)
      rcu: Move end of special early-boot RCU operation earlier
      rcu: Changes from reviews: avoid casts, fix/add warnings, improve comments
      rcu: Create rcutree plugins to handle hotplug CPU for multi-level trees
      rcu: Remove lockdep annotations from RCU's _notrace() API members
      rcu: Add #ifdef to suppress __rcu_offline_cpu() warning in !HOTPLUG_CPU builds
      rcu: Add CPU-offline processing for single-node configurations
      rcu: Add "notrace" to RCU function headers used by ftrace
      rcu: Remove CONFIG_PREEMPT_RCU
      rcu: Merge preemptable-RCU functionality into hierarchical RCU
      rcu: Simplify rcu_pending()/rcu_check_callbacks() API
      rcu: Use debugfs_remove_recursive() simplify code.
      rcu: Merge per-RCU-flavor initialization into pre-existing macro
      rcu: Fix online/offline indication for rcudata.csv trace file
      rcu: Consolidate sparse and lockdep declarations in include/linux/rcupdate.h
      rcu: Renamings to increase RCU clarity
      rcu: Move private definitions from include/linux/rcutree.h to kernel/rcutree.h
      rcu: Expunge lingering references to CONFIG_CLASSIC_RCU, optimize on !SMP
      rcu: Delay rcu_barrier() wait until beginning of next CPU-hotunplug operation.
      rcu: Fix typo in rcu_irq_exit() comment header
      rcu: Make rcupreempt_trace.c look at offline CPUs
      ...

commit 2bba22c50b06abe9fd0d23933b1e64d35b419262
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Sep 9 15:41:37 2009 +0200

    sched: Turn off child_runs_first
    
    Set child_runs_first default to off.
    
    It hurts 'optimal' make -j<NR_CPUS> workloads as make jobs
    get preempted by child tasks, reducing parallelism.
    
    Note, this patch might make existing races in user
    applications more prominent than before - so breakages
    might be bisected to this commit.
    
    Child-runs-first is broken on SMP to begin with, and we
    already had it off briefly in v2.6.23 so most of the
    offenders ought to be fixed. Would be nice not to revert
    this commit but fix those apps finally ...
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1252486344.28645.18.camel@marge.simson.net>
    [ made the sysctl independent of CONFIG_SCHED_DEBUG, in case
      people want to work around broken apps. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b7f43e3b736..3a50e8222498 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1820,8 +1820,8 @@ extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_shares_ratelimit;
 extern unsigned int sysctl_sched_shares_thresh;
-#ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_child_runs_first;
+#ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;

commit 18a3885fc1ffa92c2212ff0afdf033403d5b0fa0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 1 10:34:39 2009 +0200

    sched: Remove reciprocal for cpu_power
    
    Its a source of fail, also, now that cpu_power is dynamical,
    its a waste of time.
    
    before:
    <idle>-0   [000]   132.877936: find_busiest_group: avg_load: 0 group_load: 8241 power: 1
    
    after:
    bash-1689  [001]   137.862151: find_busiest_group: avg_load: 10636288 group_load: 10387 power: 1
    
    [ v2: build fix from From: Andreas Herrmann ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <20090901083826.425896304@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c67ddf309c84..3b7f43e3b736 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -860,15 +860,9 @@ struct sched_group {
 
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
-	 * single CPU. This is read only (except for setup, hotplug CPU).
-	 * Note : Never change cpu_power without recompute its reciprocal
+	 * single CPU.
 	 */
-	unsigned int __cpu_power;
-	/*
-	 * reciprocal value of cpu_power to avoid expensive divides
-	 * (see include/linux/reciprocal_div.h)
-	 */
-	u32 reciprocal_cpu_power;
+	unsigned int cpu_power;
 
 	/*
 	 * The CPUs this group covers.

commit e9e9250bc78e7f6342517214c0178a529807964b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 1 10:34:37 2009 +0200

    sched: Scale down cpu_power due to RT tasks
    
    Keep an average on the amount of time spend on RT tasks and use
    that fraction to scale down the cpu_power for regular tasks.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <20090901083826.287778431@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c81c921acb3..c67ddf309c84 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1831,6 +1831,7 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
+extern unsigned int sysctl_sched_time_avg;
 extern unsigned int sysctl_timer_migration;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,

commit a52bfd73589eaf88d9c95ad2c1de0b38a6b27972
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 1 10:34:35 2009 +0200

    sched: Add smt_gain
    
    The idea is that multi-threading a core yields more work
    capacity than a single thread, provide a way to express a
    static gain for threads.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <20090901083826.073345955@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 651dded25720..9c81c921acb3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -921,6 +921,7 @@ struct sched_domain {
 	unsigned int newidle_idx;
 	unsigned int wake_idx;
 	unsigned int forkexec_idx;
+	unsigned int smt_gain;
 	int flags;			/* See SD_* */
 	enum sched_domain_level level;
 

commit b5d978e0c7e79a7ff842e895c85a86b38c71f1cd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 1 10:34:33 2009 +0200

    sched: Add SD_PREFER_SIBLING
    
    Do the placement thing using SD flags.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Acked-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    LKML-Reference: <20090901083825.897028974@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c96ef2f7e68..651dded25720 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -798,18 +798,19 @@ enum cpu_idle_type {
 #define SCHED_LOAD_SCALE_FUZZ	SCHED_LOAD_SCALE
 
 #ifdef CONFIG_SMP
-#define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */
-#define SD_BALANCE_NEWIDLE	2	/* Balance when about to become idle */
-#define SD_BALANCE_EXEC		4	/* Balance on exec */
-#define SD_BALANCE_FORK		8	/* Balance on fork, clone */
-#define SD_WAKE_IDLE		16	/* Wake to idle CPU on task wakeup */
-#define SD_WAKE_AFFINE		32	/* Wake task to waking CPU */
-#define SD_WAKE_BALANCE		64	/* Perform balancing at task wakeup */
-#define SD_SHARE_CPUPOWER	128	/* Domain members share cpu power */
-#define SD_POWERSAVINGS_BALANCE	256	/* Balance for power savings */
-#define SD_SHARE_PKG_RESOURCES	512	/* Domain members share cpu pkg resources */
-#define SD_SERIALIZE		1024	/* Only a single load balancing instance */
-#define SD_WAKE_IDLE_FAR	2048	/* Gain latency sacrificing cache hit */
+#define SD_LOAD_BALANCE		0x0001	/* Do load balancing on this domain. */
+#define SD_BALANCE_NEWIDLE	0x0002	/* Balance when about to become idle */
+#define SD_BALANCE_EXEC		0x0004	/* Balance on exec */
+#define SD_BALANCE_FORK		0x0008	/* Balance on fork, clone */
+#define SD_WAKE_IDLE		0x0010	/* Wake to idle CPU on task wakeup */
+#define SD_WAKE_AFFINE		0x0020	/* Wake task to waking CPU */
+#define SD_WAKE_BALANCE		0x0040	/* Perform balancing at task wakeup */
+#define SD_SHARE_CPUPOWER	0x0080	/* Domain members share cpu power */
+#define SD_POWERSAVINGS_BALANCE	0x0100	/* Balance for power savings */
+#define SD_SHARE_PKG_RESOURCES	0x0200	/* Domain members share cpu pkg resources */
+#define SD_SERIALIZE		0x0400	/* Only a single load balancing instance */
+#define SD_WAKE_IDLE_FAR	0x0800	/* Gain latency sacrificing cache hit */
+#define SD_PREFER_SIBLING	0x1000	/* Prefer to place tasks in a sibling domain */
 
 enum powersavings_balance_level {
 	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */
@@ -829,7 +830,7 @@ static inline int sd_balance_for_mc_power(void)
 	if (sched_smt_power_savings)
 		return SD_POWERSAVINGS_BALANCE;
 
-	return 0;
+	return SD_PREFER_SIBLING;
 }
 
 static inline int sd_balance_for_package_power(void)
@@ -837,7 +838,7 @@ static inline int sd_balance_for_package_power(void)
 	if (sched_mc_power_savings | sched_smt_power_savings)
 		return SD_POWERSAVINGS_BALANCE;
 
-	return 0;
+	return SD_PREFER_SIBLING;
 }
 
 /*

commit 29e2035bddecce3eb584a8304528b50da8370a24
Merge: 868489660dab 37d0892c5a94
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 4 09:28:52 2009 +0200

    Merge branch 'linus' into core/rcu
    
    Merge reason: Avoid fuzz in init/main.c and update from rc6 to rc8.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ee18d64c1f632043a02e6f5ba5e045bb26a5465f
Author: David Howells <dhowells@redhat.com>
Date:   Wed Sep 2 09:14:21 2009 +0100

    KEYS: Add a keyctl to install a process's session keyring on its parent [try #6]
    
    Add a keyctl to install a process's session keyring onto its parent.  This
    replaces the parent's session keyring.  Because the COW credential code does
    not permit one process to change another process's credentials directly, the
    change is deferred until userspace next starts executing again.  Normally this
    will be after a wait*() syscall.
    
    To support this, three new security hooks have been provided:
    cred_alloc_blank() to allocate unset security creds, cred_transfer() to fill in
    the blank security creds and key_session_to_parent() - which asks the LSM if
    the process may replace its parent's session keyring.
    
    The replacement may only happen if the process has the same ownership details
    as its parent, and the process has LINK permission on the session keyring, and
    the session keyring is owned by the process, and the LSM permits it.
    
    Note that this requires alteration to each architecture's notify_resume path.
    This has been done for all arches barring blackfin, m68k* and xtensa, all of
    which need assembly alteration to support TIF_NOTIFY_RESUME.  This allows the
    replacement to be performed at the point the parent process resumes userspace
    execution.
    
    This allows the userspace AFS pioctl emulation to fully emulate newpag() and
    the VIOCSETTOK and VIOCSETTOK2 pioctls, all of which require the ability to
    alter the parent process's PAG membership.  However, since kAFS doesn't use
    PAGs per se, but rather dumps the keys into the session keyring, the session
    keyring of the parent must be replaced if, for example, VIOCSETTOK is passed
    the newpag flag.
    
    This can be tested with the following program:
    
            #include <stdio.h>
            #include <stdlib.h>
            #include <keyutils.h>
    
            #define KEYCTL_SESSION_TO_PARENT        18
    
            #define OSERROR(X, S) do { if ((long)(X) == -1) { perror(S); exit(1); } } while(0)
    
            int main(int argc, char **argv)
            {
                    key_serial_t keyring, key;
                    long ret;
    
                    keyring = keyctl_join_session_keyring(argv[1]);
                    OSERROR(keyring, "keyctl_join_session_keyring");
    
                    key = add_key("user", "a", "b", 1, keyring);
                    OSERROR(key, "add_key");
    
                    ret = keyctl(KEYCTL_SESSION_TO_PARENT);
                    OSERROR(ret, "KEYCTL_SESSION_TO_PARENT");
    
                    return 0;
            }
    
    Compiled and linked with -lkeyutils, you should see something like:
    
            [dhowells@andromeda ~]$ keyctl show
            Session Keyring
                   -3 --alswrv   4043  4043  keyring: _ses
            355907932 --alswrv   4043    -1   \_ keyring: _uid.4043
            [dhowells@andromeda ~]$ /tmp/newpag
            [dhowells@andromeda ~]$ keyctl show
            Session Keyring
                   -3 --alswrv   4043  4043  keyring: _ses
            1055658746 --alswrv   4043  4043   \_ user: a
            [dhowells@andromeda ~]$ /tmp/newpag hello
            [dhowells@andromeda ~]$ keyctl show
            Session Keyring
                   -3 --alswrv   4043  4043  keyring: hello
            340417692 --alswrv   4043  4043   \_ user: a
    
    Where the test program creates a new session keyring, sticks a user key named
    'a' into it and then installs it on its parent.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c7ce13c1696..9304027673b0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1292,6 +1292,7 @@ struct task_struct {
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations
 					 * (notably. ptrace) */
+	struct cred *replacement_session_keyring; /* for KEYCTL_SESSION_TO_PARENT */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock

commit 8f0dfc34e9b323a028c2ec41abb7e9de477b7a94
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Jul 20 11:26:58 2009 -0700

    sched: Provide iowait counters
    
    For counting how long an application has been waiting for
    (disk) IO, there currently is only the HZ sample driven
    information available, while for all other counters in this
    class, a high resolution version is available via
    CONFIG_SCHEDSTATS.
    
    In order to make an improved bootchart tool possible, we also
    need a higher resolution version of the iowait time.
    
    This patch below adds this scheduler statistic to the kernel.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4A64B813.1080506@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e209ae0e7a8a..9c96ef2f7e68 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1111,6 +1111,8 @@ struct sched_entity {
 	u64			wait_max;
 	u64			wait_count;
 	u64			wait_sum;
+	u64			iowait_count;
+	u64			iowait_sum;
 
 	u64			sleep_start;
 	u64			sleep_max;
@@ -1231,6 +1233,8 @@ struct task_struct {
 	unsigned did_exec:1;
 	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
 				 * execve */
+	unsigned in_iowait:1;
+
 
 	/* Revert to default priority/policy when forking */
 	unsigned sched_reset_on_fork:1;

commit f14eff1cc2f418a7c5e23aedc6a1bdca3343b871
Merge: 84e9dabf6e6a 326ba5010a54
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 2 08:20:32 2009 +0200

    Merge commit 'v2.6.31-rc8' into sched/core
    
    Merge reason: bump from rc5 to rc8, but also pick up TP_perf_assign()
                  API, a patch will be queued that depends on it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 868489660dabc0c28087cca3dbc1adbbc398c6fe
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 27 15:00:12 2009 -0700

    rcu: Changes from reviews: avoid casts, fix/add warnings, improve comments
    
    Changes suggested by review comments from Josh Triplett and
    Mathieu Desnoyers.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <20090827220012.GA30525@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3fe03151a8e6..855fd0d3f174 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1163,6 +1163,8 @@ struct sched_rt_entity {
 #endif
 };
 
+struct rcu_node;
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -1208,7 +1210,7 @@ struct task_struct {
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
-	void *rcu_blocked_node;
+	struct rcu_node *rcu_blocked_node;
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 

commit dd5d19bafd90d33043a4a14b2e2d98612caa293c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 27 14:58:16 2009 -0700

    rcu: Create rcutree plugins to handle hotplug CPU for multi-level trees
    
    When offlining CPUs from a multi-level tree, there is the
    possibility of offlining the last CPU from a given node when
    there are preempted RCU read-side critical sections that
    started life on one of the CPUs on that node.
    
    In this case, the corresponding tasks will be enqueued via the
    task_struct's rcu_node_entry list_head onto one of the
    rcu_node's blocked_tasks[] lists.  These tasks need to be moved
    somewhere else so that they will prevent the current grace
    period from ending. That somewhere is the root rcu_node.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <20090827215816.GA30472@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bfca26d63b13..3fe03151a8e6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1208,7 +1208,7 @@ struct task_struct {
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
-	int rcu_blocked_cpu;
+	void *rcu_blocked_node;
 	struct list_head rcu_node_entry;
 #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
 
@@ -1735,7 +1735,7 @@ static inline void rcu_copy_process(struct task_struct *p)
 {
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_read_unlock_special = 0;
-	p->rcu_blocked_cpu = -1;
+	p->rcu_blocked_node = NULL;
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 

commit 6b3ef48adf847f7adf11c870e3ffacac150f1564
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:53 2009 -0700

    rcu: Remove CONFIG_PREEMPT_RCU
    
    Now that CONFIG_TREE_PREEMPT_RCU is in place, there is no
    further need for CONFIG_PREEMPT_RCU.  Remove it, along with
    whatever subtle bugs it may (or may not) contain.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <125097461396-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d7f98f637a2a..bfca26d63b13 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1205,11 +1205,6 @@ struct task_struct {
 	unsigned int policy;
 	cpumask_t cpus_allowed;
 
-#ifdef CONFIG_PREEMPT_RCU
-	int rcu_read_lock_nesting;
-	int rcu_flipctr_idx;
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
-
 #ifdef CONFIG_TREE_PREEMPT_RCU
 	int rcu_read_lock_nesting;
 	char rcu_read_unlock_special;
@@ -1744,14 +1739,6 @@ static inline void rcu_copy_process(struct task_struct *p)
 	INIT_LIST_HEAD(&p->rcu_node_entry);
 }
 
-#elif defined(CONFIG_PREEMPT_RCU)
-
-static inline void rcu_copy_process(struct task_struct *p)
-{
-	p->rcu_read_lock_nesting = 0;
-	p->rcu_flipctr_idx = 0;
-}
-
 #else
 
 static inline void rcu_copy_process(struct task_struct *p)

commit f41d911f8c49a5d65c86504c19e8204bb605c4fd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:52 2009 -0700

    rcu: Merge preemptable-RCU functionality into hierarchical RCU
    
    Create a kernel/rcutree_plugin.h file that contains definitions
    for preemptable RCU (or, under the #else branch of the #ifdef,
    empty definitions for the classic non-preemptable semantics).
    These definitions fit into plugins defined in kernel/rcutree.c
    for this purpose.
    
    This variant of preemptable RCU uses a new algorithm whose
    read-side expense is roughly that of classic hierarchical RCU
    under CONFIG_PREEMPT. This new algorithm's update-side expense
    is similar to that of classic hierarchical RCU, and, in absence
    of read-side preemption or blocking, is exactly that of classic
    hierarchical RCU.  Perhaps more important, this new algorithm
    has a much simpler implementation, saving well over 1,000 lines
    of code compared to mainline's implementation of preemptable
    RCU, which will hopefully be retired in favor of this new
    algorithm.
    
    The simplifications are obtained by maintaining per-task
    nesting state for running tasks, and using a simple
    lock-protected algorithm to handle accounting when tasks block
    within RCU read-side critical sections, making use of lessons
    learned while creating numerous user-level RCU implementations
    over the past 18 months.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746134003-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3ab08e4bb6b8..d7f98f637a2a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1210,6 +1210,13 @@ struct task_struct {
 	int rcu_flipctr_idx;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
+#ifdef CONFIG_TREE_PREEMPT_RCU
+	int rcu_read_lock_nesting;
+	char rcu_read_unlock_special;
+	int rcu_blocked_cpu;
+	struct list_head rcu_node_entry;
+#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
 #endif
@@ -1723,6 +1730,36 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
 #define used_math() tsk_used_math(current)
 
+#ifdef CONFIG_TREE_PREEMPT_RCU
+
+#define RCU_READ_UNLOCK_BLOCKED (1 << 0) /* blocked while in RCU read-side. */
+#define RCU_READ_UNLOCK_NEED_QS (1 << 1) /* RCU core needs CPU response. */
+#define RCU_READ_UNLOCK_GOT_QS  (1 << 2) /* CPU has responded to RCU core. */
+
+static inline void rcu_copy_process(struct task_struct *p)
+{
+	p->rcu_read_lock_nesting = 0;
+	p->rcu_read_unlock_special = 0;
+	p->rcu_blocked_cpu = -1;
+	INIT_LIST_HEAD(&p->rcu_node_entry);
+}
+
+#elif defined(CONFIG_PREEMPT_RCU)
+
+static inline void rcu_copy_process(struct task_struct *p)
+{
+	p->rcu_read_lock_nesting = 0;
+	p->rcu_flipctr_idx = 0;
+}
+
+#else
+
+static inline void rcu_copy_process(struct task_struct *p)
+{
+}
+
+#endif
+
 #ifdef CONFIG_SMP
 extern int set_cpus_allowed_ptr(struct task_struct *p,
 				const struct cpumask *new_mask);

commit ece13879e74313e62109e0755dd3d4f172df89e2
Merge: b08dc3eba0c3 6c30c53fd5ae
Author: James Morris <jmorris@namei.org>
Date:   Thu Aug 20 09:18:42 2009 +1000

    Merge branch 'master' into next
    
    Conflicts:
            security/Kconfig
    
    Manual fix.
    
    Signed-off-by: James Morris <jmorris@namei.org>

commit 0753ba01e126020bf0f8150934903b48935b697d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Aug 18 14:11:10 2009 -0700

    mm: revert "oom: move oom_adj value"
    
    The commit 2ff05b2b (oom: move oom_adj value) moveed the oom_adj value to
    the mm_struct.  It was a very good first step for sanitize OOM.
    
    However Paul Menage reported the commit makes regression to his job
    scheduler.  Current OOM logic can kill OOM_DISABLED process.
    
    Why? His program has the code of similar to the following.
    
            ...
            set_oom_adj(OOM_DISABLE); /* The job scheduler never killed by oom */
            ...
            if (vfork() == 0) {
                    set_oom_adj(0); /* Invoked child can be killed */
                    execve("foo-bar-cmd");
            }
            ....
    
    vfork() parent and child are shared the same mm_struct.  then above
    set_oom_adj(0) doesn't only change oom_adj for vfork() child, it's also
    change oom_adj for vfork() parent.  Then, vfork() parent (job scheduler)
    lost OOM immune and it was killed.
    
    Actually, fork-setting-exec idiom is very frequently used in userland program.
    We must not break this assumption.
    
    Then, this patch revert commit 2ff05b2b and related commit.
    
    Reverted commit list
    ---------------------
    - commit 2ff05b2b4e (oom: move oom_adj value from task_struct to mm_struct)
    - commit 4d8b9135c3 (oom: avoid unnecessary mm locking and scanning for OOM_DISABLE)
    - commit 8123681022 (oom: only oom kill exiting tasks with attached memory)
    - commit 933b787b57 (mm: copy over oom_adj value at fork time)
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3ab08e4bb6b8..0f1ea4a66957 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1198,6 +1198,7 @@ struct task_struct {
 	 * a short time
 	 */
 	unsigned char fpu_counter;
+	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif

commit 1314562a9ae5f39f6f595656023c1baf970831ef
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Tue Aug 18 15:06:02 2009 +0900

    sched, task_struct: stack_canary is not needed without CC_STACKPROTECTOR
    
    The field stack_canary is only used with CC_STACKPROTECTOR.
    This patch reduces task_struct size without CC_STACKPROTECTOR.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    LKML-Reference: <4A8A44CA.2020701@ct.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 195d72d5c102..7bc2d9290837 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1237,8 +1237,10 @@ struct task_struct {
 	pid_t pid;
 	pid_t tgid;
 
+#ifdef CONFIG_CC_STACKPROTECTOR
 	/* Canary value for the -fstack-protector gcc feature */
 	unsigned long stack_canary;
+#endif
 
 	/* 
 	 * pointers to (original) parent process, youngest child, younger sibling,

commit 012a5299a29672039f42944a37984558393ef769
Merge: da34d4248bd2 90bc1a658a53
Author: James Morris <jmorris@namei.org>
Date:   Thu Aug 6 08:55:03 2009 +1000

    Merge branch 'master' into next

commit 8356b5f9c424e5831715abbce747197c30d1fd71
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Jul 29 12:15:27 2009 +0200

    itimers: Fix periodic tics precision
    
    Measure ITIMER_PROF and ITIMER_VIRT timers interval error
    between real ticks and requested by user. Take it into account
    when scheduling next tick.
    
    This patch introduce possibility where time between two
    consecutive tics is smaller then requested interval, it
    preserve however dependency that n tick is generated not
    earlier than n*interval time - counting from the beginning of
    periodic signal generation.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    LKML-Reference: <1248862529-6063-3-git-send-email-sgruszka@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b3efaddd953..a069e65e8bb7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -473,6 +473,8 @@ struct pacct_struct {
 struct cpu_itimer {
 	cputime_t expires;
 	cputime_t incr;
+	u32 error;
+	u32 incr_error;
 };
 
 /**

commit 42c4ab41a176ee784c0f28c0b29025a8fc34f05a
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Jul 29 12:15:26 2009 +0200

    itimers: Merge ITIMER_VIRT and ITIMER_PROF
    
    Both cpu itimers have same data flow in the few places, this
    patch make unification of code related with VIRT and PROF
    itimers.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    LKML-Reference: <1248862529-6063-2-git-send-email-sgruszka@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3ab08e4bb6b8..3b3efaddd953 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -470,6 +470,11 @@ struct pacct_struct {
 	unsigned long		ac_minflt, ac_majflt;
 };
 
+struct cpu_itimer {
+	cputime_t expires;
+	cputime_t incr;
+};
+
 /**
  * struct task_cputime - collected CPU time counts
  * @utime:		time spent in user mode, in &cputime_t units
@@ -564,9 +569,12 @@ struct signal_struct {
 	struct pid *leader_pid;
 	ktime_t it_real_incr;
 
-	/* ITIMER_PROF and ITIMER_VIRTUAL timers for the process */
-	cputime_t it_prof_expires, it_virt_expires;
-	cputime_t it_prof_incr, it_virt_incr;
+	/*
+	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
+	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
+	 * values are defined to 0 and 1 respectively
+	 */
+	struct cpu_itimer it[2];
 
 	/*
 	 * Thread group totals for process CPU timers.

commit 3f029d3c6d62068d59301d90c18dbde8ee402107
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Wed Jul 29 11:08:47 2009 -0400

    sched: Enhance the pre/post scheduling logic
    
    We currently have an explicit "needs_post" vtable method which
    returns a stack variable for whether we should later run
    post-schedule.  This leads to an awkward exchange of the
    variable as it bubbles back up out of the context switch. Peter
    Zijlstra observed that this information could be stored in the
    run-queue itself instead of handled on the stack.
    
    Therefore, we revert to the method of having context_switch
    return void, and update an internal rq->post_schedule variable
    when we require further processing.
    
    In addition, we fix a race condition where we try to access
    current->sched_class without holding the rq->lock.  This is
    technically racy, as the sched-class could change out from
    under us.  Instead, we reference the per-rq post_schedule
    variable with the runqueue unlocked, but with preemption
    disabled to see if we need to reacquire the rq->lock.
    
    Finally, we clean the code up slightly by removing the #ifdef
    CONFIG_SMP conditionals from the schedule() call, and implement
    some inline helper functions instead.
    
    This patch passes checkpatch, and rt-migrate.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20090729150422.17691.55590.stgit@dev.haskins.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c35bc29d2a9..195d72d5c102 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1047,7 +1047,6 @@ struct sched_class {
 			      struct rq *busiest, struct sched_domain *sd,
 			      enum cpu_idle_type idle);
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
-	int (*needs_post_schedule) (struct rq *this_rq);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
 

commit 8e9ed8b02490fea577b1eb1704c05bf43c891ed7
Merge: 716a42348cda 07903af152b0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Aug 2 14:11:26 2009 +0200

    Merge branch 'sched/urgent' into sched/core
    
    Merge reason: avoid upcoming patch conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 716a42348cdaf04534b15fbdc9c83e25baebfed5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 24 20:05:23 2009 +0200

    sched: Fix cond_resched_lock() in !CONFIG_PREEMPT
    
    The might_sleep() test inside cond_resched_lock() assumes the
    spinlock is held and then preemption is disabled. This is true
    with CONFIG_PREEMPT but the preempt_count() doesn't change
    otherwise.
    
    Check by starting from the appropriate preempt offset depending
    on the config.
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1248458723-12146-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cbbfca69aa4a..c472414953bf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2293,8 +2293,14 @@ extern int _cond_resched(void);
 
 extern int __cond_resched_lock(spinlock_t *lock);
 
+#ifdef CONFIG_PREEMPT
+#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
+
 #define cond_resched_lock(lock) ({				\
-	__might_sleep(__FILE__, __LINE__, PREEMPT_OFFSET);	\
+	__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);	\
 	__cond_resched_lock(lock);				\
 })
 

commit def01bc53d03881acfc393bd10a5c7575187e008
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 16 15:44:29 2009 +0200

    sched: Convert the only user of cond_resched_bkl to use cond_resched()
    
    fs/locks.c:flock_lock_file() is the only user of
    cond_resched_bkl()
    
    This helper doesn't do anything more than cond_resched(). The
    latter naming is enough to explain that we are rescheduling if
    needed.
    
    The bkl suffix suggests another semantics but it's actually a
    synonym of cond_resched().
    
    Reported-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1247725694-6082-7-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c41d424db887..cbbfca69aa4a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2305,8 +2305,6 @@ extern int __cond_resched_softirq(void);
 	__cond_resched_softirq();				\
 })
 
-#define cond_resched_bkl()	cond_resched()
-
 /*
  * Does a critical section need to be broken due to another
  * task waiting?: (technically does not depend on CONFIG_PREEMPT,

commit 613afbf83298efaead05ebcac23d2285609d7160
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 16 15:44:29 2009 +0200

    sched: Pull up the might_sleep() check into cond_resched()
    
    might_sleep() is called late-ish in cond_resched(), after the
    need_resched()/preempt enabled/system running tests are
    checked.
    
    It's better to check the sleeps while atomic earlier and not
    depend on some environment datas that reduce the chances to
    detect a problem.
    
    Also define cond_resched_*() helpers as macros, so that the
    FILE/LINE reported in the sleeping while atomic warning
    displays the real origin and not sched.h
    
    Changes in v2:
    
     - Call __might_sleep() directly instead of might_sleep() which
       may call cond_resched()
    
     - Turn cond_resched() into a macro so that the file:line
       couple reported refers to the caller of cond_resched() and
       not __cond_resched() itself.
    
    Changes in v3:
    
     - Also propagate this __might_sleep() pull up to
       cond_resched_lock() and cond_resched_softirq()
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1247725694-6082-6-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e2bdf18e05c4..c41d424db887 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2286,17 +2286,26 @@ static inline int need_resched(void)
  */
 extern int _cond_resched(void);
 
-static inline int cond_resched(void)
-{
-	return _cond_resched();
-}
+#define cond_resched() ({			\
+	__might_sleep(__FILE__, __LINE__, 0);	\
+	_cond_resched();			\
+})
 
-extern int cond_resched_lock(spinlock_t * lock);
-extern int cond_resched_softirq(void);
-static inline int cond_resched_bkl(void)
-{
-	return _cond_resched();
-}
+extern int __cond_resched_lock(spinlock_t *lock);
+
+#define cond_resched_lock(lock) ({				\
+	__might_sleep(__FILE__, __LINE__, PREEMPT_OFFSET);	\
+	__cond_resched_lock(lock);				\
+})
+
+extern int __cond_resched_softirq(void);
+
+#define cond_resched_softirq() ({				\
+	__might_sleep(__FILE__, __LINE__, SOFTIRQ_OFFSET);	\
+	__cond_resched_softirq();				\
+})
+
+#define cond_resched_bkl()	cond_resched()
 
 /*
  * Does a critical section need to be broken due to another

commit 6f80bd985fe242c2e6a8b6209ed20b0495d3d63b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 16 15:44:29 2009 +0200

    sched: Remove the CONFIG_PREEMPT_BKL case definition of cond_resched()
    
    CONFIG_PREEMPT_BKL doesn't exist anymore. So remove this
    config-on case definition of cond_resched().
    
    Reported-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1247725694-6082-5-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9bada20e2b23..e2bdf18e05c4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2285,17 +2285,12 @@ static inline int need_resched(void)
  * cond_resched_softirq() will enable bhs before scheduling.
  */
 extern int _cond_resched(void);
-#ifdef CONFIG_PREEMPT_BKL
-static inline int cond_resched(void)
-{
-	return 0;
-}
-#else
+
 static inline int cond_resched(void)
 {
 	return _cond_resched();
 }
-#endif
+
 extern int cond_resched_lock(spinlock_t * lock);
 extern int cond_resched_softirq(void);
 static inline int cond_resched_bkl(void)

commit 5304d5fc74a269cc6c3e70f9713684ca729abdf0
Merge: 54d35f29f492 78af08d90b8f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jul 18 15:50:22 2009 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: branch had an old upstream base (-rc1-ish), but also
                  merge to avoid a conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6301cb95c119ebf324bb96ee226fa9ddffad80a7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 17 14:15:47 2009 +0200

    sched: fix nr_uninterruptible accounting of frozen tasks really
    
    commit e3c8ca8336 (sched: do not count frozen tasks toward load) broke
    the nr_uninterruptible accounting on freeze/thaw. On freeze the task
    is excluded from accounting with a check for (task->flags &
    PF_FROZEN), but that flag is cleared before the task is thawed. So
    while we prevent that the task with state TASK_UNINTERRUPTIBLE
    is accounted to nr_uninterruptible on freeze we decrement
    nr_uninterruptible on thaw.
    
    Use a separate flag which is handled by the freezing task itself. Set
    it before calling the scheduler with TASK_UNINTERRUPTIBLE state and
    clear it after we return from frozen state.
    
    Cc: <stable@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 16a982e389fb..3ab08e4bb6b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -209,7 +209,7 @@ extern unsigned long long time_sync_thresh;
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
-				 (task->flags & PF_FROZEN) == 0)
+				 (task->flags & PF_FREEZING) == 0)
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
@@ -1680,6 +1680,7 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_FLUSHER	0x00001000	/* responsible for disk writeback */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
+#define PF_FREEZING	0x00004000	/* freeze in progress. do not account to load */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */

commit 5bb459bb45d1ad3c177485dcf0af01580aa31125
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 10 03:48:23 2009 +0200

    kernel: rename is_single_threaded(task) to current_is_single_threaded(void)
    
    - is_single_threaded(task) is not safe unless task == current,
      we can't use task->signal or task->mm.
    
    - it doesn't make sense unless task == current, the task can
      fork right after the check.
    
    Rename it to current_is_single_threaded() and kill the argument.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 16a982e389fb..0839a2c9b952 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2075,7 +2075,7 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 #define for_each_process(p) \
 	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
 
-extern bool is_single_threaded(struct task_struct *);
+extern bool current_is_single_threaded(void);
 
 /*
  * Careful: do_each_thread/while_each_thread is a double loop so

commit d86ee4809d0329d4aa0d0f2c76c2295a16862799
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 10 14:57:57 2009 +0200

    sched: optimize cond_resched()
    
    Optimize cond_resched() by removing one conditional.
    
    Currently cond_resched() checks system_state ==
    SYSTEM_RUNNING in order to avoid scheduling before the
    scheduler is running.
    
    We can however, as per suggestion of Matt, use
    PREEMPT_ACTIVE to accomplish that very same.
    
    Suggested-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2a99f1c15cf8..16a982e389fb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -501,8 +501,11 @@ struct task_cputime {
 /*
  * Disable preemption until the scheduler is running.
  * Reset by start_kernel()->sched_init()->init_idle().
+ *
+ * We include PREEMPT_ACTIVE to avoid cond_resched() from working
+ * before the scheduler is active -- see should_resched().
  */
-#define INIT_PREEMPT_COUNT	(1)
+#define INIT_PREEMPT_COUNT	(1 + PREEMPT_ACTIVE)
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts

commit c99e6efe1ba04561e7d93a81f0be07e37427e835
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 10 14:57:56 2009 +0200

    sched: INIT_PREEMPT_COUNT
    
    Pull the initial preempt_count value into a single
    definition site.
    
    Maintainers for: alpha, ia64 and m68k, please have a look,
    your arch code is funny.
    
    The header magic is a bit odd, but similar to the KERNEL_DS
    one, CPP waits with expanding these macros until the
    INIT_THREAD_INFO macro itself is expanded, which is in
    arch/*/kernel/init_task.c where we've already included
    sched.h so we're good.
    
    Cc: tony.luck@intel.com
    Cc: rth@twiddle.net
    Cc: geert@linux-m68k.org
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Matt Mackall <mpm@selenic.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0085d758d645..2a99f1c15cf8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -498,6 +498,12 @@ struct task_cputime {
 		.sum_exec_runtime = 0,				\
 	}
 
+/*
+ * Disable preemption until the scheduler is running.
+ * Reset by start_kernel()->sched_init()->init_idle().
+ */
+#define INIT_PREEMPT_COUNT	(1)
+
 /**
  * struct thread_group_cputimer - thread group interval timer counts
  * @cputime:		thread group interval timers.

commit 341c87bf346f57748230628c5ad6ee69219250e8
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Jun 30 11:41:23 2009 -0700

    elf: limit max map count to safe value
    
    With ELF, at generating coredump, some more headers other than used
    vmas are added.
    
    When max_map_count == 65536, a core generated by following kinds of
    code can be unreadable because the number of ELF's program header is
    written in 16bit in Ehdr (please see elf.h) and the number overflows.
    
    ==
            ... = mmap(); (munmap, mprotect, etc...)
            if (failed)
                    abort();
    ==
    
    This can happen in mmap/munmap/mprotect/etc...which calls split_vma().
    
    I think 65536 is not safe as _default_ and reduce it to 65530 is good
    for avoiding unexpected corrupted core.
    
    Anyway, max_map_count can be enlarged by sysctl if a user is brave..
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4d0754269884..0085d758d645 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -349,8 +349,20 @@ extern int mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner);
 struct nsproxy;
 struct user_namespace;
 
-/* Maximum number of active map areas.. This is a random (large) number */
-#define DEFAULT_MAX_MAP_COUNT	65536
+/*
+ * Default maximum number of active map areas, this limits the number of vmas
+ * per mm struct. Users can overwrite this number by sysctl but there is a
+ * problem.
+ *
+ * When a program's coredump is generated as ELF format, a section is created
+ * per a vma. In ELF, the number of sections is represented in unsigned short.
+ * This means the number of sections should be smaller than 65535 at coredump.
+ * Because the kernel adds some informative sections to a image of program at
+ * generating coredump, we need some margin. The number of extra sections is
+ * 1-3 now and depends on arch. We use "5" as safe margin, here.
+ */
+#define MAPCOUNT_ELF_CORE_MARGIN	(5)
+#define DEFAULT_MAX_MAP_COUNT	(USHORT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
 

commit 348b346b238d9c0e5694c8d0b835a099cb383835
Merge: 6c697bdf08a0 52989765629e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 29 09:16:13 2009 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: we will merge a dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 17f98dcf6010a1cfd25d179fd0ce77d3dc2685c3
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 16:27:51 2009 -0700

    pids: clean up find_task_by_pid variants
    
    find_task_by_pid_type_ns is only used to implement find_task_by_vpid and
    find_task_by_pid_ns, but both of them pass PIDTYPE_PID as first argument.
    So just fold find_task_by_pid_type_ns into find_task_by_pid_ns and use
    find_task_by_pid_ns to implement find_task_by_vpid.
    
    While we're at it also remove the exports for find_task_by_pid_ns and
    find_task_by_vpid - we don't have any modular callers left as the only
    modular caller of he old pre pid namespace find_task_by_pid (gfs2) was
    switched to pid_task which operates on a struct pid pointer instead of a
    pid_t.  Given the confusion about pid_t values vs namespace that's
    generally the better option anyway and I think we're better of restricting
    modules to do it that way.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d0342101756a..4d0754269884 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1878,9 +1878,6 @@ extern struct pid_namespace init_pid_ns;
 /*
  * find a task by one of its numerical ids
  *
- * find_task_by_pid_type_ns():
- *      it is the most generic call - it finds a task by all id,
- *      type and namespace specified
  * find_task_by_pid_ns():
  *      finds a task by its pid in the specified namespace
  * find_task_by_vpid():
@@ -1889,9 +1886,6 @@ extern struct pid_namespace init_pid_ns;
  * see also find_vpid() etc in include/linux/pid.h
  */
 
-extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,
-		struct pid_namespace *ns);
-
 extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);

commit 20ebcdda78a282d1d5266887ddf8a2d670182576
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Jun 17 16:27:16 2009 -0700

    memcg: remove unneeded forward declaration from sched.h
    
    This forward declaration seems pointless.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 02042e7f2196..d0342101756a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -92,7 +92,6 @@ struct sched_param {
 
 #include <asm/processor.h>
 
-struct mem_cgroup;
 struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;

commit 517d08699b250021303f9a7cf0d758b6dc0748ed
Merge: 8eeee4e2f04f a34601c5d841
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 16 19:50:13 2009 -0700

    Merge branch 'akpm'
    
    * akpm: (182 commits)
      fbdev: bf54x-lq043fb: use kzalloc over kmalloc/memset
      fbdev: *bfin*: fix __dev{init,exit} markings
      fbdev: *bfin*: drop unnecessary calls to memset
      fbdev: bfin-t350mcqb-fb: drop unused local variables
      fbdev: blackfin has __raw I/O accessors, so use them in fb.h
      fbdev: s1d13xxxfb: add accelerated bitblt functions
      tcx: use standard fields for framebuffer physical address and length
      fbdev: add support for handoff from firmware to hw framebuffers
      intelfb: fix a bug when changing video timing
      fbdev: use framebuffer_release() for freeing fb_info structures
      radeon: P2G2CLK_ALWAYS_ONb tested twice, should 2nd be P2G2CLK_DAC_ALWAYS_ONb?
      s3c-fb: CPUFREQ frequency scaling support
      s3c-fb: fix resource releasing on error during probing
      carminefb: fix possible access beyond end of carmine_modedb[]
      acornfb: remove fb_mmap function
      mb862xxfb: use CONFIG_OF instead of CONFIG_PPC_OF
      mb862xxfb: restrict compliation of platform driver to PPC
      Samsung SoC Framebuffer driver: add Alpha Channel support
      atmel-lcdc: fix pixclock upper bound detection
      offb: use framebuffer_alloc() to allocate fb_info struct
      ...
    
    Manually fix up conflicts due to kmemcheck in mm/slab.c

commit 2ff05b2b4eac2e63d345fc731ea151a060247f53
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jun 16 15:32:56 2009 -0700

    oom: move oom_adj value from task_struct to mm_struct
    
    The per-task oom_adj value is a characteristic of its mm more than the
    task itself since it's not possible to oom kill any thread that shares the
    mm.  If a task were to be killed while attached to an mm that could not be
    freed because another thread were set to OOM_DISABLE, it would have
    needlessly been terminated since there is no potential for future memory
    freeing.
    
    This patch moves oomkilladj (now more appropriately named oom_adj) from
    struct task_struct to struct mm_struct.  This requires task_lock() on a
    task to check its oom_adj value to protect against exec, but it's already
    necessary to take the lock when dereferencing the mm to find the total VM
    size for the badness heuristic.
    
    This fixes a livelock if the oom killer chooses a task and another thread
    sharing the same memory has an oom_adj value of OOM_DISABLE.  This occurs
    because oom_kill_task() repeatedly returns 1 and refuses to kill the
    chosen task while select_bad_process() will repeatedly choose the same
    task during the next retry.
    
    Taking task_lock() in select_bad_process() to check for OOM_DISABLE and in
    oom_kill_task() to check for threads sharing the same memory will be
    removed in the next patch in this series where it will no longer be
    necessary.
    
    Writing to /proc/pid/oom_adj for a kthread will now return -EINVAL since
    these threads are immune from oom killing already.  They simply report an
    oom_adj value of OOM_DISABLE.
    
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1048bf50540a..1bc6fae0c135 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1178,7 +1178,6 @@ struct task_struct {
 	 * a short time
 	 */
 	unsigned char fpu_counter;
-	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif

commit 58568d2a8215cb6f55caf2332017d7bdff954e1c
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Jun 16 15:31:49 2009 -0700

    cpuset,mm: update tasks' mems_allowed in time
    
    Fix allocating page cache/slab object on the unallowed node when memory
    spread is set by updating tasks' mems_allowed after its cpuset's mems is
    changed.
    
    In order to update tasks' mems_allowed in time, we must modify the code of
    memory policy.  Because the memory policy is applied in the process's
    context originally.  After applying this patch, one task directly
    manipulates anothers mems_allowed, and we use alloc_lock in the
    task_struct to protect mems_allowed and memory policy of the task.
    
    But in the fast path, we didn't use lock to protect them, because adding a
    lock may lead to performance regression.  But if we don't add a lock,the
    task might see no nodes when changing cpuset's mems_allowed to some
    non-overlapping set.  In order to avoid it, we set all new allowed nodes,
    then clear newly disallowed ones.
    
    [lee.schermerhorn@hp.com:
      The rework of mpol_new() to extract the adjusting of the node mask to
      apply cpuset and mpol flags "context" breaks set_mempolicy() and mbind()
      with MPOL_PREFERRED and a NULL nodemask--i.e., explicit local
      allocation.  Fix this by adding the check for MPOL_PREFERRED and empty
      node mask to mpol_new_mpolicy().
    
      Remove the now unneeded 'nodes = NULL' from mpol_new().
    
      Note that mpol_new_mempolicy() is always called with a non-NULL
      'nodes' parameter now that it has been removed from mpol_new().
      Therefore, we don't need to test nodes for NULL before testing it for
      'empty'.  However, just to be extra paranoid, add a VM_BUG_ON() to
      verify this assumption.]
    [lee.schermerhorn@hp.com:
    
      I don't think the function name 'mpol_new_mempolicy' is descriptive
      enough to differentiate it from mpol_new().
    
      This function applies cpuset set context, usually constraining nodes
      to those allowed by the cpuset.  However, when the 'RELATIVE_NODES flag
      is set, it also translates the nodes.  So I settled on
      'mpol_set_nodemask()', because the comment block for mpol_new() mentions
      that we need to call this function to "set nodes".
    
      Some additional minor line length, whitespace and typo cleanup.]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Paul Menage <menage@google.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c900aa530070..1048bf50540a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1318,7 +1318,8 @@ struct task_struct {
 /* Thread group tracking */
    	u32 parent_exec_id;
    	u32 self_exec_id;
-/* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
+/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
+ * mempolicy */
 	spinlock_t alloc_lock;
 
 #ifdef CONFIG_GENERIC_HARDIRQS
@@ -1386,8 +1387,7 @@ struct task_struct {
 	cputime_t acct_timexpd;	/* stime + utime since last update */
 #endif
 #ifdef CONFIG_CPUSETS
-	nodemask_t mems_allowed;
-	int cpuset_mems_generation;
+	nodemask_t mems_allowed;	/* Protected by alloc_lock */
 	int cpuset_mem_spread_rotor;
 #endif
 #ifdef CONFIG_CGROUPS
@@ -1410,7 +1410,7 @@ struct task_struct {
 	struct list_head perf_counter_list;
 #endif
 #ifdef CONFIG_NUMA
-	struct mempolicy *mempolicy;
+	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
 	short il_next;
 #endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */

commit 3959214f971417f4162926ac52ad4cd042958caa
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Tue Mar 24 15:43:30 2009 +0100

    sched: delayed cleanup of user_struct
    
    During bootup performance tracing we see repeated occurrences of
    /sys/kernel/uid/* events for the same uid, leading to a,
    in this case, rather pointless userspace processing for the
    same uid over and over.
    
    This is usually caused by tools which change their uid to "nobody",
    to run without privileges to read data supplied by untrusted users.
    
    This change delays the execution of the (already existing) scheduled
    work, to cleanup the uid after one second, so the allocated and announced
    uid can possibly be re-used by another process.
    
    This is the current behavior, where almost every invocation of a
    binary, which changes the uid, creates two events:
      $ read START < /sys/kernel/uevent_seqnum; \
      for i in `seq 100`; do su --shell=/bin/true bin; done; \
      read END < /sys/kernel/uevent_seqnum; \
      echo $(($END - $START))
      178
    
    With the delayed cleanup, we get only two events, and userspace finishes
    a bit faster too:
      $ read START < /sys/kernel/uevent_seqnum; \
      for i in `seq 100`; do su --shell=/bin/true bin; done; \
      read END < /sys/kernel/uevent_seqnum; \
      echo $(($END - $START))
      1
    
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c900aa530070..7531b1c28201 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -674,7 +674,7 @@ struct user_struct {
 	struct task_group *tg;
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;
-	struct work_struct work;
+	struct delayed_work work;
 #endif
 #endif
 

commit 19035e5b5d1e3127b4925d86f6a77964f91f2c3c
Merge: f9db6e095115 eea08f32adb3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 15 10:06:19 2009 -0700

    Merge branch 'timers-for-linus-migration' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus-migration' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      timers: Logic to move non pinned timers
      timers: /proc/sys sysctl hook to enable timer migration
      timers: Identifying the existing pinned timers
      timers: Framework for identifying pinned timers
      timers: allow deferrable timers for intervals tv2-tv5 to be deferred
    
    Fix up conflicts in kernel/sched.c and kernel/timer.c manually

commit ca94c442535a44d508c99a77e54f21a59f4fc462
Author: Lennart Poettering <lennart@poettering.net>
Date:   Mon Jun 15 17:17:47 2009 +0200

    sched: Introduce SCHED_RESET_ON_FORK scheduling policy flag
    
    This patch introduces a new flag SCHED_RESET_ON_FORK which can be passed
    to the kernel via sched_setscheduler(), ORed in the policy parameter. If
    set this will make sure that when the process forks a) the scheduling
    priority is reset to DEFAULT_PRIO if it was higher and b) the scheduling
    policy is reset to SCHED_NORMAL if it was either SCHED_FIFO or SCHED_RR.
    
    Why have this?
    
    Currently, if a process is real-time scheduled this will 'leak' to all
    its child processes. For security reasons it is often (always?) a good
    idea to make sure that if a process acquires RT scheduling this is
    confined to this process and only this process. More specifically this
    makes the per-process resource limit RLIMIT_RTTIME useful for security
    purposes, because it makes it impossible to use a fork bomb to
    circumvent the per-process RLIMIT_RTTIME accounting.
    
    This feature is also useful for tools like 'renice' which can then
    change the nice level of a process without having this spill to all its
    child processes.
    
    Why expose this via sched_setscheduler() and not other syscalls such as
    prctl() or sched_setparam()?
    
    prctl() does not take a pid parameter. Due to that it would be
    impossible to modify this flag for other processes than the current one.
    
    The struct passed to sched_setparam() can unfortunately not be extended
    without breaking compatibility, since sched_setparam() lacks a size
    parameter.
    
    How to use this from userspace? In your RT program simply replace this:
    
      sched_setscheduler(pid, SCHED_FIFO, &param);
    
    by this:
    
      sched_setscheduler(pid, SCHED_FIFO|SCHED_RESET_ON_FORK, &param);
    
    Signed-off-by: Lennart Poettering <lennart@poettering.net>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20090615152714.GA29092@tango.0pointer.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4896fdfec913..d4a2c6662f7d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -38,6 +38,8 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+/* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
+#define SCHED_RESET_ON_FORK     0x40000000
 
 #ifdef __KERNEL__
 
@@ -1209,6 +1211,10 @@ struct task_struct {
 	unsigned did_exec:1;
 	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
 				 * execve */
+
+	/* Revert to default priority/policy when forking */
+	unsigned sched_reset_on_fork:1;
+
 	pid_t pid;
 	pid_t tgid;
 

commit 9cbc1cb8cd46ce1f7645b9de249b2ce8460129bb
Merge: ca44d6e60f9d 45e3e1935e28
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 15 03:02:23 2009 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/torvalds/linux-2.6
    
    Conflicts:
            Documentation/feature-removal-schedule.txt
            drivers/scsi/fcoe/fcoe.c
            net/core/drop_monitor.c
            net/core/net-traces.c

commit 8a1ca8cedd108c8e76a6ab34079d0bbb4f244799
Merge: b640f042faa2 940010c5a314
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 14:01:07 2009 -0700

    Merge branch 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (574 commits)
      perf_counter: Turn off by default
      perf_counter: Add counter->id to the throttle event
      perf_counter: Better align code
      perf_counter: Rename L2 to LL cache
      perf_counter: Standardize event names
      perf_counter: Rename enums
      perf_counter tools: Clean up u64 usage
      perf_counter: Rename perf_counter_limit sysctl
      perf_counter: More paranoia settings
      perf_counter: powerpc: Implement generalized cache events for POWER processors
      perf_counters: powerpc: Add support for POWER7 processors
      perf_counter: Accurate period data
      perf_counter: Introduce struct for sample data
      perf_counter tools: Normalize data using per sample period data
      perf_counter: Annotate exit ctx recursion
      perf_counter tools: Propagate signals properly
      perf_counter tools: Small frequency related fixes
      perf_counter: More aggressive frequency adjustment
      perf_counter/x86: Fix the model number of Intel Core2 processors
      perf_counter, x86: Correct some event and umask values for Intel processors
      ...

commit 3296ca27f50ecbd71db1d808c7a72d311027f919
Merge: e893123c7378 73fbad283cfb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 10:01:41 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6: (44 commits)
      nommu: Provide mmap_min_addr definition.
      TOMOYO: Add description of lists and structures.
      TOMOYO: Remove unused field.
      integrity: ima audit dentry_open failure
      TOMOYO: Remove unused parameter.
      security: use mmap_min_addr indepedently of security models
      TOMOYO: Simplify policy reader.
      TOMOYO: Remove redundant markers.
      SELinux: define audit permissions for audit tree netlink messages
      TOMOYO: Remove unused mutex.
      tomoyo: avoid get+put of task_struct
      smack: Remove redundant initialization.
      integrity: nfsd imbalance bug fix
      rootplug: Remove redundant initialization.
      smack: do not beyond ARRAY_SIZE of data
      integrity: move ima_counts_get
      integrity: path_check update
      IMA: Add __init notation to ima functions
      IMA: Minimal IMA policy and boot param for TCB IMA policy
      selinux: remove obsolete read buffer limit from sel_read_bool
      ...

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit 20f3f3ca499d2c211771ba552685398b65d83859
Merge: 769f3e8c3847 41c51c98f588
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:50:03 2009 -0700

    Merge branch 'rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      rcu: rcu_sched_grace_period(): kill the bogus flush_signals()
      rculist: use list_entry_rcu in places where it's appropriate
      rculist.h: introduce list_entry_rcu() and list_first_entry_rcu()
      rcu: Update RCU tracing documentation for __rcu_pending
      rcu: Add __rcu_pending tracing to hierarchical RCU
      RCU: make treercu be default

commit 73fbad283cfbbcf02939bdbda31fc4a30e729cca
Merge: 769f3e8c3847 35f2c2f6f6ae
Author: James Morris <jmorris@namei.org>
Date:   Thu Jun 11 11:03:14 2009 +1000

    Merge branch 'next' into for-linus

commit 082ff5a2767a0679ee543f14883adbafb631ffbe
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat May 23 18:29:00 2009 +0200

    perf_counter: Change pctrl() behaviour
    
    Instead of en/dis-abling all counters acting on a particular
    task, en/dis- able all counters we created.
    
    [ v2: fix crash on first counter enable ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    LKML-Reference: <20090523163012.916937244@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9714d450f417..bc9326dcdde1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1389,6 +1389,8 @@ struct task_struct {
 #endif
 #ifdef CONFIG_PERF_COUNTERS
 	struct perf_counter_context *perf_counter_ctxp;
+	struct mutex perf_counter_mutex;
+	struct list_head perf_counter_list;
 #endif
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;

commit a63eaf34ae60bdb067a354cc8def2e8f4a01f5f4
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 22 14:17:31 2009 +1000

    perf_counter: Dynamically allocate tasks' perf_counter_context struct
    
    This replaces the struct perf_counter_context in the task_struct with
    a pointer to a dynamically allocated perf_counter_context struct.  The
    main reason for doing is this is to allow us to transfer a
    perf_counter_context from one task to another when we do lazy PMU
    switching in a later patch.
    
    This has a few side-benefits: the task_struct becomes a little smaller,
    we save some memory because only tasks that have perf_counters attached
    get a perf_counter_context allocated for them, and we can remove the
    inclusion of <linux/perf_counter.h> in sched.h, meaning that we don't
    end up recompiling nearly everything whenever perf_counter.h changes.
    
    The perf_counter_context structures are reference-counted and freed
    when the last reference is dropped.  A context can have references
    from its task and the counters on its task.  Counters can outlive the
    task so it is possible that a context will be freed well after its
    task has exited.
    
    Contexts are allocated on fork if the parent had a context, or
    otherwise the first time that a per-task counter is created on a task.
    In the latter case, we set the context pointer in the task struct
    locklessly using an atomic compare-and-exchange operation in case we
    raced with some other task in creating a context for the subject task.
    
    This also removes the task pointer from the perf_counter struct.  The
    task pointer was not used anywhere and would make it harder to move a
    context from one task to another.  Anything that needed to know which
    task a counter was attached to was already using counter->ctx->task.
    
    The __perf_counter_init_context function moves up in perf_counter.c
    so that it can be called from find_get_context, and now initializes
    the refcount, but is otherwise unchanged.
    
    We were potentially calling list_del_counter twice: once from
    __perf_counter_exit_task when the task exits and once from
    __perf_counter_remove_from_context when the counter's fd gets closed.
    This adds a check in list_del_counter so it doesn't do anything if
    the counter has already been removed from the lists.
    
    Since perf_counter_task_sched_in doesn't do anything if the task doesn't
    have a context, and leaves cpuctx->task_ctx = NULL, this adds code to
    __perf_install_in_context to set cpuctx->task_ctx if necessary, i.e. in
    the case where the current task adds the first counter to itself and
    thus creates a context for itself.
    
    This also adds similar code to __perf_counter_enable to handle a
    similar situation which can arise when the counters have been disabled
    using prctl; that also leaves cpuctx->task_ctx = NULL.
    
    [ Impact: refactor counter context management to prepare for new feature ]
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <18966.10075.781053.231153@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff59d1231519..9714d450f417 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -71,7 +71,6 @@ struct sched_param {
 #include <linux/path.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
-#include <linux/perf_counter.h>
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>
@@ -99,6 +98,7 @@ struct robust_list_head;
 struct bio;
 struct bts_tracer;
 struct fs_struct;
+struct perf_counter_context;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1387,7 +1387,9 @@ struct task_struct {
 	struct list_head pi_state_list;
 	struct futex_pi_state *pi_state_cache;
 #endif
-	struct perf_counter_context perf_counter_ctx;
+#ifdef CONFIG_PERF_COUNTERS
+	struct perf_counter_context *perf_counter_ctxp;
+#endif
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;
 	short il_next;

commit 4200efd9acda4accf24640f1e77d24fdcdb524df
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue May 19 09:22:19 2009 +0200

    sched: properly define the sched_group::cpumask and sched_domain::span fields
    
    Properly document the variable-size structure tricks we are doing
    wrt. struct sched_group and sched_domain, and use the field[0] GCC
    extension instead of defining a vla array.
    
    Dont use unions for this, as pointed out by Linus.
    
    [ Impact: cleanup, un-confuse Sparse and LLVM ]
    
    Reported-by: Jeff Garzik <jeff@garzik.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <alpine.LFD.2.01.0905180850110.3301@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index de7b3b217772..dbb1043e8656 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -839,7 +839,17 @@ struct sched_group {
 	 */
 	u32 reciprocal_cpu_power;
 
-	unsigned long cpumask[];
+	/*
+	 * The CPUs this group covers.
+	 *
+	 * NOTE: this field is variable length. (Allocated dynamically
+	 * by attaching extra space to the end of the structure,
+	 * depending on how many CPUs the kernel has booted up with)
+	 *
+	 * It is also be embedded into static data structures at build
+	 * time. (See 'struct static_sched_group' in kernel/sched.c)
+	 */
+	unsigned long cpumask[0];
 };
 
 static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
@@ -925,8 +935,17 @@ struct sched_domain {
 	char *name;
 #endif
 
-	/* span of all CPUs in this domain */
-	unsigned long span[];
+	/*
+	 * Span of all CPUs in this domain.
+	 *
+	 * NOTE: this field is variable length. (Allocated dynamically
+	 * by attaching extra space to the end of the structure,
+	 * depending on how many CPUs the kernel has booted up with)
+	 *
+	 * It is also be embedded into static data structures at build
+	 * time. (See 'struct static_sched_domain' in kernel/sched.c)
+	 */
+	unsigned long span[0];
 };
 
 static inline struct cpumask *sched_domain_span(struct sched_domain *sd)

commit 690cc3ffe33ac4a2857583c22d4c6244ae11684d
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed May 13 16:55:10 2009 +0000

    syscall: Implement a convinience function restart_syscall
    
    Currently when we have a signal pending we have the functionality
    to restart that the current system call.  There are other cases
    such as nasty lock ordering issues where it makes sense to have
    a simple fix that uses try lock and restarts the system call.
    Buying time to figure out how to rework the locking strategy.
    
    Signed-off-by: Eric W. Biederman <ebiederm@aristanetworks.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4c38bc8049c..d853f6bb0baf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2178,6 +2178,12 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
+static inline int restart_syscall(void)
+{
+	set_tsk_thread_flag(current, TIF_SIGPENDING);
+	return -ERESTARTNOINTR;
+}
+
 static inline int signal_pending(struct task_struct *p)
 {
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));

commit 2d02494f5a90f2e4b3c4c6acc85ec94674cdc431
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 2 20:08:52 2009 +0200

    sched, timers: cleanup avenrun users
    
    avenrun is an rough estimate so we don't have to worry about
    consistency of the three avenrun values. Remove the xtime lock
    dependency and provide a function to scale the values. Cleanup the
    users.
    
    [ Impact: cleanup ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6eb4892efe45..de7b3b217772 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -116,6 +116,7 @@ struct fs_struct;
  *    11 bit fractions.
  */
 extern unsigned long avenrun[];		/* Load averages */
+extern void get_avenrun(unsigned long *loads, unsigned long offset, int shift);
 
 #define FSHIFT		11		/* nr of bits of precision */
 #define FIXED_1		(1<<FSHIFT)	/* 1.0 as fixed-point */

commit dce48a84adf1806676319f6f480e30a6daa012f9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Apr 11 10:43:41 2009 +0200

    sched, timers: move calc_load() to scheduler
    
    Dimitri Sivanich noticed that xtime_lock is held write locked across
    calc_load() which iterates over all online CPUs. That can cause long
    latencies for xtime_lock readers on large SMP systems.
    
    The load average calculation is an rough estimate anyway so there is
    no real need to protect the readers vs. the update. It's not a problem
    when the avenrun array is updated while a reader copies the values.
    
    Instead of iterating over all online CPUs let the scheduler_tick code
    update the number of active tasks shortly before the avenrun update
    happens. The avenrun update itself is handled by the CPU which calls
    do_timer().
    
    [ Impact: reduce xtime_lock write locked section ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4c38bc8049c..6eb4892efe45 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -135,8 +135,8 @@ DECLARE_PER_CPU(unsigned long, process_counts);
 extern int nr_processes(void);
 extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
-extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
+extern void calc_global_load(void);
 
 extern unsigned long get_parent_ip(unsigned long addr);
 

commit 789f90fcf6b0b54e655740e9396c954378542c79
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 15 15:19:27 2009 +0200

    perf_counter: per user mlock gift
    
    Instead of a per-process mlock gift for perf-counters, use a
    per-user gift so that there is less of a DoS potential.
    
    [ Impact: allow less worst-case unprivileged memory consumption ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <20090515132018.496182835@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d1857580a132..ff59d1231519 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -674,6 +674,10 @@ struct user_struct {
 	struct work_struct work;
 #endif
 #endif
+
+#ifdef CONFIG_PERF_COUNTERS
+	atomic_long_t locked_vm;
+#endif
 };
 
 extern int uids_sysfs_init(void);

commit eea08f32adb3f97553d49a4f79a119833036000a
Author: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
Date:   Thu Apr 16 12:16:41 2009 +0530

    timers: Logic to move non pinned timers
    
    * Arun R Bharadwaj <arun@linux.vnet.ibm.com> [2009-04-16 12:11:36]:
    
    This patch migrates all non pinned timers and hrtimers to the current
    idle load balancer, from all the idle CPUs. Timers firing on busy CPUs
    are not migrated.
    
    While migrating hrtimers, care should be taken to check if migrating
    a hrtimer would result in a latency or not. So we compare the expiry of the
    hrtimer with the next timer interrupt on the target cpu and migrate the
    hrtimer only if it expires *after* the next interrupt on the target cpu.
    So, added a clockevents_get_next_event() helper function to return the
    next_event on the target cpu's clock_event_device.
    
    [ tglx: cleanups and simplifications ]
    
    Signed-off-by: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 618504010400..311dec123974 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -257,6 +257,7 @@ extern void task_rq_unlock_wait(struct task_struct *p);
 extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern int select_nohz_load_balancer(int cpu);
+extern int get_nohz_load_balancer(void);
 #else
 static inline int select_nohz_load_balancer(int cpu)
 {
@@ -1772,6 +1773,17 @@ int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,
 		loff_t *ppos);
 #endif
+#ifdef CONFIG_SCHED_DEBUG
+static inline unsigned int get_sysctl_timer_migration(void)
+{
+	return sysctl_timer_migration;
+}
+#else
+static inline unsigned int get_sysctl_timer_migration(void)
+{
+	return 1;
+}
+#endif
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 

commit cd1bb94b4a0531e8211a3774f17de831f8285f76
Author: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
Date:   Thu Apr 16 12:15:34 2009 +0530

    timers: /proc/sys sysctl hook to enable timer migration
    
    * Arun R Bharadwaj <arun@linux.vnet.ibm.com> [2009-04-16 12:11:36]:
    
    This patch creates the /proc/sys sysctl interface at
    /proc/sys/kernel/timer_migration
    
    Timer migration is enabled by default.
    
    To disable timer migration, when CONFIG_SCHED_DEBUG = y,
    
    echo 0 > /proc/sys/kernel/timer_migration
    
    Signed-off-by: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4c38bc8049c..618504010400 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1766,6 +1766,7 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
+extern unsigned int sysctl_timer_migration;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit 5e751e992f3fb08ba35e1ca8095ec8fbf9eda523
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 8 13:55:22 2009 +0100

    CRED: Rename cred_exec_mutex to reflect that it's a guard against ptrace
    
    Rename cred_exec_mutex to reflect that it's a guard against foreign
    intervention on a process's credential state, such as is made by ptrace().  The
    attachment of a debugger to a process affects execve()'s calculation of the new
    credential state - _and_ also setprocattr()'s calculation of that state.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3fa82b353c98..5932ace22400 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1247,7 +1247,9 @@ struct task_struct {
 					 * credentials (COW) */
 	const struct cred *cred;	/* effective (overridable) subjective task
 					 * credentials (COW) */
-	struct mutex cred_exec_mutex;	/* execve vs ptrace cred calculation mutex */
+	struct mutex cred_guard_mutex;	/* guard against foreign influences on
+					 * credential calculations
+					 * (notably. ptrace) */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock

commit d254117099d711f215e62427f55dfb8ebd5ad011
Merge: 07ff7a0b187f 8c9ed899b44c
Author: James Morris <jmorris@namei.org>
Date:   Fri May 8 17:56:47 2009 +1000

    Merge branch 'master' into next

commit 0ad5d703c6c0fcd385d956555460df95dff7eb7e
Merge: 44347d947f62 1cb81b143fa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:18:34 2009 +0200

    Merge branch 'tracing/hw-branch-tracing' into tracing/core
    
    Merge reason: this topic is ready for upstream now. It passed
                  Oleg's review and Andrew had no further mm/*
                  objections/observations either.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3bcac0263f0b45e67a64034ebcb69eb9abb742f4
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 29 13:45:05 2009 +0100

    SELinux: Don't flush inherited SIGKILL during execve()
    
    Don't flush inherited SIGKILL during execve() in SELinux's post cred commit
    hook.  This isn't really a security problem: if the SIGKILL came before the
    credentials were changed, then we were right to receive it at the time, and
    should honour it; if it came after the creds were changed, then we definitely
    should honour it; and in any case, all that will happen is that the process
    will be scrapped before it ever returns to userspace.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1d19c025f9d2..d3b787c7aef3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1875,6 +1875,7 @@ extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);
 extern void flush_signals(struct task_struct *);
+extern void __flush_signals(struct task_struct *);
 extern void ignore_signals(struct task_struct *);
 extern void flush_signal_handlers(struct task_struct *, int force_default);
 extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);

commit e7fd5d4b3d240f42c30a9e3d20a4689c4d3a795a
Merge: 1130b0296184 56a50adda49b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 29 14:46:59 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Merge reason: This brach was on -rc1, refresh it to almost-rc4 to pick up
                  the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 416dfdcdb894432547ead4fcb9fa6a36b396059e
Merge: 56449f437add 091069740304
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 24 10:11:18 2009 +0200

    Merge commit 'v2.6.30-rc3' into tracing/hw-branch-tracing
    
    Conflicts:
            arch/x86/kernel/ptrace.c
    
    Merge reason: fix the conflict above, and also pick up the CONFIG_BROKEN
                  dependency change from upstream so that we can remove it
                  here.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 261842b7c9099f56de2eb969c8ad65402d68e00e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 16 21:41:52 2009 -0400

    tracing: add same level recursion detection
    
    The tracing infrastructure allows for recursion. That is, an interrupt
    may interrupt the act of tracing an event, and that interrupt may very well
    perform its own trace. This is a recursive trace, and is fine to do.
    
    The problem arises when there is a bug, and the utility doing the trace
    calls something that recurses back into the tracer. This recursion is not
    caused by an external event like an interrupt, but by code that is not
    expected to recurse. The result could be a lockup.
    
    This patch adds a bitmask to the task structure that keeps track
    of the trace recursion. To find the interrupt depth, the following
    algorithm is used:
    
      level = hardirq_count() + softirq_count() + in_nmi;
    
    Here, level will be the depth of interrutps and softirqs, and even handles
    the nmi. Then the corresponding bit is set in the recursion bitmask.
    If the bit was already set, we know we had a recursion at the same level
    and we warn about it and fail the writing to the buffer.
    
    After the data has been committed to the buffer, we clear the bit.
    No atomics are needed. The only races are with interrupts and they reset
    the bitmask before returning anywy.
    
    [ Impact: detect same irq level trace recursion ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4c38bc8049c..7ede5e490913 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1428,7 +1428,9 @@ struct task_struct {
 #ifdef CONFIG_TRACING
 	/* state flags for use by tracers */
 	unsigned long trace;
-#endif
+	/* bitmask of trace recursion */
+	unsigned long trace_recursion;
+#endif /* CONFIG_TRACING */
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */

commit 05725f7eb4b8acb147c5fc7b91397b1f6bcab00d
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Tue Apr 14 20:17:16 2009 +0200

    rculist: use list_entry_rcu in places where it's appropriate
    
    Use previously introduced list_entry_rcu instead of an open-coded
    list_entry + rcu_dereference combination.
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: dipankar@in.ibm.com
    LKML-Reference: <20090414181715.GA3634@psychotron.englab.brq.redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4c38bc8049c..886df41e7452 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -77,6 +77,7 @@ struct sched_param {
 #include <linux/proportions.h>
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
+#include <linux/rculist.h>
 #include <linux/rtmutex.h>
 
 #include <linux/time.h>
@@ -2010,7 +2011,8 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 }
 #endif
 
-#define next_task(p)	list_entry(rcu_dereference((p)->tasks.next), struct task_struct, tasks)
+#define next_task(p) \
+	list_entry_rcu((p)->tasks.next, struct task_struct, tasks)
 
 #define for_each_process(p) \
 	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
@@ -2049,8 +2051,8 @@ int same_thread_group(struct task_struct *p1, struct task_struct *p2)
 
 static inline struct task_struct *next_thread(const struct task_struct *p)
 {
-	return list_entry(rcu_dereference(p->thread_group.next),
-			  struct task_struct, thread_group);
+	return list_entry_rcu(p->thread_group.next,
+			      struct task_struct, thread_group);
 }
 
 static inline int thread_group_empty(struct task_struct *p)

commit e3c8ca8336707062f3f7cb1cd7e6b3c753baccdd
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed Apr 8 19:45:12 2009 -0500

    sched: do not count frozen tasks toward load
    
    Freezing tasks via the cgroup freezer causes the load average to climb
    because the freezer's current implementation puts frozen tasks in
    uninterruptible sleep (D state).
    
    Some applications which perform job-scheduling functions consult the
    load average when making decisions.  If a cgroup is frozen, the load
    average does not provide a useful measure of the system's utilization
    to such applications.  This is especially inconvenient if the job
    scheduler employs the cgroup freezer as a mechanism for preempting low
    priority jobs.  Contrast this with using SIGSTOP for the same purpose:
    the stopped tasks do not count toward system load.
    
    Change task_contributes_to_load() to return false if the task is
    frozen.  This results in /proc/loadavg behavior that better meets
    users' expectations.
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Nigel Cunningham <nigel@tuxonice.net>
    Tested-by: Nigel Cunningham <nigel@tuxonice.net>
    Cc: <stable@kernel.org>
    Cc: containers@lists.linux-foundation.org
    Cc: linux-pm@lists.linux-foundation.org
    Cc: Matt Helsley <matthltc@us.ibm.com>
    LKML-Reference: <20090408194512.47a99b95@manatee.lan>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 98e1fe51601d..b4c38bc8049c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -205,7 +205,8 @@ extern unsigned long long time_sync_thresh;
 #define task_is_stopped_or_traced(task)	\
 			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
-				((task->state & TASK_UNINTERRUPTIBLE) != 0)
+				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
+				 (task->flags & PF_FROZEN) == 0)
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit 5ea472a77f8e4811ceee3f44a9deda6ad6e8b789
Merge: 6c009ecef8cc 577c9c456f0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 8 10:35:30 2009 +0200

    Merge commit 'v2.6.30-rc1' into perfcounters/core
    
    Conflicts:
            arch/powerpc/include/asm/systbl.h
            arch/powerpc/include/asm/unistd.h
            include/linux/init_task.h
    
    Merge reason: the conflicts are non-trivial: PowerPC placement
                  of sys_perf_counter_open has to be mixed with the
                  new preadv/pwrite syscalls.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 1551260d1f0fb1d23f264582092b862fce5e2dbd
Merge: c93f216b5b98 5e34437840d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 14:11:07 2009 -0700

    Merge branch 'core/softlockup' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core/softlockup' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softlockup: make DETECT_HUNG_TASK default depend on DETECT_SOFTLOCKUP
      softlockup: move 'one' to the softlockup section in sysctl.c
      softlockup: ensure the task has been switched out once
      softlockup: remove timestamp checking from hung_task
      softlockup: convert read_lock in hung_task to rcu_read_lock
      softlockup: check all tasks in hung_task
      softlockup: remove unused definition for spawn_softlockup_task
      softlockup: fix potential race in hung_task when resetting timeout
      softlockup: fix to allow compiling with !DETECT_HUNG_TASK
      softlockup: decouple hung tasks check from softlockup detection

commit 0f4814065ff8c24ca8bfd75c9b73502be152c287
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:48 2009 +0200

    x86, ptrace: add bts context unconditionally
    
    Add the ptrace bts context field to task_struct unconditionally.
    
    Initialize the field directly in copy_process().
    Remove all the unneeded functionality used to initialize that field.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144603.292754000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 52b8cd049c2e..451186a22ef5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1205,13 +1205,11 @@ struct task_struct {
 	struct list_head ptraced;
 	struct list_head ptrace_entry;
 
-#ifdef CONFIG_X86_PTRACE_BTS
 	/*
 	 * This is the tracer handle for the ptrace BTS extension.
 	 * This field actually belongs to the ptracer task.
 	 */
 	struct bts_context *bts;
-#endif /* CONFIG_X86_PTRACE_BTS */
 
 	/* PID/PID hash table linkage. */
 	struct pid_link pids[PIDTYPE_MAX];

commit e2b371f00a6f529f6362654239bdec8dcd510760
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:35 2009 +0200

    mm, x86, ptrace, bts: defer branch trace stopping
    
    When a ptraced task is unlinked, we need to stop branch tracing for
    that task.
    
    Since the unlink is called with interrupts disabled, and we need
    interrupts enabled to stop branch tracing, we defer the work.
    
    Collect all branch tracing related stuff in a branch tracing context.
    
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144550.712401000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a5b9a83065fa..52b8cd049c2e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -96,8 +96,8 @@ struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
 struct bio;
-struct bts_tracer;
 struct fs_struct;
+struct bts_context;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1210,12 +1210,7 @@ struct task_struct {
 	 * This is the tracer handle for the ptrace BTS extension.
 	 * This field actually belongs to the ptracer task.
 	 */
-	struct bts_tracer *bts;
-	/*
-	 * The buffer to hold the BTS data.
-	 */
-	void *bts_buffer;
-	size_t bts_size;
+	struct bts_context *bts;
 #endif /* CONFIG_X86_PTRACE_BTS */
 
 	/* PID/PID hash table linkage. */

commit a26b89f05d194413c7238e0bea071054f6b5d3c8
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:34 2009 +0200

    sched, hw-branch-tracer: add wait_task_context_switch() function to sched.h
    
    Add a function to wait until some other task has been
    switched out at least once.
    
    This differs from wait_task_inactive() subtly, in that the
    latter will wait until the task has left the CPU.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: markus.t.metzger@gmail.com
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144549.794157000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b94f3541f67b..a5b9a83065fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1993,8 +1993,10 @@ extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
+extern void wait_task_context_switch(struct task_struct *p);
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
+static inline void wait_task_context_switch(struct task_struct *p) {}
 static inline unsigned long wait_task_inactive(struct task_struct *p,
 					       long match_state)
 {

commit 5e34437840d33554f69380584311743b39e8fbeb
Merge: 77d05632baee d508afb437da
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 11:15:40 2009 +0200

    Merge branch 'linus' into core/softlockup
    
    Conflicts:
            kernel/sysctl.c

commit 4a0deca657f3dbb8a707b5dc8f173beec01e7ed2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 19 20:26:12 2009 +0100

    perf_counter: generic context switch event
    
    Impact: cleanup
    
    Use the generic software events for context switches.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Orig-LKML-Reference: <20090319194233.283522645@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75b2fc5306d8..7ed41f7c5ace 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -138,7 +138,6 @@ extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
-extern u64 cpu_nr_switches(int cpu);
 extern u64 cpu_nr_migrations(int cpu);
 
 extern unsigned long get_parent_ip(unsigned long addr);

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9efe21cb82b5dbe3b0b2ae4de4eccc64ecb94e95
Merge: de18836e447c 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 01:41:22 2009 +0200

    Merge branch 'linus' into irq/threaded
    
    Conflicts:
            include/linux/irq.h
            kernel/irq/handle.c

commit 714f83d5d9f7c785f622259dad1f4fad12d64664
Merge: 8901e7ffc2fa 645dae969c3b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 5 11:04:19 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (413 commits)
      tracing, net: fix net tree and tracing tree merge interaction
      tracing, powerpc: fix powerpc tree and tracing tree interaction
      ring-buffer: do not remove reader page from list on ring buffer free
      function-graph: allow unregistering twice
      trace: make argument 'mem' of trace_seq_putmem() const
      tracing: add missing 'extern' keywords to trace_output.h
      tracing: provide trace_seq_reserve()
      blktrace: print out BLK_TN_MESSAGE properly
      blktrace: extract duplidate code
      blktrace: fix memory leak when freeing struct blk_io_trace
      blktrace: fix blk_probes_ref chaos
      blktrace: make classic output more classic
      blktrace: fix off-by-one bug
      blktrace: fix the original blktrace
      blktrace: fix a race when creating blk_tree_root in debugfs
      blktrace: fix timestamp in binary output
      tracing, Text Edit Lock: cleanup
      tracing: filter fix for TRACE_EVENT_FORMAT events
      ftrace: Using FTRACE_WARN_ON() to check "freed record" in ftrace_release()
      x86: kretprobe-booster interrupt emulation code fix
      ...
    
    Fix up trivial conflicts in
     arch/parisc/include/asm/ftrace.h
     include/linux/memory.h
     kernel/extable.c
     kernel/module.c

commit 8fe74cf053de7ad2124a894996f84fa890a81093
Merge: c2eb2fa6d2b6 ced117c73edc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 21:09:10 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6:
      Remove two unneeded exports and make two symbols static in fs/mpage.c
      Cleanup after commit 585d3bc06f4ca57f975a5a1f698f65a45ea66225
      Trim includes of fdtable.h
      Don't crap into descriptor table in binfmt_som
      Trim includes in binfmt_elf
      Don't mess with descriptor table in load_elf_binary()
      Get rid of indirect include of fs_struct.h
      New helper - current_umask()
      check_unsafe_exec() doesn't care about signal handlers sharing
      New locking/refcounting for fs_struct
      Take fs_struct handling to new file (fs/fs_struct.c)
      Get rid of bumping fs_struct refcount in pivot_root(2)
      Kill unsharing fs_struct in __set_personality()

commit 1b0f7ffd0ea27cd3a0b9ca04e3df9522048c32a3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:39 2009 -0700

    pids: kill signal_struct-> __pgrp/__session and friends
    
    We are wasting 2 words in signal_struct without any reason to implement
    task_pgrp_nr() and task_session_nr().
    
    task_session_nr() has no callers since
    2e2ba22ea4fd4bb85f0fa37c521066db6775cbef, we can remove it.
    
    task_pgrp_nr() is still (I believe wrongly) used in fs/autofsX and
    fs/coda.
    
    This patch reimplements task_pgrp_nr() via task_pgrp_nr_ns(), and kills
    __pgrp/__session and the related helpers.
    
    The change in drivers/char/tty_io.c is cosmetic, but hopefully makes sense
    anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Alan Cox <number6@the-village.bc.nu>          [tty parts]
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 49df878a0cad..206ac003e8c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -547,25 +547,8 @@ struct signal_struct {
 
 	struct list_head cpu_timers[3];
 
-	/* job control IDs */
-
-	/*
-	 * pgrp and session fields are deprecated.
-	 * use the task_session_Xnr and task_pgrp_Xnr routines below
-	 */
-
-	union {
-		pid_t pgrp __deprecated;
-		pid_t __pgrp;
-	};
-
 	struct pid *tty_old_pgrp;
 
-	union {
-		pid_t session __deprecated;
-		pid_t __session;
-	};
-
 	/* boolean value for session group leader */
 	int leader;
 
@@ -1469,16 +1452,6 @@ static inline int rt_task(struct task_struct *p)
 	return rt_prio(p->prio);
 }
 
-static inline void set_task_session(struct task_struct *tsk, pid_t session)
-{
-	tsk->signal->__session = session;
-}
-
-static inline void set_task_pgrp(struct task_struct *tsk, pid_t pgrp)
-{
-	tsk->signal->__pgrp = pgrp;
-}
-
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -1552,11 +1525,6 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_pgrp_nr(struct task_struct *tsk)
-{
-	return tsk->signal->__pgrp;
-}
-
 static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
 					struct pid_namespace *ns)
 {
@@ -1569,11 +1537,6 @@ static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_session_nr(struct task_struct *tsk)
-{
-	return tsk->signal->__session;
-}
-
 static inline pid_t task_session_nr_ns(struct task_struct *tsk,
 					struct pid_namespace *ns)
 {
@@ -1585,6 +1548,12 @@ static inline pid_t task_session_vnr(struct task_struct *tsk)
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);
 }
 
+/* obsolete, do not use */
+static inline pid_t task_pgrp_nr(struct task_struct *tsk)
+{
+	return task_pgrp_nr_ns(tsk, &init_pid_ns);
+}
+
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.

commit 52ee2dfdd4f51cf422ea6a96a0846dc94244aa37
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:38 2009 -0700

    pids: refactor vnr/nr_ns helpers to make them safe
    
    Inho, the safety rules for vnr/nr_ns helpers are horrible and buggy.
    
    task_pid_nr_ns(task) needs rcu/tasklist depending on task == current.
    
    As for "special" pids, vnr/nr_ns helpers always need rcu.  However, if
    task != current, they are unsafe even under rcu lock, we can't trust
    task->group_leader without the special checks.
    
    And almost every helper has a callsite which needs a fix.
    
    Also, it is a bit annoying that the implementations of, say,
    task_pgrp_vnr() and task_pgrp_nr_ns() are not "symmetrical".
    
    This patch introduces the new helper, __task_pid_nr_ns(), which is always
    safe to use, and turns all other helpers into the trivial wrappers.
    
    After this I'll send another patch which converts task_tgid_xxx() as well,
    they're are a bit special.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Louis Rilling <Louis.Rilling@kerlabs.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 722dd313bf8a..49df878a0cad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1519,17 +1519,23 @@ struct pid_namespace;
  *
  * see also pid_nr() etc in include/linux/pid.h
  */
+pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
+			struct pid_namespace *ns);
 
 static inline pid_t task_pid_nr(struct task_struct *tsk)
 {
 	return tsk->pid;
 }
 
-pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
+static inline pid_t task_pid_nr_ns(struct task_struct *tsk,
+					struct pid_namespace *ns)
+{
+	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);
+}
 
 static inline pid_t task_pid_vnr(struct task_struct *tsk)
 {
-	return pid_vnr(task_pid(tsk));
+	return __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);
 }
 
 
@@ -1551,11 +1557,15 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 	return tsk->signal->__pgrp;
 }
 
-pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
+static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
+					struct pid_namespace *ns)
+{
+	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
+}
 
 static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 {
-	return pid_vnr(task_pgrp(tsk));
+	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);
 }
 
 
@@ -1564,14 +1574,17 @@ static inline pid_t task_session_nr(struct task_struct *tsk)
 	return tsk->signal->__session;
 }
 
-pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
+static inline pid_t task_session_nr_ns(struct task_struct *tsk,
+					struct pid_namespace *ns)
+{
+	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);
+}
 
 static inline pid_t task_session_vnr(struct task_struct *tsk)
 {
-	return pid_vnr(task_session(tsk));
+	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);
 }
 
-
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.

commit 6dda81f4384b94930826eded254d8c16f89a9248
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:35 2009 -0700

    pids: document task_pgrp/task_session is not safe without tasklist/rcu
    
    Even if task == current, it is not safe to dereference the result of
    task_pgrp/task_session.  We can race with another thread which changes the
    special pid via setpgid/setsid.
    
    Document this.  The next 2 patches give an example of the unsafe usage, we
    have more bad users.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Louis Rilling <Louis.Rilling@kerlabs.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b47c94e7560b..722dd313bf8a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1489,6 +1489,11 @@ static inline struct pid *task_tgid(struct task_struct *task)
 	return task->group_leader->pids[PIDTYPE_PID].pid;
 }
 
+/*
+ * Without tasklist or rcu lock it is not safe to dereference
+ * the result of task_pgrp/task_session even if task == current,
+ * we can race with another thread doing sys_setsid/sys_setpgid.
+ */
 static inline struct pid *task_pgrp(struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_PGID].pid;

commit 39c626ae47c469abdfd30c6e42eff884931380d6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:18 2009 -0700

    forget_original_parent: split out the un-ptrace part
    
    By discussion with Roland.
    
    - Rename ptrace_exit() to exit_ptrace(), and change it to do all the
      necessary work with ->ptraced list by its own.
    
    - Move this code from exit.c to ptrace.c
    
    - Update the comment in ptrace_detach() to explain the rechecking of
      the child->ptrace.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Metzger, Markus T" <markus.t.metzger@intel.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9186f8c5d5f2..b47c94e7560b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2061,6 +2061,11 @@ static inline int thread_group_empty(struct task_struct *p)
 #define delay_group_leader(p) \
 		(thread_group_leader(p) && !thread_group_empty(p))
 
+static inline int task_detached(struct task_struct *p)
+{
+	return p->exit_signal == -1;
+}
+
 /*
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also

commit 6f2c55b843836d26528c56a0968689accaedbc67
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 2 16:56:59 2009 -0700

    Simplify copy_thread()
    
    First argument unused since 2.3.11.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 481fad3a9b42..9186f8c5d5f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1975,7 +1975,8 @@ extern void mm_release(struct task_struct *, struct mm_struct *);
 /* Allocate a new mm structure and copy contents from tsk->mm */
 extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
-extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, struct task_struct *, struct pt_regs *);
+extern int copy_thread(unsigned long, unsigned long, unsigned long,
+			struct task_struct *, struct pt_regs *);
 extern void flush_thread(void);
 extern void exit_thread(void);
 

commit 8302294f43250dc337108c51882a6007f2b1e2e0
Merge: 4fe70410d9a2 2e572895bf32
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 1 21:54:19 2009 +0200

    Merge branch 'tracing/core-v2' into tracing-for-linus
    
    Conflicts:
            include/linux/slub_def.h
            lib/Kconfig.debug
            mm/slob.c
            mm/slub.c

commit 9de1581e75ba9d7979766d4ab6d56f0f2d87f7c6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Mar 31 15:19:29 2009 -0700

    get_mm_hiwater_xxx: trivial, s/define/inline/
    
    Andrew pointed out get_mm_hiwater_xxx() evaluate "mm" argument thrice/twice,
    make them inline.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 29df6374d2de..481fad3a9b42 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -391,8 +391,15 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 		(mm)->hiwater_vm = (mm)->total_vm;	\
 } while (0)
 
-#define get_mm_hiwater_rss(mm)	max((mm)->hiwater_rss, get_mm_rss(mm))
-#define get_mm_hiwater_vm(mm)	max((mm)->hiwater_vm, (mm)->total_vm)
+static inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)
+{
+	return max(mm->hiwater_rss, get_mm_rss(mm));
+}
+
+static inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)
+{
+	return max(mm->hiwater_vm, mm->total_vm);
+}
 
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);

commit 5ad4e53bd5406ee214ddc5a41f03f779b8b2d526
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 29 19:50:06 2009 -0400

    Get rid of indirect include of fs_struct.h
    
    Don't pull it in sched.h; very few files actually need it and those
    can include directly.  sched.h itself only needs forward declaration
    of struct fs_struct;
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 29df6374d2de..b4e065ea0de1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -68,7 +68,7 @@ struct sched_param {
 #include <linux/smp.h>
 #include <linux/sem.h>
 #include <linux/signal.h>
-#include <linux/fs_struct.h>
+#include <linux/path.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
 #include <linux/pid.h>
@@ -97,6 +97,7 @@ struct futex_pi_state;
 struct robust_list_head;
 struct bio;
 struct bts_tracer;
+struct fs_struct;
 
 /*
  * List of flags we want to share for kernel threads,

commit c4e1aa67ed9e4e542a064bc271ddbf152b677e91
Merge: cf2f7d7c9027 2f8501815256
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 17:17:35 2009 -0700

    Merge branch 'locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (33 commits)
      lockdep: fix deadlock in lockdep_trace_alloc
      lockdep: annotate reclaim context (__GFP_NOFS), fix SLOB
      lockdep: annotate reclaim context (__GFP_NOFS), fix
      lockdep: build fix for !PROVE_LOCKING
      lockstat: warn about disabled lock debugging
      lockdep: use stringify.h
      lockdep: simplify check_prev_add_irq()
      lockdep: get_user_chars() redo
      lockdep: simplify get_user_chars()
      lockdep: add comments to mark_lock_irq()
      lockdep: remove macro usage from mark_held_locks()
      lockdep: fully reduce mark_lock_irq()
      lockdep: merge the !_READ mark_lock_irq() helpers
      lockdep: merge the _READ mark_lock_irq() helpers
      lockdep: simplify mark_lock_irq() helpers #3
      lockdep: further simplify mark_lock_irq() helpers
      lockdep: simplify the mark_lock_irq() helpers
      lockdep: split up mark_lock_irq()
      lockdep: generate usage strings
      lockdep: generate the state bit definitions
      ...

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 831576fe40f4175e0767623cffa4aeb28157943a
Merge: 21cdbc1378e8 66fef08f7d52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 26 16:05:01 2009 -0700

    Merge branch 'sched-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (46 commits)
      sched: Add comments to find_busiest_group() function
      sched: Refactor the power savings balance code
      sched: Optimize the !power_savings_balance during fbg()
      sched: Create a helper function to calculate imbalance
      sched: Create helper to calculate small_imbalance in fbg()
      sched: Create a helper function to calculate sched_domain stats for fbg()
      sched: Define structure to store the sched_domain statistics for fbg()
      sched: Create a helper function to calculate sched_group stats for fbg()
      sched: Define structure to store the sched_group statistics for fbg()
      sched: Fix indentations in find_busiest_group() using gotos
      sched: Simple helper functions for find_busiest_group()
      sched: remove unused fields from struct rq
      sched: jiffies not printed per CPU
      sched: small optimisation of can_migrate_task()
      sched: fix typos in documentation
      sched: add avg_overlap decay
      x86, sched_clock(): mark variables read-mostly
      sched: optimize ttwu vs group scheduling
      sched: TIF_NEED_RESCHED -> need_reshed() cleanup
      sched: don't rebalance if attached on NULL domain
      ...

commit 8aef2d2856158a36c295a8d1288281e4839bff13
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 24 01:10:15 2009 -0400

    function-graph: ignore times across schedule
    
    Impact: more accurate timings
    
    The current method of function graph tracing does not take into
    account the time spent when a task is not running. This shows functions
    that call schedule have increased costs:
    
     3) + 18.664 us   |      }
     ------------------------------------------
     3)    <idle>-0    =>  kblockd-123
     ------------------------------------------
    
     3)               |      finish_task_switch() {
     3)   1.441 us    |        _spin_unlock_irq();
     3)   3.966 us    |      }
     3) ! 2959.433 us |    }
     3) ! 2961.465 us |  }
    
    This patch uses the tracepoint in the scheduling context switch to
    account for time that has elapsed while a task is scheduled out.
    Now we see:
    
     ------------------------------------------
     3)    <idle>-0    =>  edac-po-1067
     ------------------------------------------
    
     3)               |      finish_task_switch() {
     3)   0.685 us    |        _spin_unlock_irq();
     3)   2.331 us    |      }
     3) + 41.439 us   |    }
     3) + 42.663 us   |  }
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 89cd308cc7a5..471e36d30123 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1409,6 +1409,8 @@ struct task_struct {
 	int curr_ret_stack;
 	/* Stack of return addresses for return function tracing */
 	struct ftrace_ret_stack	*ret_stack;
+	/* time stamp for last schedule */
+	unsigned long long ftrace_timestamp;
 	/*
 	 * Number of functions that haven't been traced
 	 * because of depth overrun.

commit 3aa551c9b4c40018f0e261a178e3d25478dc04a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 23 18:28:15 2009 +0100

    genirq: add threaded interrupt handler support
    
    Add support for threaded interrupt handlers:
    
    A device driver can request that its main interrupt handler runs in a
    thread. To achive this the device driver requests the interrupt with
    request_threaded_irq() and provides additionally to the handler a
    thread function. The handler function is called in hard interrupt
    context and needs to check whether the interrupt originated from the
    device. If the interrupt originated from the device then the handler
    can either return IRQ_HANDLED or IRQ_WAKE_THREAD. IRQ_HANDLED is
    returned when no further action is required. IRQ_WAKE_THREAD causes
    the genirq code to invoke the threaded (main) handler. When
    IRQ_WAKE_THREAD is returned handler must have disabled the interrupt
    on the device level. This is mandatory for shared interrupt handlers,
    but we need to do it as well for obscure x86 hardware where disabling
    an interrupt on the IO_APIC level redirects the interrupt to the
    legacy PIC interrupt lines.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 46d680643f89..38b77b0f56e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1292,6 +1292,11 @@ struct task_struct {
 /* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
 	spinlock_t alloc_lock;
 
+#ifdef CONFIG_GENERIC_HARDIRQS
+	/* IRQ handler threads */
+	struct irqaction *irqaction;
+#endif
+
 	/* Protection of the PI data structures: */
 	spinlock_t pi_lock;
 

commit 703a3cd72817e99201cef84a8a7aecc60b2b3581
Merge: df7f54c012b9 8e0ee43bc2c3
Author: James Morris <jmorris@namei.org>
Date:   Tue Mar 24 10:52:46 2009 +1100

    Merge branch 'master' into next

commit 17d85bc7564571a1cce23ffdb2d2a33301876925
Merge: d95c3578120e 041b62374c7f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 05:54:43 2009 +0100

    Merge commit 'v2.6.29-rc8' into cpus4096

commit f6411fe7e09b67470a2569231d6fa566c7c29b8b
Merge: df1c99d41650 f24ade3a3332 9ead64974b05
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 04:50:44 2009 +0100

    Merge branches 'sched/clock', 'sched/urgent' and 'linus' into sched/core

commit 480c93df5b99699390f93a7024c9f60d09da0e96
Merge: aecfcde920da d820ac4c2fa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 01:33:21 2009 +0100

    Merge branch 'core/locking' into tracing/ftrace

commit 3c1f67d60e2b4f4455563928999fd41cc653645d
Merge: 03d78913f01e 9ead64974b05
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 01:29:17 2009 +0100

    Merge branch 'linus' into core/locking

commit 76e6eee03353f01bfca707d4dbb1f10a4ee27dc0
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Mar 12 14:35:43 2009 -0600

    cpumask: tsk_cpumask for accessing the struct task_struct's cpus_allowed.
    
    This allows us to change the representation (to a dangling bitmap or
    cpumask_var_t) without breaking all the callers: they can use
    tsk_cpumask() now and won't see a difference as the changes roll into
    linux-next.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8c216e057c94..011db2f4c94c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1419,6 +1419,9 @@ struct task_struct {
 #endif
 };
 
+/* Future-safe accessor for struct task_struct's cpus_allowed. */
+#define tsk_cpumask(tsk) (&(tsk)->cpus_allowed)
+
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
  * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH

commit f0ef03985130287c6c84ebe69416cf790e6cc00e
Merge: 16097439703b 31bbed527e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 16:44:14 2009 +0100

    Merge branch 'x86/core' into tracing/textedit
    
    Conflicts:
            arch/x86/Kconfig
            block/blktrace.c
            kernel/irq/handle.c
    
    Semantic conflict:
            kernel/trace/blktrace.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a1413c89ae6a4b7a9a43f7768934a81ffb5c629a
Merge: f254f3909efa dd4124a8a06b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 5 21:48:31 2009 +0100

    Merge branch 'x86/urgent' into x86/core
    
    Conflicts:
            arch/x86/include/asm/fixmap_64.h
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a140feab42d1cfd811930ab76104559c19dfc4b0
Merge: 1075414b0610 fec6c6fec3e2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 5 11:45:22 2009 +0100

    Merge commit 'v2.6.29-rc7' into core/locking

commit 28b1bd1cbc33cae95a309691d814399a69cf3070
Merge: 2602c3ba4508 1075414b0610
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 4 18:49:19 2009 +0100

    Merge branch 'core/locking' into tracing/ftrace

commit 8163d88c79dca35478a2405c837733ac50ea4c39
Merge: a1ef58f44254 fec6c6fec3e2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 4 11:42:31 2009 +0100

    Merge commit 'v2.6.29-rc7' into perfcounters/core
    
    Conflicts:
            arch/x86/mm/iomap_32.c

commit a1be621dfacbef0fd374d8acd553d71e07bf29ac
Merge: 3612fdf780e2 fec6c6fec3e2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 4 11:14:47 2009 +0100

    Merge branch 'tracing/ftrace'; commit 'v2.6.29-rc7' into tracing/core

commit 5512b3ece0cbb5024b83099963222700aa45f59e
Merge: c40c6f85a759 8325d9c09ded 54e991242850 778ef1e6cbb0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 2 12:02:36 2009 +0100

    Merge branches 'sched/clock', 'sched/urgent' and 'linus' into sched/core

commit 54e991242850edc8c53f71fa5aa3ba7a93ce38f5
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Fri Feb 27 15:13:54 2009 +0530

    sched: don't allow setuid to succeed if the user does not have rt bandwidth
    
    Impact: fix hung task with certain (non-default) rt-limit settings
    
    Corey Hickey reported that on using setuid to change the uid of a
    rt process, the process would be unkillable and not be running.
    This is because there was no rt runtime for that user group. Add
    in a check to see if a user can attach an rt task to its task group.
    On failure, return EINVAL, which is also returned in
    CONFIG_CGROUP_SCHED.
    
    Reported-by: Corey Hickey <bugfood-ml@fatooh.org>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8981e52c714f..8c216e057c94 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2291,9 +2291,13 @@ extern long sched_group_rt_runtime(struct task_group *tg);
 extern int sched_group_set_rt_period(struct task_group *tg,
 				      long rt_period_us);
 extern long sched_group_rt_period(struct task_group *tg);
+extern int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk);
 #endif
 #endif
 
+extern int task_can_switch_user(struct user_struct *up,
+					struct task_struct *tsk);
+
 #ifdef CONFIG_TASK_XACCT
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
 {

commit 1b49061d400c9e51e3ac2aac026a099fe599b9bb
Merge: 14131f2f98ac 83ce40092868
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 21:21:59 2009 +0100

    Merge branch 'sched/clock' into tracing/ftrace
    
    Conflicts:
            kernel/sched_clock.c

commit b342501cd31e5546d0c9ca8ceff5ded1832f9e5b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 20:20:29 2009 +0100

    sched: allow architectures to specify sched_clock_stable
    
    Allow CONFIG_HAVE_UNSTABLE_SCHED_CLOCK architectures to still specify
    that their sched_clock() implementation is reliable.
    
    This will be used by x86 to switch on a faster sched_clock_cpu()
    implementation on certain CPU types.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8981e52c714f..a063d19b7a7d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1670,6 +1670,16 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 	return set_cpus_allowed_ptr(p, &new_mask);
 }
 
+/*
+ * Architectures can set this to 1 if they have specified
+ * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
+ * but then during bootup it turns out that sched_clock()
+ * is reliable after all:
+ */
+#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
+extern int sched_clock_stable;
+#endif
+
 extern unsigned long long sched_clock(void);
 
 extern void sched_clock_init(void);

commit 5274f8354d6a1ed9d6688e6a89b705b94aa1b6e9
Merge: ad0b0fd554df a0490fa35dc0 d2f8d7ee1a9b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 15 21:15:16 2009 +0100

    Merge branch 'sched/urgent'; commit 'v2.6.29-rc5' into sched/core

commit cf40bd16fdad42c053040bcd3988f5fdedbb6c57
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Jan 21 08:12:39 2009 +0100

    lockdep: annotate reclaim context (__GFP_NOFS)
    
    Here is another version, with the incremental patch rolled up, and
    added reclaim context annotation to kswapd, and allocation tracing
    to slab allocators (which may only ever reach the page allocator
    in rare cases, so it is good to put annotations here too).
    
    Haven't tested this version as such, but it should be getting closer
    to merge worthy ;)
    
    --
    After noticing some code in mm/filemap.c accidentally perform a __GFP_FS
    allocation when it should not have been, I thought it might be a good idea to
    try to catch this kind of thing with lockdep.
    
    I coded up a little idea that seems to work. Unfortunately the system has to
    actually be in __GFP_FS page reclaim, then take the lock, before it will mark
    it. But at least that might still be some orders of magnitude more common
    (and more debuggable) than an actual deadlock condition, so we have some
    improvement I hope (the concept is no less complete than discovery of a lock's
    interrupt contexts).
    
    I guess we could even do the same thing with __GFP_IO (normal reclaim), and
    even GFP_NOIO locks too... but filesystems will have the most locks and fiddly
    code paths, so let's start there and see how it goes.
    
    It *seems* to work. I did a quick test.
    
    =================================
    [ INFO: inconsistent lock state ]
    2.6.28-rc6-00007-ged31348-dirty #26
    ---------------------------------
    inconsistent {in-reclaim-W} -> {ov-reclaim-W} usage.
    modprobe/8526 [HC0[0]:SC0[0]:HE1:SE1] takes:
     (testlock){--..}, at: [<ffffffffa0020055>] brd_init+0x55/0x216 [brd]
    {in-reclaim-W} state was registered at:
      [<ffffffff80267bdb>] __lock_acquire+0x75b/0x1a60
      [<ffffffff80268f71>] lock_acquire+0x91/0xc0
      [<ffffffff8070f0e1>] mutex_lock_nested+0xb1/0x310
      [<ffffffffa002002b>] brd_init+0x2b/0x216 [brd]
      [<ffffffff8020903b>] _stext+0x3b/0x170
      [<ffffffff80272ebf>] sys_init_module+0xaf/0x1e0
      [<ffffffff8020c3fb>] system_call_fastpath+0x16/0x1b
      [<ffffffffffffffff>] 0xffffffffffffffff
    irq event stamp: 3929
    hardirqs last  enabled at (3929): [<ffffffff8070f2b5>] mutex_lock_nested+0x285/0x310
    hardirqs last disabled at (3928): [<ffffffff8070f089>] mutex_lock_nested+0x59/0x310
    softirqs last  enabled at (3732): [<ffffffff8061f623>] sk_filter+0x83/0xe0
    softirqs last disabled at (3730): [<ffffffff8061f5b6>] sk_filter+0x16/0xe0
    
    other info that might help us debug this:
    1 lock held by modprobe/8526:
     #0:  (testlock){--..}, at: [<ffffffffa0020055>] brd_init+0x55/0x216 [brd]
    
    stack backtrace:
    Pid: 8526, comm: modprobe Not tainted 2.6.28-rc6-00007-ged31348-dirty #26
    Call Trace:
     [<ffffffff80265483>] print_usage_bug+0x193/0x1d0
     [<ffffffff80266530>] mark_lock+0xaf0/0xca0
     [<ffffffff80266735>] mark_held_locks+0x55/0xc0
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffff802667ca>] trace_reclaim_fs+0x2a/0x60
     [<ffffffff80285005>] __alloc_pages_internal+0x475/0x580
     [<ffffffff8070f29e>] ? mutex_lock_nested+0x26e/0x310
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffffa002006a>] brd_init+0x6a/0x216 [brd]
     [<ffffffffa0020000>] ? brd_init+0x0/0x216 [brd]
     [<ffffffff8020903b>] _stext+0x3b/0x170
     [<ffffffff8070f8b9>] ? mutex_unlock+0x9/0x10
     [<ffffffff8070f83d>] ? __mutex_unlock_slowpath+0x10d/0x180
     [<ffffffff802669ec>] ? trace_hardirqs_on_caller+0x12c/0x190
     [<ffffffff80272ebf>] sys_init_module+0xaf/0x1e0
     [<ffffffff8020c3fb>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4efb552aca47..b00a77f4999e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1313,6 +1313,7 @@ struct task_struct {
 	int lockdep_depth;
 	unsigned int lockdep_recursion;
 	struct held_lock held_locks[MAX_LOCK_DEPTH];
+	gfp_t lockdep_reclaim_gfp;
 #endif
 
 /* journalling filesystem info */

commit 1c511f740fe7031867f51831854360e8be1ba34c
Merge: e7669b8e3292 00f62f614bb7 b22f4858126a 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 10:25:18 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/ring-buffer', 'tracing/sysprof', 'tracing/urgent' and 'linus' into tracing/core

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e9c4ffb11f0b19005b5b9dc8481687a3637e5887
Merge: 4bcf349a0f90 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:34:07 2009 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit 871cafcc962fa1655c44b4f0e54d4c5cc14e273c
Merge: cf2592f59c0e b578f3fcca1e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 12 13:08:57 2009 +0100

    Merge branch 'linus' into core/softlockup

commit f9ce1f1cda8b73a36f47e424975a9dfa78b7840c
Author: Kentaro Takeda <takedakn@nttdata.co.jp>
Date:   Thu Feb 5 17:18:11 2009 +0900

    Add in_execve flag into task_struct.
    
    This patch allows LSM modules to determine whether current process is in an
    execve operation or not so that they can behave differently while an execve
    operation is in progress.
    
    This patch is needed by TOMOYO. Please see another patch titled "LSM adapter
    functions." for backgrounds.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2127e959e0f4..397c20cfb6a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1158,6 +1158,8 @@ struct task_struct {
 	/* ??? */
 	unsigned int personality;
 	unsigned did_exec:1;
+	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
+				 * execve */
 	pid_t pid;
 	pid_t tgid;
 

commit 4da94d49b2ecb0a26e716a8811c3ecc542c2a65d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 11 11:30:27 2009 +0100

    timers: fix TIMER_ABSTIME for process wide cpu timers
    
    The POSIX timer interface allows for absolute time expiry values through the
    TIMER_ABSTIME flag, therefore we have to synchronize the timer to the clock
    every time we start it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5d10fa0b6002..8981e52c714f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2201,18 +2201,7 @@ static inline int spin_needbreak(spinlock_t *lock)
  * Thread group CPU time accounting.
  */
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
-
-static inline
-void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
-{
-	struct thread_group_cputimer *cputimer = &tsk->signal->cputimer;
-	unsigned long flags;
-
-	spin_lock_irqsave(&cputimer->lock, flags);
-	cputimer->running = 1;
-	*times = cputimer->cputime;
-	spin_unlock_irqrestore(&cputimer->lock, flags);
-}
+void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
 static inline void thread_group_cputime_init(struct signal_struct *sig)
 {

commit 3fccfd67df79c6351a156eb25a7a514e5f39c4d9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Feb 10 16:37:31 2009 +0100

    timers: split process wide cpu clocks/timers, fix
    
    To decrease the chance of a missed enable, always enable the timer when we
    sample it, we'll always disable it when we find that there are no active timers
    in the jiffy tick.
    
    This fixes a flood of warnings reported by Mike Galbraith.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 79392916d6c9..5d10fa0b6002 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2209,6 +2209,7 @@ void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
 	unsigned long flags;
 
 	spin_lock_irqsave(&cputimer->lock, flags);
+	cputimer->running = 1;
 	*times = cputimer->cputime;
 	spin_unlock_irqrestore(&cputimer->lock, flags);
 }

commit 95fd4845ed0ffcab305b4f30ce1c12dc34f1b56c
Merge: d278c4843562 8e4921515c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 11 09:22:04 2009 +0100

    Merge commit 'v2.6.29-rc4' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c
            arch/x86/mm/fault.c
            drivers/acpi/processor_idle.c
            kernel/irq/handle.c

commit 23a185ca8abbeef64b6ffc33059b1d630e43ec10
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 9 22:42:47 2009 +1100

    perf_counters: make software counters work as per-cpu counters
    
    Impact: kernel crash fix
    
    Yanmin Zhang reported that using a PERF_COUNT_TASK_CLOCK software
    counter as a per-cpu counter would reliably crash the system, because
    it calls __task_delta_exec with a null pointer.  The page fault,
    context switch and cpu migration counters also won't function
    correctly as per-cpu counters since they reference the current task.
    
    This fixes the problem by redirecting the task_clock counter to the
    cpu_clock counter when used as a per-cpu counter, and by implementing
    per-cpu page fault, context switch and cpu migration counters.
    
    Along the way, this:
    
    - Initializes counter->ctx earlier, in perf_counter_alloc, so that
      sw_perf_counter_init can use it
    - Adds code to kernel/sched.c to count task migrations into each
      cpu, in rq->nr_migrations_in
    - Exports the per-cpu context switch and task migration counts
      via new functions added to kernel/sched.c
    - Makes sure that if sw_perf_counter_init fails, we don't try to
      initialize the counter as a hardware counter.  Since the user has
      passed a negative, non-raw event type, they clearly don't intend
      for it to be interpreted as a hardware event.
    
    Reported-by: "Zhang Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b85b10abf770..1e5f70062a9c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -137,6 +137,8 @@ extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
+extern u64 cpu_nr_switches(int cpu);
+extern u64 cpu_nr_migrations(int cpu);
 
 struct seq_file;
 struct cfs_rq;

commit 17406b82d621930cca8ccc1272cdac9a7dae8e40
Author: Mandeep Singh Baines <msb@google.com>
Date:   Fri Feb 6 15:37:47 2009 -0800

    softlockup: remove timestamp checking from hung_task
    
    Impact: saves sizeof(long) bytes per task_struct
    
    By guaranteeing that sysctl_hung_task_timeout_secs have elapsed between
    tasklist scans we can avoid using timestamps.
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2a2811c6239d..e0d723fea9f5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1241,7 +1241,6 @@ struct task_struct {
 #endif
 #ifdef CONFIG_DETECT_HUNG_TASK
 /* hung task detection */
-	unsigned long last_switch_timestamp;
 	unsigned long last_switch_count;
 #endif
 /* CPU-specific state of this task */

commit 4ad476e11f94fd3724c6e272d8220e99cd222b27
Merge: 304cc6ae1bf7 8e4921515c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 9 10:32:48 2009 +0100

    Merge commit 'v2.6.29-rc4' into tracing/core

commit 140573d33b703194b7e1893711e78b7f546cca7c
Merge: 34cb61359b50 ceacc2c1c85a 483b4ee60edb
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Feb 8 20:12:46 2009 +0100

    Merge branches 'sched/rt' and 'sched/urgent' into sched/core

commit 673f8205914a12e928c65afbcd78ae748f78df53
Merge: cf47b8f3d96b ae1a25da8448
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Feb 7 18:31:54 2009 +0100

    Merge branch 'linus' into core/locking
    
    Conflicts:
            fs/btrfs/locking.c

commit 7d8e23df69820e6be42bcc41d441f4860e8c76f7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 6 14:57:51 2009 +0100

    timers: split process wide cpu clocks/timers, remove spurious warning
    
    Mike Galbraith reported that the new warning in thread_group_cputimer()
    triggers en masse with Amarok running.
    
    Oleg Nesterov observed:
    
      Can't fastpath_timer_check()->thread_group_cputimer() have the
      false warning too? Suppose we had the timer, then posix_cpu_timer_del()
      removes this timer, but task_cputime_zero(&sig->cputime_expires) still
      not true.
    
    Remove the spurious debug warning.
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Explained-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 082d7619b3a1..79392916d6c9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2208,8 +2208,6 @@ void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
 	struct thread_group_cputimer *cputimer = &tsk->signal->cputimer;
 	unsigned long flags;
 
-	WARN_ON(!cputimer->running);
-
 	spin_lock_irqsave(&cputimer->lock, flags);
 	*times = cputimer->cputime;
 	spin_unlock_irqrestore(&cputimer->lock, flags);

commit 9d45cf9e36bf9bcf16df6e1cbf049807c8402823
Merge: a146649bc19d 0cd5c3c80a0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 31 17:32:31 2009 +0100

    Merge branch 'x86/urgent' into x86/apic
    
    Conflicts:
            arch/x86/mach-default/setup.c
    
    Semantic merge:
            arch/x86/kernel/irqinit_32.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4cd4c1b40d40447fb5e7ba80746c6d7ba91d7a53
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Feb 5 12:24:16 2009 +0100

    timers: split process wide cpu clocks/timers
    
    Change the process wide cpu timers/clocks so that we:
    
     1) don't mess up the kernel with too many threads,
     2) don't have a per-cpu allocation for each process,
     3) have no impact when not used.
    
    In order to accomplish this we're going to split it into two parts:
    
     - clocks; which can take all the time they want since they run
               from user context -- ie. sys_clock_gettime(CLOCK_PROCESS_CPUTIME_ID)
    
     - timers; which need constant time sampling but since they're
               explicity used, the user can pay the overhead.
    
    The clock readout will go back to a full sum of the thread group, while the
    timers will run of a global 'clock' that only runs when needed, so only
    programs that make use of the facility pay the price.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e0646a30314..082d7619b3a1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -443,7 +443,6 @@ struct pacct_struct {
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
- * @lock:		lock for fields in this struct
  *
  * This structure groups together three kinds of CPU time that are
  * tracked for threads and thread groups.  Most things considering
@@ -454,23 +453,33 @@ struct task_cputime {
 	cputime_t utime;
 	cputime_t stime;
 	unsigned long long sum_exec_runtime;
-	spinlock_t lock;
 };
 /* Alternate field names when used to cache expirations. */
 #define prof_exp	stime
 #define virt_exp	utime
 #define sched_exp	sum_exec_runtime
 
+#define INIT_CPUTIME	\
+	(struct task_cputime) {					\
+		.utime = cputime_zero,				\
+		.stime = cputime_zero,				\
+		.sum_exec_runtime = 0,				\
+	}
+
 /**
- * struct thread_group_cputime - thread group interval timer counts
- * @totals:		thread group interval timers; substructure for
- *			uniprocessor kernel, per-cpu for SMP kernel.
+ * struct thread_group_cputimer - thread group interval timer counts
+ * @cputime:		thread group interval timers.
+ * @running:		non-zero when there are timers running and
+ * 			@cputime receives updates.
+ * @lock:		lock for fields in this struct.
  *
  * This structure contains the version of task_cputime, above, that is
- * used for thread group CPU clock calculations.
+ * used for thread group CPU timer calculations.
  */
-struct thread_group_cputime {
-	struct task_cputime totals;
+struct thread_group_cputimer {
+	struct task_cputime cputime;
+	int running;
+	spinlock_t lock;
 };
 
 /*
@@ -519,10 +528,10 @@ struct signal_struct {
 	cputime_t it_prof_incr, it_virt_incr;
 
 	/*
-	 * Thread group totals for process CPU clocks.
-	 * See thread_group_cputime(), et al, for details.
+	 * Thread group totals for process CPU timers.
+	 * See thread_group_cputimer(), et al, for details.
 	 */
-	struct thread_group_cputime cputime;
+	struct thread_group_cputimer cputimer;
 
 	/* Earliest-expiration cache. */
 	struct task_cputime cputime_expires;
@@ -2191,27 +2200,26 @@ static inline int spin_needbreak(spinlock_t *lock)
 /*
  * Thread group CPU time accounting.
  */
+void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 
 static inline
-void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)
+void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
 {
-	struct task_cputime *totals = &tsk->signal->cputime.totals;
+	struct thread_group_cputimer *cputimer = &tsk->signal->cputimer;
 	unsigned long flags;
 
-	spin_lock_irqsave(&totals->lock, flags);
-	*times = *totals;
-	spin_unlock_irqrestore(&totals->lock, flags);
+	WARN_ON(!cputimer->running);
+
+	spin_lock_irqsave(&cputimer->lock, flags);
+	*times = cputimer->cputime;
+	spin_unlock_irqrestore(&cputimer->lock, flags);
 }
 
 static inline void thread_group_cputime_init(struct signal_struct *sig)
 {
-	sig->cputime.totals = (struct task_cputime){
-		.utime = cputime_zero,
-		.stime = cputime_zero,
-		.sum_exec_runtime = 0,
-	};
-
-	spin_lock_init(&sig->cputime.totals.lock);
+	sig->cputimer.cputime = INIT_CPUTIME;
+	spin_lock_init(&sig->cputimer.lock);
+	sig->cputimer.running = 0;
 }
 
 static inline void thread_group_cputime_free(struct signal_struct *sig)

commit 32bd671d6cbeda60dc73be77fa2b9037d9a9bfa0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Feb 5 12:24:15 2009 +0100

    signal: re-add dead task accumulation stats.
    
    We're going to split the process wide cpu accounting into two parts:
    
     - clocks; which can take all the time they want since they run
               from user context.
    
     - timers; which need constant time tracing but can affort the overhead
               because they're default off -- and rare.
    
    The clock readout will go back to a full sum of the thread group, for this
    we need to re-add the exit stats that were removed in the initial itimer
    rework (f06febc9: timers: fix itimer/many thread hang).
    
    Furthermore, since that full sum can be rather slow for large thread groups
    and we have the complete dead task stats, revert the do_notify_parent time
    computation.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2127e959e0f4..2e0646a30314 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -559,7 +559,7 @@ struct signal_struct {
 	 * Live threads maintain their own counters and add to these
 	 * in __exit_signal, except for the group leader.
 	 */
-	cputime_t cutime, cstime;
+	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
@@ -567,6 +567,14 @@ struct signal_struct {
 	unsigned long inblock, oublock, cinblock, coublock;
 	struct task_io_accounting ioac;
 
+	/*
+	 * Cumulative ns of schedule CPU time fo dead threads in the
+	 * group, not including a zombie group leader, (This only differs
+	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
+	 * other than jiffies.)
+	 */
+	unsigned long long sum_sched_runtime;
+
 	/*
 	 * We don't bother to synchronize most readers of this at all,
 	 * because there is no reader checking a limit that actually needs

commit 35626129abcd6a7547e84c817ef5b6eff7a8758b
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon Feb 2 16:00:29 2009 -0800

    sched: add missing kernel-doc in sched.h
    
    Add kernel-doc notation for @lock:
    
    include/linux/sched.h:457: No description found for parameter 'lock'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a7c76388731..2127e959e0f4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -443,6 +443,7 @@ struct pacct_struct {
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
+ * @lock:		lock for fields in this struct
  *
  * This structure groups together three kinds of CPU time that are
  * tracked for threads and thread groups.  Most things considering

commit dc573f9b20c8710105ac35c08ed0fe1da5160ecd
Merge: b3a8c34886d0 ecf441b593ac b1792e367053
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 3 06:25:38 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/kmemtrace' and 'linus' into tracing/core

commit 5e54f5986a579b8445aa1d5ad3435c2cf7568bed
Author: Mandeep Singh Baines <msb@google.com>
Date:   Fri Jan 30 15:29:54 2009 -0800

    softlockup: remove unused definition for spawn_softlockup_task
    
    The definition of spawn_softlockup_task in sched.h became
    unnecessary once it was converted to the early_initcall()
    interface.
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f2f94d532302..2a2811c6239d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -302,9 +302,6 @@ extern int softlockup_thresh;
 static inline void softlockup_tick(void)
 {
 }
-static inline void spawn_softlockup_task(void)
-{
-}
 static inline void touch_softlockup_watchdog(void)
 {
 }

commit 9df04e1f25effde823a600e755b51475d438f56b
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu Jan 29 14:25:26 2009 -0800

    epoll: drop max_user_instances and rely only on max_user_watches
    
    Linus suggested to put limits where the money is, and max_user_watches
    already does that w/out the need of max_user_instances.  That has the
    advantage to mitigate the potential DoS while allowing pretty generous
    default behavior.
    
    Allowing top 4% of low memory (per user) to be allocated in epoll watches,
    we have:
    
    LOMEM    MAX_WATCHES (per user)
    512MB    ~178000
    1GB      ~356000
    2GB      ~712000
    
    A box with 512MB of lomem, will meet some challenge in hitting 180K
    watches, socket buffers math teaches us.  No more max_user_instances
    limits then.
    
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Cc: Willy Tarreau <w@1wt.eu>
    Cc: Michael Kerrisk <mtk.manpages@googlemail.com>
    Cc: Bron Gondwana <brong@fastmail.fm>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 02e16d207304..5a7c76388731 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -630,7 +630,6 @@ struct user_struct {
 	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
 #endif
 #ifdef CONFIG_EPOLL
-	atomic_t epoll_devs;	/* The number of epoll descriptors currently open */
 	atomic_t epoll_watches;	/* The number of file descriptors currently watched */
 #endif
 #ifdef CONFIG_POSIX_MQUEUE

commit 3ddeb51d9c83931c1ca6abf76a38934c5a1ed918
Merge: 5a611268b69f 5ee810072175
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 12:01:51 2009 +0100

    Merge branch 'linus' into core/percpu
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c

commit 3386c05bdbd3e60ca7158253442f0a00133db28e
Merge: 1e70c7f7a9d4 6552ebae25ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 26 09:47:56 2009 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      debugobjects: add and use INIT_WORK_ON_STACK
      rcu: remove duplicate CONFIG_RCU_CPU_STALL_DETECTOR
      relay: fix lock imbalance in relay_late_setup_files
      oprofile: fix uninitialized use of struct op_entry
      rcu: move Kconfig menu
      softlock: fix false panic which can occur if softlockup_thresh is reduced
      rcu: add __cpuinit to rcu_init_percpu_data()

commit 7e49fcce1bdadd723ae6a0b3b324c4daced61563
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 22 19:01:40 2009 -0500

    trace, lockdep: manual preempt count adding for local_bh_disable
    
    Impact: fix to preempt trace triggering lockdep check_flag failure
    
    In local_bh_disable, the use of add_preempt_count causes the
    preempt tracer to start recording the time preemption is off.
    But because it already modified the preempt_count to show
    softirqs disabled, and before it called the lockdep code to
    handle this, it causes a state that lockdep can not handle.
    
    The preempt tracer will reset the ring buffer on start of a trace,
    and the ring buffer reset code does a spin_lock_irqsave. This
    calls into lockdep and lockdep will fail when it detects the
    invalid state of having softirqs disabled but the internal
    current->softirqs_enabled is still set.
    
    The fix is to manually add the SOFTIRQ_OFFSET to preempt count
    and call the preempt tracer code outside the lockdep critical
    area.
    
    Thanks to Peter Zijlstra for suggesting this solution.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cae9b81a1f8..33085b88f87b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -137,6 +137,8 @@ extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
 
+extern unsigned long get_parent_ip(unsigned long addr);
+
 struct seq_file;
 struct cfs_rq;
 struct task_group;

commit bfe2a3c3b5bf479788d5d5c5561346be6b169043
Merge: 77835492ed48 35d266a24796
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 23 10:20:15 2009 +0100

    Merge branch 'core/percpu' into perfcounters/core
    
    Conflicts:
            arch/x86/include/asm/hardirq_32.h
            arch/x86/include/asm/hardirq_64.h
    
    Semantic merge:
            arch/x86/include/asm/hardirq.h
            [ added apic_perf_irqs field. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b2b062b8163391c42b3219d466ca1ac9742b9c7b
Merge: a9de18eb761f 99937d6455ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 18 18:37:14 2009 +0100

    Merge branch 'core/percpu' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            arch/x86/include/asm/system.h
    
    Also, moved include/asm-x86/stackprotector.h to arch/x86/include/asm.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e162b39a368f0401e41b558f430c354d12a85b37
Author: Mandeep Singh Baines <msb@google.com>
Date:   Thu Jan 15 11:08:40 2009 -0800

    softlockup: decouple hung tasks check from softlockup detection
    
    Decoupling allows:
    
    * hung tasks check to happen at very low priority
    
    * hung tasks check and softlockup to be enabled/disabled independently
      at compile and/or run-time
    
    * individual panic settings to be enabled disabled independently
      at compile and/or run-time
    
    * softlockup threshold to be reduced without increasing hung tasks
      poll frequency (hung task check is expensive relative to softlock watchdog)
    
    * hung task check to be zero over-head when disabled at run-time
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 54cbabf3b871..f2f94d532302 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -297,9 +297,6 @@ extern int proc_dosoftlockup_thresh(struct ctl_table *table, int write,
 				    struct file *filp, void __user *buffer,
 				    size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
-extern unsigned long sysctl_hung_task_check_count;
-extern unsigned long sysctl_hung_task_timeout_secs;
-extern unsigned long sysctl_hung_task_warnings;
 extern int softlockup_thresh;
 #else
 static inline void softlockup_tick(void)
@@ -316,6 +313,15 @@ static inline void touch_all_softlockup_watchdogs(void)
 }
 #endif
 
+#ifdef CONFIG_DETECT_HUNG_TASK
+extern unsigned int  sysctl_hung_task_panic;
+extern unsigned long sysctl_hung_task_check_count;
+extern unsigned long sysctl_hung_task_timeout_secs;
+extern unsigned long sysctl_hung_task_warnings;
+extern int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
+					 struct file *filp, void __user *buffer,
+					 size_t *lenp, loff_t *ppos);
+#endif
 
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
@@ -1236,7 +1242,7 @@ struct task_struct {
 /* ipc stuff */
 	struct sysv_sem sysvsem;
 #endif
-#ifdef CONFIG_DETECT_SOFTLOCKUP
+#ifdef CONFIG_DETECT_HUNG_TASK
 /* hung task detection */
 	unsigned long last_switch_timestamp;
 	unsigned long last_switch_count;

commit 34cb61359b503d7aff6447acb037a5efd6ce93b2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 16 13:36:06 2009 +0100

    sched: fix !CONFIG_SCHEDSTATS build failure
    
    Stephen Rothwell reported this linux-next build failure with !CONFIG_SCHEDSTATS:
    
    | In file included from kernel/sched.c:1703:
    | kernel/sched_fair.c: In function 'adaptive_gran':
    | kernel/sched_fair.c:1324: error: 'struct sched_entity' has no member named 'avg_wakeup'
    
    The start_runtime and avg_wakeup metrics are now not just for statistics,
    but also for scheduling - so they always need to be available. (Also
    move out the nr_migrations fields - for future perfcounters usage.)
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index daf4e07bc978..5d56b54350a5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1031,6 +1031,10 @@ struct sched_entity {
 	u64			last_wakeup;
 	u64			avg_overlap;
 
+	u64			start_runtime;
+	u64			avg_wakeup;
+	u64			nr_migrations;
+
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;
@@ -1046,10 +1050,6 @@ struct sched_entity {
 	u64			exec_max;
 	u64			slice_max;
 
-	u64			start_runtime;
-	u64			avg_wakeup;
-
-	u64			nr_migrations;
 	u64			nr_migrations_cold;
 	u64			nr_failed_migrations_affine;
 	u64			nr_failed_migrations_running;

commit 831451ac4e44d3a20b581ce726ef1d1144373f7d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jan 14 12:39:18 2009 +0100

    sched: introduce avg_wakeup
    
    Introduce a new avg_wakeup statistic.
    
    avg_wakeup is a measure of how frequently a task wakes up other tasks, it
    represents the average time between wakeups, with a limit of avg_runtime
    for when it doesn't wake up anybody.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cae9b81a1f8..daf4e07bc978 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1046,6 +1046,9 @@ struct sched_entity {
 	u64			exec_max;
 	u64			slice_max;
 
+	u64			start_runtime;
+	u64			avg_wakeup;
+
 	u64			nr_migrations;
 	u64			nr_migrations_cold;
 	u64			nr_failed_migrations_affine;

commit 0d66bf6d3514b35eb6897629059443132992dbd7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Jan 12 14:01:47 2009 +0100

    mutex: implement adaptive spinning
    
    Change mutex contention behaviour such that it will sometimes busy wait on
    acquisition - moving its behaviour closer to that of spinlocks.
    
    This concept got ported to mainline from the -rt tree, where it was originally
    implemented for rtmutexes by Steven Rostedt, based on work by Gregory Haskins.
    
    Testing with Ingo's test-mutex application (http://lkml.org/lkml/2006/1/8/50)
    gave a 345% boost for VFS scalability on my testbox:
    
     # ./test-mutex-shm V 16 10 | grep "^avg ops"
     avg ops/sec:               296604
    
     # ./test-mutex-shm V 16 10 | grep "^avg ops"
     avg ops/sec:               85870
    
    The key criteria for the busy wait is that the lock owner has to be running on
    a (different) cpu. The idea is that as long as the owner is running, there is a
    fair chance it'll release the lock soon, and thus we'll be better off spinning
    instead of blocking/scheduling.
    
    Since regular mutexes (as opposed to rtmutexes) do not atomically track the
    owner, we add the owner in a non-atomic fashion and deal with the races in
    the slowpath.
    
    Furthermore, to ease the testing of the performance impact of this new code,
    there is means to disable this behaviour runtime (without having to reboot
    the system), when scheduler debugging is enabled (CONFIG_SCHED_DEBUG=y),
    by issuing the following command:
    
     # echo NO_OWNER_SPIN > /debug/sched_features
    
    This command re-enables spinning again (this is also the default):
    
     # echo OWNER_SPIN > /debug/sched_features
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9f0b372cfa6f..c34b137cd1e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -330,6 +330,7 @@ extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void __schedule(void);
 asmlinkage void schedule(void);
+extern int mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner);
 
 struct nsproxy;
 struct user_namespace;

commit 41719b03091911028116155deddc5eedf8c45e37
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jan 14 15:36:26 2009 +0100

    mutex: preemption fixes
    
    The problem is that dropping the spinlock right before schedule is a voluntary
    preemption point and can cause a schedule, right after which we schedule again.
    
    Fix this inefficiency by keeping preemption disabled until we schedule, do this
    by explicity disabling preemption and providing a schedule() variant that
    assumes preemption is already disabled.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cae9b81a1f8..9f0b372cfa6f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -328,6 +328,7 @@ extern signed long schedule_timeout(signed long timeout);
 extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
+asmlinkage void __schedule(void);
 asmlinkage void schedule(void);
 
 struct nsproxy;

commit baf48f6577e581a9adb8fe849dc80e24b21d171d
Author: Mandeep Singh Baines <msb@google.com>
Date:   Mon Jan 12 21:15:17 2009 -0800

    softlock: fix false panic which can occur if softlockup_thresh is reduced
    
    At run-time, if softlockup_thresh is changed to a much lower value,
    touch_timestamp is likely to be much older than the new softlock_thresh.
    
    This will cause a false softlockup to be detected. If softlockup_panic
    is enabled, the system will panic.
    
    The fix is to touch all watchdogs before changing softlockup_thresh.
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cae9b81a1f8..54cbabf3b871 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -293,6 +293,9 @@ extern void sched_show_task(struct task_struct *p);
 extern void softlockup_tick(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
+extern int proc_dosoftlockup_thresh(struct ctl_table *table, int write,
+				    struct file *filp, void __user *buffer,
+				    size_t *lenp, loff_t *ppos);
 extern unsigned int  softlockup_panic;
 extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;

commit 0a6d4e1dc9154c4376358663d74060d1e33d203e
Merge: c59765042f53 1563513d34ed
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 04:58:49 2009 +0100

    Merge branch 'sched/latest' of git://git.kernel.org/pub/scm/linux/kernel/git/ghaskins/linux-2.6-hacks into sched/rt

commit 506c10f26c481b7f8ef27c1c79290f68989b2e9e
Merge: e1df957670ae c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:42:53 2009 +0100

    Merge commit 'v2.6.29-rc1' into perfcounters/core
    
    Conflicts:
            include/linux/kernel_stat.h

commit 490dea45d00f01847ebebd007685d564aaf2cd98
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 24 17:06:57 2008 +0100

    itimers: remove the per-cpu-ish-ness
    
    Either we bounce once cacheline per cpu per tick, yielding n^2 bounces
    or we just bounce a single..
    
    Also, using per-cpu allocations for the thread-groups complicates the
    per-cpu allocator in that its currently aimed to be a fixed sized
    allocator and the only possible extention to that would be vmap based,
    which is seriously constrained on 32 bit archs.
    
    So making the per-cpu memory requirement depend on the number of
    processes is an issue.
    
    Lastly, it didn't deal with cpu-hotplug, although admittedly that might
    be fixable.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4cae9b81a1f8..c20943eabb4c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -450,6 +450,7 @@ struct task_cputime {
 	cputime_t utime;
 	cputime_t stime;
 	unsigned long long sum_exec_runtime;
+	spinlock_t lock;
 };
 /* Alternate field names when used to cache expirations. */
 #define prof_exp	stime
@@ -465,7 +466,7 @@ struct task_cputime {
  * used for thread group CPU clock calculations.
  */
 struct thread_group_cputime {
-	struct task_cputime *totals;
+	struct task_cputime totals;
 };
 
 /*
@@ -2180,24 +2181,30 @@ static inline int spin_needbreak(spinlock_t *lock)
  * Thread group CPU time accounting.
  */
 
-extern int thread_group_cputime_alloc(struct task_struct *);
-extern void thread_group_cputime(struct task_struct *, struct task_cputime *);
-
-static inline void thread_group_cputime_init(struct signal_struct *sig)
+static inline
+void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)
 {
-	sig->cputime.totals = NULL;
+	struct task_cputime *totals = &tsk->signal->cputime.totals;
+	unsigned long flags;
+
+	spin_lock_irqsave(&totals->lock, flags);
+	*times = *totals;
+	spin_unlock_irqrestore(&totals->lock, flags);
 }
 
-static inline int thread_group_cputime_clone_thread(struct task_struct *curr)
+static inline void thread_group_cputime_init(struct signal_struct *sig)
 {
-	if (curr->signal->cputime.totals)
-		return 0;
-	return thread_group_cputime_alloc(curr);
+	sig->cputime.totals = (struct task_cputime){
+		.utime = cputime_zero,
+		.stime = cputime_zero,
+		.sum_exec_runtime = 0,
+	};
+
+	spin_lock_init(&sig->cputime.totals.lock);
 }
 
 static inline void thread_group_cputime_free(struct signal_struct *sig)
 {
-	free_percpu(sig->cputime.totals);
 }
 
 /*

commit cfa97f993c275d193fe82c22511dfb5f1e51b661
Merge: 7238eb4ca35c db2f59c8c9b3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 6 17:10:33 2009 -0800

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: fix section mismatch
      sched: fix double kfree in failure path
      sched: clean up arch_reinit_sched_domains()
      sched: mark sched_create_sysfs_power_savings_entries() as __init
      getrusage: RUSAGE_THREAD should return ru_utime and ru_stime
      sched: fix sched_slice()
      sched_clock: prevent scd->clock from moving backwards, take #2
      sched: sched.c declare variables before they get used

commit 901608d9045146aec6f14a7777ea4b1501c379f0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 6 14:40:29 2009 -0800

    mm: introduce get_mm_hiwater_xxx(), fix taskstats->hiwater_xxx accounting
    
    xacct_add_tsk() relies on do_exit()->update_hiwater_xxx() and uses
    mm->hiwater_xxx directly, this leads to 2 problems:
    
    - taskstats_user_cmd() can call fill_pid()->xacct_add_tsk() at any
      moment before the task exits, so we should check the current values of
      rss/vm anyway.
    
    - do_exit()->update_hiwater_xxx() calls are racy.  An exiting thread can
      be preempted right before mm->hiwater_xxx = new_val, and another thread
      can use A_LOT of memory and exit in between.  When the first thread
      resumes it can be the last thread in the thread group, in that case we
      report the wrong hiwater_xxx values which do not take A_LOT into
      account.
    
    Introduce get_mm_hiwater_rss() and get_mm_hiwater_vm() helpers and change
    xacct_add_tsk() to use them.  The first helper will also be used by
    rusage->ru_maxrss accounting.
    
    Kill do_exit()->update_hiwater_xxx() calls.  Unless we are going to
    decrease rss/vm there is no point to update mm->hiwater_xxx, and nobody
    can look at this mm_struct when exit_mmap() actually unmaps the memory.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 38a3f4b15394..ea415136ac9e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -386,6 +386,9 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 		(mm)->hiwater_vm = (mm)->total_vm;	\
 } while (0)
 
+#define get_mm_hiwater_rss(mm)	max((mm)->hiwater_rss, get_mm_rss(mm))
+#define get_mm_hiwater_vm(mm)	max((mm)->hiwater_vm, (mm)->total_vm)
+
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 

commit d9be28ea9110c596a05bd2d56afa94251bd19818
Merge: c70f22d203fc 1c5745aa380e 47fea2adfc9e 238c6d54830c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 09:33:57 2009 +0100

    Merge branches 'sched/clock', 'sched/cleanups' and 'linus' into sched/urgent

commit c70f22d203fc02c805b6ed4a3483b740dc36786b
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Jan 5 19:07:50 2009 +0800

    sched: clean up arch_reinit_sched_domains()
    
    - Make arch_reinit_sched_domains() static. It was exported to be used in
      s390, but now rebuild_sched_domains() is used instead.
    
    - Make it return void.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 38a3f4b15394..91207df702e8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -912,7 +912,6 @@ static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 
 extern void partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
 				    struct sched_domain_attr *dattr_new);
-extern int arch_reinit_sched_domains(void);
 
 /* Test a flag in parent sched domain */
 static inline int test_sd_parent(struct sched_domain *sd, int flag)

commit 61420f59a589c0668f70cbe725785837c78ece90
Merge: d97106ab53f8 c742b31c03f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 3 11:56:24 2009 -0800

    Merge branch 'cputime' of git://git390.osdl.marist.edu/pub/scm/linux-2.6
    
    * 'cputime' of git://git390.osdl.marist.edu/pub/scm/linux-2.6:
      [PATCH] fast vdso implementation for CLOCK_THREAD_CPUTIME_ID
      [PATCH] improve idle cputime accounting
      [PATCH] improve precision of idle time detection.
      [PATCH] improve precision of process accounting.
      [PATCH] idle cputime accounting
      [PATCH] fix scaled & unscaled cputime accounting

commit b840d79631c882786925303c2b0f4fefc31845ed
Merge: 597b0d21626d c3d80000e3a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 2 11:44:09 2009 -0800

    Merge branch 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (66 commits)
      x86: export vector_used_by_percpu_irq
      x86: use logical apicid in x2apic_cluster's x2apic_cpu_mask_to_apicid_and()
      sched: nominate preferred wakeup cpu, fix
      x86: fix lguest used_vectors breakage, -v2
      x86: fix warning in arch/x86/kernel/io_apic.c
      sched: fix warning in kernel/sched.c
      sched: move test_sd_parent() to an SMP section of sched.h
      sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
      sched: activate active load balancing in new idle cpus
      sched: bias task wakeups to preferred semi-idle packages
      sched: nominate preferred wakeup cpu
      sched: favour lower logical cpu number for sched_mc balance
      sched: framework for sched_mc/smt_power_savings=N
      sched: convert BALANCE_FOR_xx_POWER to inline functions
      x86: use possible_cpus=NUM to extend the possible cpus allowed
      x86: fix cpu_mask_to_apicid_and to include cpu_online_mask
      x86: update io_apic.c to the new cpumask code
      x86: Introduce topology_core_cpumask()/topology_thread_cpumask()
      x86: xen: use smp_call_function_many()
      x86: use work_on_cpu in x86/kernel/cpu/mcheck/mce_amd_64.c
      ...
    
    Fixed up trivial conflict in kernel/time/tick-sched.c manually

commit 79741dd35713ff4f6fd0eafd59fa94e8a4ba922d
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Dec 31 15:11:38 2008 +0100

    [PATCH] idle cputime accounting
    
    The cpu time spent by the idle process actually doing something is
    currently accounted as idle time. This is plain wrong, the architectures
    that support VIRT_CPU_ACCOUNTING=y can do better: distinguish between the
    time spent doing nothing and the time spent by idle doing work. The first
    is accounted with account_idle_time and the second with account_system_time.
    The architectures that use the account_xxx_time interface directly and not
    the account_xxx_ticks interface now need to do the check for the idle
    process in their arch code. In particular to improve the system vs true
    idle time accounting the arch code needs to measure the true idle time
    instead of just testing for the idle process.
    To improve the tick based accounting as well we would need an architecture
    primitive that can tell us if the pt_regs of the interrupted context
    points to the magic instruction that halts the cpu.
    
    In addition idle time is no more added to the stime of the idle process.
    This field now contains the system time of the idle process as it should
    be. On systems without VIRT_CPU_ACCOUNTING this will always be zero as
    every tick that occurs while idle is running will be accounted as idle
    time.
    
    This patch contains the necessary common code changes to be able to
    distinguish idle system time and true idle time. The architectures with
    support for VIRT_CPU_ACCOUNTING need some changes to exploit this.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8395e715809d..b475d4db8053 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -284,7 +284,6 @@ long io_schedule_timeout(long timeout);
 
 extern void cpu_init (void);
 extern void trap_init(void);
-extern void account_process_tick(struct task_struct *task, int user);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 

commit a9de18eb761f7c1c860964b2e5addc1a35c7e861
Merge: b2aaf8f74cdc 6a94cb73064c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 31 08:31:57 2008 +0100

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            kernel/fork.c

commit 47fea2adfc9e16846bc57c2f64ff233b354fef39
Author: Jaswinder Singh Rajput <jaswinder@infradead.org>
Date:   Mon Dec 29 23:39:17 2008 +0530

    sched: sched.c declare variables before they get used
    
    Impact: cleanup, avoid sparse warnings
    
    In linux/sched.h moved out sysctl_sched_latency, sysctl_sched_min_granularity,
    sysctl_sched_wakeup_granularity, sysctl_sched_shares_ratelimit and
    sysctl_sched_shares_thresh from #ifdef CONFIG_SCHED_DEBUG as these variables
    are common for both.
    
    Fixes these sparse warnings:
      kernel/sched.c:825:14: warning: symbol 'sysctl_sched_shares_ratelimit' was not declared. Should it be static?
      kernel/sched.c:832:14: warning: symbol 'sysctl_sched_shares_thresh' was not declared. Should it be static?
      kernel/sched_fair.c:37:14: warning: symbol 'sysctl_sched_latency' was not declared. Should it be static?
      kernel/sched_fair.c:43:14: warning: symbol 'sysctl_sched_min_granularity' was not declared. Should it be static?
      kernel/sched_fair.c:72:14: warning: symbol 'sysctl_sched_wakeup_granularity' was not declared. Should it be static?
    
    Signed-off-by: Jaswinder Singh Rajput <jaswinderrajput@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8395e715809d..01d9fd268eb0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1651,16 +1651,16 @@ extern void wake_up_idle_cpu(int cpu);
 static inline void wake_up_idle_cpu(int cpu) { }
 #endif
 
-#ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
+extern unsigned int sysctl_sched_shares_ratelimit;
+extern unsigned int sysctl_sched_shares_thresh;
+#ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
-extern unsigned int sysctl_sched_shares_ratelimit;
-extern unsigned int sysctl_sched_shares_thresh;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit 917b627d4d981dc614519d7b34ea31a976b14e12
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Mon Dec 29 09:39:53 2008 -0500

    sched: create "pushable_tasks" list to limit pushing to one attempt
    
    The RT scheduler employs a "push/pull" design to actively balance tasks
    within the system (on a per disjoint cpuset basis).  When a task is
    awoken, it is immediately determined if there are any lower priority
    cpus which should be preempted.  This is opposed to the way normal
    SCHED_OTHER tasks behave, which will wait for a periodic rebalancing
    operation to occur before spreading out load.
    
    When a particular RQ has more than 1 active RT task, it is said to
    be in an "overloaded" state.  Once this occurs, the system enters
    the active balancing mode, where it will try to push the task away,
    or persuade a different cpu to pull it over.  The system will stay
    in this state until the system falls back below the <= 1 queued RT
    task per RQ.
    
    However, the current implementation suffers from a limitation in the
    push logic.  Once overloaded, all tasks (other than current) on the
    RQ are analyzed on every push operation, even if it was previously
    unpushable (due to affinity, etc).  Whats more, the operation stops
    at the first task that is unpushable and will not look at items
    lower in the queue.  This causes two problems:
    
    1) We can have the same tasks analyzed over and over again during each
       push, which extends out the fast path in the scheduler for no
       gain.  Consider a RQ that has dozens of tasks that are bound to a
       core.  Each one of those tasks will be encountered and skipped
       for each push operation while they are queued.
    
    2) There may be lower-priority tasks under the unpushable task that
       could have been successfully pushed, but will never be considered
       until either the unpushable task is cleared, or a pull operation
       succeeds.  The net result is a potential latency source for mid
       priority tasks.
    
    This patch aims to rectify these two conditions by introducing a new
    priority sorted list: "pushable_tasks".  A task is added to the list
    each time a task is activated or preempted.  It is removed from the
    list any time it is deactivated, made current, or fails to push.
    
    This works because a task only needs to be attempted to push once.
    After an initial failure to push, the other cpus will eventually try to
    pull the task when the conditions are proper.  This also solves the
    problem that we don't completely analyze all tasks due to encountering
    an unpushable tasks.  Now every task will have a push attempted (when
    appropriate).
    
    This reduces latency both by shorting the critical section of the
    rq->lock for certain workloads, and by making sure the algorithm
    considers all eligible tasks in the system.
    
    [ rostedt: added a couple more BUG_ONs ]
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Acked-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 836a86c32a65..440cabb2d432 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1179,6 +1179,7 @@ struct task_struct {
 #endif
 
 	struct list_head tasks;
+	struct plist_node pushable_tasks;
 
 	struct mm_struct *mm, *active_mm;
 

commit 967fc04671feea4dbf780c9e55a0bc8fcf68a14e
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Mon Dec 29 09:39:52 2008 -0500

    sched: add sched_class->needs_post_schedule() member
    
    We currently run class->post_schedule() outside of the rq->lock, which
    means that we need to test for the need to post_schedule outside of
    the lock to avoid a forced reacquistion.  This is currently not a problem
    as we only look at rq->rt.overloaded.  However, we want to enhance this
    going forward to look at more state to reduce the need to post_schedule to
    a bare minimum set.  Therefore, we introduce a new member-func called
    needs_post_schedule() which tests for the post_schedule condtion without
    actually performing the work.  Therefore it is safe to call this
    function before the rq->lock is released, because we are guaranteed not
    to drop the lock at an intermediate point (such as what post_schedule()
    may do).
    
    We will use this later in the series
    
    [ rostedt: removed paranoid BUG_ON ]
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e5f928a079e8..836a86c32a65 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1012,6 +1012,7 @@ struct sched_class {
 			      struct rq *busiest, struct sched_domain *sd,
 			      enum cpu_idle_type idle);
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
+	int (*needs_post_schedule) (struct rq *this_rq);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
 

commit e1df957670aef74ffd9a4ad93e6d2c90bf6b4845
Merge: 2b583d8bc8d7 3c92ec8ae91e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 09:45:15 2008 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            fs/exec.c
            include/linux/init_task.h
    
    Simple context conflicts.

commit a39b863342b8aba52390092be95db58f6ed56061
Merge: b0f4b285d7ed 4e202284e6ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:27:58 2008 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (31 commits)
      sched: fix warning in fs/proc/base.c
      schedstat: consolidate per-task cpu runtime stats
      sched: use RCU variant of list traversal in for_each_leaf_rt_rq()
      sched, cpuacct: export percpu cpuacct cgroup stats
      sched, cpuacct: refactoring cpuusage_read / cpuusage_write
      sched: optimize update_curr()
      sched: fix wakeup preemption clock
      sched: add missing arch_update_cpu_topology() call
      sched: let arch_update_cpu_topology indicate if topology changed
      sched: idle_balance() does not call load_balance_newidle()
      sched: fix sd_parent_degenerate on non-numa smp machine
      sched: add uid information to sched_debug for CONFIG_USER_SCHED
      sched: move double_unlock_balance() higher
      sched: update comment for move_task_off_dead_cpu
      sched: fix inconsistency when redistribute per-cpu tg->cfs_rq shares
      sched/rt: removed unneeded defintion
      sched: add hierarchical accounting to cpu accounting controller
      sched: include group statistics in /proc/sched_debug
      sched: rename SCHED_NO_NO_OMIT_FRAME_POINTER => SCHED_OMIT_FRAME_POINTER
      sched: clean up SCHED_CPUMASK_ALLOC
      ...

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit 06aaf76a7e2e4cc57eabcb8f43ec99c961fe55fe
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 18 21:30:23 2008 +0100

    sched: move test_sd_parent() to an SMP section of sched.h
    
    Impact: build fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a933d925473..e5f928a079e8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -920,6 +920,15 @@ extern void partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
 				    struct sched_domain_attr *dattr_new);
 extern int arch_reinit_sched_domains(void);
 
+/* Test a flag in parent sched domain */
+static inline int test_sd_parent(struct sched_domain *sd, int flag)
+{
+	if (sd->parent && (sd->parent->flags & flag))
+		return 1;
+
+	return 0;
+}
+
 #else /* CONFIG_SMP */
 
 struct sched_domain_attr;
@@ -1431,15 +1440,6 @@ struct task_struct {
 #endif
 };
 
-/* Test a flag in parent sched domain */
-static inline int test_sd_parent(struct sched_domain *sd, int flag)
-{
-	if (sd->parent && (sd->parent->flags & flag))
-		return 1;
-
-	return 0;
-}
-
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
  * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH

commit 100fdaee70ebf5f31b9451fbc01300c627091328
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Thu Dec 18 23:26:47 2008 +0530

    sched: add SD_BALANCE_NEWIDLE at MC and CPU level for sched_mc>0
    
    Impact: change task balancing to save power more agressively
    
    Add SD_BALANCE_NEWIDLE flag at MC level and CPU level
    if sched_mc is set.  This helps power savings and
    will not affect performance when sched_mc=0
    
    Ingo and Mike Galbraith have optimised the SD flags by
    removing SD_BALANCE_NEWIDLE at MC and CPU level.  This
    helps performance but hurts power savings since this
    slows down task consolidation by reducing the number
    of times load_balance is run.
    
        sched: fine-tune SD_MC_INIT
            commit 14800984706bf6936bbec5187f736e928be5c218
            Author: Mike Galbraith <efault@gmx.de>
            Date:   Fri Nov 7 15:26:50 2008 +0100
    
        sched: re-tune balancing -- revert
            commit 9fcd18c9e63e325dbd2b4c726623f760788d5aa8
            Author: Ingo Molnar <mingo@elte.hu>
            Date:   Wed Nov 5 16:52:08 2008 +0100
    
    This patch selectively enables SD_BALANCE_NEWIDLE flag
    only when sched_mc is set to 1 or 2.  This helps power savings
    by task consolidation and also does not hurt performance at
    sched_mc=0 where all power saving optimisations are turned off.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a96726658eca..5a933d925473 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -792,6 +792,19 @@ static inline int sd_balance_for_package_power(void)
 	return 0;
 }
 
+/*
+ * Optimise SD flags for power savings:
+ * SD_BALANCE_NEWIDLE helps agressive task consolidation and power savings.
+ * Keep default SD flags if sched_{smt,mc}_power_saving=0
+ */
+
+static inline int sd_power_saving_flags(void)
+{
+	if (sched_mc_power_savings | sched_smt_power_savings)
+		return SD_BALANCE_NEWIDLE;
+
+	return 0;
+}
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */

commit afb8a9b70b86866a60e08b2956ae4e1406390336
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Thu Dec 18 23:26:09 2008 +0530

    sched: framework for sched_mc/smt_power_savings=N
    
    Impact: extend range of /sys/devices/system/cpu/sched_mc_power_savings
    
    Currently the sched_mc/smt_power_savings variable is a boolean,
    which either enables or disables topology based power savings.
    This patch extends the behaviour of the variable from boolean to
    multivalued, such that based on the value, we decide how
    aggressively do we want to perform powersavings balance at
    appropriate sched domain based on topology.
    
    Variable levels of power saving tunable would benefit end user to
    match the required level of power savings vs performance
    trade-off depending on the system configuration and workloads.
    
    This version makes the sched_mc_power_savings global variable to
    take more values (0,1,2).  Later versions can have a single
    tunable called sched_power_savings instead of
    sched_{mc,smt}_power_savings.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1210fb0e45ff..a96726658eca 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -763,6 +763,17 @@ enum cpu_idle_type {
 #define SD_SERIALIZE		1024	/* Only a single load balancing instance */
 #define SD_WAKE_IDLE_FAR	2048	/* Gain latency sacrificing cache hit */
 
+enum powersavings_balance_level {
+	POWERSAVINGS_BALANCE_NONE = 0,  /* No power saving load balance */
+	POWERSAVINGS_BALANCE_BASIC,	/* Fill one thread/core/package
+					 * first for long running threads
+					 */
+	POWERSAVINGS_BALANCE_WAKEUP,	/* Also bias task wakeups to semi-idle
+					 * cpu package for power savings
+					 */
+	MAX_POWERSAVINGS_BALANCE_LEVELS
+};
+
 extern int sched_mc_power_savings, sched_smt_power_savings;
 
 static inline int sd_balance_for_mc_power(void)

commit 716707b29906e1d8d190defe3d646610b097a861
Author: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
Date:   Thu Dec 18 23:26:02 2008 +0530

    sched: convert BALANCE_FOR_xx_POWER to inline functions
    
    Impact: cleanup
    
    BALANCE_FOR_MC_POWER and similar macros defined in sched.h are
    not constants and have various condition checks and significant
    amount of code that is not suitable to be contain in a macro.
    Also there could be side effects on the expressions passed to
    some of them like test_sd_parent().
    
    This patch converts all complex macros related to power savings
    balance to inline functions.
    
    Signed-off-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4240f6bfa812..1210fb0e45ff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -763,15 +763,23 @@ enum cpu_idle_type {
 #define SD_SERIALIZE		1024	/* Only a single load balancing instance */
 #define SD_WAKE_IDLE_FAR	2048	/* Gain latency sacrificing cache hit */
 
-#define BALANCE_FOR_MC_POWER	\
-	(sched_smt_power_savings ? SD_POWERSAVINGS_BALANCE : 0)
+extern int sched_mc_power_savings, sched_smt_power_savings;
+
+static inline int sd_balance_for_mc_power(void)
+{
+	if (sched_smt_power_savings)
+		return SD_POWERSAVINGS_BALANCE;
 
-#define BALANCE_FOR_PKG_POWER	\
-	((sched_mc_power_savings || sched_smt_power_savings) ?	\
-	 SD_POWERSAVINGS_BALANCE : 0)
+	return 0;
+}
 
-#define test_sd_parent(sd, flag)	((sd->parent &&		\
-					 (sd->parent->flags & flag)) ? 1 : 0)
+static inline int sd_balance_for_package_power(void)
+{
+	if (sched_mc_power_savings | sched_smt_power_savings)
+		return SD_POWERSAVINGS_BALANCE;
+
+	return 0;
+}
 
 
 struct sched_group {
@@ -1399,6 +1407,15 @@ struct task_struct {
 #endif
 };
 
+/* Test a flag in parent sched domain */
+static inline int test_sd_parent(struct sched_domain *sd, int flag)
+{
+	if (sd->parent && (sd->parent->flags & flag))
+		return 1;
+
+	return 0;
+}
+
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
  * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
@@ -2256,8 +2273,6 @@ __trace_special(void *__tr, void *__data,
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
-extern int sched_mc_power_savings, sched_smt_power_savings;
-
 extern void normalize_rt_tasks(void);
 
 #ifdef CONFIG_GROUP_SCHED

commit 9c2c48020ec0dd6ecd27e5a1298f73b40d85a595
Author: Ken Chen <kenchen@google.com>
Date:   Tue Dec 16 23:41:22 2008 -0800

    schedstat: consolidate per-task cpu runtime stats
    
    Impact: simplify code
    
    When we turn on CONFIG_SCHEDSTATS, per-task cpu runtime is accumulated
    twice. Once in task->se.sum_exec_runtime and once in sched_info.cpu_time.
    These two stats are exactly the same.
    
    Given that task->se.sum_exec_runtime is always accumulated by the core
    scheduler, sched_info can reuse that data instead of duplicate the accounting.
    
    Signed-off-by: Ken Chen <kenchen@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8cccd6dc5d66..2d1e840ddd35 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -670,8 +670,7 @@ struct reclaim_state;
 struct sched_info {
 	/* cumulative counters */
 	unsigned long pcount;	      /* # of times run on this cpu */
-	unsigned long long cpu_time,  /* time spent on the cpu */
-			   run_delay; /* time spent waiting on a runqueue */
+	unsigned long long run_delay; /* time spent waiting on a runqueue */
 
 	/* timestamps */
 	unsigned long long last_arrival,/* when we last ran on a cpu */

commit 6c594c21fcb02c662f11c97be4d7d2b73060a205
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Dec 14 12:34:15 2008 +0100

    perfcounters: add task migrations counter
    
    Impact: add new feature, new sw counter
    
    Add a counter that counts the number of cross-CPU migrations a
    task is suffering.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4c530278391b..2e15be8fc792 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1014,6 +1014,8 @@ struct sched_entity {
 	u64			last_wakeup;
 	u64			avg_overlap;
 
+	u64			nr_migrations;
+
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;
@@ -1029,7 +1031,6 @@ struct sched_entity {
 	u64			exec_max;
 	u64			slice_max;
 
-	u64			nr_migrations;
 	u64			nr_migrations_cold;
 	u64			nr_failed_migrations_affine;
 	u64			nr_failed_migrations_running;

commit 45ab6b0c76d0e4cce5bd608ccf97b0f6b20f18df
Merge: 81444a799550 d65bd5ecb2bd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 12 13:48:57 2008 +0100

    Merge branch 'sched/core' into cpus4096
    
    Conflicts:
            include/linux/ftrace.h
            kernel/sched.c

commit 81444a799550214f549caf579cf65a0ca55e70b7
Merge: a64d31baed10 da485e0cb167
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 12 12:43:05 2008 +0100

    Merge branch 'tracing/fastboot' into cpus4096

commit c1dfdc7597d051b09555d4ae2acb90403e238746
Merge: efbe027e95dc 8b1fae4e4200
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 12 10:29:35 2008 +0100

    Merge commit 'v2.6.28-rc8' into sched/core

commit c2724775ce57c98b8af9694857b941dc61056516
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Thu Dec 11 13:49:59 2008 +0100

    x86, bts: provide in-kernel branch-trace interface
    
    Impact: cleanup
    
    Move the BTS bits from ptrace.c into ds.c.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b81fc5f7731..dc5ea65dc716 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1176,6 +1176,7 @@ struct task_struct {
 	 * The buffer to hold the BTS data.
 	 */
 	void *bts_buffer;
+	size_t bts_size;
 #endif /* CONFIG_X86_PTRACE_BTS */
 
 	/* PID/PID hash table linkage. */

commit 0793a61d4df8daeac6492dbf8d2f3e5713caae5e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 4 20:12:29 2008 +0100

    performance counters: core code
    
    Implement the core kernel bits of Performance Counters subsystem.
    
    The Linux Performance Counter subsystem provides an abstraction of
    performance counter hardware capabilities. It provides per task and per
    CPU counters, and it provides event capabilities on top of those.
    
    Performance counters are accessed via special file descriptors.
    There's one file descriptor per virtual counter used.
    
    The special file descriptor is opened via the perf_counter_open()
    system call:
    
     int
     perf_counter_open(u32 hw_event_type,
                       u32 hw_event_period,
                       u32 record_type,
                       pid_t pid,
                       int cpu);
    
    The syscall returns the new fd. The fd can be used via the normal
    VFS system calls: read() can be used to read the counter, fcntl()
    can be used to set the blocking mode, etc.
    
    Multiple counters can be kept open at a time, and the counters
    can be poll()ed.
    
    See more details in Documentation/perf-counters.txt.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 55e30d114477..4c530278391b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -71,6 +71,7 @@ struct sched_param {
 #include <linux/fs_struct.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
+#include <linux/perf_counter.h>
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>
@@ -1326,6 +1327,7 @@ struct task_struct {
 	struct list_head pi_state_list;
 	struct futex_pi_state *pi_state_cache;
 #endif
+	struct perf_counter_context perf_counter_ctx;
 #ifdef CONFIG_NUMA
 	struct mempolicy *mempolicy;
 	short il_next;
@@ -2285,6 +2287,13 @@ static inline void inc_syscw(struct task_struct *tsk)
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
+/*
+ * Call the function if the target task is executing on a CPU right now:
+ */
+extern void task_oncpu_function_call(struct task_struct *p,
+				     void (*func) (void *info), void *info);
+
+
 #ifdef CONFIG_MM_OWNER
 extern void mm_update_next_owner(struct mm_struct *mm);
 extern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);

commit 380c4b1411ccd6885f92b2c8ceb08433a720f44e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 6 03:43:41 2008 +0100

    tracing/function-graph-tracer: append the tracing_graph_flag
    
    Impact: Provide a way to pause the function graph tracer
    
    As suggested by Steven Rostedt, the previous patch that prevented from
    spinlock function tracing shouldn't use the raw_spinlock to fix it.
    It's much better to follow lockdep with normal spinlock, so this patch
    adds a new flag for each task to make the function graph tracer able
    to be paused. We also can send an ftrace_printk whithout worrying of
    the irrelevant traced spinlock during insertion.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4c152e0acc9e..4b81fc5f7731 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1379,6 +1379,8 @@ struct task_struct {
 	 * because of depth overrun.
 	 */
 	atomic_t trace_overrun;
+	/* Pause for the tracing */
+	atomic_t tracing_graph_pause;
 #endif
 #ifdef CONFIG_TRACING
 	/* state flags for use by tracers */

commit ea4e2bc4d9f7370e57a343ccb5e7c0ad3222ec3c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 15:36:57 2008 -0500

    ftrace: graph of a single function
    
    This patch adds the file:
    
       /debugfs/tracing/set_graph_function
    
    which can be used along with the function graph tracer.
    
    When this file is empty, the function graph tracer will act as
    usual. When the file has a function in it, the function graph
    tracer will only trace that function.
    
    For example:
    
     # echo blk_unplug > /debugfs/tracing/set_graph_function
     # cat /debugfs/tracing/trace
     [...]
     ------------------------------------------
     | 2)  make-19003  =>  kjournald-2219
     ------------------------------------------
    
     2)               |  blk_unplug() {
     2)               |    dm_unplug_all() {
     2)               |      dm_get_table() {
     2)      1.381 us |        _read_lock();
     2)      0.911 us |        dm_table_get();
     2)      1. 76 us |        _read_unlock();
     2) +   12.912 us |      }
     2)               |      dm_table_unplug_all() {
     2)               |        blk_unplug() {
     2)      0.778 us |          generic_unplug_device();
     2)      2.409 us |        }
     2)      5.992 us |      }
     2)      0.813 us |      dm_table_put();
     2) +   29. 90 us |    }
     2) +   34.532 us |  }
    
    You can add up to 32 functions into this file. Currently we limit it
    to 32, but this may change with later improvements.
    
    To add another function, use the append '>>':
    
      # echo sys_read >> /debugfs/tracing/set_graph_function
      # cat /debugfs/tracing/set_graph_function
      blk_unplug
      sys_read
    
    Using the '>' will clear out the function and write anew:
    
      # echo sys_write > /debug/tracing/set_graph_function
      # cat /debug/tracing/set_graph_function
      sys_write
    
    Note, if you have function graph running while doing this, the small
    time between clearing it and updating it will cause the graph to
    record all functions. This should not be an issue because after
    it sets the filter, only those functions will be recorded from then on.
    If you need to only record a particular function then set this
    file first before starting the function graph tracer. In the future
    this side effect may be corrected.
    
    The set_graph_function file is similar to the set_ftrace_filter but
    it does not take wild cards nor does it allow for more than one
    function to be set with a single write. There is no technical reason why
    this is the case, I just do not have the time yet to implement that.
    
    Note, dynamic ftrace must be enabled for this to appear because it
    uses the dynamic ftrace records to match the name to the mcount
    call sites.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2d0a93c31228..4c152e0acc9e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1380,6 +1380,10 @@ struct task_struct {
 	 */
 	atomic_t trace_overrun;
 #endif
+#ifdef CONFIG_TRACING
+	/* state flags for use by tracers */
+	unsigned long trace;
+#endif
 };
 
 /*

commit b8307db2477f9c551e54e0c7b643ea349a3349cd
Merge: f0461d0146ee 061e41fdb504
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 4 09:07:19 2008 +0100

    Merge commit 'v2.6.28-rc7' into tracing/core

commit ec98ce480ada787f2cfbd696980ff3564415505b
Merge: 3496f92beb9a feaf3848a813
Author: James Morris <jmorris@namei.org>
Date:   Thu Dec 4 17:16:36 2008 +1100

    Merge branch 'master' into next
    
    Conflicts:
            fs/nfsd/nfs4recover.c
    
    Manually fixed above to use new creds API functions, e.g.
    nfs4_save_creds().
    
    Signed-off-by: James Morris <jmorris@namei.org>

commit a64d31baed104be25305e9c71585d3ea4ee9a418
Merge: 1c39194878c0 061e41fdb504
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Dec 2 20:09:50 2008 +0100

    Merge branch 'linus' into cpus4096
    
    Conflicts:
            kernel/trace/ring_buffer.c

commit 7ef9964e6d1b911b78709f144000aacadd0ebc21
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Mon Dec 1 13:13:55 2008 -0800

    epoll: introduce resource usage limits
    
    It has been thought that the per-user file descriptors limit would also
    limit the resources that a normal user can request via the epoll
    interface.  Vegard Nossum reported a very simple program (a modified
    version attached) that can make a normal user to request a pretty large
    amount of kernel memory, well within the its maximum number of fds.  To
    solve such problem, default limits are now imposed, and /proc based
    configuration has been introduced.  A new directory has been created,
    named /proc/sys/fs/epoll/ and inside there, there are two configuration
    points:
    
      max_user_instances = Maximum number of devices - per user
    
      max_user_watches   = Maximum number of "watched" fds - per user
    
    The current default for "max_user_watches" limits the memory used by epoll
    to store "watches", to 1/32 of the amount of the low RAM.  As example, a
    256MB 32bit machine, will have "max_user_watches" set to roughly 90000.
    That should be enough to not break existing heavy epoll users.  The
    default value for "max_user_instances" is set to 128, that should be
    enough too.
    
    This also changes the userspace, because a new error code can now come out
    from EPOLL_CTL_ADD (-ENOSPC).  The EMFILE from epoll_create() was already
    listed, so that should be ok.
    
    [akpm@linux-foundation.org: use get_current_user()]
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: <stable@kernel.org>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Reported-by: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 644ffbda17ca..55e30d114477 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -630,6 +630,10 @@ struct user_struct {
 	atomic_t inotify_watches; /* How many inotify watches does this user have? */
 	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
 #endif
+#ifdef CONFIG_EPOLL
+	atomic_t epoll_devs;	/* The number of epoll descriptors currently open */
+	atomic_t epoll_watches;	/* The number of file descriptors currently watched */
+#endif
 #ifdef CONFIG_POSIX_MQUEUE
 	/* protected by mq_lock	*/
 	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */

commit 6c415b9234a8c71f290e5d4fddc467f103f32719
Author: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
Date:   Mon Dec 1 20:49:05 2008 +0530

    sched: add uid information to sched_debug for CONFIG_USER_SCHED
    
    Impact: extend information in /proc/sched_debug
    
    This patch adds uid information in sched_debug for CONFIG_USER_SCHED
    
    Signed-off-by: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7a69c4d224ee..d8733f07d80b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2218,6 +2218,7 @@ extern void normalize_rt_tasks(void);
 extern struct task_group init_task_group;
 #ifdef CONFIG_USER_SCHED
 extern struct task_group root_task_group;
+extern void set_tg_uid(struct user_struct *user);
 #endif
 
 extern struct task_group *sched_create_group(struct task_group *parent);

commit fb52607afcd0629776f1dc9e657647ceae81dd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 25 21:07:04 2008 +0100

    tracing/function-return-tracer: change the name into function-graph-tracer
    
    Impact: cleanup
    
    This patch changes the name of the "return function tracer" into
    function-graph-tracer which is a more suitable name for a tracing
    which makes one able to retrieve the ordered call stack during
    the code flow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d02a0ca70ee9..7ad48f2a2758 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1365,7 +1365,7 @@ struct task_struct {
 	unsigned long default_timer_slack_ns;
 
 	struct list_head	*scm_work_list;
-#ifdef CONFIG_FUNCTION_RET_TRACER
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	/* Index of current stored adress in ret_stack */
 	int curr_ret_stack;
 	/* Stack of return addresses for return function tracing */

commit 6abb11aecd888d1da6276399380b7355f127c006
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Nov 25 09:05:27 2008 +0100

    x86, bts, ptrace: move BTS buffer allocation from ds.c into ptrace.c
    
    Impact: restructure DS memory allocation to be done by the usage site of DS
    
    Require pre-allocated buffers in ds.h.
    
    Move the BTS buffer allocation for ptrace into ptrace.c.
    The pointer to the allocated buffer is stored in the traced task's
    task_struct together with the handle returned by ds_request_bts().
    
    Removes memory accounting code.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a9780eaa6737..d02a0ca70ee9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1168,6 +1168,10 @@ struct task_struct {
 	 * This field actually belongs to the ptracer task.
 	 */
 	struct bts_tracer *bts;
+	/*
+	 * The buffer to hold the BTS data.
+	 */
+	void *bts_buffer;
 #endif /* CONFIG_X86_PTRACE_BTS */
 
 	/* PID/PID hash table linkage. */

commit ca0002a179bfa532d009a9272d619732872c49bd
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Nov 25 09:01:25 2008 +0100

    x86, bts: base in-kernel ds interface on handles
    
    Impact: generalize the DS code to shared buffers
    
    Change the in-kernel ds.h interface to identify the tracer via a
    handle returned on ds_request_~().
    
    Tracers used to be identified via their task_struct.
    
    The changes are required to allow DS to be shared between different
    tasks, which is needed for perfmon2 and for ftrace.
    
    For ptrace, the handle is stored in the traced task's task_struct.
    This should probably go into a (arch-specific) ptrace context some
    time.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bee1e93c95ad..a9780eaa6737 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -96,6 +96,7 @@ struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
 struct bio;
+struct bts_tracer;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1161,6 +1162,14 @@ struct task_struct {
 	struct list_head ptraced;
 	struct list_head ptrace_entry;
 
+#ifdef CONFIG_X86_PTRACE_BTS
+	/*
+	 * This is the tracer handle for the ptrace BTS extension.
+	 * This field actually belongs to the ptracer task.
+	 */
+	struct bts_tracer *bts;
+#endif /* CONFIG_X86_PTRACE_BTS */
+
 	/* PID/PID hash table linkage. */
 	struct pid_link pids[PIDTYPE_MAX];
 	struct list_head thread_group;

commit 18b6e0414e42d95183f07d8177e3ff0241abd825
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Wed Oct 15 16:38:45 2008 -0500

    User namespaces: set of cleanups (v2)
    
    The user_ns is moved from nsproxy to user_struct, so that a struct
    cred by itself is sufficient to determine access (which it otherwise
    would not be).  Corresponding ecryptfs fixes (by David Howells) are
    here as well.
    
    Fix refcounting.  The following rules now apply:
            1. The task pins the user struct.
            2. The user struct pins its user namespace.
            3. The user namespace pins the struct user which created it.
    
    User namespaces are cloned during copy_creds().  Unsharing a new user_ns
    is no longer possible.  (We could re-add that, but it'll cause code
    duplication and doesn't seem useful if PAM doesn't need to clone user
    namespaces).
    
    When a user namespace is created, its first user (uid 0) gets empty
    keyrings and a clean group_info.
    
    This incorporates a previous patch by David Howells.  Here
    is his original patch description:
    
    >I suggest adding the attached incremental patch.  It makes the following
    >changes:
    >
    > (1) Provides a current_user_ns() macro to wrap accesses to current's user
    >     namespace.
    >
    > (2) Fixes eCryptFS.
    >
    > (3) Renames create_new_userns() to create_user_ns() to be more consistent
    >     with the other associated functions and because the 'new' in the name is
    >     superfluous.
    >
    > (4) Moves the argument and permission checks made for CLONE_NEWUSER to the
    >     beginning of do_fork() so that they're done prior to making any attempts
    >     at allocation.
    >
    > (5) Calls create_user_ns() after prepare_creds(), and gives it the new creds
    >     to fill in rather than have it return the new root user.  I don't imagine
    >     the new root user being used for anything other than filling in a cred
    >     struct.
    >
    >     This also permits me to get rid of a get_uid() and a free_uid(), as the
    >     reference the creds were holding on the old user_struct can just be
    >     transferred to the new namespace's creator pointer.
    >
    > (6) Makes create_user_ns() reset the UIDs and GIDs of the creds under
    >     preparation rather than doing it in copy_creds().
    >
    >David
    
    >Signed-off-by: David Howells <dhowells@redhat.com>
    
    Changelog:
            Oct 20: integrate dhowells comments
                    1. leave thread_keyring alone
                    2. use current_user_ns() in set_user()
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2036e9f26020..7f8015a3082e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -638,6 +638,7 @@ struct user_struct {
 	/* Hash table maintenance information */
 	struct hlist_node uidhash_node;
 	uid_t uid;
+	struct user_namespace *user_ns;
 
 #ifdef CONFIG_USER_SCHED
 	struct task_group *tg;

commit 96f874e26428ab5d2db681c100210c254775e154
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 25 02:35:14 2008 +1030

    sched: convert remaining old-style cpumask operators
    
    Impact: Trivial API conversion
    
      NR_CPUS -> nr_cpu_ids
      cpumask_t -> struct cpumask
      sizeof(cpumask_t) -> cpumask_size()
      cpumask_a = cpumask_b -> cpumask_copy(&cpumask_a, &cpumask_b)
    
      cpu_set() -> cpumask_set_cpu()
      first_cpu() -> cpumask_first()
      cpumask_of_cpu() -> cpumask_of()
      cpus_* -> cpumask_*
    
    There are some FIXMEs where we all archs to complete infrastructure
    (patches have been sent):
    
      cpu_coregroup_map -> cpu_coregroup_mask
      node_to_cpumask* -> cpumask_of_node
    
    There is also one FIXME where we pass an array of cpumasks to
    partition_sched_domains(): this implies knowing the definition of
    'struct cpumask' and the size of a cpumask.  This will be fixed in a
    future patch.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1e33e2cb7f8c..4b7b0187374c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -879,7 +879,7 @@ static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 	return to_cpumask(sd->span);
 }
 
-extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
+extern void partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
 				    struct sched_domain_attr *dattr_new);
 extern int arch_reinit_sched_domains(void);
 
@@ -888,7 +888,7 @@ extern int arch_reinit_sched_domains(void);
 struct sched_domain_attr;
 
 static inline void
-partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
+partition_sched_domains(int ndoms_new, struct cpumask *doms_new,
 			struct sched_domain_attr *dattr_new)
 {
 }
@@ -970,7 +970,7 @@ struct sched_class {
 	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
-				 const cpumask_t *newmask);
+				 const struct cpumask *newmask);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
@@ -1612,12 +1612,12 @@ extern cputime_t task_gtime(struct task_struct *p);
 
 #ifdef CONFIG_SMP
 extern int set_cpus_allowed_ptr(struct task_struct *p,
-				const cpumask_t *new_mask);
+				const struct cpumask *new_mask);
 #else
 static inline int set_cpus_allowed_ptr(struct task_struct *p,
-				       const cpumask_t *new_mask)
+				       const struct cpumask *new_mask)
 {
-	if (!cpu_isset(0, *new_mask))
+	if (!cpumask_test_cpu(0, new_mask))
 		return -EINVAL;
 	return 0;
 }
@@ -2230,8 +2230,8 @@ __trace_special(void *__tr, void *__data,
 }
 #endif
 
-extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);
-extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
+extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
+extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
 extern int sched_mc_power_savings, sched_smt_power_savings;
 

commit 6a7b3dc3440f7b5a9b67594af01ed562cdeb41e4
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 25 02:35:04 2008 +1030

    sched: convert nohz_cpu_mask to cpumask_var_t.
    
    Impact: (future) size reduction for large NR_CPUS.
    
    Dynamically allocating cpumasks (when CONFIG_CPUMASK_OFFSTACK) saves
    space for small nr_cpu_ids but big CONFIG_NR_CPUS.  cpumask_var_t
    is just a struct cpumask for !CONFIG_CPUMASK_OFFSTACK.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c5be6c6bc741..1e33e2cb7f8c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -249,7 +249,7 @@ extern void init_idle_bootup_task(struct task_struct *idle);
 extern int runqueue_is_locked(void);
 extern void task_rq_unlock_wait(struct task_struct *p);
 
-extern cpumask_t nohz_cpu_mask;
+extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern int select_nohz_load_balancer(int cpu);
 #else

commit 6c99e9ad47d9c082bd096f42fb49e397b05d58a8
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 25 02:35:04 2008 +1030

    sched: convert struct sched_group/sched_domain cpumask_ts to variable bitmaps
    
    Impact: (future) size reduction for large NR_CPUS.
    
    We move the 'cpumask' member of sched_group to the end, so when we
    kmalloc it we can do a minimal allocation: saves space for small
    nr_cpu_ids but big CONFIG_NR_CPUS.  Similar trick for 'span' in
    sched_domain.
    
    This isn't quite as good as converting to a cpumask_var_t, as some
    sched_groups are actually static, but it's safer: we don't have to
    figure out where to call alloc_cpumask_var/free_cpumask_var.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2b95aa9f779b..c5be6c6bc741 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -771,7 +771,6 @@ enum cpu_idle_type {
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
-	cpumask_t cpumask;
 
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
@@ -784,11 +783,13 @@ struct sched_group {
 	 * (see include/linux/reciprocal_div.h)
 	 */
 	u32 reciprocal_cpu_power;
+
+	unsigned long cpumask[];
 };
 
 static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 {
-	return &sg->cpumask;
+	return to_cpumask(sg->cpumask);
 }
 
 enum sched_domain_level {
@@ -814,7 +815,6 @@ struct sched_domain {
 	struct sched_domain *parent;	/* top domain must be null terminated */
 	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
-	cpumask_t span;			/* span of all CPUs in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */
@@ -869,11 +869,14 @@ struct sched_domain {
 #ifdef CONFIG_SCHED_DEBUG
 	char *name;
 #endif
+
+	/* span of all CPUs in this domain */
+	unsigned long span[];
 };
 
 static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
 {
-	return &sd->span;
+	return to_cpumask(sd->span);
 }
 
 extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,

commit 758b2cdc6f6a22c702bd8f2344382fb1270b2161
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Nov 25 02:35:04 2008 +1030

    sched: wrap sched_group and sched_domain cpumask accesses.
    
    Impact: trivial wrap of member accesses
    
    This eases the transition in the next patch.
    
    We also get rid of a temporary cpumask in find_idlest_cpu() thanks to
    for_each_cpu_and, and sched_balance_self() due to getting weight before
    setting sd to NULL.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ce5c603c51a..2b95aa9f779b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -786,6 +786,11 @@ struct sched_group {
 	u32 reciprocal_cpu_power;
 };
 
+static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
+{
+	return &sg->cpumask;
+}
+
 enum sched_domain_level {
 	SD_LV_NONE = 0,
 	SD_LV_SIBLING,
@@ -866,6 +871,11 @@ struct sched_domain {
 #endif
 };
 
+static inline struct cpumask *sched_domain_span(struct sched_domain *sd)
+{
+	return &sd->span;
+}
+
 extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
 				    struct sched_domain_attr *dattr_new);
 extern int arch_reinit_sched_domains(void);

commit 943f3d030003e1fa5f77647328e805441213bf49
Merge: 64b7482de253 b19b3c74c7bb 6f893fb2e892
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 24 17:46:57 2008 +0100

    Merge branches 'sched/core', 'core/core' and 'tracing/core' into cpus4096

commit 64b7482de253c10efa2589a6212e3d2093a3efc7
Merge: 957ad0166e9f 50ee91765e25
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 24 17:37:12 2008 +0100

    Merge branch 'sched/rt' into sched/core

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c8e0db464206..bee1e93c95ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1352,6 +1352,17 @@ struct task_struct {
 	unsigned long default_timer_slack_ns;
 
 	struct list_head	*scm_work_list;
+#ifdef CONFIG_FUNCTION_RET_TRACER
+	/* Index of current stored adress in ret_stack */
+	int curr_ret_stack;
+	/* Stack of return addresses for return function tracing */
+	struct ftrace_ret_stack	*ret_stack;
+	/*
+	 * Number of functions that haven't been traced
+	 * because of depth overrun.
+	 */
+	atomic_t trace_overrun;
+#endif
 };
 
 /*
@@ -2006,18 +2017,6 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 {
 	*task_thread_info(p) = *task_thread_info(org);
 	task_thread_info(p)->task = p;
-
-#ifdef CONFIG_FUNCTION_RET_TRACER
-	/*
-	 * When fork() creates a child process, this function is called.
-	 * But the child task may not inherit the return adresses traced
-	 * by the return function tracer because it will directly execute
-	 * in userspace and will not return to kernel functions its parent
-	 * used.
-	 */
-	task_thread_info(p)->curr_ret_stack = -1;
-	atomic_set(&task_thread_info(p)->trace_overrun, 0);
-#endif
 }
 
 static inline unsigned long *end_of_stack(struct task_struct *p)

commit 0231022cc32d5f2e7f3c06b75691dda0ad6aec33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 17 03:22:41 2008 +0100

    tracing/function-return-tracer: add the overrun field
    
    Impact: help to find the better depth of trace
    
    We decided to arbitrary define the depth of function return trace as
    "20". Perhaps this is not enough. To help finding an optimal depth, we
    measure now the overrun: the number of functions that have been missed
    for the current thread. By default this is not displayed, we have to
    do set a particular flag on the return tracer: echo overrun >
    /debug/tracing/trace_options And the overrun will be printed on the
    right.
    
    As the trace shows below, the current 20 depth is not enough.
    
    update_wall_time+0x37f/0x8c0 -> update_xtime_cache (345 ns) (Overruns: 2838)
    update_wall_time+0x384/0x8c0 -> clocksource_get_next (1141 ns) (Overruns: 2838)
    do_timer+0x23/0x100 -> update_wall_time (3882 ns) (Overruns: 2838)
    tick_do_update_jiffies64+0xbf/0x160 -> do_timer (5339 ns) (Overruns: 2838)
    tick_sched_timer+0x6a/0xf0 -> tick_do_update_jiffies64 (7209 ns) (Overruns: 2838)
    vgacon_set_cursor_size+0x98/0x120 -> native_io_delay (2613 ns) (Overruns: 274)
    vgacon_cursor+0x16e/0x1d0 -> vgacon_set_cursor_size (33151 ns) (Overruns: 274)
    set_cursor+0x5f/0x80 -> vgacon_cursor (36432 ns) (Overruns: 274)
    con_flush_chars+0x34/0x40 -> set_cursor (38790 ns) (Overruns: 274)
    release_console_sem+0x1ec/0x230 -> up (721 ns) (Overruns: 274)
    release_console_sem+0x225/0x230 -> wake_up_klogd (316 ns) (Overruns: 274)
    con_flush_chars+0x39/0x40 -> release_console_sem (2996 ns) (Overruns: 274)
    con_write+0x22/0x30 -> con_flush_chars (46067 ns) (Overruns: 274)
    n_tty_write+0x1cc/0x360 -> con_write (292670 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2a/0x90 -> native_apic_mem_write (330 ns) (Overruns: 274)
    irq_enter+0x17/0x70 -> idle_cpu (413 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2f/0x90 -> irq_enter (1525 ns) (Overruns: 274)
    ktime_get_ts+0x40/0x70 -> getnstimeofday (465 ns) (Overruns: 274)
    ktime_get_ts+0x60/0x70 -> set_normalized_timespec (436 ns) (Overruns: 274)
    ktime_get+0x16/0x30 -> ktime_get_ts (2501 ns) (Overruns: 274)
    hrtimer_interrupt+0x77/0x1a0 -> ktime_get (3439 ns) (Overruns: 274)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 61c8cc36028a..c8e0db464206 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2016,6 +2016,7 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 	 * used.
 	 */
 	task_thread_info(p)->curr_ret_stack = -1;
+	atomic_set(&task_thread_info(p)->trace_overrun, 0);
 #endif
 }
 

commit 2b828925652340277a889cbc11b2d0637f7cdaf7
Merge: 3a3b7ce93369 58e20d8d344b
Author: James Morris <jmorris@namei.org>
Date:   Fri Nov 14 11:29:12 2008 +1100

    Merge branch 'master' into next
    
    Conflicts:
            security/keys/internal.h
            security/keys/process_keys.c
            security/keys/request_key.c
    
    Fixed conflicts above by using the non 'tsk' versions.
    
    Signed-off-by: James Morris <jmorris@namei.org>

commit 3b11a1decef07c19443d24ae926982bc8ec9f4c0
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:26 2008 +1100

    CRED: Differentiate objective and effective subjective credentials on a task
    
    Differentiate the objective and real subjective credentials from the effective
    subjective credentials on a task by introducing a second credentials pointer
    into the task_struct.
    
    task_struct::real_cred then refers to the objective and apparent real
    subjective credentials of a task, as perceived by the other tasks in the
    system.
    
    task_struct::cred then refers to the effective subjective credentials of a
    task, as used by that task when it's actually running.  These are not visible
    to the other tasks in the system.
    
    __task_cred(task) then refers to the objective/real credentials of the task in
    question.
    
    current_cred() refers to the effective subjective credentials of the current
    task.
    
    prepare_creds() uses the objective creds as a base and commit_creds() changes
    both pointers in the task_struct (indeed commit_creds() requires them to be the
    same).
    
    override_creds() and revert_creds() change the subjective creds pointer only,
    and the former returns the old subjective creds.  These are used by NFSD,
    faccessat() and do_coredump(), and will by used by CacheFiles.
    
    In SELinux, current_has_perm() is provided as an alternative to
    task_has_perm().  This uses the effective subjective context of current,
    whereas task_has_perm() uses the objective/real context of the subject.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 121d655e460d..3443123b0709 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1145,7 +1145,10 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
-	const struct cred *cred;	/* actual/objective task credentials (COW) */
+	const struct cred *real_cred;	/* objective and real subjective task
+					 * credentials (COW) */
+	const struct cred *cred;	/* effective (overridable) subjective task
+					 * credentials (COW) */
 	struct mutex cred_exec_mutex;	/* execve vs ptrace cred calculation mutex */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path

commit d84f4f992cbd76e8f39c488cf0c5d123843923b1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:23 2008 +1100

    CRED: Inaugurate COW credentials
    
    Inaugurate copy-on-write credentials management.  This uses RCU to manage the
    credentials pointer in the task_struct with respect to accesses by other tasks.
    A process may only modify its own credentials, and so does not need locking to
    access or modify its own credentials.
    
    A mutex (cred_replace_mutex) is added to the task_struct to control the effect
    of PTRACE_ATTACHED on credential calculations, particularly with respect to
    execve().
    
    With this patch, the contents of an active credentials struct may not be
    changed directly; rather a new set of credentials must be prepared, modified
    and committed using something like the following sequence of events:
    
            struct cred *new = prepare_creds();
            int ret = blah(new);
            if (ret < 0) {
                    abort_creds(new);
                    return ret;
            }
            return commit_creds(new);
    
    There are some exceptions to this rule: the keyrings pointed to by the active
    credentials may be instantiated - keyrings violate the COW rule as managing
    COW keyrings is tricky, given that it is possible for a task to directly alter
    the keys in a keyring in use by another task.
    
    To help enforce this, various pointers to sets of credentials, such as those in
    the task_struct, are declared const.  The purpose of this is compile-time
    discouragement of altering credentials through those pointers.  Once a set of
    credentials has been made public through one of these pointers, it may not be
    modified, except under special circumstances:
    
      (1) Its reference count may incremented and decremented.
    
      (2) The keyrings to which it points may be modified, but not replaced.
    
    The only safe way to modify anything else is to create a replacement and commit
    using the functions described in Documentation/credentials.txt (which will be
    added by a later patch).
    
    This patch and the preceding patches have been tested with the LTP SELinux
    testsuite.
    
    This patch makes several logical sets of alteration:
    
     (1) execve().
    
         This now prepares and commits credentials in various places in the
         security code rather than altering the current creds directly.
    
     (2) Temporary credential overrides.
    
         do_coredump() and sys_faccessat() now prepare their own credentials and
         temporarily override the ones currently on the acting thread, whilst
         preventing interference from other threads by holding cred_replace_mutex
         on the thread being dumped.
    
         This will be replaced in a future patch by something that hands down the
         credentials directly to the functions being called, rather than altering
         the task's objective credentials.
    
     (3) LSM interface.
    
         A number of functions have been changed, added or removed:
    
         (*) security_capset_check(), ->capset_check()
         (*) security_capset_set(), ->capset_set()
    
             Removed in favour of security_capset().
    
         (*) security_capset(), ->capset()
    
             New.  This is passed a pointer to the new creds, a pointer to the old
             creds and the proposed capability sets.  It should fill in the new
             creds or return an error.  All pointers, barring the pointer to the
             new creds, are now const.
    
         (*) security_bprm_apply_creds(), ->bprm_apply_creds()
    
             Changed; now returns a value, which will cause the process to be
             killed if it's an error.
    
         (*) security_task_alloc(), ->task_alloc_security()
    
             Removed in favour of security_prepare_creds().
    
         (*) security_cred_free(), ->cred_free()
    
             New.  Free security data attached to cred->security.
    
         (*) security_prepare_creds(), ->cred_prepare()
    
             New. Duplicate any security data attached to cred->security.
    
         (*) security_commit_creds(), ->cred_commit()
    
             New. Apply any security effects for the upcoming installation of new
             security by commit_creds().
    
         (*) security_task_post_setuid(), ->task_post_setuid()
    
             Removed in favour of security_task_fix_setuid().
    
         (*) security_task_fix_setuid(), ->task_fix_setuid()
    
             Fix up the proposed new credentials for setuid().  This is used by
             cap_set_fix_setuid() to implicitly adjust capabilities in line with
             setuid() changes.  Changes are made to the new credentials, rather
             than the task itself as in security_task_post_setuid().
    
         (*) security_task_reparent_to_init(), ->task_reparent_to_init()
    
             Removed.  Instead the task being reparented to init is referred
             directly to init's credentials.
    
             NOTE!  This results in the loss of some state: SELinux's osid no
             longer records the sid of the thread that forked it.
    
         (*) security_key_alloc(), ->key_alloc()
         (*) security_key_permission(), ->key_permission()
    
             Changed.  These now take cred pointers rather than task pointers to
             refer to the security context.
    
     (4) sys_capset().
    
         This has been simplified and uses less locking.  The LSM functions it
         calls have been merged.
    
     (5) reparent_to_kthreadd().
    
         This gives the current thread the same credentials as init by simply using
         commit_thread() to point that way.
    
     (6) __sigqueue_alloc() and switch_uid()
    
         __sigqueue_alloc() can't stop the target task from changing its creds
         beneath it, so this function gets a reference to the currently applicable
         user_struct which it then passes into the sigqueue struct it returns if
         successful.
    
         switch_uid() is now called from commit_creds(), and possibly should be
         folded into that.  commit_creds() should take care of protecting
         __sigqueue_alloc().
    
     (7) [sg]et[ug]id() and co and [sg]et_current_groups.
    
         The set functions now all use prepare_creds(), commit_creds() and
         abort_creds() to build and check a new set of credentials before applying
         it.
    
         security_task_set[ug]id() is called inside the prepared section.  This
         guarantees that nothing else will affect the creds until we've finished.
    
         The calling of set_dumpable() has been moved into commit_creds().
    
         Much of the functionality of set_user() has been moved into
         commit_creds().
    
         The get functions all simply access the data directly.
    
     (8) security_task_prctl() and cap_task_prctl().
    
         security_task_prctl() has been modified to return -ENOSYS if it doesn't
         want to handle a function, or otherwise return the return value directly
         rather than through an argument.
    
         Additionally, cap_task_prctl() now prepares a new set of credentials, even
         if it doesn't end up using it.
    
     (9) Keyrings.
    
         A number of changes have been made to the keyrings code:
    
         (a) switch_uid_keyring(), copy_keys(), exit_keys() and suid_keys() have
             all been dropped and built in to the credentials functions directly.
             They may want separating out again later.
    
         (b) key_alloc() and search_process_keyrings() now take a cred pointer
             rather than a task pointer to specify the security context.
    
         (c) copy_creds() gives a new thread within the same thread group a new
             thread keyring if its parent had one, otherwise it discards the thread
             keyring.
    
         (d) The authorisation key now points directly to the credentials to extend
             the search into rather pointing to the task that carries them.
    
         (e) Installing thread, process or session keyrings causes a new set of
             credentials to be created, even though it's not strictly necessary for
             process or session keyrings (they're shared).
    
    (10) Usermode helper.
    
         The usermode helper code now carries a cred struct pointer in its
         subprocess_info struct instead of a new session keyring pointer.  This set
         of credentials is derived from init_cred and installed on the new process
         after it has been cloned.
    
         call_usermodehelper_setup() allocates the new credentials and
         call_usermodehelper_freeinfo() discards them if they haven't been used.  A
         special cred function (prepare_usermodeinfo_creds()) is provided
         specifically for call_usermodehelper_setup() to call.
    
         call_usermodehelper_setkeys() adjusts the credentials to sport the
         supplied keyring as the new session keyring.
    
    (11) SELinux.
    
         SELinux has a number of changes, in addition to those to support the LSM
         interface changes mentioned above:
    
         (a) selinux_setprocattr() no longer does its check for whether the
             current ptracer can access processes with the new SID inside the lock
             that covers getting the ptracer's SID.  Whilst this lock ensures that
             the check is done with the ptracer pinned, the result is only valid
             until the lock is released, so there's no point doing it inside the
             lock.
    
    (12) is_single_threaded().
    
         This function has been extracted from selinux_setprocattr() and put into
         a file of its own in the lib/ directory as join_session_keyring() now
         wants to use it too.
    
         The code in SELinux just checked to see whether a task shared mm_structs
         with other tasks (CLONE_VM), but that isn't good enough.  We really want
         to know if they're part of the same thread group (CLONE_THREAD).
    
    (13) nfsd.
    
         The NFS server daemon now has to use the COW credentials to set the
         credentials it is going to use.  It really needs to pass the credentials
         down to the functions it calls, but it can't do that until other patches
         in this series have been applied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2913252989b3..121d655e460d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1145,7 +1145,8 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
-	struct cred *cred;	/* actual/objective task credentials */
+	const struct cred *cred;	/* actual/objective task credentials (COW) */
+	struct mutex cred_exec_mutex;	/* execve vs ptrace cred calculation mutex */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
@@ -1720,7 +1721,6 @@ static inline struct user_struct *get_uid(struct user_struct *u)
 	return u;
 }
 extern void free_uid(struct user_struct *);
-extern void switch_uid(struct user_struct *);
 extern void release_uids(struct user_namespace *ns);
 
 #include <asm/current.h>
@@ -1870,6 +1870,8 @@ static inline unsigned long wait_task_inactive(struct task_struct *p,
 #define for_each_process(p) \
 	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
 
+extern bool is_single_threaded(struct task_struct *);
+
 /*
  * Careful: do_each_thread/while_each_thread is a double loop so
  *          'break' will not work as expected - use goto instead.

commit bb952bb98a7e479262c7eb25d5592545a3af147d
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:20 2008 +1100

    CRED: Separate per-task-group keyrings from signal_struct
    
    Separate per-task-group keyrings from signal_struct and dangle their anchor
    from the cred struct rather than the signal_struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 740cf946c8cc..2913252989b3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -571,12 +571,6 @@ struct signal_struct {
 	 */
 	struct rlimit rlim[RLIM_NLIMITS];
 
-	/* keep the process-shared keyrings here so that they do the right
-	 * thing in threads created with CLONE_THREAD */
-#ifdef CONFIG_KEYS
-	struct key *session_keyring;	/* keyring inherited over fork */
-	struct key *process_keyring;	/* keyring private to this process */
-#endif
 #ifdef CONFIG_BSD_PROCESS_ACCT
 	struct pacct_struct pacct;	/* per-process accounting information */
 #endif

commit f1752eec6145c97163dbce62d17cf5d928e28a27
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:17 2008 +1100

    CRED: Detach the credentials from task_struct
    
    Detach the credentials from task_struct, duplicating them in copy_process()
    and releasing them in __put_task_struct().
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c8b92502354d..740cf946c8cc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1151,7 +1151,6 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
-	struct cred __temp_cred __deprecated; /* temporary credentials to be removed */
 	struct cred *cred;	/* actual/objective task credentials */
 
 	char comm[TASK_COMM_LEN]; /* executable name excluding path

commit b6dff3ec5e116e3af6f537d4caedcad6b9e5082a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:16 2008 +1100

    CRED: Separate task security context from task_struct
    
    Separate the task security context from task_struct.  At this point, the
    security data is temporarily embedded in the task_struct with two pointers
    pointing to it.
    
    Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
    entry.S via asm-offsets.
    
    With comment fixes Signed-off-by: Marc Dionne <marc.c.dionne@gmail.com>
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b483f39a7112..c8b92502354d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -660,6 +660,7 @@ extern struct user_struct *find_user(uid_t);
 extern struct user_struct root_user;
 #define INIT_USER (&root_user)
 
+
 struct backing_dev_info;
 struct reclaim_state;
 
@@ -883,38 +884,7 @@ partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
 #endif	/* !CONFIG_SMP */
 
 struct io_context;			/* See blkdev.h */
-#define NGROUPS_SMALL		32
-#define NGROUPS_PER_BLOCK	((unsigned int)(PAGE_SIZE / sizeof(gid_t)))
-struct group_info {
-	int ngroups;
-	atomic_t usage;
-	gid_t small_block[NGROUPS_SMALL];
-	int nblocks;
-	gid_t *blocks[0];
-};
-
-/*
- * get_group_info() must be called with the owning task locked (via task_lock())
- * when task != current.  The reason being that the vast majority of callers are
- * looking at current->group_info, which can not be changed except by the
- * current task.  Changing current->group_info requires the task lock, too.
- */
-#define get_group_info(group_info) do { \
-	atomic_inc(&(group_info)->usage); \
-} while (0)
 
-#define put_group_info(group_info) do { \
-	if (atomic_dec_and_test(&(group_info)->usage)) \
-		groups_free(group_info); \
-} while (0)
-
-extern struct group_info *groups_alloc(int gidsetsize);
-extern void groups_free(struct group_info *group_info);
-extern int set_current_groups(struct group_info *group_info);
-extern int groups_search(struct group_info *group_info, gid_t grp);
-/* access the groups "array" with this macro */
-#define GROUP_AT(gi, i) \
-    ((gi)->blocks[(i)/NGROUPS_PER_BLOCK][(i)%NGROUPS_PER_BLOCK])
 
 #ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
 extern void prefetch_stack(struct task_struct *t);
@@ -1181,17 +1151,9 @@ struct task_struct {
 	struct list_head cpu_timers[3];
 
 /* process credentials */
-	uid_t uid,euid,suid,fsuid;
-	gid_t gid,egid,sgid,fsgid;
-	struct group_info *group_info;
-	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted, cap_bset;
-	struct user_struct *user;
-	unsigned securebits;
-#ifdef CONFIG_KEYS
-	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
-	struct key *request_key_auth;	/* assumed request_key authority */
-	struct key *thread_keyring;	/* keyring private to this thread */
-#endif
+	struct cred __temp_cred __deprecated; /* temporary credentials to be removed */
+	struct cred *cred;	/* actual/objective task credentials */
+
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
 				       it with task_lock())
@@ -1228,9 +1190,6 @@ struct task_struct {
 	int (*notifier)(void *priv);
 	void *notifier_data;
 	sigset_t *notifier_mask;
-#ifdef CONFIG_SECURITY
-	void *security;
-#endif
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL
 	uid_t loginuid;
@@ -1787,9 +1746,6 @@ extern void wake_up_new_task(struct task_struct *tsk,
 extern void sched_fork(struct task_struct *p, int clone_flags);
 extern void sched_dead(struct task_struct *p);
 
-extern int in_group_p(gid_t);
-extern int in_egroup_p(gid_t);
-
 extern void proc_caches_init(void);
 extern void flush_signals(struct task_struct *);
 extern void ignore_signals(struct task_struct *);

commit 60a011c736e7dd09a0b01ca6a051a416f3f52ffb
Merge: d06bbd669539 19b3e9671c5a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 12 10:17:09 2008 +0100

    Merge branch 'tracing/function-return-tracer' into tracing/fastboot

commit 50ee91765e25e7967a7b69cd5cc2bcab85e2eeb8
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Tue Nov 11 18:13:23 2008 +0530

    sched/rt: removed unneeded defintion
    
    Impact: cleanup
    
    This function no longer exists, so remove the defintion.
    
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b483f39a7112..c6bfb34d978e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -258,8 +258,6 @@ static inline int select_nohz_load_balancer(int cpu)
 }
 #endif
 
-extern unsigned long rt_needs_cpu(int cpu);
-
 /*
  * Only dump TASK_* tasks. (0 for all tasks)
  */

commit caf4b323b02a16c92fba449952ac6515ddc76d7a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 07:03:45 2008 +0100

    tracing, x86: add low level support for ftrace return tracing
    
    Impact: add infrastructure for function-return tracing
    
    Add low level support for ftrace return tracing.
    
    This plug-in stores return addresses on the thread_info structure of
    the current task.
    
    The index of the current return address is initialized when the task
    is the first one (init) and when a process forks (the child). It is
    not needed when a task does a sys_execve because after this syscall,
    it still needs to return on the kernel functions it called.
    
    Note that the code of return_to_handler has been suggested by Steven
    Rostedt as almost all of the ideas of improvements in this V3.
    
    For purpose of security, arch/x86/kernel/process_32.c is not traced
    because __switch_to() changes the current task during its execution.
    That could cause inconsistency in the stored return address of this
    function even if I didn't have any crash after testing with tracing on
    this function enabled.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 295b7c756ca6..df77abe860c9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2005,6 +2005,17 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 {
 	*task_thread_info(p) = *task_thread_info(org);
 	task_thread_info(p)->task = p;
+
+#ifdef CONFIG_FUNCTION_RET_TRACER
+	/*
+	 * When fork() creates a child process, this function is called.
+	 * But the child task may not inherit the return adresses traced
+	 * by the return function tracer because it will directly execute
+	 * in userspace and will not return to kernel functions its parent
+	 * used.
+	 */
+	task_thread_info(p)->curr_ret_stack = -1;
+#endif
 }
 
 static inline unsigned long *end_of_stack(struct task_struct *p)

commit ad474caca3e2a0550b7ce0706527ad5ab389a4d4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Nov 10 15:39:30 2008 +0100

    fix for account_group_exec_runtime(), make sure ->signal can't be freed under rq->lock
    
    Impact: fix hang/crash on ia64 under high load
    
    This is ugly, but the simplest patch by far.
    
    Unlike other similar routines, account_group_exec_runtime() could be
    called "implicitly" from within scheduler after exit_notify(). This
    means we can race with the parent doing release_task(), we can't just
    check ->signal != NULL.
    
    Change __exit_signal() to do spin_unlock_wait(&task_rq(tsk)->lock)
    before __cleanup_signal() to make sure ->signal can't be freed under
    task_rq(tsk)->lock. Note that task_rq_unlock_wait() doesn't care
    about the case when tsk changes cpu/rq under us, this should be OK.
    
    Thanks to Ingo who nacked my previous buggy patch.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Reported-by: Doug Chapman <doug.chapman@hp.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 295b7c756ca6..644ffbda17ca 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -247,6 +247,7 @@ extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern int runqueue_is_locked(void);
+extern void task_rq_unlock_wait(struct task_struct *p);
 
 extern cpumask_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)

commit f8d570a4745835f2238a33b537218a1bb03fc671
Author: David Miller <davem@davemloft.net>
Date:   Thu Nov 6 00:37:40 2008 -0800

    net: Fix recursive descent in __scm_destroy().
    
    __scm_destroy() walks the list of file descriptors in the scm_fp_list
    pointed to by the scm_cookie argument.
    
    Those, in turn, can close sockets and invoke __scm_destroy() again.
    
    There is nothing which limits how deeply this can occur.
    
    The idea for how to fix this is from Linus.  Basically, we do all of
    the fput()s at the top level by collecting all of the scm_fp_list
    objects hit by an fput().  Inside of the initial __scm_destroy() we
    keep running the list until it is empty.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b483f39a7112..295b7c756ca6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1349,6 +1349,8 @@ struct task_struct {
 	 */
 	unsigned long timer_slack_ns;
 	unsigned long default_timer_slack_ns;
+
+	struct list_head	*scm_work_list;
 };
 
 /*

commit 8c82a17e9c924c0e9f13e75e4c2f6bca19a4b516
Merge: 4ce72a2c063a 57f8f7b60db6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 24 12:48:46 2008 +0200

    Merge commit 'v2.6.28-rc1' into sched/urgent

commit 88ed86fee6651033de9b7038dac7869a9f19775a
Merge: 3856d30ded1f 59c7572e82d6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 23 12:04:37 2008 -0700

    Merge branch 'proc' of git://git.kernel.org/pub/scm/linux/kernel/git/adobriyan/proc
    
    * 'proc' of git://git.kernel.org/pub/scm/linux/kernel/git/adobriyan/proc: (35 commits)
      proc: remove fs/proc/proc_misc.c
      proc: move /proc/vmcore creation to fs/proc/vmcore.c
      proc: move pagecount stuff to fs/proc/page.c
      proc: move all /proc/kcore stuff to fs/proc/kcore.c
      proc: move /proc/schedstat boilerplate to kernel/sched_stats.h
      proc: move /proc/modules boilerplate to kernel/module.c
      proc: move /proc/diskstats boilerplate to block/genhd.c
      proc: move /proc/zoneinfo boilerplate to mm/vmstat.c
      proc: move /proc/vmstat boilerplate to mm/vmstat.c
      proc: move /proc/pagetypeinfo boilerplate to mm/vmstat.c
      proc: move /proc/buddyinfo boilerplate to mm/vmstat.c
      proc: move /proc/vmallocinfo to mm/vmalloc.c
      proc: move /proc/slabinfo boilerplate to mm/slub.c, mm/slab.c
      proc: move /proc/slab_allocators boilerplate to mm/slab.c
      proc: move /proc/interrupts boilerplate code to fs/proc/interrupts.c
      proc: move /proc/stat to fs/proc/stat.c
      proc: move rest of /proc/partitions code to block/genhd.c
      proc: move /proc/cpuinfo code to fs/proc/cpuinfo.c
      proc: move /proc/devices code to fs/proc/devices.c
      proc: move rest of /proc/locks to fs/locks.c
      ...

commit 1f6d6e8ebe73ba9d9d4c693f7f6f50f661dbd6e4
Merge: db563fc2e805 268a3dcfea20
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 23 10:53:02 2008 -0700

    Merge branch 'v28-range-hrtimers-for-linus-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'v28-range-hrtimers-for-linus-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (37 commits)
      hrtimers: add missing docbook comments to struct hrtimer
      hrtimers: simplify hrtimer_peek_ahead_timers()
      hrtimers: fix docbook comments
      DECLARE_PER_CPU needs linux/percpu.h
      hrtimers: fix typo
      rangetimers: fix the bug reported by Ingo for real
      rangetimer: fix BUG_ON reported by Ingo
      rangetimer: fix x86 build failure for the !HRTIMERS case
      select: fix alpha OSF wrapper
      select: fix alpha OSF wrapper
      hrtimer: peek at the timer queue just before going idle
      hrtimer: make the futex() system call use the per process slack value
      hrtimer: make the nanosleep() syscall use the per process slack
      hrtimer: fix signed/unsigned bug in slack estimator
      hrtimer: show the timer ranges in /proc/timer_list
      hrtimer: incorporate feedback from Peter Zijlstra
      hrtimer: add a hrtimer_start_range() function
      hrtimer: another build fix
      hrtimer: fix build bug found by Ingo
      hrtimer: make select() and poll() use the hrtimer range feature
      ...

commit 133e887f90208d339088dd60cb1d08a72ba27288
Merge: e82cff752f57 0c4b83da58ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 23 09:37:16 2008 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: disable the hrtick for now
      sched: revert back to per-rq vruntime
      sched: fair scheduler should not resched rt tasks
      sched: optimize group load balancer
      sched: minor fast-path overhead reduction
      sched: fix the wrong mask_len, cleanup
      sched: kill unused scheduler decl.
      sched: fix the wrong mask_len
      sched: only update rq->clock while holding rq->lock

commit b5aadf7f14c1acc94956aa257e018e9de3881f41
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 13:23:43 2008 +0400

    proc: move /proc/schedstat boilerplate to kernel/sched_stats.h
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c38db536e07..7f60cb9b53cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -681,10 +681,6 @@ struct sched_info {
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 
-#ifdef CONFIG_SCHEDSTATS
-extern const struct file_operations proc_schedstat_operations;
-#endif /* CONFIG_SCHEDSTATS */
-
 #ifdef CONFIG_TASK_DELAY_ACCT
 struct task_delay_info {
 	spinlock_t	lock;

commit 4ce72a2c063a7fa8e42a9435440ae3364115a58d
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Oct 22 15:25:26 2008 +0800

    sched: add CONFIG_SMP consistency
    
    a patch from Henrik Austad did this:
    
    >> Do not declare select_task_rq as part of sched_class when CONFIG_SMP is
    >> not set.
    
    Peter observed:
    
    > While a proper cleanup, could you do it by re-arranging the methods so
    > as to not create an additional ifdef?
    
    Do not declare select_task_rq and some other methods as part of sched_class
    when CONFIG_SMP is not set.
    
    Also gather those methods to avoid CONFIG_SMP mess.
    
    Idea-by: Henrik Austad <henrik.austad@gmail.com>
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Henrik Austad <henrik@austad.us>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4f59c8e8597d..c05b45faef18 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -897,7 +897,6 @@ struct sched_class {
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
 	void (*yield_task) (struct rq *rq);
-	int  (*select_task_rq)(struct task_struct *p, int sync);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int sync);
 
@@ -905,6 +904,8 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
+	int  (*select_task_rq)(struct task_struct *p, int sync);
+
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest, unsigned long max_load_move,
 			struct sched_domain *sd, enum cpu_idle_type idle,
@@ -916,16 +917,17 @@ struct sched_class {
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
-#endif
 
-	void (*set_curr_task) (struct rq *rq);
-	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
-	void (*task_new) (struct rq *rq, struct task_struct *p);
 	void (*set_cpus_allowed)(struct task_struct *p,
 				 const cpumask_t *newmask);
 
 	void (*rq_online)(struct rq *rq);
 	void (*rq_offline)(struct rq *rq);
+#endif
+
+	void (*set_curr_task) (struct rq *rq);
+	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
+	void (*task_new) (struct rq *rq, struct task_struct *p);
 
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task,
 			       int running);

commit 268a3dcfea2077fca60d3715caa5c96f9b5e6ea7
Merge: c4bd822e7b12 592aa999d6a2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 22 09:48:06 2008 +0200

    Merge branch 'timers/range-hrtimers' into v28-range-hrtimers-for-linus-v2
    
    Conflicts:
    
            kernel/time/tick-sched.c
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 99ebcf8285df28f32fd2d1c19a7166e70f00309c
Merge: 72558dde738b c465a76af658
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:19:56 2008 -0700

    Merge branch 'v28-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'v28-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (36 commits)
      fix documentation of sysrq-q really
      Fix documentation of sysrq-q
      timer_list: add base address to clock base
      timer_list: print cpu number of clockevents device
      timer_list: print real timer address
      NOHZ: restart tick device from irq_enter()
      NOHZ: split tick_nohz_restart_sched_tick()
      NOHZ: unify the nohz function calls in irq_enter()
      timers: fix itimer/many thread hang, fix
      timers: fix itimer/many thread hang, v3
      ntp: improve adjtimex frequency rounding
      timekeeping: fix rounding problem during clock update
      ntp: let update_persistent_clock() sleep
      hrtimer: reorder struct hrtimer to save 8 bytes on 64bit builds
      posix-timers: lock_timer: make it readable
      posix-timers: lock_timer: kill the bogus ->it_id check
      posix-timers: kill ->it_sigev_signo and ->it_sigev_value
      posix-timers: sys_timer_create: cleanup the error handling
      posix-timers: move the initialization of timer->sigq from send to create path
      posix-timers: sys_timer_create: simplify and s/tasklist/rcu/
      ...
    
    Fix trivial conflicts due to sysrq-q description clahes in
    Documentation/sysrq.txt and drivers/char/sysrq.c

commit 656eb2cd5da153762f2e8419ca117ce12ef522c3
Author: Roland McGrath <roland@redhat.com>
Date:   Sat Oct 18 20:28:23 2008 -0700

    add CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS
    
    This adds a kconfig option to change the /proc/PID/coredump_filter default.
    Fedora has been carrying a trivial patch to change the hard-wired value for
    this default, since Fedora 8.  The default default can't change safely
    because there are old GDB versions out there (all before 6.7) that are
    confused by the core dump files created by the MMF_DUMP_ELF_HEADERS setting.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Michael Kerrisk <mtk.manpages@googlemail.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Kawai Hidehiro <hidehiro.kawai.ez@hitachi.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 017cc914ef1f..f52dbd3587a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -411,7 +411,13 @@ extern int get_dumpable(struct mm_struct *mm);
 	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
 #define MMF_DUMP_FILTER_DEFAULT \
 	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED) |\
-	 (1 << MMF_DUMP_HUGETLB_PRIVATE))
+	 (1 << MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)
+
+#ifdef CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS
+# define MMF_DUMP_MASK_DEFAULT_ELF	(1 << MMF_DUMP_ELF_HEADERS)
+#else
+# define MMF_DUMP_MASK_DEFAULT_ELF	0
+#endif
 
 struct sighand_struct {
 	atomic_t		count;

commit e575f111dc0f27044e170580e7de50985ab3e011
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Sat Oct 18 20:27:08 2008 -0700

    coredump_filter: add hugepage dumping
    
    Presently hugepage's vma has a VM_RESERVED flag in order not to be
    swapped.  But a VM_RESERVED vma isn't core dumped because this flag is
    often used for some kernel vmas (e.g.  vmalloc, sound related).
    
    Thus hugepages are never dumped and it can't be debugged easily.  Many
    developers want hugepages to be included into core-dump.
    
    However, We can't read generic VM_RESERVED area because this area is often
    IO mapping area.  then these area reading may change device state.  it is
    definitly undesiable side-effect.
    
    So adding a hugepage specific bit to the coredump filter is better.  It
    will be able to hugepage core dumping and doesn't cause any side-effect to
    any i/o devices.
    
    In additional, libhugetlb use hugetlb private mapping pages as anonymous
    page.  Then, hugepage private mapping pages should be core dumped by
    default.
    
    Then, /proc/[pid]/core_dump_filter has two new bits.
    
     - bit 5 mean hugetlb private mapping pages are dumped or not. (default: yes)
     - bit 6 mean hugetlb shared mapping pages are dumped or not.  (default: no)
    
    I tested by following method.
    
    % ulimit -c unlimited
    % ./crash_hugepage  50
    % ./crash_hugepage  50  -p
    % ls -lh
    % gdb ./crash_hugepage core
    %
    % echo 0x43 > /proc/self/coredump_filter
    % ./crash_hugepage  50
    % ./crash_hugepage  50  -p
    % ls -lh
    % gdb ./crash_hugepage core
    
    #include <stdlib.h>
    #include <stdio.h>
    #include <unistd.h>
    #include <sys/mman.h>
    #include <string.h>
    
    #include "hugetlbfs.h"
    
    int main(int argc, char** argv){
            char* p;
            int ch;
            int mmap_flags = MAP_SHARED;
            int fd;
            int nr_pages;
    
            while((ch = getopt(argc, argv, "p")) != -1) {
                    switch (ch) {
                    case 'p':
                            mmap_flags &= ~MAP_SHARED;
                            mmap_flags |= MAP_PRIVATE;
                            break;
                    default:
                            /* nothing*/
                            break;
                    }
            }
            argc -= optind;
            argv += optind;
    
            if (argc == 0){
                    printf("need # of pages\n");
                    exit(1);
            }
    
            nr_pages = atoi(argv[0]);
            if (nr_pages < 2) {
                    printf("nr_pages must >2\n");
                    exit(1);
            }
    
            fd = hugetlbfs_unlinked_fd();
            p = mmap(NULL, nr_pages * gethugepagesize(),
                     PROT_READ|PROT_WRITE, mmap_flags, fd, 0);
    
            sleep(2);
    
            *(p + gethugepagesize()) = 1; /* COW */
            sleep(2);
    
            /* crash! */
            *(int*)0 = 1;
    
            return 0;
    }
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Kawai Hidehiro <hidehiro.kawai.ez@hitachi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: William Irwin <wli@holomorphy.com>
    Cc: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c226c7b82946..017cc914ef1f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -403,12 +403,15 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_DUMP_MAPPED_PRIVATE	4
 #define MMF_DUMP_MAPPED_SHARED	5
 #define MMF_DUMP_ELF_HEADERS	6
+#define MMF_DUMP_HUGETLB_PRIVATE 7
+#define MMF_DUMP_HUGETLB_SHARED  8
 #define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
-#define MMF_DUMP_FILTER_BITS	5
+#define MMF_DUMP_FILTER_BITS	7
 #define MMF_DUMP_FILTER_MASK \
 	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
 #define MMF_DUMP_FILTER_DEFAULT \
-	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED))
+	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED) |\
+	 (1 << MMF_DUMP_HUGETLB_PRIVATE))
 
 struct sighand_struct {
 	atomic_t		count;

commit ffda12a17a324103e9900fa1035309811eecbfe5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Oct 17 19:27:02 2008 +0200

    sched: optimize group load balancer
    
    I noticed that tg_shares_up() unconditionally takes rq-locks for all cpus
    in the sched_domain. This hurts.
    
    We need the rq-locks whenever we change the weight of the per-cpu group sched
    entities. To allevate this a little, only change the weight when the new
    weight is at least shares_thresh away from the old value.
    
    This avoids the rq-lock for the top level entries, since those will never
    be re-weighted, and fuzzes the lower level entries a little to gain performance
    in semi-stable situations.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6eda6ad735dc..4f59c8e8597d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1621,6 +1621,7 @@ extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_shares_ratelimit;
+extern unsigned int sysctl_sched_shares_thresh;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit c465a76af658b443075d6efee1c3131257643020
Merge: 2d42244ae71d 1b02469088ac fb02fbc14d17 d40e944c25fb 1508487e7f16 322acf6585f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 20 13:14:06 2008 +0200

    Merge branches 'timers/clocksource', 'timers/hrtimers', 'timers/nohz', 'timers/ntp', 'timers/posixtimers' and 'timers/debug' into v28-timers-for-linus

commit 651dab4264e4ba0e563f5ff56f748127246e9065
Merge: 40b860625355 2e532d68a2b3
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Oct 17 09:20:26 2008 -0700

    Merge commit 'linus/master' into merge-linus
    
    Conflicts:
    
            arch/x86/kvm/i8254.c

commit e62b4853983d032dcb3cde9fb20407dc556f47bc
Author: David Miller <davem@davemloft.net>
Date:   Thu Oct 16 21:14:11 2008 -0700

    sched: kill unused scheduler decl.
    
    I noticed this while making investigations into the tbench
    regressions.  Please apply.
    
    sched: Remove hrtick_resched() extern decl.
    
    This function was removed by 31656519e132f6612584815f128c83976a9aaaef
    ("sched, x86: clean up hrtick implementation").
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c226c7b82946..6eda6ad735dc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -287,7 +287,6 @@ extern void trap_init(void);
 extern void account_process_tick(struct task_struct *task, int user);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
-extern void hrtick_resched(void);
 
 extern void sched_show_task(struct task_struct *p);
 

commit b2aaf8f74cdc84a9182f6cabf198b7763bcb9d40
Merge: 4f962d4d6592 278429cff880
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Oct 15 13:46:29 2008 +0200

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/kernel/Makefile
            include/asm-x86/pda.h

commit 365d46dc9be9b3c833990a06f3994b1987eda578
Merge: 5dc64a3442b9 fd0480883066
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Oct 12 12:35:23 2008 +0200

    Merge branch 'linus' into x86/xen
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/process_64.c
            arch/x86/xen/enlighten.c

commit a5d8c3483a6e19aca95ef6a2c5890e33bfa5b293
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 9 11:35:51 2008 +0200

    sched debug: add name to sched_domain sysctl entries
    
    add /proc/sys/kernel/sched_domain/cpu0/domain0/name, to make
    it easier to see which specific scheduler domain remained at
    that entry.
    
    Since we process the scheduler domain tree and
    simplify it, it's not always immediately clear during debugging
    which domain came from where.
    
    depends on CONFIG_SCHED_DEBUG=y.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d8e699b55858..5d0819ee442a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -824,6 +824,9 @@ struct sched_domain {
 	unsigned int ttwu_move_affine;
 	unsigned int ttwu_move_balance;
 #endif
+#ifdef CONFIG_SCHED_DEBUG
+	char *name;
+#endif
 };
 
 extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,

commit 7086efe1c1536f6bc160e7d60a9bfd645b91f279
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang, v3
    
    - fix UP lockup
    - another set of UP/SMP cleanups and simplifications
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b982fb48c8f0..23d9d5464544 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2134,7 +2134,6 @@ static inline int thread_group_cputime_clone_thread(struct task_struct *curr)
 	return thread_group_cputime_alloc(curr);
 }
 
-
 static inline void thread_group_cputime_free(struct signal_struct *sig)
 {
 	free_percpu(sig->cputime.totals);

commit bb34d92f643086d546b49cef680f6f305ed84414
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang, v2
    
    This is the second resubmission of the posix timer rework patch, posted
    a few days ago.
    
    This includes the changes from the previous resubmittion, which addressed
    Oleg Nesterov's comments, removing the RCU stuff from the patch and
    un-inlining the thread_group_cputime() function for SMP.
    
    In addition, per Ingo Molnar it simplifies the UP code, consolidating much
    of it with the SMP version and depending on lower-level SMP/UP handling to
    take care of the differences.
    
    It also cleans up some UP compile errors, moves the scheduler stats-related
    macros into kernel/sched_stats.h, cleans up a merge error in
    kernel/fork.c and has a few other minor fixes and cleanups as suggested
    by Oleg and Ingo. Thanks for the review, guys.
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7ce8d4e53565..b982fb48c8f0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -454,15 +454,9 @@ struct task_cputime {
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU clock calculations.
  */
-#ifdef CONFIG_SMP
 struct thread_group_cputime {
 	struct task_cputime *totals;
 };
-#else
-struct thread_group_cputime {
-	struct task_cputime totals;
-};
-#endif
 
 /*
  * NOTE! "signal_struct" does not have it's own
@@ -2124,193 +2118,26 @@ static inline int spin_needbreak(spinlock_t *lock)
 /*
  * Thread group CPU time accounting.
  */
-#ifdef CONFIG_SMP
 
-extern int thread_group_cputime_alloc_smp(struct task_struct *);
-extern void thread_group_cputime_smp(struct task_struct *, struct task_cputime *);
+extern int thread_group_cputime_alloc(struct task_struct *);
+extern void thread_group_cputime(struct task_struct *, struct task_cputime *);
 
 static inline void thread_group_cputime_init(struct signal_struct *sig)
 {
 	sig->cputime.totals = NULL;
 }
 
-static inline int thread_group_cputime_clone_thread(struct task_struct *curr,
-						    struct task_struct *new)
+static inline int thread_group_cputime_clone_thread(struct task_struct *curr)
 {
 	if (curr->signal->cputime.totals)
 		return 0;
-	return thread_group_cputime_alloc_smp(curr);
+	return thread_group_cputime_alloc(curr);
 }
 
-static inline void thread_group_cputime_free(struct signal_struct *sig)
-{
-	free_percpu(sig->cputime.totals);
-}
-
-/**
- * thread_group_cputime - Sum the thread group time fields across all CPUs.
- *
- * This is a wrapper for the real routine, thread_group_cputime_smp().  See
- * that routine for details.
- */
-static inline void thread_group_cputime(
-	struct task_struct *tsk,
-	struct task_cputime *times)
-{
-	thread_group_cputime_smp(tsk, times);
-}
-
-/**
- * thread_group_cputime_account_user - Maintain utime for a thread group.
- *
- * @tgtimes:	Pointer to thread_group_cputime structure.
- * @cputime:	Time value by which to increment the utime field of that
- *		structure.
- *
- * If thread group time is being maintained, get the structure for the
- * running CPU and update the utime field there.
- */
-static inline void thread_group_cputime_account_user(
-	struct thread_group_cputime *tgtimes,
-	cputime_t cputime)
-{
-	if (tgtimes->totals) {
-		struct task_cputime *times;
-
-		times = per_cpu_ptr(tgtimes->totals, get_cpu());
-		times->utime = cputime_add(times->utime, cputime);
-		put_cpu_no_resched();
-	}
-}
-
-/**
- * thread_group_cputime_account_system - Maintain stime for a thread group.
- *
- * @tgtimes:	Pointer to thread_group_cputime structure.
- * @cputime:	Time value by which to increment the stime field of that
- *		structure.
- *
- * If thread group time is being maintained, get the structure for the
- * running CPU and update the stime field there.
- */
-static inline void thread_group_cputime_account_system(
-	struct thread_group_cputime *tgtimes,
-	cputime_t cputime)
-{
-	if (tgtimes->totals) {
-		struct task_cputime *times;
-
-		times = per_cpu_ptr(tgtimes->totals, get_cpu());
-		times->stime = cputime_add(times->stime, cputime);
-		put_cpu_no_resched();
-	}
-}
-
-/**
- * thread_group_cputime_account_exec_runtime - Maintain exec runtime for a
- *						thread group.
- *
- * @tgtimes:	Pointer to thread_group_cputime structure.
- * @ns:		Time value by which to increment the sum_exec_runtime field
- *		of that structure.
- *
- * If thread group time is being maintained, get the structure for the
- * running CPU and update the sum_exec_runtime field there.
- */
-static inline void thread_group_cputime_account_exec_runtime(
-	struct thread_group_cputime *tgtimes,
-	unsigned long long ns)
-{
-	if (tgtimes->totals) {
-		struct task_cputime *times;
-
-		times = per_cpu_ptr(tgtimes->totals, get_cpu());
-		times->sum_exec_runtime += ns;
-		put_cpu_no_resched();
-	}
-}
-
-#else /* CONFIG_SMP */
-
-static inline void thread_group_cputime_init(struct signal_struct *sig)
-{
-	sig->cputime.totals.utime = cputime_zero;
-	sig->cputime.totals.stime = cputime_zero;
-	sig->cputime.totals.sum_exec_runtime = 0;
-}
-
-static inline int thread_group_cputime_alloc(struct task_struct *tsk)
-{
-	return 0;
-}
 
 static inline void thread_group_cputime_free(struct signal_struct *sig)
 {
-}
-
-static inline int thread_group_cputime_clone_thread(struct task_struct *curr,
-						     struct task_struct *tsk)
-{
-	return 0;
-}
-
-static inline void thread_group_cputime(struct task_struct *tsk,
-					 struct task_cputime *cputime)
-{
-	*cputime = tsk->signal->cputime.totals;
-}
-
-static inline void thread_group_cputime_account_user(
-	struct thread_group_cputime *tgtimes,
-	cputime_t cputime)
-{
-	tgtimes->totals.utime = cputime_add(tgtimes->totals.utime, cputime);
-}
-
-static inline void thread_group_cputime_account_system(
-	struct thread_group_cputime *tgtimes,
-	cputime_t cputime)
-{
-	tgtimes->totals.stime = cputime_add(tgtimes->totals.stime, cputime);
-}
-
-static inline void thread_group_cputime_account_exec_runtime(
-	struct thread_group_cputime *tgtimes,
-	unsigned long long ns)
-{
-	tgtimes->totals.sum_exec_runtime += ns;
-}
-
-#endif /* CONFIG_SMP */
-
-static inline void account_group_user_time(struct task_struct *tsk,
-					    cputime_t cputime)
-{
-	struct signal_struct *sig;
-
-	sig = tsk->signal;
-	if (likely(sig))
-		thread_group_cputime_account_user(&sig->cputime, cputime);
-}
-
-static inline void account_group_system_time(struct task_struct *tsk,
-					      cputime_t cputime)
-{
-	struct signal_struct *sig;
-
-	sig = tsk->signal;
-	if (likely(sig))
-		thread_group_cputime_account_system(&sig->cputime, cputime);
-}
-
-static inline void account_group_exec_runtime(struct task_struct *tsk,
-					       unsigned long long ns)
-{
-	struct signal_struct *sig;
-
-	sig = tsk->signal;
-	if (likely(sig))
-		thread_group_cputime_account_exec_runtime(&sig->cputime, ns);
+	free_percpu(sig->cputime.totals);
 }
 
 /*

commit 15afe09bf496ae10c989e1a375a6b5da7bd3e16e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Sep 20 23:38:02 2008 +0200

    sched: wakeup preempt when small overlap
    
    Lin Ming reported a 10% OLTP regression against 2.6.27-rc4.
    
    The difference seems to come from different preemption agressiveness,
    which affects the cache footprint of the workload and its effective
    cache trashing.
    
    Aggresively preempt a task if its avg overlap is very small, this should
    avoid the task going to sleep and find it still running when we schedule
    back to it - saving a wakeup.
    
    Reported-by: Lin Ming <ming.m.lin@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b3b7a8f32477..d8e699b55858 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -897,7 +897,7 @@ struct sched_class {
 	void (*yield_task) (struct rq *rq);
 	int  (*select_task_rq)(struct task_struct *p, int sync);
 
-	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
+	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int sync);
 
 	struct task_struct * (*pick_next_task) (struct rq *rq);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);

commit 5ce73a4a5a4893a1aa4cdeed1b1a5a6de42c43b6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 14 17:11:46 2008 +0200

    timers: fix itimer/many thread hang, cleanups
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ed355f02d329..7ce8d4e53565 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -430,7 +430,7 @@ struct pacct_struct {
  * @utime:		time spent in user mode, in &cputime_t units
  * @stime:		time spent in kernel mode, in &cputime_t units
  * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
- * 
+ *
  * This structure groups together three kinds of CPU time that are
  * tracked for threads and thread groups.  Most things considering
  * CPU time want to group these counts together and treat all three

commit 0a8eaa4f9b58759595a1bfe13a1295fdc25ba026
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 14 17:03:52 2008 +0200

    timers: fix itimer/many thread hang, fix #2
    
    fix the UP build:
    
    In file included from arch/x86/kernel/asm-offsets_32.c:9,
                     from arch/x86/kernel/asm-offsets.c:3:
    include/linux/sched.h: In function â€˜thread_group_cputime_clone_threadâ€™:
    include/linux/sched.h:2272: warning: no return statement in function returning non-void
    include/linux/sched.h: In function â€˜thread_group_cputime_account_userâ€™:
    include/linux/sched.h:2284: error: invalid type argument of â€˜->â€™ (have â€˜struct task_cputimeâ€™)
    include/linux/sched.h:2284: error: invalid type argument of â€˜->â€™ (have â€˜struct task_cputimeâ€™)
    include/linux/sched.h: In function â€˜thread_group_cputime_account_systemâ€™:
    include/linux/sched.h:2291: error: invalid type argument of â€˜->â€™ (have â€˜struct task_cputimeâ€™)
    include/linux/sched.h:2291: error: invalid type argument of â€˜->â€™ (have â€˜struct task_cputimeâ€™)
    include/linux/sched.h: In function â€˜thread_group_cputime_account_exec_runtimeâ€™:
    include/linux/sched.h:2298: error: invalid type argument of â€˜->â€™ (have â€˜struct task_cputimeâ€™)
    distcc[14501] ERROR: compile arch/x86/kernel/asm-offsets.c on a/30 failed
    make[1]: *** [arch/x86/kernel/asm-offsets.s] Error 1
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 26d7a5f2d0ba..ed355f02d329 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2251,6 +2251,7 @@ static inline void thread_group_cputime_free(struct signal_struct *sig)
 static inline int thread_group_cputime_clone_thread(struct task_struct *curr,
 						     struct task_struct *tsk)
 {
+	return 0;
 }
 
 static inline void thread_group_cputime(struct task_struct *tsk,
@@ -2263,21 +2264,21 @@ static inline void thread_group_cputime_account_user(
 	struct thread_group_cputime *tgtimes,
 	cputime_t cputime)
 {
-	tgtimes->totals->utime = cputime_add(tgtimes->totals->utime, cputime);
+	tgtimes->totals.utime = cputime_add(tgtimes->totals.utime, cputime);
 }
 
 static inline void thread_group_cputime_account_system(
 	struct thread_group_cputime *tgtimes,
 	cputime_t cputime)
 {
-	tgtimes->totals->stime = cputime_add(tgtimes->totals->stime, cputime);
+	tgtimes->totals.stime = cputime_add(tgtimes->totals.stime, cputime);
 }
 
 static inline void thread_group_cputime_account_exec_runtime(
 	struct thread_group_cputime *tgtimes,
 	unsigned long long ns)
 {
-	tgtimes->totals->sum_exec_runtime += ns;
+	tgtimes->totals.sum_exec_runtime += ns;
 }
 
 #endif /* CONFIG_SMP */

commit f06febc96ba8e0af80bcc3eaec0a109e88275fac
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang
    
    Overview
    
    This patch reworks the handling of POSIX CPU timers, including the
    ITIMER_PROF, ITIMER_VIRT timers and rlimit handling.  It was put together
    with the help of Roland McGrath, the owner and original writer of this code.
    
    The problem we ran into, and the reason for this rework, has to do with using
    a profiling timer in a process with a large number of threads.  It appears
    that the performance of the old implementation of run_posix_cpu_timers() was
    at least O(n*3) (where "n" is the number of threads in a process) or worse.
    Everything is fine with an increasing number of threads until the time taken
    for that routine to run becomes the same as or greater than the tick time, at
    which point things degrade rather quickly.
    
    This patch fixes bug 9906, "Weird hang with NPTL and SIGPROF."
    
    Code Changes
    
    This rework corrects the implementation of run_posix_cpu_timers() to make it
    run in constant time for a particular machine.  (Performance may vary between
    one machine and another depending upon whether the kernel is built as single-
    or multiprocessor and, in the latter case, depending upon the number of
    running processors.)  To do this, at each tick we now update fields in
    signal_struct as well as task_struct.  The run_posix_cpu_timers() function
    uses those fields to make its decisions.
    
    We define a new structure, "task_cputime," to contain user, system and
    scheduler times and use these in appropriate places:
    
    struct task_cputime {
            cputime_t utime;
            cputime_t stime;
            unsigned long long sum_exec_runtime;
    };
    
    This is included in the structure "thread_group_cputime," which is a new
    substructure of signal_struct and which varies for uniprocessor versus
    multiprocessor kernels.  For uniprocessor kernels, it uses "task_cputime" as
    a simple substructure, while for multiprocessor kernels it is a pointer:
    
    struct thread_group_cputime {
            struct task_cputime totals;
    };
    
    struct thread_group_cputime {
            struct task_cputime *totals;
    };
    
    We also add a new task_cputime substructure directly to signal_struct, to
    cache the earliest expiration of process-wide timers, and task_cputime also
    replaces the it_*_expires fields of task_struct (used for earliest expiration
    of thread timers).  The "thread_group_cputime" structure contains process-wide
    timers that are updated via account_user_time() and friends.  In the non-SMP
    case the structure is a simple aggregator; unfortunately in the SMP case that
    simplicity was not achievable due to cache-line contention between CPUs (in
    one measured case performance was actually _worse_ on a 16-cpu system than
    the same test on a 4-cpu system, due to this contention).  For SMP, the
    thread_group_cputime counters are maintained as a per-cpu structure allocated
    using alloc_percpu().  The timer functions update only the timer field in
    the structure corresponding to the running CPU, obtained using per_cpu_ptr().
    
    We define a set of inline functions in sched.h that we use to maintain the
    thread_group_cputime structure and hide the differences between UP and SMP
    implementations from the rest of the kernel.  The thread_group_cputime_init()
    function initializes the thread_group_cputime structure for the given task.
    The thread_group_cputime_alloc() is a no-op for UP; for SMP it calls the
    out-of-line function thread_group_cputime_alloc_smp() to allocate and fill
    in the per-cpu structures and fields.  The thread_group_cputime_free()
    function, also a no-op for UP, in SMP frees the per-cpu structures.  The
    thread_group_cputime_clone_thread() function (also a UP no-op) for SMP calls
    thread_group_cputime_alloc() if the per-cpu structures haven't yet been
    allocated.  The thread_group_cputime() function fills the task_cputime
    structure it is passed with the contents of the thread_group_cputime fields;
    in UP it's that simple but in SMP it must also safely check that tsk->signal
    is non-NULL (if it is it just uses the appropriate fields of task_struct) and,
    if so, sums the per-cpu values for each online CPU.  Finally, the three
    functions account_group_user_time(), account_group_system_time() and
    account_group_exec_runtime() are used by timer functions to update the
    respective fields of the thread_group_cputime structure.
    
    Non-SMP operation is trivial and will not be mentioned further.
    
    The per-cpu structure is always allocated when a task creates its first new
    thread, via a call to thread_group_cputime_clone_thread() from copy_signal().
    It is freed at process exit via a call to thread_group_cputime_free() from
    cleanup_signal().
    
    All functions that formerly summed utime/stime/sum_sched_runtime values from
    from all threads in the thread group now use thread_group_cputime() to
    snapshot the values in the thread_group_cputime structure or the values in
    the task structure itself if the per-cpu structure hasn't been allocated.
    
    Finally, the code in kernel/posix-cpu-timers.c has changed quite a bit.
    The run_posix_cpu_timers() function has been split into a fast path and a
    slow path; the former safely checks whether there are any expired thread
    timers and, if not, just returns, while the slow path does the heavy lifting.
    With the dedicated thread group fields, timers are no longer "rebalanced" and
    the process_timer_rebalance() function and related code has gone away.  All
    summing loops are gone and all code that used them now uses the
    thread_group_cputime() inline.  When process-wide timers are set, the new
    task_cputime structure in signal_struct is used to cache the earliest
    expiration; this is checked in the fast path.
    
    Performance
    
    The fix appears not to add significant overhead to existing operations.  It
    generally performs the same as the current code except in two cases, one in
    which it performs slightly worse (Case 5 below) and one in which it performs
    very significantly better (Case 2 below).  Overall it's a wash except in those
    two cases.
    
    I've since done somewhat more involved testing on a dual-core Opteron system.
    
    Case 1: With no itimer running, for a test with 100,000 threads, the fixed
            kernel took 1428.5 seconds, 513 seconds more than the unfixed system,
            all of which was spent in the system.  There were twice as many
            voluntary context switches with the fix as without it.
    
    Case 2: With an itimer running at .01 second ticks and 4000 threads (the most
            an unmodified kernel can handle), the fixed kernel ran the test in
            eight percent of the time (5.8 seconds as opposed to 70 seconds) and
            had better tick accuracy (.012 seconds per tick as opposed to .023
            seconds per tick).
    
    Case 3: A 4000-thread test with an initial timer tick of .01 second and an
            interval of 10,000 seconds (i.e. a timer that ticks only once) had
            very nearly the same performance in both cases:  6.3 seconds elapsed
            for the fixed kernel versus 5.5 seconds for the unfixed kernel.
    
    With fewer threads (eight in these tests), the Case 1 test ran in essentially
    the same time on both the modified and unmodified kernels (5.2 seconds versus
    5.8 seconds).  The Case 2 test ran in about the same time as well, 5.9 seconds
    versus 5.4 seconds but again with much better tick accuracy, .013 seconds per
    tick versus .025 seconds per tick for the unmodified kernel.
    
    Since the fix affected the rlimit code, I also tested soft and hard CPU limits.
    
    Case 4: With a hard CPU limit of 20 seconds and eight threads (and an itimer
            running), the modified kernel was very slightly favored in that while
            it killed the process in 19.997 seconds of CPU time (5.002 seconds of
            wall time), only .003 seconds of that was system time, the rest was
            user time.  The unmodified kernel killed the process in 20.001 seconds
            of CPU (5.014 seconds of wall time) of which .016 seconds was system
            time.  Really, though, the results were too close to call.  The results
            were essentially the same with no itimer running.
    
    Case 5: With a soft limit of 20 seconds and a hard limit of 2000 seconds
            (where the hard limit would never be reached) and an itimer running,
            the modified kernel exhibited worse tick accuracy than the unmodified
            kernel: .050 seconds/tick versus .028 seconds/tick.  Otherwise,
            performance was almost indistinguishable.  With no itimer running this
            test exhibited virtually identical behavior and times in both cases.
    
    In times past I did some limited performance testing.  those results are below.
    
    On a four-cpu Opteron system without this fix, a sixteen-thread test executed
    in 3569.991 seconds, of which user was 3568.435s and system was 1.556s.  On
    the same system with the fix, user and elapsed time were about the same, but
    system time dropped to 0.007 seconds.  Performance with eight, four and one
    thread were comparable.  Interestingly, the timer ticks with the fix seemed
    more accurate:  The sixteen-thread test with the fix received 149543 ticks
    for 0.024 seconds per tick, while the same test without the fix received 58720
    for 0.061 seconds per tick.  Both cases were configured for an interval of
    0.01 seconds.  Again, the other tests were comparable.  Each thread in this
    test computed the primes up to 25,000,000.
    
    I also did a test with a large number of threads, 100,000 threads, which is
    impossible without the fix.  In this case each thread computed the primes only
    up to 10,000 (to make the runtime manageable).  System time dominated, at
    1546.968 seconds out of a total 2176.906 seconds (giving a user time of
    629.938s).  It received 147651 ticks for 0.015 seconds per tick, still quite
    accurate.  There is obviously no comparable test without the fix.
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d9120c5ad15..26d7a5f2d0ba 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -425,6 +425,45 @@ struct pacct_struct {
 	unsigned long		ac_minflt, ac_majflt;
 };
 
+/**
+ * struct task_cputime - collected CPU time counts
+ * @utime:		time spent in user mode, in &cputime_t units
+ * @stime:		time spent in kernel mode, in &cputime_t units
+ * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
+ * 
+ * This structure groups together three kinds of CPU time that are
+ * tracked for threads and thread groups.  Most things considering
+ * CPU time want to group these counts together and treat all three
+ * of them in parallel.
+ */
+struct task_cputime {
+	cputime_t utime;
+	cputime_t stime;
+	unsigned long long sum_exec_runtime;
+};
+/* Alternate field names when used to cache expirations. */
+#define prof_exp	stime
+#define virt_exp	utime
+#define sched_exp	sum_exec_runtime
+
+/**
+ * struct thread_group_cputime - thread group interval timer counts
+ * @totals:		thread group interval timers; substructure for
+ *			uniprocessor kernel, per-cpu for SMP kernel.
+ *
+ * This structure contains the version of task_cputime, above, that is
+ * used for thread group CPU clock calculations.
+ */
+#ifdef CONFIG_SMP
+struct thread_group_cputime {
+	struct task_cputime *totals;
+};
+#else
+struct thread_group_cputime {
+	struct task_cputime totals;
+};
+#endif
+
 /*
  * NOTE! "signal_struct" does not have it's own
  * locking, because a shared signal_struct always
@@ -470,6 +509,17 @@ struct signal_struct {
 	cputime_t it_prof_expires, it_virt_expires;
 	cputime_t it_prof_incr, it_virt_incr;
 
+	/*
+	 * Thread group totals for process CPU clocks.
+	 * See thread_group_cputime(), et al, for details.
+	 */
+	struct thread_group_cputime cputime;
+
+	/* Earliest-expiration cache. */
+	struct task_cputime cputime_expires;
+
+	struct list_head cpu_timers[3];
+
 	/* job control IDs */
 
 	/*
@@ -500,7 +550,7 @@ struct signal_struct {
 	 * Live threads maintain their own counters and add to these
 	 * in __exit_signal, except for the group leader.
 	 */
-	cputime_t utime, stime, cutime, cstime;
+	cputime_t cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
@@ -508,14 +558,6 @@ struct signal_struct {
 	unsigned long inblock, oublock, cinblock, coublock;
 	struct task_io_accounting ioac;
 
-	/*
-	 * Cumulative ns of scheduled CPU time for dead threads in the
-	 * group, not including a zombie group leader.  (This only differs
-	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
-	 * other than jiffies.)
-	 */
-	unsigned long long sum_sched_runtime;
-
 	/*
 	 * We don't bother to synchronize most readers of this at all,
 	 * because there is no reader checking a limit that actually needs
@@ -527,8 +569,6 @@ struct signal_struct {
 	 */
 	struct rlimit rlim[RLIM_NLIMITS];
 
-	struct list_head cpu_timers[3];
-
 	/* keep the process-shared keyrings here so that they do the right
 	 * thing in threads created with CLONE_THREAD */
 #ifdef CONFIG_KEYS
@@ -1134,8 +1174,7 @@ struct task_struct {
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 
-  	cputime_t it_prof_expires, it_virt_expires;
-	unsigned long long it_sched_expires;
+	struct task_cputime cputime_expires;
 	struct list_head cpu_timers[3];
 
 /* process credentials */
@@ -1585,6 +1624,7 @@ extern unsigned long long cpu_clock(int cpu);
 
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
+extern unsigned long long thread_group_sched_runtime(struct task_struct *task);
 
 /* sched_exec is called by processes performing an exec */
 #ifdef CONFIG_SMP
@@ -2081,6 +2121,197 @@ static inline int spin_needbreak(spinlock_t *lock)
 #endif
 }
 
+/*
+ * Thread group CPU time accounting.
+ */
+#ifdef CONFIG_SMP
+
+extern int thread_group_cputime_alloc_smp(struct task_struct *);
+extern void thread_group_cputime_smp(struct task_struct *, struct task_cputime *);
+
+static inline void thread_group_cputime_init(struct signal_struct *sig)
+{
+	sig->cputime.totals = NULL;
+}
+
+static inline int thread_group_cputime_clone_thread(struct task_struct *curr,
+						    struct task_struct *new)
+{
+	if (curr->signal->cputime.totals)
+		return 0;
+	return thread_group_cputime_alloc_smp(curr);
+}
+
+static inline void thread_group_cputime_free(struct signal_struct *sig)
+{
+	free_percpu(sig->cputime.totals);
+}
+
+/**
+ * thread_group_cputime - Sum the thread group time fields across all CPUs.
+ *
+ * This is a wrapper for the real routine, thread_group_cputime_smp().  See
+ * that routine for details.
+ */
+static inline void thread_group_cputime(
+	struct task_struct *tsk,
+	struct task_cputime *times)
+{
+	thread_group_cputime_smp(tsk, times);
+}
+
+/**
+ * thread_group_cputime_account_user - Maintain utime for a thread group.
+ *
+ * @tgtimes:	Pointer to thread_group_cputime structure.
+ * @cputime:	Time value by which to increment the utime field of that
+ *		structure.
+ *
+ * If thread group time is being maintained, get the structure for the
+ * running CPU and update the utime field there.
+ */
+static inline void thread_group_cputime_account_user(
+	struct thread_group_cputime *tgtimes,
+	cputime_t cputime)
+{
+	if (tgtimes->totals) {
+		struct task_cputime *times;
+
+		times = per_cpu_ptr(tgtimes->totals, get_cpu());
+		times->utime = cputime_add(times->utime, cputime);
+		put_cpu_no_resched();
+	}
+}
+
+/**
+ * thread_group_cputime_account_system - Maintain stime for a thread group.
+ *
+ * @tgtimes:	Pointer to thread_group_cputime structure.
+ * @cputime:	Time value by which to increment the stime field of that
+ *		structure.
+ *
+ * If thread group time is being maintained, get the structure for the
+ * running CPU and update the stime field there.
+ */
+static inline void thread_group_cputime_account_system(
+	struct thread_group_cputime *tgtimes,
+	cputime_t cputime)
+{
+	if (tgtimes->totals) {
+		struct task_cputime *times;
+
+		times = per_cpu_ptr(tgtimes->totals, get_cpu());
+		times->stime = cputime_add(times->stime, cputime);
+		put_cpu_no_resched();
+	}
+}
+
+/**
+ * thread_group_cputime_account_exec_runtime - Maintain exec runtime for a
+ *						thread group.
+ *
+ * @tgtimes:	Pointer to thread_group_cputime structure.
+ * @ns:		Time value by which to increment the sum_exec_runtime field
+ *		of that structure.
+ *
+ * If thread group time is being maintained, get the structure for the
+ * running CPU and update the sum_exec_runtime field there.
+ */
+static inline void thread_group_cputime_account_exec_runtime(
+	struct thread_group_cputime *tgtimes,
+	unsigned long long ns)
+{
+	if (tgtimes->totals) {
+		struct task_cputime *times;
+
+		times = per_cpu_ptr(tgtimes->totals, get_cpu());
+		times->sum_exec_runtime += ns;
+		put_cpu_no_resched();
+	}
+}
+
+#else /* CONFIG_SMP */
+
+static inline void thread_group_cputime_init(struct signal_struct *sig)
+{
+	sig->cputime.totals.utime = cputime_zero;
+	sig->cputime.totals.stime = cputime_zero;
+	sig->cputime.totals.sum_exec_runtime = 0;
+}
+
+static inline int thread_group_cputime_alloc(struct task_struct *tsk)
+{
+	return 0;
+}
+
+static inline void thread_group_cputime_free(struct signal_struct *sig)
+{
+}
+
+static inline int thread_group_cputime_clone_thread(struct task_struct *curr,
+						     struct task_struct *tsk)
+{
+}
+
+static inline void thread_group_cputime(struct task_struct *tsk,
+					 struct task_cputime *cputime)
+{
+	*cputime = tsk->signal->cputime.totals;
+}
+
+static inline void thread_group_cputime_account_user(
+	struct thread_group_cputime *tgtimes,
+	cputime_t cputime)
+{
+	tgtimes->totals->utime = cputime_add(tgtimes->totals->utime, cputime);
+}
+
+static inline void thread_group_cputime_account_system(
+	struct thread_group_cputime *tgtimes,
+	cputime_t cputime)
+{
+	tgtimes->totals->stime = cputime_add(tgtimes->totals->stime, cputime);
+}
+
+static inline void thread_group_cputime_account_exec_runtime(
+	struct thread_group_cputime *tgtimes,
+	unsigned long long ns)
+{
+	tgtimes->totals->sum_exec_runtime += ns;
+}
+
+#endif /* CONFIG_SMP */
+
+static inline void account_group_user_time(struct task_struct *tsk,
+					    cputime_t cputime)
+{
+	struct signal_struct *sig;
+
+	sig = tsk->signal;
+	if (likely(sig))
+		thread_group_cputime_account_user(&sig->cputime, cputime);
+}
+
+static inline void account_group_system_time(struct task_struct *tsk,
+					      cputime_t cputime)
+{
+	struct signal_struct *sig;
+
+	sig = tsk->signal;
+	if (likely(sig))
+		thread_group_cputime_account_system(&sig->cputime, cputime);
+}
+
+static inline void account_group_exec_runtime(struct task_struct *tsk,
+					       unsigned long long ns)
+{
+	struct signal_struct *sig;
+
+	sig = tsk->signal;
+	if (likely(sig))
+		thread_group_cputime_account_exec_runtime(&sig->cputime, ns);
+}
+
 /*
  * Reevaluate whether the task has signals pending delivery.
  * Wake the task if so.

commit f7d0b926ac8c8ec0c7a83ee69409bd2e6bb39f81
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue Sep 9 15:43:22 2008 -0700

    mm: define USE_SPLIT_PTLOCKS rather than repeating expression
    
    Define USE_SPLIT_PTLOCKS as a constant expression rather than repeating
    "NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS" all over the place.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d9120c5ad15..272c35309df2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -352,7 +352,7 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 extern void arch_unmap_area(struct mm_struct *, unsigned long);
 extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 
-#if NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS
+#if USE_SPLIT_PTLOCKS
 /*
  * The mm counters are not protected by its page_table_lock,
  * so must be incremented atomically.
@@ -363,7 +363,7 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #define inc_mm_counter(mm, member) atomic_long_inc(&(mm)->_##member)
 #define dec_mm_counter(mm, member) atomic_long_dec(&(mm)->_##member)
 
-#else  /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
+#else  /* !USE_SPLIT_PTLOCKS */
 /*
  * The mm counters are protected by its page_table_lock,
  * so can be incremented directly.
@@ -374,7 +374,7 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #define inc_mm_counter(mm, member) (mm)->_##member++
 #define dec_mm_counter(mm, member) (mm)->_##member--
 
-#endif /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
+#endif /* !USE_SPLIT_PTLOCKS */
 
 #define get_mm_rss(mm)					\
 	(get_mm_counter(mm, file_rss) + get_mm_counter(mm, anon_rss))

commit 7f79d852ed30a06eebf7497afe9334a726db3d40
Merge: aef745fca016 70bb08962ea9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 6 16:51:57 2008 +0200

    Merge branch 'linus' into sched/devel

commit 6976675d94042fbd446231d1bd8b7de71a980ada
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Sep 1 15:52:40 2008 -0700

    hrtimer: create a "timer_slack" field in the task struct
    
    We want to be able to control the default "rounding" that is used by
    select() and poll() and friends. This is a per process property
    (so that we can have a "nice" like program to start certain programs with
    a looser or stricter rounding) that can be set/get via a prctl().
    
    For this purpose, a field called "timer_slack_ns" is added to the task
    struct. In addition, a field called "default_timer_slack"ns" is added
    so that tasks easily can temporarily to a more/less accurate slack and then
    back to the default.
    
    The default value of the slack is set to 50 usec; this is significantly less
    than 2.6.27's average select() and poll() timing error but still allows
    the kernel to group timers somewhat to preserve power behavior. Applications
    and admins can override this via the prctl()
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d9120c5ad15..dcc03fd5a7f3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1301,6 +1301,12 @@ struct task_struct {
 	int latency_record_count;
 	struct latency_record latency_record[LT_SAVECOUNT];
 #endif
+	/*
+	 * time slack values; these are used to round up poll() and
+	 * select() etc timeout values. These are in nanoseconds.
+	 */
+	unsigned long timer_slack_ns;
+	unsigned long default_timer_slack_ns;
 };
 
 /*

commit 49048622eae698e5c4ae61f7e71200f265ccc529
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Fri Sep 5 18:12:23 2008 +0200

    sched: fix process time monotonicity
    
    Spencer reported a problem where utime and stime were going negative despite
    the fixes in commit b27f03d4bdc145a09fb7b0c0e004b29f1ee555fa. The suspected
    reason for the problem is that signal_struct maintains it's own utime and
    stime (of exited tasks), these are not updated using the new task_utime()
    routine, hence sig->utime can go backwards and cause the same problem
    to occur (sig->utime, adds tsk->utime and not task_utime()). This patch
    fixes the problem
    
    TODO: using max(task->prev_utime, derived utime) works for now, but a more
    generic solution is to implement cputime_max() and use the cputime_gt()
    function for comparison.
    
    Reported-by: spencer@bluehost.com
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cfb0d87b99fc..3d9120c5ad15 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1475,6 +1475,10 @@ static inline void put_task_struct(struct task_struct *t)
 		__put_task_struct(t);
 }
 
+extern cputime_t task_utime(struct task_struct *p);
+extern cputime_t task_stime(struct task_struct *p);
+extern cputime_t task_gtime(struct task_struct *p);
+
 /*
  * Per process flags
  */

commit bee367ed066e26c14263d808136fba8eec3bd70a
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Fri Aug 1 13:24:08 2008 +0100

    sched: reorder struct sched_rt_entity to remove padding on 64 bit builds
    
    remove 8 bytes of padding on 64 bit builds
    (also removes 8 bytes from task_struct)
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a8058e44f58..08a87b5f29e1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1010,8 +1010,8 @@ struct sched_entity {
 
 struct sched_rt_entity {
 	struct list_head run_list;
-	unsigned int time_slice;
 	unsigned long timeout;
+	unsigned int time_slice;
 	int nr_cpus_allowed;
 
 	struct sched_rt_entity *back;

commit 07dd20e0324f4d3e33bde1944d4f7771a09c498c
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Fri Aug 1 13:18:04 2008 +0100

    sched: reorder signal_struct to remove 8 bytes on 64 bit builds
    
    reorder structure to remove 8 bytes of padding on 64 bit builds
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cfb0d87b99fc..5a8058e44f58 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -451,8 +451,8 @@ struct signal_struct {
 	 * - everyone except group_exit_task is stopped during signal delivery
 	 *   of fatal signals, group_exit_task processes the signal.
 	 */
-	struct task_struct	*group_exit_task;
 	int			notify_count;
+	struct task_struct	*group_exit_task;
 
 	/* thread group stop support, overloads group_exit_code too */
 	int			group_stop_count;

commit 9e2b2dc4133f65272a6d3c5dcb2ce63f8a87cae9
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 13 16:20:04 2008 +0100

    CRED: Introduce credential access wrappers
    
    The patches that are intended to introduce copy-on-write credentials for 2.6.28
    require abstraction of access to some fields of the task structure,
    particularly for the case of one task accessing another's credentials where RCU
    will have to be observed.
    
    Introduced here are trivial no-op versions of the desired accessors for current
    and other tasks so that other subsystems can start to be converted over more
    easily.
    
    Wrappers are introduced into a new header (linux/cred.h) for UID/GID,
    EUID/EGID, SUID/SGID, FSUID/FSGID, cap_effective and current's subscribed
    user_struct.  These wrappers are macros because the ordering between header
    files mitigates against making them inline functions.
    
    linux/cred.h is #included from linux/sched.h.
    
    Further, XFS is modified such that it no longer defines and uses parameterised
    versions of current_fs[ug]id(), thus getting rid of the namespace collision
    otherwise incurred.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5850bfb968a8..cfb0d87b99fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -87,6 +87,7 @@ struct sched_param {
 #include <linux/task_io_accounting.h>
 #include <linux/kobject.h>
 #include <linux/latencytop.h>
+#include <linux/cred.h>
 
 #include <asm/processor.h>
 

commit c1955a3d4762e7a9bf84035eb3c4886a900f0d15
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 11 08:59:03 2008 +0200

    sched_clock: delay using sched_clock()
    
    Some arch's can't handle sched_clock() being called too early - delay
    this until sched_clock_init() has been called.
    
    Reported-by: Bill Gatliff <bgat@billgatliff.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Nishanth Aravamudan <nacc@us.ibm.com>
    CC: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea436bc1a0e2..5850bfb968a8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1551,16 +1551,10 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 
 extern unsigned long long sched_clock(void);
 
-#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
-static inline void sched_clock_init(void)
-{
-}
-
-static inline u64 sched_clock_cpu(int cpu)
-{
-	return sched_clock();
-}
+extern void sched_clock_init(void);
+extern u64 sched_clock_cpu(int cpu);
 
+#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
 static inline void sched_clock_tick(void)
 {
 }
@@ -1573,8 +1567,6 @@ static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
 #else
-extern void sched_clock_init(void);
-extern u64 sched_clock_cpu(int cpu);
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);

commit e4e4e534faa3c2be4e165ce414f44b76ada7208c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 14 08:50:02 2008 +0200

    sched clock: revert various sched_clock() changes
    
    Found an interactivity problem on a quad core test-system - simple
    CPU loops would occasionally delay the system un an unacceptable way.
    
    After much debugging with Peter Zijlstra it turned out that the problem
    is caused by the string of sched_clock() changes - they caused the CPU
    clock to jump backwards a bit - which confuses the scheduler arithmetics.
    
    (which is unsigned for performance reasons)
    
    So revert:
    
     # c300ba2: sched_clock: and multiplier for TSC to gtod drift
     # c0c8773: sched_clock: only update deltas with local reads.
     # af52a90: sched_clock: stop maximum check on NO HZ
     # f7cce27: sched_clock: widen the max and min time
    
    This solves the interactivity problems.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Mike Galbraith <efault@gmx.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5270d449ff9d..ea436bc1a0e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1572,28 +1572,13 @@ static inline void sched_clock_idle_sleep_event(void)
 static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
-
-#ifdef CONFIG_NO_HZ
-static inline void sched_clock_tick_stop(int cpu)
-{
-}
-
-static inline void sched_clock_tick_start(int cpu)
-{
-}
-#endif
-
-#else /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
+#else
 extern void sched_clock_init(void);
 extern u64 sched_clock_cpu(int cpu);
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
-#ifdef CONFIG_NO_HZ
-extern void sched_clock_tick_stop(int cpu);
-extern void sched_clock_tick_start(int cpu);
 #endif
-#endif /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
 
 /*
  * For kernel-internal use: high-speed (but slightly incorrect) per-cpu

commit 940389b8afad6495211614c13eb91ef7001773ec
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Mon Jul 28 00:48:12 2008 +0200

    task IO accounting: move all IO statistics in struct task_io_accounting
    
    Simplify the code of include/linux/task_io_accounting.h.
    
    It is also more reasonable to have all the task i/o-related statistics in a
    single struct (task_io_accounting).
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 034c1ca6b332..5270d449ff9d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -505,7 +505,7 @@ struct signal_struct {
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
-	struct proc_io_accounting ioac;
+	struct task_io_accounting ioac;
 
 	/*
 	 * Cumulative ns of scheduled CPU time for dead threads in the
@@ -1253,7 +1253,7 @@ struct task_struct {
 
 	unsigned long ptrace_message;
 	siginfo_t *last_siginfo; /* For ptrace use.  */
-	struct proc_io_accounting ioac;
+	struct task_io_accounting ioac;
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
@@ -2183,22 +2183,22 @@ extern long sched_group_rt_period(struct task_group *tg);
 #ifdef CONFIG_TASK_XACCT
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
 {
-	tsk->ioac.chr.rchar += amt;
+	tsk->ioac.rchar += amt;
 }
 
 static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
 {
-	tsk->ioac.chr.wchar += amt;
+	tsk->ioac.wchar += amt;
 }
 
 static inline void inc_syscr(struct task_struct *tsk)
 {
-	tsk->ioac.chr.syscr++;
+	tsk->ioac.syscr++;
 }
 
 static inline void inc_syscw(struct task_struct *tsk)
 {
-	tsk->ioac.chr.syscw++;
+	tsk->ioac.syscw++;
 }
 #else
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)

commit 5995477ab7f3522c497c9c4a1c55373e9d655574
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sun Jul 27 17:29:15 2008 +0200

    task IO accounting: improve code readability
    
    Put all i/o statistics in struct proc_io_accounting and use inline functions to
    initialize and increment statistics, removing a lot of single variable
    assignments.
    
    This also reduces the kernel size as following (with CONFIG_TASK_XACCT=y and
    CONFIG_TASK_IO_ACCOUNTING=y).
    
        text    data     bss     dec     hex filename
       11651       0       0   11651    2d83 kernel/exit.o.before
       11619       0       0   11619    2d63 kernel/exit.o.after
       10886     132     136   11154    2b92 kernel/fork.o.before
       10758     132     136   11026    2b12 kernel/fork.o.after
    
     3082029  807968 4818600 8708597  84e1f5 vmlinux.o.before
     3081869  807968 4818600 8708437  84e155 vmlinux.o.after
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f59318a0099b..034c1ca6b332 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -505,10 +505,7 @@ struct signal_struct {
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
-#ifdef CONFIG_TASK_XACCT
-	u64 rchar, wchar, syscr, syscw;
-#endif
-	struct task_io_accounting ioac;
+	struct proc_io_accounting ioac;
 
 	/*
 	 * Cumulative ns of scheduled CPU time for dead threads in the
@@ -1256,11 +1253,7 @@ struct task_struct {
 
 	unsigned long ptrace_message;
 	siginfo_t *last_siginfo; /* For ptrace use.  */
-#ifdef CONFIG_TASK_XACCT
-/* i/o counters(bytes read/written, #syscalls */
-	u64 rchar, wchar, syscr, syscw;
-#endif
-	struct task_io_accounting ioac;
+	struct proc_io_accounting ioac;
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
@@ -2190,22 +2183,22 @@ extern long sched_group_rt_period(struct task_group *tg);
 #ifdef CONFIG_TASK_XACCT
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
 {
-	tsk->rchar += amt;
+	tsk->ioac.chr.rchar += amt;
 }
 
 static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
 {
-	tsk->wchar += amt;
+	tsk->ioac.chr.wchar += amt;
 }
 
 static inline void inc_syscr(struct task_struct *tsk)
 {
-	tsk->syscr++;
+	tsk->ioac.chr.syscr++;
 }
 
 static inline void inc_syscw(struct task_struct *tsk)
 {
-	tsk->syscw++;
+	tsk->ioac.chr.syscw++;
 }
 #else
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)

commit 85ba2d862e521375a8ee01526c5c46b1f24bb4af
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:58 2008 -0700

    tracehook: wait_task_inactive
    
    This extends wait_task_inactive() with a new argument so it can be used in
    a "soft" mode where it will check for the task changing state unexpectedly
    and back off.  There is no change to existing callers.  This lays the
    groundwork to allow robust, noninvasive tracing that can try to sample a
    blocked thread but back off safely if it wakes up.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a95d84d0da95..f59318a0099b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1882,9 +1882,13 @@ extern void set_task_comm(struct task_struct *tsk, char *from);
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
-extern void wait_task_inactive(struct task_struct * p);
+extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
-#define wait_task_inactive(p)	do { } while (0)
+static inline unsigned long wait_task_inactive(struct task_struct *p,
+					       long match_state)
+{
+	return 1;
+}
 #endif
 
 #define next_task(p)	list_entry(rcu_dereference((p)->tasks.next), struct task_struct, tasks)

commit 2b2a1ff64afbadac842bbc58c5166962cf4f7664
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:54 2008 -0700

    tracehook: death
    
    This moves the ptrace logic in task death (exit_notify) into tracehook.h
    inlines.  Some code is rearranged slightly to make things nicer.  There is
    no change, only cleanup.
    
    There is one hook called with the tasklist_lock write-locked, as ptrace
    needs.  There is also a new hook called after exit_state changes and
    without locks.  This is a better place for tracing work to be in the
    future, since it doesn't delay the whole system with locking.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index adb8077dc463..a95d84d0da95 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1796,7 +1796,7 @@ extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern void do_notify_parent(struct task_struct *, int);
+extern int do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);

commit 7babe8db99d305340cf4828ce1f5a1481d5622ef
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Fri Jul 25 19:45:11 2008 -0700

    Full conversion to early_initcall() interface, remove old interface
    
    A previous patch added the early_initcall(), to allow a cleaner hooking of
    pre-SMP initcalls.  Now we remove the older interface, converting all
    existing users to the new one.
    
    [akpm@linux-foundation.org: cleanups]
    [akpm@linux-foundation.org: build fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3260a5c42b91..adb8077dc463 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -292,7 +292,6 @@ extern void sched_show_task(struct task_struct *p);
 
 #ifdef CONFIG_DETECT_SOFTLOCKUP
 extern void softlockup_tick(void);
-extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern unsigned int  softlockup_panic;
@@ -2222,14 +2221,6 @@ static inline void inc_syscw(struct task_struct *tsk)
 }
 #endif
 
-#ifdef CONFIG_SMP
-void migration_init(void);
-#else
-static inline void migration_init(void)
-{
-}
-#endif
-
 #ifndef TASK_SIZE_OF
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif

commit 16d69265b930f7e2fa9eea381715696f780718f4
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Jul 25 19:44:36 2008 -0700

    uninline arch_pick_mmap_layout()
    
    Fix this, on avr32:
    
      include/linux/utsname.h:35,
                       from init/main.c:20:
      include/linux/sched.h: In function 'arch_pick_mmap_layout':
      include/linux/sched.h:2149: error: implicit declaration of function 'PAGE_ALIGN'
    
    Reported-by: Adrian Bunk <bunk@kernel.org>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 42036ffe6b00..3260a5c42b91 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2139,16 +2139,7 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 
 #endif /* CONFIG_SMP */
 
-#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT
 extern void arch_pick_mmap_layout(struct mm_struct *mm);
-#else
-static inline void arch_pick_mmap_layout(struct mm_struct *mm)
-{
-	mm->mmap_base = TASK_UNMAPPED_BASE;
-	mm->get_unmapped_area = arch_get_unmapped_area;
-	mm->unmap_area = arch_unmap_area;
-}
-#endif
 
 #ifdef CONFIG_TRACING
 extern void

commit 873b47717732c2f33a4b14de02571a4295a02f0c
Author: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
Date:   Fri Jul 25 01:48:52 2008 -0700

    per-task-delay-accounting: add memory reclaim delay
    
    Sometimes, application responses become bad under heavy memory load.
    Applications take a bit time to reclaim memory.  The statistics, how long
    memory reclaim takes, will be useful to measure memory usage.
    
    This patch adds accounting memory reclaim to per-task-delay-accounting for
    accounting the time of do_try_to_free_pages().
    
    <i.e>
    
    - When System is under low memory load,
      memory reclaim may not occur.
    
    $ free
                 total       used       free     shared    buffers     cached
    Mem:       8197800    1577300    6620500          0       4808    1516724
    -/+ buffers/cache:      55768    8142032
    Swap:     16386292          0   16386292
    
    $ vmstat 1
    procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
     0  0      0 5069748  10612 3014060    0    0     0     0    3   26  0  0 100  0
     0  0      0 5069748  10612 3014060    0    0     0     0    4   22  0  0 100  0
     0  0      0 5069748  10612 3014060    0    0     0     0    3   18  0  0 100  0
    
    Measure the time of tar command.
    
    $ ls -s test.dat
    1501472 test.dat
    
    $ time tar cvf test.tar test.dat
    real    0m13.388s
    user    0m0.116s
    sys     0m5.304s
    
    $ ./delayget -d -p <pid>
    CPU             count     real total  virtual total    delay total
                      428     5528345500     5477116080       62749891
    IO              count    delay total
                      338     8078977189
    SWAP            count    delay total
                        0              0
    RECLAIM         count    delay total
                        0              0
    
    - When system is under heavy memory load
      memory reclaim may occur.
    
    $ vmstat 1
    procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
     0  0 7159032  49724   1812   3012    0    0     0     0    3   24  0  0 100  0
     0  0 7159032  49724   1812   3012    0    0     0     0    4   24  0  0 100  0
     0  0 7159032  49848   1812   3012    0    0     0     0    3   22  0  0 100  0
    
    In this case, one process uses more 8G memory
    by execution of malloc() and memset().
    
    $ time tar cvf test.tar test.dat
    real    1m38.563s        <-  increased by 85 sec
    user    0m0.140s
    sys     0m7.060s
    
    $ ./delayget -d -p <pid>
    CPU             count     real total  virtual total    delay total
                     9021     7140446250     7315277975      923201824
    IO              count    delay total
                     8965    90466349669
    SWAP            count    delay total
                        3       21036367
    RECLAIM         count    delay total
                      740    61011951153
    
    In the later case, the value of RECLAIM is increasing.
    So, taskstats can show how much memory reclaim influences TAT.
    
    Signed-off-by: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujistu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d22ffe06d0eb..42036ffe6b00 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -672,6 +672,10 @@ struct task_delay_info {
 				/* io operations performed */
 	u32 swapin_count;	/* total count of the number of swapin block */
 				/* io operations performed */
+
+	struct timespec freepages_start, freepages_end;
+	u64 freepages_delay;	/* wait for memory reclaim */
+	u32 freepages_count;	/* total count of memory reclaim */
 };
 #endif	/* CONFIG_TASK_DELAY_ACCT */
 

commit 297c5d92634c809cef23d73e7b2556f2528ff7e2
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Fri Jul 25 01:48:49 2008 -0700

    task IO accounting: provide distinct tgid/tid I/O statistics
    
    Report per-thread I/O statistics in /proc/pid/task/tid/io and aggregate
    parent I/O statistics in /proc/pid/io.  This approach follows the same
    model used to account per-process and per-thread CPU times.
    
    As a practial application, this allows for example to quickly find the top
    I/O consumer when a process spawns many child threads that perform the
    actual I/O work, because the aggregated I/O statistics can always be found
    in /proc/pid/io.
    
    [ Oleg Nesterov points out that we should check that the task is still
      alive before we iterate over the threads, but also says that we can do
      that fixup on top of this later.  - Linus ]
    
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Cc: Matt Heaton <matt@hostmonster.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Acked-by-with-comments: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af780f299c7c..d22ffe06d0eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -506,6 +506,10 @@ struct signal_struct {
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
+#ifdef CONFIG_TASK_XACCT
+	u64 rchar, wchar, syscr, syscw;
+#endif
+	struct task_io_accounting ioac;
 
 	/*
 	 * Cumulative ns of scheduled CPU time for dead threads in the

commit 49b5cf34727a6c1be1568ab28e89a2d9a6bf51e0
Author: Jonathan Lim <jlim@sgi.com>
Date:   Fri Jul 25 01:48:40 2008 -0700

    accounting: account for user time when updating memory integrals
    
    Adapt acct_update_integrals() to include user time when calculating the time
    difference.  The units of acct_rss_mem1 and acct_vm_mem1 are also changed from
    pages-jiffies to pages-usecs to avoid calling jiffies_to_usecs() in
    xacct_add_tsk() which might overflow.
    
    Signed-off-by: Jonathan Lim <jlim@sgi.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 354ef478a80d..af780f299c7c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1257,7 +1257,7 @@ struct task_struct {
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
-	cputime_t acct_stimexpd;/* stime since last update */
+	cputime_t acct_timexpd;	/* stime + utime since last update */
 #endif
 #ifdef CONFIG_CPUSETS
 	nodemask_t mems_allowed;

commit dbda0de52618d13d1b927c7ba7bb839cfddc4e8c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Jul 25 01:48:37 2008 -0700

    pidns: remove find_task_by_pid, unused for a long time
    
    It seems to me that it was a mistake marking this function as deprecated
    and scheduling it for removal, rather than resolutely removing it after
    the last caller's death.
    
    Anyway - better late, then never.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 182da1550fad..354ef478a80d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1715,8 +1715,6 @@ extern struct pid_namespace init_pid_ns;
  *      finds a task by its pid in the specified namespace
  * find_task_by_vpid():
  *      finds a task by its virtual pid
- * find_task_by_pid():
- *      finds a task by its global pid
  *
  * see also find_vpid() etc in include/linux/pid.h
  */
@@ -1724,10 +1722,6 @@ extern struct pid_namespace init_pid_ns;
 extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,
 		struct pid_namespace *ns);
 
-static inline struct task_struct *__deprecated find_task_by_pid(pid_t nr)
-{
-	return find_task_by_pid_type_ns(PIDTYPE_PID, nr, &init_pid_ns);
-}
 extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);

commit e49859e71e0318b564de1546bdc30fab738f9deb
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Jul 25 01:48:36 2008 -0700

    pidns: remove now unused find_pid function.
    
    This one had the only users so far - the kill_proc, which is removed, so
    drop this (invalid in namespaced world) call too.
    
    And of course - erase all references on it from comments.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 134cb5cb506c..182da1550fad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1718,7 +1718,7 @@ extern struct pid_namespace init_pid_ns;
  * find_task_by_pid():
  *      finds a task by its global pid
  *
- * see also find_pid() etc in include/linux/pid.h
+ * see also find_vpid() etc in include/linux/pid.h
  */
 
 extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,

commit 19b0cfcca41dd772065671ad0584e1cea0f3fd13
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Jul 25 01:48:35 2008 -0700

    pidns: remove now unused kill_proc function
    
    This function operated on a pid_t to kill a task, which is no longer valid
    in a containerized system.
    
    It has finally lost all its users and we can safely remove it from the
    tree.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0560999eb1db..134cb5cb506c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1800,7 +1800,6 @@ extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
 extern void zap_other_threads(struct task_struct *p);
-extern int kill_proc(pid_t, int, int);
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);

commit 246bb0b1deb29726990620d8b5e55ca29f331362
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:38 2008 -0700

    kill PF_BORROWED_MM in favour of PF_KTHREAD
    
    Kill PF_BORROWED_MM.  Change use_mm/unuse_mm to not play with ->flags, and
    do s/PF_BORROWED_MM/PF_KTHREAD/ for a couple of other users.
    
    No functional changes yet.  But this allows us to do further
    fixes/cleanups.
    
    oom_kill/ptrace/etc often check "p->mm != NULL" to filter out the
    kthreads, this is wrong because of use_mm().  The problem with
    PF_BORROWED_MM is that we need task_lock() to avoid races.  With this
    patch we can check PF_KTHREAD directly, or use a simple lockless helper:
    
            /* The result must not be dereferenced !!! */
            struct mm_struct *__get_task_mm(struct task_struct *tsk)
            {
                    if (tsk->flags & PF_KTHREAD)
                            return NULL;
                    return tsk->mm;
            }
    
    Note also ecard_task().  It runs with ->mm != NULL, but it's the kernel
    thread without PF_BORROWED_MM.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eec64a4adb9d..0560999eb1db 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1483,7 +1483,6 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
-#define PF_KTHREAD	0x00000020	/* I am a kernel thread */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */
@@ -1497,7 +1496,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_KSWAPD	0x00040000	/* I am kswapd */
 #define PF_SWAPOFF	0x00080000	/* I am in swapoff */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
-#define PF_BORROWED_MM	0x00200000	/* I am a kthread doing use_mm */
+#define PF_KTHREAD	0x00200000	/* I am a kernel thread */
 #define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */
 #define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
 #define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */

commit 7b34e4283c685f5cc6ba6d30e939906eee0d4bcf
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:37 2008 -0700

    introduce PF_KTHREAD flag
    
    Introduce the new PF_KTHREAD flag to mark the kernel threads.  It is set
    by INIT_TASK() and copied to the forked childs (we could set it in
    kthreadd() along with PF_NOFREEZE instead).
    
    daemonize() was changed as well.  In that case testing of PF_KTHREAD is
    racy, but daemonize() is hopeless anyway.
    
    This flag is cleared in do_execve(), before search_binary_handler().
    Probably not the best place, we can do this in exec_mmap() or in
    start_thread(), or clear it along with PF_FORKNOEXEC.  But I think this
    doesn't matter in practice, and if do_execve() fails kthread should die
    soon.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 79e749dbf81e..eec64a4adb9d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1483,6 +1483,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
+#define PF_KTHREAD	0x00000020	/* I am a kernel thread */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */

commit 364d3c13c17f45da6d638011078d4c4d3070d719
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:36 2008 -0700

    ptrace: give more respect to SIGKILL
    
    ptrace_stop() has some complicated checks to prevent the scheduling in the
    TASK_TRACED state with the pending SIGKILL, but these checks are racy, and
    they depend on arch_ptrace_stop_needed().
    
    This patch assumes that the traced task should die asap if it was killed by
    SIGKILL, in that case schedule()->signal_pending_state() has no reason to
    ignore the TASK_WAKEKILL part of TASK_TRACED, and we can kill this nasty
    special case.
    
    Note: do_exit()->ptrace_notify() is special, the killed task can already
    dequeue SIGKILL at this point. Another indication that fatal_signal_pending()
    is not exactly right.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6aca4a16e377..79e749dbf81e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2054,9 +2054,6 @@ static inline int signal_pending_state(long state, struct task_struct *p)
 	if (!signal_pending(p))
 		return 0;
 
-	if (state & (__TASK_STOPPED | __TASK_TRACED))
-		return 0;
-
 	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
 }
 

commit 8b05c7e6e159d2f33c9275281b8b909a89eb7c5d
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Jul 23 21:26:53 2008 -0700

    add a helper function to test if an object is on the stack
    
    lib/debugobjects.c has a function to test if an object is on the stack.
    The block layer and ide needs it (they need to avoid DMA from/to stack
    buffers).  This patch moves the function to include/linux/sched.h so that
    everyone can use it.
    
    lib/debugobjects.c uses current->stack but this patch uses a
    task_stack_page() accessor, which is a preferable way to access the stack.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc7e592c473a..6aca4a16e377 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1983,6 +1983,13 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 
 #endif
 
+static inline int object_is_on_stack(void *obj)
+{
+	void *stack = task_stack_page(current);
+
+	return (obj >= stack) && (obj < (stack + THREAD_SIZE));
+}
+
 extern void thread_info_cache_init(void);
 
 /* set thread flags in other task's structures

commit 7f9dce38378f0a4a298e885553d6bb7121376376
Merge: 26dcce0fabbe ba42059fbd0a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 23 19:36:53 2008 -0700

    Merge branch 'sched/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: hrtick_enabled() should use cpu_active()
      sched, x86: clean up hrtick implementation
      sched: fix build error, provide partition_sched_domains() unconditionally
      sched: fix warning in inc_rt_tasks() to not declare variable 'rq' if it's not needed
      cpu hotplug: Make cpu_active_map synchronization dependency clear
      cpu hotplug, sched: Introduce cpu_active_map and redo sched domain managment (take 2)
      sched: rework of "prioritize non-migratable tasks over migratable ones"
      sched: reduce stack size in isolated_cpu_setup()
      Revert parts of "ftrace: do not trace scheduler functions"
    
    Fixed up conflicts in include/asm-x86/thread_info.h (due to the
    TIF_SINGLESTEP unification vs TIF_HRTICK_RESCHED removal) and
    kernel/sched_fair.c (due to cpu_active_map vs for_each_cpu_mask_nr()
    introduction).

commit d7b6de14a0ef8a376f9d57b867545b47302b7bfb
Merge: 30d38542ec77 4dca10a96041
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 23 18:34:13 2008 -0700

    Merge branch 'core/softlockup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core/softlockup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softlockup: fix invalid proc_handler for softlockup_panic
      softlockup: fix watchdog task wakeup frequency
      softlockup: fix watchdog task wakeup frequency
      softlockup: show irqtrace
      softlockup: print a module list on being stuck
      softlockup: fix NMI hangs due to lock race - 2.6.26-rc regression
      softlockup: fix false positives on nohz if CPU is 100% idle for more than 60 seconds
      softlockup: fix softlockup_thresh fix
      softlockup: fix softlockup_thresh unaligned access and disable detection at runtime
      softlockup: allow panic on lockup

commit 1b427c153a08fdbc092c2bdbf845b92fda58d857
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 14:01:39 2008 +0200

    sched: fix build error, provide partition_sched_domains() unconditionally
    
    provide an empty partition_sched_domains() definition for the UP case:
    
     include/linux/cpuset.h: In function â€˜rebuild_sched_domains':
     include/linux/cpuset.h:163: error: implicit declaration of function â€˜partition_sched_domains'
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1941d8b5cf11..26da921530fe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -824,7 +824,16 @@ extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
 				    struct sched_domain_attr *dattr_new);
 extern int arch_reinit_sched_domains(void);
 
-#endif	/* CONFIG_SMP */
+#else /* CONFIG_SMP */
+
+struct sched_domain_attr;
+
+static inline void
+partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
+			struct sched_domain_attr *dattr_new)
+{
+}
+#endif	/* !CONFIG_SMP */
 
 struct io_context;			/* See blkdev.h */
 #define NGROUPS_SMALL		32

commit f470021adb9190819c03d6d8c5c860a17480aa6d
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Mar 24 18:36:23 2008 -0700

    ptrace children revamp
    
    ptrace no longer fiddles with the children/sibling links, and the
    old ptrace_children list is gone.  Now ptrace, whether of one's own
    children or another's via PTRACE_ATTACH, just uses the new ptraced
    list instead.
    
    There should be no user-visible difference that matters.  The only
    change is the order in which do_wait() sees multiple stopped
    children and stopped ptrace attachees.  Since wait_task_stopped()
    was changed earlier so it no longer reorders the children list, we
    already know this won't cause any new problems.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ba2f859c6e4f..1941d8b5cf11 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1062,12 +1062,6 @@ struct task_struct {
 #endif
 
 	struct list_head tasks;
-	/*
-	 * ptrace_list/ptrace_children forms the list of my children
-	 * that were stolen by a ptracer.
-	 */
-	struct list_head ptrace_children;
-	struct list_head ptrace_list;
 
 	struct mm_struct *mm, *active_mm;
 
@@ -1089,18 +1083,25 @@ struct task_struct {
 	/* 
 	 * pointers to (original) parent process, youngest child, younger sibling,
 	 * older sibling, respectively.  (p->father can be replaced with 
-	 * p->parent->pid)
+	 * p->real_parent->pid)
 	 */
-	struct task_struct *real_parent; /* real parent process (when being debugged) */
-	struct task_struct *parent;	/* parent process */
+	struct task_struct *real_parent; /* real parent process */
+	struct task_struct *parent; /* recipient of SIGCHLD, wait4() reports */
 	/*
-	 * children/sibling forms the list of my children plus the
-	 * tasks I'm ptracing.
+	 * children/sibling forms the list of my natural children
 	 */
 	struct list_head children;	/* list of my children */
 	struct list_head sibling;	/* linkage in my parent's children list */
 	struct task_struct *group_leader;	/* threadgroup leader */
 
+	/*
+	 * ptraced is the list of tasks this task is using ptrace on.
+	 * This includes both natural children and PTRACE_ATTACH targets.
+	 * p->ptrace_entry is p's link on the p->parent->ptraced list.
+	 */
+	struct list_head ptraced;
+	struct list_head ptrace_entry;
+
 	/* PID/PID hash table linkage. */
 	struct pid_link pids[PIDTYPE_MAX];
 	struct list_head thread_group;
@@ -1876,9 +1877,6 @@ extern void wait_task_inactive(struct task_struct * p);
 #define wait_task_inactive(p)	do { } while (0)
 #endif
 
-#define remove_parent(p)	list_del_init(&(p)->sibling)
-#define add_parent(p)		list_add_tail(&(p)->sibling,&(p)->parent->children)
-
 #define next_task(p)	list_entry(rcu_dereference((p)->tasks.next), struct task_struct, tasks)
 
 #define for_each_process(p) \

commit ebb12db51f6c13b30752fcf506baad4c617b153c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Jun 11 22:04:29 2008 +0200

    Freezer: Introduce PF_FREEZER_NOSIG
    
    The freezer currently attempts to distinguish kernel threads from
    user space tasks by checking if their mm pointer is unset and it
    does not send fake signals to kernel threads.  However, there are
    kernel threads, mostly related to networking, that behave like
    user space tasks and may want to be sent a fake signal to be frozen.
    
    Introduce the new process flag PF_FREEZER_NOSIG that will be set
    by default for all kernel threads and make the freezer only send
    fake signals to the tasks having PF_FREEZER_NOSIG unset.  Provide
    the set_freezable_with_signal() function to be called by the kernel
    threads that want to be sent a fake signal for freezing.
    
    This patch should not change the freezer's observable behavior.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Pavel Machek <pavel@suse.cz>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 21349173d148..ba2f859c6e4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1494,6 +1494,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezeable */
+#define PF_FREEZER_NOSIG 0x80000000	/* Freezer won't send signals to it */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 1e09481365ce248dbb4eb06dad70129bb5807037
Merge: 3e2f69fdd1b0 b9d2252c1e44
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 23:12:58 2008 +0200

    Merge branch 'linus' into core/softlockup
    
    Conflicts:
    
            kernel/softlockup.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 948769a5ba304ed3329a2f42ee3561f04a0b5692
Merge: e18425a0abc8 773dc8eacaed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 14:50:49 2008 -0700

    Merge branch 'sched/new-API-sched_setscheduler' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched/new-API-sched_setscheduler' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: add new API sched_setscheduler_nocheck: add a flag to control access checks

commit 5806b81ac1c0c52665b91723fd4146a4f86e386b
Merge: d14c8a680ccf 6712e299b7dc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 16:11:52 2008 +0200

    Merge branch 'auto-ftrace-next' into tracing/for-linus
    
    Conflicts:
    
            arch/x86/kernel/entry_32.S
            arch/x86/kernel/process_32.c
            arch/x86/kernel/process_64.c
            arch/x86/lib/Makefile
            include/asm-x86/irqflags.h
            kernel/Makefile
            kernel/sched.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 361833efac4d277d209008e1e0658e597bc1bdef
Merge: 54ef76f37bcc c300ba252829
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 12:19:13 2008 +0200

    Merge branch 'sched/clock' into sched/devel

commit af52a90a14cdaa54ecbfb6e6982abb13466a4b56
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Jul 7 14:16:52 2008 -0400

    sched_clock: stop maximum check on NO HZ
    
    Working with ftrace I would get large jumps of 11 millisecs or more with
    the clock tracer. This killed the latencing timings of ftrace and also
    caused the irqoff self tests to fail.
    
    What was happening is with NO_HZ the idle would stop the jiffy counter and
    before the jiffy counter was updated the sched_clock would have a bad
    delta jiffies to compare with the gtod with the maximum.
    
    The jiffies would stop and the last sched_tick would record the last gtod.
    On wakeup, the sched clock update would compare the gtod + delta jiffies
    (which would be zero) and compare it to the TSC. The TSC would have
    correctly (with a stable TSC) moved forward several jiffies. But because the
    jiffies has not been updated yet the clock would be prevented from moving
    forward because it would appear that the TSC jumped too far ahead.
    
    The clock would then virtually stop, until the jiffies are updated. Then
    the next sched clock update would see that the clock was very much behind
    since the delta jiffies is now correct. This would then jump the clock
    forward by several jiffies.
    
    This caused ftrace to report several milliseconds of interrupts off
    latency at every resume from NO_HZ idle.
    
    This patch adds hooks into the nohz code to disable the checking of the
    maximum clock update when nohz is in effect. It resumes the max check
    when nohz has updated the jiffies again.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c5d3f847ca8d..33a8f42041fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1573,13 +1573,28 @@ static inline void sched_clock_idle_sleep_event(void)
 static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
-#else
+
+#ifdef CONFIG_NO_HZ
+static inline void sched_clock_tick_stop(int cpu)
+{
+}
+
+static inline void sched_clock_tick_start(int cpu)
+{
+}
+#endif
+
+#else /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
 extern void sched_clock_init(void);
 extern u64 sched_clock_cpu(int cpu);
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
+#ifdef CONFIG_NO_HZ
+extern void sched_clock_tick_stop(int cpu);
+extern void sched_clock_tick_start(int cpu);
 #endif
+#endif /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
 
 /*
  * For kernel-internal use: high-speed (but slightly incorrect) per-cpu

commit 2398f2c6d34b43025f274fc42eaca34d23ec2320
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 27 13:41:35 2008 +0200

    sched: update shares on wakeup
    
    We found that the affine wakeup code needs rather accurate load figures
    to be effective. The trouble is that updating the load figures is fairly
    expensive with group scheduling. Therefore ratelimit the updating.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eaf821072dbd..835b6c6fcc56 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -783,6 +783,8 @@ struct sched_domain {
 	unsigned int balance_interval;	/* initialise to 1. units in ms. */
 	unsigned int nr_balance_failed; /* initialise to 0 */
 
+	u64 last_update;
+
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */
 	unsigned int lb_count[CPU_MAX_IDLE_TYPES];
@@ -1605,6 +1607,7 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
+extern unsigned int sysctl_sched_shares_ratelimit;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit b6a86c746f5b708012809958462234d19e9c8177
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 27 13:41:18 2008 +0200

    sched: fix sched_domain aggregation
    
    Keeping the aggregate on the first cpu of the sched domain has two problems:
     - it could collide between different sched domains on different cpus
     - it could slow things down because of the remote accesses
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 97a58b622ee1..eaf821072dbd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -765,7 +765,6 @@ struct sched_domain {
 	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
 	cpumask_t span;			/* span of all CPUs in this domain */
-	int first_cpu;			/* cache of the first cpu in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */

commit c09595f63bb1909c5dc4dca288f4fe818561b5f3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 27 13:41:14 2008 +0200

    sched: revert revert of: fair-group: SMP-nice for group scheduling
    
    Try again..
    
    Initial commit: 18d95a2832c1392a2d63227a7a6d433cb9f2037e
    Revert: 6363ca57c76b7b83639ca8c83fc285fa26a7880e
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eaf821072dbd..97a58b622ee1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -765,6 +765,7 @@ struct sched_domain {
 	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
 	cpumask_t span;			/* span of all CPUs in this domain */
+	int first_cpu;			/* cache of the first cpu in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */

commit 5ce001b0e56638c726270d4f9e05d46d4250dfbb
Merge: 7c9f8861e6c9 543cf4cb3fe6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 25 12:27:29 2008 +0200

    Merge branch 'linus' into stackprotector

commit 961ccddd59d627b89bd3dc284b6517833bbdf25d
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jun 23 13:55:38 2008 +1000

    sched: add new API sched_setscheduler_nocheck: add a flag to control access checks
    
    Hidehiro Kawai noticed that sched_setscheduler() can fail in
    stop_machine: it calls sched_setscheduler() from insmod, which can
    have CAP_SYS_MODULE without CAP_SYS_NICE.
    
    Two cases could have failed, so are changed to sched_setscheduler_nocheck:
      kernel/softirq.c:cpu_callback()
            - CPU hotplug callback
      kernel/stop_machine.c:__stop_machine_run()
            - Called from various places, including modprobe()
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: sugita <yumiko.sugita.yf@hitachi.com>
    Cc: Satoshi OSHIMA <satoshi.oshima.fk@hitachi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c5d3f847ca8d..fe3b9b5d7390 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1655,6 +1655,8 @@ extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
+extern int sched_setscheduler_nocheck(struct task_struct *, int,
+				      struct sched_param *);
 extern struct task_struct *idle_task(int cpu);
 extern struct task_struct *curr_task(int cpu);
 extern void set_curr_task(int cpu, struct task_struct *p);

commit 8bbd54d69e9c66adbf544e21d8dcfb15fb9198f7
Merge: 8c2238eaaf0f 066519068ad2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 16 11:24:43 2008 +0200

    Merge branch 'linus' into core/softlockup

commit e765ee90da62535ac7d7a97f2464f9646539d683
Merge: a4500b84c516 066519068ad2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 16 11:15:58 2008 +0200

    Merge branch 'linus' into tracing/ftrace

commit f9e8e07e074a880e110922759dcdb369fecdf07c
Merge: e9886ca3a93d 066519068ad2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jun 16 11:15:21 2008 +0200

    Merge branch 'linus' into sched-devel

commit 9985b0bab332289f14837eff3c6e0bcc658b58f7
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jun 5 12:57:11 2008 -0700

    sched: prevent bound kthreads from changing cpus_allowed
    
    Kthreads that have called kthread_bind() are bound to specific cpus, so
    other tasks should not be able to change their cpus_allowed from under
    them.  Otherwise, it is possible to move kthreads, such as the migration
    or software watchdog threads, so they are not allowed access to the cpu
    they work on.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Menage <menage@google.com>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d25acf600a32..2db1485f865d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1486,6 +1486,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
 #define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
+#define PF_THREAD_BOUND	0x04000000	/* Thread bound to specific cpu */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezeable */

commit 16882c1e962b4be5122fc05aaf2afc10fd9e2d15
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Jun 8 21:20:41 2008 +0400

    sched: fix TASK_WAKEKILL vs SIGKILL race
    
    schedule() has the special "TASK_INTERRUPTIBLE && signal_pending()" case,
    this allows us to do
    
            current->state = TASK_INTERRUPTIBLE;
            schedule();
    
    without fear to sleep with pending signal.
    
    However, the code like
    
            current->state = TASK_KILLABLE;
            schedule();
    
    is not right, schedule() doesn't take TASK_WAKEKILL into account. This means
    that mutex_lock_killable(), wait_for_completion_killable(), down_killable(),
    schedule_timeout_killable() can miss SIGKILL (and btw the second SIGKILL has
    no effect).
    
    Introduce the new helper, signal_pending_state(), and change schedule() to
    use it. Hopefully it will have more users, that is why the task's state is
    passed separately.
    
    Note this "__TASK_STOPPED | __TASK_TRACED" check in signal_pending_state().
    This is needed to preserve the current behaviour (ptrace_notify). I hope
    this check will be removed soon, but this (afaics good) change needs the
    separate discussion.
    
    The fast path is "(state & (INTERRUPTIBLE | WAKEKILL)) + signal_pending(p)",
    basically the same that schedule() does now. However, this patch of course
    bloats schedule().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ae0be3c62375..c5d3f847ca8d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2026,6 +2026,19 @@ static inline int fatal_signal_pending(struct task_struct *p)
 	return signal_pending(p) && __fatal_signal_pending(p);
 }
 
+static inline int signal_pending_state(long state, struct task_struct *p)
+{
+	if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
+		return 0;
+	if (!signal_pending(p))
+		return 0;
+
+	if (state & (__TASK_STOPPED | __TASK_TRACED))
+		return 0;
+
+	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
+}
+
 static inline int need_resched(void)
 {
 	return unlikely(test_thread_flag(TIF_NEED_RESCHED));

commit 1f11eb6a8bc92536d9e93ead48fa3ffbd1478571
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Wed Jun 4 15:04:05 2008 -0400

    sched: fix cpupri hotplug support
    
    The RT folks over at RedHat found an issue w.r.t. hotplug support which
    was traced to problems with the cpupri infrastructure in the scheduler:
    
    https://bugzilla.redhat.com/show_bug.cgi?id=449676
    
    This bug affects 23-rt12+, 24-rtX, 25-rtX, and sched-devel.  This patch
    applies to 25.4-rt4, though it should trivially apply to most cpupri enabled
    kernels mentioned above.
    
    It turned out that the issue was that offline cpus could get inadvertently
    registered with cpupri so that they were erroneously selected during
    migration decisions.  The end result would be an OOPS as the offline cpu
    had tasks routed to it.
    
    This patch generalizes the old join/leave domain interface into an
    online/offline interface, and adjusts the root-domain/hotplug code to
    utilize it.
    
    I was able to easily reproduce the issue prior to this patch, and am no
    longer able to reproduce it after this patch.  I can offline cpus
    indefinately and everything seems to be in working order.
    
    Thanks to Arnaldo (acme), Thomas, and Peter for doing the legwork to point
    me in the right direction.  Also thank you to Peter for reviewing the
    early iterations of this patch.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea2857b99596..d25acf600a32 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -903,8 +903,8 @@ struct sched_class {
 	void (*set_cpus_allowed)(struct task_struct *p,
 				 const cpumask_t *newmask);
 
-	void (*join_domain)(struct rq *rq);
-	void (*leave_domain)(struct rq *rq);
+	void (*rq_online)(struct rq *rq);
+	void (*rq_offline)(struct rq *rq);
 
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task,
 			       int running);

commit c7aceaba042702538b23cf4e0de1b2891ad8e671
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Thu May 15 12:09:15 2008 +0100

    sched: reorder task_struct to reduce padding on 64bit builds
    
    This patch removes 24 bytes of padding and allows 1 extra object per
    slab on my fedora based config.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc36c3aea018..ea2857b99596 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1021,6 +1021,7 @@ struct task_struct {
 #endif
 
 	int prio, static_prio, normal_prio;
+	unsigned int rt_priority;
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
@@ -1104,7 +1105,6 @@ struct task_struct {
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
 	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
 
-	unsigned int rt_priority;
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
 	cputime_t prev_utime, prev_stime;
@@ -1123,12 +1123,12 @@ struct task_struct {
 	gid_t gid,egid,sgid,fsgid;
 	struct group_info *group_info;
 	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted, cap_bset;
-	unsigned securebits;
 	struct user_struct *user;
+	unsigned securebits;
 #ifdef CONFIG_KEYS
+	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 	struct key *request_key_auth;	/* assumed request_key authority */
 	struct key *thread_keyring;	/* keyring private to this thread */
-	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 #endif
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
@@ -1215,8 +1215,8 @@ struct task_struct {
 # define MAX_LOCK_DEPTH 48UL
 	u64 curr_chain_key;
 	int lockdep_depth;
-	struct held_lock held_locks[MAX_LOCK_DEPTH];
 	unsigned int lockdep_recursion;
+	struct held_lock held_locks[MAX_LOCK_DEPTH];
 #endif
 
 /* journalling filesystem info */
@@ -1244,10 +1244,6 @@ struct task_struct {
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
 	cputime_t acct_stimexpd;/* stime since last update */
 #endif
-#ifdef CONFIG_NUMA
-  	struct mempolicy *mempolicy;
-	short il_next;
-#endif
 #ifdef CONFIG_CPUSETS
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;
@@ -1266,6 +1262,10 @@ struct task_struct {
 #endif
 	struct list_head pi_state_list;
 	struct futex_pi_state *pi_state_cache;
+#endif
+#ifdef CONFIG_NUMA
+	struct mempolicy *mempolicy;
+	short il_next;
 #endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;

commit 554ec22f075d46e4363520a407d2b7eeb5dfdd43
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:21:03 2008 +0200

    namespacecheck: more sched.c fixes
    
    [ Stephen Rothwell <sfr@canb.auug.org.au>: build fix ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ae0be3c62375..dc36c3aea018 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -134,7 +134,6 @@ extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
-extern unsigned long weighted_cpuload(const int cpu);
 
 struct seq_file;
 struct cfs_rq;
@@ -823,23 +822,6 @@ extern int arch_reinit_sched_domains(void);
 
 #endif	/* CONFIG_SMP */
 
-/*
- * A runqueue laden with a single nice 0 task scores a weighted_cpuload of
- * SCHED_LOAD_SCALE. This function returns 1 if any cpu is laden with a
- * task of nice 0 or enough lower priority tasks to bring up the
- * weighted_cpuload
- */
-static inline int above_background_load(void)
-{
-	unsigned long cpu;
-
-	for_each_online_cpu(cpu) {
-		if (weighted_cpuload(cpu) >= SCHED_LOAD_SCALE)
-			return 1;
-	}
-	return 0;
-}
-
 struct io_context;			/* See blkdev.h */
 #define NGROUPS_SMALL		32
 #define NGROUPS_PER_BLOCK	((unsigned int)(PAGE_SIZE / sizeof(gid_t)))

commit 6715930654e06c4d2e66e718ea159079f71838f4
Merge: ea3f01f8afd3 e490517a039a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 29 16:05:05 2008 +0200

    Merge commit 'linus/master' into sched-fixes-for-linus

commit 6363ca57c76b7b83639ca8c83fc285fa26a7880e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 29 11:28:57 2008 +0200

    revert ("sched: fair-group: SMP-nice for group scheduling")
    
    Yanmin Zhang reported:
    
    Comparing with 2.6.25, volanoMark has big regression with kernel 2.6.26-rc1.
    It's about 50% on my 8-core stoakley, 16-core tigerton, and Itanium Montecito.
    
    With bisect, I located the following patch:
    
    | 18d95a2832c1392a2d63227a7a6d433cb9f2037e is first bad commit
    | commit 18d95a2832c1392a2d63227a7a6d433cb9f2037e
    | Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
    | Date:   Sat Apr 19 19:45:00 2008 +0200
    |
    |     sched: fair-group: SMP-nice for group scheduling
    
    Revert it so that we get v2.6.25 behavior.
    
    Bisected-by: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5395a6176f4b..8a888499954e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -766,7 +766,6 @@ struct sched_domain {
 	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
 	cpumask_t span;			/* span of all CPUs in this domain */
-	int first_cpu;			/* cache of the first cpu in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */

commit cbaffba12ce08beb3e80bfda148ee0fa14aac188
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon May 26 20:55:42 2008 +0400

    posix timers: discard SI_TIMER signals on exec
    
    Based on Roland's patch. This approach was suggested by Austin Clements
    from the very beginning, and then by Linus.
    
    As Austin pointed out, the execing task can be killed by SI_TIMER signal
    because exec flushes the signal handlers, but doesn't discard the pending
    signals generated by posix timers. Perhaps not a bug, but people find this
    surprising. See http://bugzilla.kernel.org/show_bug.cgi?id=10460
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Austin Clements <amdragon+kernelbugzilla@mit.edu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5395a6176f4b..3e05e5474749 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1848,7 +1848,9 @@ extern void exit_thread(void);
 extern void exit_files(struct task_struct *);
 extern void __cleanup_signal(struct signal_struct *);
 extern void __cleanup_sighand(struct sighand_struct *);
+
 extern void exit_itimers(struct signal_struct *);
+extern void flush_itimer_signals(void);
 
 extern NORET_TYPE void do_group_exit(int);
 

commit 7c9f8861e6c9c839f913e49b98c3854daca18f27
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Tue Apr 22 16:38:23 2008 -0500

    stackprotector: use canary at end of stack to indicate overruns at oops time
    
    (Updated with a common max-stack-used checker that knows about
    the canary, as suggested by Joe Perches)
    
    Use a canary at the end of the stack to clearly indicate
    at oops time whether the stack has ever overflowed.
    
    This is a very simple implementation with a couple of
    drawbacks:
    
    1) a thread may legitimately use exactly up to the last
       word on the stack
    
     -- but the chances of doing this and then oopsing later seem slim
    
    2) it's possible that the stack usage isn't dense enough
       that the canary location could get skipped over
    
     -- but the worst that happens is that we don't flag the overrun
     -- though this happens fairly often in my testing :(
    
    With the code in place, an intentionally-bloated stack oops might
    do:
    
    BUG: unable to handle kernel paging request at ffff8103f84cc680
    IP: [<ffffffff810253df>] update_curr+0x9a/0xa8
    PGD 8063 PUD 0
    Thread overran stack or stack corrupted
    Oops: 0000 [1] SMP
    CPU 0
    ...
    
    ... unless the stack overrun is so bad that it corrupts some other
    thread.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d6a515158783..c5181e77f305 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1969,6 +1969,19 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 
 extern void thread_info_cache_init(void);
 
+#ifdef CONFIG_DEBUG_STACK_USAGE
+static inline unsigned long stack_not_used(struct task_struct *p)
+{
+	unsigned long *n = end_of_stack(p);
+
+	do { 	/* Skip over canary */
+		n++;
+	} while (!*n);
+
+	return (unsigned long)n - (unsigned long)end_of_stack(p);
+}
+#endif
+
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit e00320875d0cc5f8099a7227b2f25fbb3231268d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 14 08:48:23 2008 +0100

    x86: fix stackprotector canary updates during context switches
    
    fix a bug noticed and fixed by pageexec@freemail.hu.
    
    if built with -fstack-protector-all then we'll have canary checks built
    into the __switch_to() function. That does not work well with the
    canary-switching code there: while we already use the %rsp of the
    new task, we still call __switch_to() whith the previous task's canary
    value in the PDA, hence the __switch_to() ssp prologue instructions
    will store the previous canary. Then we update the PDA and upon return
    from __switch_to() the canary check triggers and we panic.
    
    so update the canary after we have called __switch_to(), where we are
    at the same stackframe level as the last stackframe of the next
    (and now freshly current) task.
    
    Note: this means that we call __switch_to() [and its sub-functions]
    still with the old canary, but that is not a problem, both the previous
    and the next task has a high-quality canary. The only (mostly academic)
    disadvantage is that the canary of one task may leak onto the stack of
    another task, increasing the risk of information leaks, were an attacker
    able to read the stack of specific tasks (but not that of others).
    
    To solve this we'll have to reorganize the way we switch tasks, and move
    the PDA setting into the switch_to() assembly code. That will happen in
    another patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5395a6176f4b..d6a515158783 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1096,10 +1096,9 @@ struct task_struct {
 	pid_t pid;
 	pid_t tgid;
 
-#ifdef CONFIG_CC_STACKPROTECTOR
 	/* Canary value for the -fstack-protector gcc feature */
 	unsigned long stack_canary;
-#endif
+
 	/* 
 	 * pointers to (original) parent process, youngest child, younger sibling,
 	 * older sibling, respectively.  (p->father can be replaced with 

commit 9383d9679056e6cc4e7ff70f31da945a268238f4
Author: Dimitri Sivanich <sivanich@sgi.com>
Date:   Mon May 12 21:21:14 2008 +0200

    softlockup: fix softlockup_thresh unaligned access and disable detection at runtime
    
    Fix unaligned access errors when setting softlockup_thresh on
    64 bit platforms.
    
    Allow softlockup detection to be disabled by setting
    softlockup_thresh <= 0.
    
    Detect that boot time softlockup detection has been disabled
    earlier in softlockup_tick.
    
    Signed-off-by: Dimitri Sivanich <sivanich@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 71f5972dc48e..ea26221644e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -295,10 +295,10 @@ extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern unsigned int  softlockup_panic;
-extern unsigned long softlockup_thresh;
 extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;
 extern unsigned long sysctl_hung_task_warnings;
+extern int softlockup_thresh;
 #else
 static inline void softlockup_tick(void)
 {

commit 9c44bc03fff44ff04237a7d92e35304a0e50c331
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:21:04 2008 +0200

    softlockup: allow panic on lockup
    
    allow users to configure the softlockup detector to generate a panic
    instead of a warning message.
    
    high-availability systems might opt for this strict method (combined
    with panic_timeout= boot option/sysctl), instead of generating
    softlockup warnings ad infinitum.
    
    also, automated tests work better if the system reboots reliably (into
    a safe kernel) in case of a lockup.
    
    The full spectrum of configurability is supported: boot option, sysctl
    option and Kconfig option.
    
    it's default-disabled.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5395a6176f4b..71f5972dc48e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -294,7 +294,8 @@ extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
-extern unsigned long  softlockup_thresh;
+extern unsigned int  softlockup_panic;
+extern unsigned long softlockup_thresh;
 extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;
 extern unsigned long sysctl_hung_task_warnings;

commit 5b82a1b08a00b2adca3d9dd9777efff40b7aaaa1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Mon May 12 21:21:10 2008 +0200

    Port ftrace to markers
    
    Porting ftrace to the marker infrastructure.
    
    Don't need to chain to the wakeup tracer from the sched tracer, because markers
    support multiple probes connected.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    CC: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 360ca99033d2..c0b1c69b55ce 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2131,38 +2131,6 @@ __trace_special(void *__tr, void *__data,
 }
 #endif
 
-#ifdef CONFIG_CONTEXT_SWITCH_TRACER
-extern void
-ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next);
-extern void
-ftrace_wake_up_task(void *rq, struct task_struct *wakee,
-		    struct task_struct *curr);
-extern void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data);
-extern void
-ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
-#else
-static inline void
-ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next)
-{
-}
-static inline void
-sched_trace_special(unsigned long p1, unsigned long p2, unsigned long p3)
-{
-}
-static inline void
-ftrace_wake_up_task(void *rq, struct task_struct *wakee,
-		    struct task_struct *curr)
-{
-}
-static inline void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data)
-{
-}
-static inline void
-ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3)
-{
-}
-#endif
-
 extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 

commit 88a4216c3ec4281fc7e6725cc3a3ccd01fb1aa14
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:53 2008 +0200

    ftrace: sched special
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5b186bed54bc..360ca99033d2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2138,6 +2138,8 @@ extern void
 ftrace_wake_up_task(void *rq, struct task_struct *wakee,
 		    struct task_struct *curr);
 extern void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data);
+extern void
+ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
 #else
 static inline void
 ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next)
@@ -2155,6 +2157,10 @@ ftrace_wake_up_task(void *rq, struct task_struct *wakee,
 static inline void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data)
 {
 }
+static inline void
+ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3)
+{
+}
 #endif
 
 extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);

commit 1a3c3034336320554a3342572dae98d69e054fc7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:52 2008 +0200

    ftrace: fix __trace_special()
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a3970b563757..5b186bed54bc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2119,6 +2119,18 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 }
 #endif
 
+#ifdef CONFIG_TRACING
+extern void
+__trace_special(void *__tr, void *__data,
+		unsigned long arg1, unsigned long arg2, unsigned long arg3);
+#else
+static inline void
+__trace_special(void *__tr, void *__data,
+		unsigned long arg1, unsigned long arg2, unsigned long arg3)
+{
+}
+#endif
+
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 extern void
 ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next);
@@ -2126,9 +2138,6 @@ extern void
 ftrace_wake_up_task(void *rq, struct task_struct *wakee,
 		    struct task_struct *curr);
 extern void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data);
-extern void
-__trace_special(void *__tr, void *__data,
-		unsigned long arg1, unsigned long arg2, unsigned long arg3);
 #else
 static inline void
 ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next)
@@ -2146,11 +2155,6 @@ ftrace_wake_up_task(void *rq, struct task_struct *wakee,
 static inline void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data)
 {
 }
-static inline void
-__trace_special(void *__tr, void *__data,
-		unsigned long arg1, unsigned long arg2, unsigned long arg3)
-{
-}
 #endif
 
 extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);

commit 017730c11241e26577673eb9d957cfc66172ea91
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:52 2008 +0200

    ftrace: fix wakeups
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 652d380ae563..a3970b563757 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -246,6 +246,8 @@ extern asmlinkage void schedule_tail(struct task_struct *prev);
 extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 
+extern int runqueue_is_locked(void);
+
 extern cpumask_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern int select_nohz_load_balancer(int cpu);

commit 4e65551905fb0300ae7e667cbaa41ee2e3f29a13
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:52 2008 +0200

    ftrace: sched tracer, trace full rbtree
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 05744f9cb096..652d380ae563 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2119,20 +2119,34 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 extern void
-ftrace_ctx_switch(struct task_struct *prev, struct task_struct *next);
+ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next);
+extern void
+ftrace_wake_up_task(void *rq, struct task_struct *wakee,
+		    struct task_struct *curr);
+extern void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data);
+extern void
+__trace_special(void *__tr, void *__data,
+		unsigned long arg1, unsigned long arg2, unsigned long arg3);
 #else
 static inline void
-ftrace_ctx_switch(struct task_struct *prev, struct task_struct *next)
+ftrace_ctx_switch(void *rq, struct task_struct *prev, struct task_struct *next)
+{
+}
+static inline void
+sched_trace_special(unsigned long p1, unsigned long p2, unsigned long p3)
+{
+}
+static inline void
+ftrace_wake_up_task(void *rq, struct task_struct *wakee,
+		    struct task_struct *curr)
+{
+}
+static inline void ftrace_all_fair_tasks(void *__rq, void *__tr, void *__data)
 {
 }
-#endif
-
-#ifdef CONFIG_SCHED_TRACER
-extern void
-ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr);
-#else
 static inline void
-ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr)
+__trace_special(void *__tr, void *__data,
+		unsigned long arg1, unsigned long arg2, unsigned long arg3)
 {
 }
 #endif

commit 8ac0fca4ccb355ce50471d7aa3f10f5900b28b95
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: sched tracer fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6e26f1fdbfe2..05744f9cb096 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2130,17 +2130,11 @@ ftrace_ctx_switch(struct task_struct *prev, struct task_struct *next)
 #ifdef CONFIG_SCHED_TRACER
 extern void
 ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr);
-extern void
-ftrace_wake_up_new_task(struct task_struct *wakee, struct task_struct *curr);
 #else
 static inline void
 ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr)
 {
 }
-static inline void
-ftrace_wake_up_new_task(struct task_struct *wakee, struct task_struct *curr)
-{
-}
 #endif
 
 extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);

commit 7c731e0a495e25e79dc1e9e68772a67a55721a65
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:41 2008 +0200

    ftrace: make the task state char-string visible to all
    
    The tracer wants to be able to convert the state number
    into a user visible character. This patch pulls that conversion
    string out the scheduler into the header. This way if it were to
    ever change, other parts of the kernel will know.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 717cab8a0c83..6e26f1fdbfe2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2237,6 +2237,8 @@ static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 }
 #endif /* CONFIG_MM_OWNER */
 
+#define TASK_STATE_TO_CHAR_STR "RSDTtZX"
+
 #endif /* __KERNEL__ */
 
 #endif

commit bd3bff9e20f454b242d979ec2f9a4dca0d5fa06f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:41 2008 +0200

    sched: add latency tracer callbacks to the scheduler
    
    add 3 lightweight callbacks to the tracer backend.
    
    zero impact if tracing is turned off.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5395a6176f4b..717cab8a0c83 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2117,6 +2117,32 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 }
 #endif
 
+#ifdef CONFIG_CONTEXT_SWITCH_TRACER
+extern void
+ftrace_ctx_switch(struct task_struct *prev, struct task_struct *next);
+#else
+static inline void
+ftrace_ctx_switch(struct task_struct *prev, struct task_struct *next)
+{
+}
+#endif
+
+#ifdef CONFIG_SCHED_TRACER
+extern void
+ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr);
+extern void
+ftrace_wake_up_new_task(struct task_struct *wakee, struct task_struct *curr);
+#else
+static inline void
+ftrace_wake_up_task(struct task_struct *wakee, struct task_struct *curr)
+{
+}
+static inline void
+ftrace_wake_up_new_task(struct task_struct *wakee, struct task_struct *curr)
+{
+}
+#endif
+
 extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 

commit c714a534d85576af21b06be605ca55cb2fb887ee
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 12 13:34:13 2008 -0700

    Make 'cond_resched()' nullification depend on PREEMPT_BKL
    
    Because it's not correct with a non-preemptable BKL and just causes
    PREEMPT kernels to have longer latencies than non-PREEMPT ones (which is
    obviously not the point of it at all).
    
    Of course, that config option actually got removed as an option earlier,
    so for now this basically disables it entirely, but if BKL preemption is
    ever resurrected it will be a meaningful optimization.  And in the
    meantime, it at least documents the intent of the code, while not doing
    the wrong thing.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5a63f2d72af6..5395a6176f4b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2038,7 +2038,7 @@ static inline int need_resched(void)
  * cond_resched_softirq() will enable bhs before scheduling.
  */
 extern int _cond_resched(void);
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPT_BKL
 static inline int cond_resched(void)
 {
 	return 0;

commit 9404ef02974a5411687b6c1b8ef3984305620e02
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 12 10:14:22 2008 -0700

    Fix up 'need_resched()' definition
    
    We should not go through the task pointer to get at the thread info,
    since it's usually cheaper to just access the thread info directly.
    
    So don't make the code look up 'current', when we can just use the
    thread info accessor functions directly.  This generally avoids one
    level of indirection and tends to work better together with code that
    also looks at other thread flags (eg preempt_count).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ab9f32f9238..5a63f2d72af6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2027,7 +2027,7 @@ static inline int fatal_signal_pending(struct task_struct *p)
 
 static inline int need_resched(void)
 {
-	return unlikely(test_tsk_need_resched(current));
+	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
 }
 
 /*

commit c3921ab71507b108d51a0f1ee960f80cd668a93d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 11 16:04:48 2008 -0700

    Add new 'cond_resched_bkl()' helper function
    
    It acts exactly like a regular 'cond_resched()', but will not get
    optimized away when CONFIG_PREEMPT is set.
    
    Normal kernel code is already preemptable in the presense of
    CONFIG_PREEMPT, so cond_resched() is optimized away (see commit
    02b67cc3ba36bdba351d6c3a00593f4ec550d9d3 "sched: do not do
    cond_resched() when CONFIG_PREEMPT").
    
    But when wanting to conditionally reschedule while holding a lock, you
    need to use "cond_sched_lock(lock)", and the new function is the BKL
    equivalent of that.
    
    Also make fs/locks.c use it.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0c35b0343a76..4ab9f32f9238 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2037,13 +2037,13 @@ static inline int need_resched(void)
  * cond_resched_lock() will drop the spinlock before scheduling,
  * cond_resched_softirq() will enable bhs before scheduling.
  */
+extern int _cond_resched(void);
 #ifdef CONFIG_PREEMPT
 static inline int cond_resched(void)
 {
 	return 0;
 }
 #else
-extern int _cond_resched(void);
 static inline int cond_resched(void)
 {
 	return _cond_resched();
@@ -2051,6 +2051,10 @@ static inline int cond_resched(void)
 #endif
 extern int cond_resched_lock(spinlock_t * lock);
 extern int cond_resched_softirq(void);
+static inline int cond_resched_bkl(void)
+{
+	return _cond_resched();
+}
 
 /*
  * Does a critical section need to be broken due to another

commit 3e51f33fcc7f55e6df25d15b55ed10c8b4da84cd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat May 3 18:29:28 2008 +0200

    sched: add optional support for CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
    
    this replaces the rq->clock stuff (and possibly cpu_clock()).
    
     - architectures that have an 'imperfect' hardware clock can set
       CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
    
     - the 'jiffie' window might be superfulous when we update tick_gtod
       before the __update_sched_clock() call in sched_clock_tick()
    
     - cpu_clock() might be implemented as:
    
         sched_clock_cpu(smp_processor_id())
    
       if the accuracy proves good enough - how far can TSC drift in a
       single jiffie when considering the filtering and idle hooks?
    
    [ mingo@elte.hu: various fixes and cleanups ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 54c9ca26b7d8..0c35b0343a76 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1553,6 +1553,35 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 
 extern unsigned long long sched_clock(void);
 
+#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
+static inline void sched_clock_init(void)
+{
+}
+
+static inline u64 sched_clock_cpu(int cpu)
+{
+	return sched_clock();
+}
+
+static inline void sched_clock_tick(void)
+{
+}
+
+static inline void sched_clock_idle_sleep_event(void)
+{
+}
+
+static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
+{
+}
+#else
+extern void sched_clock_init(void);
+extern u64 sched_clock_cpu(int cpu);
+extern void sched_clock_tick(void);
+extern void sched_clock_idle_sleep_event(void);
+extern void sched_clock_idle_wakeup_event(u64 delta_ns);
+#endif
+
 /*
  * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
  * clock constructed from sched_clock():

commit 690229a0912ca2fef8b542fe4d8b73acfcdc6e24
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 23 09:31:35 2008 +0200

    sched: make clock sync tunable by architecture code
    
    make time_sync_thresh tunable to architecture code.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 698b5a4d25a7..54c9ca26b7d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -158,6 +158,8 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 }
 #endif
 
+extern unsigned long long time_sync_thresh;
+
 /*
  * Task state bitmask. NOTE! These bits are also
  * encoded in fs/proc/array.c: get_task_state().

commit 8ae121ac8666b0421aa20fd80d4597ec66fa54bc
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Wed Apr 23 07:13:29 2008 -0400

    sched: fix RT task-wakeup logic
    
    Dmitry Adamushko pointed out a logic error in task_wake_up_rt() where we
    will always evaluate to "true".  You can find the thread here:
    
    http://lkml.org/lkml/2008/4/22/296
    
    In reality, we only want to try to push tasks away when a wake up request is
    not going to preempt the current task.  So lets fix it.
    
    Note: We introduce test_tsk_need_resched() instead of open-coding the flag
    check so that the merge-conflict with -rt should help remind us that we
    may need to support NEEDS_RESCHED_DELAYED in the future, too.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    CC: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    CC: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 03c238088aee..698b5a4d25a7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1977,6 +1977,11 @@ static inline void clear_tsk_need_resched(struct task_struct *tsk)
 	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
 }
 
+static inline int test_tsk_need_resched(struct task_struct *tsk)
+{
+	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
+}
+
 static inline int signal_pending(struct task_struct *p)
 {
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
@@ -1991,7 +1996,7 @@ static inline int fatal_signal_pending(struct task_struct *p)
 
 static inline int need_resched(void)
 {
-	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
+	return unlikely(test_tsk_need_resched(current));
 }
 
 /*

commit 5cd204550b1a006f2b0c986b0e0f53220ebfd391
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Apr 30 00:54:24 2008 -0700

    Deprecate find_task_by_pid()
    
    There are some places that are known to operate on tasks'
    global pids only:
    
    * the rest_init() call (called on boot)
    * the kgdb's getthread
    * the create_kthread() (since the kthread is run in init ns)
    
    So use the find_task_by_pid_ns(..., &init_pid_ns) there
    and schedule the find_task_by_pid for removal.
    
    [sukadev@us.ibm.com: Fix warning in kernel/pid.c]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 86e60796db62..03c238088aee 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1677,7 +1677,10 @@ extern struct pid_namespace init_pid_ns;
 extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,
 		struct pid_namespace *ns);
 
-extern struct task_struct *find_task_by_pid(pid_t nr);
+static inline struct task_struct *__deprecated find_task_by_pid(pid_t nr)
+{
+	return find_task_by_pid_type_ns(PIDTYPE_PID, nr, &init_pid_ns);
+}
 extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);

commit f3de272b821accbc8387211977c2de4f38468d05
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Apr 30 00:53:09 2008 -0700

    signals: use HAVE_SET_RESTORE_SIGMASK
    
    Change all the #ifdef TIF_RESTORE_SIGMASK conditionals in non-arch code to
    #ifdef HAVE_SET_RESTORE_SIGMASK.  If arch code defines it first, the generic
    set_restore_sigmask() using TIF_RESTORE_SIGMASK is not defined.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fe970cdca83c..86e60796db62 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1175,7 +1175,7 @@ struct task_struct {
 	struct sighand_struct *sighand;
 
 	sigset_t blocked, real_blocked;
-	sigset_t saved_sigmask;		/* To be restored with TIF_RESTORE_SIGMASK */
+	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
 	struct sigpending pending;
 
 	unsigned long sas_ss_sp;

commit fae5fa44f1fd079ffbed8e0add929dd7bbd1347f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:53:03 2008 -0700

    signals: fix /sbin/init protection from unwanted signals
    
    The global init has a lot of long standing problems with the unhandled fatal
    signals.
    
            - The "is_global_init(current)" check in get_signal_to_deliver()
              protects only the main thread. Sub-thread can dequee the fatal
              signal and shutdown the whole thread group except the main thread.
              If it dequeues SIGSTOP /sbin/init will be stopped, this is not
              right too. Note that we can't use is_global_init(->group_leader),
              this breaks exec and this can't solve other problems we have.
    
            - Even if afterwards ignored, the fatal signals sets SIGNAL_GROUP_EXIT
              on delivery. This breaks exec, has other bad implications, and this
              is just wrong.
    
    Introduce the new SIGNAL_UNKILLABLE flag to fix these problems.  It also helps
    to solve some other problems addressed by the subsequent patches.
    
    Currently we use this flag for the global init only, but it could also be used
    by kthreads and (perhaps) by the sub-namespace inits.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0917b3df12d5..fe970cdca83c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -561,6 +561,8 @@ struct signal_struct {
 #define SIGNAL_CLD_CONTINUED	0x00000020
 #define SIGNAL_CLD_MASK		(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)
 
+#define SIGNAL_UNKILLABLE	0x00000040 /* for init: ignore fatal signals */
+
 /* If true, all threads except ->group_exit_task have pending SIGKILL */
 static inline int signal_group_exit(const struct signal_struct *sig)
 {

commit ac5c215383f43a106ba4ef298126bf78c126f5e9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:52:57 2008 -0700

    signals: join send_sigqueue() with send_group_sigqueue()
    
    We export send_sigqueue() and send_group_sigqueue() for the only user,
    posix_timer_event().  This is a bit silly, because both are just trivial
    helpers on top of do_send_sigqueue() and because the we pass the unused
    .si_signo parameter.
    
    Kill them both, rename do_send_sigqueue() to send_sigqueue(), and export it.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ef5615270342..0917b3df12d5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1751,8 +1751,7 @@ extern void zap_other_threads(struct task_struct *p);
 extern int kill_proc(pid_t, int, int);
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
-extern int send_sigqueue(int, struct sigqueue *,  struct task_struct *);
-extern int send_group_sigqueue(int, struct sigqueue *,  struct task_struct *);
+extern int send_sigqueue(struct sigqueue *,  struct task_struct *, int group);
 extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
 

commit e442055193e4584218006e616c9bdce0c5e9ae5c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:52:44 2008 -0700

    signals: re-assign CLD_CONTINUED notification from the sender to reciever
    
    Based on discussion with Jiri and Roland.
    
    In short: currently handle_stop_signal(SIGCONT, p) sends the notification to
    p->parent, with this patch p itself notifies its parent when it becomes
    running.
    
    handle_stop_signal(SIGCONT) has to drop ->siglock temporary in order to notify
    the parent with do_notify_parent_cldstop().  This leads to multiple problems:
    
            - as Jiri Kosina pointed out, the stopped task can resume without
              actually seeing SIGCONT which may have a handler.
    
            - we race with another sig_kernel_stop() signal which may come in
              that window.
    
            - we race with sig_fatal() signals which may set SIGNAL_GROUP_EXIT
              in that window.
    
            - we can't avoid taking tasklist_lock() while sending SIGCONT.
    
    With this patch handle_stop_signal() just sets the new SIGNAL_CLD_CONTINUED
    flag in p->signal->flags and returns.  The notification is sent by the first
    task which returns from finish_stop() (there should be at least one) or any
    other signalled thread from get_signal_to_deliver().
    
    This is a user-visible change.  Say, currently kill(SIGCONT, stopped_child)
    can't return without seeing SIGCHLD, with this patch SIGCHLD can be delayed
    unpredictably.  Another difference is that if the child is ptraced by another
    process, CLD_CONTINUED may be delivered to ->real_parent after ptrace_detach()
    while currently it always goes to the tracer which doesn't actually need this
    notification.  Hopefully not a problem.
    
    The patch asks for the futher obvious cleanups, I'll send them separately.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1d02babdb2c7..ef5615270342 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -554,6 +554,12 @@ struct signal_struct {
 #define SIGNAL_STOP_DEQUEUED	0x00000002 /* stop signal dequeued */
 #define SIGNAL_STOP_CONTINUED	0x00000004 /* SIGCONT since WCONTINUED reap */
 #define SIGNAL_GROUP_EXIT	0x00000008 /* group exit in progress */
+/*
+ * Pending notifications to parent.
+ */
+#define SIGNAL_CLD_STOPPED	0x00000010
+#define SIGNAL_CLD_CONTINUED	0x00000020
+#define SIGNAL_CLD_MASK		(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)
 
 /* If true, all threads except ->group_exit_task have pending SIGKILL */
 static inline int signal_group_exit(const struct signal_struct *sig)

commit cf475ad28ac35cc9ba612d67158f29b73b38b05d
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Tue Apr 29 01:00:16 2008 -0700

    cgroups: add an owner to the mm_struct
    
    Remove the mem_cgroup member from mm_struct and instead adds an owner.
    
    This approach was suggested by Paul Menage.  The advantage of this approach
    is that, once the mm->owner is known, using the subsystem id, the cgroup
    can be determined.  It also allows several control groups that are
    virtually grouped by mm_struct, to exist independent of the memory
    controller i.e., without adding mem_cgroup's for each controller, to
    mm_struct.
    
    A new config option CONFIG_MM_OWNER is added and the memory resource
    controller selects this config option.
    
    This patch also adds cgroup callbacks to notify subsystems when mm->owner
    changes.  The mm_cgroup_changed callback is called with the task_lock() of
    the new task held and is called just prior to changing the mm->owner.
    
    I am indebted to Paul Menage for the several reviews of this patchset and
    helping me make it lighter and simpler.
    
    This patch was tested on a powerpc box, it was compiled with both the
    MM_OWNER config turned on and off.
    
    After the thread group leader exits, it's moved to init_css_state by
    cgroup_exit(), thus all future charges from runnings threads would be
    redirected to the init_css_set's subsystem.
    
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Sudhir Kumar <skumar@linux.vnet.ibm.com>
    Cc: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Cc: Hirokazu Takahashi <taka@valinux.co.jp>
    Cc: David Rientjes <rientjes@google.com>,
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 024d72b47a0c..1d02babdb2c7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2148,6 +2148,19 @@ static inline void migration_init(void)
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
+#ifdef CONFIG_MM_OWNER
+extern void mm_update_next_owner(struct mm_struct *mm);
+extern void mm_init_owner(struct mm_struct *mm, struct task_struct *p);
+#else
+static inline void mm_update_next_owner(struct mm_struct *mm)
+{
+}
+
+static inline void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
+{
+}
+#endif /* CONFIG_MM_OWNER */
+
 #endif /* __KERNEL__ */
 
 #endif

commit 3898b1b4ebff8dcfbcf1807e0661585e06c9a91c
Author: Andrew G. Morgan <morgan@kernel.org>
Date:   Mon Apr 28 02:13:40 2008 -0700

    capabilities: implement per-process securebits
    
    Filesystem capability support makes it possible to do away with (set)uid-0
    based privilege and use capabilities instead.  That is, with filesystem
    support for capabilities but without this present patch, it is (conceptually)
    possible to manage a system with capabilities alone and never need to obtain
    privilege via (set)uid-0.
    
    Of course, conceptually isn't quite the same as currently possible since few
    user applications, certainly not enough to run a viable system, are currently
    prepared to leverage capabilities to exercise privilege.  Further, many
    applications exist that may never get upgraded in this way, and the kernel
    will continue to want to support their setuid-0 base privilege needs.
    
    Where pure-capability applications evolve and replace setuid-0 binaries, it is
    desirable that there be a mechanisms by which they can contain their
    privilege.  In addition to leveraging the per-process bounding and inheritable
    sets, this should include suppressing the privilege of the uid-0 superuser
    from the process' tree of children.
    
    The feature added by this patch can be leveraged to suppress the privilege
    associated with (set)uid-0.  This suppression requires CAP_SETPCAP to
    initiate, and only immediately affects the 'current' process (it is inherited
    through fork()/exec()).  This reimplementation differs significantly from the
    historical support for securebits which was system-wide, unwieldy and which
    has ultimately withered to a dead relic in the source of the modern kernel.
    
    With this patch applied a process, that is capable(CAP_SETPCAP), can now drop
    all legacy privilege (through uid=0) for itself and all subsequently
    fork()'d/exec()'d children with:
    
      prctl(PR_SET_SECUREBITS, 0x2f);
    
    This patch represents a no-op unless CONFIG_SECURITY_FILE_CAPABILITIES is
    enabled at configure time.
    
    [akpm@linux-foundation.org: fix uninitialised var warning]
    [serue@us.ibm.com: capabilities: use cap_task_prctl when !CONFIG_SECURITY]
    Signed-off-by: Andrew G. Morgan <morgan@kernel.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Reviewed-by: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Paul Moore <paul.moore@hp.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9a4f3e63e3bf..024d72b47a0c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -68,7 +68,6 @@ struct sched_param {
 #include <linux/smp.h>
 #include <linux/sem.h>
 #include <linux/signal.h>
-#include <linux/securebits.h>
 #include <linux/fs_struct.h>
 #include <linux/compiler.h>
 #include <linux/completion.h>
@@ -1133,7 +1132,7 @@ struct task_struct {
 	gid_t gid,egid,sgid,fsgid;
 	struct group_info *group_info;
 	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted, cap_bset;
-	unsigned keep_capabilities:1;
+	unsigned securebits;
 	struct user_struct *user;
 #ifdef CONFIG_KEYS
 	struct key *request_key_auth;	/* assumed request_key authority */

commit 402b08622d9ac6e32e25289573272e0f21bb58a7
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Mar 25 18:47:10 2008 +0100

    s390: KVM preparation: provide hook to enable pgstes in user pagetable
    
    The SIE instruction on s390 uses the 2nd half of the page table page to
    virtualize the storage keys of a guest. This patch offers the s390_enable_sie
    function, which reorganizes the page tables of a single-threaded process to
    reserve space in the page table:
    s390_enable_sie makes sure that the process is single threaded and then uses
    dup_mm to create a new mm with reorganized page tables. The old mm is freed
    and the process has now a page status extended field after every page table.
    
    Code that wants to exploit pgstes should SELECT CONFIG_PGSTE.
    
    This patch has a small common code hit, namely making dup_mm non-static.
    
    Edit (Carsten): I've modified Martin's patch, following Jeremy Fitzhardinge's
    review feedback. Now we do have the prototype for dup_mm in
    include/linux/sched.h. Following Martin's suggestion, s390_enable_sie() does now
    call task_lock() to prevent race against ptrace modification of mm_users.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d0bd97044abd..9a4f3e63e3bf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1798,6 +1798,8 @@ extern void mmput(struct mm_struct *);
 extern struct mm_struct *get_task_mm(struct task_struct *task);
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
+/* Allocate a new mm structure and copy contents from tsk->mm */
+extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
 extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, struct task_struct *, struct pt_regs *);
 extern void flush_thread(void);

commit 8c9843e57a7d9d7a090d6467a0f1f3afb8031527
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Apr 18 16:56:15 2008 +1000

    [POWERPC] Add thread_info_cache_init() weak hook
    
    Some architectures need to maintain a kmem cache for thread info
    structures.  The next commit adds that to powerpc to fix an alignment
    problem.
    
    There is no good arch callback to use to initialize that cache
    that I can find, so this adds a new one in the form of a weak
    function whose default is empty.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 311380e5fe89..d0bd97044abd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1926,6 +1926,8 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 
 #endif
 
+extern void thread_info_cache_init(void);
+
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit 429f731dea577bdd43693940cdca524135287e6a
Merge: 85b375a61308 d2f5e80862d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 21 15:41:27 2008 -0700

    Merge branch 'semaphore' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc
    
    * 'semaphore' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc:
      Deprecate the asm/semaphore.h files in feature-removal-schedule.
      Convert asm/semaphore.h users to linux/semaphore.h
      security: Remove unnecessary inclusions of asm/semaphore.h
      lib: Remove unnecessary inclusions of asm/semaphore.h
      kernel: Remove unnecessary inclusions of asm/semaphore.h
      include: Remove unnecessary inclusions of asm/semaphore.h
      fs: Remove unnecessary inclusions of asm/semaphore.h
      drivers: Remove unnecessary inclusions of asm/semaphore.h
      net: Remove unnecessary inclusions of asm/semaphore.h
      arch: Remove unnecessary inclusions of asm/semaphore.h

commit 4a55bd5e97b1775913f88f11108a4f144f590e89
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: fair-group: de-couple load-balancing from the rb-trees
    
    De-couple load-balancing from the rb-trees, so that I can change their
    organization.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 887f5db8942d..be6914014c70 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -946,6 +946,7 @@ struct load_weight {
 struct sched_entity {
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;
+	struct list_head	group_node;
 	unsigned int		on_rq;
 
 	u64			exec_start;

commit 58d6c2d72f8628f39e8689fbde8aa177fcf00a37
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: rt-group: optimize dequeue_rt_stack
    
    Now that the group hierarchy can have an arbitrary depth the O(n^2) nature
    of RT task dequeues will really hurt. Optimize this by providing space to
    store the tree path, so we can walk it the other way.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0a32059e6ed4..887f5db8942d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1005,6 +1005,7 @@ struct sched_rt_entity {
 	unsigned long timeout;
 	int nr_cpus_allowed;
 
+	struct sched_rt_entity *back;
 #ifdef CONFIG_RT_GROUP_SCHED
 	struct sched_rt_entity	*parent;
 	/* rq on which this entity is (to be) queued: */

commit 18d95a2832c1392a2d63227a7a6d433cb9f2037e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: fair-group: SMP-nice for group scheduling
    
    Implement SMP nice support for the full group hierarchy.
    
    On each load-balance action, compile a sched_domain wide view of the full
    task_group tree. We compute the domain wide view when walking down the
    hierarchy, and readjust the weights when walking back up.
    
    After collecting and readjusting the domain wide view, we try to balance the
    tasks within the task_groups. The current approach is a naively balance each
    task group until we've moved the targeted amount of load.
    
    Inspired by Srivatsa Vaddsgiri's previous code and Abhishek Chandra's H-SMP
    paper.
    
    XXX: there will be some numerical issues due to the limited nature of
         SCHED_LOAD_SCALE wrt to representing a task_groups influence on the
         total weight. When the tree is deep enough, or the task weight small
         enough, we'll run out of bits.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Abhishek Chandra <chandra@cs.umn.edu>
    CC: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11f47249cdd2..0a32059e6ed4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -758,6 +758,7 @@ struct sched_domain {
 	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
 	cpumask_t span;			/* span of all CPUs in this domain */
+	int first_cpu;			/* cache of the first cpu in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */

commit 1d3504fcf5606579d60b649d19f44b3871c1ddae
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Tue Apr 15 14:04:23 2008 +0900

    sched, cpuset: customize sched domains, core
    
    [rebased for sched-devel/latest]
    
     - Add a new cpuset file, having levels:
         sched_relax_domain_level
    
     - Modify partition_sched_domains() and build_sched_domains()
       to take attributes parameter passed from cpuset.
    
     - Fill newidle_idx for node domains which currently unused but
       might be required if sched_relax_domain_level become higher.
    
     - We can change the default level by boot option 'relax_domain_level='.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ada24022d230..11f47249cdd2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -704,6 +704,7 @@ enum cpu_idle_type {
 #define SD_POWERSAVINGS_BALANCE	256	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	512	/* Domain members share cpu pkg resources */
 #define SD_SERIALIZE		1024	/* Only a single load balancing instance */
+#define SD_WAKE_IDLE_FAR	2048	/* Gain latency sacrificing cache hit */
 
 #define BALANCE_FOR_MC_POWER	\
 	(sched_smt_power_savings ? SD_POWERSAVINGS_BALANCE : 0)
@@ -733,6 +734,24 @@ struct sched_group {
 	u32 reciprocal_cpu_power;
 };
 
+enum sched_domain_level {
+	SD_LV_NONE = 0,
+	SD_LV_SIBLING,
+	SD_LV_MC,
+	SD_LV_CPU,
+	SD_LV_NODE,
+	SD_LV_ALLNODES,
+	SD_LV_MAX
+};
+
+struct sched_domain_attr {
+	int relax_domain_level;
+};
+
+#define SD_ATTR_INIT	(struct sched_domain_attr) {	\
+	.relax_domain_level = -1,			\
+}
+
 struct sched_domain {
 	/* These fields must be setup */
 	struct sched_domain *parent;	/* top domain must be null terminated */
@@ -750,6 +769,7 @@ struct sched_domain {
 	unsigned int wake_idx;
 	unsigned int forkexec_idx;
 	int flags;			/* See SD_* */
+	enum sched_domain_level level;
 
 	/* Runtime fields. */
 	unsigned long last_balance;	/* init to jiffies. units in jiffies */
@@ -789,7 +809,8 @@ struct sched_domain {
 #endif
 };
 
-extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new);
+extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new,
+				    struct sched_domain_attr *dattr_new);
 extern int arch_reinit_sched_domains(void);
 
 #endif	/* CONFIG_SMP */

commit eff766a65c60237bfa865160c3129de31fab591b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: fix the task_group hierarchy for UID grouping
    
    UID grouping doesn't actually have a task_group representing the root of
    the task_group tree. Add one.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fa14781747cb..ada24022d230 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2051,6 +2051,9 @@ extern void normalize_rt_tasks(void);
 #ifdef CONFIG_GROUP_SCHED
 
 extern struct task_group init_task_group;
+#ifdef CONFIG_USER_SCHED
+extern struct task_group root_task_group;
+#endif
 
 extern struct task_group *sched_create_group(struct task_group *parent);
 extern void sched_destroy_group(struct task_group *tg);

commit ec7dc8ac73e4a56ed03b673f026f08c0d547f597
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Sat Apr 19 19:44:59 2008 +0200

    sched: allow the group scheduler to have multiple levels
    
    This patch makes the group scheduler multi hierarchy aware.
    
    [a.p.zijlstra@chello.nl: rt-parts and assorted fixes]
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 79c025c3b627..fa14781747cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2052,7 +2052,7 @@ extern void normalize_rt_tasks(void);
 
 extern struct task_group init_task_group;
 
-extern struct task_group *sched_create_group(void);
+extern struct task_group *sched_create_group(struct task_group *parent);
 extern void sched_destroy_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit cd8ba7cd9be0192348c2836cb6645d9b2cd2bfd2
Author: Mike Travis <travis@sgi.com>
Date:   Wed Mar 26 14:23:49 2008 -0700

    sched: add new set_cpus_allowed_ptr function
    
    Add a new function that accepts a pointer to the "newly allowed cpus"
    cpumask argument.
    
    int set_cpus_allowed_ptr(struct task_struct *p, const cpumask_t *new_mask)
    
    The current set_cpus_allowed() function is modified to use the above
    but this does not result in an ABI change.  And with some compiler
    optimization help, it may not introduce any additional overhead.
    
    Additionally, to enforce the read only nature of the new_mask arg, the
    "const" property is migrated to sub-functions called by set_cpus_allowed.
    This silences compiler warnings.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 383502dfda17..79c025c3b627 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -889,7 +889,8 @@ struct sched_class {
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
-	void (*set_cpus_allowed)(struct task_struct *p, cpumask_t *newmask);
+	void (*set_cpus_allowed)(struct task_struct *p,
+				 const cpumask_t *newmask);
 
 	void (*join_domain)(struct rq *rq);
 	void (*leave_domain)(struct rq *rq);
@@ -1502,15 +1503,21 @@ static inline void put_task_struct(struct task_struct *t)
 #define used_math() tsk_used_math(current)
 
 #ifdef CONFIG_SMP
-extern int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask);
+extern int set_cpus_allowed_ptr(struct task_struct *p,
+				const cpumask_t *new_mask);
 #else
-static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
+static inline int set_cpus_allowed_ptr(struct task_struct *p,
+				       const cpumask_t *new_mask)
 {
-	if (!cpu_isset(0, new_mask))
+	if (!cpu_isset(0, *new_mask))
 		return -EINVAL;
 	return 0;
 }
 #endif
+static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
+{
+	return set_cpus_allowed_ptr(p, &new_mask);
+}
 
 extern unsigned long long sched_clock(void);
 

commit b53e921ba1cff8453dc9a87a84052fa12d5b30bd
Author: Mike Travis <travis@sgi.com>
Date:   Fri Apr 4 18:11:08 2008 -0700

    generic: reduce stack pressure in sched_affinity
    
      * Modify sched_affinity functions to pass cpumask_t variables by reference
        instead of by value.
    
      * Use new set_cpus_allowed_ptr function.
    
    Depends on:
            [sched-devel]: sched: add new set_cpus_allowed_ptr function
    
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Cliff Wickman <cpw@sgi.com>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index be5d31752dbd..383502dfda17 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2034,7 +2034,7 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 }
 #endif
 
-extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
+extern long sched_setaffinity(pid_t pid, const cpumask_t *new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 
 extern int sched_mc_power_savings, sched_smt_power_savings;

commit d0b27fa77854b149ad4af08b0fe47fe712a47ade
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:44:57 2008 +0200

    sched: rt-group: synchonised bandwidth period
    
    Various SMP balancing algorithms require that the bandwidth period
    run in sync.
    
    Possible improvements are moving the rt_bandwidth thing into root_domain
    and keeping a span per rt_bandwidth which marks throttled cpus.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 15f05ff453d8..be5d31752dbd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1563,6 +1563,10 @@ int sched_nr_latency_handler(struct ctl_table *table, int write,
 extern unsigned int sysctl_sched_rt_period;
 extern int sysctl_sched_rt_runtime;
 
+int sched_rt_handler(struct ctl_table *table, int write,
+		struct file *filp, void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 extern unsigned int sysctl_sched_compat_yield;
 
 #ifdef CONFIG_RT_MUTEXES
@@ -2052,6 +2056,9 @@ extern unsigned long sched_group_shares(struct task_group *tg);
 extern int sched_group_set_rt_runtime(struct task_group *tg,
 				      long rt_runtime_us);
 extern long sched_group_rt_runtime(struct task_group *tg);
+extern int sched_group_set_rt_period(struct task_group *tg,
+				      long rt_period_us);
+extern long sched_group_rt_period(struct task_group *tg);
 #endif
 #endif
 

commit 50df5d6aea6694ca481b8005900401e8c95c2603
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 14 16:09:59 2008 +0100

    sched: remove sysctl_sched_batch_wakeup_granularity
    
    it's unused.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6a1e7afb099b..15f05ff453d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1551,7 +1551,6 @@ static inline void wake_up_idle_cpu(int cpu) { }
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
-extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;

commit 5a6483feb0c5193519625d0ea8c4254364d423cc
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Tue Feb 26 10:00:17 2008 -0500

    include: Remove unnecessary inclusions of asm/semaphore.h
    
    None of these files use any of the functionality promised by
    asm/semaphore.h.  It's possible that they (or some user of them) rely
    on it dragging in some unrelated header file, but I can't build all
    these files, so we'll have to fix any build failures as they come up.
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6a1e7afb099b..a37b5964828a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -61,7 +61,6 @@ struct sched_param {
 #include <linux/mm_types.h>
 
 #include <asm/system.h>
-#include <asm/semaphore.h>
 #include <asm/page.h>
 #include <asm/ptrace.h>
 #include <asm/cputime.h>

commit 06d8308c61e54346585b2691c13ee3f90cb6fb2f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 22 09:20:24 2008 +0100

    NOHZ: reevaluate idle sleep length after add_timer_on()
    
    add_timer_on() can add a timer on a CPU which is currently in a long
    idle sleep, but the timer wheel is not reevaluated by the nohz code on
    that CPU. So a timer can be delayed for quite a long time. This
    triggered a false positive in the clocksource watchdog code.
    
    To avoid this we need to wake up the idle CPU and enforce the
    reevaluation of the timer wheel for the next timer event.
    
    Add a function, which checks a given CPU for idle state, marks the
    idle task with NEED_RESCHED and sends a reschedule IPI to notify the
    other CPU of the change in the timer wheel.
    
    Call this function from add_timer_on().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: stable@kernel.org
    
    --
     include/linux/sched.h |    6 ++++++
     kernel/sched.c        |   43 +++++++++++++++++++++++++++++++++++++++++++
     kernel/timer.c        |   10 +++++++++-
     3 files changed, 58 insertions(+), 1 deletion(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fed07d03364e..6a1e7afb099b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1541,6 +1541,12 @@ static inline void idle_task_exit(void) {}
 
 extern void sched_idle_next(void);
 
+#if defined(CONFIG_NO_HZ) && defined(CONFIG_SMP)
+extern void wake_up_idle_cpu(int cpu);
+#else
+static inline void wake_up_idle_cpu(int cpu) { }
+#endif
+
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;

commit 9aefd0abd8610e8f3bb097debf3afb73f8b7b210
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Mar 12 18:31:58 2008 +0100

    sched: add exported arch_reinit_sched_domains() to header file.
    
    Needed so it can be called from outside of sched.c.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3625fcaf5d0f..fed07d03364e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -790,6 +790,7 @@ struct sched_domain {
 };
 
 extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new);
+extern int arch_reinit_sched_domains(void);
 
 #endif	/* CONFIG_SMP */
 

commit 4ae7d5cefd4aa3560e359a3b0f03e12adc8b5c86
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 19 01:42:00 2008 +0100

    sched: improve affine wakeups
    
    improve affine wakeups. Maintain the 'overlap' metric based on CFS's
    sum_exec_runtime - which means the amount of time a task executes
    after it wakes up some other task.
    
    Use the 'overlap' for the wakeup decisions: if the 'overlap' is short,
    it means there's strong workload coupling between this task and the
    woken up task. If the 'overlap' is large then the workload is decoupled
    and the scheduler will move them to separate CPUs more easily.
    
    ( Also slightly move the preempt_check within try_to_wake_up() - this has
      no effect on functionality but allows 'early wakeups' (for still-on-rq
      tasks) to be correctly accounted as well.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11d8e9a74eff..3625fcaf5d0f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -929,6 +929,9 @@ struct sched_entity {
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
 
+	u64			last_wakeup;
+	u64			avg_overlap;
+
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;

commit 810b38179e9e4d4f57b4b733767bb08f8291a965
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Feb 29 15:21:01 2008 -0500

    sched: retain vruntime
    
    Kei Tokunaga reported an interactivity problem when moving tasks
    between control groups.
    
    Tasks would retain their old vruntime when moved between groups, this
    can cause funny lags. Re-set the vruntime on group move to fit within
    the new tree.
    
    Reported-by: Kei Tokunaga <tokunaga.keiich@jp.fujitsu.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9ae4030067a9..11d8e9a74eff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -899,6 +899,10 @@ struct sched_class {
 			     int running);
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
 			     int oldprio, int running);
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	void (*moved_group) (struct task_struct *p);
+#endif
 };
 
 struct load_weight {

commit 62fb185130e4d420f71a30ff59d8b16b74ef5d2b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Feb 25 17:34:02 2008 +0100

    sched: revert load_balance_monitor() changes
    
    The following commits cause a number of regressions:
    
      commit 58e2d4ca581167c2a079f4ee02be2f0bc52e8729
      Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
      Date:   Fri Jan 25 21:08:00 2008 +0100
      sched: group scheduling, change how cpu load is calculated
    
      commit 6b2d7700266b9402e12824e11e0099ae6a4a6a79
      Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
      Date:   Fri Jan 25 21:08:00 2008 +0100
      sched: group scheduler, fix fairness of cpu bandwidth allocation for task groups
    
    Namely:
     - very frequent wakeups on SMP, reported by PowerTop users.
     - cacheline trashing on (large) SMP
     - some latencies larger than 500ms
    
    While there is a mergeable patch to fix the latter, the former issues
    are not fixable in a manner suitable for .25 (we're at -rc3 now).
    
    Hence we revert them and try again in v2.6.26.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Tested-by: Alexey Zaytsev <alexey.zaytsev@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c9621f8bf87..9ae4030067a9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1542,10 +1542,6 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
-#if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)
-extern unsigned int sysctl_sched_min_bal_int_shares;
-extern unsigned int sysctl_sched_max_bal_int_shares;
-#endif
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit 3fca96eed1cc9fb524aec536bba8ae921563f1bb
Merge: 98c1fc934c09 1481197b5011
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Tue Feb 26 07:49:15 2008 -0800

    Merge branch 'v2.6.25-rc3-lockdep' of git://git.kernel.org/pub/scm/linux/kernel/git/peterz/linux-2.6-lockdep
    
    * 'v2.6.25-rc3-lockdep' of git://git.kernel.org/pub/scm/linux/kernel/git/peterz/linux-2.6-lockdep:
      Subject: lockdep: include all lock classes in all_lock_classes
      lockdep: increase MAX_LOCK_DEPTH

commit bdb9441e9c325d50b5ae17f7d3205d65b8ed2e5f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Feb 25 23:02:48 2008 +0100

    lockdep: increase MAX_LOCK_DEPTH
    
    Some code paths exceed the current max lock depth (XFS), so increase
    this limit a bit. I looked at making this a dynamic allocated array,
    but we should not advocate insane lock depths, so stay with this as
    long as it works...
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e217d188a102..e3ea12437547 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1189,7 +1189,7 @@ struct task_struct {
 	int softirq_context;
 #endif
 #ifdef CONFIG_LOCKDEP
-# define MAX_LOCK_DEPTH 30UL
+# define MAX_LOCK_DEPTH 48UL
 	u64 curr_chain_key;
 	int lockdep_depth;
 	struct held_lock held_locks[MAX_LOCK_DEPTH];

commit 2d07b255c7b8a9723010e5c74778e058dc05162e
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Feb 15 09:56:34 2008 -0800

    sched: add declaration of sched_tail to sched.h
    
    Avoids sparse warnings:
    kernel/sched.c:2170:17: warning: symbol 'schedule_tail' was not declared. Should it be static?
    
    Avoids the need for an external declaration in arch/um/process.c
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e217d188a102..9c17e828d6d4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -242,6 +242,7 @@ struct task_struct;
 
 extern void sched_init(void);
 extern void sched_init_smp(void);
+extern asmlinkage void schedule_tail(struct task_struct *prev);
 extern void init_idle(struct task_struct *idle, int cpu);
 extern void init_idle_bootup_task(struct task_struct *idle);
 

commit b3c97528689619fc66569b30bf83d09d9929521a
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Wed Feb 13 15:03:15 2008 -0800

    include/linux: Remove all users of FASTCALL() macro
    
    FASTCALL() is always expanded to empty, remove it.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b9bb313fe1ae..e217d188a102 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -323,7 +323,7 @@ extern char __sched_text_start[], __sched_text_end[];
 extern int in_sched_functions(unsigned long addr);
 
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
-extern signed long FASTCALL(schedule_timeout(signed long timeout));
+extern signed long schedule_timeout(signed long timeout);
 extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
@@ -1648,10 +1648,10 @@ extern void release_uids(struct user_namespace *ns);
 
 extern void do_timer(unsigned long ticks);
 
-extern int FASTCALL(wake_up_state(struct task_struct * tsk, unsigned int state));
-extern int FASTCALL(wake_up_process(struct task_struct * tsk));
-extern void FASTCALL(wake_up_new_task(struct task_struct * tsk,
-						unsigned long clone_flags));
+extern int wake_up_state(struct task_struct *tsk, unsigned int state);
+extern int wake_up_process(struct task_struct *tsk);
+extern void wake_up_new_task(struct task_struct *tsk,
+				unsigned long clone_flags);
 #ifdef CONFIG_SMP
  extern void kick_process(struct task_struct *tsk);
 #else
@@ -1741,7 +1741,7 @@ static inline int sas_ss_flags(unsigned long sp)
 extern struct mm_struct * mm_alloc(void);
 
 /* mmdrop drops the mm and the page tables */
-extern void FASTCALL(__mmdrop(struct mm_struct *));
+extern void __mmdrop(struct mm_struct *);
 static inline void mmdrop(struct mm_struct * mm)
 {
 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
@@ -1925,7 +1925,7 @@ static inline int signal_pending(struct task_struct *p)
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
 }
 
-extern int FASTCALL(__fatal_signal_pending(struct task_struct *p));
+extern int __fatal_signal_pending(struct task_struct *p);
 
 static inline int fatal_signal_pending(struct task_struct *p)
 {

commit 052f1dc7eb02300b05170ae341ccd03b76207778
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 13 15:45:40 2008 +0100

    sched: rt-group: make rt groups scheduling configurable
    
    Make the rt group scheduler compile time configurable.
    Keep it experimental for now.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 142eb293f9c4..b9bb313fe1ae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -590,7 +590,7 @@ struct user_struct {
 	struct hlist_node uidhash_node;
 	uid_t uid;
 
-#ifdef CONFIG_FAIR_USER_SCHED
+#ifdef CONFIG_USER_SCHED
 	struct task_group *tg;
 #ifdef CONFIG_SYSFS
 	struct kobject kobj;
@@ -973,7 +973,7 @@ struct sched_rt_entity {
 	unsigned long timeout;
 	int nr_cpus_allowed;
 
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_RT_GROUP_SCHED
 	struct sched_rt_entity	*parent;
 	/* rq on which this entity is (to be) queued: */
 	struct rt_rq		*rt_rq;
@@ -2027,19 +2027,22 @@ extern int sched_mc_power_savings, sched_smt_power_savings;
 
 extern void normalize_rt_tasks(void);
 
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED
 
 extern struct task_group init_task_group;
 
 extern struct task_group *sched_create_group(void);
 extern void sched_destroy_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
+#ifdef CONFIG_FAIR_GROUP_SCHED
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 extern unsigned long sched_group_shares(struct task_group *tg);
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
 extern int sched_group_set_rt_runtime(struct task_group *tg,
 				      long rt_runtime_us);
 extern long sched_group_rt_runtime(struct task_group *tg);
-
+#endif
 #endif
 
 #ifdef CONFIG_TASK_XACCT

commit 9f0c1e560c43327b70998e6c702b2f01321130d9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 13 15:45:39 2008 +0100

    sched: rt-group: interface
    
    Change the rt_ratio interface to rt_runtime_us, to match rt_period_us.
    This avoids picking a granularity for the ratio.
    
    Extend the /sys/kernel/uids/<uid>/ interface to allow setting
    the group's rt_runtime.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 00e144117326..142eb293f9c4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1541,8 +1541,6 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
-extern unsigned int sysctl_sched_rt_period;
-extern unsigned int sysctl_sched_rt_ratio;
 #if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)
 extern unsigned int sysctl_sched_min_bal_int_shares;
 extern unsigned int sysctl_sched_max_bal_int_shares;
@@ -1552,6 +1550,8 @@ int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,
 		loff_t *ppos);
 #endif
+extern unsigned int sysctl_sched_rt_period;
+extern int sysctl_sched_rt_runtime;
 
 extern unsigned int sysctl_sched_compat_yield;
 
@@ -2036,6 +2036,9 @@ extern void sched_destroy_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 extern unsigned long sched_group_shares(struct task_group *tg);
+extern int sched_group_set_rt_runtime(struct task_group *tg,
+				      long rt_runtime_us);
+extern long sched_group_rt_runtime(struct task_group *tg);
 
 #endif
 

commit 146a505d498c36de98ec161d791dd50beca7f9a3
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Feb 8 04:19:22 2008 -0800

    Get rid of the kill_pgrp_info() function
    
    There's only one caller left - the kill_pgrp one - so merge these two
    functions and forget the kill_pgrp_info one.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Reviewed-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b2d161d87db7..00e144117326 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1689,7 +1689,6 @@ extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int force_sigsegv(int, struct task_struct *);
 extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
-extern int kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
 extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_t, u32);
 extern int kill_pgrp(struct pid *pid, int sig, int priv);

commit fea9d175545b38cb3e84569400419eb81bc90fa3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:19 2008 -0800

    ITIMER_REAL: convert to use struct pid
    
    signal_struct->tsk points to the ->group_leader and thus we have the nasty
    code in de_thread() which has to change it and restart ->real_timer if the
    leader is changed.
    
    Use "struct pid *leader_pid" instead.  This also allows us to kill now
    unneeded send_group_sig_info().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3deb6e5d3096..b2d161d87db7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -460,7 +460,7 @@ struct signal_struct {
 
 	/* ITIMER_REAL timer for the process */
 	struct hrtimer real_timer;
-	struct task_struct *tsk;
+	struct pid *leader_pid;
 	ktime_t it_real_incr;
 
 	/* ITIMER_PROF and ITIMER_VIRTUAL timers for the process */
@@ -1686,7 +1686,6 @@ extern void block_all_signals(int (*notifier)(void *priv), void *priv,
 extern void unblock_all_signals(void);
 extern void release_task(struct task_struct * p);
 extern int send_sig_info(int, struct siginfo *, struct task_struct *);
-extern int send_group_sig_info(int, struct siginfo *, struct task_struct *);
 extern int force_sigsegv(int, struct task_struct *);
 extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);

commit 44c4e1b2581f7273ab14ef30b6430618801c57b1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Feb 8 04:19:15 2008 -0800

    pid: Extend/Fix pid_vnr
    
    pid_vnr returns the user space pid with respect to the pid namespace the
    struct pid was allocated in.  What we want before we return a pid to user
    space is the user space pid with respect to the pid namespace of current.
    
    pid_vnr is a very nice optimization but because it isn't quite what we want
    it is easy to use pid_vnr at times when we aren't certain the struct pid
    was allocated in our pid namespace.
    
    Currently this describes at least tiocgpgrp and tiocgsid in ttyio.c the
    parent process reported in the core dumps and the parent process in
    get_signal_to_deliver.
    
    So unless the performance impact is huge having an interface that does what
    we want instead of always what we want should be much more reliable and
    much less error prone.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d1c9b7f1d51e..3deb6e5d3096 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1332,9 +1332,8 @@ struct pid_namespace;
  * from various namespaces
  *
  * task_xid_nr()     : global id, i.e. the id seen from the init namespace;
- * task_xid_vnr()    : virtual id, i.e. the id seen from the namespace the task
- *                     belongs to. this only makes sence when called in the
- *                     context of the task that belongs to the same namespace;
+ * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of
+ *                     current.
  * task_xid_nr_ns()  : id seen from the ns specified;
  *
  * set_task_vxid()   : assigns a virtual id to a task;

commit 8520d7c7f8611216e3b270becec95bb35b6899d4
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:09 2008 -0800

    teach set_special_pids() to use struct pid
    
    Change set_special_pids() to work with struct pid, not pid_t from global name
    space. This again speedups and imho cleanups the code, also a preparation for
    the next patch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8a4812c1c038..d1c9b7f1d51e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1632,7 +1632,7 @@ extern struct task_struct *find_task_by_vpid(pid_t nr);
 extern struct task_struct *find_task_by_pid_ns(pid_t nr,
 		struct pid_namespace *ns);
 
-extern void __set_special_pids(pid_t session, pid_t pgrp);
+extern void __set_special_pids(struct pid *pid);
 
 /* per-UID process charging. */
 extern struct user_struct * alloc_uid(struct user_namespace *, uid_t);

commit 78fb74669e80883323391090e4d26d17fe29488f
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Thu Feb 7 00:13:51 2008 -0800

    Memory controller: accounting setup
    
    Basic setup routines, the mm_struct has a pointer to the cgroup that
    it belongs to and the the page has a page_cgroup associated with it.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7c8ca05c3cae..8a4812c1c038 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -92,6 +92,7 @@ struct sched_param {
 
 #include <asm/processor.h>
 
+struct mem_cgroup;
 struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;

commit 1bf47346d75790ebd2563d909d48046961c7ffd5
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed Feb 6 01:37:56 2008 -0800

    kernel/sys.c: get rid of expensive divides in groups_sort()
    
    groups_sort() can be quite long if user loads a large gid table.
    
    This is because GROUP_AT(group_info, some_integer) uses an integer divide.
    So having to do XXX thousand divides during one syscall can lead to very
    high latencies.  (NGROUPS_MAX=65536)
    
    In the past (25 Mar 2006), an analog problem was found in groups_search()
    (commit d74beb9f33a5f16d2965f11b275e401f225c949d ) and at that time I
    changed some variables to unsigned int.
    
    I believe that a more generic fix is to make sure NGROUPS_PER_BLOCK is
    unsigned.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c13be3a21e8..7c8ca05c3cae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -810,7 +810,7 @@ static inline int above_background_load(void)
 
 struct io_context;			/* See blkdev.h */
 #define NGROUPS_SMALL		32
-#define NGROUPS_PER_BLOCK	((int)(PAGE_SIZE / sizeof(gid_t)))
+#define NGROUPS_PER_BLOCK	((unsigned int)(PAGE_SIZE / sizeof(gid_t)))
 struct group_info {
 	int ngroups;
 	atomic_t usage;

commit 3b7391de67da515c91f48aa371de77cb6cc5c07e
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Feb 4 22:29:45 2008 -0800

    capabilities: introduce per-process capability bounding set
    
    The capability bounding set is a set beyond which capabilities cannot grow.
     Currently cap_bset is per-system.  It can be manipulated through sysctl,
    but only init can add capabilities.  Root can remove capabilities.  By
    default it includes all caps except CAP_SETPCAP.
    
    This patch makes the bounding set per-process when file capabilities are
    enabled.  It is inherited at fork from parent.  Noone can add elements,
    CAP_SETPCAP is required to remove them.
    
    One example use of this is to start a safer container.  For instance, until
    device namespaces or per-container device whitelists are introduced, it is
    best to take CAP_MKNOD away from a container.
    
    The bounding set will not affect pP and pE immediately.  It will only
    affect pP' and pE' after subsequent exec()s.  It also does not affect pI,
    and exec() does not constrain pI'.  So to really start a shell with no way
    of regain CAP_MKNOD, you would do
    
            prctl(PR_CAPBSET_DROP, CAP_MKNOD);
            cap_t cap = cap_get_proc();
            cap_value_t caparray[1];
            caparray[0] = CAP_MKNOD;
            cap_set_flag(cap, CAP_INHERITABLE, 1, caparray, CAP_DROP);
            cap_set_proc(cap);
            cap_free(cap);
    
    The following test program will get and set the bounding
    set (but not pI).  For instance
    
            ./bset get
                    (lists capabilities in bset)
            ./bset drop cap_net_raw
                    (starts shell with new bset)
                    (use capset, setuid binary, or binary with
                    file capabilities to try to increase caps)
    
    ************************************************************
    cap_bound.c
    ************************************************************
     #include <sys/prctl.h>
     #include <linux/capability.h>
     #include <sys/types.h>
     #include <unistd.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
    
     #ifndef PR_CAPBSET_READ
     #define PR_CAPBSET_READ 23
     #endif
    
     #ifndef PR_CAPBSET_DROP
     #define PR_CAPBSET_DROP 24
     #endif
    
    int usage(char *me)
    {
            printf("Usage: %s get\n", me);
            printf("       %s drop <capability>\n", me);
            return 1;
    }
    
     #define numcaps 32
    char *captable[numcaps] = {
            "cap_chown",
            "cap_dac_override",
            "cap_dac_read_search",
            "cap_fowner",
            "cap_fsetid",
            "cap_kill",
            "cap_setgid",
            "cap_setuid",
            "cap_setpcap",
            "cap_linux_immutable",
            "cap_net_bind_service",
            "cap_net_broadcast",
            "cap_net_admin",
            "cap_net_raw",
            "cap_ipc_lock",
            "cap_ipc_owner",
            "cap_sys_module",
            "cap_sys_rawio",
            "cap_sys_chroot",
            "cap_sys_ptrace",
            "cap_sys_pacct",
            "cap_sys_admin",
            "cap_sys_boot",
            "cap_sys_nice",
            "cap_sys_resource",
            "cap_sys_time",
            "cap_sys_tty_config",
            "cap_mknod",
            "cap_lease",
            "cap_audit_write",
            "cap_audit_control",
            "cap_setfcap"
    };
    
    int getbcap(void)
    {
            int comma=0;
            unsigned long i;
            int ret;
    
            printf("i know of %d capabilities\n", numcaps);
            printf("capability bounding set:");
            for (i=0; i<numcaps; i++) {
                    ret = prctl(PR_CAPBSET_READ, i);
                    if (ret < 0)
                            perror("prctl");
                    else if (ret==1)
                            printf("%s%s", (comma++) ? ", " : " ", captable[i]);
            }
            printf("\n");
            return 0;
    }
    
    int capdrop(char *str)
    {
            unsigned long i;
    
            int found=0;
            for (i=0; i<numcaps; i++) {
                    if (strcmp(captable[i], str) == 0) {
                            found=1;
                            break;
                    }
            }
            if (!found)
                    return 1;
            if (prctl(PR_CAPBSET_DROP, i)) {
                    perror("prctl");
                    return 1;
            }
            return 0;
    }
    
    int main(int argc, char *argv[])
    {
            if (argc<2)
                    return usage(argv[0]);
            if (strcmp(argv[1], "get")==0)
                    return getbcap();
            if (strcmp(argv[1], "drop")!=0 || argc<3)
                    return usage(argv[0]);
            if (capdrop(argv[2])) {
                    printf("unknown capability\n");
                    return 1;
            }
            return execl("/bin/bash", "/bin/bash", NULL);
    }
    ************************************************************
    
    [serue@us.ibm.com: fix typo]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew G. Morgan <morgan@kernel.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Casey Schaufler <casey@schaufler-ca.com>a
    Signed-off-by: "Serge E. Hallyn" <serue@us.ibm.com>
    Tested-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c30d174a02fa..9c13be3a21e8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1098,7 +1098,7 @@ struct task_struct {
 	uid_t uid,euid,suid,fsuid;
 	gid_t gid,egid,sgid,fsgid;
 	struct group_info *group_info;
-	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted;
+	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted, cap_bset;
 	unsigned keep_capabilities:1;
 	struct user_struct *user;
 #ifdef CONFIG_KEYS

commit 824552574162ac00ae636fa41386b1072379ea4a
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Mon Feb 4 22:28:59 2008 -0800

    maps4: rework TASK_SIZE macros
    
    The following replaces the earlier patches sent.  It should address
    David Rientjes's comments, and has been compile tested on all the
    architectures that it touches, save for parisc.
    
    For the /proc/<pid>/pagemap code[1], we need to able to query how
    much virtual address space a particular task has.  The trick is
    that we do it through /proc and can't use TASK_SIZE since it
    references "current" on some arches.  The process opening the
    /proc file might be a 32-bit process opening a 64-bit process's
    pagemap file.
    
    x86_64 already has a TASK_SIZE_OF() macro:
    
    #define TASK_SIZE_OF(child)     ((test_tsk_thread_flag(child, TIF_IA32)) ? IA32_PAGE_OFFSET : TASK_SIZE64)
    
    I'd like to have that for other architectures.  So, add it
    for all the architectures that actually use "current" in
    their TASK_SIZE.  For the others, just add a quick #define
    in sched.h to use plain old TASK_SIZE.
    
    1. http://www.linuxworld.com/news/2007/042407-kernel.html
    
    - MIPS portion from Ralf Baechle <ralf@linux-mips.org>
    
    [akpm@linux-foundation.org: fix mips build]
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Matt Mackall <mpm@selenic.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 483ea4e1accf..c30d174a02fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2087,6 +2087,10 @@ static inline void migration_init(void)
 }
 #endif
 
+#ifndef TASK_SIZE_OF
+#define TASK_SIZE_OF(tsk)	TASK_SIZE
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif

commit ed5d2cac114202fe2978a9cbcab8f5032796d538
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon Feb 4 22:27:24 2008 -0800

    exec: rework the group exit and fix the race with kill
    
    As Roland pointed out, we have the very old problem with exec.  de_thread()
    sets SIGNAL_GROUP_EXIT, kills other threads, changes ->group_leader and then
    clears signal->flags.  All signals (even fatal ones) sent in this window
    (which is not too small) will be lost.
    
    With this patch exec doesn't abuse SIGNAL_GROUP_EXIT.  signal_group_exit(),
    the new helper, should be used to detect exit_group() or exec() in progress.
    It can have more users, but this patch does only strictly necessary changes.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 680bb03a4b90..483ea4e1accf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -555,6 +555,13 @@ struct signal_struct {
 #define SIGNAL_STOP_CONTINUED	0x00000004 /* SIGCONT since WCONTINUED reap */
 #define SIGNAL_GROUP_EXIT	0x00000008 /* group exit in progress */
 
+/* If true, all threads except ->group_exit_task have pending SIGKILL */
+static inline int signal_group_exit(const struct signal_struct *sig)
+{
+	return	(sig->flags & SIGNAL_GROUP_EXIT) ||
+		(sig->group_exit_task != NULL);
+}
+
 /*
  * Some day this will be a full-fledged user tracking system..
  */

commit 59714d65dfbc86d5cb93adc5bac57a921cc2fa84
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Feb 4 22:27:21 2008 -0800

    get_task_comm(): return the result
    
    It was dumb to make get_task_comm() return void.  Change it to return a
    pointer to the resulting output for caller convenience.
    
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af6947e69b40..680bb03a4b90 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1770,7 +1770,7 @@ extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned lon
 struct task_struct *fork_idle(int);
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
-extern void get_task_comm(char *to, struct task_struct *tsk);
+extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
 extern void wait_task_inactive(struct task_struct * p);

commit 4746ec5b01ed07205a91e4f7ed9de9d70f371407
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Jan 8 10:06:53 2008 -0500

    [AUDIT] add session id to audit messages
    
    In order to correlate audit records to an individual login add a session
    id.  This is incremented every time a user logs in and is included in
    almost all messages which currently output the auid.  The field is
    labeled ses=  or oses=
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e2730389089..af6947e69b40 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1141,6 +1141,7 @@ struct task_struct {
 	struct audit_context *audit_context;
 #ifdef CONFIG_AUDITSYSCALL
 	uid_t loginuid;
+	unsigned int sessionid;
 #endif
 	seccomp_t seccomp;
 

commit bfef93a5d1fb5654fe2025276c55e202d10b5255
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Jan 10 04:53:18 2008 -0500

    [PATCH] get rid of loginuid races
    
    Keeping loginuid in audit_context is racy and results in messier
    code.  Taken to task_struct, out of the way of ->audit_context
    changes.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6c333579d9da..5e2730389089 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1139,6 +1139,9 @@ struct task_struct {
 	void *security;
 #endif
 	struct audit_context *audit_context;
+#ifdef CONFIG_AUDITSYSCALL
+	uid_t loginuid;
+#endif
 	seccomp_t seccomp;
 
 /* Thread group tracking */

commit 75659ca0c10992dcb39258518368a0f6f56e935d
Merge: fbdde7bd274d 2dfe485a2c8a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 1 11:45:47 2008 +1100

    Merge branch 'task_killable' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc
    
    * 'task_killable' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc: (22 commits)
      Remove commented-out code copied from NFS
      NFS: Switch from intr mount option to TASK_KILLABLE
      Add wait_for_completion_killable
      Add wait_event_killable
      Add schedule_timeout_killable
      Use mutex_lock_killable in vfs_readdir
      Add mutex_lock_killable
      Use lock_page_killable
      Add lock_page_killable
      Add fatal_signal_pending
      Add TASK_WAKEKILL
      exit: Use task_is_*
      signal: Use task_is_*
      sched: Use task_contributes_to_load, TASK_ALL and TASK_NORMAL
      ptrace: Use task_is_*
      power: Use task_is_*
      wait: Use TASK_NORMAL
      proc/base.c: Use task_is_*
      proc/array.c: Use TASK_REPORT
      perfmon: Use task_is_*
      ...
    
    Fixed up conflicts in NFS/sunrpc manually..

commit 95c354fe9f7d6decc08a92aa26eb233ecc2155bf
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Jan 30 13:31:20 2008 +0100

    spinlock: lockbreak cleanup
    
    The break_lock data structure and code for spinlocks is quite nasty.
    Not only does it double the size of a spinlock but it changes locking to
    a potentially less optimal trylock.
    
    Put all of that under CONFIG_GENERIC_LOCKBREAK, and introduce a
    __raw_spin_is_contended that uses the lock data itself to determine whether
    there are waiters on the lock, to be used if CONFIG_GENERIC_LOCKBREAK is
    not set.
    
    Rename need_lockbreak to spin_needbreak, make it use spin_is_contended to
    decouple it from the spinlock implementation, and make it typesafe (rwlocks
    do not have any need_lockbreak sites -- why do they even get bloated up
    with that break_lock then?).
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2d0546e884ea..9d4797609aa5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1922,23 +1922,16 @@ extern int cond_resched_softirq(void);
 
 /*
  * Does a critical section need to be broken due to another
- * task waiting?:
+ * task waiting?: (technically does not depend on CONFIG_PREEMPT,
+ * but a general need for low latency)
  */
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_SMP)
-# define need_lockbreak(lock) ((lock)->break_lock)
-#else
-# define need_lockbreak(lock) 0
-#endif
-
-/*
- * Does a critical section need to be broken due to another
- * task waiting or preemption being signalled:
- */
-static inline int lock_need_resched(spinlock_t *lock)
+static inline int spin_needbreak(spinlock_t *lock)
 {
-	if (need_lockbreak(lock) || need_resched())
-		return 1;
+#ifdef CONFIG_PREEMPT
+	return spin_is_contended(lock);
+#else
 	return 0;
+#endif
 }
 
 /*

commit fadad878cc0640cc9cd5569998bf54b693f7b38b
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:54:47 2008 +0100

    kernel: add CLONE_IO to specifically request sharing of IO contexts
    
    syslets (or other threads/processes that want io context sharing) can
    set this to enforce sharing of io context.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 80837e7d527e..2d0546e884ea 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -27,6 +27,7 @@
 #define CLONE_NEWUSER		0x10000000	/* New user namespace */
 #define CLONE_NEWPID		0x20000000	/* New pid namespace */
 #define CLONE_NEWNET		0x40000000	/* New network namespace */
+#define CLONE_IO		0x80000000	/* Clone io context */
 
 /*
  * Scheduling policies

commit fd0928df98b9578be8a786ac0cb78a47a5e17a20
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:52:45 2008 +0100

    ioprio: move io priority from task_struct to io_context
    
    This is where it belongs and then it doesn't take up space for a
    process that doesn't do IO.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index df5b24ee80b3..80837e7d527e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -975,7 +975,6 @@ struct task_struct {
 	struct hlist_head preempt_notifiers;
 #endif
 
-	unsigned short ioprio;
 	/*
 	 * fpu_counter contains the number of consecutive context switches
 	 * that the FPU is used. If this is over a threshold, the lazy fpu

commit 6d082592b62689fb91578d0338d04a9f50991990
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Jan 25 21:08:35 2008 +0100

    sched: keep total / count stats in addition to the max for
    
    Right now, the linux kernel (with scheduler statistics enabled) keeps track
    of the maximum time a process is waiting to be scheduled. While the maximum
    is a very useful metric, tracking average and total is equally useful
    (at least for latencytop) to figure out the accumulated effect of scheduler
    delays. The accumulated effect is important to judge the performance impact
    of scheduler tuning/behavior.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 734f6d8f6ed5..df5b24ee80b3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -895,6 +895,8 @@ struct sched_entity {
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;
+	u64			wait_count;
+	u64			wait_sum;
 
 	u64			sleep_start;
 	u64			sleep_max;

commit 286100a6cf1c1f692e5f81d14b364ff12b7662f5
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Jan 25 21:08:34 2008 +0100

    sched, futex: detach sched.h and futex.h
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 53534f90a96a..734f6d8f6ed5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -78,7 +78,6 @@ struct sched_param {
 #include <linux/proportions.h>
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
-#include <linux/futex.h>
 #include <linux/rtmutex.h>
 
 #include <linux/time.h>
@@ -94,6 +93,7 @@ struct sched_param {
 
 struct exec_domain;
 struct futex_pi_state;
+struct robust_list_head;
 struct bio;
 
 /*

commit 90739081ef8d5495d50abba9c5d333be9acd872a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:34 2008 +0100

    softlockup: fix signedness
    
    fix softlockup tunables signedness.
    
    mark tunables read-mostly.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dfc76e172f3f..53534f90a96a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -269,10 +269,10 @@ extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
-extern int softlockup_thresh;
+extern unsigned long  softlockup_thresh;
 extern unsigned long sysctl_hung_task_check_count;
 extern unsigned long sysctl_hung_task_timeout_secs;
-extern long sysctl_hung_task_warnings;
+extern unsigned long sysctl_hung_task_warnings;
 #else
 static inline void softlockup_tick(void)
 {

commit 9745512ce79de686df354dc70a8d1a74d801892d
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Jan 25 21:08:34 2008 +0100

    sched: latencytop support
    
    LatencyTOP kernel infrastructure; it measures latencies in the
    scheduler and tracks it system wide and per process.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index acadcab89ef9..dfc76e172f3f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -88,6 +88,7 @@ struct sched_param {
 #include <linux/hrtimer.h>
 #include <linux/task_io_accounting.h>
 #include <linux/kobject.h>
+#include <linux/latencytop.h>
 
 #include <asm/processor.h>
 
@@ -1220,6 +1221,10 @@ struct task_struct {
 	int make_it_fail;
 #endif
 	struct prop_local_single dirties;
+#ifdef CONFIG_LATENCYTOP
+	int latency_record_count;
+	struct latency_record latency_record[LT_SAVECOUNT];
+#endif
 };
 
 /*

commit 48d5e258216f1c7713633439beb98a38c7290649
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:31 2008 +0100

    sched: rt throttling vs no_hz
    
    We need to teach no_hz about the rt throttling because its tick driven.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04eecbf0241e..acadcab89ef9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -230,6 +230,8 @@ static inline int select_nohz_load_balancer(int cpu)
 }
 #endif
 
+extern unsigned long rt_needs_cpu(int cpu);
+
 /*
  * Only dump TASK_* tasks. (0 for all tasks)
  */

commit 6f505b16425a51270058e4a93441fe64de3dd435
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:30 2008 +0100

    sched: rt group scheduling
    
    Extend group scheduling to also cover the realtime classes. It uses the time
    limiting introduced by the previous patch to allow multiple realtime groups.
    
    The hard time limit is required to keep behaviour deterministic.
    
    The algorithms used make the realtime scheduler O(tg), linear scaling wrt the
    number of task groups. This is the worst case behaviour I can't seem to get out
    of, the avg. case of the algorithms can be improved, I focused on correctness
    and worst case.
    
    [ akpm@linux-foundation.org: move side-effects out of BUG_ON(). ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d5ea144df836..04eecbf0241e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -934,6 +934,15 @@ struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned int time_slice;
 	unsigned long timeout;
+	int nr_cpus_allowed;
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	struct sched_rt_entity	*parent;
+	/* rq on which this entity is (to be) queued: */
+	struct rt_rq		*rt_rq;
+	/* rq "owned" by this entity/group: */
+	struct rt_rq		*my_q;
+#endif
 };
 
 struct task_struct {
@@ -978,7 +987,6 @@ struct task_struct {
 
 	unsigned int policy;
 	cpumask_t cpus_allowed;
-	int nr_cpus_allowed;
 
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;

commit fa85ae2418e6843953107cd6a06f645752829bc0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:29 2008 +0100

    sched: rt time limit
    
    Very simple time limit on the realtime scheduling classes.
    Allow the rq's realtime class to consume sched_rt_ratio of every
    sched_rt_period slice. If the class exceeds this quota the fair class
    will preempt the realtime class.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 43e0339d65fc..d5ea144df836 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1490,6 +1490,8 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
+extern unsigned int sysctl_sched_rt_period;
+extern unsigned int sysctl_sched_rt_ratio;
 #if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)
 extern unsigned int sysctl_sched_min_bal_int_shares;
 extern unsigned int sysctl_sched_max_bal_int_shares;

commit 8f4d37ec073c17e2d4aa8851df5837d798606d6f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:29 2008 +0100

    sched: high-res preemption tick
    
    Use HR-timers (when available) to deliver an accurate preemption tick.
    
    The regular scheduler tick that runs at 1/HZ can be too coarse when nice
    level are used. The fairness system will still keep the cpu utilisation 'fair'
    by then delaying the task that got an excessive amount of CPU time but try to
    minimize this by delivering preemption points spot-on.
    
    The average frequency of this extra interrupt is sched_latency / nr_latency.
    Which need not be higher than 1/HZ, its just that the distribution within the
    sched_latency period is important.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7907845c2348..43e0339d65fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -257,6 +257,7 @@ extern void trap_init(void);
 extern void account_process_tick(struct task_struct *task, int user);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
+extern void hrtick_resched(void);
 
 extern void sched_show_task(struct task_struct *p);
 
@@ -849,7 +850,7 @@ struct sched_class {
 #endif
 
 	void (*set_curr_task) (struct rq *rq);
-	void (*task_tick) (struct rq *rq, struct task_struct *p);
+	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
 	void (*set_cpus_allowed)(struct task_struct *p, cpumask_t *newmask);
 

commit 02b67cc3ba36bdba351d6c3a00593f4ec550d9d3
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Jan 25 21:08:28 2008 +0100

    sched: do not do cond_resched() when CONFIG_PREEMPT
    
    Why do we even have cond_resched when real preemption
    is on? It seems to be a waste of space and time.
    
    remove cond_resched with CONFIG_PREEMPT on.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fe3f8fbc614e..7907845c2348 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1885,7 +1885,18 @@ static inline int need_resched(void)
  * cond_resched_lock() will drop the spinlock before scheduling,
  * cond_resched_softirq() will enable bhs before scheduling.
  */
-extern int cond_resched(void);
+#ifdef CONFIG_PREEMPT
+static inline int cond_resched(void)
+{
+	return 0;
+}
+#else
+extern int _cond_resched(void);
+static inline int cond_resched(void)
+{
+	return _cond_resched();
+}
+#endif
 extern int cond_resched_lock(spinlock_t * lock);
 extern int cond_resched_softirq(void);
 

commit 78f2c7db6068fd6ef75b8c120f04a388848eacb5
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:27 2008 +0100

    sched: SCHED_FIFO/SCHED_RR watchdog timer
    
    Introduce a new rlimit that allows the user to set a runtime timeout on
    real-time tasks their slice. Once this limit is exceeded the task will receive
    SIGXCPU.
    
    So it measures runtime since the last sleep.
    
    Input and ideas by Thomas Gleixner and Lennart Poettering.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Lennart Poettering <mzxreary@0pointer.de>
    CC: Michael Kerrisk <mtk.manpages@googlemail.com>
    CC: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a06d09ebd5c6..fe3f8fbc614e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -932,6 +932,7 @@ struct sched_entity {
 struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned int time_slice;
+	unsigned long timeout;
 };
 
 struct task_struct {

commit fa717060f1ab7eb6570f2fb49136f838fc9195a9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:27 2008 +0100

    sched: sched_rt_entity
    
    Move the task_struct members specific to rt scheduling together.
    A future optimization could be to put sched_entity and sched_rt_entity
    into a union.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    CC: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 72e1b8ecfbe1..a06d09ebd5c6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -929,6 +929,11 @@ struct sched_entity {
 #endif
 };
 
+struct sched_rt_entity {
+	struct list_head run_list;
+	unsigned int time_slice;
+};
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -945,9 +950,9 @@ struct task_struct {
 #endif
 
 	int prio, static_prio, normal_prio;
-	struct list_head run_list;
 	const struct sched_class *sched_class;
 	struct sched_entity se;
+	struct sched_rt_entity rt;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -972,7 +977,6 @@ struct task_struct {
 	unsigned int policy;
 	cpumask_t cpus_allowed;
 	int nr_cpus_allowed;
-	unsigned int time_slice;
 
 #ifdef CONFIG_PREEMPT_RCU
 	int rcu_read_lock_nesting;

commit e260be673a15b6125068270e0216a3bfbfc12f87
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 25 21:08:24 2008 +0100

    Preempt-RCU: implementation
    
    This patch implements a new version of RCU which allows its read-side
    critical sections to be preempted. It uses a set of counter pairs
    to keep track of the read-side critical sections and flips them
    when all tasks exit read-side critical section. The details
    of this implementation can be found in this paper -
    
            http://www.rdrop.com/users/paulmck/RCU/OLSrtRCU.2006.08.11a.pdf
    
    and the article-
    
            http://lwn.net/Articles/253651/
    
    This patch was developed as a part of the -rt kernel development and
    meant to provide better latencies when read-side critical sections of
    RCU don't disable preemption.  As a consequence of keeping track of RCU
    readers, the readers have a slight overhead (optimizations in the paper).
    This implementation co-exists with the "classic" RCU implementations
    and can be switched to at compiler.
    
    Also includes RCU tracing summarized in debugfs.
    
    [ akpm@linux-foundation.org: build fixes on non-preempt architectures ]
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Reviewed-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f2044e707004..72e1b8ecfbe1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -974,6 +974,11 @@ struct task_struct {
 	int nr_cpus_allowed;
 	unsigned int time_slice;
 
+#ifdef CONFIG_PREEMPT_RCU
+	int rcu_read_lock_nesting;
+	int rcu_flipctr_idx;
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
+
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
 #endif

commit cb46984504048db946cd551c261df4e70d59a8ea
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jan 25 21:08:22 2008 +0100

    sched: RT-balance, add new methods to sched_class
    
    Dmitry Adamushko found that the current implementation of the RT
    balancing code left out changes to the sched_setscheduler and
    rt_mutex_setprio.
    
    This patch addresses this issue by adding methods to the schedule classes
    to handle being switched out of (switched_from) and being switched into
    (switched_to) a sched_class. Also a method for changing of priorities
    is also added (prio_changed).
    
    This patch also removes some duplicate logic between rt_mutex_setprio and
    sched_setscheduler.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c67d2c2f0111..f2044e707004 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -855,6 +855,13 @@ struct sched_class {
 
 	void (*join_domain)(struct rq *rq);
 	void (*leave_domain)(struct rq *rq);
+
+	void (*switched_from) (struct rq *this_rq, struct task_struct *task,
+			       int running);
+	void (*switched_to) (struct rq *this_rq, struct task_struct *task,
+			     int running);
+	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
+			     int oldprio, int running);
 };
 
 struct load_weight {

commit 9a897c5a6701bcb6f099f7ca20194999102729fd
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jan 25 21:08:22 2008 +0100

    sched: RT-balance, replace hooks with pre/post schedule and wakeup methods
    
    To make the main sched.c code more agnostic to the schedule classes.
    Instead of having specific hooks in the schedule code for the RT class
    balancing. They are replaced with a pre_schedule, post_schedule
    and task_wake_up methods. These methods may be used by any of the classes
    but currently, only the sched_rt class implements them.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e69f19369e4..c67d2c2f0111 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -843,6 +843,9 @@ struct sched_class {
 	int (*move_one_task) (struct rq *this_rq, int this_cpu,
 			      struct rq *busiest, struct sched_domain *sd,
 			      enum cpu_idle_type idle);
+	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
+	void (*post_schedule) (struct rq *this_rq);
+	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
 #endif
 
 	void (*set_curr_task) (struct rq *rq);

commit 57d885fea0da0e9541d7730a9e1dcf734981a173
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Fri Jan 25 21:08:18 2008 +0100

    sched: add sched-domain roots
    
    We add the notion of a root-domain which will be used later to rescope
    global variables to per-domain variables.  Each exclusive cpuset
    essentially defines an island domain by fully partitioning the member cpus
    from any other cpuset.  However, we currently still maintain some
    policy/state as global variables which transcend all cpusets.  Consider,
    for instance, rt-overload state.
    
    Whenever a new exclusive cpuset is created, we also create a new
    root-domain object and move each cpu member to the root-domain's span.
    By default the system creates a single root-domain with all cpus as
    members (mimicking the global state we have today).
    
    We add some plumbing for storing class specific data in our root-domain.
    Whenever a RQ is switching root-domains (because of repartitioning) we
    give each sched_class the opportunity to remove any state from its old
    domain and add state to the new one.  This logic doesn't have any clients
    yet but it will later in the series.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    CC: Christoph Lameter <clameter@sgi.com>
    CC: Paul Jackson <pj@sgi.com>
    CC: Simon Derr <simon.derr@bull.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6fbbf38555ac..2e69f19369e4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -849,6 +849,9 @@ struct sched_class {
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
 	void (*set_cpus_allowed)(struct task_struct *p, cpumask_t *newmask);
+
+	void (*join_domain)(struct rq *rq);
+	void (*leave_domain)(struct rq *rq);
 };
 
 struct load_weight {

commit e7693a362ec84bb5b6fd441d8a8b4b9d568a7a0c
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Fri Jan 25 21:08:09 2008 +0100

    sched: de-SCHED_OTHER-ize the RT path
    
    The current wake-up code path tries to determine if it can optimize the
    wake-up to "this_cpu" by computing load calculations.  The problem is that
    these calculations are only relevant to SCHED_OTHER tasks where load is king.
    For RT tasks, priority is king.  So the load calculation is completely wasted
    bandwidth.
    
    Therefore, we create a new sched_class interface to help with
    pre-wakeup routing decisions and move the load calculation as a function
    of CFS task's class.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b07a2cf76401..6fbbf38555ac 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -827,6 +827,7 @@ struct sched_class {
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
 	void (*yield_task) (struct rq *rq);
+	int  (*select_task_rq)(struct task_struct *p, int sync);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
 

commit 73fe6aae84400e2b475e2a1dc4e8592cd3ed6e69
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Fri Jan 25 21:08:07 2008 +0100

    sched: add RT-balance cpu-weight
    
    Some RT tasks (particularly kthreads) are bound to one specific CPU.
    It is fairly common for two or more bound tasks to get queued up at the
    same time.  Consider, for instance, softirq_timer and softirq_sched.  A
    timer goes off in an ISR which schedules softirq_thread to run at RT50.
    Then the timer handler determines that it's time to smp-rebalance the
    system so it schedules softirq_sched to run.  So we are in a situation
    where we have two RT50 tasks queued, and the system will go into
    rt-overload condition to request other CPUs for help.
    
    This causes two problems in the current code:
    
    1) If a high-priority bound task and a low-priority unbounded task queue
       up behind the running task, we will fail to ever relocate the unbounded
       task because we terminate the search on the first unmovable task.
    
    2) We spend precious futile cycles in the fast-path trying to pull
       overloaded tasks over.  It is therefore optimial to strive to avoid the
       overhead all together if we can cheaply detect the condition before
       overload even occurs.
    
    This patch tries to achieve this optimization by utilizing the hamming
    weight of the task->cpus_allowed mask.  A weight of 1 indicates that
    the task cannot be migrated.  We will then utilize this information to
    skip non-migratable tasks and to eliminate uncessary rebalance attempts.
    
    We introduce a per-rq variable to count the number of migratable tasks
    that are currently running.  We only go into overload if we have more
    than one rt task, AND at least one of them is migratable.
    
    In addition, we introduce a per-task variable to cache the cpus_allowed
    weight, since the hamming calculation is probably relatively expensive.
    We only update the cached value when the mask is updated which should be
    relatively infrequent, especially compared to scheduling frequency
    in the fast path.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0846f1f9e196..b07a2cf76401 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -847,6 +847,7 @@ struct sched_class {
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
+	void (*set_cpus_allowed)(struct task_struct *p, cpumask_t *newmask);
 };
 
 struct load_weight {
@@ -956,6 +957,7 @@ struct task_struct {
 
 	unsigned int policy;
 	cpumask_t cpus_allowed;
+	int nr_cpus_allowed;
 	unsigned int time_slice;
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)

commit 82a1fcb90287052aabfa235e7ffc693ea003fe69
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:02 2008 +0100

    softlockup: automatically detect hung TASK_UNINTERRUPTIBLE tasks
    
    this patch extends the soft-lockup detector to automatically
    detect hung TASK_UNINTERRUPTIBLE tasks. Such hung tasks are
    printed the following way:
    
     ------------------>
     INFO: task prctl:3042 blocked for more than 120 seconds.
     "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message
     prctl         D fd5e3793     0  3042   2997
            f6050f38 00000046 00000001 fd5e3793 00000009 c06d8264 c06dae80 00000286
            f6050f40 f6050f00 f7d34d90 f7d34fc8 c1e1be80 00000001 f6050000 00000000
            f7e92d00 00000286 f6050f18 c0489d1a f6050f40 00006605 00000000 c0133a5b
     Call Trace:
      [<c04883a5>] schedule_timeout+0x6d/0x8b
      [<c04883d8>] schedule_timeout_uninterruptible+0x15/0x17
      [<c0133a76>] msleep+0x10/0x16
      [<c0138974>] sys_prctl+0x30/0x1e2
      [<c0104c52>] sysenter_past_esp+0x5f/0xa5
      =======================
     2 locks held by prctl/3042:
     #0:  (&sb->s_type->i_mutex_key#5){--..}, at: [<c0197d11>] do_fsync+0x38/0x7a
     #1:  (jbd_handle){--..}, at: [<c01ca3d2>] journal_start+0xc7/0xe9
     <------------------
    
    the current default timeout is 120 seconds. Such messages are printed
    up to 10 times per bootup. If the system has crashed already then the
    messages are not printed.
    
    if lockdep is enabled then all held locks are printed as well.
    
    this feature is a natural extension to the softlockup-detector (kernel
    locked up without scheduling) and to the NMI watchdog (kernel locked up
    with IRQs disabled).
    
    [ Gautham R Shenoy <ego@in.ibm.com>: CPU hotplug fixes. ]
    [ Andrew Morton <akpm@linux-foundation.org>: build warning fix. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 288245f83bd4..0846f1f9e196 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -258,12 +258,17 @@ extern void account_process_tick(struct task_struct *task, int user);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 
+extern void sched_show_task(struct task_struct *p);
+
 #ifdef CONFIG_DETECT_SOFTLOCKUP
 extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern int softlockup_thresh;
+extern unsigned long sysctl_hung_task_check_count;
+extern unsigned long sysctl_hung_task_timeout_secs;
+extern long sysctl_hung_task_warnings;
 #else
 static inline void softlockup_tick(void)
 {
@@ -1041,6 +1046,11 @@ struct task_struct {
 /* ipc stuff */
 	struct sysv_sem sysvsem;
 #endif
+#ifdef CONFIG_DETECT_SOFTLOCKUP
+/* hung task detection */
+	unsigned long last_switch_timestamp;
+	unsigned long last_switch_count;
+#endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /* filesystem information */

commit 6b2d7700266b9402e12824e11e0099ae6a4a6a79
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Fri Jan 25 21:08:00 2008 +0100

    sched: group scheduler, fix fairness of cpu bandwidth allocation for task groups
    
    The current load balancing scheme isn't good enough for precise
    group fairness.
    
    For example: on a 8-cpu system, I created 3 groups as under:
    
            a = 8 tasks (cpu.shares = 1024)
            b = 4 tasks (cpu.shares = 1024)
            c = 3 tasks (cpu.shares = 1024)
    
    a, b and c are task groups that have equal weight. We would expect each
    of the groups to receive 33.33% of cpu bandwidth under a fair scheduler.
    
    This is what I get with the latest scheduler git tree:
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------------------
    Col1  | Col2    | Col3  |  Col4
    ------|---------|-------|-------------------------------------------------------
    a     | 277.676 | 57.8% | 54.1%  54.1%  54.1%  54.2%  56.7%  62.2%  62.8% 64.5%
    b     | 116.108 | 24.2% | 47.4%  48.1%  48.7%  49.3%
    c     |  86.326 | 18.0% | 47.5%  47.9%  48.5%
    --------------------------------------------------------------------------------
    
    Explanation of o/p:
    
    Col1 -> Group name
    Col2 -> Cumulative execution time (in seconds) received by all tasks of that
            group in a 60sec window across 8 cpus
    Col3 -> CPU bandwidth received by the group in the 60sec window, expressed in
            percentage. Col3 data is derived as:
                    Col3 = 100 * Col2 / (NR_CPUS * 60)
    Col4 -> CPU bandwidth received by each individual task of the group.
                    Col4 = 100 * cpu_time_recd_by_task / 60
    
    [I can share the test case that produces a similar o/p if reqd]
    
    The deviation from desired group fairness is as below:
    
            a = +24.47%
            b = -9.13%
            c = -15.33%
    
    which is quite high.
    
    After the patch below is applied, here are the results:
    
    --------------------------------------------------------------------------------
    Col1  | Col2    | Col3  |  Col4
    ------|---------|-------|-------------------------------------------------------
    a     | 163.112 | 34.0% | 33.2%  33.4%  33.5%  33.5%  33.7%  34.4%  34.8% 35.3%
    b     | 156.220 | 32.5% | 63.3%  64.5%  66.1%  66.5%
    c     | 160.653 | 33.5% | 85.8%  90.6%  91.4%
    --------------------------------------------------------------------------------
    
    Deviation from desired group fairness is as below:
    
            a = +0.67%
            b = -0.83%
            c = +0.17%
    
    which is far better IMO. Most of other runs have yielded a deviation within
    +-2% at the most, which is good.
    
    Why do we see bad (group) fairness with current scheuler?
    =========================================================
    
    Currently cpu's weight is just the summation of individual task weights.
    This can yield incorrect results. For ex: consider three groups as below
    on a 2-cpu system:
    
            CPU0    CPU1
    ---------------------------
            A (10)  B(5)
                    C(5)
    ---------------------------
    
    Group A has 10 tasks, all on CPU0, Group B and C have 5 tasks each all
    of which are on CPU1. Each task has the same weight (NICE_0_LOAD =
    1024).
    
    The current scheme would yield a cpu weight of 10240 (10*1024) for each cpu and
    the load balancer will think both CPUs are perfectly balanced and won't
    move around any tasks. This, however, would yield this bandwidth:
    
            A = 50%
            B = 25%
            C = 25%
    
    which is not the desired result.
    
    What's changing in the patch?
    =============================
    
            - How cpu weights are calculated when CONFIF_FAIR_GROUP_SCHED is
              defined (see below)
            - API Change
                    - Two tunables introduced in sysfs (under SCHED_DEBUG) to
                      control the frequency at which the load balance monitor
                      thread runs.
    
    The basic change made in this patch is how cpu weight (rq->load.weight) is
    calculated. Its now calculated as the summation of group weights on a cpu,
    rather than summation of task weights. Weight exerted by a group on a
    cpu is dependent on the shares allocated to it and also the number of
    tasks the group has on that cpu compared to the total number of
    (runnable) tasks the group has in the system.
    
    Let,
            W(K,i)  = Weight of group K on cpu i
            T(K,i)  = Task load present in group K's cfs_rq on cpu i
            T(K)    = Total task load of group K across various cpus
            S(K)    = Shares allocated to group K
            NRCPUS  = Number of online cpus in the scheduler domain to
                      which group K is assigned.
    
    Then,
            W(K,i) = S(K) * NRCPUS * T(K,i) / T(K)
    
    A load balance monitor thread is created at bootup, which periodically
    runs and adjusts group's weight on each cpu. To avoid its overhead, two
    min/max tunables are introduced (under SCHED_DEBUG) to control the rate
    at which it runs.
    
    Fixes from: Peter Zijlstra <a.p.zijlstra@chello.nl>
    
    - don't start the load_balance_monitor when there is only a single cpu.
    - rename the kthread because its currently longer than TASK_COMM_LEN
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d6eacda765ca..288245f83bd4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1453,6 +1453,10 @@ extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
+#if defined(CONFIG_FAIR_GROUP_SCHED) && defined(CONFIG_SMP)
+extern unsigned int sysctl_sched_min_bal_int_shares;
+extern unsigned int sysctl_sched_max_bal_int_shares;
+#endif
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit eb41d9465cdafee45e0cb30f3b7338646221908e
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Fri Nov 2 13:47:53 2007 +0100

    fix struct user_info export's sysfs interaction
    
    Clean up the use of ksets and kobjects. Kobjects are instances of
    objects (like struct user_info), ksets are collections of objects of a
    similar type (like the uids directory containing the user_info directories).
    So, use kobjects for the user_info directories, and a kset for the "uids"
    directory.
    
    On object cleanup, the final kobject_put() was missing.
    
    Cc: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cc14656f8682..d6eacda765ca 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -552,18 +552,13 @@ struct user_struct {
 #ifdef CONFIG_FAIR_USER_SCHED
 	struct task_group *tg;
 #ifdef CONFIG_SYSFS
-	struct kset kset;
-	struct subsys_attribute user_attr;
+	struct kobject kobj;
 	struct work_struct work;
 #endif
 #endif
 };
 
-#ifdef CONFIG_FAIR_USER_SCHED
-extern int uids_kobject_init(void);
-#else
-static inline int uids_kobject_init(void) { return 0; }
-#endif
+extern int uids_sysfs_init(void);
 
 extern struct user_struct *find_user(uid_t);
 

commit 84427eaef1fb91704c7112bdb598c810003b99f3
Author: Roland McGrath <roland@redhat.com>
Date:   Thu Jan 10 12:52:04 2008 -0800

    remove task_ppid_nr_ns
    
    task_ppid_nr_ns is called in three places.  One of these should never
    have called it.  In the other two, using it broke the existing
    semantics.  This was presumably accidental.  If the function had not
    been there, it would have been much more obvious to the eye that those
    patches were changing the behavior.  We don't need this function.
    
    In task_state, the pid of the ptracer is not the ppid of the ptracer.
    
    In do_task_stat, ppid is the tgid of the real_parent, not its pid.
    I also moved the call outside of lock_task_sighand, since it doesn't
    need it.
    
    In sys_getppid, ppid is the tgid of the real_parent, not its pid.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ac3d496fbd20..cc14656f8682 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1255,13 +1255,6 @@ struct pid_namespace;
  *
  * set_task_vxid()   : assigns a virtual id to a task;
  *
- * task_ppid_nr_ns() : the parent's id as seen from the namespace specified.
- *                     the result depends on the namespace and whether the
- *                     task in question is the namespace's init. e.g. for the
- *                     namespace's init this will return 0 when called from
- *                     the namespace of this init, or appropriate id otherwise.
- *
- *
  * see also pid_nr() etc in include/linux/pid.h
  */
 
@@ -1317,12 +1310,6 @@ static inline pid_t task_session_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_ppid_nr_ns(struct task_struct *tsk,
-		struct pid_namespace *ns)
-{
-	return pid_nr_ns(task_pid(rcu_dereference(tsk->real_parent)), ns);
-}
-
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.

commit 294d5cc233d81ec4aec77ebc60dc5752a3d0082a
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Thu Dec 6 11:59:46 2007 -0500

    Add schedule_timeout_killable
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 95395c143bab..e4921aad4063 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -314,6 +314,7 @@ extern int in_sched_functions(unsigned long addr);
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long FASTCALL(schedule_timeout(signed long timeout));
 extern signed long schedule_timeout_interruptible(signed long timeout);
+extern signed long schedule_timeout_killable(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 

commit f776d12dd16da1b0cd55a1240002c1b31f315d5d
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Thu Dec 6 11:15:50 2007 -0500

    Add fatal_signal_pending
    
    Like signal_pending, but it's only true for signals which are fatal to
    this process
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 70d87f2fd23e..95395c143bab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1872,7 +1872,14 @@ static inline int signal_pending(struct task_struct *p)
 {
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
 }
-  
+
+extern int FASTCALL(__fatal_signal_pending(struct task_struct *p));
+
+static inline int fatal_signal_pending(struct task_struct *p)
+{
+	return signal_pending(p) && __fatal_signal_pending(p);
+}
+
 static inline int need_resched(void)
 {
 	return unlikely(test_thread_flag(TIF_NEED_RESCHED));

commit f021a3c2b14d0dd082c2cee890c204d9e1dee52b
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Thu Dec 6 11:13:16 2007 -0500

    Add TASK_WAKEKILL
    
    Set TASK_WAKEKILL for TASK_STOPPED and TASK_TRACED, add TASK_KILLABLE and
    use TASK_WAKEKILL in signal_wake_up()
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 69233c7fe28d..70d87f2fd23e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -170,27 +170,33 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define TASK_RUNNING		0
 #define TASK_INTERRUPTIBLE	1
 #define TASK_UNINTERRUPTIBLE	2
-#define TASK_STOPPED		4
-#define TASK_TRACED		8
+#define __TASK_STOPPED		4
+#define __TASK_TRACED		8
 /* in tsk->exit_state */
 #define EXIT_ZOMBIE		16
 #define EXIT_DEAD		32
 /* in tsk->state again */
 #define TASK_DEAD		64
+#define TASK_WAKEKILL		128
+
+/* Convenience macros for the sake of set_task_state */
+#define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
+#define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
+#define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)
 
 /* Convenience macros for the sake of wake_up */
 #define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
-#define TASK_ALL		(TASK_NORMAL | TASK_STOPPED | TASK_TRACED)
+#define TASK_ALL		(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
 
 /* get_task_state() */
 #define TASK_REPORT		(TASK_RUNNING | TASK_INTERRUPTIBLE | \
-				 TASK_UNINTERRUPTIBLE | TASK_STOPPED | \
-				 TASK_TRACED)
+				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
+				 __TASK_TRACED)
 
-#define task_is_traced(task)	((task->state & TASK_TRACED) != 0)
-#define task_is_stopped(task)	((task->state & TASK_STOPPED) != 0)
+#define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
+#define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
 #define task_is_stopped_or_traced(task)	\
-			((task->state & (TASK_STOPPED | TASK_TRACED)) != 0)
+			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task->state & TASK_UNINTERRUPTIBLE) != 0)
 

commit 92a1f4bc7af13949d2185449307088cf98b4755d
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Thu Dec 6 10:55:25 2007 -0500

    Add macros to replace direct uses of TASK_ flags
    
    With the changes to support TASK_KILLABLE, ->state becomes a bitmask, and
    moving these tests to convenience macros will fix all the users.
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ac3d496fbd20..69233c7fe28d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -178,6 +178,22 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 /* in tsk->state again */
 #define TASK_DEAD		64
 
+/* Convenience macros for the sake of wake_up */
+#define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
+#define TASK_ALL		(TASK_NORMAL | TASK_STOPPED | TASK_TRACED)
+
+/* get_task_state() */
+#define TASK_REPORT		(TASK_RUNNING | TASK_INTERRUPTIBLE | \
+				 TASK_UNINTERRUPTIBLE | TASK_STOPPED | \
+				 TASK_TRACED)
+
+#define task_is_traced(task)	((task->state & TASK_TRACED) != 0)
+#define task_is_stopped(task)	((task->state & TASK_STOPPED) != 0)
+#define task_is_stopped_or_traced(task)	\
+			((task->state & (TASK_STOPPED | TASK_TRACED)) != 0)
+#define task_contributes_to_load(task)	\
+				((task->state & TASK_UNINTERRUPTIBLE) != 0)
+
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
 #define set_task_state(tsk, state_value)		\

commit deaf2227ddf657a260e923db44b6f0974d9bb782
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 28 15:52:56 2007 +0100

    sched: clean up, move __sched_text_start/end to sched.h
    
    move __sched_text_start/end to sched.h. No code changed:
    
       text    data     bss     dec     hex filename
      26582    2310      28   28920    70f8 sched.o.before
      26582    2310      28   28920    70f8 sched.o.after
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ee800e7a70de..ac3d496fbd20 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -282,6 +282,10 @@ static inline void touch_all_softlockup_watchdogs(void)
 
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
+
+/* Linker adds these: start and end of __sched functions */
+extern char __sched_text_start[], __sched_text_end[];
+
 /* Is this address in the __sched functions? */
 extern int in_sched_functions(unsigned long addr);
 

commit e6fe6649b4ec11aa3075e394b4d8743eebe1f64c
Author: Adrian Bunk <bunk@kernel.org>
Date:   Fri Nov 9 22:39:39 2007 +0100

    sched: proper prototype for kernel/sched.c:migration_init()
    
    This patch adds a proper prototype for migration_init() in
    include/linux/sched.h
    
    Since there's no point in always returning 0 to a caller that doesn't check
    the return value it also changes the function to return void.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2cc789fef711..ee800e7a70de 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1988,6 +1988,14 @@ static inline void inc_syscw(struct task_struct *tsk)
 }
 #endif
 
+#ifdef CONFIG_SMP
+void migration_init(void);
+#else
+static inline void migration_init(void)
+{
+}
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif

commit b82d9fdd848abfbe7263a4ecd9bbb55e575100a6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 9 22:39:39 2007 +0100

    sched: avoid large irq-latencies in smp-balancing
    
    SMP balancing is done with IRQs disabled and can iterate the full rq.
    When rqs are large this can cause large irq-latencies. Limit the nr of
    iterations on each run.
    
    This fixes a scheduling latency regression reported by the -rt folks.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Tested-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 93fd30d6dac4..2cc789fef711 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1466,6 +1466,7 @@ extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
+extern unsigned int sysctl_sched_nr_migrate;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,

commit 3e3e13f399ac8060a20d14d210a28dc02dda372e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Nov 9 22:39:39 2007 +0100

    sched: remove PREEMPT_RESTRICT
    
    remove PREEMPT_RESTRICT. (this is a separate commit so that any
    regression related to the removal itself is bisectable)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 951759e30c09..93fd30d6dac4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -863,7 +863,6 @@ struct sched_entity {
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;
 	unsigned int		on_rq;
-	int			peer_preempt;
 
 	u64			exec_start;
 	u64			sum_exec_runtime;

commit fa13a5a1f25f671d084d8884be96fc48d9b68275
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 9 22:39:38 2007 +0100

    sched: restore deterministic CPU accounting on powerpc
    
    Since powerpc started using CONFIG_GENERIC_CLOCKEVENTS, the
    deterministic CPU accounting (CONFIG_VIRT_CPU_ACCOUNTING) has been
    broken on powerpc, because we end up counting user time twice: once in
    timer_interrupt() and once in update_process_times().
    
    This fixes the problem by pulling the code in update_process_times
    that updates utime and stime into a separate function called
    account_process_tick.  If CONFIG_VIRT_CPU_ACCOUNTING is not defined,
    there is a version of account_process_tick in kernel/timer.c that
    simply accounts a whole tick to either utime or stime as before.  If
    CONFIG_VIRT_CPU_ACCOUNTING is defined, then arch code gets to
    implement account_process_tick.
    
    This also lets us simplify the s390 code a bit; it means that the s390
    timer interrupt can now call update_process_times even when
    CONFIG_VIRT_CPU_ACCOUNTING is turned on, and can just implement a
    suitable account_process_tick().
    
    account_process_tick() now takes the task_struct * as an argument.
    Tested both with and without CONFIG_VIRT_CPU_ACCOUNTING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5457b6234e11..951759e30c09 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -254,6 +254,7 @@ long io_schedule_timeout(long timeout);
 
 extern void cpu_init (void);
 extern void trap_init(void);
+extern void account_process_tick(struct task_struct *task, int user);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 

commit b2be5e96dc0b5a179cf4cb98e65cfb605752ca26
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Nov 9 22:39:37 2007 +0100

    sched: reintroduce the sched_min_granularity tunable
    
    we lost the sched_min_granularity tunable to a clever optimization
    that uses the sched_latency/min_granularity ratio - but the ratio
    is quite unintuitive to users and can also crash the kernel if the
    ratio is set to 0. So reintroduce the min_granularity tunable,
    while keeping the ratio maintained internally.
    
    no functionality changed.
    
    [ mingo@elte.hu: some fixlets. ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 155d7438f7ad..5457b6234e11 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1460,12 +1460,16 @@ extern void sched_idle_next(void);
 
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
-extern unsigned int sysctl_sched_nr_latency;
+extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
+
+int sched_nr_latency_handler(struct ctl_table *table, int write,
+		struct file *file, void __user *buffer, size_t *length,
+		loff_t *ppos);
 #endif
 
 extern unsigned int sysctl_sched_compat_yield;

commit 9301899be75b464ef097f0b5af7af6d9bd8f68a7
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Tue Oct 30 00:26:32 2007 +0100

    sched: fix /proc/<PID>/stat stime/utime monotonicity, part 2
    
    Extend Peter's patch to fix accounting issues, by keeping stime
    monotonic too.
    
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Tested-by: Frans Pop <elendil@planet.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0b1fe6e0b17..155d7438f7ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1009,7 +1009,7 @@ struct task_struct {
 	unsigned int rt_priority;
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
-	cputime_t prev_utime;
+	cputime_t prev_utime, prev_stime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
 	struct timespec real_start_time;	/* boot based time */

commit 73a2bcb0edb9ffb0b007b3546b430e2c6e415eee
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Oct 29 21:18:11 2007 +0100

    sched: keep utime/stime monotonic
    
    keep utime/stime monotonic.
    
    cpustats use utime/stime as a ratio against sum_exec_runtime, as a
    consequence it can happen - when the ratio changes faster than time
    accumulates - that either can be appear to go backwards.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3c07d595979f..b0b1fe6e0b17 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1009,6 +1009,7 @@ struct task_struct {
 	unsigned int rt_priority;
 	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
+	cputime_t prev_utime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
 	struct timespec real_start_time;	/* boot based time */

commit e868171a94b637158a3930c9adfb448d0df163cd
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Fri Oct 26 12:17:22 2007 +0400

    De-constify sched.h
    
    [PATCH] De-constify sched.h
    
    This reverts commit a8972ccf00b7184a743eb6cd9bc7f3443357910c ("sched:
    constify sched.h")
    
     1) Patch doesn't change any code here, so gcc is already smart enough
        to "feel" constness in such simple functions.
     2) There is no such thing as const task_struct.  Anyone who think
        otherwise deserves compiler warning.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 24e08d1d900d..3c07d595979f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1201,7 +1201,7 @@ static inline int rt_prio(int prio)
 	return 0;
 }
 
-static inline int rt_task(const struct task_struct *p)
+static inline int rt_task(struct task_struct *p)
 {
 	return rt_prio(p->prio);
 }
@@ -1216,22 +1216,22 @@ static inline void set_task_pgrp(struct task_struct *tsk, pid_t pgrp)
 	tsk->signal->__pgrp = pgrp;
 }
 
-static inline struct pid *task_pid(const struct task_struct *task)
+static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
 }
 
-static inline struct pid *task_tgid(const struct task_struct *task)
+static inline struct pid *task_tgid(struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_PID].pid;
 }
 
-static inline struct pid *task_pgrp(const struct task_struct *task)
+static inline struct pid *task_pgrp(struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_PGID].pid;
 }
 
-static inline struct pid *task_session(const struct task_struct *task)
+static inline struct pid *task_session(struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_SID].pid;
 }
@@ -1260,7 +1260,7 @@ struct pid_namespace;
  * see also pid_nr() etc in include/linux/pid.h
  */
 
-static inline pid_t task_pid_nr(const struct task_struct *tsk)
+static inline pid_t task_pid_nr(struct task_struct *tsk)
 {
 	return tsk->pid;
 }
@@ -1273,7 +1273,7 @@ static inline pid_t task_pid_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_tgid_nr(const struct task_struct *tsk)
+static inline pid_t task_tgid_nr(struct task_struct *tsk)
 {
 	return tsk->tgid;
 }
@@ -1286,7 +1286,7 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_pgrp_nr(const struct task_struct *tsk)
+static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {
 	return tsk->signal->__pgrp;
 }
@@ -1299,7 +1299,7 @@ static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_session_nr(const struct task_struct *tsk)
+static inline pid_t task_session_nr(struct task_struct *tsk)
 {
 	return tsk->signal->__session;
 }
@@ -1326,7 +1326,7 @@ static inline pid_t task_ppid_nr_ns(struct task_struct *tsk,
  * If pid_alive fails, then pointers within the task structure
  * can be stale and must not be dereferenced.
  */
-static inline int pid_alive(const struct task_struct *p)
+static inline int pid_alive(struct task_struct *p)
 {
 	return p->pids[PIDTYPE_PID].pid != NULL;
 }
@@ -1337,7 +1337,7 @@ static inline int pid_alive(const struct task_struct *p)
  *
  * Check if a task structure is the first user space task the kernel created.
  */
-static inline int is_global_init(const struct task_struct *tsk)
+static inline int is_global_init(struct task_struct *tsk)
 {
 	return tsk->pid == 1;
 }
@@ -1474,7 +1474,7 @@ extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
 #else
-static inline int rt_mutex_getprio(const struct task_struct *p)
+static inline int rt_mutex_getprio(struct task_struct *p)
 {
 	return p->normal_prio;
 }
@@ -1726,7 +1726,7 @@ extern void wait_task_inactive(struct task_struct * p);
  * all we care about is that we have a task with the appropriate
  * pid, we don't actually care if we have the right task.
  */
-static inline int has_group_leader_pid(const struct task_struct *p)
+static inline int has_group_leader_pid(struct task_struct *p)
 {
 	return p->pid == p->tgid;
 }
@@ -1743,7 +1743,7 @@ static inline struct task_struct *next_thread(const struct task_struct *p)
 			  struct task_struct, thread_group);
 }
 
-static inline int thread_group_empty(const struct task_struct *p)
+static inline int thread_group_empty(struct task_struct *p)
 {
 	return list_empty(&p->thread_group);
 }

commit 681f3e68541d6f03e3e05d21fe15093578b8b539
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Wed Oct 24 18:23:51 2007 +0200

    sched: isolate SMP balancing code a bit more
    
    At the moment, a lot of load balancing code that is irrelevant to non
    SMP systems gets included during non SMP builds.
    
    This patch addresses this issue and reduces the binary size on non
    SMP systems:
    
       text    data     bss     dec     hex filename
      10983      28    1192   12203    2fab sched.o.before
      10739      28    1192   11959    2eb7 sched.o.after
    
    Signed-off-by: Peter Williams <pwil3058@bigpond.net.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 639241f4f3d1..24e08d1d900d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -828,6 +828,7 @@ struct sched_class {
 	struct task_struct * (*pick_next_task) (struct rq *rq);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
+#ifdef CONFIG_SMP
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest, unsigned long max_load_move,
 			struct sched_domain *sd, enum cpu_idle_type idle,
@@ -836,6 +837,7 @@ struct sched_class {
 	int (*move_one_task) (struct rq *this_rq, int this_cpu,
 			      struct rq *busiest, struct sched_domain *sd,
 			      enum cpu_idle_type idle);
+#endif
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);

commit e1d1484f72127a5580d37c379f6a5b2c2786434c
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Wed Oct 24 18:23:51 2007 +0200

    sched: reduce balance-tasks overhead
    
    At the moment, balance_tasks() provides low level functionality for both
      move_tasks() and move_one_task() (indirectly) via the load_balance()
    function (in the sched_class interface) which also provides dual
    functionality.  This dual functionality complicates the interfaces and
    internal mechanisms and makes the run time overhead of operations that
    are called with two run queue locks held.
    
    This patch addresses this issue and reduces the overhead of these
    operations.
    
    Signed-off-by: Peter Williams <pwil3058@bigpond.net.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 52288a647692..639241f4f3d1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -829,11 +829,14 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
-			struct rq *busiest,
-			unsigned long max_nr_move, unsigned long max_load_move,
+			struct rq *busiest, unsigned long max_load_move,
 			struct sched_domain *sd, enum cpu_idle_type idle,
 			int *all_pinned, int *this_best_prio);
 
+	int (*move_one_task) (struct rq *this_rq, int this_cpu,
+			      struct rq *busiest, struct sched_domain *sd,
+			      enum cpu_idle_type idle);
+
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
 	void (*task_new) (struct rq *rq, struct task_struct *p);

commit a8972ccf00b7184a743eb6cd9bc7f3443357910c
Author: Joe Perches <joe@perches.com>
Date:   Wed Oct 24 18:23:50 2007 +0200

    sched: constify sched.h
    
    Add const to some struct task_struct * uses
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 13df99fb2769..52288a647692 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1196,7 +1196,7 @@ static inline int rt_prio(int prio)
 	return 0;
 }
 
-static inline int rt_task(struct task_struct *p)
+static inline int rt_task(const struct task_struct *p)
 {
 	return rt_prio(p->prio);
 }
@@ -1211,22 +1211,22 @@ static inline void set_task_pgrp(struct task_struct *tsk, pid_t pgrp)
 	tsk->signal->__pgrp = pgrp;
 }
 
-static inline struct pid *task_pid(struct task_struct *task)
+static inline struct pid *task_pid(const struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
 }
 
-static inline struct pid *task_tgid(struct task_struct *task)
+static inline struct pid *task_tgid(const struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_PID].pid;
 }
 
-static inline struct pid *task_pgrp(struct task_struct *task)
+static inline struct pid *task_pgrp(const struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_PGID].pid;
 }
 
-static inline struct pid *task_session(struct task_struct *task)
+static inline struct pid *task_session(const struct task_struct *task)
 {
 	return task->group_leader->pids[PIDTYPE_SID].pid;
 }
@@ -1255,7 +1255,7 @@ struct pid_namespace;
  * see also pid_nr() etc in include/linux/pid.h
  */
 
-static inline pid_t task_pid_nr(struct task_struct *tsk)
+static inline pid_t task_pid_nr(const struct task_struct *tsk)
 {
 	return tsk->pid;
 }
@@ -1268,7 +1268,7 @@ static inline pid_t task_pid_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_tgid_nr(struct task_struct *tsk)
+static inline pid_t task_tgid_nr(const struct task_struct *tsk)
 {
 	return tsk->tgid;
 }
@@ -1281,7 +1281,7 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_pgrp_nr(struct task_struct *tsk)
+static inline pid_t task_pgrp_nr(const struct task_struct *tsk)
 {
 	return tsk->signal->__pgrp;
 }
@@ -1294,7 +1294,7 @@ static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 }
 
 
-static inline pid_t task_session_nr(struct task_struct *tsk)
+static inline pid_t task_session_nr(const struct task_struct *tsk)
 {
 	return tsk->signal->__session;
 }
@@ -1321,7 +1321,7 @@ static inline pid_t task_ppid_nr_ns(struct task_struct *tsk,
  * If pid_alive fails, then pointers within the task structure
  * can be stale and must not be dereferenced.
  */
-static inline int pid_alive(struct task_struct *p)
+static inline int pid_alive(const struct task_struct *p)
 {
 	return p->pids[PIDTYPE_PID].pid != NULL;
 }
@@ -1332,7 +1332,7 @@ static inline int pid_alive(struct task_struct *p)
  *
  * Check if a task structure is the first user space task the kernel created.
  */
-static inline int is_global_init(struct task_struct *tsk)
+static inline int is_global_init(const struct task_struct *tsk)
 {
 	return tsk->pid == 1;
 }
@@ -1469,7 +1469,7 @@ extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
 #else
-static inline int rt_mutex_getprio(struct task_struct *p)
+static inline int rt_mutex_getprio(const struct task_struct *p)
 {
 	return p->normal_prio;
 }
@@ -1721,7 +1721,7 @@ extern void wait_task_inactive(struct task_struct * p);
  * all we care about is that we have a task with the appropriate
  * pid, we don't actually care if we have the right task.
  */
-static inline int has_group_leader_pid(struct task_struct *p)
+static inline int has_group_leader_pid(const struct task_struct *p)
 {
 	return p->pid == p->tgid;
 }
@@ -1738,7 +1738,7 @@ static inline struct task_struct *next_thread(const struct task_struct *p)
 			  struct task_struct, thread_group);
 }
 
-static inline int thread_group_empty(struct task_struct *p)
+static inline int thread_group_empty(const struct task_struct *p)
 {
 	return list_empty(&p->thread_group);
 }

commit 9a2e70572e94e21e7ec4186702d045415422bda0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:39 2007 -0700

    Isolate the explicit usage of signal->pgrp
    
    The pgrp field is not used widely around the kernel so it is now marked as
    deprecated with appropriate comment.
    
    The initialization of INIT_SIGNALS is trimmed because
    a) they are set to 0 automatically;
    b) gcc cannot properly initialize two anonymous (the second one
       is the one with the session) unions. In this particular case
       to make it compile we'd have to add some field initialized
       right before the .pgrp.
    
    This is the same patch as the 1ec320afdc9552c92191d5f89fcd1ebe588334ca one
    (from Cedric), but for the pgrp field.
    
    Some progress report:
    
    We have to deprecate the pid, tgid, session and pgrp fields on struct
    task_struct and struct signal_struct.  The session and pgrp are already
    deprecated.  The tgid value is close to being such - the worst known usage
    in in fs/locks.c and audit code.  The pid field deprecation is mainly
    blocked by numerous printk-s around the kernel that print the tsk->pid to
    log.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4bbbe12880d7..13df99fb2769 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -429,7 +429,17 @@ struct signal_struct {
 	cputime_t it_prof_incr, it_virt_incr;
 
 	/* job control IDs */
-	pid_t pgrp;
+
+	/*
+	 * pgrp and session fields are deprecated.
+	 * use the task_session_Xnr and task_pgrp_Xnr routines below
+	 */
+
+	union {
+		pid_t pgrp __deprecated;
+		pid_t __pgrp;
+	};
+
 	struct pid *tty_old_pgrp;
 
 	union {
@@ -1196,6 +1206,11 @@ static inline void set_task_session(struct task_struct *tsk, pid_t session)
 	tsk->signal->__session = session;
 }
 
+static inline void set_task_pgrp(struct task_struct *tsk, pid_t pgrp)
+{
+	tsk->signal->__pgrp = pgrp;
+}
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -1268,7 +1283,7 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 
 static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {
-	return tsk->signal->pgrp;
+	return tsk->signal->__pgrp;
 }
 
 pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);

commit 029190c515f15f512ac85de8fc686d4dbd0ae731
Author: Paul Jackson <pj@sgi.com>
Date:   Thu Oct 18 23:40:20 2007 -0700

    cpuset sched_load_balance flag
    
    Add a new per-cpuset flag called 'sched_load_balance'.
    
    When enabled in a cpuset (the default value) it tells the kernel scheduler
    that the scheduler should provide the normal load balancing on the CPUs in
    that cpuset, sometimes moving tasks from one CPU to a second CPU if the
    second CPU is less loaded and if that task is allowed to run there.
    
    When disabled (write "0" to the file) then it tells the kernel scheduler
    that load balancing is not required for the CPUs in that cpuset.
    
    Now even if this flag is disabled for some cpuset, the kernel may still
    have to load balance some or all the CPUs in that cpuset, if some
    overlapping cpuset has its sched_load_balance flag enabled.
    
    If there are some CPUs that are not in any cpuset whose sched_load_balance
    flag is enabled, the kernel scheduler will not load balance tasks to those
    CPUs.
    
    Moreover the kernel will partition the 'sched domains' (non-overlapping
    sets of CPUs over which load balancing is attempted) into the finest
    granularity partition that it can find, while still keeping any two CPUs
    that are in the same shed_load_balance enabled cpuset in the same element
    of the partition.
    
    This serves two purposes:
     1) It provides a mechanism for real time isolation of some CPUs, and
     2) it can be used to improve performance on systems with many CPUs
        by supporting configurations in which load balancing is not done
        across all CPUs at once, but rather only done in several smaller
        disjoint sets of CPUs.
    
    This mechanism replaces the earlier overloading of the per-cpuset
    flag 'cpu_exclusive', which overloading was removed in an earlier
    patch: cpuset-remove-sched-domain-hooks-from-cpusets
    
    See further the Documentation and comments in the code itself.
    
    [akpm@linux-foundation.org: don't be weird]
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cbd8731a66e6..4bbbe12880d7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -737,6 +737,8 @@ struct sched_domain {
 #endif
 };
 
+extern void partition_sched_domains(int ndoms_new, cpumask_t *doms_new);
+
 #endif	/* CONFIG_SMP */
 
 /*

commit 2f2a3a46fcafa7a12d61454f67f932dfe7d84c60
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:19 2007 -0700

    Uninline the task_xid_nr_ns() calls
    
    Since these are expanded into call to pid_nr_ns() anyway, it's OK to move
    the whole routine out-of-line.  This is a cheap way to save ~100 bytes from
    vmlinux.  Together with the previous two patches, it saves half-a-kilo from
    the vmlinux.
    
    Un-inline other (currently inlined) functions must be done with additional
    performance testing.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eb2ae68804aa..cbd8731a66e6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1243,11 +1243,7 @@ static inline pid_t task_pid_nr(struct task_struct *tsk)
 	return tsk->pid;
 }
 
-static inline pid_t task_pid_nr_ns(struct task_struct *tsk,
-		struct pid_namespace *ns)
-{
-	return pid_nr_ns(task_pid(tsk), ns);
-}
+pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
 
 static inline pid_t task_pid_vnr(struct task_struct *tsk)
 {
@@ -1260,11 +1256,7 @@ static inline pid_t task_tgid_nr(struct task_struct *tsk)
 	return tsk->tgid;
 }
 
-static inline pid_t task_tgid_nr_ns(struct task_struct *tsk,
-		struct pid_namespace *ns)
-{
-	return pid_nr_ns(task_tgid(tsk), ns);
-}
+pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
 
 static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 {
@@ -1277,11 +1269,7 @@ static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 	return tsk->signal->pgrp;
 }
 
-static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
-		struct pid_namespace *ns)
-{
-	return pid_nr_ns(task_pgrp(tsk), ns);
-}
+pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
 
 static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 {
@@ -1294,11 +1282,7 @@ static inline pid_t task_session_nr(struct task_struct *tsk)
 	return tsk->signal->__session;
 }
 
-static inline pid_t task_session_nr_ns(struct task_struct *tsk,
-		struct pid_namespace *ns)
-{
-	return pid_nr_ns(task_session(tsk), ns);
-}
+pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);
 
 static inline pid_t task_session_vnr(struct task_struct *tsk)
 {

commit bac0abd6174e427404dd197cdbefece31e97329b
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:18 2007 -0700

    Isolate some explicit usage of task->tgid
    
    With pid namespaces this field is now dangerous to use explicitly, so hide
    it behind the helpers.
    
    Also the pid and pgrp fields o task_struct and signal_struct are to be
    deprecated.  Unfortunately this patch cannot be sent right now as this
    leads to tons of warnings, so start isolating them, and deprecate later.
    
    Actually the p->tgid == pid has to be changed to has_group_leader_pid(),
    but Oleg pointed out that in case of posix cpu timers this is the same, and
    thread_group_leader() is more preferable.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f4d969e85612..eb2ae68804aa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1725,6 +1725,12 @@ static inline int has_group_leader_pid(struct task_struct *p)
 	return p->pid == p->tgid;
 }
 
+static inline
+int same_thread_group(struct task_struct *p1, struct task_struct *p2)
+{
+	return p1->tgid == p2->tgid;
+}
+
 static inline struct task_struct *next_thread(const struct task_struct *p)
 {
 	return list_entry(rcu_dereference(p->thread_group.next),

commit 228ebcbe634a30aec35132ea4375721bcc41bec0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:16 2007 -0700

    Uninline find_task_by_xxx set of functions
    
    The find_task_by_something is a set of macros are used to find task by pid
    depending on what kind of pid is proposed - global or virtual one.  All of
    them are wrappers above the most generic one - find_task_by_pid_type_ns() -
    and just substitute some args for it.
    
    It turned out, that dereferencing the current->nsproxy->pid_ns construction
    and pushing one more argument on the stack inline cause kernel text size to
    grow.
    
    This patch moves all this stuff out-of-line into kernel/pid.c.  Together
    with the next patch it saves a bit less than 400 bytes from the .text
    section.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1301c0875370..f4d969e85612 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1523,9 +1523,8 @@ extern struct pid_namespace init_pid_ns;
  *      type and namespace specified
  * find_task_by_pid_ns():
  *      finds a task by its pid in the specified namespace
- * find_task_by_pid_type():
- *      finds a task by its global id with the specified type, e.g.
- *      by global session id
+ * find_task_by_vpid():
+ *      finds a task by its virtual pid
  * find_task_by_pid():
  *      finds a task by its global pid
  *
@@ -1535,12 +1534,10 @@ extern struct pid_namespace init_pid_ns;
 extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,
 		struct pid_namespace *ns);
 
-#define find_task_by_pid_ns(nr, ns)	\
-		find_task_by_pid_type_ns(PIDTYPE_PID, nr, ns)
-#define find_task_by_pid_type(type, nr)	\
-		find_task_by_pid_type_ns(type, nr, &init_pid_ns)
-#define find_task_by_pid(nr)		\
-		find_task_by_pid_type(PIDTYPE_PID, nr)
+extern struct task_struct *find_task_by_pid(pid_t nr);
+extern struct task_struct *find_task_by_vpid(pid_t nr);
+extern struct task_struct *find_task_by_pid_ns(pid_t nr,
+		struct pid_namespace *ns);
 
 extern void __set_special_pids(pid_t session, pid_t pgrp);
 

commit 30e49c263e36341b60b735cbef5ca37912549264
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:10 2007 -0700

    pid namespaces: allow cloning of new namespace
    
    When clone() is invoked with CLONE_NEWPID, create a new pid namespace and then
    create a new struct pid for the new process.  Allocate pid_t's for the new
    process in the new pid namespace and all ancestor pid namespaces.  Make the
    newly cloned process the session and process group leader.
    
    Since the active pid namespace is special and expected to be the first entry
    in pid->upid_list, preserve the order of pid namespaces.
    
    The size of 'struct pid' is dependent on the the number of pid namespaces the
    process exists in, so we use multiple pid-caches'.  Only one pid cache is
    created during system startup and this used by processes that exist only in
    init_pid_ns.
    
    When a process clones its pid namespace, we create additional pid caches as
    necessary and use the pid cache to allocate 'struct pids' for that depth.
    
    Note, that with this patch the newly created namespace won't work, since the
    rest of the kernel still uses global pids, but this is to be fixed soon.  Init
    pid namespace still works.
    
    [oleg@tv-sign.ru: merge fix]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0bf326143a9..1301c0875370 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -25,6 +25,7 @@
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 #define CLONE_NEWIPC		0x08000000	/* New ipcs */
 #define CLONE_NEWUSER		0x10000000	/* New user namespace */
+#define CLONE_NEWPID		0x20000000	/* New pid namespace */
 #define CLONE_NEWNET		0x40000000	/* New network namespace */
 
 /*

commit b461cc03828c743aed6b3855b9ab0d39a9d54ec5
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:09 2007 -0700

    pid namespaces: miscellaneous preparations for pid namespaces
    
    * remove pid.h from pid_namespaces.h;
    * rework is_(cgroup|global)_init;
    * optimize (get|put)_pid_ns for init_pid_ns;
    * declare task_child_reaper to return actual reaper.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 511cc4549f58..b0bf326143a9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1329,19 +1329,17 @@ static inline int pid_alive(struct task_struct *p)
  * @tsk: Task structure to be checked.
  *
  * Check if a task structure is the first user space task the kernel created.
- *
- * TODO: We should inline this function after some cleanups in pid_namespace.h
  */
-extern int is_global_init(struct task_struct *tsk);
+static inline int is_global_init(struct task_struct *tsk)
+{
+	return tsk->pid == 1;
+}
 
 /*
  * is_container_init:
  * check whether in the task is init in its own pid namespace.
  */
-static inline int is_container_init(struct task_struct *tsk)
-{
-	return tsk->pid == 1;
-}
+extern int is_container_init(struct task_struct *tsk);
 
 extern struct pid *cad_pid;
 

commit 198fe21b0a17fe9c68cb519ecc566534b04f122b
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:06 2007 -0700

    pid namespaces: helpers to find the task by its numerical ids
    
    When searching the task by numerical id on may need to find it using global
    pid (as it is done now in kernel) or by its virtual id, e.g.  when sending a
    signal to a task from one namespace the sender will specify the task's virtual
    id and we should find the task by this value.
    
    [akpm@linux-foundation.org: fix gfs2 linkage]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 77e8cad3b17a..511cc4549f58 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1514,8 +1514,35 @@ extern struct task_struct init_task;
 
 extern struct   mm_struct init_mm;
 
-#define find_task_by_pid(nr)	find_task_by_pid_type(PIDTYPE_PID, nr)
-extern struct task_struct *find_task_by_pid_type(int type, int pid);
+extern struct pid_namespace init_pid_ns;
+
+/*
+ * find a task by one of its numerical ids
+ *
+ * find_task_by_pid_type_ns():
+ *      it is the most generic call - it finds a task by all id,
+ *      type and namespace specified
+ * find_task_by_pid_ns():
+ *      finds a task by its pid in the specified namespace
+ * find_task_by_pid_type():
+ *      finds a task by its global id with the specified type, e.g.
+ *      by global session id
+ * find_task_by_pid():
+ *      finds a task by its global pid
+ *
+ * see also find_pid() etc in include/linux/pid.h
+ */
+
+extern struct task_struct *find_task_by_pid_type_ns(int type, int pid,
+		struct pid_namespace *ns);
+
+#define find_task_by_pid_ns(nr, ns)	\
+		find_task_by_pid_type_ns(PIDTYPE_PID, nr, ns)
+#define find_task_by_pid_type(type, nr)	\
+		find_task_by_pid_type_ns(type, nr, &init_pid_ns)
+#define find_task_by_pid(nr)		\
+		find_task_by_pid_type(PIDTYPE_PID, nr)
+
 extern void __set_special_pids(pid_t session, pid_t pgrp);
 
 /* per-UID process charging. */

commit 7af5729474b5b8ad385adadab78d6e723e7655a3
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:06 2007 -0700

    pid namespaces: helpers to obtain pid numbers
    
    When showing pid to user or getting the pid numerical id for in-kernel use the
    value of this id may differ depending on the namespace.
    
    This set of helpers is used to get the global pid nr, the virtual (i.e.  seen
    by task in its namespace) nr and the nr as it is seen from the specified
    namespace.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 47cf81d62047..77e8cad3b17a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1188,16 +1188,6 @@ static inline int rt_task(struct task_struct *p)
 	return rt_prio(p->prio);
 }
 
-static inline pid_t task_pgrp_nr(struct task_struct *tsk)
-{
-	return tsk->signal->pgrp;
-}
-
-static inline pid_t task_session_nr(struct task_struct *tsk)
-{
-	return tsk->signal->__session;
-}
-
 static inline void set_task_session(struct task_struct *tsk, pid_t session)
 {
 	tsk->signal->__session = session;
@@ -1223,6 +1213,104 @@ static inline struct pid *task_session(struct task_struct *task)
 	return task->group_leader->pids[PIDTYPE_SID].pid;
 }
 
+struct pid_namespace;
+
+/*
+ * the helpers to get the task's different pids as they are seen
+ * from various namespaces
+ *
+ * task_xid_nr()     : global id, i.e. the id seen from the init namespace;
+ * task_xid_vnr()    : virtual id, i.e. the id seen from the namespace the task
+ *                     belongs to. this only makes sence when called in the
+ *                     context of the task that belongs to the same namespace;
+ * task_xid_nr_ns()  : id seen from the ns specified;
+ *
+ * set_task_vxid()   : assigns a virtual id to a task;
+ *
+ * task_ppid_nr_ns() : the parent's id as seen from the namespace specified.
+ *                     the result depends on the namespace and whether the
+ *                     task in question is the namespace's init. e.g. for the
+ *                     namespace's init this will return 0 when called from
+ *                     the namespace of this init, or appropriate id otherwise.
+ *
+ *
+ * see also pid_nr() etc in include/linux/pid.h
+ */
+
+static inline pid_t task_pid_nr(struct task_struct *tsk)
+{
+	return tsk->pid;
+}
+
+static inline pid_t task_pid_nr_ns(struct task_struct *tsk,
+		struct pid_namespace *ns)
+{
+	return pid_nr_ns(task_pid(tsk), ns);
+}
+
+static inline pid_t task_pid_vnr(struct task_struct *tsk)
+{
+	return pid_vnr(task_pid(tsk));
+}
+
+
+static inline pid_t task_tgid_nr(struct task_struct *tsk)
+{
+	return tsk->tgid;
+}
+
+static inline pid_t task_tgid_nr_ns(struct task_struct *tsk,
+		struct pid_namespace *ns)
+{
+	return pid_nr_ns(task_tgid(tsk), ns);
+}
+
+static inline pid_t task_tgid_vnr(struct task_struct *tsk)
+{
+	return pid_vnr(task_tgid(tsk));
+}
+
+
+static inline pid_t task_pgrp_nr(struct task_struct *tsk)
+{
+	return tsk->signal->pgrp;
+}
+
+static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
+		struct pid_namespace *ns)
+{
+	return pid_nr_ns(task_pgrp(tsk), ns);
+}
+
+static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
+{
+	return pid_vnr(task_pgrp(tsk));
+}
+
+
+static inline pid_t task_session_nr(struct task_struct *tsk)
+{
+	return tsk->signal->__session;
+}
+
+static inline pid_t task_session_nr_ns(struct task_struct *tsk,
+		struct pid_namespace *ns)
+{
+	return pid_nr_ns(task_session(tsk), ns);
+}
+
+static inline pid_t task_session_vnr(struct task_struct *tsk)
+{
+	return pid_vnr(task_session(tsk));
+}
+
+
+static inline pid_t task_ppid_nr_ns(struct task_struct *tsk,
+		struct pid_namespace *ns)
+{
+	return pid_nr_ns(task_pid(rcu_dereference(tsk->real_parent)), ns);
+}
+
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.

commit b460cbc581a53cc088ceba80608021dd49c63c43
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Thu Oct 18 23:39:52 2007 -0700

    pid namespaces: define is_global_init() and is_container_init()
    
    is_init() is an ambiguous name for the pid==1 check.  Split it into
    is_global_init() and is_container_init().
    
    A cgroup init has it's tsk->pid == 1.
    
    A global init also has it's tsk->pid == 1 and it's active pid namespace
    is the init_pid_ns.  But rather than check the active pid namespace,
    compare the task structure with 'init_pid_ns.child_reaper', which is
    initialized during boot to the /sbin/init process and never changes.
    
    Changelog:
    
            2.6.22-rc4-mm2-pidns1:
            - Use 'init_pid_ns.child_reaper' to determine if a given task is the
              global init (/sbin/init) process. This would improve performance
              and remove dependence on the task_pid().
    
            2.6.21-mm2-pidns2:
    
            - [Sukadev Bhattiprolu] Changed is_container_init() calls in {powerpc,
              ppc,avr32}/traps.c for the _exception() call to is_global_init().
              This way, we kill only the cgroup if the cgroup's init has a
              bug rather than force a kernel panic.
    
    [akpm@linux-foundation.org: fix comment]
    [sukadev@us.ibm.com: Use is_global_init() in arch/m32r/mm/fault.c]
    [bunk@stusta.de: kernel/pid.c: remove unused exports]
    [sukadev@us.ibm.com: Fix capability.c to work with threaded init]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Herbert Poetzel <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index df6049e5e8bf..47cf81d62047 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1237,12 +1237,20 @@ static inline int pid_alive(struct task_struct *p)
 }
 
 /**
- * is_init - check if a task structure is init
+ * is_global_init - check if a task structure is init
  * @tsk: Task structure to be checked.
  *
  * Check if a task structure is the first user space task the kernel created.
+ *
+ * TODO: We should inline this function after some cleanups in pid_namespace.h
+ */
+extern int is_global_init(struct task_struct *tsk);
+
+/*
+ * is_container_init:
+ * check whether in the task is init in its own pid namespace.
  */
-static inline int is_init(struct task_struct *tsk)
+static inline int is_container_init(struct task_struct *tsk)
 {
 	return tsk->pid == 1;
 }

commit a47afb0f9d794d525a372c8d69902147cc88222a
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Thu Oct 18 23:39:46 2007 -0700

    pid namespaces: round up the API
    
    The set of functions process_session, task_session, process_group and
    task_pgrp is confusing, as the names can be mixed with each other when looking
    at the code for a long time.
    
    The proposals are to
    * equip the functions that return the integer with _nr suffix to
      represent that fact,
    * and to make all functions work with task (not process) by making
      the common prefix of the same name.
    
    For monotony the routines signal_session() and set_signal_session() are
    replaced with task_session_nr() and set_task_session(), especially since they
    are only used with the explicit task->signal dereference.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Acked-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 93a55f2e5ef6..df6049e5e8bf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1188,24 +1188,19 @@ static inline int rt_task(struct task_struct *p)
 	return rt_prio(p->prio);
 }
 
-static inline pid_t process_group(struct task_struct *tsk)
+static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {
 	return tsk->signal->pgrp;
 }
 
-static inline pid_t signal_session(struct signal_struct *sig)
+static inline pid_t task_session_nr(struct task_struct *tsk)
 {
-	return sig->__session;
+	return tsk->signal->__session;
 }
 
-static inline pid_t process_session(struct task_struct *tsk)
+static inline void set_task_session(struct task_struct *tsk, pid_t session)
 {
-	return signal_session(tsk->signal);
-}
-
-static inline void set_signal_session(struct signal_struct *sig, pid_t session)
-{
-	sig->__session = session;
+	tsk->signal->__session = session;
 }
 
 static inline struct pid *task_pid(struct task_struct *task)

commit 8793d854edbc2774943a4b0de3304dc73991159a
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:39 2007 -0700

    Task Control Groups: make cpusets a client of cgroups
    
    Remove the filesystem support logic from the cpusets system and makes cpusets
    a cgroup subsystem
    
    The "cpuset" filesystem becomes a dummy filesystem; attempts to mount it get
    passed through to the cgroup filesystem with the appropriate options to
    emulate the old cpuset filesystem behaviour.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1aa1cfa63b37..93a55f2e5ef6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -756,8 +756,6 @@ static inline int above_background_load(void)
 }
 
 struct io_context;			/* See blkdev.h */
-struct cpuset;
-
 #define NGROUPS_SMALL		32
 #define NGROUPS_PER_BLOCK	((int)(PAGE_SIZE / sizeof(gid_t)))
 struct group_info {
@@ -1125,7 +1123,6 @@ struct task_struct {
 	short il_next;
 #endif
 #ifdef CONFIG_CPUSETS
-	struct cpuset *cpuset;
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;
 	int cpuset_mem_spread_rotor;

commit 817929ec274bcfe771586d338bb31d1659615686
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:36 2007 -0700

    Task Control Groups: shared cgroup subsystem group arrays
    
    Replace the struct css_set embedded in task_struct with a pointer; all tasks
    that have the same set of memberships across all hierarchies will share a
    css_set object, and will be linked via their css_sets field to the "tasks"
    list_head in the css_set.
    
    Assuming that many tasks share the same cgroup assignments, this reduces
    overall space usage and keeps the size of the task_struct down (three pointers
    added to task_struct compared to a non-cgroups kernel, no matter how many
    subsystems are registered).
    
    [akpm@linux-foundation.org: fix a printk]
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index af2ed4bae678..1aa1cfa63b37 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -894,34 +894,6 @@ struct sched_entity {
 #endif
 };
 
-#ifdef CONFIG_CGROUPS
-
-#define SUBSYS(_x) _x ## _subsys_id,
-enum cgroup_subsys_id {
-#include <linux/cgroup_subsys.h>
-	CGROUP_SUBSYS_COUNT
-};
-#undef SUBSYS
-
-/* A css_set is a structure holding pointers to a set of
- * cgroup_subsys_state objects.
- */
-
-struct css_set {
-
-	/* Set of subsystem states, one for each subsystem. NULL for
-	 * subsystems that aren't part of this hierarchy. These
-	 * pointers reduce the number of dereferences required to get
-	 * from a task to its state for a given cgroup, but result
-	 * in increased space usage if tasks are in wildly different
-	 * groupings across different hierarchies. This array is
-	 * immutable after creation */
-	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
-
-};
-
-#endif /* CONFIG_CGROUPS */
-
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -1159,7 +1131,10 @@ struct task_struct {
 	int cpuset_mem_spread_rotor;
 #endif
 #ifdef CONFIG_CGROUPS
-	struct css_set cgroups;
+	/* Control Group info protected by css_set_lock */
+	struct css_set *cgroups;
+	/* cg_list protected by css_set_lock and tsk->alloc_lock */
+	struct list_head cg_list;
 #endif
 #ifdef CONFIG_FUTEX
 	struct robust_list_head __user *robust_list;

commit ddbcc7e8e50aefe467c01cac3dec71f118cd8ac2
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:30 2007 -0700

    Task Control Groups: basic task cgroup framework
    
    Generic Process Control Groups
    --------------------------
    
    There have recently been various proposals floating around for
    resource management/accounting and other task grouping subsystems in
    the kernel, including ResGroups, User BeanCounters, NSProxy
    cgroups, and others.  These all need the basic abstraction of being
    able to group together multiple processes in an aggregate, in order to
    track/limit the resources permitted to those processes, or control
    other behaviour of the processes, and all implement this grouping in
    different ways.
    
    This patchset provides a framework for tracking and grouping processes
    into arbitrary "cgroups" and assigning arbitrary state to those
    groupings, in order to control the behaviour of the cgroup as an
    aggregate.
    
    The intention is that the various resource management and
    virtualization/cgroup efforts can also become task cgroup
    clients, with the result that:
    
    - the userspace APIs are (somewhat) normalised
    
    - it's easier to test e.g. the ResGroups CPU controller in
     conjunction with the BeanCounters memory controller, or use either of
    them as the resource-control portion of a virtual server system.
    
    - the additional kernel footprint of any of the competing resource
     management systems is substantially reduced, since it doesn't need
     to provide process grouping/containment, hence improving their
     chances of getting into the kernel
    
    This patch:
    
    Add the main task cgroups framework - the cgroup filesystem, and the
    basic structures for tracking membership and associating subsystem state
    objects to tasks.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 10a83d8d5775..af2ed4bae678 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -894,6 +894,34 @@ struct sched_entity {
 #endif
 };
 
+#ifdef CONFIG_CGROUPS
+
+#define SUBSYS(_x) _x ## _subsys_id,
+enum cgroup_subsys_id {
+#include <linux/cgroup_subsys.h>
+	CGROUP_SUBSYS_COUNT
+};
+#undef SUBSYS
+
+/* A css_set is a structure holding pointers to a set of
+ * cgroup_subsys_state objects.
+ */
+
+struct css_set {
+
+	/* Set of subsystem states, one for each subsystem. NULL for
+	 * subsystems that aren't part of this hierarchy. These
+	 * pointers reduce the number of dereferences required to get
+	 * from a task to its state for a given cgroup, but result
+	 * in increased space usage if tasks are in wildly different
+	 * groupings across different hierarchies. This array is
+	 * immutable after creation */
+	struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
+
+};
+
+#endif /* CONFIG_CGROUPS */
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	void *stack;
@@ -1130,6 +1158,9 @@ struct task_struct {
 	int cpuset_mems_generation;
 	int cpuset_mem_spread_rotor;
 #endif
+#ifdef CONFIG_CGROUPS
+	struct css_set cgroups;
+#endif
 #ifdef CONFIG_FUTEX
 	struct robust_list_head __user *robust_list;
 #ifdef CONFIG_COMPAT
@@ -1625,7 +1656,8 @@ static inline int thread_group_empty(struct task_struct *p)
 /*
  * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
- * pins the final release of task.io_context.  Also protects ->cpuset.
+ * pins the final release of task.io_context.  Also protects ->cpuset and
+ * ->cgroup.subsys[].
  *
  * Nests both inside and outside of read_lock(&tasklist_lock).
  * It must not be nested with write_lock_irq(&tasklist_lock),

commit 54e840dd5021ae03c5d2b4158b191bb67f584b75
Merge: 32c15bb978c0 480b9434c542
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Thu Oct 18 14:54:03 2007 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched:
      sched: reduce schedstat variable overhead a bit
      sched: add KERN_CONT annotation
      sched: cleanup, make struct rq comments more consistent
      sched: cleanup, fix spacing
      sched: fix return value of wait_for_completion_interruptible()

commit c66f08be7e3ad0a28bcd9a0aef766fdf08ea0ec6
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Oct 18 03:06:34 2007 -0700

    Add scaled time to taskstats based process accounting
    
    This adds items to the taststats struct to account for user and system
    time based on scaling the CPU frequency and instruction issue rates.
    
    Adds account_(user|system)_time_scaled callbacks which architectures
    can use to account for time using this mechanism.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 269b234609b8..7accc04e23ab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -991,7 +991,7 @@ struct task_struct {
 	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
 
 	unsigned int rt_priority;
-	cputime_t utime, stime;
+	cputime_t utime, stime, utimescaled, stimescaled;
 	cputime_t gtime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */

commit 6212e3a388fdda3f19fa660ef5a30edf54d1dcfd
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Oct 18 03:04:56 2007 -0700

    Remove struct task_struct::io_wait
    
    Hell knows what happened in commit 63b05203af57e7de4f3bb63b8b81d43bc196d32b
    during 2.6.9 development.  Commit introduced io_wait field which remained
    write-only than and still remains write-only.
    
    Also garbage collect macros which "use" io_wait.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c204ab0d4df1..269b234609b8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1110,13 +1110,6 @@ struct task_struct {
 
 	unsigned long ptrace_message;
 	siginfo_t *last_siginfo; /* For ptrace use.  */
-/*
- * current io wait handle: wait queue entry to use for io waits
- * If this thread is processing aio, this points at the waitqueue
- * inside the currently handled kiocb. It may be NULL (i.e. default
- * to a stack based synchronous wait) if its doing sync IO.
- */
-	wait_queue_t *io_wait;
 #ifdef CONFIG_TASK_XACCT
 /* i/o counters(bytes read/written, #syscalls */
 	u64 rchar, wchar, syscr, syscw;

commit 480b9434c542ddf2833aaed3dabba71bc0b787b5
Author: Ken Chen <kenchen@google.com>
Date:   Thu Oct 18 21:32:56 2007 +0200

    sched: reduce schedstat variable overhead a bit
    
    schedstat is useful in investigating CPU scheduler behavior.  Ideally,
    I think it is beneficial to have it on all the time.  However, the
    cost of turning it on in production system is quite high, largely due
    to number of events it collects and also due to its large memory
    footprint.
    
    Most of the fields probably don't need to be full 64-bit on 64-bit
    arch.  Rolling over 4 billion events will most like take a long time
    and user space tool can be made to accommodate that.  I'm proposing
    kernel to cut back most of variable width on 64-bit system.  (note,
    the following patch doesn't affect 32-bit system).
    
    Signed-off-by: Ken Chen <kenchen@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c204ab0d4df1..2f9c1261f202 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -569,7 +569,7 @@ struct sched_info {
 			   last_queued;	/* when we were last queued to run */
 #ifdef CONFIG_SCHEDSTATS
 	/* BKL stats */
-	unsigned long bkl_count;
+	unsigned int bkl_count;
 #endif
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
@@ -705,34 +705,34 @@ struct sched_domain {
 
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */
-	unsigned long lb_count[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_failed[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_balanced[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_imbalance[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_gained[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_hot_gained[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_nobusyg[CPU_MAX_IDLE_TYPES];
-	unsigned long lb_nobusyq[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_count[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_failed[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_balanced[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_gained[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];
+	unsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];
 
 	/* Active load balancing */
-	unsigned long alb_count;
-	unsigned long alb_failed;
-	unsigned long alb_pushed;
+	unsigned int alb_count;
+	unsigned int alb_failed;
+	unsigned int alb_pushed;
 
 	/* SD_BALANCE_EXEC stats */
-	unsigned long sbe_count;
-	unsigned long sbe_balanced;
-	unsigned long sbe_pushed;
+	unsigned int sbe_count;
+	unsigned int sbe_balanced;
+	unsigned int sbe_pushed;
 
 	/* SD_BALANCE_FORK stats */
-	unsigned long sbf_count;
-	unsigned long sbf_balanced;
-	unsigned long sbf_pushed;
+	unsigned int sbf_count;
+	unsigned int sbf_balanced;
+	unsigned int sbf_pushed;
 
 	/* try_to_wake_up() stats */
-	unsigned long ttwu_wake_remote;
-	unsigned long ttwu_move_affine;
-	unsigned long ttwu_move_balance;
+	unsigned int ttwu_wake_remote;
+	unsigned int ttwu_move_affine;
+	unsigned int ttwu_move_balance;
 #endif
 };
 

commit e6d5a11dad44b8ae18ca8fc4ecb72ccccfa0a2d2
Merge: b6257a9036f0 b9dca1e0fcb6
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Oct 17 09:11:18 2007 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched:
      sched: fix new task startup crash
      sched: fix !SYSFS build breakage
      sched: fix improper load balance across sched domain
      sched: more robust sd-sysctl entry freeing

commit 57c521ce6125e15e99e56c902cb8da96bee7b36d
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Oct 16 23:31:35 2007 -0700

    ifdef struct task_struct::security
    
    For those who don't care about CONFIG_SECURITY.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Serge E. Hallyn" <serge@hallyn.com>
    Cc: Casey Schaufler <casey@schaufler-ca.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 774cb435c7d8..3de5aa210feb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1044,8 +1044,9 @@ struct task_struct {
 	int (*notifier)(void *priv);
 	void *notifier_data;
 	sigset_t *notifier_mask;
-	
+#ifdef CONFIG_SECURITY
 	void *security;
+#endif
 	struct audit_context *audit_context;
 	seccomp_t seccomp;
 

commit 18796aa00243a594a2bd6733f1360aa38c3cd8f4
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:30:26 2007 -0700

    task_struct: move ->fpu_counter and ->oomkilladj
    
    There is nice 2 byte hole after struct task_struct::ioprio field
    into which we can put two 1-byte fields: ->fpu_counter and ->oomkilladj.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Acked-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 884699fa8c1f..774cb435c7d8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -918,6 +918,16 @@ struct task_struct {
 #endif
 
 	unsigned short ioprio;
+	/*
+	 * fpu_counter contains the number of consecutive context switches
+	 * that the FPU is used. If this is over a threshold, the lazy fpu
+	 * saving becomes unlazy to save the trap. This is an unsigned char
+	 * so that after 256 times the counter wraps and the behavior turns
+	 * lazy again; this to deal with bursty apps that only use FPU for
+	 * a short time
+	 */
+	unsigned char fpu_counter;
+	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif
@@ -1003,16 +1013,6 @@ struct task_struct {
 	struct key *thread_keyring;	/* keyring private to this thread */
 	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 #endif
-	/*
-	 * fpu_counter contains the number of consecutive context switches
-	 * that the FPU is used. If this is over a threshold, the lazy fpu
-	 * saving becomes unlazy to save the trap. This is an unsigned char
-	 * so that after 256 times the counter wraps and the behavior turns
-	 * lazy again; this to deal with bursty apps that only use FPU for
-	 * a short time
-	 */
-	unsigned char fpu_counter;
-	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
 				       it with task_lock())

commit 970a8645ca051225a32401e4c80b50fc0a49c081
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:30:09 2007 -0700

    user.c: #ifdef ->mq_bytes
    
    For those who deselect POSIX message queues.
    
    Reduces SLAB size of user_struct from 64 to 32 bytes here, SLUB size -- from
    40 bytes to 32 bytes.
    
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 57166582a3f5..884699fa8c1f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -518,8 +518,10 @@ struct user_struct {
 	atomic_t inotify_watches; /* How many inotify watches does this user have? */
 	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
 #endif
+#ifdef CONFIG_POSIX_MQUEUE
 	/* protected by mq_lock	*/
 	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
+#endif
 	unsigned long locked_shm; /* How many pages of mlocked shm ? */
 
 #ifdef CONFIG_KEYS

commit 42b2dd0a02c512cf59c96f5c227bf54bfe5bbf08
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:27:30 2007 -0700

    Shrink task_struct if CONFIG_FUTEX=n
    
    robust_list, compat_robust_list, pi_state_list, pi_state_cache are
    really used if futexes are on.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 87f2eb27ee10..57166582a3f5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1132,13 +1132,14 @@ struct task_struct {
 	int cpuset_mems_generation;
 	int cpuset_mem_spread_rotor;
 #endif
+#ifdef CONFIG_FUTEX
 	struct robust_list_head __user *robust_list;
 #ifdef CONFIG_COMPAT
 	struct compat_robust_list_head __user *compat_robust_list;
 #endif
 	struct list_head pi_state_list;
 	struct futex_pi_state *pi_state_cache;
-
+#endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;
 

commit 3befe7ceb8d39a56a9ea55dd2da8b4bd9ddcdd36
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Oct 16 23:27:14 2007 -0700

    Shrink struct task_struct::oomkilladj
    
    oomkilladj is int, but values which can be assigned to it are -17, [-16,
    15], thus fitting into s8.
    
    While patch itself doesn't help in making task_struct smaller, because of
    natural alignment of ->link_count, it will make picture clearer wrt futher
    task_struct reduction patches.  My plan is to move ->fpu_counter and
    ->oomkilladj after ->ioprio filling hole on i386 and x86_64.  But that's
    for later, because bloated distro configs need looking at as well.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cb952bc225e4..87f2eb27ee10 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1010,7 +1010,7 @@ struct task_struct {
 	 * a short time
 	 */
 	unsigned char fpu_counter;
-	int oomkilladj; /* OOM kill score adjustment (bit shift). */
+	s8 oomkilladj; /* OOM kill score adjustment (bit shift). */
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock
 				       it with task_lock())

commit 82df39738ba9e02c057fa99b7461a56117d36119
Author: Roland McGrath <roland@redhat.com>
Date:   Tue Oct 16 23:27:02 2007 -0700

    Add MMF_DUMP_ELF_HEADERS
    
    This adds the MMF_DUMP_ELF_HEADERS option to /proc/pid/coredump_filter.
    This dumps the first page (only) of a private file mapping if it appears to
    be a mapping of an ELF file.  Including these pages in the core dump may
    give sufficient identifying information to associate the original DSO and
    executable file images and their debugging information with a core file in
    a generic way just from its contents (e.g.  when those binaries were built
    with ld --build-id).  I expect this to become the default behavior
    eventually.  Existing versions of gdb can be confused by the core dumps it
    creates, so it won't enabled by default for some time to come.  Soon many
    people will have systems with a gdb that handle these dumps, so they can
    arrange to set the bit at boot and have it inherited system-wide.
    
    This also cleans up the checking of the MMF_DUMP_* flag bits, which did not
    need to be using atomic macros.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e643357eda05..cb952bc225e4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -359,8 +359,9 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_DUMP_ANON_SHARED	3
 #define MMF_DUMP_MAPPED_PRIVATE	4
 #define MMF_DUMP_MAPPED_SHARED	5
+#define MMF_DUMP_ELF_HEADERS	6
 #define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
-#define MMF_DUMP_FILTER_BITS	4
+#define MMF_DUMP_FILTER_BITS	5
 #define MMF_DUMP_FILTER_MASK \
 	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
 #define MMF_DUMP_FILTER_DEFAULT \

commit c4f3b63fe15b4629aa1ec163c95ab30423d0f76a
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Tue Oct 16 23:26:09 2007 -0700

    softlockup: add a /proc tuning parameter
    
    Control the trigger limit for softlockup warnings.  This is useful for
    debugging softlockups, by lowering the softlockup_thresh to identify
    possible softlockups earlier.
    
    This patch:
    1. Adds a sysctl softlockup_thresh with valid values of 1-60s
       (Higher value to disable false positives)
    2. Changes the softlockup printk to print the cpu softlockup time
    
    [akpm@linux-foundation.org: Fix various warnings and add definition of "two"]
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Shai Fultheim <shai@scalex86.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 59738efff8ad..e643357eda05 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -261,6 +261,7 @@ extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 extern void touch_all_softlockup_watchdogs(void);
+extern int softlockup_thresh;
 #else
 static inline void softlockup_tick(void)
 {

commit 3e26c149c358529b1605f8959341d34bc4b880a3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:50 2007 -0700

    mm: dirty balancing for tasks
    
    Based on ideas of Andrew:
      http://marc.info/?l=linux-kernel&m=102912915020543&w=2
    
    Scale the bdi dirty limit inversly with the tasks dirty rate.
    This makes heavy writers have a lower dirty limit than the occasional writer.
    
    Andrea proposed something similar:
      http://lwn.net/Articles/152277/
    
    The main disadvantage to his patch is that he uses an unrelated quantity to
    measure time, which leaves him with a workload dependant tunable. Other than
    that the two approaches appear quite similar.
    
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 592e3a55f818..59738efff8ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -74,6 +74,7 @@ struct sched_param {
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>
+#include <linux/proportions.h>
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
 #include <linux/futex.h>
@@ -1149,6 +1150,7 @@ struct task_struct {
 #ifdef CONFIG_FAULT_INJECTION
 	int make_it_fail;
 #endif
+	struct prop_local_single dirties;
 };
 
 /*

commit b1a8c172c318534b96d0f0f1aecdad3898118b98
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Wed Oct 17 16:55:11 2007 +0200

    sched: fix !SYSFS build breakage
    
    When CONFIG_SYSFS is not set, CONFIG_FAIR_USER_SCHED fails to build
    with
    
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1488): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1490): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1480): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1494): undefined reference to `kernel_subsys'
    
    This patch fixes this build error.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 592e3a55f818..4ac7d51ad0e1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -530,10 +530,12 @@ struct user_struct {
 
 #ifdef CONFIG_FAIR_USER_SCHED
 	struct task_group *tg;
+#ifdef CONFIG_SYSFS
 	struct kset kset;
 	struct subsys_attribute user_attr;
 	struct work_struct work;
 #endif
+#endif
 };
 
 #ifdef CONFIG_FAIR_USER_SCHED

commit 607717a65d92858fd925bec05baae4d142719f27
Author: Paul Jackson <pj@sgi.com>
Date:   Tue Oct 16 01:27:43 2007 -0700

    cpuset: remove sched domain hooks from cpusets
    
    Remove the cpuset hooks that defined sched domains depending on the setting
    of the 'cpu_exclusive' flag.
    
    The cpu_exclusive flag can only be set on a child if it is set on the
    parent.
    
    This made that flag painfully unsuitable for use as a flag defining a
    partitioning of a system.
    
    It was entirely unobvious to a cpuset user what partitioning of sched
    domains they would be causing when they set that one cpu_exclusive bit on
    one cpuset, because it depended on what CPUs were in the remainder of that
    cpusets siblings and child cpusets, after subtracting out other
    cpu_exclusive cpusets.
    
    Furthermore, there was no way on production systems to query the
    result.
    
    Using the cpu_exclusive flag for this was simply wrong from the get go.
    
    Fortunately, it was sufficiently borked that so far as I know, almost no
    successful use has been made of this.  One real time group did use it to
    affectively isolate CPUs from any load balancing efforts.  They are willing
    to adapt to alternative mechanisms for this, such as someway to manipulate
    the list of isolated CPUs on a running system.  They can do without this
    present cpu_exclusive based mechanism while we develop an alternative.
    
    There is a real risk, to the best of my understanding, of users
    accidentally setting up a partitioned scheduler domains, inhibiting desired
    load balancing across all their CPUs, due to the nonobvious (from the
    cpuset perspective) side affects of the cpu_exclusive flag.
    
    Furthermore, since there was no way on a running system to see what one was
    doing with sched domains, this change will be invisible to any using code.
    Unless they have real insight to the scheduler load balancing choices, they
    will be unable to detect that this change has been made in the kernel's
    behaviour.
    
    Initial discussion on lkml of this patch has generated much comment.  My
    (probably controversial) take on that discussion is that it has reached a
    rough concensus that the current cpuset cpu_exclusive mechanism for
    defining sched domains is borked.  There is no concensus on the
    replacement.  But since we can remove this mechanism, and since its
    continued presence risks causing unwanted partitioning of the schedulers
    load balancing, we should remove it while we can, as we proceed to work the
    replacement scheduler domain mechanisms.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Dinakar Guniguntala <dino@in.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 285ee4827a3c..592e3a55f818 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -729,9 +729,6 @@ struct sched_domain {
 #endif
 };
 
-extern int partition_sched_domains(cpumask_t *partition1,
-				    cpumask_t *partition2);
-
 #endif	/* CONFIG_SMP */
 
 /*

commit c92ff1bde06f69d59b40f3194016aee51cc5da55
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Oct 16 01:24:43 2007 -0700

    move mm_struct and vm_area_struct
    
    Move the definitions of struct mm_struct and struct vma_area_struct to
    include/mm_types.h.  This allows to define more function in asm/pgtable.h
    and friends with inline assemblies instead of macros.  Compile tested on
    i386, powerpc, powerpc64, s390-32, s390-64 and x86_64.
    
    [aurelien@aurel32.net: build fix]
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Aurelien Jarno <aurelien@aurel32.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 228e0a8ce248..285ee4827a3c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,8 +1,6 @@
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
-#include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
-
 /*
  * cloning flags:
  */
@@ -58,12 +56,12 @@ struct sched_param {
 #include <linux/cpumask.h>
 #include <linux/errno.h>
 #include <linux/nodemask.h>
+#include <linux/mm_types.h>
 
 #include <asm/system.h>
 #include <asm/semaphore.h>
 #include <asm/page.h>
 #include <asm/ptrace.h>
-#include <asm/mmu.h>
 #include <asm/cputime.h>
 
 #include <linux/smp.h>
@@ -319,7 +317,6 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #define add_mm_counter(mm, member, value) atomic_long_add(value, &(mm)->_##member)
 #define inc_mm_counter(mm, member) atomic_long_inc(&(mm)->_##member)
 #define dec_mm_counter(mm, member) atomic_long_dec(&(mm)->_##member)
-typedef atomic_long_t mm_counter_t;
 
 #else  /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
 /*
@@ -331,7 +328,6 @@ typedef atomic_long_t mm_counter_t;
 #define add_mm_counter(mm, member, value) (mm)->_##member += (value)
 #define inc_mm_counter(mm, member) (mm)->_##member++
 #define dec_mm_counter(mm, member) (mm)->_##member--
-typedef unsigned long mm_counter_t;
 
 #endif /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
 
@@ -368,74 +364,6 @@ extern int get_dumpable(struct mm_struct *mm);
 #define MMF_DUMP_FILTER_DEFAULT \
 	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED))
 
-struct mm_struct {
-	struct vm_area_struct * mmap;		/* list of VMAs */
-	struct rb_root mm_rb;
-	struct vm_area_struct * mmap_cache;	/* last find_vma result */
-	unsigned long (*get_unmapped_area) (struct file *filp,
-				unsigned long addr, unsigned long len,
-				unsigned long pgoff, unsigned long flags);
-	void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
-	unsigned long mmap_base;		/* base of mmap area */
-	unsigned long task_size;		/* size of task vm space */
-	unsigned long cached_hole_size;         /* if non-zero, the largest hole below free_area_cache */
-	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
-	pgd_t * pgd;
-	atomic_t mm_users;			/* How many users with user space? */
-	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
-	int map_count;				/* number of VMAs */
-	struct rw_semaphore mmap_sem;
-	spinlock_t page_table_lock;		/* Protects page tables and some counters */
-
-	struct list_head mmlist;		/* List of maybe swapped mm's.  These are globally strung
-						 * together off init_mm.mmlist, and are protected
-						 * by mmlist_lock
-						 */
-
-	/* Special counters, in some configurations protected by the
-	 * page_table_lock, in other configurations by being atomic.
-	 */
-	mm_counter_t _file_rss;
-	mm_counter_t _anon_rss;
-
-	unsigned long hiwater_rss;	/* High-watermark of RSS usage */
-	unsigned long hiwater_vm;	/* High-water virtual memory usage */
-
-	unsigned long total_vm, locked_vm, shared_vm, exec_vm;
-	unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;
-	unsigned long start_code, end_code, start_data, end_data;
-	unsigned long start_brk, brk, start_stack;
-	unsigned long arg_start, arg_end, env_start, env_end;
-
-	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */
-
-	cpumask_t cpu_vm_mask;
-
-	/* Architecture-specific MM context */
-	mm_context_t context;
-
-	/* Swap token stuff */
-	/*
-	 * Last value of global fault stamp as seen by this process.
-	 * In other words, this value gives an indication of how long
-	 * it has been since this task got the token.
-	 * Look at mm/thrash.c
-	 */
-	unsigned int faultstamp;
-	unsigned int token_priority;
-	unsigned int last_interval;
-
-	unsigned long flags; /* Must use atomic bitops to access the bits */
-
-	/* coredumping support */
-	int core_waiters;
-	struct completion *core_startup_done, core_done;
-
-	/* aio bits */
-	rwlock_t		ioctx_list_lock;
-	struct kioctx		*ioctx_list;
-};
-
 struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];

commit 94886b84b1bcdc95f34f70e7fce407efefe472e1
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: guest CPU accounting: maintain stats in account_system_time()
    
    modify account_system_time() to add cputime to cpustat->guest if we are
    running a VCPU. We add this cputime to cpustat->user instead of
    cpustat->system because this part of KVM code is in fact user code
    although it is executed in the kernel. We duplicate VCPU time between
    guest and user to allow an unmodified "top(1)" to display correct value.
    A modified "top(1)" is able to display good cpu user time and cpu guest
    time by subtracting cpu guest time from cpu user time. Update "gtime" in
    task_struct accordingly.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Acked-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fefce22e4c16..228e0a8ce248 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1342,6 +1342,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
+#define PF_VCPU		0x00000010	/* I'm a virtual CPU */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */

commit 9ac52315d4cf5f561f36dabaf0720c00d3553162
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: guest CPU accounting: add guest-CPU /proc/<pid>/stat fields
    
    like for cpustat, introduce the "gtime" (guest time of the task) and
    "cgtime" (guest time of the task children) fields for the
    tasks. Modify signal_struct and task_struct.
    
    Modify /proc/<pid>/stat to display these new fields.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Acked-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3a6e05e77715..fefce22e4c16 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -517,6 +517,8 @@ struct signal_struct {
 	 * in __exit_signal, except for the group leader.
 	 */
 	cputime_t utime, stime, cutime, cstime;
+	cputime_t gtime;
+	cputime_t cgtime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 	unsigned long inblock, oublock, cinblock, coublock;
@@ -1048,6 +1050,7 @@ struct task_struct {
 
 	unsigned int rt_priority;
 	cputime_t utime, stime;
+	cputime_t gtime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
 	struct timespec real_start_time;	/* boot based time */

commit cc367732ff0b1c63d0d7bdd11e6d1661794ef6a3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:18 2007 +0200

    sched: debug, improve migration statistics
    
    add new migration statistics when SCHED_DEBUG and SCHEDSTATS
    is enabled. Available in /proc/<PID>/sched.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fcc9a5ada1a2..3a6e05e77715 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -931,6 +931,24 @@ struct sched_entity {
 	u64			block_max;
 	u64			exec_max;
 	u64			slice_max;
+
+	u64			nr_migrations;
+	u64			nr_migrations_cold;
+	u64			nr_failed_migrations_affine;
+	u64			nr_failed_migrations_running;
+	u64			nr_failed_migrations_hot;
+	u64			nr_forced_migrations;
+	u64			nr_forced2_migrations;
+
+	u64			nr_wakeups;
+	u64			nr_wakeups_sync;
+	u64			nr_wakeups_migrate;
+	u64			nr_wakeups_local;
+	u64			nr_wakeups_remote;
+	u64			nr_wakeups_affine;
+	u64			nr_wakeups_affine_attempts;
+	u64			nr_wakeups_passive;
+	u64			nr_wakeups_idle;
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit da84d96176729fb48a8458561e5d8647103168b8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:18 2007 +0200

    sched: reintroduce cache-hot affinity
    
    reintroduce a simplified version of cache-hot/cold scheduling
    affinity. This improves performance with certain SMP workloads,
    such as sysbench.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8be5b57768c0..fcc9a5ada1a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1415,6 +1415,7 @@ extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
+extern unsigned int sysctl_sched_migration_cost;
 #endif
 
 extern unsigned int sysctl_sched_compat_yield;

commit 95938a35c5562afa7af7252821e44132391a3db8
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Oct 15 17:00:14 2007 +0200

    sched: prevent wakeup over-scheduling
    
    Prevent wakeup over-scheduling.  Once a task has been preempted by a
    task of the same or lower priority, it becomes ineligible for repeated
    preemption by same until it has been ticked, or slept.  Instead, the
    task is marked for preemption at the next tick.  Tasks of higher
    priority still preempt immediately.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 04233c8974d9..8be5b57768c0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -912,6 +912,7 @@ struct sched_entity {
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;
 	unsigned int		on_rq;
+	int			peer_preempt;
 
 	u64			exec_start;
 	u64			sum_exec_runtime;

commit 5cb350baf580017da38199625b7365b1763d7180
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:14 2007 +0200

    sched: group scheduling, sysfs tunables
    
    Add tunables in sysfs to modify a user's cpu share.
    
    A directory is created in sysfs for each new user in the system.
    
            /sys/kernel/uids/<uid>/cpu_share
    
    Reading this file returns the cpu shares granted for the user.
    Writing into this file modifies the cpu share for the user. Only an
    administrator is allowed to modify a user's cpu share.
    
    Ex:
            # cd /sys/kernel/uids/
            # cat 512/cpu_share
            1024
            # echo 2048 > 512/cpu_share
            # cat 512/cpu_share
            2048
            #
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3cddbfc0c91d..04233c8974d9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -87,6 +87,7 @@ struct sched_param {
 #include <linux/timer.h>
 #include <linux/hrtimer.h>
 #include <linux/task_io_accounting.h>
+#include <linux/kobject.h>
 
 #include <asm/processor.h>
 
@@ -599,9 +600,18 @@ struct user_struct {
 
 #ifdef CONFIG_FAIR_USER_SCHED
 	struct task_group *tg;
+	struct kset kset;
+	struct subsys_attribute user_attr;
+	struct work_struct work;
 #endif
 };
 
+#ifdef CONFIG_FAIR_USER_SCHED
+extern int uids_kobject_init(void);
+#else
+static inline int uids_kobject_init(void) { return 0; }
+#endif
+
 extern struct user_struct *find_user(uid_t);
 
 extern struct user_struct root_user;
@@ -1848,6 +1858,7 @@ extern struct task_group *sched_create_group(void);
 extern void sched_destroy_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
+extern unsigned long sched_group_shares(struct task_group *tg);
 
 #endif
 

commit 4cf86d77f5942336e7cd9de874b38b3c83b54d5e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:14 2007 +0200

    sched: cleanup: rename task_grp to task_group
    
    cleanup: rename task_grp to task_group. No need to save two characters
    and 'grp' is annoying to read.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 49c7b374eac8..3cddbfc0c91d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -136,7 +136,7 @@ extern unsigned long weighted_cpuload(const int cpu);
 
 struct seq_file;
 struct cfs_rq;
-struct task_grp;
+struct task_group;
 #ifdef CONFIG_SCHED_DEBUG
 extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
 extern void proc_sched_set_task(struct task_struct *p);
@@ -598,7 +598,7 @@ struct user_struct {
 	uid_t uid;
 
 #ifdef CONFIG_FAIR_USER_SCHED
-	struct task_grp *tg;
+	struct task_group *tg;
 #endif
 };
 
@@ -1842,12 +1842,12 @@ extern void normalize_rt_tasks(void);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 
-extern struct task_grp init_task_grp;
+extern struct task_group init_task_group;
 
-extern struct task_grp *sched_create_group(void);
-extern void sched_destroy_group(struct task_grp *tg);
+extern struct task_group *sched_create_group(void);
+extern void sched_destroy_group(struct task_group *tg);
 extern void sched_move_task(struct task_struct *tsk);
-extern int sched_group_set_shares(struct task_grp *tg, unsigned long shares);
+extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 
 #endif
 

commit af92723262f3e0c431083f668b605a1dcdbe8f3d
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Oct 15 17:00:13 2007 +0200

    sched: cleanup, remove the TASK_NONINTERACTIVE flag
    
    Here's another piece of low hanging obsolete fruit.
    
    Remove obsolete TASK_NONINTERACTIVE.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 47e3717a0356..49c7b374eac8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -175,8 +175,7 @@ print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #define EXIT_ZOMBIE		16
 #define EXIT_DEAD		32
 /* in tsk->state again */
-#define TASK_NONINTERACTIVE	64
-#define TASK_DEAD		128
+#define TASK_DEAD		64
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit 5522d5d5f70005faeffff3ffc0cfa8eec0155de4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:12 2007 +0200

    sched: mark scheduling classes as const
    
    mark scheduling classes as const. The speeds up the code
    a bit and shrinks it:
    
       text    data     bss     dec     hex filename
      40027    4018     292   44337    ad31 sched.o.before
      40190    3842     292   44324    ad24 sched.o.after
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 97f736b749c2..47e3717a0356 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -863,7 +863,7 @@ struct rq;
 struct sched_domain;
 
 struct sched_class {
-	struct sched_class *next;
+	const struct sched_class *next;
 
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
@@ -949,7 +949,7 @@ struct task_struct {
 
 	int prio, static_prio, normal_prio;
 	struct list_head run_list;
-	struct sched_class *sched_class;
+	const struct sched_class *sched_class;
 	struct sched_entity se;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS

commit 5f6d858ecca78f71755859a346d845e302973cd1
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Oct 15 17:00:12 2007 +0200

    sched: speed up and simplify vslice calculations
    
    speed up and simplify vslice calculations.
    
    [ From: Mike Galbraith <efault@gmx.de>: build fix ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d5daca4bcc6b..97f736b749c2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1400,7 +1400,7 @@ extern void sched_idle_next(void);
 
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
-extern unsigned int sysctl_sched_min_granularity;
+extern unsigned int sysctl_sched_nr_latency;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;

commit 2d72376b3af1e7d4d4515ebfd0f4383f2e92c343
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:12 2007 +0200

    sched: clean up schedstats, cnt -> count
    
    rename all 'cnt' fields and variables to the less yucky 'count' name.
    
    yuckage noticed by Andrew Morton.
    
    no change in code, other than the /proc/sched_debug bkl_count string got
    a bit larger:
    
       text    data     bss     dec     hex filename
      38236    3506      24   41766    a326 sched.o.before
      38240    3506      24   41770    a32a sched.o.after
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c33227b0f82..d5daca4bcc6b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -614,7 +614,7 @@ struct reclaim_state;
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 struct sched_info {
 	/* cumulative counters */
-	unsigned long pcnt;	      /* # of times run on this cpu */
+	unsigned long pcount;	      /* # of times run on this cpu */
 	unsigned long long cpu_time,  /* time spent on the cpu */
 			   run_delay; /* time spent waiting on a runqueue */
 
@@ -623,7 +623,7 @@ struct sched_info {
 			   last_queued;	/* when we were last queued to run */
 #ifdef CONFIG_SCHEDSTATS
 	/* BKL stats */
-	unsigned long bkl_cnt;
+	unsigned long bkl_count;
 #endif
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
@@ -759,7 +759,7 @@ struct sched_domain {
 
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */
-	unsigned long lb_cnt[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_count[CPU_MAX_IDLE_TYPES];
 	unsigned long lb_failed[CPU_MAX_IDLE_TYPES];
 	unsigned long lb_balanced[CPU_MAX_IDLE_TYPES];
 	unsigned long lb_imbalance[CPU_MAX_IDLE_TYPES];
@@ -769,17 +769,17 @@ struct sched_domain {
 	unsigned long lb_nobusyq[CPU_MAX_IDLE_TYPES];
 
 	/* Active load balancing */
-	unsigned long alb_cnt;
+	unsigned long alb_count;
 	unsigned long alb_failed;
 	unsigned long alb_pushed;
 
 	/* SD_BALANCE_EXEC stats */
-	unsigned long sbe_cnt;
+	unsigned long sbe_count;
 	unsigned long sbe_balanced;
 	unsigned long sbe_pushed;
 
 	/* SD_BALANCE_FORK stats */
-	unsigned long sbf_cnt;
+	unsigned long sbf_count;
 	unsigned long sbf_balanced;
 	unsigned long sbf_pushed;
 

commit 94359f05cb7e1fed0deccc83ebc30a1175a9ae16
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:11 2007 +0200

    sched: undo some of the recent changes
    
    undo some of the recent changes that are not needed after all,
    such as last_min_vruntime.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d74830cc51eb..2c33227b0f82 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -908,7 +908,6 @@ struct sched_entity {
 	u64			sum_exec_runtime;
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
-	u64			last_min_vruntime;
 
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;

commit 67e9fb2a39a1d454218d50383094940982be138f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Oct 15 17:00:10 2007 +0200

    sched: add vslice
    
    add vslice: the load-dependent "virtual slice" a task should
    run ideally, so that the observed latency stays within the
    sched_latency window.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c33227b0f82..d74830cc51eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -908,6 +908,7 @@ struct sched_entity {
 	u64			sum_exec_runtime;
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
+	u64			last_min_vruntime;
 
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;

commit c18b8a7cbcbac46497ee1ce656b0e68197c7581d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:10 2007 +0200

    sched: remove unneeded tunables
    
    remove unneeded tunables.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 920eb7354d0a..2c33227b0f82 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1403,8 +1403,6 @@ extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
-extern unsigned int sysctl_sched_stat_granularity;
-extern unsigned int sysctl_sched_runtime_limit;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 #endif

commit b8efb56172bc55082b8490778b07ef73eea0b551
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:10 2007 +0200

    sched debug: BKL usage statistics
    
    add per task and per rq BKL usage statistics.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d0cc58311b13..920eb7354d0a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -621,6 +621,10 @@ struct sched_info {
 	/* timestamps */
 	unsigned long long last_arrival,/* when we last ran on a cpu */
 			   last_queued;	/* when we were last queued to run */
+#ifdef CONFIG_SCHEDSTATS
+	/* BKL stats */
+	unsigned long bkl_cnt;
+#endif
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 

commit 24e377a83220ef05c9b5bec7e01d65eed6609aa6
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:09 2007 +0200

    sched: add fair-user scheduler
    
    Enable user-id based fair group scheduling. This is useful for anyone
    who wants to test the group scheduler w/o having to enable
    CONFIG_CGROUPS.
    
    A separate scheduling group (i.e struct task_grp) is automatically created for
    every new user added to the system. Upon uid change for a task, it is made to
    move to the corresponding scheduling group.
    
    A /proc tunable (/proc/root_user_share) is also provided to tune root
    user's quota of cpu bandwidth.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 03c13b663e4b..d0cc58311b13 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -597,6 +597,10 @@ struct user_struct {
 	/* Hash table maintenance information */
 	struct hlist_node uidhash_node;
 	uid_t uid;
+
+#ifdef CONFIG_FAIR_USER_SCHED
+	struct task_grp *tg;
+#endif
 };
 
 extern struct user_struct *find_user(uid_t);

commit 9b5b77512dce239fa168183fa71896712232e95a
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:09 2007 +0200

    sched: clean up code under CONFIG_FAIR_GROUP_SCHED
    
    With the view of supporting user-id based fair scheduling (and not just
    container-based fair scheduling), this patch renames several functions
    and makes them independent of whether they are being used for container
    or user-id based fair scheduling.
    
    Also fix a problem reported by KAMEZAWA Hiroyuki (wrt allocating
    less-sized array for tg->cfs_rq[] and tf->se[]).
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 66169005f008..03c13b663e4b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -136,6 +136,7 @@ extern unsigned long weighted_cpuload(const int cpu);
 
 struct seq_file;
 struct cfs_rq;
+struct task_grp;
 #ifdef CONFIG_SCHED_DEBUG
 extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
 extern void proc_sched_set_task(struct task_struct *p);
@@ -1834,6 +1835,17 @@ extern int sched_mc_power_savings, sched_smt_power_savings;
 
 extern void normalize_rt_tasks(void);
 
+#ifdef CONFIG_FAIR_GROUP_SCHED
+
+extern struct task_grp init_task_grp;
+
+extern struct task_grp *sched_create_group(void);
+extern void sched_destroy_group(struct task_grp *tg);
+extern void sched_move_task(struct task_struct *tsk);
+extern int sched_group_set_shares(struct task_grp *tg, unsigned long shares);
+
+#endif
+
 #ifdef CONFIG_TASK_XACCT
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
 {

commit 83b699ed20f5218580a1b7042064082e2e05f8c5
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:08 2007 +0200

    sched: revert recent removal of set_curr_task()
    
    Revert removal of set_curr_task.
    Use put_prev_task/set_curr_task when changing groups/policies
    
    Signed-off-by: Srivatsa Vaddagiri < vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index abcb02738d95..66169005f008 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -871,6 +871,7 @@ struct sched_class {
 			struct sched_domain *sd, enum cpu_idle_type idle,
 			int *all_pinned, int *this_best_prio);
 
+	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
 };

commit f6b53205e17c8ca481c69ed579a35a650a4b481a
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Mon Oct 15 17:00:08 2007 +0200

    sched: rework enqueue/dequeue_entity() to get rid of set_curr_task()
    
    rework enqueue/dequeue_entity() to get rid of
    sched_class::set_curr_task(). This simplifies sched_setscheduler(),
    rt_mutex_setprio() and sched_move_tasks().
    
       text    data     bss     dec     hex filename
      24330    2734      20   27084    69cc sched.o.before
      24233    2730      20   26983    6967 sched.o.after
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 66169005f008..abcb02738d95 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -871,7 +871,6 @@ struct sched_class {
 			struct sched_domain *sd, enum cpu_idle_type idle,
 			int *all_pinned, int *this_best_prio);
 
-	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
 	void (*task_new) (struct rq *rq, struct task_struct *p);
 };

commit 4530d7ab0fb8d5056b68c376949e2d5c4db7817e
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Mon Oct 15 17:00:08 2007 +0200

    sched: simplify sched_class::yield_task()
    
    the 'p' (task_struct) parameter in the sched_class :: yield_task() is
    redundant as the caller is always the 'current'. Get rid of it.
    
       text    data     bss     dec     hex filename
      24341    2734      20   27095    69d7 sched.o.before
      24330    2734      20   27084    69cc sched.o.after
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f776a30b403e..66169005f008 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -858,7 +858,7 @@ struct sched_class {
 
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
-	void (*yield_task) (struct rq *rq, struct task_struct *p);
+	void (*yield_task) (struct rq *rq);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
 

commit 30cfdcfc5f180fc21a3dad6ae3b7b2a9ee112186
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Mon Oct 15 17:00:07 2007 +0200

    sched: do not keep current in the tree and get rid of sched_entity::fair_key
    
    Get rid of 'sched_entity::fair_key'.
    
    As a side effect, 'current' is not kept withing the tree for
    SCHED_NORMAL/BATCH tasks anymore. This simplifies some parts of code
    (e.g. entity_tick() and yield_task_fair()) and also somewhat optimizes
    them (e.g. a single update_curr() now vs. dequeue/enqueue() before in
    entity_tick()).
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 572df1bbaeec..f776a30b403e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -891,7 +891,6 @@ struct load_weight {
  *     6 se->load.weight
  */
 struct sched_entity {
-	s64			fair_key;
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;
 	unsigned int		on_rq;

commit bbdba7c0e1161934ae881ad00e4db49830f5ef59
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:06 2007 +0200

    sched: remove wait_runtime fields and features
    
    remove wait_runtime based fields and features, now that the CFS
    math has been changed over to the vruntime metric.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 353630d6ae4b..572df1bbaeec 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -888,13 +888,9 @@ struct load_weight {
  *     4 se->block_start
  *     4 se->run_node
  *     4 se->sleep_start
- *     4 se->sleep_start_fair
  *     6 se->load.weight
- *     7 se->delta_fair
- *    15 se->wait_runtime
  */
 struct sched_entity {
-	long			wait_runtime;
 	s64			fair_key;
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;
@@ -904,12 +900,10 @@ struct sched_entity {
 	u64			sum_exec_runtime;
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
-	u64			wait_start_fair;
 
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
 	u64			wait_max;
-	s64			sum_wait_runtime;
 
 	u64			sleep_start;
 	u64			sleep_max;
@@ -919,9 +913,6 @@ struct sched_entity {
 	u64			block_max;
 	u64			exec_max;
 	u64			slice_max;
-
-	unsigned long		wait_runtime_overruns;
-	unsigned long		wait_runtime_underruns;
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit e22f5bbf86d8cce710d5c8ba5bf57832e73aab8c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:06 2007 +0200

    sched: remove wait_runtime limit
    
    remove the wait_runtime-limit fields and the code depending on it, now
    that the math has been changed over to rely on the vruntime metric.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e5c457fba86..353630d6ae4b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -905,7 +905,6 @@ struct sched_entity {
 	u64			vruntime;
 	u64			prev_sum_exec_runtime;
 	u64			wait_start_fair;
-	u64			sleep_start_fair;
 
 #ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;

commit e9acbff6484df51fd880e0f5fe0224e8be34c17b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:04 2007 +0200

    sched: introduce se->vruntime
    
    introduce se->vruntime as a sum of weighted delta-exec's, and use that
    as the key into the tree.
    
    the idea to use absolute virtual time as the basic metric of scheduling
    has been first raised by William Lee Irwin, advanced by Tong Li and first
    prototyped by Roman Zippel in the "Really Fair Scheduler" (RFS) patchset.
    
    also see:
    
       http://lkml.org/lkml/2007/9/2/76
    
    for a simpler variant of this patch.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3c38a5040e8f..5e5c457fba86 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -902,6 +902,7 @@ struct sched_entity {
 
 	u64			exec_start;
 	u64			sum_exec_runtime;
+	u64			vruntime;
 	u64			prev_sum_exec_runtime;
 	u64			wait_start_fair;
 	u64			sleep_start_fair;

commit 8ebc91d93669af39dbed50914d7daf457eeb43be
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:03 2007 +0200

    sched: remove stat_gran
    
    remove the stat_gran code - it was disabled by default and it causes
    unnecessary overhead.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index befca3f9364a..3c38a5040e8f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -895,9 +895,6 @@ struct load_weight {
  */
 struct sched_entity {
 	long			wait_runtime;
-	unsigned long		delta_fair_run;
-	unsigned long		delta_fair_sleep;
-	unsigned long		delta_exec;
 	s64			fair_key;
 	struct load_weight	load;		/* for load-balancing */
 	struct rb_node		run_node;

commit 2bd8e6d422a4f44c0994f909317eba80b0fe08a1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:02 2007 +0200

    sched: use constants if !CONFIG_SCHED_DEBUG
    
    use constants if !CONFIG_SCHED_DEBUG.
    
    this speeds up the code and reduces code-size:
    
        text    data     bss     dec     hex filename
       27464    3014      16   30494    771e sched.o.before
       26929    3010      20   29959    7507 sched.o.after
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9761b165d563..befca3f9364a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1402,15 +1402,18 @@ static inline void idle_task_exit(void) {}
 
 extern void sched_idle_next(void);
 
+#ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_stat_granularity;
 extern unsigned int sysctl_sched_runtime_limit;
-extern unsigned int sysctl_sched_compat_yield;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
+#endif
+
+extern unsigned int sysctl_sched_compat_yield;
 
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);

commit eba1ed4b7e52720e3099325874811c38a5ec1562
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:02 2007 +0200

    sched: debug: track maximum 'slice'
    
    track the maximum amount of time a task has executed while
    the CPU load was at least 2x. (i.e. at least two nice-0
    tasks were runnable)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 833f7dc2b8de..9761b165d563 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -921,6 +921,7 @@ struct sched_entity {
 	u64			block_start;
 	u64			block_max;
 	u64			exec_max;
+	u64			slice_max;
 
 	unsigned long		wait_runtime_overruns;
 	unsigned long		wait_runtime_underruns;

commit 169e36742572934f5d846cfa5f9d76e72d505db4
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Sep 27 17:10:06 2007 -0700

    [NETNS]: CLONE_NEWNET don't use the same clone flag as the pid namespace.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a4a141055c44..833f7dc2b8de 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -27,7 +27,7 @@
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 #define CLONE_NEWIPC		0x08000000	/* New ipcs */
 #define CLONE_NEWUSER		0x10000000	/* New user namespace */
-#define CLONE_NEWNET		0x20000000	/* New network namespace */
+#define CLONE_NEWNET		0x40000000	/* New network namespace */
 
 /*
  * Scheduling policies

commit 9dd776b6d7b0b85966b6ddd03e2b2aae59012ab1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Sep 26 22:04:26 2007 -0700

    [NET]: Add network namespace clone & unshare support.
    
    This patch allows you to create a new network namespace
    using sys_clone, or sys_unshare.
    
    As the network namespace is still experimental and under development
    clone and unshare support is only made available when CONFIG_NET_NS is
    selected at compile time.
    
    As this patch introduces network namespace support into code paths
    that exist when the CONFIG_NET is not selected there are a few
    additions made to net_namespace.h to allow a few more functions
    to be used when the networking stack is not compiled in.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 313c6b6e774f..a4a141055c44 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -27,6 +27,7 @@
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 #define CLONE_NEWIPC		0x08000000	/* New ipcs */
 #define CLONE_NEWUSER		0x10000000	/* New user namespace */
+#define CLONE_NEWNET		0x20000000	/* New network namespace */
 
 /*
  * Scheduling policies

commit 0c2043abefacac97b6d01129c1123a466c95b7c1
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Sun Oct 7 16:17:38 2007 -0700

    Don't do load-average calculations at even 5-second intervals
    
    It turns out that there are a few other five-second timers in the
    kernel, and if the timers get in sync, the load-average can get
    artificially inflated by events that just happen to coincide.
    
    So just offset the load average calculation it by a timer tick.
    
    Noticed by Anders BostrÃ¶m, for whom the coincidence started triggering
    on one of his machines with the JBD jiffies rounding code (JBD is one of
    the subsystems that also end up using a 5-second timer by default).
    
    Tested-by: Anders BostrÃ¶m <anders@bostrom.dyndns.org>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a01ac6dd5f5e..313c6b6e774f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -113,7 +113,7 @@ extern unsigned long avenrun[];		/* Load averages */
 
 #define FSHIFT		11		/* nr of bits of precision */
 #define FIXED_1		(1<<FSHIFT)	/* 1.0 as fixed-point */
-#define LOAD_FREQ	(5*HZ)		/* 5 sec intervals */
+#define LOAD_FREQ	(5*HZ+1)	/* 5 sec intervals */
 #define EXP_1		1884		/* 1/exp(5sec/1min) as fixed-point */
 #define EXP_5		2014		/* 1/exp(5sec/5min) */
 #define EXP_15		2037		/* 1/exp(5sec/15min) */

commit b8fceee17a310f189188599a8fa5e9beaff57eb0
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu Sep 20 12:40:16 2007 -0700

    signalfd simplification
    
    This simplifies signalfd code, by avoiding it to remain attached to the
    sighand during its lifetime.
    
    In this way, the signalfd remain attached to the sighand only during
    poll(2) (and select and epoll) and read(2).  This also allows to remove
    all the custom "tsk == current" checks in kernel/signal.c, since
    dequeue_signal() will only be called by "current".
    
    I think this is also what Ben was suggesting time ago.
    
    The external effect of this, is that a thread can extract only its own
    private signals and the group ones.  I think this is an acceptable
    behaviour, in that those are the signals the thread would be able to
    fetch w/out signalfd.
    
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3de79016f2a6..a01ac6dd5f5e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -438,7 +438,7 @@ struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];
 	spinlock_t		siglock;
-	struct list_head        signalfd_list;
+	wait_queue_head_t	signalfd_wqh;
 };
 
 struct pacct_struct {

commit 1799e35d5baab6e06168b46cc78b968e728ea3d1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Sep 19 23:34:46 2007 +0200

    sched: add /proc/sys/kernel/sched_compat_yield
    
    add /proc/sys/kernel/sched_compat_yield to make sys_sched_yield()
    more agressive, by moving the yielding task to the last position
    in the rbtree.
    
    with sched_compat_yield=0:
    
       PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
      2539 mingo     20   0  1576  252  204 R   50  0.0   0:02.03 loop_yield
      2541 mingo     20   0  1576  244  196 R   50  0.0   0:02.05 loop
    
    with sched_compat_yield=1:
    
       PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
      2584 mingo     20   0  1576  248  196 R   99  0.0   0:52.45 loop
      2582 mingo     20   0  1576  256  204 R    0  0.0   0:00.00 loop_yield
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5445eaec6908..3de79016f2a6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1406,6 +1406,7 @@ extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_stat_granularity;
 extern unsigned int sysctl_sched_runtime_limit;
+extern unsigned int sysctl_sched_compat_yield;
 extern unsigned int sysctl_sched_child_runs_first;
 extern unsigned int sysctl_sched_features;
 

commit 28f300d23674fa01ae747c66ce861d4ee6aebe8c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Tue Sep 18 22:46:45 2007 -0700

    Fix user namespace exiting OOPs
    
    It turned out, that the user namespace is released during the do_exit() in
    exit_task_namespaces(), but the struct user_struct is released only during the
    put_task_struct(), i.e.  MUCH later.
    
    On debug kernels with poisoned slabs this will cause the oops in
    uid_hash_remove() because the head of the chain, which resides inside the
    struct user_namespace, will be already freed and poisoned.
    
    Since the uid hash itself is required only when someone can search it, i.e.
    when the namespace is alive, we can safely unhash all the user_struct-s from
    it during the namespace exiting.  The subsequent free_uid() will complete the
    user_struct destruction.
    
    For example simple program
    
       #include <sched.h>
    
       char stack[2 * 1024 * 1024];
    
       int f(void *foo)
       {
            return 0;
       }
    
       int main(void)
       {
            clone(f, stack + 1 * 1024 * 1024, 0x10000000, 0);
            return 0;
       }
    
    run on kernel with CONFIG_USER_NS turned on will oops the
    kernel immediately.
    
    This was spotted during OpenVZ kernel testing.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Acked-by: "Serge E. Hallyn" <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6239bc2c2baa..5445eaec6908 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1472,6 +1472,7 @@ static inline struct user_struct *get_uid(struct user_struct *u)
 }
 extern void free_uid(struct user_struct *);
 extern void switch_uid(struct user_struct *);
+extern void release_uids(struct user_namespace *ns);
 
 #include <asm/current.h>
 

commit 735de2230f09741077a645a913de0a04b10208bf
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Tue Sep 18 22:46:44 2007 -0700

    Convert uid hash to hlist
    
    Surprisingly, but (spotted by Alexey Dobriyan) the uid hash still uses
    list_heads, thus occupying twice as much place as it could.  Convert it to
    hlist_heads.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f4e324ed2e44..6239bc2c2baa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -593,7 +593,7 @@ struct user_struct {
 #endif
 
 	/* Hash table maintenance information */
-	struct list_head uidhash_list;
+	struct hlist_node uidhash_node;
 	uid_t uid;
 };
 

commit f6cf891c4d7128f9f91243fc0b9ce99e10fa1586
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 28 12:53:24 2007 +0200

    sched: make the scheduler converge to the ideal latency
    
    de-HZ-ification of the granularity defaults unearthed a pre-existing
    property of CFS: while it correctly converges to the granularity goal,
    it does not prevent run-time fluctuations in the range of
    [-gran ... 0 ... +gran].
    
    With the increase of the granularity due to the removal of HZ
    dependencies, this becomes visible in chew-max output (with 5 tasks
    running):
    
     out:  28 . 27. 32 | flu:  0 .  0 | ran:    9 .   13 | per:   37 .   40
     out:  27 . 27. 32 | flu:  0 .  0 | ran:   17 .   13 | per:   44 .   40
     out:  27 . 27. 32 | flu:  0 .  0 | ran:    9 .   13 | per:   36 .   40
     out:  29 . 27. 32 | flu:  2 .  0 | ran:   17 .   13 | per:   46 .   40
     out:  28 . 27. 32 | flu:  0 .  0 | ran:    9 .   13 | per:   37 .   40
     out:  29 . 27. 32 | flu:  0 .  0 | ran:   18 .   13 | per:   47 .   40
     out:  28 . 27. 32 | flu:  0 .  0 | ran:    9 .   13 | per:   37 .   40
    
    average slice is the ideal 13 msecs and the period is picture-perfect 40
    msecs. But the 'ran' field fluctuates around 13.33 msecs and there's no
    mechanism in CFS to keep that from happening: it's a perfectly valid
    solution that CFS finds.
    
    to fix this we add a granularity/preemption rule that knows about
    the "target latency", which makes tasks that run longer than the ideal
    latency run a bit less. The simplest approach is to simply decrease the
    preemption granularity when a task overruns its ideal latency. For this
    we have to track how much the task executed since its last preemption.
    
    ( this adds a new field to task_struct, but we can eliminate that
      overhead in 2.6.24 by putting all the scheduler timestamps into an
      anonymous union. )
    
    with this change in place, chew-max output is fluctuation-less all
    around:
    
     out:  28 . 27. 39 | flu:  0 .  2 | ran:   13 .   13 | per:   41 .   40
     out:  28 . 27. 39 | flu:  0 .  2 | ran:   13 .   13 | per:   41 .   40
     out:  28 . 27. 39 | flu:  0 .  2 | ran:   13 .   13 | per:   41 .   40
     out:  28 . 27. 39 | flu:  0 .  2 | ran:   13 .   13 | per:   41 .   40
     out:  28 . 27. 39 | flu:  0 .  1 | ran:   13 .   13 | per:   41 .   40
     out:  28 . 27. 39 | flu:  0 .  1 | ran:   13 .   13 | per:   41 .   40
    
    this patch has no impact on any fastpath or on any globally observable
    scheduling property. (unless you have sharp enough eyes to see
    millisecond-level ruckles in glxgears smoothness :-)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mike Galbraith <efault@gmx.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bd6a0320a770..f4e324ed2e44 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -904,6 +904,7 @@ struct sched_entity {
 
 	u64			exec_start;
 	u64			sum_exec_runtime;
+	u64			prev_sum_exec_runtime;
 	u64			wait_start_fair;
 	u64			sleep_start_fair;
 

commit 172ac3dbb7d3e528ac53d08a34df88d1ac53c534
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Aug 25 18:41:53 2007 +0200

    sched: cleanup, sched_granularity -> sched_min_granularity
    
    due to adaptive granularity scheduling the role of sched_granularity
    has changed to "minimum granularity", so rename the variable (and the
    tunable) accordingly.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 322764e04052..bd6a0320a770 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1400,7 +1400,7 @@ static inline void idle_task_exit(void) {}
 extern void sched_idle_next(void);
 
 extern unsigned int sysctl_sched_latency;
-extern unsigned int sysctl_sched_granularity;
+extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;
 extern unsigned int sysctl_sched_stat_granularity;

commit 218050855ece4e923106ab614ac65afa0f618df3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Aug 25 18:41:53 2007 +0200

    sched: adaptive scheduler granularity
    
    Instead of specifying the preemption granularity, specify the wanted
    latency. By fixing the granlarity to a constany the wakeup latency
    it a function of the number of running tasks on the rq.
    
    Invert this relation.
    
    sysctl_sched_granularity becomes a minimum for the dynamic granularity
    computed from the new sysctl_sched_latency.
    
    Then use this latency to do more intelligent granularity decisions: if
    there are fewer tasks running then we can schedule coarser. This helps
    performance while still always keeping the latency target.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ba78807eab91..322764e04052 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1399,6 +1399,7 @@ static inline void idle_task_exit(void) {}
 
 extern void sched_idle_next(void);
 
+extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_batch_wakeup_granularity;

commit f8700df7c419781efb34696de7e7f49717f8ede7
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Aug 23 15:18:02 2007 +0200

    sched: fix broken SMT/MC optimizations
    
    On a four package system with HT - HT load balancing optimizations were
    broken.  For example, if two tasks end up running on two logical threads
    of one of the packages, scheduler is not able to pull one of the tasks
    to a completely idle package.
    
    In this scenario, for nice-0 tasks, imbalance calculated by scheduler
    will be 512 and find_busiest_queue() will return 0 (as each cpu's load
    is 1024 > imbalance and has only one task running).
    
    Similarly MC scheduler optimizations also get fixed with this patch.
    
    [ mingo@elte.hu: restored fair balancing by increasing the fuzz and
                     adding it back to the power decision, without the /2
                     factor. ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1845b2e99a87..ba78807eab91 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -681,7 +681,7 @@ enum cpu_idle_type {
 #define SCHED_LOAD_SHIFT	10
 #define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
 
-#define SCHED_LOAD_SCALE_FUZZ	(SCHED_LOAD_SCALE >> 1)
+#define SCHED_LOAD_SCALE_FUZZ	SCHED_LOAD_SCALE
 
 #ifdef CONFIG_SMP
 #define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */

commit 2aa44d0567ed21b47b87d68819415d48194cb923
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 23 15:18:02 2007 +0200

    sched: sched_clock_idle_[sleep|wakeup]_event()
    
    construct a more or less wall-clock time out of sched_clock(), by
    using ACPI-idle's existing knowledge about how much time we spent
    idling. This allows the rq clock to work around TSC-stops-in-C2,
    TSC-gets-corrupted-in-C3 type of problems.
    
    ( Besides the scheduler's statistics this also benefits blktrace and
      printk-timestamps as well. )
    
    Furthermore, the precise before-C2/C3-sleep and after-C2/C3-wakeup
    callbacks allow the scheduler to get out the most of the period where
    the CPU has a reliable TSC. This results in slightly more precise
    task statistics.
    
    the ACPI bits were acked by Len.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Len Brown <len.brown@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 682ef87da6eb..1845b2e99a87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1388,7 +1388,8 @@ extern void sched_exec(void);
 #define sched_exec()   {}
 #endif
 
-extern void sched_clock_unstable_event(void);
+extern void sched_clock_idle_sleep_event(void);
+extern void sched_clock_idle_wakeup_event(u64 delta_ns);
 
 #ifdef CONFIG_HOTPLUG_CPU
 extern void idle_task_exit(void);

commit ee0827d8b5271094380410cf21d8c48c109a773a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:49 2007 +0200

    sched: remove the 'u64 now' parameter from ->task_new()
    
    remove the 'u64 now' parameter from ->task_new().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9afb66a49358..682ef87da6eb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -872,7 +872,7 @@ struct sched_class {
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
-	void (*task_new) (struct rq *rq, struct task_struct *p, u64 now);
+	void (*task_new) (struct rq *rq, struct task_struct *p);
 };
 
 struct load_weight {

commit 31ee529cc2254e8b62880535ec8f21a4c5e1c091
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:49 2007 +0200

    sched: remove the 'u64 now' parameter from ->put_prev_task()
    
    remove the 'u64 now' parameter from ->put_prev_task().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c6ad4071c791..9afb66a49358 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -862,7 +862,7 @@ struct sched_class {
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
 
 	struct task_struct * (*pick_next_task) (struct rq *rq);
-	void (*put_prev_task) (struct rq *rq, struct task_struct *p, u64 now);
+	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest,

commit fb8d47240246e20f864f0724a23a7220cd1c59ac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:48 2007 +0200

    sched: remove the 'u64 now' parameter from ->pick_next_task()
    
    remove the 'u64 now' parameter from ->pick_next_task().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c7815a6b70e0..c6ad4071c791 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -861,7 +861,7 @@ struct sched_class {
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
 
-	struct task_struct * (*pick_next_task) (struct rq *rq, u64 now);
+	struct task_struct * (*pick_next_task) (struct rq *rq);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p, u64 now);
 
 	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,

commit f02231e51a280f1a0fee4d03ad8f50048e06cced
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:48 2007 +0200

    sched: remove the 'u64 now' parameter from ->dequeue_task()
    
    remove the 'u64 now' parameter from ->dequeue_task().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b11dedfbab6e..c7815a6b70e0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -856,8 +856,7 @@ struct sched_class {
 	struct sched_class *next;
 
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
-	void (*dequeue_task) (struct rq *rq, struct task_struct *p,
-			      int sleep, u64 now);
+	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
 	void (*yield_task) (struct rq *rq, struct task_struct *p);
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);

commit fd390f6a04f22fb457d6fd1855964f79536525de
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:48 2007 +0200

    sched: remove the 'u64 now' parameter from ->enqueue_task()
    
    remove the 'u64 now' parameter from ->enqueue_task().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 62ddddb49db3..b11dedfbab6e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -855,8 +855,7 @@ struct sched_domain;
 struct sched_class {
 	struct sched_class *next;
 
-	void (*enqueue_task) (struct rq *rq, struct task_struct *p,
-			      int wakeup, u64 now);
+	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p,
 			      int sleep, u64 now);
 	void (*yield_task) (struct rq *rq, struct task_struct *p);

commit 5cef9eca3837a8dcf605a360e213c4179a07c41a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 9 11:16:47 2007 +0200

    sched: remove the 'u64 now' parameter from print_cfs_rq()
    
    remove the 'u64 now' parameter from print_cfs_rq().
    
    ( identity transformation that causes no change in functionality. )
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 513b81c60e87..62ddddb49db3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -139,7 +139,7 @@ struct cfs_rq;
 extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
 extern void proc_sched_set_task(struct task_struct *p);
 extern void
-print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq, u64 now);
+print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
 #else
 static inline void
 proc_sched_show_task(struct task_struct *p, struct seq_file *m)
@@ -149,7 +149,7 @@ static inline void proc_sched_set_task(struct task_struct *p)
 {
 }
 static inline void
-print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq, u64 now)
+print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 {
 }
 #endif

commit a4ac01c36e286dd1b9a1d5cd7422c5af51dc55f8
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Thu Aug 9 11:16:46 2007 +0200

    sched: fix bug in balance_tasks()
    
    There are two problems with balance_tasks() and how it used:
    
    1. The variables best_prio and best_prio_seen (inherited from the old
    move_tasks()) were only required to handle problems caused by the
    active/expired arrays, the order in which they were processed and the
    possibility that the task with the highest priority could be on either.
      These issues are no longer present and the extra overhead associated
    with their use is unnecessary (and possibly wrong).
    
    2. In the absence of CONFIG_FAIR_GROUP_SCHED being set, the same
    this_best_prio variable needs to be used by all scheduling classes or
    there is a risk of moving too much load.  E.g. if the highest priority
    task on this at the beginning is a fairly low priority task and the rt
    class migrates a task (during its turn) then that moved task becomes the
    new highest priority task on this_rq but when the sched_fair class
    initializes its copy of this_best_prio it will get the priority of the
    original highest priority task as, due to the run queue locks being
    held, the reschedule triggered by pull_task() will not have taken place.
      This could result in inappropriate overriding of skip_for_load and
    excessive load being moved.
    
    The attached patch addresses these problems by deleting all reference to
    best_prio and best_prio_seen and making this_best_prio a reference
    parameter to the various functions involved.
    
    load_balance_fair() has also been modified so that this_best_prio is
    only reset (in the loop) if CONFIG_FAIR_GROUP_SCHED is set.  This should
    preserve the effect of helping spread groups' higher priority tasks
    around the available CPUs while improving system performance when
    CONFIG_FAIR_GROUP_SCHED isn't set.
    
    Signed-off-by: Peter Williams <pwil3058@bigpond.net.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 24bce423f10d..513b81c60e87 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -870,7 +870,7 @@ struct sched_class {
 			struct rq *busiest,
 			unsigned long max_nr_move, unsigned long max_load_move,
 			struct sched_domain *sd, enum cpu_idle_type idle,
-			int *all_pinned);
+			int *all_pinned, int *this_best_prio);
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);

commit 4301065920b0cbde3986519582347e883b166f3e
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Thu Aug 9 11:16:46 2007 +0200

    sched: simplify move_tasks()
    
    The move_tasks() function is currently multiplexed with two distinct
    capabilities:
    
    1. attempt to move a specified amount of weighted load from one run
    queue to another; and
    2. attempt to move a specified number of tasks from one run queue to
    another.
    
    The first of these capabilities is used in two places, load_balance()
    and load_balance_idle(), and in both of these cases the return value of
    move_tasks() is used purely to decide if tasks/load were moved and no
    notice of the actual number of tasks moved is taken.
    
    The second capability is used in exactly one place,
    active_load_balance(), to attempt to move exactly one task and, as
    before, the return value is only used as an indicator of success or failure.
    
    This multiplexing of sched_task() was introduced, by me, as part of the
    smpnice patches and was motivated by the fact that the alternative, one
    function to move specified load and one to move a single task, would
    have led to two functions of roughly the same complexity as the old
    move_tasks() (or the new balance_tasks()).  However, the new modular
    design of the new CFS scheduler allows a simpler solution to be adopted
    and this patch addresses that solution by:
    
    1. adding a new function, move_one_task(), to be used by
    active_load_balance(); and
    2. making move_tasks() a single purpose function that tries to move a
    specified weighted load and returns 1 for success and 0 for failure.
    
    One of the consequences of these changes is that neither move_one_task()
    or the new move_tasks() care how many tasks sched_class.load_balance()
    moves and this enables its interface to be simplified by returning the
    amount of load moved as its result and removing the load_moved pointer
    from the argument list.  This helps simplify the new move_tasks() and
    slightly reduces the amount of work done in each of
    sched_class.load_balance()'s implementations.
    
    Further simplification, e.g. changes to balance_tasks(), are possible
    but (slightly) complicated by the special needs of load_balance_fair()
    so I've left them to a later patch (if this one gets accepted).
    
    NB Since move_tasks() gets called with two run queue locks held even
    small reductions in overhead are worthwhile.
    
    [ mingo@elte.hu ]
    
    this change also reduces code size nicely:
    
       text    data     bss     dec     hex filename
       39216    3618      24   42858    a76a sched.o.before
       39173    3618      24   42815    a73f sched.o.after
    
    Signed-off-by: Peter Williams <pwil3058@bigpond.net.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17249fae5014..24bce423f10d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -866,11 +866,11 @@ struct sched_class {
 	struct task_struct * (*pick_next_task) (struct rq *rq, u64 now);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p, u64 now);
 
-	int (*load_balance) (struct rq *this_rq, int this_cpu,
+	unsigned long (*load_balance) (struct rq *this_rq, int this_cpu,
 			struct rq *busiest,
 			unsigned long max_nr_move, unsigned long max_load_move,
 			struct sched_domain *sd, enum cpu_idle_type idle,
-			int *all_pinned, unsigned long *total_load_moved);
+			int *all_pinned);
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);

commit 94c18227d1e3f02de5b345bd3cd5c960214dc9c8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 2 17:41:40 2007 +0200

    [PATCH] sched: reduce task_struct size
    
    more task_struct size reduction, by moving the debugging/instrumentation
    fields to under CONFIG_SCHEDSTATS:
    
     (i386, nodebug):
    
                              size
                              ----
         pre-CFS              1328
             CFS              1472
             CFS+patch        1376
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c9e0c2a6a950..17249fae5014 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -904,23 +904,28 @@ struct sched_entity {
 	struct rb_node		run_node;
 	unsigned int		on_rq;
 
+	u64			exec_start;
+	u64			sum_exec_runtime;
 	u64			wait_start_fair;
+	u64			sleep_start_fair;
+
+#ifdef CONFIG_SCHEDSTATS
 	u64			wait_start;
-	u64			exec_start;
+	u64			wait_max;
+	s64			sum_wait_runtime;
+
 	u64			sleep_start;
-	u64			sleep_start_fair;
-	u64			block_start;
 	u64			sleep_max;
+	s64			sum_sleep_runtime;
+
+	u64			block_start;
 	u64			block_max;
 	u64			exec_max;
-	u64			wait_max;
-	u64			last_ran;
 
-	u64			sum_exec_runtime;
-	s64			sum_wait_runtime;
-	s64			sum_sleep_runtime;
 	unsigned long		wait_runtime_overruns;
 	unsigned long		wait_runtime_underruns;
+#endif
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	struct sched_entity	*parent;
 	/* rq on which this entity is (to be) queued: */

commit cad60d93e18ba52b6f069b2edb031c89bf603b07
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 2 17:41:40 2007 +0200

    [PATCH] sched: ->task_new cleanup
    
    make sched_class.task_new == NULL a 'default method', this
    allows the removal of task_rt_new.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81eec7e36c84..c9e0c2a6a950 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -874,7 +874,7 @@ struct sched_class {
 
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p);
-	void (*task_new) (struct rq *rq, struct task_struct *p);
+	void (*task_new) (struct rq *rq, struct task_struct *p, u64 now);
 };
 
 struct load_weight {

commit 362a7016637648c6aefc98b706298baedfaa1543
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Aug 2 17:41:40 2007 +0200

    [PATCH] sched: remove cache_hot_time
    
    remove the last unused remains of cache_hot_time.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e490271acf6..81eec7e36c84 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -734,7 +734,6 @@ struct sched_domain {
 	unsigned long max_interval;	/* Maximum balance interval ms */
 	unsigned int busy_factor;	/* less balancing by factor if busy */
 	unsigned int imbalance_pct;	/* No balance until over watermark */
-	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
 	unsigned int busy_idx;
 	unsigned int idle_idx;

commit d02c7a8cf208eb80a3ccbff40a6bebe8902af35a
Author: Con Kolivas <kernel@kolivas.org>
Date:   Thu Jul 26 13:40:43 2007 +0200

    [PATCH] sched: add above_background_load() function
    
    Add an above_background_load() function which can be used by other
    subsystems to detect if there is anything besides niced tasks running.
    
    Place it in sched.h to allow it to be compiled out if not used.
    
    Unused for now, but it is a useful hint to the IO scheduler and to
    swap-prefetch.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Cc: Peter Williams <pwil3058@bigpond.net.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7a4de8768748..2e490271acf6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -786,6 +786,22 @@ extern int partition_sched_domains(cpumask_t *partition1,
 
 #endif	/* CONFIG_SMP */
 
+/*
+ * A runqueue laden with a single nice 0 task scores a weighted_cpuload of
+ * SCHED_LOAD_SCALE. This function returns 1 if any cpu is laden with a
+ * task of nice 0 or enough lower priority tasks to bring up the
+ * weighted_cpuload
+ */
+static inline int above_background_load(void)
+{
+	unsigned long cpu;
+
+	for_each_online_cpu(cpu) {
+		if (weighted_cpuload(cpu) >= SCHED_LOAD_SCALE)
+			return 1;
+	}
+	return 0;
+}
 
 struct io_context;			/* See blkdev.h */
 struct cpuset;

commit e107be36efb2a233833e8c9899039a370e4b2318
Author: Avi Kivity <avi@qumranet.com>
Date:   Thu Jul 26 13:40:43 2007 +0200

    [PATCH] sched: arch preempt notifier mechanism
    
    This adds a general mechanism whereby a task can request the scheduler to
    notify it whenever it is preempted or scheduled back in.  This allows the
    task to swap any special-purpose registers like the fpu or Intel's VT
    registers.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>
    [ mingo@elte.hu: fixes, cleanups ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7c61b50823fa..7a4de8768748 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -935,6 +935,11 @@ struct task_struct {
 	struct sched_class *sched_class;
 	struct sched_entity se;
 
+#ifdef CONFIG_PREEMPT_NOTIFIERS
+	/* list of struct preempt_notifier: */
+	struct hlist_head preempt_notifiers;
+#endif
+
 	unsigned short ioprio;
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;

commit b47e8608a08766ef8121cd747d3aaf6c3dc22649
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 26 13:40:43 2007 +0200

    [PATCH] sched: increase SCHED_LOAD_SCALE_FUZZ
    
    increase SCHED_LOAD_SCALE_FUZZ that adds a small amount of
    over-balancing: to help distribute CPU-bound tasks more fairly on SMP
    systems.
    
    the problem of unfair balancing was noticed and reported by Tong N Li.
    
    10 CPU-bound tasks running on 8 CPUs, v2.6.23-rc1:
    
      PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
     2572 mingo     20   0  1576  244  196 R  100  0.0   1:03.61 loop
     2578 mingo     20   0  1576  248  196 R  100  0.0   1:03.59 loop
     2576 mingo     20   0  1576  248  196 R  100  0.0   1:03.52 loop
     2571 mingo     20   0  1576  244  196 R  100  0.0   1:03.46 loop
     2569 mingo     20   0  1576  244  196 R   99  0.0   1:03.36 loop
     2570 mingo     20   0  1576  244  196 R   95  0.0   1:00.55 loop
     2577 mingo     20   0  1576  248  196 R   50  0.0   0:31.88 loop
     2574 mingo     20   0  1576  248  196 R   50  0.0   0:31.87 loop
     2573 mingo     20   0  1576  248  196 R   50  0.0   0:31.86 loop
     2575 mingo     20   0  1576  248  196 R   50  0.0   0:31.86 loop
    
    v2.6.23-rc1 + patch:
    
      PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
     2681 mingo     20   0  1576  244  196 R   85  0.0   3:51.68 loop
     2688 mingo     20   0  1576  244  196 R   81  0.0   3:46.35 loop
     2682 mingo     20   0  1576  244  196 R   80  0.0   3:43.68 loop
     2685 mingo     20   0  1576  248  196 R   80  0.0   3:45.97 loop
     2683 mingo     20   0  1576  248  196 R   80  0.0   3:40.25 loop
     2679 mingo     20   0  1576  244  196 R   80  0.0   3:33.53 loop
     2680 mingo     20   0  1576  244  196 R   79  0.0   3:43.53 loop
     2686 mingo     20   0  1576  244  196 R   79  0.0   3:39.31 loop
     2687 mingo     20   0  1576  244  196 R   78  0.0   3:33.31 loop
     2684 mingo     20   0  1576  244  196 R   77  0.0   3:27.52 loop
    
    so they now nicely converge to the expected 80% long-term CPU usage.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 33b9b4841ee7..7c61b50823fa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -681,7 +681,7 @@ enum cpu_idle_type {
 #define SCHED_LOAD_SHIFT	10
 #define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
 
-#define SCHED_LOAD_SCALE_FUZZ	(SCHED_LOAD_SCALE >> 5)
+#define SCHED_LOAD_SCALE_FUZZ	(SCHED_LOAD_SCALE >> 1)
 
 #ifdef CONFIG_SMP
 #define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */

commit e436d80085133858bf2613a630365e8a0459fd58
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 19 21:28:35 2007 +0200

    [PATCH] sched: implement cpu_clock(cpu) high-speed time source
    
    Implement the cpu_clock(cpu) interface for kernel-internal use:
    high-speed (but slightly incorrect) per-cpu clock constructed from
    sched_clock().
    
    This API, unused at the moment, will be used in the future by blktrace,
    by the softlockup-watchdog, by printk and by lockstat.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 94f624aef017..33b9b4841ee7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1348,6 +1348,13 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 #endif
 
 extern unsigned long long sched_clock(void);
+
+/*
+ * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
+ * clock constructed from sched_clock():
+ */
+extern unsigned long long cpu_clock(int cpu);
+
 extern unsigned long long
 task_sched_runtime(struct task_struct *task);
 

commit 3cb4a0bb1e773e3c41800b33a3f7dab32bd06c64
Author: Kawai, Hidehiro <hidehiro.kawai.ez@hitachi.com>
Date:   Thu Jul 19 01:48:28 2007 -0700

    coredump masking: add an interface for core dump filter
    
    This patch adds an interface to set/reset flags which determines each memory
    segment should be dumped or not when a core file is generated.
    
    /proc/<pid>/coredump_filter file is provided to access the flags.  You can
    change the flag status for a particular process by writing to or reading from
    the file.
    
    The flag status is inherited to the child process when it is created.
    
    Signed-off-by: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8dbd08366400..94f624aef017 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -349,8 +349,22 @@ extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 
 /* mm flags */
+/* dumpable bits */
 #define MMF_DUMPABLE      0  /* core dump is permitted */
 #define MMF_DUMP_SECURELY 1  /* core file is readable only by root */
+#define MMF_DUMPABLE_BITS 2
+
+/* coredump filter bits */
+#define MMF_DUMP_ANON_PRIVATE	2
+#define MMF_DUMP_ANON_SHARED	3
+#define MMF_DUMP_MAPPED_PRIVATE	4
+#define MMF_DUMP_MAPPED_SHARED	5
+#define MMF_DUMP_FILTER_SHIFT	MMF_DUMPABLE_BITS
+#define MMF_DUMP_FILTER_BITS	4
+#define MMF_DUMP_FILTER_MASK \
+	(((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)
+#define MMF_DUMP_FILTER_DEFAULT \
+	((1 << MMF_DUMP_ANON_PRIVATE) |	(1 << MMF_DUMP_ANON_SHARED))
 
 struct mm_struct {
 	struct vm_area_struct * mmap;		/* list of VMAs */

commit 6c5d523826dc639df709ed0f88c5d2ce25379652
Author: Kawai, Hidehiro <hidehiro.kawai.ez@hitachi.com>
Date:   Thu Jul 19 01:48:27 2007 -0700

    coredump masking: reimplementation of dumpable using two flags
    
    This patch changes mm_struct.dumpable to a pair of bit flags.
    
    set_dumpable() converts three-value dumpable to two flags and stores it into
    lower two bits of mm_struct.flags instead of mm_struct.dumpable.
    get_dumpable() behaves in the opposite way.
    
    [akpm@linux-foundation.org: export set_dumpable]
    Signed-off-by: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 731edaca8ffd..8dbd08366400 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -345,6 +345,13 @@ typedef unsigned long mm_counter_t;
 		(mm)->hiwater_vm = (mm)->total_vm;	\
 } while (0)
 
+extern void set_dumpable(struct mm_struct *mm, int value);
+extern int get_dumpable(struct mm_struct *mm);
+
+/* mm flags */
+#define MMF_DUMPABLE      0  /* core dump is permitted */
+#define MMF_DUMP_SECURELY 1  /* core file is readable only by root */
+
 struct mm_struct {
 	struct vm_area_struct * mmap;		/* list of VMAs */
 	struct rb_root mm_rb;
@@ -402,7 +409,7 @@ struct mm_struct {
 	unsigned int token_priority;
 	unsigned int last_interval;
 
-	unsigned char dumpable:2;
+	unsigned long flags; /* Must use atomic bitops to access the bits */
 
 	/* coredumping support */
 	int core_waiters;

commit 77ec739d8d0979477fc91f530403805afa2581a4
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Sun Jul 15 23:41:01 2007 -0700

    user namespace: add unshare
    
    This patch enables the unshare of user namespaces.
    
    It adds a new clone flag CLONE_NEWUSER and implements copy_user_ns() which
    resets the current user_struct and adds a new root user (uid == 0)
    
    For now, unsharing the user namespace allows a process to reset its
    user_struct accounting and uid 0 in the new user namespace should be contained
    using appropriate means, for instance selinux
    
    The plan, when the full support is complete (all uid checks covered), is to
    keep the original user's rights in the original namespace, and let a process
    become uid 0 in the new namespace, with full capabilities to the new
    namespace.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c667255d70db..731edaca8ffd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -26,6 +26,7 @@
 #define CLONE_STOPPED		0x02000000	/* Start in stopped state */
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 #define CLONE_NEWIPC		0x08000000	/* New ipcs */
+#define CLONE_NEWUSER		0x10000000	/* New user namespace */
 
 /*
  * Scheduling policies

commit acce292c82d4d82d35553b928df2b0597c3a9c78
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Sun Jul 15 23:40:59 2007 -0700

    user namespace: add the framework
    
    Basically, it will allow a process to unshare its user_struct table,
    resetting at the same time its own user_struct and all the associated
    accounting.
    
    A new root user (uid == 0) is added to the user namespace upon creation.
    Such root users have full privileges and it seems that theses privileges
    should be controlled through some means (process capabilities ?)
    
    The unshare is not included in this patch.
    
    Changes since [try #4]:
            - Updated get_user_ns and put_user_ns to accept NULL, and
              get_user_ns to return the namespace.
    
    Changes since [try #3]:
            - moved struct user_namespace to files user_namespace.{c,h}
    
    Changes since [try #2]:
            - removed struct user_namespace* argument from find_user()
    
    Changes since [try #1]:
            - removed struct user_namespace* argument from find_user()
            - added a root_user per user namespace
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b579624477f4..c667255d70db 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -287,6 +287,7 @@ extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 
 struct nsproxy;
+struct user_namespace;
 
 /* Maximum number of active map areas.. This is a random (large) number */
 #define DEFAULT_MAX_MAP_COUNT	65536
@@ -1408,7 +1409,7 @@ extern struct task_struct *find_task_by_pid_type(int type, int pid);
 extern void __set_special_pids(pid_t session, pid_t pgrp);
 
 /* per-UID process charging. */
-extern struct user_struct * alloc_uid(uid_t);
+extern struct user_struct * alloc_uid(struct user_namespace *, uid_t);
 static inline struct user_struct *get_uid(struct user_struct *u)
 {
 	atomic_inc(&u->__count);

commit 522ed7767e800cff6c650ec64b0ee0677303119c
Author: Miloslav Trmac <mitr@redhat.com>
Date:   Sun Jul 15 23:40:56 2007 -0700

    Audit: add TTY input auditing
    
    Add TTY input auditing, used to audit system administrator's actions.  This is
    required by various security standards such as DCID 6/3 and PCI to provide
    non-repudiation of administrator's actions and to allow a review of past
    actions if the administrator seems to overstep their duties or if the system
    becomes misconfigured for unknown reasons.  These requirements do not make it
    necessary to audit TTY output as well.
    
    Compared to an user-space keylogger, this approach records TTY input using the
    audit subsystem, correlated with other audit events, and it is completely
    transparent to the user-space application (e.g.  the console ioctls still
    work).
    
    TTY input auditing works on a higher level than auditing all system calls
    within the session, which would produce an overwhelming amount of mostly
    useless audit events.
    
    Add an "audit_tty" attribute, inherited across fork ().  Data read from TTYs
    by process with the attribute is sent to the audit subsystem by the kernel.
    The audit netlink interface is extended to allow modifying the audit_tty
    attribute, and to allow sending explanatory audit events from user-space (for
    example, a shell might send an event containing the final command, after the
    interactive command-line editing and history expansion is performed, which
    might be difficult to decipher from the TTY input alone).
    
    Because the "audit_tty" attribute is inherited across fork (), it would be set
    e.g.  for sshd restarted within an audited session.  To prevent this, the
    audit_tty attribute is cleared when a process with no open TTY file
    descriptors (e.g.  after daemon startup) opens a TTY.
    
    See https://www.redhat.com/archives/linux-audit/2007-June/msg00000.html for a
    more detailed rationale document for an older version of this patch.
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Miloslav Trmac <mitr@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Paul Fulghum <paulkf@microgate.com>
    Cc: Casey Schaufler <casey@schaufler-ca.com>
    Cc: Steve Grubb <sgrubb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3cffc1204663..b579624477f4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -529,6 +529,10 @@ struct signal_struct {
 #ifdef CONFIG_TASKSTATS
 	struct taskstats *stats;
 #endif
+#ifdef CONFIG_AUDIT
+	unsigned audit_tty;
+	struct tty_audit_buf *tty_audit_buf;
+#endif
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */

commit 924b42d5a2dbe508407a0a6290d3751f826bccdd
Author: Tomas Janousek <tjanouse@redhat.com>
Date:   Sun Jul 15 23:39:42 2007 -0700

    Use boot based time for process start time and boot time in /proc
    
    Commit 411187fb05cd11676b0979d9fbf3291db69dbce2 caused boot time to move and
    process start times to become invalid after suspend.  Using boot based time
    for those restores the old behaviour and fixes the issue.
    
    [akpm@linux-foundation.org: little cleanup]
    Signed-off-by: Tomas Janousek <tjanouse@redhat.com>
    Cc: Tomas Smetana <tsmetana@redhat.com>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cfb680585ab8..3cffc1204663 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -972,7 +972,8 @@ struct task_struct {
 	unsigned int rt_priority;
 	cputime_t utime, stime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
-	struct timespec start_time;
+	struct timespec start_time; 		/* monotonic time */
+	struct timespec real_start_time;	/* boot based time */
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 

commit 6fb43d7b50e49a36f8be3199141bec473e5ecb00
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:01 2007 +0200

    sched: micro-optimize mmdrop()
    
    micro-optimize mmdrop(). Improves schedule()'s assembly a bit.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 785ec8465bd3..cfb680585ab8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1514,7 +1514,7 @@ extern struct mm_struct * mm_alloc(void);
 extern void FASTCALL(__mmdrop(struct mm_struct *));
 static inline void mmdrop(struct mm_struct * mm)
 {
-	if (atomic_dec_and_test(&mm->mm_count))
+	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 		__mmdrop(mm);
 }
 

commit 43ae34cb4cd650d1eb4460a8253a8e747ba052ac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: scheduler debugging, core
    
    scheduler debugging core: implement /proc/sched_debug and
    /proc/<PID>/sched files for scheduler debugging.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c9d65738bb7a..785ec8465bd3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -132,6 +132,26 @@ extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long weighted_cpuload(const int cpu);
 
+struct seq_file;
+struct cfs_rq;
+#ifdef CONFIG_SCHED_DEBUG
+extern void proc_sched_show_task(struct task_struct *p, struct seq_file *m);
+extern void proc_sched_set_task(struct task_struct *p);
+extern void
+print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq, u64 now);
+#else
+static inline void
+proc_sched_show_task(struct task_struct *p, struct seq_file *m)
+{
+}
+static inline void proc_sched_set_task(struct task_struct *p)
+{
+}
+static inline void
+print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq, u64 now)
+{
+}
+#endif
 
 /*
  * Task state bitmask. NOTE! These bits are also

commit 7dd593608df3f9d4e4531cfe29f28c3a3766a0ee
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: remove old cpu accounting field
    
    remove the old cpu-accounting field from signal_struct, now
    that the code is using CFS's stats.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aa582be8cafa..c9d65738bb7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -482,7 +482,6 @@ struct signal_struct {
 	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
 	 * other than jiffies.)
 	 */
-	unsigned long sched_time;
 	unsigned long long sum_sched_runtime;
 
 	/*

commit 0c57d5893e4a9857ff22ec9e379f6bdbdad50850
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: remove batch_task()
    
    batch_task() in sched.h is now unused - remove it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index efa3beb007ff..aa582be8cafa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1157,11 +1157,6 @@ static inline int rt_task(struct task_struct *p)
 	return rt_prio(p->prio);
 }
 
-static inline int batch_task(struct task_struct *p)
-{
-	return p->policy == SCHED_BATCH;
-}
-
 static inline pid_t process_group(struct task_struct *tsk)
 {
 	return tsk->signal->pgrp;

commit 50e645a8a1a91f57dd5d8454620be5f1cb0fc089
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: remove interactivity types from sched.h
    
    remove now-unused types/fields used by the old scheduler.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ce0c5adc9eb0..efa3beb007ff 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -788,7 +788,6 @@ struct mempolicy;
 struct pipe_inode_info;
 struct uts_namespace;
 
-struct prio_array;
 struct rq;
 struct sched_domain;
 
@@ -884,10 +883,9 @@ struct task_struct {
 	int oncpu;
 #endif
 #endif
-	int load_weight;	/* for niceness load balancing purposes */
+
 	int prio, static_prio, normal_prio;
 	struct list_head run_list;
-	struct prio_array *array;
 	struct sched_class *sched_class;
 	struct sched_entity se;
 
@@ -895,13 +893,10 @@ struct task_struct {
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
 #endif
-	unsigned long sleep_avg;
-	unsigned long long timestamp, last_ran;
-	unsigned long long sched_time; /* sched_clock time spent running */
 
 	unsigned int policy;
 	cpumask_t cpus_allowed;
-	unsigned int time_slice, first_time_slice;
+	unsigned int time_slice;
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;

commit ad46c2c4ebcead75cd364a79b63b134393094fb9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: clean up fastcall uses of sched_fork()/sched_exit()
    
    sched_fork()/sched_exit() does not need to specify fastcall anymore,
    as the x86 kernel defaults to regparm3, and no assembly code calls
    these functions.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e64dbd4cd829..ce0c5adc9eb0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1416,8 +1416,8 @@ extern void FASTCALL(wake_up_new_task(struct task_struct * tsk,
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void FASTCALL(sched_fork(struct task_struct * p, int clone_flags));
-extern void FASTCALL(sched_exit(struct task_struct * p));
+extern void sched_fork(struct task_struct *p, int clone_flags);
+extern void sched_dead(struct task_struct *p);
 
 extern int in_group_p(gid_t);
 extern int in_egroup_p(gid_t);

commit 172ba844a8851c3edd13c0a979cdf46bd5e3cc1a
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: update delay-accounting to use CFS's precise stats
    
    update delay-accounting to use CFS's precise stats.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fa895b309da0..e64dbd4cd829 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -562,13 +562,13 @@ struct reclaim_state;
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 struct sched_info {
 	/* cumulative counters */
-	unsigned long	cpu_time,	/* time spent on the cpu */
-			run_delay,	/* time spent waiting on a runqueue */
-			pcnt;		/* # of timeslices run on this cpu */
+	unsigned long pcnt;	      /* # of times run on this cpu */
+	unsigned long long cpu_time,  /* time spent on the cpu */
+			   run_delay; /* time spent waiting on a runqueue */
 
 	/* timestamps */
-	unsigned long	last_arrival,	/* when we last ran on a cpu */
-			last_queued;	/* when we were last queued to run */
+	unsigned long long last_arrival,/* when we last ran on a cpu */
+			   last_queued;	/* when we were last queued to run */
 };
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 

commit bb29ab26863c022743143f27956cc0ca362f258c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:59 2007 +0200

    sched: x86, track TSC-unstable events
    
    track TSC-unstable events and propagate it to the scheduler code.
    Also allow sched_clock() to be used when the TSC is unstable,
    the rq_clock() wrapper creates a reliable clock out of it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index be2460e6f55b..fa895b309da0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1321,6 +1321,8 @@ extern void sched_exec(void);
 #define sched_exec()   {}
 #endif
 
+extern void sched_clock_unstable_event(void);
+
 #ifdef CONFIG_HOTPLUG_CPU
 extern void idle_task_exit(void);
 #else

commit f2ac58ee617fd9f6cd9922fbcd291b661d7c9954
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:59 2007 +0200

    sched: remove sleep_type
    
    remove the sleep_type heuristics from the core scheduler - scheduling
    policy is implemented in the scheduling-policy modules. (and CFS does
    not use this type of sleep-type heuristics)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4dcc61cca00a..be2460e6f55b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -788,13 +788,6 @@ struct mempolicy;
 struct pipe_inode_info;
 struct uts_namespace;
 
-enum sleep_type {
-	SLEEP_NORMAL,
-	SLEEP_NONINTERACTIVE,
-	SLEEP_INTERACTIVE,
-	SLEEP_INTERRUPTED,
-};
-
 struct prio_array;
 struct rq;
 struct sched_domain;
@@ -905,7 +898,6 @@ struct task_struct {
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;
 	unsigned long long sched_time; /* sched_clock time spent running */
-	enum sleep_type sleep_type;
 
 	unsigned int policy;
 	cpumask_t cpus_allowed;

commit e05606d3301525aa67b081ad9fccade2b31ab35a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:59 2007 +0200

    sched: clean up the rt priority macros
    
    clean up the rt priority macros, pointed out by Andrew Morton.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3e7f1890e55d..4dcc61cca00a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -525,31 +525,6 @@ struct signal_struct {
 #define SIGNAL_STOP_CONTINUED	0x00000004 /* SIGCONT since WCONTINUED reap */
 #define SIGNAL_GROUP_EXIT	0x00000008 /* group exit in progress */
 
-
-/*
- * Priority of a process goes from 0..MAX_PRIO-1, valid RT
- * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
- * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
- * values are inverted: lower p->prio value means higher priority.
- *
- * The MAX_USER_RT_PRIO value allows the actual maximum
- * RT priority to be separate from the value exported to
- * user-space.  This allows kernel threads to set their
- * priority to a value higher than any user task. Note:
- * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
- */
-
-#define MAX_USER_RT_PRIO	100
-#define MAX_RT_PRIO		MAX_USER_RT_PRIO
-
-#define MAX_PRIO		(MAX_RT_PRIO + 40)
-
-#define rt_prio(prio)		unlikely((prio) < MAX_RT_PRIO)
-#define rt_task(p)		rt_prio((p)->prio)
-#define batch_task(p)		(unlikely((p)->policy == SCHED_BATCH))
-#define is_rt_policy(p)		((p) != SCHED_NORMAL && (p) != SCHED_BATCH)
-#define has_rt_policy(p)	unlikely(is_rt_policy((p)->policy))
-
 /*
  * Some day this will be a full-fledged user tracking system..
  */
@@ -1164,6 +1139,42 @@ struct task_struct {
 #endif
 };
 
+/*
+ * Priority of a process goes from 0..MAX_PRIO-1, valid RT
+ * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
+ * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
+ * values are inverted: lower p->prio value means higher priority.
+ *
+ * The MAX_USER_RT_PRIO value allows the actual maximum
+ * RT priority to be separate from the value exported to
+ * user-space.  This allows kernel threads to set their
+ * priority to a value higher than any user task. Note:
+ * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
+ */
+
+#define MAX_USER_RT_PRIO	100
+#define MAX_RT_PRIO		MAX_USER_RT_PRIO
+
+#define MAX_PRIO		(MAX_RT_PRIO + 40)
+#define DEFAULT_PRIO		(MAX_RT_PRIO + 20)
+
+static inline int rt_prio(int prio)
+{
+	if (unlikely(prio < MAX_RT_PRIO))
+		return 1;
+	return 0;
+}
+
+static inline int rt_task(struct task_struct *p)
+{
+	return rt_prio(p->prio);
+}
+
+static inline int batch_task(struct task_struct *p)
+{
+	return p->policy == SCHED_BATCH;
+}
+
 static inline pid_t process_group(struct task_struct *tsk)
 {
 	return tsk->signal->pgrp;

commit 41b86e9c510ae66639bf29d3201e1d2384a7fde6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: make posix-cpu-timers use CFS's accounting information
    
    update the posix-cpu-timers code to use CFS's CPU accounting information.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 995eb407c234..3e7f1890e55d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -482,7 +482,8 @@ struct signal_struct {
 	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
 	 * other than jiffies.)
 	 */
-	unsigned long long sched_time;
+	unsigned long sched_time;
+	unsigned long long sum_sched_runtime;
 
 	/*
 	 * We don't bother to synchronize most readers of this at all,
@@ -1308,7 +1309,7 @@ static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 
 extern unsigned long long sched_clock(void);
 extern unsigned long long
-current_sched_time(const struct task_struct *current_task);
+task_sched_runtime(struct task_struct *task);
 
 /* sched_exec is called by processes performing an exec */
 #ifdef CONFIG_SMP

commit 20b8a59f2461e1be911dce2cfafefab9d22e4eee
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: cfs, core data types
    
    add the CFS data types to sched.h.
    
    (the old scheduler is still fully intact.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 90420321994f..995eb407c234 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -820,6 +820,86 @@ enum sleep_type {
 };
 
 struct prio_array;
+struct rq;
+struct sched_domain;
+
+struct sched_class {
+	struct sched_class *next;
+
+	void (*enqueue_task) (struct rq *rq, struct task_struct *p,
+			      int wakeup, u64 now);
+	void (*dequeue_task) (struct rq *rq, struct task_struct *p,
+			      int sleep, u64 now);
+	void (*yield_task) (struct rq *rq, struct task_struct *p);
+
+	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
+
+	struct task_struct * (*pick_next_task) (struct rq *rq, u64 now);
+	void (*put_prev_task) (struct rq *rq, struct task_struct *p, u64 now);
+
+	int (*load_balance) (struct rq *this_rq, int this_cpu,
+			struct rq *busiest,
+			unsigned long max_nr_move, unsigned long max_load_move,
+			struct sched_domain *sd, enum cpu_idle_type idle,
+			int *all_pinned, unsigned long *total_load_moved);
+
+	void (*set_curr_task) (struct rq *rq);
+	void (*task_tick) (struct rq *rq, struct task_struct *p);
+	void (*task_new) (struct rq *rq, struct task_struct *p);
+};
+
+struct load_weight {
+	unsigned long weight, inv_weight;
+};
+
+/*
+ * CFS stats for a schedulable entity (task, task-group etc)
+ *
+ * Current field usage histogram:
+ *
+ *     4 se->block_start
+ *     4 se->run_node
+ *     4 se->sleep_start
+ *     4 se->sleep_start_fair
+ *     6 se->load.weight
+ *     7 se->delta_fair
+ *    15 se->wait_runtime
+ */
+struct sched_entity {
+	long			wait_runtime;
+	unsigned long		delta_fair_run;
+	unsigned long		delta_fair_sleep;
+	unsigned long		delta_exec;
+	s64			fair_key;
+	struct load_weight	load;		/* for load-balancing */
+	struct rb_node		run_node;
+	unsigned int		on_rq;
+
+	u64			wait_start_fair;
+	u64			wait_start;
+	u64			exec_start;
+	u64			sleep_start;
+	u64			sleep_start_fair;
+	u64			block_start;
+	u64			sleep_max;
+	u64			block_max;
+	u64			exec_max;
+	u64			wait_max;
+	u64			last_ran;
+
+	u64			sum_exec_runtime;
+	s64			sum_wait_runtime;
+	s64			sum_sleep_runtime;
+	unsigned long		wait_runtime_overruns;
+	unsigned long		wait_runtime_underruns;
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	struct sched_entity	*parent;
+	/* rq on which this entity is (to be) queued: */
+	struct cfs_rq		*cfs_rq;
+	/* rq "owned" by this entity/group: */
+	struct cfs_rq		*my_q;
+#endif
+};
 
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
@@ -839,6 +919,8 @@ struct task_struct {
 	int prio, static_prio, normal_prio;
 	struct list_head run_list;
 	struct prio_array *array;
+	struct sched_class *sched_class;
+	struct sched_entity se;
 
 	unsigned short ioprio;
 #ifdef CONFIG_BLK_DEV_IO_TRACE

commit bf0f6f24a1ece8988b243aefe84ee613099a9245
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: cfs core, kernel/sched_fair.c
    
    add kernel/sched_fair.c - which implements the bulk of CFS's
    behavioral changes for SCHED_OTHER tasks.
    
    see Documentation/sched-design-CFS.txt about details.
    
    Authors:
    
     Ingo Molnar <mingo@elte.hu>
     Dmitry Adamushko <dmitry.adamushko@gmail.com>
     Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
     Mike Galbraith <efault@gmx.de>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d5084e7c48cf..90420321994f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1243,6 +1243,14 @@ static inline void idle_task_exit(void) {}
 
 extern void sched_idle_next(void);
 
+extern unsigned int sysctl_sched_granularity;
+extern unsigned int sysctl_sched_wakeup_granularity;
+extern unsigned int sysctl_sched_batch_wakeup_granularity;
+extern unsigned int sysctl_sched_stat_granularity;
+extern unsigned int sysctl_sched_runtime_limit;
+extern unsigned int sysctl_sched_child_runs_first;
+extern unsigned int sysctl_sched_features;
+
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);

commit 9aa7b369819940cb1f3c74ba210516739a32ad95
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: increase the resolution of smpnice
    
    increase SMP-nice's resolution. This is needed by CFS to
    implement SCHED_IDLE and cleaned up nice level support.
    
    no behavioral changes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 61a111fe2b7a..d5084e7c48cf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -652,7 +652,14 @@ enum cpu_idle_type {
 /*
  * sched-domains (multiprocessor balancing) declarations:
  */
-#define SCHED_LOAD_SCALE	128UL	/* increase resolution of load */
+
+/*
+ * Increase resolution of nice-level calculations:
+ */
+#define SCHED_LOAD_SHIFT	10
+#define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
+
+#define SCHED_LOAD_SCALE_FUZZ	(SCHED_LOAD_SCALE >> 5)
 
 #ifdef CONFIG_SMP
 #define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */

commit 1df21055e34b6a68d62cf0c524b9e52deebd7ead
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: add init_idle_bootup_task()
    
    add the init_idle_bootup_task() callback to the bootup thread,
    unused at the moment. (CFS will use it to switch the scheduling
    class of the boot thread to the idle class)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b912e753ca0..61a111fe2b7a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -195,6 +195,7 @@ struct task_struct;
 extern void sched_init(void);
 extern void sched_init_smp(void);
 extern void init_idle(struct task_struct *idle, int cpu);
+extern void init_idle_bootup_task(struct task_struct *idle);
 
 extern cpumask_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)

commit c65cc8705256ad7524c97564b4fe3ca9782bf6d1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: uninline set_task_cpu()
    
    uninline set_task_cpu(): CFS will add more code to it.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8764cda0feca..4b912e753ca0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1633,10 +1633,7 @@ static inline unsigned int task_cpu(const struct task_struct *p)
 	return task_thread_info(p)->cpu;
 }
 
-static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
-{
-	task_thread_info(p)->cpu = cpu;
-}
+extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
 
 #else
 

commit 0437e109e1841607f2988891eaa36c531c6aa6ac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:57 2007 +0200

    sched: zap the migration init / cache-hot balancing code
    
    the SMP load-balancer uses the boot-time migration-cost estimation
    code to attempt to improve the quality of balancing. The reason for
    this code is that the discrete priority queues do not preserve
    the order of scheduling accurately, so the load-balancer skips
    tasks that were running on a CPU 'recently'.
    
    this code is fundamental fragile: the boot-time migration cost detector
    doesnt really work on systems that had large L3 caches, it caused boot
    delays on large systems and the whole cache-hot concept made the
    balancing code pretty undeterministic as well.
    
    (and hey, i wrote most of it, so i can say it out loud that it sucks ;-)
    
    under CFS the same purpose of cache affinity can be achieved without
    any special cache-hot special-case: tasks are sorted in the 'timeline'
    tree and the SMP balancer picks tasks from the left side of the
    tree, thus the most cache-cold task is balanced automatically.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7e74262f98e1..8764cda0feca 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -754,12 +754,6 @@ struct sched_domain {
 extern int partition_sched_domains(cpumask_t *partition1,
 				    cpumask_t *partition2);
 
-/*
- * Maximum cache size the migration-costs auto-tuning code will
- * search from:
- */
-extern unsigned int max_cache_size;
-
 #endif	/* CONFIG_SMP */
 
 

commit 0e6aca43e08a62a48d6770e9a159dbec167bf4c6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:57 2007 +0200

    sched: add SCHED_IDLE policy
    
    this patch adds the SCHED_IDLE policy to sched.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2acfb23f3681..7e74262f98e1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,6 +34,8 @@
 #define SCHED_FIFO		1
 #define SCHED_RR		2
 #define SCHED_BATCH		3
+/* SCHED_ISO: reserved but not implemented yet */
+#define SCHED_IDLE		5
 
 #ifdef __KERNEL__
 

commit d15bcfdbe1818478891d714343f037cfe60875f0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:57 2007 +0200

    sched: rename idle_type/SCHED_IDLE
    
    enum idle_type (used by the load-balancer) clashes with the
    SCHED_IDLE name that we want to introduce. 'CPU_IDLE' instead
    of 'SCHED_IDLE' is more descriptive as well.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 693f0e6c54d4..2acfb23f3681 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -639,12 +639,11 @@ static inline int sched_info_on(void)
 #endif
 }
 
-enum idle_type
-{
-	SCHED_IDLE,
-	NOT_IDLE,
-	NEWLY_IDLE,
-	MAX_IDLE_TYPES
+enum cpu_idle_type {
+	CPU_IDLE,
+	CPU_NOT_IDLE,
+	CPU_NEWLY_IDLE,
+	CPU_MAX_IDLE_TYPES
 };
 
 /*
@@ -719,14 +718,14 @@ struct sched_domain {
 
 #ifdef CONFIG_SCHEDSTATS
 	/* load_balance() stats */
-	unsigned long lb_cnt[MAX_IDLE_TYPES];
-	unsigned long lb_failed[MAX_IDLE_TYPES];
-	unsigned long lb_balanced[MAX_IDLE_TYPES];
-	unsigned long lb_imbalance[MAX_IDLE_TYPES];
-	unsigned long lb_gained[MAX_IDLE_TYPES];
-	unsigned long lb_hot_gained[MAX_IDLE_TYPES];
-	unsigned long lb_nobusyg[MAX_IDLE_TYPES];
-	unsigned long lb_nobusyq[MAX_IDLE_TYPES];
+	unsigned long lb_cnt[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_failed[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_balanced[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_imbalance[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_gained[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_hot_gained[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_nobusyg[CPU_MAX_IDLE_TYPES];
+	unsigned long lb_nobusyq[CPU_MAX_IDLE_TYPES];
 
 	/* Active load balancing */
 	unsigned long alb_cnt;

commit 778e9a9c3e7193ea9f434f382947155ffb59c755
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Fri Jun 8 13:47:00 2007 -0700

    pi-futex: fix exit races and locking problems
    
    1. New entries can be added to tsk->pi_state_list after task completed
       exit_pi_state_list(). The result is memory leakage and deadlocks.
    
    2. handle_mm_fault() is called under spinlock. The result is obvious.
    
    3. results in self-inflicted deadlock inside glibc.
       Sometimes futex_lock_pi returns -ESRCH, when it is not expected
       and glibc enters to for(;;) sleep() to simulate deadlock. This problem
       is quite obvious and I think the patch is right. Though it looks like
       each "if" in futex_lock_pi() got some stupid special case "else if". :-)
    
    4. sometimes futex_lock_pi() returns -EDEADLK,
       when nobody has the lock. The reason is also obvious (see comment
       in the patch), but correct fix is far beyond my comprehension.
       I guess someone already saw this, the chunk:
    
                            if (rt_mutex_trylock(&q.pi_state->pi_mutex))
                                    ret = 0;
    
       is obviously from the same opera. But it does not work, because the
       rtmutex is really taken at this point: wake_futex_pi() of previous
       owner reassigned it to us. My fix works. But it looks very stupid.
       I would think about removal of shift of ownership in wake_futex_pi()
       and making all the work in context of process taking lock.
    
    From: Thomas Gleixner <tglx@linutronix.de>
    
    Fix 1) Avoid the tasklist lock variant of the exit race fix by adding
        an additional state transition to the exit code.
    
        This fixes also the issue, when a task with recursive segfaults
        is not able to release the futexes.
    
    Fix 2) Cleanup the lookup_pi_state() failure path and solve the -ESRCH
        problem finally.
    
    Fix 3) Solve the fixup_pi_state_owner() problem which needs to do the fixup
        in the lock protected section by using the in_atomic userspace access
        functions.
    
        This removes also the ugly lock drop / unqueue inside of fixup_pi_state()
    
    Fix 4) Fix a stale lock in the error path of futex_wake_pi()
    
    Added some error checks for verification.
    
    The -EDEADLK problem is solved by the rtmutex fixups.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d58e74b98367..693f0e6c54d4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1162,6 +1162,7 @@ static inline void put_task_struct(struct task_struct *t)
 					/* Not implemented yet, only for 486*/
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
+#define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */

commit 7bb44adef39ad3bda2be40bb34686bc56bd563a5
Author: Roland McGrath <roland@redhat.com>
Date:   Wed May 23 13:57:44 2007 -0700

    recalc_sigpending_tsk fixes
    
    Steve Hawkes discovered a problem where recalc_sigpending_tsk was called in
    do_sigaction but no signal_wake_up call was made, preventing later signals
    from waking up blocked threads with TIF_SIGPENDING already set.
    
    In fact, the few other calls to recalc_sigpending_tsk outside the signals
    code are also subject to this problem in other race conditions.
    
    This change makes recalc_sigpending_tsk private to the signals code.  It
    changes the outside calls, as well as do_sigaction, to use the new
    recalc_sigpending_and_wake instead.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: <Steve.Hawkes@motorola.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 870b75e348ab..d58e74b98367 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1616,11 +1616,13 @@ static inline int lock_need_resched(spinlock_t *lock)
 	return 0;
 }
 
-/* Reevaluate whether the task has signals pending delivery.
-   This is required every time the blocked sigset_t changes.
-   callers must hold sighand->siglock.  */
-
-extern FASTCALL(void recalc_sigpending_tsk(struct task_struct *t));
+/*
+ * Reevaluate whether the task has signals pending delivery.
+ * Wake the task if so.
+ * This is required every time the blocked sigset_t changes.
+ * callers must hold sighand->siglock.
+ */
+extern void recalc_sigpending_and_wake(struct task_struct *t);
 extern void recalc_sigpending(void);
 
 extern void signal_wake_up(struct task_struct *t, int resume_stopped);

commit ba96a0c88098697a63e80157718b7440414ed24d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 23 13:57:25 2007 -0700

    freezer: fix vfork problem
    
    Currently try_to_freeze_tasks() has to wait until all of the vforked processes
    exit and for this reason every user can make it fail.  To fix this problem we
    can introduce the additional process flag PF_FREEZER_SKIP to be used by tasks
    that do not want to be counted as freezable by the freezer and want to have
    TIF_FREEZE set nevertheless.  Then, this flag can be set by tasks using
    sys_vfork() before they call wait_for_completion(&vfork) and cleared after
    they have woken up.  After clearing it, the tasks should call try_to_freeze()
    as soon as possible.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a81897e2a244..870b75e348ab 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1182,6 +1182,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
+#define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezeable */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit cabca0cb0d0e8579428d8f8c3f606e2f01d26d14
Merge: 853da0022023 87c1efbfeac4
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Fri May 11 09:58:49 2007 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/data/git/linux-2.6-block
    
    * 'for-linus' of git://git.kernel.dk/data/git/linux-2.6-block:
      Fix compile/link of init/do_mounts.c with !CONFIG_BLOCK
      When stacked block devices are in-use (e.g. md or dm), the recursive calls

commit fba2afaaec790dc5ab4ae8827972f342211bbb86
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu May 10 22:23:13 2007 -0700

    signal/timer/event: signalfd core
    
    This patch series implements the new signalfd() system call.
    
    I took part of the original Linus code (and you know how badly it can be
    broken :), and I added even more breakage ;) Signals are fetched from the same
    signal queue used by the process, so signalfd will compete with standard
    kernel delivery in dequeue_signal().  If you want to reliably fetch signals on
    the signalfd file, you need to block them with sigprocmask(SIG_BLOCK).  This
    seems to be working fine on my Dual Opteron machine.  I made a quick test
    program for it:
    
    http://www.xmailserver.org/signafd-test.c
    
    The signalfd() system call implements signal delivery into a file descriptor
    receiver.  The signalfd file descriptor if created with the following API:
    
    int signalfd(int ufd, const sigset_t *mask, size_t masksize);
    
    The "ufd" parameter allows to change an existing signalfd sigmask, w/out going
    to close/create cycle (Linus idea).  Use "ufd" == -1 if you want a brand new
    signalfd file.
    
    The "mask" allows to specify the signal mask of signals that we are interested
    in.  The "masksize" parameter is the size of "mask".
    
    The signalfd fd supports the poll(2) and read(2) system calls.  The poll(2)
    will return POLLIN when signals are available to be dequeued.  As a direct
    consequence of supporting the Linux poll subsystem, the signalfd fd can use
    used together with epoll(2) too.
    
    The read(2) system call will return a "struct signalfd_siginfo" structure in
    the userspace supplied buffer.  The return value is the number of bytes copied
    in the supplied buffer, or -1 in case of error.  The read(2) call can also
    return 0, in case the sighand structure to which the signalfd was attached,
    has been orphaned.  The O_NONBLOCK flag is also supported, and read(2) will
    return -EAGAIN in case no signal is available.
    
    If the size of the buffer passed to read(2) is lower than sizeof(struct
    signalfd_siginfo), -EINVAL is returned.  A read from the signalfd can also
    return -ERESTARTSYS in case a signal hits the process.  The format of the
    struct signalfd_siginfo is, and the valid fields depends of the (->code &
    __SI_MASK) value, in the same way a struct siginfo would:
    
    struct signalfd_siginfo {
            __u32 signo;    /* si_signo */
            __s32 err;      /* si_errno */
            __s32 code;     /* si_code */
            __u32 pid;      /* si_pid */
            __u32 uid;      /* si_uid */
            __s32 fd;       /* si_fd */
            __u32 tid;      /* si_fd */
            __u32 band;     /* si_band */
            __u32 overrun;  /* si_overrun */
            __u32 trapno;   /* si_trapno */
            __s32 status;   /* si_status */
            __s32 svint;    /* si_int */
            __u64 svptr;    /* si_ptr */
            __u64 utime;    /* si_utime */
            __u64 stime;    /* si_stime */
            __u64 addr;     /* si_addr */
    };
    
    [akpm@linux-foundation.org: fix signalfd_copyinfo() on i386]
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 75f44379b7e9..97c0c7da58ef 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -391,6 +391,7 @@ struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];
 	spinlock_t		siglock;
+	struct list_head        signalfd_list;
 };
 
 struct pacct_struct {

commit 6eaeeaba39e5fa3d52a0bb8de15e995516ae251a
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu May 10 22:22:37 2007 -0700

    getrusage(): fill ru_inblock and ru_oublock fields if possible
    
    If CONFIG_TASK_IO_ACCOUNTING is defined, we update io accounting counters for
    each task.
    
    This patch permits reporting of values using the well known getrusage()
    syscall, filling ru_inblock and ru_oublock instead of null values.
    
    As TASK_IO_ACCOUNTING currently counts bytes counts, we approximate blocks
    count doing : nr_blocks = nr_bytes / 512
    
    Example of use :
    ----------------------
    After patch is applied, /usr/bin/time command can now give a good
    approximation of IO that the process had to do.
    
    $ /usr/bin/time grep tototo /usr/include/*
    Command exited with non-zero status 1
    0.00user 0.02system 0:02.11elapsed 1%CPU (0avgtext+0avgdata 0maxresident)k
    24288inputs+0outputs (0major+259minor)pagefaults 0swaps
    
    $ /usr/bin/time dd if=/dev/zero of=/tmp/testfile count=1000
    1000+0 enregistrements lus
    1000+0 enregistrements Ã©crits
    512000 octets (512 kB) copiÃ©s, 0,00326601 seconde, 157 MB/s
    0.00user 0.00system 0:00.00elapsed 80%CPU (0avgtext+0avgdata 0maxresident)k
    0inputs+3000outputs (0major+299minor)pagefaults 0swaps
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17b72d88c4cb..75f44379b7e9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -469,6 +469,7 @@ struct signal_struct {
 	cputime_t utime, stime, cutime, cstime;
 	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
+	unsigned long inblock, oublock, cinblock, coublock;
 
 	/*
 	 * Cumulative ns of scheduled CPU time for dead threads in the

commit d89d87965dcbe6fe4f96a2a7e8421b3a75f634d1
Author: Neil Brown <neilb@suse.de>
Date:   Tue May 1 09:53:42 2007 +0200

    When stacked block devices are in-use (e.g. md or dm), the recursive calls
    
    to generic_make_request can use up a lot of space, and we would rather they
    didn't.
    
    As generic_make_request is a void function, and as it is generally not
    expected that it will have any effect immediately, it is safe to delay any
    call to generic_make_request until there is sufficient stack space
    available.
    
    As ->bi_next is reserved for the driver to use, it can have no valid value
    when generic_make_request is called, and as __make_request implicitly
    assumes it will be NULL (ELEVATOR_BACK_MERGE fork of switch) we can be
    certain that all callers set it to NULL.  We can therefore safely use
    bi_next to link pending requests together, providing we clear it before
    making the real call.
    
    So, we choose to allow each thread to only be active in one
    generic_make_request at a time.  If a subsequent (recursive) call is made,
    the bio is linked into a per-thread list, and is handled when the active
    call completes.
    
    As the list of pending bios is per-thread, there are no locking issues to
    worry about.
    
    I say above that it is "safe to delay any call...".  There are, however,
    some behaviours of a make_request_fn which would make it unsafe.  These
    include any behaviour that assumes anything will have changed after a
    recursive call to generic_make_request.
    
    These could include:
     - waiting for that call to finish and call it's bi_end_io function.
       md use to sometimes do this (marking the superblock dirty before
       completing a write) but doesn't any more
     - inspecting the bio for fields that generic_make_request might
       change, such as bi_sector or bi_bdev.  It is hard to see a good
       reason for this, and I don't think anyone actually does it.
     - inspecing the queue to see if, e.g. it is 'full' yet.  Again, I
       think this is very unlikely to be useful, or to be done.
    
    Signed-off-by: Neil Brown <neilb@suse.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: <dm-devel@redhat.com>
    
    Alasdair G Kergon <agk@redhat.com> said:
    
     I can see nothing wrong with this in principle.
    
     For device-mapper at the moment though it's essential that, while the bio
     mappings may now get delayed, they still get processed in exactly
     the same order as they were passed to generic_make_request().
    
     My main concern is whether the timing changes implicit in this patch
     will make the rare data-corrupting races in the existing snapshot code
     more likely. (I'm working on a fix for these races, but the unfinished
     patch is already several hundred lines long.)
    
     It would be helpful if some people on this mailing list would test
     this patch in various scenarios and report back.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17b72d88c4cb..e38c436ee12b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -88,6 +88,7 @@ struct sched_param {
 
 struct exec_domain;
 struct futex_pi_state;
+struct bio;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1014,6 +1015,9 @@ struct task_struct {
 /* journalling filesystem info */
 	void *journal_info;
 
+/* stacked block device info */
+	struct bio *bio_list, **bio_tail;
+
 /* VM state */
 	struct reclaim_state *reclaim_state;
 

commit f7e4217b007d1f73e7e3cf10ba4fea4a608c603f
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Wed May 9 02:35:17 2007 -0700

    rename thread_info to stack
    
    This finally renames the thread_info field in task structure to stack, so that
    the assumptions about this field are gone and archs have more freedom about
    placing the thread_info structure.
    
    Nonbroken archs which have a proper thread pointer can do the access to both
    current thread and task structure via a single pointer.
    
    It'll allow for a few more cleanups of the fork code, from which e.g.  ia64
    could benefit.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    [akpm@linux-foundation.org: build fix]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: Greg Ungerer <gerg@uclinux.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Chris Zankel <chris@zankel.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 28000b1658f9..17b72d88c4cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -817,7 +817,7 @@ struct prio_array;
 
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
-	struct thread_info *thread_info;
+	void *stack;
 	atomic_t usage;
 	unsigned int flags;	/* per process flags, defined below */
 	unsigned int ptrace;
@@ -1513,8 +1513,8 @@ static inline void unlock_task_sighand(struct task_struct *tsk,
 
 #ifndef __HAVE_THREAD_FUNCTIONS
 
-#define task_thread_info(task) (task)->thread_info
-#define task_stack_page(task) ((void*)((task)->thread_info))
+#define task_thread_info(task)	((struct thread_info *)(task)->stack)
+#define task_stack_page(task)	((task)->stack)
 
 static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
 {
@@ -1524,7 +1524,7 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 
 static inline unsigned long *end_of_stack(struct task_struct *p)
 {
-	return (unsigned long *)(p->thread_info + 1);
+	return (unsigned long *)(task_thread_info(p) + 1);
 }
 
 #endif

commit 10ab825bdef8df510f99c703a5a2d9b13a4e31a5
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:37 2007 -0700

    change kernel threads to ignore signals instead of blocking them
    
    Currently kernel threads use sigprocmask(SIG_BLOCK) to protect against
    signals.  This doesn't prevent the signal delivery, this only blocks
    signal_wake_up().  Every "killall -33 kthreadd" means a "struct siginfo"
    leak.
    
    Change kthreadd_setup() to set all handlers to SIG_IGN instead of blocking
    them (make a new helper ignore_signals() for that).  If the kernel thread
    needs some signal, it should use allow_signal() anyway, and in that case it
    should not use CLONE_SIGHAND.
    
    Note that we can't change daemonize() (should die!) in the same way,
    because it can be used along with CLONE_SIGHAND.  This means that
    allow_signal() still should unblock the signal to work correctly with
    daemonize()ed threads.
    
    However, disallow_signal() doesn't block the signal any longer but ignores
    it.
    
    NOTE: with or without this patch the kernel threads are not protected from
    handle_stop_signal(), this seems harmless, but not good.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3d95c480f58d..28000b1658f9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1317,6 +1317,7 @@ extern int in_egroup_p(gid_t);
 
 extern void proc_caches_init(void);
 extern void flush_signals(struct task_struct *);
+extern void ignore_signals(struct task_struct *);
 extern void flush_signal_handlers(struct task_struct *, int force_default);
 extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
 

commit 5517d86bea237c1d7078840182d9ebc0fe4c1afc
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue May 8 00:32:57 2007 -0700

    Speed up divides by cpu_power in scheduler
    
    I noticed expensive divides done in try_to_wakeup() and
    find_busiest_group() on a bi dual core Opteron machine (total of 4 cores),
    moderatly loaded (15.000 context switch per second)
    
    oprofile numbers :
    
    CPU: AMD64 processors, speed 2600.05 MHz (estimated)
    Counted CPU_CLK_UNHALTED events (Cycles outside of halt state) with a unit
    mask of 0x00 (No unit mask) count 50000
    samples  %        symbol name
    ...
    613914    1.0498  try_to_wake_up
        834  0.0013 :ffffffff80227ae1:   div    %rcx
    77513  0.1191 :ffffffff80227ae4:   mov    %rax,%r11
    
    608893    1.0413  find_busiest_group
       1841  0.0031 :ffffffff802260bf:       div    %rdi
    140109  0.2394 :ffffffff802260c2:       test   %sil,%sil
    
    Some of these divides can use the reciprocal divides we introduced some
    time ago (currently used in slab AFAIK)
    
    We can assume a load will fit in a 32bits number, because with a
    SCHED_LOAD_SCALE=128 value, its still a theorical limit of 33554432
    
    When/if we reach this limit one day, probably cpus will have a fast
    hardware divide and we can zap the reciprocal divide trick.
    
    Ingo suggested to rename cpu_power to __cpu_power to make clear it should
    not be modified without changing its reciprocal value too.
    
    I did not convert the divide in cpu_avg_load_per_task(), because tracking
    nr_running changes may be not worth it ?  We could use a static table of 32
    reciprocal values but it would add a conditional branch and table lookup.
    
    [akpm@linux-foundation.org: !SMP build fix]
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 15ab3e039535..3d95c480f58d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -680,8 +680,14 @@ struct sched_group {
 	/*
 	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
 	 * single CPU. This is read only (except for setup, hotplug CPU).
+	 * Note : Never change cpu_power without recompute its reciprocal
 	 */
-	unsigned long cpu_power;
+	unsigned int __cpu_power;
+	/*
+	 * reciprocal value of cpu_power to avoid expensive divides
+	 * (see include/linux/reciprocal_div.h)
+	 */
+	u32 reciprocal_cpu_power;
 };
 
 struct sched_domain {

commit 46cb4b7c88fa5517f64b5bee42939ea3614cddcb
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue May 8 00:32:51 2007 -0700

    sched: dynticks idle load balancing
    
    Fix the process idle load balancing in the presence of dynticks.  cpus for
    which ticks are stopped will sleep till the next event wakes it up.
    Potentially these sleeps can be for large durations and during which today,
    there is no periodic idle load balancing being done.
    
    This patch nominates an owner among the idle cpus, which does the idle load
    balancing on behalf of the other idle cpus.  And once all the cpus are
    completely idle, then we can stop this idle load balancing too.  Checks added
    in fast path are minimized.  Whenever there are busy cpus in the system, there
    will be an owner(idle cpu) doing the system wide idle load balancing.
    
    Open items:
    1. Intelligent owner selection (like an idle core in a busy package).
    2. Merge with rcu's nohz_cpu_mask?
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6312521df2c1..15ab3e039535 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -194,6 +194,14 @@ extern void sched_init_smp(void);
 extern void init_idle(struct task_struct *idle, int cpu);
 
 extern cpumask_t nohz_cpu_mask;
+#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
+extern int select_nohz_load_balancer(int cpu);
+#else
+static inline int select_nohz_load_balancer(int cpu)
+{
+	return 0;
+}
+#endif
 
 /*
  * Only dump TASK_* tasks. (0 for all tasks)

commit 04c9167f91e309c9c4ea982992aa08e83b2eb42e
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Tue May 8 00:28:05 2007 -0700

    add touch_all_softlockup_watchdogs()
    
    Add touch_all_softlockup_watchdogs() to allow the softlockup watchdog
    timers on all cpus to be updated.  This is used to prevent sysrq-t from
    generating a spurious watchdog message when generating lots of output.
    
    Softlockup watchdogs use sched_clock() as its timebase, which is inherently
    per-cpu (at least, when it is measuring unstolen time).  Because of this,
    it isn't possible for one CPU to directly update the other CPU's timers,
    but it is possible to tell the other CPUs to do update themselves
    appropriately.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Acked-by: Chris Lalancette <clalance@redhat.com>
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Rick Lindsley <ricklind@us.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d464bd0d6578..6312521df2c1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -226,6 +226,7 @@ extern void scheduler_tick(void);
 extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
+extern void touch_all_softlockup_watchdogs(void);
 #else
 static inline void softlockup_tick(void)
 {
@@ -236,6 +237,9 @@ static inline void spawn_softlockup_task(void)
 static inline void touch_softlockup_watchdog(void)
 {
 }
+static inline void touch_all_softlockup_watchdogs(void)
+{
+}
 #endif
 
 

commit 3367b994fe4f131ab1240600682a1981de7cad0c
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue May 8 00:27:52 2007 -0700

    <linux/sysdev.h> needs to include <linux/module.h>
    
    sysdev.h uses THIS_MODULE so should include <linux/module.h>.
    
    [akpm@linux-foundation.org: couple of fixes]
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d9acbbb39f96..d464bd0d6578 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1641,10 +1641,7 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 
-#include <linux/sysdev.h>
 extern int sched_mc_power_savings, sched_smt_power_savings;
-extern struct sysdev_attribute attr_sched_mc_power_savings, attr_sched_smt_power_savings;
-extern int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls);
 
 extern void normalize_rt_tasks(void);
 

commit 97dc32cdb1b53832801159d5f634b41aad9d0a23
Author: William Cohen <wcohen@redhat.com>
Date:   Tue May 8 00:23:41 2007 -0700

    reduce size of task_struct on 64-bit machines
    
    This past week I was playing around with that pahole tool
    (http://oops.ghostprotocols.net:81/acme/dwarves/) and looking at the size
    of various struct in the kernel.  I was surprised by the size of the
    task_struct on x86_64, approaching 4K.  I looked through the fields in
    task_struct and found that a number of them were declared as "unsigned
    long" rather than "unsigned int" despite them appearing okay as 32-bit
    sized fields.  On x86_64 "unsigned long" ends up being 8 bytes in size and
    forces 8 byte alignment.  Is there a reason there a reason they are
    "unsigned long"?
    
    The patch below drops the size of the struct from 3808 bytes (60 64-byte
    cachelines) to 3760 bytes (59 64-byte cachelines).  A couple other fields
    in the task struct take a signficant amount of space:
    
    struct thread_struct       thread;               688
    struct held_lock           held_locks[30];       1680
    
    CONFIG_LOCKDEP is turned on in the .config
    
    [akpm@linux-foundation.org: fix printk warnings]
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a1707583de49..d9acbbb39f96 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -801,8 +801,8 @@ struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	struct thread_info *thread_info;
 	atomic_t usage;
-	unsigned long flags;	/* per process flags, defined below */
-	unsigned long ptrace;
+	unsigned int flags;	/* per process flags, defined below */
+	unsigned int ptrace;
 
 	int lock_depth;		/* BKL lock depth */
 
@@ -825,7 +825,7 @@ struct task_struct {
 	unsigned long long sched_time; /* sched_clock time spent running */
 	enum sleep_type sleep_type;
 
-	unsigned long policy;
+	unsigned int policy;
 	cpumask_t cpus_allowed;
 	unsigned int time_slice, first_time_slice;
 
@@ -845,11 +845,11 @@ struct task_struct {
 
 /* task state */
 	struct linux_binfmt *binfmt;
-	long exit_state;
+	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
 	/* ??? */
-	unsigned long personality;
+	unsigned int personality;
 	unsigned did_exec:1;
 	pid_t pid;
 	pid_t tgid;
@@ -881,7 +881,7 @@ struct task_struct {
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
 	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
 
-	unsigned long rt_priority;
+	unsigned int rt_priority;
 	cputime_t utime, stime;
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time;

commit 39bc89fd4019b164002adaacef92c4140e37955a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 25 20:50:03 2007 -0700

    make SysRq-T show all tasks again
    
    show_state() (SysRq-T) developed the buggy habbit of not showing
    TASK_RUNNING tasks.  This was due to the mistaken belief that state_filter
    == -1 would be a pass-through filter - while in reality it did not let
    TASK_RUNNING == 0 p->state values through.
    
    Fix this by restoring the original '!state_filter means all tasks'
    special-case i had in the original version.  Test-built and test-booted on
    i686, SysRq-T now works as intended.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 49fe2997a016..a1707583de49 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -196,13 +196,13 @@ extern void init_idle(struct task_struct *idle, int cpu);
 extern cpumask_t nohz_cpu_mask;
 
 /*
- * Only dump TASK_* tasks. (-1 for all tasks)
+ * Only dump TASK_* tasks. (0 for all tasks)
  */
 extern void show_state_filter(unsigned long state_filter);
 
 static inline void show_state(void)
 {
-	show_state_filter(-1);
+	show_state_filter(0);
 }
 
 extern void show_regs(struct pt_regs *);

commit 69f7c0a1be84b10a81b6edcce2dbee0cdec26eba
Author: Con Kolivas <kernel@kolivas.org>
Date:   Mon Mar 5 00:30:29 2007 -0800

    [PATCH] sched: remove SMT nice
    
    Remove the SMT-nice feature which idles sibling cpus on SMT cpus to
    facilitiate nice working properly where cpu power is shared.  The idling of
    cpus in the presence of runnable tasks is considered too fragile, easy to
    break with outside code, and the complexity of managing this system if an
    architecture comes along with many logical cores sharing cpu power will be
    unworkable.
    
    Remove the associated per_cpu_gain variable in sched_domains used only by
    this code.
    
    Also:
    
      The reason is that with dynticks enabled, this code breaks without yet
      further tweaks so dynticks brought on the rapid demise of this code.  So
      either we tweak this code or kill it off entirely.  It was Ingo's preference
      to kill it off.  Either way this needs to happen for 2.6.21 since dynticks
      has gone in.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6f7c9a4d80e5..49fe2997a016 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -684,7 +684,6 @@ struct sched_domain {
 	unsigned int imbalance_pct;	/* No balance until over watermark */
 	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
-	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
 	unsigned int busy_idx;
 	unsigned int idle_idx;
 	unsigned int newidle_idx;

commit b0138a6cb7923a997d278b47c176778534d1095b
Merge: 6572d6d7d0f9 1055a8af093f
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Mon Feb 26 12:48:06 2007 -0800

    Merge master.kernel.org:/pub/scm/linux/kernel/git/kyle/parisc-2.6
    
    * master.kernel.org:/pub/scm/linux/kernel/git/kyle/parisc-2.6: (78 commits)
      [PARISC] Use symbolic last syscall in __NR_Linux_syscalls
      [PARISC] Add missing statfs64 and fstatfs64 syscalls
      Revert "[PARISC] Optimize TLB flush on SMP systems"
      [PARISC] Compat signal fixes for 64-bit parisc
      [PARISC] Reorder syscalls to match unistd.h
      Revert "[PATCH] make kernel/signal.c:kill_proc_info() static"
      [PARISC] fix sys_rt_sigqueueinfo
      [PARISC] fix section mismatch warnings in harmony sound driver
      [PARISC] do not export get_register/set_register
      [PARISC] add ENTRY()/ENDPROC() and simplify assembly of HP/UX emulation code
      [PARISC] convert to use CONFIG_64BIT instead of __LP64__
      [PARISC] use CONFIG_64BIT instead of __LP64__
      [PARISC] add ASM_EXCEPTIONTABLE_ENTRY() macro
      [PARISC] more ENTRY(), ENDPROC(), END() conversions
      [PARISC] fix ENTRY() and ENDPROC() for 64bit-parisc
      [PARISC] Fixes /proc/cpuinfo cache output on B160L
      [PARISC] implement standard ENTRY(), END() and ENDPROC()
      [PARISC] kill ENTRY_SYS_CPUS
      [PARISC] clean up debugging printks in smp.c
      [PARISC] factor syscall_restart code out of do_signal
      ...
    
    Fix conflict in include/linux/sched.h due to kill_proc_info() being made
    publicly available to PARISC again.

commit c3de4b38153a201cfc8561abb093a1b482fd3abb
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Fri Feb 9 08:11:47 2007 -0700

    Revert "[PATCH] make kernel/signal.c:kill_proc_info() static"
    
    This reverts commit d3228a887cae75ef2b8b1211c31c539bef5a5698.
    DeBunk this code.  We need it for compat_sys_rt_sigqueueinfo.
    
    Signed-off-by: Kyle McMartin <kyle@parisc-linux.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 446373535190..c9045815e62e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1329,6 +1329,7 @@ extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
+extern int kill_proc_info(int, struct siginfo *, pid_t);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);

commit 27b0b2f44adffe0193a695bb528a83b550b8e54b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:53:02 2007 -0800

    [PATCH] pid: remove the now unused kill_pg kill_pg_info and __kill_pg_info
    
    Now that I have changed all of the in-tree users remove the old version of
    these functions.  This should make it clear to any out of tree users that they
    should be using kill_pgrp kill_pgrp_info or __kill_pgrp_info instead.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 39d40c518531..5053dc01fad4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1329,14 +1329,11 @@ extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
 extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_t, u32);
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
-extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
-extern int kill_pg_info(int, struct siginfo *, pid_t);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
 extern void zap_other_threads(struct task_struct *p);
-extern int kill_pg(pid_t, int, int);
 extern int kill_proc(pid_t, int, int);
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);

commit ab521dc0f8e117fd808d3e425216864d60390500
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:53:00 2007 -0800

    [PATCH] tty: update the tty layer to work with struct pid
    
    Of kernel subsystems that work with pids the tty layer is probably the largest
    consumer.  But it has the nice virtue that the assiation with a session only
    lasts until the session leader exits.  Which means that no reference counting
    is required.  So using struct pid winds up being a simple optimization to
    avoid hash table lookups.
    
    In the long term the use of pid_nr also ensures that when we have multiple pid
    spaces mixed everything will work correctly.
    
    Signed-off-by: Eric W. Biederman <eric@maxwell.lnxi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 76c8e2dc48dd..39d40c518531 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -436,7 +436,7 @@ struct signal_struct {
 
 	/* job control IDs */
 	pid_t pgrp;
-	pid_t tty_old_pgrp;
+	struct pid *tty_old_pgrp;
 
 	union {
 		pid_t session __deprecated;

commit 4b98d11b40f03382918796f3c5c936d5495d20a4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Feb 10 01:46:45 2007 -0800

    [PATCH] ifdef ->rchar, ->wchar, ->syscr, ->syscw from task_struct
    
    They are fat: 4x8 bytes in task_struct.
    They are uncoditionally updated in every fork, read, write and sendfile.
    They are used only if you have some "extended acct fields feature".
    
    And please, please, please, read(2) knows about bytes, not characters,
    why it is called "rchar"?
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 446373535190..76c8e2dc48dd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1013,8 +1013,10 @@ struct task_struct {
  * to a stack based synchronous wait) if its doing sync IO.
  */
 	wait_queue_t *io_wait;
+#ifdef CONFIG_TASK_XACCT
 /* i/o counters(bytes read/written, #syscalls */
 	u64 rchar, wchar, syscr, syscw;
+#endif
 	struct task_io_accounting ioac;
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
@@ -1649,6 +1651,44 @@ extern int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls);
 
 extern void normalize_rt_tasks(void);
 
+#ifdef CONFIG_TASK_XACCT
+static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
+{
+	tsk->rchar += amt;
+}
+
+static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
+{
+	tsk->wchar += amt;
+}
+
+static inline void inc_syscr(struct task_struct *tsk)
+{
+	tsk->syscr++;
+}
+
+static inline void inc_syscw(struct task_struct *tsk)
+{
+	tsk->syscw++;
+}
+#else
+static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
+{
+}
+
+static inline void add_wchar(struct task_struct *tsk, ssize_t amt)
+{
+}
+
+static inline void inc_syscr(struct task_struct *tsk)
+{
+}
+
+static inline void inc_syscw(struct task_struct *tsk)
+{
+}
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif

commit 8a102eed9c4e1d21bad07a8fd97bd4fbf125d966
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Dec 13 00:34:30 2006 -0800

    [PATCH] PM: Fix SMP races in the freezer
    
    Currently, to tell a task that it should go to the refrigerator, we set the
    PF_FREEZE flag for it and send a fake signal to it.  Unfortunately there
    are two SMP-related problems with this approach.  First, a task running on
    another CPU may be updating its flags while the freezer attempts to set
    PF_FREEZE for it and this may leave the task's flags in an inconsistent
    state.  Second, there is a potential race between freeze_process() and
    refrigerator() in which freeze_process() running on one CPU is reading a
    task's PF_FREEZE flag while refrigerator() running on another CPU has just
    set PF_FROZEN for the same task and attempts to reset PF_FREEZE for it.  If
    the refrigerator wins the race, freeze_process() will state that PF_FREEZE
    hasn't been set for the task and will set it unnecessarily, so the task
    will go to the refrigerator once again after it's been thawed.
    
    To solve first of these problems we need to stop using PF_FREEZE to tell
    tasks that they should go to the refrigerator.  Instead, we can introduce a
    special TIF_*** flag and use it for this purpose, since it is allowed to
    change the other tasks' TIF_*** flags and there are special calls for it.
    
    To avoid the freeze_process()-refrigerator() race we can make
    freeze_process() to always check the task's PF_FROZEN flag after it's read
    its "freeze" flag.  We should also make sure that refrigerator() will
    always reset the task's "freeze" flag after it's set PF_FROZEN for it.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea92e5c89089..446373535190 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1144,7 +1144,6 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_FLUSHER	0x00001000	/* responsible for disk writeback */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
-#define PF_FREEZE	0x00004000	/* this task is being frozen for suspend now */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */

commit 06066714f6016cffcb249f6ab21b7919de1bc859
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Sun Dec 10 02:20:35 2006 -0800

    [PATCH] sched: remove lb_stopbalance counter
    
    Remove scheduler stats lb_stopbalance counter.  This counter can be
    calculated by: lb_balanced - lb_nobusyg - lb_nobusyq.  There is no need to
    create gazillion counters while we can derive the value.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 72d6927d29ed..ea92e5c89089 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -707,7 +707,6 @@ struct sched_domain {
 	unsigned long lb_hot_gained[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyg[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyq[MAX_IDLE_TYPES];
-	unsigned long lb_stopbalance[MAX_IDLE_TYPES];
 
 	/* Active load balancing */
 	unsigned long alb_cnt;

commit 783609c6cb4eaa23f2ac5c968a44483584ec133f
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Sun Dec 10 02:20:33 2006 -0800

    [PATCH] sched: decrease number of load balances
    
    Currently at a particular domain, each cpu in the sched group will do a
    load balance at the frequency of balance_interval.  More the cores and
    threads, more the cpus will be in each sched group at SMP and NUMA domain.
    And we endup spending quite a bit of time doing load balancing in those
    domains.
    
    Fix this by making only one cpu(first idle cpu or first cpu in the group if
    all the cpus are busy) in the sched group do the load balance at that
    particular sched domain and this load will slowly percolate down to the
    other cpus with in that group(when they do load balancing at lower
    domains).
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea92e5c89089..72d6927d29ed 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -707,6 +707,7 @@ struct sched_domain {
 	unsigned long lb_hot_gained[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyg[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyq[MAX_IDLE_TYPES];
+	unsigned long lb_stopbalance[MAX_IDLE_TYPES];
 
 	/* Active load balancing */
 	unsigned long alb_cnt;

commit 08c183f31bdbb709f177f6d3110d5f288ea33933
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Dec 10 02:20:29 2006 -0800

    [PATCH] sched: add option to serialize load balancing
    
    Large sched domains can be very expensive to scan.  Add an option SD_SERIALIZE
    to the sched domain flags.  If that flag is set then we make sure that no
    other such domain is being balanced.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Peter Williams <pwil3058@bigpond.net.au>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1208feab46e0..ea92e5c89089 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -648,6 +648,7 @@ enum idle_type
 #define SD_SHARE_CPUPOWER	128	/* Domain members share cpu power */
 #define SD_POWERSAVINGS_BALANCE	256	/* Balance for power savings */
 #define SD_SHARE_PKG_RESOURCES	512	/* Domain members share cpu pkg resources */
+#define SD_SERIALIZE		1024	/* Only a single load balancing instance */
 
 #define BALANCE_FOR_MC_POWER	\
 	(sched_smt_power_savings ? SD_POWERSAVINGS_BALANCE : 0)

commit 7c3ab7381e79dfc7db14a67c6f4f3285664e1ec2
Author: Andrew Morton <akpm@osdl.org>
Date:   Sun Dec 10 02:19:19 2006 -0800

    [PATCH] io-accounting: core statistics
    
    The present per-task IO accounting isn't very useful.  It simply counts the
    number of bytes passed into read() and write().  So if a process reads 1MB
    from an already-cached file, it is accused of having performed 1MB of I/O,
    which is wrong.
    
    (David Wright had some comments on the applicability of the present logical IO accounting:
    
      For billing purposes it is useless but for workload analysis it is very
      useful
    
      read_bytes/read_calls  average read request size
      write_bytes/write_calls average write request size
    
      read_bytes/read_blocks ie logical/physical can indicate hit rate or thrashing
      write_bytes/write_blocks  ie logical/physical  guess since pdflush writes can
                                                    be missed
    
      I often look for logical larger than physical to see filesystem cache
      problems.  And the bytes/cpusec can help find applications that are
      dominating the cache and causing slow interactive response from page cache
      contention.
    
      I want to find the IO intensive applications and make sure they are doing
      efficient IO.  Thus the acctcms(sysV) or csacms command would give the high
      IO commands).
    
    This patchset adds new accounting which tries to be more accurate.  We account
    for three things:
    
    reads:
    
      attempt to count the number of bytes which this process really did cause
      to be fetched from the storage layer.  Done at the submit_bio() level, so it
      is accurate for block-backed filesystems.  I also attempt to wire up NFS and
      CIFS.
    
    writes:
    
      attempt to count the number of bytes which this process caused to be sent
      to the storage layer.  This is done at page-dirtying time.
    
      The big inaccuracy here is truncate.  If a process writes 1MB to a file
      and then deletes the file, it will in fact perform no writeout.  But it will
      have been accounted as having caused 1MB of write.
    
      So...
    
    cancelled_writes:
    
      account the number of bytes which this process caused to not happen, by
      truncating pagecache.
    
      We _could_ just subtract this from the process's `write' accounting.  But
      that means that some processes would be reported to have done negative
      amounts of write IO, which is silly.
    
      So we just report the raw number and punt this decision up to userspace.
    
    Now, we _could_ account for writes at the physical I/O level.  But
    
    - This would require that we track memory-dirtying tasks at the per-page
      level (would require a new pointer in struct page).
    
    - It would mean that IO statistics for a process are usually only available
      long after that process has exitted.  Which means that we probably cannot
      communicate this info via taskstats.
    
    This patch:
    
    Wire up the kernel-private data structures and the accessor functions to
    manipulate them.
    
    Cc: Jay Lan <jlan@sgi.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Cc: David Wright <daw@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ad9c46071ff8..1208feab46e0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -82,6 +82,7 @@ struct sched_param {
 #include <linux/resource.h>
 #include <linux/timer.h>
 #include <linux/hrtimer.h>
+#include <linux/task_io_accounting.h>
 
 #include <asm/processor.h>
 
@@ -1013,6 +1014,7 @@ struct task_struct {
 	wait_queue_t *io_wait;
 /* i/o counters(bytes read/written, #syscalls */
 	u64 rchar, wchar, syscr, syscw;
+	struct task_io_accounting ioac;
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */

commit f4f154fd920b2178382a6a24a236348e4429ebc1
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Dec 8 02:39:47 2006 -0800

    [PATCH] fault injection: process filtering for fault-injection capabilities
    
    This patch provides process filtering feature.
    The process filter allows failing only permitted processes
    by /proc/<pid>/make-it-fail
    
    Please see the example that demostrates how to inject slab allocation
    failures into module init/cleanup code
    in Documentation/fault-injection/fault-injection.txt
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f0317edea141..ad9c46071ff8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1045,6 +1045,9 @@ struct task_struct {
 #ifdef	CONFIG_TASK_DELAY_ACCT
 	struct task_delay_info *delays;
 #endif
+#ifdef CONFIG_FAULT_INJECTION
+	int make_it_fail;
+#endif
 };
 
 static inline pid_t process_group(struct task_struct *tsk)

commit 84d737866e2babdeab0c6b18ea155c6a649663b8
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Fri Dec 8 02:38:01 2006 -0800

    [PATCH] add child reaper to pid_namespace
    
    Add a per pid_namespace child-reaper.  This is needed so processes are reaped
    within the same pid space and do not spill over to the parent pid space.  Its
    also needed so containers preserve existing semantic that pid == 1 would reap
    orphaned children.
    
    This is based on Eric Biederman's patch: http://lkml.org/lkml/2006/2/6/285
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6fec1d419714..f0317edea141 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1400,7 +1400,6 @@ extern NORET_TYPE void do_group_exit(int);
 extern void daemonize(const char *, ...);
 extern int allow_signal(int);
 extern int disallow_signal(int);
-extern struct task_struct *child_reaper;
 
 extern int do_execve(char *, char __user * __user *, char __user * __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);

commit 1ec320afdc9552c92191d5f89fcd1ebe588334ca
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:55 2006 -0800

    [PATCH] add process_session() helper routine: deprecate old field
    
    Add an anonymous union and ((deprecated)) to catch direct usage of the
    session field.
    
    [akpm@osdl.org: fix various missed conversions]
    [jdike@addtoit.com: fix UML bug]
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 270d864a8ff1..6fec1d419714 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -436,7 +436,12 @@ struct signal_struct {
 	/* job control IDs */
 	pid_t pgrp;
 	pid_t tty_old_pgrp;
-	pid_t session;
+
+	union {
+		pid_t session __deprecated;
+		pid_t __session;
+	};
+
 	/* boolean value for session group leader */
 	int leader;
 
@@ -1047,9 +1052,19 @@ static inline pid_t process_group(struct task_struct *tsk)
 	return tsk->signal->pgrp;
 }
 
+static inline pid_t signal_session(struct signal_struct *sig)
+{
+	return sig->__session;
+}
+
 static inline pid_t process_session(struct task_struct *tsk)
 {
-	return tsk->signal->session;
+	return signal_session(tsk->signal);
+}
+
+static inline void set_signal_session(struct signal_struct *sig, pid_t session)
+{
+	sig->__session = session;
 }
 
 static inline struct pid *task_pid(struct task_struct *task)

commit 937949d9edbf4049bd41af6c9f92c26280584564
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:54 2006 -0800

    [PATCH] add process_session() helper routine
    
    Replace occurences of task->signal->session by a new process_session() helper
    routine.
    
    It will be useful for pid namespaces to abstract the session pid number.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5e8a0ba61749..270d864a8ff1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1047,6 +1047,11 @@ static inline pid_t process_group(struct task_struct *tsk)
 	return tsk->signal->pgrp;
 }
 
+static inline pid_t process_session(struct task_struct *tsk)
+{
+	return tsk->signal->session;
+}
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;

commit ae424ae4b5bcd820ad6ee6f0b986c4e14ed4d6cf
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Dec 8 02:36:08 2006 -0800

    [PATCH] make set_special_pids() static
    
    Make set_special_pids() static, the only caller is daemonize().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dede82c63445..5e8a0ba61749 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1240,7 +1240,6 @@ extern struct   mm_struct init_mm;
 
 #define find_task_by_pid(nr)	find_task_by_pid_type(PIDTYPE_PID, nr)
 extern struct task_struct *find_task_by_pid_type(int type, int pid);
-extern void set_special_pids(pid_t session, pid_t pgrp);
 extern void __set_special_pids(pid_t session, pid_t pgrp);
 
 /* per-UID process charging. */

commit 15ad7cdcfd76450d4beebc789ec646664238184d
Author: Helge Deller <deller@gmx.de>
Date:   Wed Dec 6 20:40:36 2006 -0800

    [PATCH] struct seq_operations and struct file_operations constification
    
     - move some file_operations structs into the .rodata section
    
     - move static strings from policy_types[] array into the .rodata section
    
     - fix generic seq_operations usages, so that those structs may be defined
       as "const" as well
    
    [akpm@osdl.org: couple of fixes]
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3a767242e72f..dede82c63445 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -573,7 +573,7 @@ struct sched_info {
 #endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 
 #ifdef CONFIG_SCHEDSTATS
-extern struct file_operations proc_schedstat_operations;
+extern const struct file_operations proc_schedstat_operations;
 #endif /* CONFIG_SCHEDSTATS */
 
 #ifdef CONFIG_TASK_DELAY_ACCT

commit d3228a887cae75ef2b8b1211c31c539bef5a5698
Author: Adrian Bunk <bunk@stusta.de>
Date:   Wed Dec 6 20:38:22 2006 -0800

    [PATCH] make kernel/signal.c:kill_proc_info() static
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0a90cefb0b0d..3a767242e72f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1305,7 +1305,6 @@ extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
-extern int kill_proc_info(int, struct siginfo *, pid_t);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);

commit e59e2ae2c29700117a54e85c106017c24837119f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 6 20:35:59 2006 -0800

    [PATCH] SysRq-X: show blocked tasks
    
    Add SysRq-X support: show blocked (TASK_UNINTERRUPTIBLE) tasks only.
    
    Useful for debugging IO stalls.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 837a012f573c..0a90cefb0b0d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -194,7 +194,16 @@ extern void init_idle(struct task_struct *idle, int cpu);
 
 extern cpumask_t nohz_cpu_mask;
 
-extern void show_state(void);
+/*
+ * Only dump TASK_* tasks. (-1 for all tasks)
+ */
+extern void show_state_filter(unsigned long state_filter);
+
+static inline void show_state(void)
+{
+	show_state_filter(-1);
+}
+
 extern void show_regs(struct pt_regs *);
 
 /*

commit 7dfb71030f7636a0d65200158113c37764552f93
Author: Nigel Cunningham <ncunningham@linuxmail.org>
Date:   Wed Dec 6 20:34:23 2006 -0800

    [PATCH] Add include/linux/freezer.h and move definitions from sched.h
    
    Move process freezing functions from include/linux/sched.h to freezer.h, so
    that modifications to the freezer or the kernel configuration don't require
    recompiling just about everything.
    
    [akpm@osdl.org: fix ueagle driver]
    Signed-off-by: Nigel Cunningham <nigel@suspend2.net>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index acfd2e15c5f2..837a012f573c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1618,87 +1618,6 @@ extern int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls);
 
 extern void normalize_rt_tasks(void);
 
-#ifdef CONFIG_PM
-/*
- * Check if a process has been frozen
- */
-static inline int frozen(struct task_struct *p)
-{
-	return p->flags & PF_FROZEN;
-}
-
-/*
- * Check if there is a request to freeze a process
- */
-static inline int freezing(struct task_struct *p)
-{
-	return p->flags & PF_FREEZE;
-}
-
-/*
- * Request that a process be frozen
- * FIXME: SMP problem. We may not modify other process' flags!
- */
-static inline void freeze(struct task_struct *p)
-{
-	p->flags |= PF_FREEZE;
-}
-
-/*
- * Sometimes we may need to cancel the previous 'freeze' request
- */
-static inline void do_not_freeze(struct task_struct *p)
-{
-	p->flags &= ~PF_FREEZE;
-}
-
-/*
- * Wake up a frozen process
- */
-static inline int thaw_process(struct task_struct *p)
-{
-	if (frozen(p)) {
-		p->flags &= ~PF_FROZEN;
-		wake_up_process(p);
-		return 1;
-	}
-	return 0;
-}
-
-/*
- * freezing is complete, mark process as frozen
- */
-static inline void frozen_process(struct task_struct *p)
-{
-	p->flags = (p->flags & ~PF_FREEZE) | PF_FROZEN;
-}
-
-extern void refrigerator(void);
-extern int freeze_processes(void);
-extern void thaw_processes(void);
-
-static inline int try_to_freeze(void)
-{
-	if (freezing(current)) {
-		refrigerator();
-		return 1;
-	} else
-		return 0;
-}
-#else
-static inline int frozen(struct task_struct *p) { return 0; }
-static inline int freezing(struct task_struct *p) { return 0; }
-static inline void freeze(struct task_struct *p) { BUG(); }
-static inline int thaw_process(struct task_struct *p) { return 1; }
-static inline void frozen_process(struct task_struct *p) { BUG(); }
-
-static inline void refrigerator(void) {}
-static inline int freeze_processes(void) { BUG(); return 0; }
-static inline void thaw_processes(void) {}
-
-static inline int try_to_freeze(void) { return 0; }
-
-#endif /* CONFIG_PM */
 #endif /* __KERNEL__ */
 
 #endif

commit 36de6437866bbb1d37e2312ff4f95ee4ed6d2b61
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Wed Dec 6 20:33:42 2006 -0800

    [PATCH] Save some bytes in struct mm_struct
    
    Before:
    [acme@newtoy net-2.6.20]$ pahole --cacheline 32 kernel/sched.o mm_struct
    
    /* include2/asm/processor.h:542 */
    struct mm_struct {
            struct vm_area_struct *    mmap;                 /*     0     4 */
            struct rb_root             mm_rb;                /*     4     4 */
            struct vm_area_struct *    mmap_cache;           /*     8     4 */
            long unsigned int          (*get_unmapped_area)(); /*    12     4 */
            void                       (*unmap_area)();      /*    16     4 */
            long unsigned int          mmap_base;            /*    20     4 */
            long unsigned int          task_size;            /*    24     4 */
            long unsigned int          cached_hole_size;     /*    28     4 */
            /* ---------- cacheline 1 boundary ---------- */
            long unsigned int          free_area_cache;      /*    32     4 */
            pgd_t *                    pgd;                  /*    36     4 */
            atomic_t                   mm_users;             /*    40     4 */
            atomic_t                   mm_count;             /*    44     4 */
            int                        map_count;            /*    48     4 */
            struct rw_semaphore        mmap_sem;             /*    52    64 */
            spinlock_t                 page_table_lock;      /*   116    40 */
            struct list_head           mmlist;               /*   156     8 */
            mm_counter_t               _file_rss;            /*   164     4 */
            mm_counter_t               _anon_rss;            /*   168     4 */
            long unsigned int          hiwater_rss;          /*   172     4 */
            long unsigned int          hiwater_vm;           /*   176     4 */
            long unsigned int          total_vm;             /*   180     4 */
            long unsigned int          locked_vm;            /*   184     4 */
            long unsigned int          shared_vm;            /*   188     4 */
            /* ---------- cacheline 6 boundary ---------- */
            long unsigned int          exec_vm;              /*   192     4 */
            long unsigned int          stack_vm;             /*   196     4 */
            long unsigned int          reserved_vm;          /*   200     4 */
            long unsigned int          def_flags;            /*   204     4 */
            long unsigned int          nr_ptes;              /*   208     4 */
            long unsigned int          start_code;           /*   212     4 */
            long unsigned int          end_code;             /*   216     4 */
            long unsigned int          start_data;           /*   220     4 */
            /* ---------- cacheline 7 boundary ---------- */
            long unsigned int          end_data;             /*   224     4 */
            long unsigned int          start_brk;            /*   228     4 */
            long unsigned int          brk;                  /*   232     4 */
            long unsigned int          start_stack;          /*   236     4 */
            long unsigned int          arg_start;            /*   240     4 */
            long unsigned int          arg_end;              /*   244     4 */
            long unsigned int          env_start;            /*   248     4 */
            long unsigned int          env_end;              /*   252     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          saved_auxv[44];       /*   256   176 */
            unsigned int               dumpable:2;           /*   432     4 */
            cpumask_t                  cpu_vm_mask;          /*   436     4 */
            mm_context_t               context;              /*   440    68 */
            long unsigned int          swap_token_time;      /*   508     4 */
            /* ---------- cacheline 16 boundary ---------- */
            char                       recent_pagein;        /*   512     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            int                        core_waiters;         /*   516     4 */
            struct completion *        core_startup_done;    /*   520     4 */
            struct completion          core_done;            /*   524    52 */
            rwlock_t                   ioctx_list_lock;      /*   576    36 */
            struct kioctx *            ioctx_list;           /*   612     4 */
    }; /* size: 616, sum members: 613, holes: 1, sum holes: 3, cachelines: 20,
          last cacheline: 8 bytes */
    
    After:
    
    [acme@newtoy net-2.6.20]$ pahole --cacheline 32 kernel/sched.o mm_struct
    /* include2/asm/processor.h:542 */
    struct mm_struct {
            struct vm_area_struct *    mmap;                 /*     0     4 */
            struct rb_root             mm_rb;                /*     4     4 */
            struct vm_area_struct *    mmap_cache;           /*     8     4 */
            long unsigned int          (*get_unmapped_area)(); /*    12     4 */
            void                       (*unmap_area)();      /*    16     4 */
            long unsigned int          mmap_base;            /*    20     4 */
            long unsigned int          task_size;            /*    24     4 */
            long unsigned int          cached_hole_size;     /*    28     4 */
            /* ---------- cacheline 1 boundary ---------- */
            long unsigned int          free_area_cache;      /*    32     4 */
            pgd_t *                    pgd;                  /*    36     4 */
            atomic_t                   mm_users;             /*    40     4 */
            atomic_t                   mm_count;             /*    44     4 */
            int                        map_count;            /*    48     4 */
            struct rw_semaphore        mmap_sem;             /*    52    64 */
            spinlock_t                 page_table_lock;      /*   116    40 */
            struct list_head           mmlist;               /*   156     8 */
            mm_counter_t               _file_rss;            /*   164     4 */
            mm_counter_t               _anon_rss;            /*   168     4 */
            long unsigned int          hiwater_rss;          /*   172     4 */
            long unsigned int          hiwater_vm;           /*   176     4 */
            long unsigned int          total_vm;             /*   180     4 */
            long unsigned int          locked_vm;            /*   184     4 */
            long unsigned int          shared_vm;            /*   188     4 */
            /* ---------- cacheline 6 boundary ---------- */
            long unsigned int          exec_vm;              /*   192     4 */
            long unsigned int          stack_vm;             /*   196     4 */
            long unsigned int          reserved_vm;          /*   200     4 */
            long unsigned int          def_flags;            /*   204     4 */
            long unsigned int          nr_ptes;              /*   208     4 */
            long unsigned int          start_code;           /*   212     4 */
            long unsigned int          end_code;             /*   216     4 */
            long unsigned int          start_data;           /*   220     4 */
            /* ---------- cacheline 7 boundary ---------- */
            long unsigned int          end_data;             /*   224     4 */
            long unsigned int          start_brk;            /*   228     4 */
            long unsigned int          brk;                  /*   232     4 */
            long unsigned int          start_stack;          /*   236     4 */
            long unsigned int          arg_start;            /*   240     4 */
            long unsigned int          arg_end;              /*   244     4 */
            long unsigned int          env_start;            /*   248     4 */
            long unsigned int          env_end;              /*   252     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          saved_auxv[44];       /*   256   176 */
            cpumask_t                  cpu_vm_mask;          /*   432     4 */
            mm_context_t               context;              /*   436    68 */
            long unsigned int          swap_token_time;      /*   504     4 */
            char                       recent_pagein;        /*   508     1 */
            unsigned char              dumpable:2;           /*   509     1 */
    
            /* XXX 2 bytes hole, try to pack */
    
            int                        core_waiters;         /*   512     4 */
            struct completion *        core_startup_done;    /*   516     4 */
            struct completion          core_done;            /*   520    52 */
            rwlock_t                   ioctx_list_lock;      /*   572    36 */
            struct kioctx *            ioctx_list;           /*   608     4 */
    }; /* size: 612, sum members: 610, holes: 1, sum holes: 2, cachelines: 20,
          last cacheline: 4 bytes */
    
    [acme@newtoy net-2.6.20]$ codiff -V /tmp/sched.o.before kernel/sched.o
    /pub/scm/linux/kernel/git/acme/net-2.6.20/kernel/sched.c:
      struct mm_struct |   -4
        dumpable:2;
         from: unsigned int          /*   432(30)    4(2) */
         to:   unsigned char         /*   509(6)     1(2) */
    < SNIP other offset changes >
     1 struct changed
    [acme@newtoy net-2.6.20]$
    
    I'm not aware of any problem about using 2 byte wide bitfields where
    previously a 4 byte wide one was, holler if there is any, I wouldn't be
    surprised, bitfields are things from hell.
    
    For the curious, 432(30) means: at offset 432 from the struct start, at
    offset 30 in the bitfield (yeah, it comes backwards, hellish, huh?) ditto
    for 509(6), while 4(2) and 1(2) means "struct field size(bitfield size)".
    
    Now we have a 2 bytes hole and are using only 4 bytes of the last 32
    bytes cacheline, any takers? :-)
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index cad6a16260f7..acfd2e15c5f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -338,7 +338,6 @@ struct mm_struct {
 
 	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */
 
-	unsigned dumpable:2;
 	cpumask_t cpu_vm_mask;
 
 	/* Architecture-specific MM context */
@@ -355,6 +354,8 @@ struct mm_struct {
 	unsigned int token_priority;
 	unsigned int last_interval;
 
+	unsigned char dumpable:2;
+
 	/* coredumping support */
 	int core_waiters;
 	struct completion *core_startup_done, core_done;

commit 7602bdf2fd14a40dd9b104e516fdc05e1bd17952
Author: Ashwin Chaugule <ashwin.chaugule@celunite.com>
Date:   Wed Dec 6 20:31:57 2006 -0800

    [PATCH] new scheme to preempt swap token
    
    The new swap token patches replace the current token traversal algo.  The old
    algo had a crude timeout parameter that was used to handover the token from
    one task to another.  This algo, transfers the token to the tasks that are in
    need of the token.  The urgency for the token is based on the number of times
    a task is required to swap-in pages.  Accordingly, the priority of a task is
    incremented if it has been badly affected due to swap-outs.  To ensure that
    the token doesnt bounce around rapidly, the token holders are given a priority
    boost.  The priority of tasks is also decremented, if their rate of swap-in's
    keeps reducing.  This way, the condition to check whether to pre-empt the swap
    token, is a matter of comparing two task's priority fields.
    
    [akpm@osdl.org: cleanups]
    Signed-off-by: Ashwin Chaugule <ashwin.chaugule@celunite.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index eafe4a7b8237..cad6a16260f7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -344,9 +344,16 @@ struct mm_struct {
 	/* Architecture-specific MM context */
 	mm_context_t context;
 
-	/* Token based thrashing protection. */
-	unsigned long swap_token_time;
-	char recent_pagein;
+	/* Swap token stuff */
+	/*
+	 * Last value of global fault stamp as seen by this process.
+	 * In other words, this value gives an indication of how long
+	 * it has been since this task got the token.
+	 * Look at mm/thrash.c
+	 */
+	unsigned int faultstamp;
+	unsigned int token_priority;
+	unsigned int last_interval;
 
 	/* coredumping support */
 	int core_waiters;

commit b8534d7bd89df0cd41cd47bcd6733a05ea9a691a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Oct 28 10:38:53 2006 -0700

    [PATCH] taskstats: kill ->taskstats_lock in favor of ->siglock
    
    signal_struct is (mostly) protected by ->sighand->siglock, I think we don't
    need ->taskstats_lock to protect ->stats.  This also allows us to simplify the
    locking in fill_tgid().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6735c1cf334c..eafe4a7b8237 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -466,7 +466,6 @@ struct signal_struct {
 	struct pacct_struct pacct;	/* per-process accounting information */
 #endif
 #ifdef CONFIG_TASKSTATS
-	spinlock_t stats_lock;
 	struct taskstats *stats;
 #endif
 };

commit 3260259f0084e51ce21503b130473b78871e7077
Author: Henne <henne@nachtwindheim.de>
Date:   Fri Oct 6 00:44:01 2006 -0700

    [PATCH] sched: fix a kerneldoc error on is_init()
    
    Fix a kerneldoc warning and reorderd the description for is_init().
    
    Signed-off-by: Henrik Kretzschmar <henne@nachtwindheim.de>
    Cc: "Randy.Dunlap" <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 331f4502e92b..6735c1cf334c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1065,9 +1065,10 @@ static inline int pid_alive(struct task_struct *p)
 }
 
 /**
- * is_init - check if a task structure is the first user space
- *	     task the kernel created.
- * @p: Task structure to be checked.
+ * is_init - check if a task structure is init
+ * @tsk: Task structure to be checked.
+ *
+ * Check if a task structure is the first user space task the kernel created.
  */
 static inline int is_init(struct task_struct *tsk)
 {

commit 89c4710ee9bbbefe6a4d469d9f36266a92c275c5
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Oct 3 01:14:09 2006 -0700

    [PATCH] sched: cleanup sched_group cpu_power setup
    
    Up to now sched group's cpu_power for each sched domain is initialized
    independently.  This made the setup code ugly as the new sched domains are
    getting added.
    
    Make the sched group cpu_power setup code generic, by using domain child
    field and new domain flag in sched_domain.  For most of the sched
    domains(except NUMA), sched group's cpu_power is now computed generically
    using the domain properties of itself and of the child domain.
    
    sched groups in NUMA domains are setup little differently and hence they
    don't use this generic mechanism.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8e26c9069f15..331f4502e92b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -625,9 +625,17 @@ enum idle_type
 #define SD_WAKE_BALANCE		64	/* Perform balancing at task wakeup */
 #define SD_SHARE_CPUPOWER	128	/* Domain members share cpu power */
 #define SD_POWERSAVINGS_BALANCE	256	/* Balance for power savings */
+#define SD_SHARE_PKG_RESOURCES	512	/* Domain members share cpu pkg resources */
 
-#define BALANCE_FOR_POWER	((sched_mc_power_savings || sched_smt_power_savings) \
-				 ? SD_POWERSAVINGS_BALANCE : 0)
+#define BALANCE_FOR_MC_POWER	\
+	(sched_smt_power_savings ? SD_POWERSAVINGS_BALANCE : 0)
+
+#define BALANCE_FOR_PKG_POWER	\
+	((sched_mc_power_savings || sched_smt_power_savings) ?	\
+	 SD_POWERSAVINGS_BALANCE : 0)
+
+#define test_sd_parent(sd, flag)	((sd->parent &&		\
+					 (sd->parent->flags & flag)) ? 1 : 0)
 
 
 struct sched_group {

commit 1a84887080dc15f048db7c3a643e98f1435790d6
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Oct 3 01:14:08 2006 -0700

    [PATCH] sched: introduce child field in sched_domain
    
    Introduce the child field in sched_domain struct and use it in
    sched_balance_self().
    
    We will also use this field in cleaning up the sched group cpu_power
    setup(done in a different patch) code.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 38530232d92f..8e26c9069f15 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -644,6 +644,7 @@ struct sched_group {
 struct sched_domain {
 	/* These fields must be setup */
 	struct sched_domain *parent;	/* top domain must be null terminated */
+	struct sched_domain *child;	/* bottom domain must be null terminated */
 	struct sched_group *groups;	/* the balancing groups of the domain */
 	cpumask_t span;			/* span of all CPUs in this domain */
 	unsigned long min_interval;	/* Minimum balance interval ms */

commit 9ec52099e4b8678a60e9f93e41ad87885d64f3e6
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Mon Oct 2 02:19:00 2006 -0700

    [PATCH] replace cad_pid by a struct pid
    
    There are a few places in the kernel where the init task is signaled.  The
    ctrl+alt+del sequence is one them.  It kills a task, usually init, using a
    cached pid (cad_pid).
    
    This patch replaces the pid_t by a struct pid to avoid pid wrap around
    problem.  The struct pid is initialized at boot time in init() and can be
    modified through systctl with
    
            /proc/sys/kernel/cad_pid
    
    [ I haven't found any distro using it ? ]
    
    It also introduces a small helper routine kill_cad_pid() which is used
    where it seemed ok to use cad_pid instead of pid 1.
    
    [akpm@osdl.org: cleanups, build fix]
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9ba959e34266..38530232d92f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1065,6 +1065,8 @@ static inline int is_init(struct task_struct *tsk)
 	return tsk->pid == 1;
 }
 
+extern struct pid *cad_pid;
+
 extern void free_task(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
@@ -1292,6 +1294,11 @@ extern int send_group_sigqueue(int, struct sigqueue *,  struct task_struct *);
 extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
 
+static inline int kill_cad_pid(int sig, int priv)
+{
+	return kill_pid(cad_pid, sig, priv);
+}
+
 /* These can be the second arg to send_sig_info/send_group_sig_info.  */
 #define SEND_SIG_NOINFO ((struct siginfo *) 0)
 #define SEND_SIG_PRIV	((struct siginfo *) 1)

commit 25b21cb2f6d69b0475b134e0a3e8e269137270fa
Author: Kirill Korotaev <dev@openvz.org>
Date:   Mon Oct 2 02:18:19 2006 -0700

    [PATCH] IPC namespace core
    
    This patch set allows to unshare IPCs and have a private set of IPC objects
    (sem, shm, msg) inside namespace.  Basically, it is another building block of
    containers functionality.
    
    This patch implements core IPC namespace changes:
    - ipc_namespace structure
    - new config option CONFIG_IPC_NS
    - adds CLONE_NEWIPC flag
    - unshare support
    
    [clg@fr.ibm.com: small fix for unshare of ipc namespace]
    [akpm@osdl.org: build fix]
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a973e7012315..9ba959e34266 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -25,6 +25,7 @@
 #define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
 #define CLONE_STOPPED		0x02000000	/* Start in stopped state */
 #define CLONE_NEWUTS		0x04000000	/* New utsname group? */
+#define CLONE_NEWIPC		0x08000000	/* New ipcs */
 
 /*
  * Scheduling policies

commit 071df104f808b8195c40643dcb4d060681742e29
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:17 2006 -0700

    [PATCH] namespaces: utsname: implement CLONE_NEWUTS flag
    
    Implement a CLONE_NEWUTS flag, and use it at clone and sys_unshare.
    
    [clg@fr.ibm.com: IPC unshare fix]
    [bunk@stusta.de: cleanup]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 46d6f5be72f2..a973e7012315 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -24,6 +24,7 @@
 #define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
 #define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
 #define CLONE_STOPPED		0x02000000	/* Start in stopped state */
+#define CLONE_NEWUTS		0x04000000	/* New utsname group? */
 
 /*
  * Scheduling policies

commit 4865ecf1315b450ab3317a745a6678c04d311e40
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:14 2006 -0700

    [PATCH] namespaces: utsname: implement utsname namespaces
    
    This patch defines the uts namespace and some manipulators.
    Adds the uts namespace to task_struct, and initializes a
    system-wide init namespace.
    
    It leaves a #define for system_utsname so sysctl will compile.
    This define will be removed in a separate patch.
    
    [akpm@osdl.org: build fix, cleanup]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 670b89a20070..46d6f5be72f2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -753,6 +753,7 @@ static inline void prefetch_stack(struct task_struct *t) { }
 struct audit_context;		/* See audit.c */
 struct mempolicy;
 struct pipe_inode_info;
+struct uts_namespace;
 
 enum sleep_type {
 	SLEEP_NORMAL,

commit 1651e14e28a2d9f446018ef522882e0709a2ce4f
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:08 2006 -0700

    [PATCH] namespaces: incorporate fs namespace into nsproxy
    
    This moves the mount namespace into the nsproxy.  The mount namespace count
    now refers to the number of nsproxies point to it, rather than the number of
    tasks.  As a result, the unshare_namespace() function in kernel/fork.c no
    longer checks whether it is being shared.
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4fa631fa55e3..670b89a20070 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -238,7 +238,6 @@ extern signed long schedule_timeout_interruptible(signed long timeout);
 extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 
-struct namespace;
 struct nsproxy;
 
 /* Maximum number of active map areas.. This is a random (large) number */
@@ -897,8 +896,7 @@ struct task_struct {
 	struct fs_struct *fs;
 /* open file information */
 	struct files_struct *files;
-/* namespace */
-	struct namespace *namespace;
+/* namespaces */
 	struct nsproxy *nsproxy;
 /* signal handlers */
 	struct signal_struct *signal;

commit ab516013ad9ca47f1d3a936fa81303bfbf734d52
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:06 2006 -0700

    [PATCH] namespaces: add nsproxy
    
    This patch adds a nsproxy structure to the task struct.  Later patches will
    move the fs namespace pointer into this structure, and introduce a new utsname
    namespace into the nsproxy.
    
    The vserver and openvz functionality, then, would be implemented in large part
    by virtualizing/isolating more and more resources into namespaces, each
    contained in the nsproxy.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a7fff3304bd6..4fa631fa55e3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -239,6 +239,7 @@ extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 
 struct namespace;
+struct nsproxy;
 
 /* Maximum number of active map areas.. This is a random (large) number */
 #define DEFAULT_MAX_MAP_COUNT	65536
@@ -898,6 +899,7 @@ struct task_struct {
 	struct files_struct *files;
 /* namespace */
 	struct namespace *namespace;
+	struct nsproxy *nsproxy;
 /* signal handlers */
 	struct signal_struct *signal;
 	struct sighand_struct *sighand;

commit 2425c08b37244005ff221efe4957d8aaff18609c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Oct 2 02:17:28 2006 -0700

    [PATCH] usb: fixup usb so it uses struct pid
    
    The problem with remembering a user space process by its pid is that it is
    possible that the process will exit, pid wrap around will occur.
    Converting to a struct pid avoid that problem, and paves the way for
    implementing a pid namespace.
    
    Also since usb is the only user of kill_proc_info_as_uid rename
    kill_proc_info_as_uid to kill_pid_info_as_uid and have the new version take
    a struct pid.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b7c99265ace..a7fff3304bd6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1269,12 +1269,12 @@ extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 extern int kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
 extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
+extern int kill_pid_info_as_uid(int, struct siginfo *, struct pid *, uid_t, uid_t, u32);
 extern int kill_pgrp(struct pid *pid, int sig, int priv);
 extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern int kill_proc_info_as_uid(int, struct siginfo *, pid_t, uid_t, uid_t, u32);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);

commit 3fbc96486459324e182717b03c50c90c880be6ec
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Mon Oct 2 02:17:24 2006 -0700

    [PATCH] Define struct pspace
    
    Define a per-container pid space object.  And create one instance of this
    object, init_pspace, to define the entire pid space.  Subsequent patches
    will provide/use interfaces to create/destroy pid spaces.
    
    Its a subset/rework of Eric Biederman's patch
    http://lkml.org/lkml/2006/2/6/285 .
    
    Signed-off-by: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dd6c2164e4a4..3b7c99265ace 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -118,7 +118,6 @@ extern unsigned long avenrun[];		/* Load averages */
 
 extern unsigned long total_forks;
 extern int nr_threads;
-extern int last_pid;
 DECLARE_PER_CPU(unsigned long, process_counts);
 extern int nr_processes(void);
 extern unsigned long nr_running(void);

commit c4b92fc112f7be5cce308128236ff75cc98535c3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Oct 2 02:17:10 2006 -0700

    [PATCH] pid: implement signal functions that take a struct pid *
    
    Currently the signal functions all either take a task or a pid_t argument.
    This patch implements variants that take a struct pid *.  After all of the
    users have been update it is my intention to remove the variants that take a
    pid_t as using pid_t can be more work (an extra hash table lookup) and
    difficult to get right in the presence of multiple pid namespaces.
    
    There are two kinds of functions introduced in this patch.  The are the
    general use functions kill_pgrp and kill_pid which take a priv argument that
    is ultimately used to create the appropriate siginfo information, Then there
    are _kill_pgrp_info, kill_pgrp_info, kill_pid_info the internal implementation
    helpers that take an explicit siginfo.
    
    The distinction is made because filling out an explcit siginfo is tricky, and
    will be even more tricky when pid namespaces are introduced.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 660b02f80523..dd6c2164e4a4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1267,6 +1267,11 @@ extern int send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int send_group_sig_info(int, struct siginfo *, struct task_struct *);
 extern int force_sigsegv(int, struct task_struct *);
 extern int force_sig_info(int, struct siginfo *, struct task_struct *);
+extern int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
+extern int kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);
+extern int kill_pid_info(int sig, struct siginfo *info, struct pid *pid);
+extern int kill_pgrp(struct pid *pid, int sig, int priv);
+extern int kill_pid(struct pid *pid, int sig, int priv);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
 extern int kill_proc_info(int, struct siginfo *, pid_t);

commit 22c935f47c03399c78e64c71b757eb36fa917ff6
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Oct 2 02:17:09 2006 -0700

    [PATCH] pid: implement access helpers for a tacks various process groups
    
    In the last round of cleaning up the pid hash table a more general struct pid
    was introduced, that can be referenced counted.
    
    With the more general struct pid most if not all places where we store a pid_t
    we can now store a struct pid * and remove the need for a hash table lookup,
    and avoid any possible problems with pid roll over.
    
    Looking forward to the pid namespaces struct pid * gives us an absolute form a
    pid so we can compare and use them without caring which pid namespace we are
    in.
    
    This patchset introduces the infrastructure needed to use struct pid instead
    of pid_t, and then it goes on to convert two different kernel users that
    currently store a pid_t value.
    
    There are a lot more places to go but this is enough to get the basic idea.
    
    Before we can merge a pid namespace patch all of the kernel pid_t users need
    to be examined.  Those that deal with user space processes need to be
    converted to using a struct pid *.  Those that deal with kernel processes need
    to converted to using the kthread api.  A rare few that only use their current
    processes pid values get to be left alone.
    
    This patch:
    
    task_session returns the struct pid of a tasks session.
    task_pgrp    returns the struct pid of a tasks process group.
    task_tgid    returns the struct pid of a tasks thread group.
    task_pid     returns the struct pid of a tasks process id.
    
    These can be used to avoid unnecessary hash table lookups, and to implement
    safe pid comparisions in the face of a pid namespace.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index be658e33bd26..660b02f80523 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1020,6 +1020,26 @@ static inline pid_t process_group(struct task_struct *tsk)
 	return tsk->signal->pgrp;
 }
 
+static inline struct pid *task_pid(struct task_struct *task)
+{
+	return task->pids[PIDTYPE_PID].pid;
+}
+
+static inline struct pid *task_tgid(struct task_struct *task)
+{
+	return task->group_leader->pids[PIDTYPE_PID].pid;
+}
+
+static inline struct pid *task_pgrp(struct task_struct *task)
+{
+	return task->group_leader->pids[PIDTYPE_PGID].pid;
+}
+
+static inline struct pid *task_session(struct task_struct *task)
+{
+	return task->group_leader->pids[PIDTYPE_SID].pid;
+}
+
 /**
  * pid_alive - check that a task structure is not stale
  * @p: Task structure to be checked.

commit 0804ef4b0de7121261f77c565b20a11ac694e877
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Oct 2 02:17:04 2006 -0700

    [PATCH] proc: readdir race fix (take 3)
    
    The problem: An opendir, readdir, closedir sequence can fail to report
    process ids that are continually in use throughout the sequence of system
    calls.  For this race to trigger the process that proc_pid_readdir stops at
    must exit before readdir is called again.
    
    This can cause ps to fail to report processes, and it is in violation of
    posix guarantees and normal application expectations with respect to
    readdir.
    
    Currently there is no way to work around this problem in user space short
    of providing a gargantuan buffer to user space so the directory read all
    happens in on system call.
    
    This patch implements the normal directory semantics for proc, that
    guarantee that a directory entry that is neither created nor destroyed
    while reading the directory entry will be returned.  For directory that are
    either created or destroyed during the readdir you may or may not see them.
     Furthermore you may seek to a directory offset you have previously seen.
    
    These are the guarantee that ext[23] provides and that posix requires, and
    more importantly that user space expects.  Plus it is a simple semantic to
    implement reliable service.  It is just a matter of calling readdir a
    second time if you are wondering if something new has show up.
    
    These better semantics are implemented by scanning through the pids in
    numerical order and by making the file offset a pid plus a fixed offset.
    
    The pid scan happens on the pid bitmap, which when you look at it is
    remarkably efficient for a brute force algorithm.  Given that a typical
    cache line is 64 bytes and thus covers space for 64*8 == 200 pids.  There
    are only 40 cache lines for the entire 32K pid space.  A typical system
    will have 100 pids or more so this is actually fewer cache lines we have to
    look at to scan a linked list, and the worst case of having to scan the
    entire pid bitmap is pretty reasonable.
    
    If we need something more efficient we can go to a more efficient data
    structure for indexing the pids, but for now what we have should be
    sufficient.
    
    In addition this takes no additional locks and is actually less code than
    what we are doing now.
    
    Also another very subtle bug in this area has been fixed.  It is possible
    to catch a task in the middle of de_thread where a thread is assuming the
    thread of it's thread group leader.  This patch carefully handles that case
    so if we hit it we don't fail to return the pid, that is undergoing the
    de_thread dance.
    
    Thanks to KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com> for
    providing the first fix, pointing this out and working on it.
    
    [oleg@tv-sign.ru: fix it]
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Jean Delvare <jdelvare@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7ef899c47c29..be658e33bd26 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1358,6 +1358,17 @@ extern void wait_task_inactive(struct task_struct * p);
 /* de_thread depends on thread_group_leader not being a pid based check */
 #define thread_group_leader(p)	(p == p->group_leader)
 
+/* Do to the insanities of de_thread it is possible for a process
+ * to have the pid of the thread group leader without actually being
+ * the thread group leader.  For iteration through the pids in proc
+ * all we care about is that we have a task with the appropriate
+ * pid, we don't actually care if we have the right task.
+ */
+static inline int has_group_leader_pid(struct task_struct *p)
+{
+	return p->pid == p->tgid;
+}
+
 static inline struct task_struct *next_thread(const struct task_struct *p)
 {
 	return list_entry(rcu_dereference(p->thread_group.next),

commit db5fed26b2e0beed939b773dd5896077a1794d65
Author: Jay Lan <jlan@sgi.com>
Date:   Sat Sep 30 23:29:00 2006 -0700

    [PATCH] csa accounting taskstats update
    
    ChangeLog:
       Feedbacks from Andrew Morton:
       - define TS_COMM_LEN to 32
       - change acct_stimexpd field of task_struct to be of
         cputime_t, which is to be used to save the tsk->stime
         of last timer interrupt update.
       - a new Documentation/accounting/taskstats-struct.txt
         to describe fields of taskstats struct.
    
       Feedback from Balbir Singh:
       - keep the stime of a task to be zero when both stime
         and utime are zero as recoreded in task_struct.
    
       Misc:
       - convert accumulated RSS/VM from platform dependent
         pages-ticks to MBytes-usecs in the kernel
    
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4ddeb0f982fb..7ef899c47c29 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -984,7 +984,7 @@ struct task_struct {
 #if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
-	clock_t acct_stimexpd;	/* clock_t-converted stime since last update */
+	cputime_t acct_stimexpd;/* stime since last update */
 #endif
 #ifdef CONFIG_NUMA
   	struct mempolicy *mempolicy;

commit 8f0ab5147951267134612570604cf8341901a80c
Author: Jay Lan <jlan@engr.sgi.com>
Date:   Sat Sep 30 23:28:59 2006 -0700

    [PATCH] csa: convert CONFIG tag for extended accounting routines
    
    There were a few accounting data/macros that are used in CSA but are #ifdef'ed
    inside CONFIG_BSD_PROCESS_ACCT.  This patch is to change those ifdef's from
    CONFIG_BSD_PROCESS_ACCT to CONFIG_TASK_XACCT.  A few defines are moved from
    kernel/acct.c and include/linux/acct.h to kernel/tsacct.c and
    include/linux/tsacct_kern.h.
    
    Signed-off-by: Jay Lan <jlan@sgi.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fc4a9873ec10..4ddeb0f982fb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -981,7 +981,7 @@ struct task_struct {
 	wait_queue_t *io_wait;
 /* i/o counters(bytes read/written, #syscalls */
 	u64 rchar, wchar, syscr, syscw;
-#if defined(CONFIG_BSD_PROCESS_ACCT)
+#if defined(CONFIG_TASK_XACCT)
 	u64 acct_rss_mem1;	/* accumulated rss usage */
 	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
 	clock_t acct_stimexpd;	/* clock_t-converted stime since last update */

commit 0d67a46df0125e20d14f12dbd3646f1f1bf23e8c
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 29 19:05:56 2006 +0100

    [PATCH] BLOCK: Remove duplicate declaration of exit_io_context() [try #6]
    
    Remove the duplicate declaration of exit_io_context() from linux/sched.h.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a06fc89cf6e5..fc4a9873ec10 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -710,7 +710,6 @@ extern unsigned int max_cache_size;
 
 
 struct io_context;			/* See blkdev.h */
-void exit_io_context(void);
 struct cpuset;
 
 #define NGROUPS_SMALL		32

commit c394cc9fbb367f87faa2228ec2eabacd2d4701c6
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:01:11 2006 -0700

    [PATCH] introduce TASK_DEAD state
    
    I am not sure about this patch, I am asking Ingo to take a decision.
    
    task_struct->state == EXIT_DEAD is a very special case, to avoid a confusion
    it makes sense to introduce a new state, TASK_DEAD, while EXIT_DEAD should
    live only in ->exit_state as documented in sched.h.
    
    Note that this state is not visible to user-space, get_task_state() masks off
    unsuitable states.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9763de334f09..a06fc89cf6e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -148,6 +148,7 @@ extern unsigned long weighted_cpuload(const int cpu);
 #define EXIT_DEAD		32
 /* in tsk->state again */
 #define TASK_NONINTERACTIVE	64
+#define TASK_DEAD		128
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit 55a101f8f71a3d3dbda7b5c77083ffe47552f831
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:01:10 2006 -0700

    [PATCH] kill PF_DEAD flag
    
    After the previous change (->flags & PF_DEAD) <=> (->state == EXIT_DEAD), we
    don't need PF_DEAD any longer.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fbc69cc3923d..9763de334f09 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1061,7 +1061,6 @@ static inline void put_task_struct(struct task_struct *t)
 					/* Not implemented yet, only for 486*/
 #define PF_STARTING	0x00000002	/* being created */
 #define PF_EXITING	0x00000004	/* getting shut down */
-#define PF_DEAD		0x00000008	/* Dead */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
 #define PF_DUMPCORE	0x00000200	/* dumped core */

commit 57a6f51c4281aa3119975473c70dce0480d322bd
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:00:49 2006 -0700

    [PATCH] introduce is_rt_policy() helper
    
    Imho, makes the code a bit easier to read.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 503dea61ff99..fbc69cc3923d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -504,8 +504,8 @@ struct signal_struct {
 #define rt_prio(prio)		unlikely((prio) < MAX_RT_PRIO)
 #define rt_task(p)		rt_prio((p)->prio)
 #define batch_task(p)		(unlikely((p)->policy == SCHED_BATCH))
-#define has_rt_policy(p) \
-	unlikely((p)->policy != SCHED_NORMAL && (p)->policy != SCHED_BATCH)
+#define is_rt_policy(p)		((p) != SCHED_NORMAL && (p) != SCHED_BATCH)
+#define has_rt_policy(p)	unlikely(is_rt_policy((p)->policy))
 
 /*
  * Some day this will be a full-fledged user tracking system..

commit 3171a0305d62e6627a24bff35af4f997e4988a80
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Fri Sep 29 02:00:32 2006 -0700

    [PATCH] simplify update_times (avoid jiffies/jiffies_64 aliasing problem)
    
    Pass ticks to do_timer() and update_times(), and adjust x86_64 and s390
    timer interrupt handler with this change.
    
    Currently update_times() calculates ticks by "jiffies - wall_jiffies", but
    callers of do_timer() should know how many ticks to update.  Passing ticks
    get rid of this redundant calculation.  Also there are another redundancy
    pointed out by Martin Schwidefsky.
    
    This cleanup make a barrier added by
    5aee405c662ca644980c184774277fc6d0769a84 needless.  So this patch removes
    it.
    
    As a bonus, this cleanup make wall_jiffies can be removed easily, since now
    wall_jiffies is always synced with jiffies.  (This patch does not really
    remove wall_jiffies.  It would be another cleanup patch)
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Acked-by: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Hirokazu Takata <takata.hirokazu@renesas.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Chris Zankel <chris@zankel.net>
    Acked-by: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ed2af8671589..503dea61ff99 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1206,7 +1206,7 @@ extern void switch_uid(struct user_struct *);
 
 #include <asm/current.h>
 
-extern void do_timer(struct pt_regs *);
+extern void do_timer(unsigned long ticks);
 
 extern int FASTCALL(wake_up_state(struct task_struct * tsk, unsigned int state));
 extern int FASTCALL(wake_up_process(struct task_struct * tsk));

commit f400e198b2ed26ce55b22a1412ded0896e7516ac
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Fri Sep 29 02:00:07 2006 -0700

    [PATCH] pidspace: is_init()
    
    This is an updated version of Eric Biederman's is_init() patch.
    (http://lkml.org/lkml/2006/2/6/280).  It applies cleanly to 2.6.18-rc3 and
    replaces a few more instances of ->pid == 1 with is_init().
    
    Further, is_init() checks pid and thus removes dependency on Eric's other
    patches for now.
    
    Eric's original description:
    
            There are a lot of places in the kernel where we test for init
            because we give it special properties.  Most  significantly init
            must not die.  This results in code all over the kernel test
            ->pid == 1.
    
            Introduce is_init to capture this case.
    
            With multiple pid spaces for all of the cases affected we are
            looking for only the first process on the system, not some other
            process that has pid == 1.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: <lxc-devel@lists.sourceforge.net>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3696f2f7126d..ed2af8671589 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1033,6 +1033,16 @@ static inline int pid_alive(struct task_struct *p)
 	return p->pids[PIDTYPE_PID].pid != NULL;
 }
 
+/**
+ * is_init - check if a task structure is the first user space
+ *	     task the kernel created.
+ * @p: Task structure to be checked.
+ */
+static inline int is_init(struct task_struct *tsk)
+{
+	return tsk->pid == 1;
+}
+
 extern void free_task(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 

commit 6c5c934153513dc72e2d6464f39e8ef1f27c0a3e
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 29 01:59:40 2006 -0700

    [PATCH] ifdef blktrace debugging fields
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 27122575d902..3696f2f7126d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -784,8 +784,9 @@ struct task_struct {
 	struct prio_array *array;
 
 	unsigned short ioprio;
+#ifdef CONFIG_BLK_DEV_IO_TRACE
 	unsigned int btrace_seq;
-
+#endif
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;
 	unsigned long long sched_time; /* sched_clock time spent running */

commit 3d5b6fccc4b900cc4267692f015ea500bad4c6bf
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 29 01:59:40 2006 -0700

    [PATCH] task_struct: ifdef Missed'em V IPC
    
    ipc/sem.c only.
    
    $ agrep sysvsem -w -n
    ipc/sem.c:912:  undo_list = current->sysvsem.undo_list;
    ipc/sem.c:932:  undo_list = current->sysvsem.undo_list;
    ipc/sem.c:954:  undo_list = current->sysvsem.undo_list;
    ipc/sem.c:963:          current->sysvsem.undo_list = undo_list;
    ipc/sem.c:1247:         tsk->sysvsem.undo_list = undo_list;
    ipc/sem.c:1249:         tsk->sysvsem.undo_list = NULL;
    ipc/sem.c:1271: undo_list = tsk->sysvsem.undo_list;
    include/linux/sched.h:876:      struct sysv_sem sysvsem;
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9d4aa7f95bc8..27122575d902 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -886,8 +886,10 @@ struct task_struct {
 				     - initialized normally by flush_old_exec */
 /* file system info */
 	int link_count, total_link_count;
+#ifdef CONFIG_SYSVIPC
 /* ipc stuff */
 	struct sysv_sem sysvsem;
+#endif
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /* filesystem information */

commit 0a4254058037eb172758961d0a5b94f4320a1425
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Tue Sep 26 10:52:38 2006 +0200

    [PATCH] Add the canary field to the PDA area and the task struct
    
    This patch adds the per thread cookie field to the task struct and the PDA.
    Also it makes sure that the PDA value gets the new cookie value at context
    switch, and that a new task gets a new cookie at task creation time.
    
    Signed-off-by: Arjan van Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andi Kleen <ak@suse.de>
    CC: Andi Kleen <ak@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 807556c5bcd2..9d4aa7f95bc8 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -819,6 +819,11 @@ struct task_struct {
 	unsigned did_exec:1;
 	pid_t pid;
 	pid_t tgid;
+
+#ifdef CONFIG_CC_STACKPROTECTOR
+	/* Canary value for the -fstack-protector gcc feature */
+	unsigned long stack_canary;
+#endif
 	/* 
 	 * pointers to (original) parent process, youngest child, younger sibling,
 	 * older sibling, respectively.  (p->father can be replaced with 

commit e07e23e1fd3000289fc7ccc6c71879070d3b19e0
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Tue Sep 26 10:52:36 2006 +0200

    [PATCH] non lazy "sleazy" fpu implementation
    
    Right now the kernel on x86-64 has a 100% lazy fpu behavior: after *every*
    context switch a trap is taken for the first FPU use to restore the FPU
    context lazily.  This is of course great for applications that have very
    sporadic or no FPU use (since then you avoid doing the expensive
    save/restore all the time).  However for very frequent FPU users...  you
    take an extra trap every context switch.
    
    The patch below adds a simple heuristic to this code: After 5 consecutive
    context switches of FPU use, the lazy behavior is disabled and the context
    gets restored every context switch.  If the app indeed uses the FPU, the
    trap is avoided.  (the chance of the 6th time slice using FPU after the
    previous 5 having done so are quite high obviously).
    
    After 256 switches, this is reset and lazy behavior is returned (until
    there are 5 consecutive ones again).  The reason for this is to give apps
    that do longer bursts of FPU use still the lazy behavior back after some
    time.
    
    [akpm@osdl.org: place new task_struct field next to jit_keyring to save space]
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 34ed0d99b1bd..807556c5bcd2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -865,6 +865,15 @@ struct task_struct {
 	struct key *thread_keyring;	/* keyring private to this thread */
 	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 #endif
+	/*
+	 * fpu_counter contains the number of consecutive context switches
+	 * that the FPU is used. If this is over a threshold, the lazy fpu
+	 * saving becomes unlazy to save the trap. This is an unsigned char
+	 * so that after 256 times the counter wraps and the behavior turns
+	 * lazy again; this to deal with bursty apps that only use FPU for
+	 * a short time
+	 */
+	unsigned char fpu_counter;
 	int oomkilladj; /* OOM kill score adjustment (bit shift). */
 	char comm[TASK_COMM_LEN]; /* executable name excluding path
 				     - access with [gs]et_task_comm (which lock

commit 35df17c57cecb08f0120fb18926325f1093dc429
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Thu Aug 31 21:27:38 2006 -0700

    [PATCH] task delay accounting fixes
    
    Cleanup allocation and freeing of tsk->delays used by delay accounting.
    This solves two problems reported for delay accounting:
    
    1. oops in __delayacct_blkio_ticks
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1844.html
    
    Currently tsk->delays is getting freed too early in task exit which can
    cause a NULL tsk->delays to get accessed via reading of /proc/<tgid>/stats.
     The patch fixes this problem by freeing tsk->delays closer to when
    task_struct itself is freed up.  As a result, it also eliminates the use of
    tsk->delays_lock which was only being used (inadequately) to safeguard
    access to tsk->delays while a task was exiting.
    
    2. Possible memory leak in kernel/delayacct.c
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1389.html
    
    The patch cleans up tsk->delays allocations after a bad fork which was
    missing earlier.
    
    The patch has been tested to fix the problems listed above and stress
    tested with rapid calls to delay accounting's taskstats command interface
    (which is the other path that can access the same data, besides the /proc
    interface causing the oops above).
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6674fc1e51bf..34ed0d99b1bd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -994,7 +994,6 @@ struct task_struct {
 	 */
 	struct pipe_inode_info *splice_pipe;
 #ifdef	CONFIG_TASK_DELAY_ACCT
-	spinlock_t delays_lock;
 	struct task_delay_info *delays;
 #endif
 };

commit a7ef7878ea7c8bca9b624db3f61223cdadda2a0a
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sat Aug 5 12:13:42 2006 -0700

    [PATCH] Make suspend possible with a traced process at a breakpoint
    
    It should be possible to suspend, either to RAM or to disk, if there's a
    traced process that has just reached a breakpoint.  However, this is a
    special case, because its parent process might have been frozen already and
    then we are unable to deliver the "freeze" signal to the traced process.
    If this happens, it's better to cancel the freezing of the traced process.
    
    Ref. http://bugzilla.kernel.org/show_bug.cgi?id=6787
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6afa72e080cb..6674fc1e51bf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1557,6 +1557,14 @@ static inline void freeze(struct task_struct *p)
 	p->flags |= PF_FREEZE;
 }
 
+/*
+ * Sometimes we may need to cancel the previous 'freeze' request
+ */
+static inline void do_not_freeze(struct task_struct *p)
+{
+	p->flags &= ~PF_FREEZE;
+}
+
 /*
  * Wake up a frozen process
  */

commit ad4ecbcba72855a2b5319b96e2a3a65ed1ca3bfd
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:44 2006 -0700

    [PATCH] delay accounting taskstats interface send tgid once
    
    Send per-tgid data only once during exit of a thread group instead of once
    with each member thread exit.
    
    Currently, when a thread exits, besides its per-tid data, the per-tgid data
    of its thread group is also sent out, if its thread group is non-empty.
    The per-tgid data sent consists of the sum of per-tid stats for all
    *remaining* threads of the thread group.
    
    This patch modifies this sending in two ways:
    
    - the per-tgid data is sent only when the last thread of a thread group
      exits.  This cuts down heavily on the overhead of sending/receiving
      per-tgid data, especially when other exploiters of the taskstats
      interface aren't interested in per-tgid stats
    
    - the semantics of the per-tgid data sent are changed.  Instead of being
      the sum of per-tid data for remaining threads, the value now sent is the
      true total accumalated statistics for all threads that are/were part of
      the thread group.
    
    The patch also addresses a minor issue where failure of one accounting
    subsystem to fill in the taskstats structure was causing the send of
    taskstats to not be sent at all.
    
    The patch has been tested for stability and run cerberus for over 4 hours
    on an SMP.
    
    [akpm@osdl.org: bugfixes]
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3c5610ca0c92..6afa72e080cb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -463,6 +463,10 @@ struct signal_struct {
 #ifdef CONFIG_BSD_PROCESS_ACCT
 	struct pacct_struct pacct;	/* per-process accounting information */
 #endif
+#ifdef CONFIG_TASKSTATS
+	spinlock_t stats_lock;
+	struct taskstats *stats;
+#endif
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */

commit 6f44993fe1d7b2b097f6ac60cd5835c6f5ca0874
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:41 2006 -0700

    [PATCH] per-task-delay-accounting: delay accounting usage of taskstats interface
    
    Usage of taskstats interface by delay accounting.
    
    Signed-off-by: Shailabh Nagar <nagar@us.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f751062d89a2..3c5610ca0c92 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -990,6 +990,7 @@ struct task_struct {
 	 */
 	struct pipe_inode_info *splice_pipe;
 #ifdef	CONFIG_TASK_DELAY_ACCT
+	spinlock_t delays_lock;
 	struct task_delay_info *delays;
 #endif
 };

commit 52f17b6c2bd443e7806a161e9d10a983650db01d
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Fri Jul 14 00:24:38 2006 -0700

    [PATCH] per-task-delay-accounting: cpu delay collection via schedstats
    
    Make the task-related schedstats functions callable by delay accounting even
    if schedstats collection isn't turned on.  This removes the dependency of
    delay accounting on schedstats.
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2f43f1fb7de7..f751062d89a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -537,7 +537,7 @@ extern struct user_struct root_user;
 struct backing_dev_info;
 struct reclaim_state;
 
-#ifdef CONFIG_SCHEDSTATS
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 struct sched_info {
 	/* cumulative counters */
 	unsigned long	cpu_time,	/* time spent on the cpu */
@@ -548,9 +548,11 @@ struct sched_info {
 	unsigned long	last_arrival,	/* when we last ran on a cpu */
 			last_queued;	/* when we were last queued to run */
 };
+#endif /* defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT) */
 
+#ifdef CONFIG_SCHEDSTATS
 extern struct file_operations proc_schedstat_operations;
-#endif
+#endif /* CONFIG_SCHEDSTATS */
 
 #ifdef CONFIG_TASK_DELAY_ACCT
 struct task_delay_info {
@@ -580,7 +582,19 @@ struct task_delay_info {
 	u32 swapin_count;	/* total count of the number of swapin block */
 				/* io operations performed */
 };
+#endif	/* CONFIG_TASK_DELAY_ACCT */
+
+static inline int sched_info_on(void)
+{
+#ifdef CONFIG_SCHEDSTATS
+	return 1;
+#elif defined(CONFIG_TASK_DELAY_ACCT)
+	extern int delayacct_on;
+	return delayacct_on;
+#else
+	return 0;
 #endif
+}
 
 enum idle_type
 {
@@ -777,7 +791,7 @@ struct task_struct {
 	cpumask_t cpus_allowed;
 	unsigned int time_slice, first_time_slice;
 
-#ifdef CONFIG_SCHEDSTATS
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
 #endif
 

commit 0ff922452df86f3e9a2c6f705c4588ec62d096a7
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:37 2006 -0700

    [PATCH] per-task-delay-accounting: sync block I/O and swapin delay collection
    
    Unlike earlier iterations of the delay accounting patches, now delays are only
    collected for the actual I/O waits rather than try and cover the delays seen
    in I/O submission paths.
    
    Account separately for block I/O delays incurred as a result of swapin page
    faults whose frequency can be affected by the task/process' rss limit.  Hence
    swapin delays can act as feedback for rss limit changes independent of I/O
    priority changes.
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7a54e62763c5..2f43f1fb7de7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -566,6 +566,19 @@ struct task_delay_info {
 	 * Atomicity of updates to XXX_delay, XXX_count protected by
 	 * single lock above (split into XXX_lock if contention is an issue).
 	 */
+
+	/*
+	 * XXX_count is incremented on every XXX operation, the delay
+	 * associated with the operation is added to XXX_delay.
+	 * XXX_delay contains the accumulated delay time in nanoseconds.
+	 */
+	struct timespec blkio_start, blkio_end;	/* Shared by blkio, swapin */
+	u64 blkio_delay;	/* wait for sync block io completion */
+	u64 swapin_delay;	/* wait for swapin block io completion */
+	u32 blkio_count;	/* total count of the number of sync block */
+				/* io operations performed */
+	u32 swapin_count;	/* total count of the number of swapin block */
+				/* io operations performed */
 };
 #endif
 

commit ca74e92b4698276b6696f15a801759f50944f387
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:36 2006 -0700

    [PATCH] per-task-delay-accounting: setup
    
    Initialization code related to collection of per-task "delay" statistics which
    measure how long it had to wait for cpu, sync block io, swapping etc.  The
    collection of statistics and the interface are in other patches.  This patch
    sets up the data structures and allows the statistics collection to be
    disabled through a kernel boot parameter.
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c876e27ff93..7a54e62763c5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -552,6 +552,23 @@ struct sched_info {
 extern struct file_operations proc_schedstat_operations;
 #endif
 
+#ifdef CONFIG_TASK_DELAY_ACCT
+struct task_delay_info {
+	spinlock_t	lock;
+	unsigned int	flags;	/* Private per-task flags */
+
+	/* For each stat XXX, add following, aligned appropriately
+	 *
+	 * struct timespec XXX_start, XXX_end;
+	 * u64 XXX_delay;
+	 * u32 XXX_count;
+	 *
+	 * Atomicity of updates to XXX_delay, XXX_count protected by
+	 * single lock above (split into XXX_lock if contention is an issue).
+	 */
+};
+#endif
+
 enum idle_type
 {
 	SCHED_IDLE,
@@ -945,6 +962,9 @@ struct task_struct {
 	 * cache last used pipe for splice
 	 */
 	struct pipe_inode_info *splice_pipe;
+#ifdef	CONFIG_TASK_DELAY_ACCT
+	struct task_delay_info *delays;
+#endif
 };
 
 static inline pid_t process_group(struct task_struct *tsk)

commit 70b97a7f0b19cf1f2619deb5cc41e8b78c591aa7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:42 2006 -0700

    [PATCH] sched: cleanup, convert sched.c-internal typedefs to struct
    
    convert:
    
     - runqueue_t to 'struct rq'
     - prio_array_t to 'struct prio_array'
     - migration_req_t to 'struct migration_req'
    
    I was the one who added these but they are both against the kernel coding
    style and also were used inconsistently at places.  So just get rid of them at
    once, now that we are flushing the scheduler patch-queue anyway.
    
    Conversion was mostly scripted, the result was reviewed and all secondary
    whitespace and style impact (if any) was fixed up by hand.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c2797f04d931..1c876e27ff93 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -534,7 +534,6 @@ extern struct user_struct *find_user(uid_t);
 extern struct user_struct root_user;
 #define INIT_USER (&root_user)
 
-typedef struct prio_array prio_array_t;
 struct backing_dev_info;
 struct reclaim_state;
 
@@ -715,6 +714,8 @@ enum sleep_type {
 	SLEEP_INTERRUPTED,
 };
 
+struct prio_array;
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	struct thread_info *thread_info;
@@ -732,7 +733,7 @@ struct task_struct {
 	int load_weight;	/* for niceness load balancing purposes */
 	int prio, static_prio, normal_prio;
 	struct list_head run_list;
-	prio_array_t *array;
+	struct prio_array *array;
 
 	unsigned short ioprio;
 	unsigned int btrace_seq;

commit 36c8b586896f60cb91a4fd526233190b34316baf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:41 2006 -0700

    [PATCH] sched: cleanup, remove task_t, convert to struct task_struct
    
    cleanup: remove task_t and convert all the uses to struct task_struct. I
    introduced it for the scheduler anno and it was a mistake.
    
    Conversion was mostly scripted, the result was reviewed and all
    secondary whitespace and style impact (if any) was fixed up by hand.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8ebddba4448d..c2797f04d931 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -184,11 +184,11 @@ extern unsigned long weighted_cpuload(const int cpu);
 extern rwlock_t tasklist_lock;
 extern spinlock_t mmlist_lock;
 
-typedef struct task_struct task_t;
+struct task_struct;
 
 extern void sched_init(void);
 extern void sched_init_smp(void);
-extern void init_idle(task_t *idle, int cpu);
+extern void init_idle(struct task_struct *idle, int cpu);
 
 extern cpumask_t nohz_cpu_mask;
 
@@ -383,7 +383,7 @@ struct signal_struct {
 	wait_queue_head_t	wait_chldexit;	/* for wait4() */
 
 	/* current thread group signal load-balancing target: */
-	task_t			*curr_target;
+	struct task_struct	*curr_target;
 
 	/* shared signal handling: */
 	struct sigpending	shared_pending;
@@ -699,7 +699,7 @@ extern int groups_search(struct group_info *group_info, gid_t grp);
     ((gi)->blocks[(i)/NGROUPS_PER_BLOCK][(i)%NGROUPS_PER_BLOCK])
 
 #ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
-extern void prefetch_stack(struct task_struct*);
+extern void prefetch_stack(struct task_struct *t);
 #else
 static inline void prefetch_stack(struct task_struct *t) { }
 #endif
@@ -1031,9 +1031,9 @@ static inline void put_task_struct(struct task_struct *t)
 #define used_math() tsk_used_math(current)
 
 #ifdef CONFIG_SMP
-extern int set_cpus_allowed(task_t *p, cpumask_t new_mask);
+extern int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask);
 #else
-static inline int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+static inline int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
 {
 	if (!cpu_isset(0, new_mask))
 		return -EINVAL;
@@ -1042,7 +1042,8 @@ static inline int set_cpus_allowed(task_t *p, cpumask_t new_mask)
 #endif
 
 extern unsigned long long sched_clock(void);
-extern unsigned long long current_sched_time(const task_t *current_task);
+extern unsigned long long
+current_sched_time(const struct task_struct *current_task);
 
 /* sched_exec is called by processes performing an exec */
 #ifdef CONFIG_SMP
@@ -1060,27 +1061,27 @@ static inline void idle_task_exit(void) {}
 extern void sched_idle_next(void);
 
 #ifdef CONFIG_RT_MUTEXES
-extern int rt_mutex_getprio(task_t *p);
-extern void rt_mutex_setprio(task_t *p, int prio);
-extern void rt_mutex_adjust_pi(task_t *p);
+extern int rt_mutex_getprio(struct task_struct *p);
+extern void rt_mutex_setprio(struct task_struct *p, int prio);
+extern void rt_mutex_adjust_pi(struct task_struct *p);
 #else
-static inline int rt_mutex_getprio(task_t *p)
+static inline int rt_mutex_getprio(struct task_struct *p)
 {
 	return p->normal_prio;
 }
 # define rt_mutex_adjust_pi(p)		do { } while (0)
 #endif
 
-extern void set_user_nice(task_t *p, long nice);
-extern int task_prio(const task_t *p);
-extern int task_nice(const task_t *p);
-extern int can_nice(const task_t *p, const int nice);
-extern int task_curr(const task_t *p);
+extern void set_user_nice(struct task_struct *p, long nice);
+extern int task_prio(const struct task_struct *p);
+extern int task_nice(const struct task_struct *p);
+extern int can_nice(const struct task_struct *p, const int nice);
+extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
-extern task_t *idle_task(int cpu);
-extern task_t *curr_task(int cpu);
-extern void set_curr_task(int cpu, task_t *p);
+extern struct task_struct *idle_task(int cpu);
+extern struct task_struct *curr_task(int cpu);
+extern void set_curr_task(int cpu, struct task_struct *p);
 
 void yield(void);
 
@@ -1137,8 +1138,8 @@ extern void FASTCALL(wake_up_new_task(struct task_struct * tsk,
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void FASTCALL(sched_fork(task_t * p, int clone_flags));
-extern void FASTCALL(sched_exit(task_t * p));
+extern void FASTCALL(sched_fork(struct task_struct * p, int clone_flags));
+extern void FASTCALL(sched_exit(struct task_struct * p));
 
 extern int in_group_p(gid_t);
 extern int in_egroup_p(gid_t);
@@ -1243,17 +1244,17 @@ extern NORET_TYPE void do_group_exit(int);
 extern void daemonize(const char *, ...);
 extern int allow_signal(int);
 extern int disallow_signal(int);
-extern task_t *child_reaper;
+extern struct task_struct *child_reaper;
 
 extern int do_execve(char *, char __user * __user *, char __user * __user *, struct pt_regs *);
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
-task_t *fork_idle(int);
+struct task_struct *fork_idle(int);
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
 extern void get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
-extern void wait_task_inactive(task_t * p);
+extern void wait_task_inactive(struct task_struct * p);
 #else
 #define wait_task_inactive(p)	do { } while (0)
 #endif
@@ -1279,13 +1280,13 @@ extern void wait_task_inactive(task_t * p);
 /* de_thread depends on thread_group_leader not being a pid based check */
 #define thread_group_leader(p)	(p == p->group_leader)
 
-static inline task_t *next_thread(const task_t *p)
+static inline struct task_struct *next_thread(const struct task_struct *p)
 {
 	return list_entry(rcu_dereference(p->thread_group.next),
-				task_t, thread_group);
+			  struct task_struct, thread_group);
 }
 
-static inline int thread_group_empty(task_t *p)
+static inline int thread_group_empty(struct task_struct *p)
 {
 	return list_empty(&p->thread_group);
 }

commit fbb9ce9530fd9b66096d5187fa6a115d16d9746c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:50 2006 -0700

    [PATCH] lockdep: core
    
    Do 'make oldconfig' and accept all the defaults for new config options -
    reboot into the kernel and if everything goes well it should boot up fine and
    you should have /proc/lockdep and /proc/lockdep_stats files.
    
    Typically if the lock validator finds some problem it will print out
    voluminous debug output that begins with "BUG: ..." and which syslog output
    can be used by kernel developers to figure out the precise locking scenario.
    
    What does the lock validator do?  It "observes" and maps all locking rules as
    they occur dynamically (as triggered by the kernel's natural use of spinlocks,
    rwlocks, mutexes and rwsems).  Whenever the lock validator subsystem detects a
    new locking scenario, it validates this new rule against the existing set of
    rules.  If this new rule is consistent with the existing set of rules then the
    new rule is added transparently and the kernel continues as normal.  If the
    new rule could create a deadlock scenario then this condition is printed out.
    
    When determining validity of locking, all possible "deadlock scenarios" are
    considered: assuming arbitrary number of CPUs, arbitrary irq context and task
    context constellations, running arbitrary combinations of all the existing
    locking scenarios.  In a typical system this means millions of separate
    scenarios.  This is why we call it a "locking correctness" validator - for all
    rules that are observed the lock validator proves it with mathematical
    certainty that a deadlock could not occur (assuming that the lock validator
    implementation itself is correct and its internal data structures are not
    corrupted by some other kernel subsystem).  [see more details and conditionals
    of this statement in include/linux/lockdep.h and
    Documentation/lockdep-design.txt]
    
    Furthermore, this "all possible scenarios" property of the validator also
    enables the finding of complex, highly unlikely multi-CPU multi-context races
    via single single-context rules, increasing the likelyhood of finding bugs
    drastically.  In practical terms: the lock validator already found a bug in
    the upstream kernel that could only occur on systems with 3 or more CPUs, and
    which needed 3 very unlikely code sequences to occur at once on the 3 CPUs.
    That bug was found and reported on a single-CPU system (!).  So in essence a
    race will be found "piecemail-wise", triggering all the necessary components
    for the race, without having to reproduce the race scenario itself!  In its
    short existence the lock validator found and reported many bugs before they
    actually caused a real deadlock.
    
    To further increase the efficiency of the validator, the mapping is not per
    "lock instance", but per "lock-class".  For example, all struct inode objects
    in the kernel have inode->inotify_mutex.  If there are 10,000 inodes cached,
    then there are 10,000 lock objects.  But ->inotify_mutex is a single "lock
    type", and all locking activities that occur against ->inotify_mutex are
    "unified" into this single lock-class.  The advantage of the lock-class
    approach is that all historical ->inotify_mutex uses are mapped into a single
    (and as narrow as possible) set of locking rules - regardless of how many
    different tasks or inode structures it took to build this set of rules.  The
    set of rules persist during the lifetime of the kernel.
    
    To see the rough magnitude of checking that the lock validator does, here's a
    portion of /proc/lockdep_stats, fresh after bootup:
    
     lock-classes:                            694 [max: 2048]
     direct dependencies:                  1598 [max: 8192]
     indirect dependencies:               17896
     all direct dependencies:             16206
     dependency chains:                    1910 [max: 8192]
     in-hardirq chains:                      17
     in-softirq chains:                     105
     in-process chains:                    1065
     stack-trace entries:                 38761 [max: 131072]
     combined max dependencies:         2033928
     hardirq-safe locks:                     24
     hardirq-unsafe locks:                  176
     softirq-safe locks:                     53
     softirq-unsafe locks:                  137
     irq-safe locks:                         59
     irq-unsafe locks:                      176
    
    The lock validator has observed 1598 actual single-thread locking patterns,
    and has validated all possible 2033928 distinct locking scenarios.
    
    More details about the design of the lock validator can be found in
    Documentation/lockdep-design.txt, which can also found at:
    
       http://redhat.com/~mingo/lockdep-patches/lockdep-design.txt
    
    [bunk@stusta.de: cleanups]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ad7a89014d29..8ebddba4448d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -886,6 +886,13 @@ struct task_struct {
 	int hardirq_context;
 	int softirq_context;
 #endif
+#ifdef CONFIG_LOCKDEP
+# define MAX_LOCK_DEPTH 30UL
+	u64 curr_chain_key;
+	int lockdep_depth;
+	struct held_lock held_locks[MAX_LOCK_DEPTH];
+	unsigned int lockdep_recursion;
+#endif
 
 /* journalling filesystem info */
 	void *journal_info;

commit de30a2b355ea85350ca2f58f3b9bf4e5bc007986
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:42 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, core
    
    Accurate hard-IRQ-flags and softirq-flags state tracing.
    
    This allows us to attach extra functionality to IRQ flags on/off
    events (such as trace-on/off).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bdabeee10a78..ad7a89014d29 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -871,6 +871,21 @@ struct task_struct {
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
 #endif
+#ifdef CONFIG_TRACE_IRQFLAGS
+	unsigned int irq_events;
+	int hardirqs_enabled;
+	unsigned long hardirq_enable_ip;
+	unsigned int hardirq_enable_event;
+	unsigned long hardirq_disable_ip;
+	unsigned int hardirq_disable_event;
+	int softirqs_enabled;
+	unsigned long softirq_disable_ip;
+	unsigned int softirq_disable_event;
+	unsigned long softirq_enable_ip;
+	unsigned int softirq_enable_event;
+	int hardirq_context;
+	int softirq_context;
+#endif
 
 /* journalling filesystem info */
 	void *journal_info;

commit 9a11b49a805665e13a56aa067afaf81d43ec1514
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:33 2006 -0700

    [PATCH] lockdep: better lock debugging
    
    Generic lock debugging:
    
     - generalized lock debugging framework. For example, a bug in one lock
       subsystem turns off debugging in all lock subsystems.
    
     - got rid of the caller address passing (__IP__/__IP_DECL__/etc.) from
       the mutex/rtmutex debugging code: it caused way too much prototype
       hackery, and lockdep will give the same information anyway.
    
     - ability to do silent tests
    
     - check lock freeing in vfree too.
    
     - more finegrained debugging options, to allow distributions to
       turn off more expensive debugging features.
    
    There's no separate 'held mutexes' list anymore - but there's a 'held locks'
    stack within lockdep, which unifies deadlock detection across all lock
    classes.  (this is independent of the lockdep validation stuff - lockdep first
    checks whether we are holding a lock already)
    
    Here are the current debugging options:
    
    CONFIG_DEBUG_MUTEXES=y
    CONFIG_DEBUG_LOCK_ALLOC=y
    
    which do:
    
     config DEBUG_MUTEXES
              bool "Mutex debugging, basic checks"
    
     config DEBUG_LOCK_ALLOC
             bool "Detect incorrect freeing of live mutexes"
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index aaf723308ed4..bdabeee10a78 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -865,10 +865,6 @@ struct task_struct {
 	struct plist_head pi_waiters;
 	/* Deadlock detection and priority inheritance handling */
 	struct rt_mutex_waiter *pi_blocked_on;
-# ifdef CONFIG_DEBUG_RT_MUTEXES
-	spinlock_t held_list_lock;
-	struct list_head held_list_head;
-# endif
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES

commit 8f95dc58d0505516f5cc212a966aea2f2cdb5e44
Author: David Quigley <dpquigl@tycho.nsa.gov>
Date:   Fri Jun 30 01:55:47 2006 -0700

    [PATCH] SELinux: add security hook call to kill_proc_info_as_uid
    
    This patch adds a call to the extended security_task_kill hook introduced by
    the prior patch to the kill_proc_info_as_uid function so that these signals
    can be properly mediated by security modules.  It also updates the existing
    hook call in check_kill_permission.
    
    Signed-off-by: David Quigley <dpquigl@tycho.nsa.gov>
    Signed-off-by: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 821f0481ebe1..aaf723308ed4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1153,7 +1153,7 @@ extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
-extern int kill_proc_info_as_uid(int, struct siginfo *, pid_t, uid_t, uid_t);
+extern int kill_proc_info_as_uid(int, struct siginfo *, pid_t, uid_t, uid_t, u32);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);

commit 95e02ca9bb5324360e7dea1ea1c563036d84a5e6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 27 02:55:02 2006 -0700

    [PATCH] rtmutex: Propagate priority settings into PI lock chains
    
    When the priority of a task, which is blocked on a lock, changes we must
    propagate this change into the PI lock chain.  Therefor the chain walk code
    is changed to get rid of the references to current to avoid false positives
    in the deadlock detector, as setscheduler might be called by a task which
    holds the lock on which the task whose priority is changed is blocked.
    
    Also add some comments about the get/put_task_struct usage to avoid
    confusion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4e6be7de5ad..821f0481ebe1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1044,11 +1044,13 @@ extern void sched_idle_next(void);
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(task_t *p);
 extern void rt_mutex_setprio(task_t *p, int prio);
+extern void rt_mutex_adjust_pi(task_t *p);
 #else
 static inline int rt_mutex_getprio(task_t *p)
 {
 	return p->normal_prio;
 }
+# define rt_mutex_adjust_pi(p)		do { } while (0)
 #endif
 
 extern void set_user_nice(task_t *p, long nice);

commit c87e2837be82df479a6bae9f155c43516d2feebc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:58 2006 -0700

    [PATCH] pi-futex: futex_lock_pi/futex_unlock_pi support
    
    This adds the actual pi-futex implementation, based on rt-mutexes.
    
    [dino@in.ibm.com: fix an oops-causing race]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Dinakar Guniguntala <dino@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edadd13cf53f..b4e6be7de5ad 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -84,6 +84,7 @@ struct sched_param {
 #include <asm/processor.h>
 
 struct exec_domain;
+struct futex_pi_state;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -915,6 +916,8 @@ struct task_struct {
 #ifdef CONFIG_COMPAT
 	struct compat_robust_list_head __user *compat_robust_list;
 #endif
+	struct list_head pi_state_list;
+	struct futex_pi_state *pi_state_cache;
 
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;

commit 61a87122869b6340a63b6f9f84097d3688604b90
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 27 02:54:56 2006 -0700

    [PATCH] pi-futex: rt mutex tester
    
    RT-mutex tester: scriptable tester for rt mutexes, which allows userspace
    scripting of mutex unit-tests (and dynamic tests as well), using the actual
    rt-mutex implementation of the kernel.
    
    [akpm@osdl.org: fixlet]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6ea23c9af413..edadd13cf53f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -982,6 +982,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
+#define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 23f78d4a03c53cbd75d87a795378ea540aa08c86
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:53 2006 -0700

    [PATCH] pi-futex: rt mutex core
    
    Core functions for the rt-mutex subsystem.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6f167645e7e2..6ea23c9af413 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -73,6 +73,7 @@ struct sched_param {
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
 #include <linux/futex.h>
+#include <linux/rtmutex.h>
 
 #include <linux/time.h>
 #include <linux/param.h>
@@ -858,6 +859,17 @@ struct task_struct {
 	/* Protection of the PI data structures: */
 	spinlock_t pi_lock;
 
+#ifdef CONFIG_RT_MUTEXES
+	/* PI waiters blocked on a rt_mutex held by this task */
+	struct plist_head pi_waiters;
+	/* Deadlock detection and priority inheritance handling */
+	struct rt_mutex_waiter *pi_blocked_on;
+# ifdef CONFIG_DEBUG_RT_MUTEXES
+	spinlock_t held_list_lock;
+	struct list_head held_list_head;
+# endif
+#endif
+
 #ifdef CONFIG_DEBUG_MUTEXES
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;

commit b29739f902ee76a05493fb7d2303490fc75364f4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:51 2006 -0700

    [PATCH] pi-futex: scheduler support for pi
    
    Add framework to boost/unboost the priority of RT tasks.
    
    This consists of:
    
     - caching the 'normal' priority in ->normal_prio
     - providing a functions to set/get the priority of the task
     - make sched_setscheduler() aware of boosting
    
    The effective_prio() cleanups also fix a priority-calculation bug pointed out
    by Andrey Gelman, in set_user_nice().
    
    has_rt_policy() fix: Peter Williams <pwil3058@bigpond.net.au>
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Andrey Gelman <agelman@012.net.il>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0bc81a151e50..6f167645e7e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -495,8 +495,11 @@ struct signal_struct {
 
 #define MAX_PRIO		(MAX_RT_PRIO + 40)
 
-#define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
+#define rt_prio(prio)		unlikely((prio) < MAX_RT_PRIO)
+#define rt_task(p)		rt_prio((p)->prio)
 #define batch_task(p)		(unlikely((p)->policy == SCHED_BATCH))
+#define has_rt_policy(p) \
+	unlikely((p)->policy != SCHED_NORMAL && (p)->policy != SCHED_BATCH)
 
 /*
  * Some day this will be a full-fledged user tracking system..
@@ -725,7 +728,7 @@ struct task_struct {
 #endif
 #endif
 	int load_weight;	/* for niceness load balancing purposes */
-	int prio, static_prio;
+	int prio, static_prio, normal_prio;
 	struct list_head run_list;
 	prio_array_t *array;
 
@@ -852,6 +855,9 @@ struct task_struct {
 /* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
 	spinlock_t alloc_lock;
 
+	/* Protection of the PI data structures: */
+	spinlock_t pi_lock;
+
 #ifdef CONFIG_DEBUG_MUTEXES
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
@@ -1018,6 +1024,17 @@ static inline void idle_task_exit(void) {}
 #endif
 
 extern void sched_idle_next(void);
+
+#ifdef CONFIG_RT_MUTEXES
+extern int rt_mutex_getprio(task_t *p);
+extern void rt_mutex_setprio(task_t *p, int prio);
+#else
+static inline int rt_mutex_getprio(task_t *p)
+{
+	return p->normal_prio;
+}
+#endif
+
 extern void set_user_nice(task_t *p, long nice);
 extern int task_prio(const task_t *p);
 extern int task_nice(const task_t *p);

commit 5c45bf279d378d436ce45825c0f136696c7b6109
Author: Siddha, Suresh B <suresh.b.siddha@intel.com>
Date:   Tue Jun 27 02:54:42 2006 -0700

    [PATCH] sched: mc/smt power savings sched policy
    
    sysfs entries 'sched_mc_power_savings' and 'sched_smt_power_savings' in
    /sys/devices/system/cpu/ control the MC/SMT power savings policy for the
    scheduler.
    
    Based on the values (1-enable, 0-disable) for these controls, sched groups
    cpu power will be determined for different domains.  When power savings
    policy is enabled and under light load conditions, scheduler will minimize
    the physical packages/cpu cores carrying the load and thus conserving
    power(with a perf impact based on the workload characteristics...  see OLS
    2005 CMP kernel scheduler paper for more details..)
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Con Kolivas <kernel@kolivas.org>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ab8ffc54423a..0bc81a151e50 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -570,6 +570,11 @@ enum idle_type
 #define SD_WAKE_AFFINE		32	/* Wake task to waking CPU */
 #define SD_WAKE_BALANCE		64	/* Perform balancing at task wakeup */
 #define SD_SHARE_CPUPOWER	128	/* Domain members share cpu power */
+#define SD_POWERSAVINGS_BALANCE	256	/* Balance for power savings */
+
+#define BALANCE_FOR_POWER	((sched_mc_power_savings || sched_smt_power_savings) \
+				 ? SD_POWERSAVINGS_BALANCE : 0)
+
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
@@ -1412,6 +1417,11 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 
+#include <linux/sysdev.h>
+extern int sched_mc_power_savings, sched_smt_power_savings;
+extern struct sysdev_attribute attr_sched_mc_power_savings, attr_sched_smt_power_savings;
+extern int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls);
+
 extern void normalize_rt_tasks(void);
 
 #ifdef CONFIG_PM

commit 51888ca25a03125e742ef84d4ddfd74e139707a0
Author: Srivatsa Vaddagiri <vatsa@in.ibm.com>
Date:   Tue Jun 27 02:54:38 2006 -0700

    [PATCH] sched_domain: handle kmalloc failure
    
    Try to handle mem allocation failures in build_sched_domains by bailing out
    and cleaning up thus-far allocated memory.  The patch has a direct consequence
    that we disable load balancing completely (even at sibling level) upon *any*
    memory allocation failure.
    
    [Lee.Schermerhorn@hp.com: bugfix]
    Signed-off-by: Srivatsa Vaddagir <vatsa@in.ibm.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 74a1e39e0d3d..ab8ffc54423a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -639,7 +639,7 @@ struct sched_domain {
 #endif
 };
 
-extern void partition_sched_domains(cpumask_t *partition1,
+extern int partition_sched_domains(cpumask_t *partition1,
 				    cpumask_t *partition2);
 
 /*

commit 2dd73a4f09beacadde827a032cf15fd8b1fa3d48
Author: Peter Williams <pwil3058@bigpond.net.au>
Date:   Tue Jun 27 02:54:34 2006 -0700

    [PATCH] sched: implement smpnice
    
    Problem:
    
    The introduction of separate run queues per CPU has brought with it "nice"
    enforcement problems that are best described by a simple example.
    
    For the sake of argument suppose that on a single CPU machine with a
    nice==19 hard spinner and a nice==0 hard spinner running that the nice==0
    task gets 95% of the CPU and the nice==19 task gets 5% of the CPU.  Now
    suppose that there is a system with 2 CPUs and 2 nice==19 hard spinners and
    2 nice==0 hard spinners running.  The user of this system would be entitled
    to expect that the nice==0 tasks each get 95% of a CPU and the nice==19
    tasks only get 5% each.  However, whether this expectation is met is pretty
    much down to luck as there are four equally likely distributions of the
    tasks to the CPUs that the load balancing code will consider to be balanced
    with loads of 2.0 for each CPU.  Two of these distributions involve one
    nice==0 and one nice==19 task per CPU and in these circumstances the users
    expectations will be met.  The other two distributions both involve both
    nice==0 tasks being on one CPU and both nice==19 being on the other CPU and
    each task will get 50% of a CPU and the user's expectations will not be
    met.
    
    Solution:
    
    The solution to this problem that is implemented in the attached patch is
    to use weighted loads when determining if the system is balanced and, when
    an imbalance is detected, to move an amount of weighted load between run
    queues (as opposed to a number of tasks) to restore the balance.  Once
    again, the easiest way to explain why both of these measures are necessary
    is to use a simple example.  Suppose that (in a slight variation of the
    above example) that we have a two CPU system with 4 nice==0 and 4 nice=19
    hard spinning tasks running and that the 4 nice==0 tasks are on one CPU and
    the 4 nice==19 tasks are on the other CPU.  The weighted loads for the two
    CPUs would be 4.0 and 0.2 respectively and the load balancing code would
    move 2 tasks resulting in one CPU with a load of 2.0 and the other with
    load of 2.2.  If this was considered to be a big enough imbalance to
    justify moving a task and that task was moved using the current
    move_tasks() then it would move the highest priority task that it found and
    this would result in one CPU with a load of 3.0 and the other with a load
    of 1.2 which would result in the movement of a task in the opposite
    direction and so on -- infinite loop.  If, on the other hand, an amount of
    load to be moved is calculated from the imbalance (in this case 0.1) and
    move_tasks() skips tasks until it find ones whose contributions to the
    weighted load are less than this amount it would move two of the nice==19
    tasks resulting in a system with 2 nice==0 and 2 nice=19 on each CPU with
    loads of 2.1 for each CPU.
    
    One of the advantages of this mechanism is that on a system where all tasks
    have nice==0 the load balancing calculations would be mathematically
    identical to the current load balancing code.
    
    Notes:
    
    struct task_struct:
    
    has a new field load_weight which (in a trade off of space for speed)
    stores the contribution that this task makes to a CPU's weighted load when
    it is runnable.
    
    struct runqueue:
    
    has a new field raw_weighted_load which is the sum of the load_weight
    values for the currently runnable tasks on this run queue.  This field
    always needs to be updated when nr_running is updated so two new inline
    functions inc_nr_running() and dec_nr_running() have been created to make
    sure that this happens.  This also offers a convenient way to optimize away
    this part of the smpnice mechanism when CONFIG_SMP is not defined.
    
    int try_to_wake_up():
    
    in this function the value SCHED_LOAD_BALANCE is used to represent the load
    contribution of a single task in various calculations in the code that
    decides which CPU to put the waking task on.  While this would be a valid
    on a system where the nice values for the runnable tasks were distributed
    evenly around zero it will lead to anomalous load balancing if the
    distribution is skewed in either direction.  To overcome this problem
    SCHED_LOAD_SCALE has been replaced by the load_weight for the relevant task
    or by the average load_weight per task for the queue in question (as
    appropriate).
    
    int move_tasks():
    
    The modifications to this function were complicated by the fact that
    active_load_balance() uses it to move exactly one task without checking
    whether an imbalance actually exists.  This precluded the simple
    overloading of max_nr_move with max_load_move and necessitated the addition
    of the latter as an extra argument to the function.  The internal
    implementation is then modified to move up to max_nr_move tasks and
    max_load_move of weighted load.  This slightly complicates the code where
    move_tasks() is called and if ever active_load_balance() is changed to not
    use move_tasks() the implementation of move_tasks() should be simplified
    accordingly.
    
    struct sched_group *find_busiest_group():
    
    Similar to try_to_wake_up(), there are places in this function where
    SCHED_LOAD_SCALE is used to represent the load contribution of a single
    task and the same issues are created.  A similar solution is adopted except
    that it is now the average per task contribution to a group's load (as
    opposed to a run queue) that is required.  As this value is not directly
    available from the group it is calculated on the fly as the queues in the
    groups are visited when determining the busiest group.
    
    A key change to this function is that it is no longer to scale down
    *imbalance on exit as move_tasks() uses the load in its scaled form.
    
    void set_user_nice():
    
    has been modified to update the task's load_weight field when it's nice
    value and also to ensure that its run queue's raw_weighted_load field is
    updated if it was runnable.
    
    From: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    
    With smpnice, sched groups with highest priority tasks can mask the imbalance
    between the other sched groups with in the same domain.  This patch fixes some
    of the listed down scenarios by not considering the sched groups which are
    lightly loaded.
    
    a) on a simple 4-way MP system, if we have one high priority and 4 normal
       priority tasks, with smpnice we would like to see the high priority task
       scheduled on one cpu, two other cpus getting one normal task each and the
       fourth cpu getting the remaining two normal tasks.  but with current
       smpnice extra normal priority task keeps jumping from one cpu to another
       cpu having the normal priority task.  This is because of the
       busiest_has_loaded_cpus, nr_loaded_cpus logic..  We are not including the
       cpu with high priority task in max_load calculations but including that in
       total and avg_load calcuations..  leading to max_load < avg_load and load
       balance between cpus running normal priority tasks(2 Vs 1) will always show
       imbalanace as one normal priority and the extra normal priority task will
       keep moving from one cpu to another cpu having normal priority task..
    
    b) 4-way system with HT (8 logical processors).  Package-P0 T0 has a
       highest priority task, T1 is idle.  Package-P1 Both T0 and T1 have 1 normal
       priority task each..  P2 and P3 are idle.  With this patch, one of the
       normal priority tasks on P1 will be moved to P2 or P3..
    
    c) With the current weighted smp nice calculations, it doesn't always make
       sense to look at the highest weighted runqueue in the busy group..
       Consider a load balance scenario on a DP with HT system, with Package-0
       containing one high priority and one low priority, Package-1 containing one
       low priority(with other thread being idle)..  Package-1 thinks that it need
       to take the low priority thread from Package-0.  And find_busiest_queue()
       returns the cpu thread with highest priority task..  And ultimately(with
       help of active load balance) we move high priority task to Package-1.  And
       same continues with Package-0 now, moving high priority task from package-1
       to package-0..  Even without the presence of active load balance, load
       balance will fail to balance the above scenario..  Fix find_busiest_queue
       to use "imbalance" when it is lightly loaded.
    
    [kernel@kolivas.org: sched: store weighted load on up]
    [kernel@kolivas.org: sched: add discrete weighted cpu load function]
    [suresh.b.siddha@intel.com: sched: remove dead code]
    Signed-off-by: Peter Williams <pwil3058@bigpond.com.au>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Cc: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Cc: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 122a25c1b997..74a1e39e0d3d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -123,6 +123,7 @@ extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
+extern unsigned long weighted_cpuload(const int cpu);
 
 
 /*
@@ -558,9 +559,9 @@ enum idle_type
 /*
  * sched-domains (multiprocessor balancing) declarations:
  */
-#ifdef CONFIG_SMP
 #define SCHED_LOAD_SCALE	128UL	/* increase resolution of load */
 
+#ifdef CONFIG_SMP
 #define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */
 #define SD_BALANCE_NEWIDLE	2	/* Balance when about to become idle */
 #define SD_BALANCE_EXEC		4	/* Balance on exec */
@@ -713,9 +714,12 @@ struct task_struct {
 
 	int lock_depth;		/* BKL lock depth */
 
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+#ifdef CONFIG_SMP
+#ifdef __ARCH_WANT_UNLOCKED_CTXSW
 	int oncpu;
 #endif
+#endif
+	int load_weight;	/* for niceness load balancing purposes */
 	int prio, static_prio;
 	struct list_head run_list;
 	prio_array_t *array;

commit 48e6484d49020dba3578ad117b461e8a391e8f0f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jun 26 00:25:48 2006 -0700

    [PATCH] proc: Rewrite the proc dentry flush on exit optimization
    
    To keep the dcache from filling up with dead /proc entries we flush them on
    process exit.  However over the years that code has gotten hairy with a
    dentry_pointer and a lock in task_struct and misdocumented as a correctness
    feature.
    
    I have rewritten this code to look and see if we have a corresponding entry in
    the dcache and if so flush it on process exit.  This removes the extra fields
    in the task_struct and allows me to trivially handle the case of a
    /proc/<tgid>/task/<pid> entry as well as the current /proc/<pid> entries.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8d11d9310db0..122a25c1b997 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -842,8 +842,6 @@ struct task_struct {
    	u32 self_exec_id;
 /* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
 	spinlock_t alloc_lock;
-/* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
-	spinlock_t proc_lock;
 
 #ifdef CONFIG_DEBUG_MUTEXES
 	/* mutex deadlock detection */
@@ -856,7 +854,6 @@ struct task_struct {
 /* VM state */
 	struct reclaim_state *reclaim_state;
 
-	struct dentry *proc_dentry;
 	struct backing_dev_info *backing_dev_info;
 
 	struct io_context *io_context;

commit 77787bfb44da6e6166af088226707aeccee27968
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:26 2006 -0700

    [PATCH] pacct: none-delayed process accounting accumulation
    
    In current 2.6.17 implementation, signal_struct refered from task_struct is
    used for per-process data structure.  The pacct facility also uses it as a
    per-process data structure to store stime, utime, minflt, majflt.  But those
    members are saved in __exit_signal().  It's too late.
    
    For example, if some threads exits at same time, pacct facility has a
    possibility to drop accountings for a part of those threads.  (see, the
    following 'The results of original 2.6.17 kernel') I think accounting
    information should be completely collected into the per-process data structure
    before writing out an accounting record.
    
    This patch fixes this matter.  Accumulation of stime, utime, minflt and majflt
    are done before generating accounting record.
    
    [mingo@elte.hu: fix acct_collect() siglock bug found by lockdep]
    Signed-off-by: KaiGai Kohei <kaigai@ak.jp.nec.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d8429dc250f0..8d11d9310db0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -362,6 +362,8 @@ struct pacct_struct {
 	int			ac_flag;
 	long			ac_exitcode;
 	unsigned long		ac_mem;
+	cputime_t		ac_utime, ac_stime;
+	unsigned long		ac_minflt, ac_majflt;
 };
 
 /*

commit f6ec29a42d7ac3b309a9cef179b686d23986ab98
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:25 2006 -0700

    [PATCH] pacct: avoidance to refer the last thread as a representation of the process
    
    When pacct facility generate an 'ac_flag' field in accounting record, it
    refers a task_struct of the thread which died last in the process.  But any
    other task_structs are ignored.
    
    Therefore, pacct facility drops ASU flag even if root-privilege operations are
    used by any other threads except the last one.  In addition, AFORK flag is
    always set when the thread of group-leader didn't die last, although this
    process has called execve() after fork().
    
    We have a same matter in ac_exitcode.  The recorded ac_exitcode is an exit
    code of the last thread in the process.  There is a possibility this exitcode
    is not the group leader's one.

diff --git a/include/linux/sched.h b/include/linux/sched.h
index abada7c1d5e4..d8429dc250f0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -359,6 +359,8 @@ struct sighand_struct {
 };
 
 struct pacct_struct {
+	int			ac_flag;
+	long			ac_exitcode;
 	unsigned long		ac_mem;
 };
 

commit 0e4648141af02331f21aabcd34940c70f09a2d04
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:24 2006 -0700

    [PATCH] pacct: add pacct_struct to fix some pacct bugs.
    
    The pacct facility need an i/o operation when an accounting record is
    generated.  There is a possibility to wake OOM killer up.  If OOM killer is
    activated, it kills some processes to make them release process memory
    regions.
    
    But acct_process() is called in the killed processes context before calling
    exit_mm(), so those processes cannot release own memory.  In the results, any
    processes stop in this point and it finally cause a system stall.

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 38b4791e6a5d..abada7c1d5e4 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -358,6 +358,10 @@ struct sighand_struct {
 	spinlock_t		siglock;
 };
 
+struct pacct_struct {
+	unsigned long		ac_mem;
+};
+
 /*
  * NOTE! "signal_struct" does not have it's own
  * locking, because a shared signal_struct always
@@ -449,6 +453,9 @@ struct signal_struct {
 	struct key *session_keyring;	/* keyring inherited over fork */
 	struct key *process_keyring;	/* keyring private to this process */
 #endif
+#ifdef CONFIG_BSD_PROCESS_ACCT
+	struct pacct_struct pacct;	/* per-process accounting information */
+#endif
 };
 
 /* Context switch must be unlocked if interrupts are to be enabled */

commit b31dc66a54ad986b6b73bdc49c8efc17cbad1833
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Jun 13 08:26:10 2006 +0200

    [PATCH] Kill PF_SYNCWRITE flag
    
    A process flag to indicate whether we are doing sync io is incredibly
    ugly. It also causes performance problems when one does a lot of async
    io and then proceeds to sync it. Part of the io will go out as async,
    and the other part as sync. This causes a disconnect between the
    previously submitted io and the synced io. For io schedulers such as CFQ,
    this will cause us lost merges and suboptimal behaviour in scheduling.
    
    Remove PF_SYNCWRITE completely from the fsync/msync paths, and let
    the O_DIRECT path just directly indicate that the writes are sync
    by using WRITE_SYNC instead.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a9d23c7d1b25..38b4791e6a5d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -941,12 +941,11 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_KSWAPD	0x00040000	/* I am kswapd */
 #define PF_SWAPOFF	0x00080000	/* I am in swapoff */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
-#define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
-#define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
-#define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
-#define PF_SWAPWRITE	0x01000000	/* Allowed to write to swap */
-#define PF_SPREAD_PAGE	0x04000000	/* Spread page cache over cpuset */
-#define PF_SPREAD_SLAB	0x08000000	/* Spread some slab caches over cpuset */
+#define PF_BORROWED_MM	0x00200000	/* I am a kthread doing use_mm */
+#define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */
+#define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */
+#define PF_SPREAD_PAGE	0x01000000	/* Spread page cache over cpuset */
+#define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 
 /*

commit 260ea1013283d8acbb451459ed1ca560c1445c20
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Jun 23 02:05:18 2006 -0700

    [PATCH] ptrace: document the locking rules
    
    After a lot of reading the code and thinking about how it behaves I have
    managed to figure out what the current ptrace locking rules are.  The
    current code is in much better that it appears at first glance.  The
    troublesome code paths are actually the code paths that violate the current
    rules.
    
    ptrace uses simple exclusive access as it's locking.  You can only touch
    task->ptrace if the task is stopped and you are the ptracer, or if the task
    is running and are the task itself.
    
    Very simple, very easy to maintain.  It just needs to be documented so
    people know not to touch ptrace from elsewhere.
    
    Currently we do have a few pieces of code that are in violation of this
    rule.  Particularly the core dump code, and ptrace_attach.  But so far the
    code looks fixable.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 267f15257040..a9d23c7d1b25 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1225,7 +1225,7 @@ static inline int thread_group_empty(task_t *p)
 		(thread_group_leader(p) && !thread_group_empty(p))
 
 /*
- * Protects ->fs, ->files, ->mm, ->ptrace, ->group_info, ->comm, keyring
+ * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
  * pins the final release of task.io_context.  Also protects ->cpuset.
  *

commit d9eaec9e295a84a80b663996d0489fcff3a1dca9
Merge: cee4cca740d2 41757106b9ca
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:37:56 2006 -0700

    Merge branch 'audit.b21' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit-current
    
    * 'audit.b21' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit-current: (25 commits)
      [PATCH] make set_loginuid obey audit_enabled
      [PATCH] log more info for directory entry change events
      [PATCH] fix AUDIT_FILTER_PREPEND handling
      [PATCH] validate rule fields' types
      [PATCH] audit: path-based rules
      [PATCH] Audit of POSIX Message Queue Syscalls v.2
      [PATCH] fix se_sen audit filter
      [PATCH] deprecate AUDIT_POSSBILE
      [PATCH] inline more audit helpers
      [PATCH] proc_loginuid_write() uses simple_strtoul() on non-terminated array
      [PATCH] update of IPC audit record cleanup
      [PATCH] minor audit updates
      [PATCH] fix audit_krule_to_{rule,data} return values
      [PATCH] add filtering by ppid
      [PATCH] log ppid
      [PATCH] collect sid of those who send signals to auditd
      [PATCH] execve argument logging
      [PATCH] fix deadlocks in AUDIT_LIST/AUDIT_LIST_RULES
      [PATCH] audit_panic() is audit-internal
      [PATCH] inotify (5/5): update kernel documentation
      ...
    
    Manual fixup of conflict in unclude/linux/inotify.h

commit 2d9048e201bfb67ba21f05e647b1286b8a4a5667
Author: Amy Griffis <amy.griffis@hp.com>
Date:   Thu Jun 1 13:10:59 2006 -0700

    [PATCH] inotify (1/5): split kernel API from userspace support
    
    The following series of patches introduces a kernel API for inotify,
    making it possible for kernel modules to benefit from inotify's
    mechanism for watching inodes.  With these patches, inotify will
    maintain for each caller a list of watches (via an embedded struct
    inotify_watch), where each inotify_watch is associated with a
    corresponding struct inode.  The caller registers an event handler and
    specifies for which filesystem events their event handler should be
    called per inotify_watch.
    
    Signed-off-by: Amy Griffis <amy.griffis@hp.com>
    Acked-by: Robert Love <rml@novell.com>
    Acked-by: John McCutchan <john@johnmccutchan.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 29b7d4f87d20..864e5a70ff65 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -494,7 +494,7 @@ struct user_struct {
 	atomic_t processes;	/* How many processes does this user have? */
 	atomic_t files;		/* How many open files does this user have? */
 	atomic_t sigpending;	/* How many pending signals does this user have? */
-#ifdef CONFIG_INOTIFY
+#ifdef CONFIG_INOTIFY_USER
 	atomic_t inotify_watches; /* How many inotify watches does this user have? */
 	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
 #endif

commit b7b3c76a0a21c5a98124e90c47c488f7e4166f87
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Thu Apr 27 00:12:56 2006 +0100

    Sanitise linux/sched.h for userspace consumption
    
    There was a whole load of crap exposed which should have been inside the
    existing #ifdef __KERNEL__ part. Also hide struct sched_param for now,
    since glibc has its own and doesn't like being given ours (yet).
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2e05e402df4f..701b8cbceb05 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,7 +1,44 @@
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
+#include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
+
+/*
+ * cloning flags:
+ */
+#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
+#define CLONE_VM	0x00000100	/* set if VM shared between processes */
+#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
+#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
+#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
+#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
+#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
+#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
+#define CLONE_THREAD	0x00010000	/* Same thread group? */
+#define CLONE_NEWNS	0x00020000	/* New namespace group? */
+#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
+#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
+#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
+#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
+#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
+#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
+#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
+#define CLONE_STOPPED		0x02000000	/* Start in stopped state */
+
+/*
+ * Scheduling policies
+ */
+#define SCHED_NORMAL		0
+#define SCHED_FIFO		1
+#define SCHED_RR		2
+#define SCHED_BATCH		3
+
 #ifdef __KERNEL__
+
+struct sched_param {
+	int sched_priority;
+};
+
 #include <asm/param.h>	/* for HZ */
 
 #include <linux/capability.h>
@@ -44,34 +81,9 @@
 #include <linux/hrtimer.h>
 
 #include <asm/processor.h>
-#endif
-
-#include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
 
 struct exec_domain;
 
-/*
- * cloning flags:
- */
-#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
-#define CLONE_VM	0x00000100	/* set if VM shared between processes */
-#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
-#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
-#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
-#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
-#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
-#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
-#define CLONE_THREAD	0x00010000	/* Same thread group? */
-#define CLONE_NEWNS	0x00020000	/* New namespace group? */
-#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
-#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
-#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
-#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
-#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
-#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
-#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
-#define CLONE_STOPPED		0x02000000	/* Start in stopped state */
-
 /*
  * List of flags we want to share for kernel threads,
  * if only because they are not used by them anyway.
@@ -158,20 +170,6 @@ extern unsigned long nr_iowait(void);
 /* Task command name length */
 #define TASK_COMM_LEN 16
 
-/*
- * Scheduling policies
- */
-#define SCHED_NORMAL		0
-#define SCHED_FIFO		1
-#define SCHED_RR		2
-#define SCHED_BATCH		3
-
-struct sched_param {
-	int sched_priority;
-};
-
-#ifdef __KERNEL__
-
 #include <linux/spinlock.h>
 
 /*

commit a3b6714e1744a5e841753d74aca1de5972f24e6d
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Tue Apr 25 14:54:40 2006 +0100

    Partially sanitise linux/sched.h for userspace consumption
    
    For now, just make sure all inclusion of private header files is done
    within #ifdef __KERNEL__. There'll be more to clean up later.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 29b7d4f87d20..2e05e402df4f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1,9 +1,9 @@
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
+#ifdef __KERNEL__
 #include <asm/param.h>	/* for HZ */
 
-#include <linux/config.h>
 #include <linux/capability.h>
 #include <linux/threads.h>
 #include <linux/kernel.h>
@@ -37,6 +37,15 @@
 #include <linux/rcupdate.h>
 #include <linux/futex.h>
 
+#include <linux/time.h>
+#include <linux/param.h>
+#include <linux/resource.h>
+#include <linux/timer.h>
+#include <linux/hrtimer.h>
+
+#include <asm/processor.h>
+#endif
+
 #include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
 
 struct exec_domain;
@@ -103,13 +112,6 @@ extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
 
-#include <linux/time.h>
-#include <linux/param.h>
-#include <linux/resource.h>
-#include <linux/timer.h>
-#include <linux/hrtimer.h>
-
-#include <asm/processor.h>
 
 /*
  * Task state bitmask. NOTE! These bits are also

commit 5e85d4abe3f43bb5362f384bab0e20ef082ce0b5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 18 22:20:16 2006 -0700

    [PATCH] task: Make task list manipulations RCU safe
    
    While we can currently walk through thread groups, process groups, and
    sessions with just the rcu_read_lock, this opens the door to walking the
    entire task list.
    
    We already have all of the other RCU guarantees so there is no cost in
    doing this, this should be enough so that proc can stop taking the
    tasklist lock during readdir.
    
    prev_task was killed because it has no users, and using it will miss new
    tasks when doing an rcu traversal.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b7d31e2e1729..29b7d4f87d20 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1192,8 +1192,7 @@ extern void wait_task_inactive(task_t * p);
 #define remove_parent(p)	list_del_init(&(p)->sibling)
 #define add_parent(p)		list_add_tail(&(p)->sibling,&(p)->parent->children)
 
-#define next_task(p)	list_entry((p)->tasks.next, struct task_struct, tasks)
-#define prev_task(p)	list_entry((p)->tasks.prev, struct task_struct, tasks)
+#define next_task(p)	list_entry(rcu_dereference((p)->tasks.next), struct task_struct, tasks)
 
 #define for_each_process(p) \
 	for (p = &init_task ; (p = next_task(p)) != &init_task ; )

commit 64541d19702cfdb7ea946fdc20faee849f6874b1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Apr 14 12:43:15 2006 -0600

    [PATCH] kill unushed __put_task_struct_cb
    
    Somehow in the midst of dotting i's and crossing t's during
    the merge up to rc1 we wound up keeping __put_task_struct_cb
    when it should have been killed as it no longer has any users.
    Sorry I probably should have caught this while it was
    still in the -mm tree.
    
    Having the old code there gets confusing when reading
    through the code and trying to understand what is
    happening.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e3539c14e47e..b7d31e2e1729 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -911,7 +911,6 @@ static inline int pid_alive(struct task_struct *p)
 extern void free_task(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
-extern void __put_task_struct_cb(struct rcu_head *rhp);
 extern void __put_task_struct(struct task_struct *t);
 
 static inline void put_task_struct(struct task_struct *t)

commit 88dd9c16cecbd105bbe7711b6120333f6f7b5474
Merge: 6dde43255355 d1195c516a9a
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Apr 11 06:34:02 2006 -0700

    Merge branch 'splice' of git://brick.kernel.dk/data/git/linux-2.6-block
    
    * 'splice' of git://brick.kernel.dk/data/git/linux-2.6-block:
      [PATCH] vfs: add splice_write and splice_read to documentation
      [PATCH] Remove sys_ prefix of new syscalls from __NR_sys_*
      [PATCH] splice: warning fix
      [PATCH] another round of fs/pipe.c cleanups
      [PATCH] splice: comment styles
      [PATCH] splice: add Ingo as addition copyright holder
      [PATCH] splice: unlikely() optimizations
      [PATCH] splice: speedups and optimizations
      [PATCH] pipe.c/fifo.c code cleanups
      [PATCH] get rid of the PIPE_*() macros
      [PATCH] splice: speedup __generic_file_splice_read
      [PATCH] splice: add direct fd <-> fd splicing support
      [PATCH] splice: add optional input and output offsets
      [PATCH] introduce a "kernel-internal pipe object" abstraction
      [PATCH] splice: be smarter about calling do_page_cache_readahead()
      [PATCH] splice: optimize the splice buffer mapping
      [PATCH] splice: cleanup __generic_file_splice_read()
      [PATCH] splice: only call wake_up_interruptible() when we really have to
      [PATCH] splice: potential !page dereference
      [PATCH] splice: mark the io page as accessed

commit a9cdf410ca8f59b52bc7061a6751050010c7cc5b
Author: Keith Owens <kaos@sgi.com>
Date:   Mon Apr 10 22:54:07 2006 -0700

    [PATCH] Reinstate const in next_thread()
    
    Before commit 47e65328a7b1cdfc4e3102e50d60faf94ebba7d3, next_thread() took
    a const task_t.  Reinstate the const qualifier, getting the next thread
    never changes the current thread.
    
    Signed-off-by: Keith Owens <kaos@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a3e4f6b503a3..83d657811d01 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1206,7 +1206,7 @@ extern void wait_task_inactive(task_t * p);
 /* de_thread depends on thread_group_leader not being a pid based check */
 #define thread_group_leader(p)	(p == p->group_leader)
 
-static inline task_t *next_thread(task_t *p)
+static inline task_t *next_thread(const task_t *p)
 {
 	return list_entry(rcu_dereference(p->thread_group.next),
 				task_t, thread_group);

commit b92ce55893745e011edae70830b8bc863be881f9
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Apr 11 13:52:07 2006 +0200

    [PATCH] splice: add direct fd <-> fd splicing support
    
    It's more efficient for sendfile() emulation. Basically we cache an
    internal private pipe and just use that as the intermediate area for
    pages. Direct splicing is not available from sys_splice(), it is only
    meant to be used for sendfile() emulation.
    
    Additional patch from Ingo Molnar to avoid the PIPE_BUFFERS loop at
    exit for the normal fast path.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 541f4828f5e7..e194ec75833d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -684,6 +684,7 @@ static inline void prefetch_stack(struct task_struct *t) { }
 
 struct audit_context;		/* See audit.c */
 struct mempolicy;
+struct pipe_inode_info;
 
 enum sleep_type {
 	SLEEP_NORMAL,
@@ -882,6 +883,11 @@ struct task_struct {
 
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;
+
+	/*
+	 * cache last used pipe for splice
+	 */
+	struct pipe_inode_info *splice_pipe;
 };
 
 static inline pid_t process_group(struct task_struct *tsk)

commit de12a7878c11f3b282d640888aa635e0711d0b5e
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Apr 10 17:16:49 2006 -0600

    [PATCH] de_thread: Don't confuse users do_each_thread.
    
    Oleg Nesterov spotted two interesting bugs with the current de_thread
    code.  The simplest is a long standing double decrement of
    __get_cpu_var(process_counts) in __unhash_process.  Caused by
    two processes exiting when only one was created.
    
    The other is that since we no longer detach from the thread_group list
    it is possible for do_each_thread when run under the tasklist_lock to
    see the same task_struct twice.  Once on the task list as a
    thread_group_leader, and once on the thread list of another
    thread.
    
    The double appearance in do_each_thread can cause a double increment
    of mm_core_waiters in zap_threads resulting in problems later on in
    coredump_wait.
    
    To remedy those two problems this patch takes the simple approach
    of changing the old thread group leader into a child thread.
    The only routine in release_task that cares is __unhash_process,
    and it can be trivially seen that we handle cleaning up a
    thread group leader properly.
    
    Since de_thread doesn't change the pid of the exiting leader process
    and instead shares it with the new leader process.  I change
    thread_group_leader to recognize group leadership based on the
    group_leader field and not based on pids.  This should also be
    slightly cheaper then the existing thread_group_leader macro.
    
    I performed a quick audit and I couldn't see any user of
    thread_group_leader that cared about the difference.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 541f4828f5e7..a3e4f6b503a3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1203,7 +1203,8 @@ extern void wait_task_inactive(task_t * p);
 #define while_each_thread(g, t) \
 	while ((t = next_thread(t)) != g)
 
-#define thread_group_leader(p)	(p->pid == p->tgid)
+/* de_thread depends on thread_group_leader not being a pid based check */
+#define thread_group_leader(p)	(p == p->group_leader)
 
 static inline task_t *next_thread(task_t *p)
 {

commit 92476d7fc0326a409ab1d3864a04093a6be9aca7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Mar 31 02:31:42 2006 -0800

    [PATCH] pidhash: Refactor the pid hash table
    
    Simplifies the code, reduces the need for 4 pid hash tables, and makes the
    code more capable.
    
    In the discussions I had with Oleg it was felt that to a large extent the
    cleanup itself justified the work.  With struct pid being dynamically
    allocated meant we could create the hash table entry when the pid was
    allocated and free the hash table entry when the pid was freed.  Instead of
    playing with the hash lists when ever a process would attach or detach to a
    process.
    
    For myself the fact that it gave what my previous task_ref patch gave for free
    with simpler code was a big win.  The problem is that if you hold a reference
    to struct task_struct you lock in 10K of low memory.  If you do that in a user
    controllable way like /proc does, with an unprivileged but hostile user space
    application with typical resource limits of 1000 fds and 100 processes I can
    trigger the OOM killer by consuming all of low memory with task structs, on a
    machine wight 1GB of low memory.
    
    If I instead hold a reference to struct pid which holds a pointer to my
    task_struct, I don't suffer from that problem because struct pid is 2 orders
    of magnitude smaller.  In fact struct pid is small enough that most other
    kernel data structures dwarf it, so simply limiting the number of referring
    data structures is enough to prevent exhaustion of low memory.
    
    This splits the current struct pid into two structures, struct pid and struct
    pid_link, and reduces our number of hash tables from PIDTYPE_MAX to just one.
    struct pid_link is the per process linkage into the hash tables and lives in
    struct task_struct.  struct pid is given an indepedent lifetime, and holds
    pointers to each of the pid types.
    
    The independent life of struct pid simplifies attach_pid, and detach_pid,
    because we are always manipulating the list of pids and not the hash table.
    In addition in giving struct pid an indpendent life it makes the concept much
    more powerful.
    
    Kernel data structures can now embed a struct pid * instead of a pid_t and
    not suffer from pid wrap around problems or from keeping unnecessarily
    large amounts of memory allocated.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7e0ff5dba986..541f4828f5e7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -760,7 +760,7 @@ struct task_struct {
 	struct task_struct *group_leader;	/* threadgroup leader */
 
 	/* PID/PID hash table linkage. */
-	struct pid pids[PIDTYPE_MAX];
+	struct pid_link pids[PIDTYPE_MAX];
 	struct list_head thread_group;
 
 	struct completion *vfork_done;		/* for vfork() */
@@ -899,7 +899,7 @@ static inline pid_t process_group(struct task_struct *tsk)
  */
 static inline int pid_alive(struct task_struct *p)
 {
-	return p->pids[PIDTYPE_PID].nr != 0;
+	return p->pids[PIDTYPE_PID].pid != NULL;
 }
 
 extern void free_task(struct task_struct *tsk);

commit 8c7904a00b06d2ee51149794b619e07369fcf9d4
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Mar 31 02:31:37 2006 -0800

    [PATCH] task: RCU protect task->usage
    
    A big problem with rcu protected data structures that are also reference
    counted is that you must jump through several hoops to increase the reference
    count.  I think someone finally implemented atomic_inc_not_zero(&count) to
    automate the common case.  Unfortunately this means you must special case the
    rcu access case.
    
    When data structures are only visible via rcu in a manner that is not
    determined by the reference count on the object (i.e.  tasks are visible until
    their zombies are reaped) there is a much simpler technique we can employ.
    Simply delaying the decrement of the reference count until the rcu interval is
    over.
    
    What that means is that the proc code that looks up a task and later
    wants to sleep can now do:
    
    rcu_read_lock();
    task = find_task_by_pid(some_pid);
    if (task) {
            get_task_struct(task);
    }
    rcu_read_unlock();
    
    The effect on the rest of the kernel is that put_task_struct becomes cheaper
    and immediate, and in the case where the task has been reaped it frees the
    task immediate instead of unnecessarily waiting an until the rcu interval is
    over.
    
    Cleanup of task_struct does not happen when its reference count drops to
    zero, instead cleanup happens when release_task is called.  Tasks can only
    be looked up via rcu before release_task is called.  All rcu protected
    members of task_struct are freed by release_task.
    
    Therefore we can move call_rcu from put_task_struct into release_task.  And
    we can modify release_task to not immediately release the reference count
    but instead have it call put_task_struct from the function it gives to
    call_rcu.
    
    The end result:
    
    - get_task_struct is safe in an rcu context where we have just looked
      up the task.
    
    - put_task_struct() simplifies into its old pre rcu self.
    
    This reorganization also makes put_task_struct uncallable from modules as
    it is not exported but it does not appear to be called from any modules so
    this should not be an issue, and is trivially fixed.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 95f248ba36c9..7e0ff5dba986 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -911,7 +911,7 @@ extern void __put_task_struct(struct task_struct *t);
 static inline void put_task_struct(struct task_struct *t)
 {
 	if (atomic_dec_and_test(&t->usage))
-		call_rcu(&t->rcu, __put_task_struct_cb);
+		__put_task_struct(t);
 }
 
 /*

commit 158d9ebd19280582da172626ad3edda1a626dace
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Mar 31 02:31:34 2006 -0800

    [PATCH] resurrect __put_task_struct
    
    This just got nuked in mainline.  Bring it back because Eric's patches use it.
    
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78c40dd2e19a..95f248ba36c9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -906,6 +906,7 @@ extern void free_task(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
 extern void __put_task_struct_cb(struct rcu_head *rhp);
+extern void __put_task_struct(struct task_struct *t);
 
 static inline void put_task_struct(struct task_struct *t)
 {

commit d425b274ba83ba4e7746a40446ec0ba3267de51f
Author: Con Kolivas <kernel@kolivas.org>
Date:   Fri Mar 31 02:31:29 2006 -0800

    [PATCH] sched: activate SCHED BATCH expired
    
    To increase the strength of SCHED_BATCH as a scheduling hint we can
    activate batch tasks on the expired array since by definition they are
    latency insensitive tasks.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c4fd3fcd3feb..78c40dd2e19a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -484,6 +484,7 @@ struct signal_struct {
 #define MAX_PRIO		(MAX_RT_PRIO + 40)
 
 #define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
+#define batch_task(p)		(unlikely((p)->policy == SCHED_BATCH))
 
 /*
  * Some day this will be a full-fledged user tracking system..

commit 3dee386e14045484a6c41c8f03a263f9d79de740
Author: Con Kolivas <kernel@kolivas.org>
Date:   Fri Mar 31 02:31:23 2006 -0800

    [PATCH] sched: cleanup task_activated()
    
    The activated flag in task_struct is used to track different sleep types and
    its usage is somewhat obfuscated.  Convert the variable to an enum with more
    descriptive names without altering the function.
    
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ab84adf5bb9a..c4fd3fcd3feb 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -684,6 +684,13 @@ static inline void prefetch_stack(struct task_struct *t) { }
 struct audit_context;		/* See audit.c */
 struct mempolicy;
 
+enum sleep_type {
+	SLEEP_NORMAL,
+	SLEEP_NONINTERACTIVE,
+	SLEEP_INTERACTIVE,
+	SLEEP_INTERRUPTED,
+};
+
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
 	struct thread_info *thread_info;
@@ -706,7 +713,7 @@ struct task_struct {
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;
 	unsigned long long sched_time; /* sched_clock time spent running */
-	int activated;
+	enum sleep_type sleep_type;
 
 	unsigned long policy;
 	cpumask_t cpus_allowed;

commit db1b1fefc2cecbff2e4214062fa8c680cb6e7b7d
Author: Jack Steiner <steiner@sgi.com>
Date:   Fri Mar 31 02:31:21 2006 -0800

    [PATCH] sched: reduce overhead of calc_load
    
    Currently, count_active_tasks() calls both nr_running() &
    nr_interruptible().  Each of these functions does a "for_each_cpu" & reads
    values from the runqueue of each cpu.  Although this is not a lot of
    instructions, each runqueue may be located on different node.  Depending on
    the architecture, a unique TLB entry may be required to access each
    runqueue.
    
    Since there may be more runqueues than cpu TLB entries, a scan of all
    runqueues can trash the TLB.  Each memory reference incurs a TLB miss &
    refill.
    
    In addition, the runqueue cacheline that contains nr_running &
    nr_uninterruptible may be evicted from the cache between the two passes.
    This causes unnecessary cache misses.
    
    Combining nr_running() & nr_interruptible() into a single function
    substantially reduces the TLB & cache misses on large systems.  This should
    have no measureable effect on smaller systems.
    
    On a 128p IA64 system running a memory stress workload, the new function
    reduced the overhead of calc_load() from 605 usec/call to 324 usec/call.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d04186d8cc68..ab84adf5bb9a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -100,6 +100,7 @@ DECLARE_PER_CPU(unsigned long, process_counts);
 extern int nr_processes(void);
 extern unsigned long nr_running(void);
 extern unsigned long nr_uninterruptible(void);
+extern unsigned long nr_active(void);
 extern unsigned long nr_iowait(void);
 
 #include <linux/time.h>

commit a7e5328a06a2beee3a2bbfaf87ce2a7bbe937de1
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:27 2006 -0800

    [PATCH] cleanup __exit_signal->cleanup_sighand path
    
    Move 'tsk->sighand = NULL' from cleanup_sighand() to __exit_signal().  This
    makes the exit path more understandable and allows us to do
    cleanup_sighand() outside of ->siglock protected section.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 99855f694ebd..d04186d8cc68 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1151,7 +1151,7 @@ extern void exit_thread(void);
 
 extern void exit_files(struct task_struct *);
 extern void __cleanup_signal(struct signal_struct *);
-extern void cleanup_sighand(struct task_struct *);
+extern void __cleanup_sighand(struct sighand_struct *);
 extern void exit_itimers(struct signal_struct *);
 
 extern NORET_TYPE void do_group_exit(int);

commit 47e65328a7b1cdfc4e3102e50d60faf94ebba7d3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:25 2006 -0800

    [PATCH] pids: kill PIDTYPE_TGID
    
    This patch kills PIDTYPE_TGID pid_type thus saving one hash table in
    kernel/pid.c and speeding up subthreads create/destroy a bit.  It is also a
    preparation for the further tref/pids rework.
    
    This patch adds 'struct list_head thread_group' to 'struct task_struct'
    instead.
    
    We don't detach group leader from PIDTYPE_PID namespace until another
    thread inherits it's ->pid == ->tgid, so we are safe wrt premature
    free_pidmap(->tgid) call.
    
    Currently there are no users of find_task_by_pid_type(PIDTYPE_TGID).
    Should the need arise, we can use find_task_by_pid()->group_leader.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-By: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a913fca9e70d..99855f694ebd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -752,6 +752,7 @@ struct task_struct {
 
 	/* PID/PID hash table linkage. */
 	struct pid pids[PIDTYPE_MAX];
+	struct list_head thread_group;
 
 	struct completion *vfork_done;		/* for vfork() */
 	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
@@ -1192,13 +1193,17 @@ extern void wait_task_inactive(task_t * p);
 #define while_each_thread(g, t) \
 	while ((t = next_thread(t)) != g)
 
-extern task_t * FASTCALL(next_thread(const task_t *p));
-
 #define thread_group_leader(p)	(p->pid == p->tgid)
 
+static inline task_t *next_thread(task_t *p)
+{
+	return list_entry(rcu_dereference(p->thread_group.next),
+				task_t, thread_group);
+}
+
 static inline int thread_group_empty(task_t *p)
 {
-	return list_empty(&p->pids[PIDTYPE_TGID].pid_list);
+	return list_empty(&p->thread_group);
 }
 
 #define delay_group_leader(p) \

commit 6a14c5c9da0b4c34b5be783403c54f0396fcfe77
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:18 2006 -0800

    [PATCH] move __exit_signal() to kernel/exit.c
    
    __exit_signal() is private to release_task() now.  I think it is better to
    make it static in kernel/exit.c and export flush_sigqueue() instead - this
    function is much more simple and straightforward.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 921148277da9..a913fca9e70d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1151,7 +1151,6 @@ extern void exit_thread(void);
 extern void exit_files(struct task_struct *);
 extern void __cleanup_signal(struct signal_struct *);
 extern void cleanup_sighand(struct task_struct *);
-extern void __exit_signal(struct task_struct *);
 extern void exit_itimers(struct signal_struct *);
 
 extern NORET_TYPE void do_group_exit(int);

commit c81addc9d3a0ebff2155e0cd86f90820ab97147e
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:17 2006 -0800

    [PATCH] rename __exit_sighand to cleanup_sighand
    
    Cosmetic, rename __exit_sighand to cleanup_sighand and move it close to
    copy_sighand().
    
    This matches copy_signal/cleanup_signal naming, and I think it is easier to
    follow.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7dd430b697aa..921148277da9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1150,8 +1150,8 @@ extern void exit_thread(void);
 
 extern void exit_files(struct task_struct *);
 extern void __cleanup_signal(struct signal_struct *);
+extern void cleanup_sighand(struct task_struct *);
 extern void __exit_signal(struct task_struct *);
-extern void __exit_sighand(struct task_struct *);
 extern void exit_itimers(struct signal_struct *);
 
 extern NORET_TYPE void do_group_exit(int);

commit 6b3934ef52712ece50605dfc72e55d00c580831a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:16 2006 -0800

    [PATCH] copy_process: cleanup bad_fork_cleanup_signal
    
    __exit_signal() does important cleanups atomically under ->siglock.  It is
    also called from copy_process's error path.  This is not good, for example we
    can't move __unhash_process() under ->siglock for that reason.
    
    We should not mix these 2 paths, just look at ugly 'if (p->sighand)' under
    'bad_fork_cleanup_sighand:' label.  For copy_process() case it is sufficient
    to just backout copy_signal(), nothing more.
    
    Again, nobody can see this task yet.  For CLONE_THREAD case we just decrement
    signal->count, otherwise nobody can see this ->signal and we can free it
    lockless.
    
    This patch assumes it is safe to do exit_thread_group_keys() without
    tasklist_lock.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 69c2a1e1529e..7dd430b697aa 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1149,7 +1149,7 @@ extern void flush_thread(void);
 extern void exit_thread(void);
 
 extern void exit_files(struct task_struct *);
-extern void exit_signal(struct task_struct *);
+extern void __cleanup_signal(struct signal_struct *);
 extern void __exit_signal(struct task_struct *);
 extern void __exit_sighand(struct task_struct *);
 extern void exit_itimers(struct signal_struct *);

commit 7001510d0cbf51ad202dd2d0744f54104285cbb9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:14 2006 -0800

    [PATCH] copy_process: cleanup bad_fork_cleanup_sighand
    
    The only caller of exit_sighand(tsk) is copy_process's error path.  We can
    call __exit_sighand() directly and kill exit_sighand().
    
    This 'tsk' was not yet registered in pid_hash[] or init_task.tasks, it has no
    external references, nobody can see it, and
    
            IF (clone_flags & CLONE_SIGHAND)
                    At least 'current' has a reference to ->sighand, this
                    means atomic_dec_and_test(sighand->count) can't be true.
    
            ELSE
                    Nobody can see this ->sighand, this means we can free it
                    without any locking.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ca1fd31aae97..69c2a1e1529e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1151,7 +1151,6 @@ extern void exit_thread(void);
 extern void exit_files(struct task_struct *);
 extern void exit_signal(struct task_struct *);
 extern void __exit_signal(struct task_struct *);
-extern void exit_sighand(struct task_struct *);
 extern void __exit_sighand(struct task_struct *);
 extern void exit_itimers(struct signal_struct *);
 

commit f63ee72e0fb82e504a0489490babc7612c7cd6c2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:13 2006 -0800

    [PATCH] introduce lock_task_sighand() helper
    
    Add lock_task_sighand() helper and converts group_send_sig_info() to use
    it.  Hopefully we will have more users soon.
    
    This patch also removes '!sighand->count' and '!p->usage' checks, I think
    they both are bogus, racy and unneeded (but probably it makes sense to
    restore them as BUG_ON()s).
    
    ->sighand is cleared and it's ->count is decremented in release_task() with
    sighand->siglock held, so it is a bug to have '!p->usage || !->count' after
    we already locked and verified it is the same.  On the other hand, an
    already dead task without ->sighand can have a non-zero ->usage due to
    ptrace, for example.
    
    If we read the stale value of ->sighand we must see the change after
    spin_lock(), because that change was done while holding that same old
    ->sighand.siglock.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index bbcfc873bd98..ca1fd31aae97 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1225,6 +1225,15 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
+extern struct sighand_struct *lock_task_sighand(struct task_struct *tsk,
+							unsigned long *flags);
+
+static inline void unlock_task_sighand(struct task_struct *tsk,
+						unsigned long *flags)
+{
+	spin_unlock_irqrestore(&tsk->sighand->siglock, *flags);
+}
+
 #ifndef __HAVE_THREAD_FUNCTIONS
 
 #define task_thread_info(task) (task)->thread_info

commit aa1757f90bea3f598b6e5d04d922a6a60200f1da
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:12 2006 -0800

    [PATCH] convert sighand_cache to use SLAB_DESTROY_BY_RCU
    
    This patch borrows a clever Hugh's 'struct anon_vma' trick.
    
    Without tasklist_lock held we can't trust task->sighand until we locked it
    and re-checked that it is still the same.
    
    But this means we don't need to defer 'kmem_cache_free(sighand)'.  We can
    return the memory to slab immediately, all we need is to be sure that
    sighand->siglock can't dissapear inside rcu protected section.
    
    To do so we need to initialize ->siglock inside ctor function,
    SLAB_DESTROY_BY_RCU does the rest.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ddc0df7f8bf5..bbcfc873bd98 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -355,16 +355,8 @@ struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];
 	spinlock_t		siglock;
-	struct rcu_head		rcu;
 };
 
-extern void sighand_free_cb(struct rcu_head *rhp);
-
-static inline void sighand_free(struct sighand_struct *sp)
-{
-	call_rcu(&sp->rcu, sighand_free_cb);
-}
-
 /*
  * NOTE! "signal_struct" does not have it's own
  * locking, because a shared signal_struct always

commit 73b9ebfe126a4a886ee46cbab637374d7024668a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:07 2006 -0800

    [PATCH] pidhash: don't count idle threads
    
    fork_idle() does unhash_process() just after copy_process().  Contrary,
    boot_cpu's idle thread explicitely registers itself for each pid_type with nr
    = 0.
    
    copy_process() already checks p->pid != 0 before process_counts++, I think we
    can just skip attach_pid() calls and job control inits for idle threads and
    kill unhash_process().  We don't need to cleanup ->proc_dentry in fork_idle()
    because with this patch idle threads are never hashed in
    kernel/pid.c:pid_hash[].
    
    We don't need to hash pid == 0 in pidmap_init().  free_pidmap() is never
    called with pid == 0 arg, so it will never be reused.  So it is still possible
    to use pid == 0 in any PIDTYPE_xxx namespace from kernel/pid.c's POV.
    
    However with this patch we don't hash pid == 0 for PIDTYPE_PID case.  We still
    have have PIDTYPE_PGID/PIDTYPE_SID entries with pid == 0: /sbin/init and
    kernel threads which don't call daemonize().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1f16fb1fea22..ddc0df7f8bf5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1214,8 +1214,6 @@ static inline int thread_group_empty(task_t *p)
 #define delay_group_leader(p) \
 		(thread_group_leader(p) && !thread_group_empty(p))
 
-extern void unhash_process(struct task_struct *p);
-
 /*
  * Protects ->fs, ->files, ->mm, ->ptrace, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also

commit c97d98931ac52ef110b62d9b75c6a6f2bfbc1898
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:06 2006 -0800

    [PATCH] kill SET_LINKS/REMOVE_LINKS
    
    Both SET_LINKS() and SET_LINKS/REMOVE_LINKS() have exactly one caller, and
    these callers already check thread_group_leader().
    
    This patch kills theese macros, they mix two different things: setting
    process's parent and registering it in init_task.tasks list.  Callers are
    updated to do these actions by hand.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b4b14c32b28a..1f16fb1fea22 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1186,18 +1186,6 @@ extern void wait_task_inactive(task_t * p);
 #define remove_parent(p)	list_del_init(&(p)->sibling)
 #define add_parent(p)		list_add_tail(&(p)->sibling,&(p)->parent->children)
 
-#define REMOVE_LINKS(p) do {					\
-	if (thread_group_leader(p))				\
-		list_del_init(&(p)->tasks);			\
-	remove_parent(p);					\
-	} while (0)
-
-#define SET_LINKS(p) do {					\
-	if (thread_group_leader(p))				\
-		list_add_tail(&(p)->tasks,&init_task.tasks);	\
-	add_parent(p);						\
-	} while (0)
-
 #define next_task(p)	list_entry((p)->tasks.next, struct task_struct, tasks)
 #define prev_task(p)	list_entry((p)->tasks.prev, struct task_struct, tasks)
 

commit 8fafabd86f1b75ed3cc6a6ffbe6c3e53e3d8457d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:05 2006 -0800

    [PATCH] remove add_parent()'s parent argument
    
    add_parent(p, parent) is always called with parent == p->parent, and it makes
    no sense to do it differently.  This patch removes this argument.
    
    No changes in affected .o files.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f5ab98bbb6b..b4b14c32b28a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1184,7 +1184,7 @@ extern void wait_task_inactive(task_t * p);
 #endif
 
 #define remove_parent(p)	list_del_init(&(p)->sibling)
-#define add_parent(p, parent)	list_add_tail(&(p)->sibling,&(parent)->children)
+#define add_parent(p)		list_add_tail(&(p)->sibling,&(p)->parent->children)
 
 #define REMOVE_LINKS(p) do {					\
 	if (thread_group_leader(p))				\
@@ -1195,7 +1195,7 @@ extern void wait_task_inactive(task_t * p);
 #define SET_LINKS(p) do {					\
 	if (thread_group_leader(p))				\
 		list_add_tail(&(p)->tasks,&init_task.tasks);	\
-	add_parent(p, (p)->parent);				\
+	add_parent(p);						\
 	} while (0)
 
 #define next_task(p)	list_entry((p)->tasks.next, struct task_struct, tasks)

commit 6c99c5cb94319a601b5ec5ee31c331f84755dd74
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 28 16:11:00 2006 -0800

    [PATCH] Remove dead kill_sl prototype from sched.h
    
    The kill_sl function doesn't exist in the kernel so a prototype is completely
    unnecessary.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20b4f0372e44..5f5ab98bbb6b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1101,7 +1101,6 @@ extern void force_sig_specific(int, struct task_struct *);
 extern int send_sig(int, struct task_struct *, int);
 extern void zap_other_threads(struct task_struct *p);
 extern int kill_pg(pid_t, int, int);
-extern int kill_sl(pid_t, int, int);
 extern int kill_proc(pid_t, int, int);
 extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);

commit 34f192c6527f20c47ccec239e7d51a27691b93fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 27 01:16:24 2006 -0800

    [PATCH] lightweight robust futexes: compat
    
    32-bit syscall compatibility support.  (This patch also moves all futex
    related compat functionality into kernel/futex_compat.c.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Acked-by: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fd4848f2d750..20b4f0372e44 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -874,6 +874,9 @@ struct task_struct {
 	int cpuset_mem_spread_rotor;
 #endif
 	struct robust_list_head __user *robust_list;
+#ifdef CONFIG_COMPAT
+	struct compat_robust_list_head __user *compat_robust_list;
+#endif
 
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;

commit 0771dfefc9e538f077d0b43b6dec19a5a67d0e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 27 01:16:22 2006 -0800

    [PATCH] lightweight robust futexes: core
    
    Add the core infrastructure for robust futexes: structure definitions, the new
    syscalls and the do_exit() based cleanup mechanism.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Acked-by: Ulrich Drepper <drepper@redhat.com>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 036d14d2bf90..fd4848f2d750 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -35,6 +35,7 @@
 #include <linux/topology.h>
 #include <linux/seccomp.h>
 #include <linux/rcupdate.h>
+#include <linux/futex.h>
 
 #include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
 
@@ -872,6 +873,8 @@ struct task_struct {
 	int cpuset_mems_generation;
 	int cpuset_mem_spread_rotor;
 #endif
+	struct robust_list_head __user *robust_list;
+
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;
 };

commit 05cfb614ddbf3181540ce09d44d96486f8ba8d6a
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Sun Mar 26 01:38:12 2006 -0800

    [PATCH] hrtimers: remove data field
    
    The nanosleep cleanup allows to remove the data field of hrtimer.  The
    callback function can use container_of() to get it's own data.  Since the
    hrtimer structure is anyway embedded in other structures, this adds no
    overhead.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e0054c1b9a09..036d14d2bf90 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -402,6 +402,7 @@ struct signal_struct {
 
 	/* ITIMER_REAL timer for the process */
 	struct hrtimer real_timer;
+	struct task_struct *tsk;
 	ktime_t it_real_incr;
 
 	/* ITIMER_PROF and ITIMER_VIRTUAL timers for the process */

commit 6687a97d4041f996f725902d2990e5de6ef5cbe5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 24 03:18:41 2006 -0800

    [PATCH] timer-irq-driven soft-watchdog, cleanups
    
    Make the softlockup detector purely timer-interrupt driven, removing
    softirq-context (timer) dependencies.  This means that if the softlockup
    watchdog triggers, it has truly observed a longer than 10 seconds
    scheduling delay of a SCHED_FIFO prio 99 task.
    
    (the patch also turns off the softlockup detector during the initial bootup
    phase and does small style fixes)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2cda439ece43..e0054c1b9a09 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -206,11 +206,11 @@ extern void update_process_times(int user);
 extern void scheduler_tick(void);
 
 #ifdef CONFIG_DETECT_SOFTLOCKUP
-extern void softlockup_tick(struct pt_regs *regs);
+extern void softlockup_tick(void);
 extern void spawn_softlockup_task(void);
 extern void touch_softlockup_watchdog(void);
 #else
-static inline void softlockup_tick(struct pt_regs *regs)
+static inline void softlockup_tick(void)
 {
 }
 static inline void spawn_softlockup_task(void)

commit c61afb181c649754ea221f104e268cbacfc993e3
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:08 2006 -0800

    [PATCH] cpuset memory spread slab cache optimizations
    
    The hooks in the slab cache allocator code path for support of NUMA
    mempolicies and cpuset memory spreading are in an important code path.  Many
    systems will use neither feature.
    
    This patch optimizes those hooks down to a single check of some bits in the
    current tasks task_struct flags.  For non NUMA systems, this hook and related
    code is already ifdef'd out.
    
    The optimization is done by using another task flag, set if the task is using
    a non-default NUMA mempolicy.  Taking this flag bit along with the
    PF_SPREAD_PAGE and PF_SPREAD_SLAB flag bits added earlier in this 'cpuset
    memory spreading' patch set, one can check for the combination of any of these
    special case memory placement mechanisms with a single test of the current
    tasks task_struct flags.
    
    This patch also tightens up the code, to save a few bytes of kernel text
    space, and moves some of it out of line.  Due to the nested inlines called
    from multiple places, we were ending up with three copies of this code, which
    once we get off the main code path (for local node allocation) seems a bit
    wasteful of instruction memory.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0e37cfa09f5..2cda439ece43 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -932,6 +932,7 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_SWAPWRITE	0x01000000	/* Allowed to write to swap */
 #define PF_SPREAD_PAGE	0x04000000	/* Spread page cache over cpuset */
 #define PF_SPREAD_SLAB	0x08000000	/* Spread some slab caches over cpuset */
+#define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 825a46af5ac171f9f41f794a0a00165588ba1589
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:03 2006 -0800

    [PATCH] cpuset memory spread basic implementation
    
    This patch provides the implementation and cpuset interface for an alternative
    memory allocation policy that can be applied to certain kinds of memory
    allocations, such as the page cache (file system buffers) and some slab caches
    (such as inode caches).
    
    The policy is called "memory spreading." If enabled, it spreads out these
    kinds of memory allocations over all the nodes allowed to a task, instead of
    preferring to place them on the node where the task is executing.
    
    All other kinds of allocations, including anonymous pages for a tasks stack
    and data regions, are not affected by this policy choice, and continue to be
    allocated preferring the node local to execution, as modified by the NUMA
    mempolicy.
    
    There are two boolean flag files per cpuset that control where the kernel
    allocates pages for the file system buffers and related in kernel data
    structures.  They are called 'memory_spread_page' and 'memory_spread_slab'.
    
    If the per-cpuset boolean flag file 'memory_spread_page' is set, then the
    kernel will spread the file system buffers (page cache) evenly over all the
    nodes that the faulting task is allowed to use, instead of preferring to put
    those pages on the node where the task is running.
    
    If the per-cpuset boolean flag file 'memory_spread_slab' is set, then the
    kernel will spread some file system related slab caches, such as for inodes
    and dentries evenly over all the nodes that the faulting task is allowed to
    use, instead of preferring to put those pages on the node where the task is
    running.
    
    The implementation is simple.  Setting the cpuset flags 'memory_spread_page'
    or 'memory_spread_cache' turns on the per-process flags PF_SPREAD_PAGE or
    PF_SPREAD_SLAB, respectively, for each task that is in the cpuset or
    subsequently joins that cpuset.  In subsequent patches, the page allocation
    calls for the affected page cache and slab caches are modified to perform an
    inline check for these flags, and if set, a call to a new routine
    cpuset_mem_spread_node() returns the node to prefer for the allocation.
    
    The cpuset_mem_spread_node() routine is also simple.  It uses the value of a
    per-task rotor cpuset_mem_spread_rotor to select the next node in the current
    tasks mems_allowed to prefer for the allocation.
    
    This policy can provide substantial improvements for jobs that need to place
    thread local data on the corresponding node, but that need to access large
    file system data sets that need to be spread across the several nodes in the
    jobs cpuset in order to fit.  Without this patch, especially for jobs that
    might have one thread reading in the data set, the memory allocation across
    the nodes in the jobs cpuset can become very uneven.
    
    A couple of Copyright year ranges are updated as well.  And a couple of email
    addresses that can be found in the MAINTAINERS file are removed.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e60a91d5b369..b0e37cfa09f5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -869,6 +869,7 @@ struct task_struct {
 	struct cpuset *cpuset;
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;
+	int cpuset_mem_spread_rotor;
 #endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */
 	struct rcu_head rcu;
@@ -929,6 +930,8 @@ static inline void put_task_struct(struct task_struct *t)
 #define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
 #define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
 #define PF_SWAPWRITE	0x01000000	/* Allowed to write to swap */
+#define PF_SPREAD_PAGE	0x04000000	/* Spread page cache over cpuset */
+#define PF_SPREAD_SLAB	0x08000000	/* Spread some slab caches over cpuset */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 2056a782f8e7e65fd4bfd027506b4ce1c5e9ccd4
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Mar 23 20:00:26 2006 +0100

    [PATCH] Block queue IO tracing support (blktrace) as of 2006-03-23
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 62e6314382f0..e60a91d5b369 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -706,6 +706,7 @@ struct task_struct {
 	prio_array_t *array;
 
 	unsigned short ioprio;
+	unsigned int btrace_seq;
 
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;

commit 7cd9013be6c22f3ff6f777354f766c8c0b955e17
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Mar 11 03:27:18 2006 -0800

    [PATCH] remove __put_task_struct_cb export again
    
    The patch '[PATCH] RCU signal handling' [1] added an export for
    __put_task_struct_cb, a put_task_struct helper newly introduced in that
    patch.  But the put_task_struct couldn't be used modular previously as
    __put_task_struct wasn't exported.  There are not callers of it in modular
    code, and it shouldn't be exported because we don't want drivers to hold
    references to task_structs.
    
    This patch removes the export and folds __put_task_struct into
    __put_task_struct_cb as there's no other caller.
    
    [1] http://www2.kernel.org/git/gitweb.cgi?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=e56d090310d7625ecb43a1eeebd479f04affb48b
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff2e09c953b9..62e6314382f0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -892,7 +892,6 @@ static inline int pid_alive(struct task_struct *p)
 }
 
 extern void free_task(struct task_struct *tsk);
-extern void __put_task_struct(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
 extern void __put_task_struct_cb(struct rcu_head *rhp);

commit 0551fbd29e16fccd46e41b7d01bf0f8f39b14212
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 28 16:59:19 2006 -0800

    [PATCH] Add mm->task_size and fix powerpc vdso
    
    This patch adds mm->task_size to keep track of the task size of a given mm
    and uses that to fix the powerpc vdso so that it uses the mm task size to
    decide what pages to fault in instead of the current thread flags (which
    broke when ptracing).
    
    (akpm: I expect that mm_struct.task_size will become the way in which we
    finally sort out the confusion between 32-bit processes and 32-bit mm's.  It
    may need tweaks, but at this stage this patch is powerpc-only.)
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b6f51e3a38ec..ff2e09c953b9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -298,8 +298,9 @@ struct mm_struct {
 				unsigned long addr, unsigned long len,
 				unsigned long pgoff, unsigned long flags);
 	void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
-        unsigned long mmap_base;		/* base of mmap area */
-        unsigned long cached_hole_size;         /* if non-zero, the largest hole below free_area_cache */
+	unsigned long mmap_base;		/* base of mmap area */
+	unsigned long task_size;		/* size of task vm space */
+	unsigned long cached_hole_size;         /* if non-zero, the largest hole below free_area_cache */
 	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
 	pgd_t * pgd;
 	atomic_t mm_users;			/* How many users with user space? */

commit d6077cb80cde4506720f9165eba99ee07438513f
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Tue Feb 14 13:53:10 2006 -0800

    [PATCH] sched: revert "filter affine wakeups"
    
    Revert commit d7102e95b7b9c00277562c29aad421d2d521c5f6:
    
        [PATCH] sched: filter affine wakeups
    
    Apparently caused more than 10% performance regression for aim7 benchmark.
    The setup in use is 16-cpu HP rx8620, 64Gb of memory and 12 MSA1000s with 144
    disks.  Each disk is 72Gb with a single ext3 filesystem (courtesy of HP, who
    supplied benchmark results).
    
    The problem is, for aim7, the wake-up pattern is random, but it still needs
    load balancing action in the wake-up path to achieve best performance.  With
    the above commit, lack of load balancing hurts that workload.
    
    However, for workloads like database transaction processing, the requirement
    is exactly opposite.  In the wake up path, best performance is achieved with
    absolutely zero load balancing.  We simply wake up the process on the CPU that
    it was previously run.  Worst performance is obtained when we do load
    balancing at wake up.
    
    There isn't an easy way to auto detect the workload characteristics.  Ingo's
    earlier patch that detects idle CPU and decide whether to load balance or not
    doesn't perform with aim7 either since all CPUs are busy (it causes even
    bigger perf.  regression).
    
    Revert commit d7102e95b7b9c00277562c29aad421d2d521c5f6, which causes more
    than 10% performance regression with aim7.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9c1da0269a18..b6f51e3a38ec 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -697,11 +697,8 @@ struct task_struct {
 
 	int lock_depth;		/* BKL lock depth */
 
-#if defined(CONFIG_SMP)
-	int last_waker_cpu;	/* CPU that last woke this task up */
-#if defined(__ARCH_WANT_UNLOCKED_CTXSW)
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
 	int oncpu;
-#endif
 #endif
 	int prio, static_prio;
 	struct list_head run_list;

commit 9ac95f2f90e022c16d293d7978faddf7e779a1a9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Feb 9 22:41:50 2006 +0300

    [PATCH] do_sigaction: cleanup ->sa_mask manipulation
    
    Clear unblockable signals beforehand.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 0cfcd1c7865e..9c1da0269a18 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1098,7 +1098,7 @@ extern struct sigqueue *sigqueue_alloc(void);
 extern void sigqueue_free(struct sigqueue *);
 extern int send_sigqueue(int, struct sigqueue *,  struct task_struct *);
 extern int send_group_sigqueue(int, struct sigqueue *,  struct task_struct *);
-extern int do_sigaction(int, const struct k_sigaction *, struct k_sigaction *);
+extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);
 extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
 
 /* These can be the second arg to send_sig_info/send_group_sig_info.  */

commit 150256d8aadb3a337c31efa9e175cbd25bf06b06
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Jan 18 17:43:57 2006 -0800

    [PATCH] Generic sys_rt_sigsuspend()
    
    The TIF_RESTORE_SIGMASK flag allows us to have a generic implementation of
    sys_rt_sigsuspend() instead of duplicating it for each architecture.  This
    provides such an implementation and makes arch/powerpc use it.
    
    It also tidies up the ppc32 sys_sigsuspend() to use TIF_RESTORE_SIGMASK.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2df1a1a2fee5..0cfcd1c7865e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -809,6 +809,7 @@ struct task_struct {
 	struct sighand_struct *sighand;
 
 	sigset_t blocked, real_blocked;
+	sigset_t saved_sigmask;		/* To be restored with TIF_RESTORE_SIGMASK */
 	struct sigpending pending;
 
 	unsigned long sas_ss_sp;

commit b0a9499c3dd50d333e2aedb7e894873c58da3785
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 14 13:20:41 2006 -0800

    [PATCH] sched: add new SCHED_BATCH policy
    
    Add a new SCHED_BATCH (3) scheduling policy: such tasks are presumed
    CPU-intensive, and will acquire a constant +5 priority level penalty.  Such
    policy is nice for workloads that are non-interactive, but which do not
    want to give up their nice levels.  The policy is also useful for workloads
    that want a deterministic scheduling policy without interactivity causing
    extra preemptions (between that workload's tasks).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a72e17135421..2df1a1a2fee5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -160,6 +160,7 @@ extern unsigned long nr_iowait(void);
 #define SCHED_NORMAL		0
 #define SCHED_FIFO		1
 #define SCHED_RR		2
+#define SCHED_BATCH		3
 
 struct sched_param {
 	int sched_priority;
@@ -470,9 +471,9 @@ struct signal_struct {
 
 /*
  * Priority of a process goes from 0..MAX_PRIO-1, valid RT
- * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL tasks are
- * in the range MAX_RT_PRIO..MAX_PRIO-1. Priority values
- * are inverted: lower p->prio value means higher priority.
+ * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
+ * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
+ * values are inverted: lower p->prio value means higher priority.
  *
  * The MAX_USER_RT_PRIO value allows the actual maximum
  * RT priority to be separate from the value exported to

commit 9fc658763bf992e778243ebe898b03746151ab88
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Jan 12 01:05:34 2006 -0800

    [PATCH] missing helper - task_stack_page()
    
    Patchset annotates arch/* uses of ->thread_info.  Ones that really are about
    access of thread_info of given process are simply switched to
    task_thread_info(task); ones that deal with access to objects on stack are
    switched to new helper - task_stack_page().  A _lot_ of the latter are
    actually open-coded instances of "find where pt_regs are"; those are
    consolidated into task_pt_regs(task) (many architectures actually have such
    helper already).
    
    Note that these annotations are not mandatory - any code not converted to
    these helpers still works.  However, they clean up a lot of places and have
    actually caught a number of bugs, so converting out of tree ports would be a
    good idea...
    
    As an example of breakage caught by that stuff, see i386 pt_regs mess - we
    used to have it open-coded in a bunch of places and when back in April Stas
    had fixed a bug in copy_thread(), the rest had been left out of sync.  That
    required two followup patches (the latest - just before 2.6.15) _and_ still
    had left /proc/*/stat eip field broken.  Try ps -eo eip on i386 and watch the
    junk...
    
    This patch:
    
    new helper - task_stack_page(task).  Returns pointer to the memory object
    containing task stack; usually thread_info of task sits in the beginning
    of that object.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b5ef92a043a6..a72e17135421 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1240,6 +1240,7 @@ static inline void task_unlock(struct task_struct *p)
 #ifndef __HAVE_THREAD_FUNCTIONS
 
 #define task_thread_info(task) (task)->thread_info
+#define task_stack_page(task) ((void*)((task)->thread_info))
 
 static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
 {

commit d7102e95b7b9c00277562c29aad421d2d521c5f6
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Thu Jan 12 01:05:32 2006 -0800

    [PATCH] sched: filter affine wakeups
    
    )
    
    From: Nick Piggin <nickpiggin@yahoo.com.au>
    
    Track the last waker CPU, and only consider wakeup-balancing if there's a
    match between current waker CPU and the previous waker CPU.  This ensures
    that there is some correlation between two subsequent wakeup events before
    we move the task.  Should help random-wakeup workloads on large SMP
    systems, by reducing the migration attempts by a factor of nr_cpus.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5d6b9228bba9..b5ef92a043a6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -696,8 +696,11 @@ struct task_struct {
 
 	int lock_depth;		/* BKL lock depth */
 
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+#if defined(CONFIG_SMP)
+	int last_waker_cpu;	/* CPU that last woke this task up */
+#if defined(__ARCH_WANT_UNLOCKED_CTXSW)
 	int oncpu;
+#endif
 #endif
 	int prio, static_prio;
 	struct list_head run_list;

commit 198e2f181163233b379dc7ce8a6d7516b84042e7
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Thu Jan 12 01:05:30 2006 -0800

    [PATCH] scheduler cache-hot-autodetect
    
    )
    
    From: Ingo Molnar <mingo@elte.hu>
    
    This is the latest version of the scheduler cache-hot-auto-tune patch.
    
    The first problem was that detection time scaled with O(N^2), which is
    unacceptable on larger SMP and NUMA systems. To solve this:
    
    - I've added a 'domain distance' function, which is used to cache
      measurement results. Each distance is only measured once. This means
      that e.g. on NUMA distances of 0, 1 and 2 might be measured, on HT
      distances 0 and 1, and on SMP distance 0 is measured. The code walks
      the domain tree to determine the distance, so it automatically follows
      whatever hierarchy an architecture sets up. This cuts down on the boot
      time significantly and removes the O(N^2) limit. The only assumption
      is that migration costs can be expressed as a function of domain
      distance - this covers the overwhelming majority of existing systems,
      and is a good guess even for more assymetric systems.
    
      [ People hacking systems that have assymetries that break this
        assumption (e.g. different CPU speeds) should experiment a bit with
        the cpu_distance() function. Adding a ->migration_distance factor to
        the domain structure would be one possible solution - but lets first
        see the problem systems, if they exist at all. Lets not overdesign. ]
    
    Another problem was that only a single cache-size was used for measuring
    the cost of migration, and most architectures didnt set that variable
    up. Furthermore, a single cache-size does not fit NUMA hierarchies with
    L3 caches and does not fit HT setups, where different CPUs will often
    have different 'effective cache sizes'. To solve this problem:
    
    - Instead of relying on a single cache-size provided by the platform and
      sticking to it, the code now auto-detects the 'effective migration
      cost' between two measured CPUs, via iterating through a wide range of
      cachesizes. The code searches for the maximum migration cost, which
      occurs when the working set of the test-workload falls just below the
      'effective cache size'. I.e. real-life optimized search is done for
      the maximum migration cost, between two real CPUs.
    
      This, amongst other things, has the positive effect hat if e.g. two
      CPUs share a L2/L3 cache, a different (and accurate) migration cost
      will be found than between two CPUs on the same system that dont share
      any caches.
    
    (The reliable measurement of migration costs is tricky - see the source
    for details.)
    
    Furthermore i've added various boot-time options to override/tune
    migration behavior.
    
    Firstly, there's a blanket override for autodetection:
    
            migration_cost=1000,2000,3000
    
    will override the depth 0/1/2 values with 1msec/2msec/3msec values.
    
    Secondly, there's a global factor that can be used to increase (or
    decrease) the autodetected values:
    
            migration_factor=120
    
    will increase the autodetected values by 20%. This option is useful to
    tune things in a workload-dependent way - e.g. if a workload is
    cache-insensitive then CPU utilization can be maximized by specifying
    migration_factor=0.
    
    I've tested the autodetection code quite extensively on x86, on 3
    P3/Xeon/2MB, and the autodetected values look pretty good:
    
    Dual Celeron (128K L2 cache):
    
     ---------------------
     migration cost matrix (max_cache_size: 131072, cpu: 467 MHz):
     ---------------------
               [00]    [01]
     [00]:     -     1.7(1)
     [01]:   1.7(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (0) 1.7 (1784008)
     ---------------------
    
    Here the slow memory subsystem dominates system performance, and even
    though caches are small, the migration cost is 1.7 msecs.
    
    Dual HT P4 (512K L2 cache):
    
     ---------------------
     migration cost matrix (max_cache_size: 524288, cpu: 2379 MHz):
     ---------------------
               [00]    [01]    [02]    [03]
     [00]:     -     0.4(1)  0.0(0)  0.4(1)
     [01]:   0.4(1)    -     0.4(1)  0.0(0)
     [02]:   0.0(0)  0.4(1)    -     0.4(1)
     [03]:   0.4(1)  0.0(0)  0.4(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (33900) 0.4 (448514)
     ---------------------
    
    Here it can be seen that there is no migration cost between two HT
    siblings (CPU#0/2 and CPU#1/3 are separate physical CPUs). A fast memory
    system makes inter-physical-CPU migration pretty cheap: 0.4 msecs.
    
    8-way P3/Xeon [2MB L2 cache]:
    
     ---------------------
     migration cost matrix (max_cache_size: 2097152, cpu: 700 MHz):
     ---------------------
               [00]    [01]    [02]    [03]    [04]    [05]    [06]    [07]
     [00]:     -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [01]:  19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [02]:  19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [03]:  19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1) 19.2(1)
     [04]:  19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1) 19.2(1)
     [05]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1) 19.2(1)
     [06]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -    19.2(1)
     [07]:  19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1) 19.2(1)    -
     ---------------------
     cacheflush times [2]: 0.0 (0) 19.2 (19281756)
     ---------------------
    
    This one has huge caches and a relatively slow memory subsystem - so the
    migration cost is 19 msecs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Cc: <wilder@us.ibm.com>
    Signed-off-by: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3b74c4bf2934..5d6b9228bba9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -631,7 +631,14 @@ struct sched_domain {
 
 extern void partition_sched_domains(cpumask_t *partition1,
 				    cpumask_t *partition2);
-#endif /* CONFIG_SMP */
+
+/*
+ * Maximum cache size the migration-costs auto-tuning code will
+ * search from:
+ */
+extern unsigned int max_cache_size;
+
+#endif	/* CONFIG_SMP */
 
 
 struct io_context;			/* See blkdev.h */

commit c59ede7b78db329949d9cdcd7064e22d357560ef
Author: Randy.Dunlap <rdunlap@xenotime.net>
Date:   Wed Jan 11 12:17:46 2006 -0800

    [PATCH] move capable() to capability.h
    
    - Move capable() from sched.h to capability.h;
    
    - Use <linux/capability.h> where capable() is used
            (in include/, block/, ipc/, kernel/, a few drivers/,
            mm/, security/, & sound/;
            many more drivers/ to go)
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2ae8711bfba1..3b74c4bf2934 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1112,10 +1112,6 @@ static inline int sas_ss_flags(unsigned long sp)
 		: on_sig_stack(sp) ? SS_ONSTACK : 0);
 }
 
-
-/* code is in security.c or kernel/sys.c if !SECURITY */
-extern int capable(int cap);
-
 /*
  * Routines for handling mm_structs
  */

commit e16885c5ad624a6efe1b1bf764e075d75f65a788
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 11 12:17:45 2006 -0800

    [PATCH] uninline capable()
    
    Uninline capable().  Saves 2K of kernel text on a generic .config, and 1K on a
    tiny config.  In addition it makes the use of capable more consistent between
    CONFIG_SECURITY and !CONFIG_SECURITY
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c4ee35dd18ae..2ae8711bfba1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1113,19 +1113,8 @@ static inline int sas_ss_flags(unsigned long sp)
 }
 
 
-#ifdef CONFIG_SECURITY
-/* code is in security.c */
+/* code is in security.c or kernel/sys.c if !SECURITY */
 extern int capable(int cap);
-#else
-static inline int capable(int cap)
-{
-	if (cap_raised(current->cap_effective, cap)) {
-		current->flags |= PF_SUPERPRIV;
-		return 1;
-	}
-	return 0;
-}
-#endif
 
 /*
  * Routines for handling mm_structs

commit 4c29c4c5f28616f2a87f0e6499aa9776d9be58ad
Author: Adrian Bunk <bunk@stusta.de>
Date:   Mon Jan 9 20:54:50 2006 -0800

    [PATCH] include/linux/sched.h: no need to guard the normalize_rt_tasks() prototype
    
    There's no need to guard the normalize_rt_tasks() prototype with an #ifdef
    CONFIG_MAGIC_SYSRQ.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ee4677ad204e..c4ee35dd18ae 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1390,12 +1390,8 @@ static inline void arch_pick_mmap_layout(struct mm_struct *mm)
 extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 
-#ifdef CONFIG_MAGIC_SYSRQ
-
 extern void normalize_rt_tasks(void);
 
-#endif
-
 #ifdef CONFIG_PM
 /*
  * Check if a process has been frozen

commit 2ff678b8da6478d861c1b0ecb3ac14575760e906
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jan 9 20:52:34 2006 -0800

    [PATCH] hrtimer: switch itimers to hrtimer
    
    switch itimers to a hrtimers-based implementation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 85b53f87c703..ee4677ad204e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -105,6 +105,7 @@ extern unsigned long nr_iowait(void);
 #include <linux/param.h>
 #include <linux/resource.h>
 #include <linux/timer.h>
+#include <linux/hrtimer.h>
 
 #include <asm/processor.h>
 
@@ -398,8 +399,8 @@ struct signal_struct {
 	struct list_head posix_timers;
 
 	/* ITIMER_REAL timer for the process */
-	struct timer_list real_timer;
-	unsigned long it_real_value, it_real_incr;
+	struct hrtimer real_timer;
+	ktime_t it_real_incr;
 
 	/* ITIMER_PROF and ITIMER_VIRTUAL timers for the process */
 	cputime_t it_prof_expires, it_virt_expires;

commit 408894ee4dd4debfdedd472eb4d8414892fc90f6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jan 9 15:59:20 2006 -0800

    [PATCH] mutex subsystem, debugging code
    
    mutex implementation - add debugging code.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 78eb92ae4d94..85b53f87c703 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -817,6 +817,11 @@ struct task_struct {
 /* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
 	spinlock_t proc_lock;
 
+#ifdef CONFIG_DEBUG_MUTEXES
+	/* mutex deadlock detection */
+	struct mutex_waiter *blocked_on;
+#endif
+
 /* journalling filesystem info */
 	void *journal_info;
 

commit b5f545c880a2a47947ba2118b2509644ab7a2969
Author: David Howells <dhowells@redhat.com>
Date:   Sun Jan 8 01:02:47 2006 -0800

    [PATCH] keys: Permit running process to instantiate keys
    
    Make it possible for a running process (such as gssapid) to be able to
    instantiate a key, as was requested by Trond Myklebust for NFS4.
    
    The patch makes the following changes:
    
     (1) A new, optional key type method has been added. This permits a key type
         to intercept requests at the point /sbin/request-key is about to be
         spawned and do something else with them - passing them over the
         rpc_pipefs files or netlink sockets for instance.
    
         The uninstantiated key, the authorisation key and the intended operation
         name are passed to the method.
    
     (2) The callout_info is no longer passed as an argument to /sbin/request-key
         to prevent unauthorised viewing of this data using ps or by looking in
         /proc/pid/cmdline.
    
         This means that the old /sbin/request-key program will not work with the
         patched kernel as it will expect to see an extra argument that is no
         longer there.
    
         A revised keyutils package will be made available tomorrow.
    
     (3) The callout_info is now attached to the authorisation key. Reading this
         key will retrieve the information.
    
     (4) A new field has been added to the task_struct. This holds the
         authorisation key currently active for a thread. Searches now look here
         for the caller's set of keys rather than looking for an auth key in the
         lowest level of the session keyring.
    
         This permits a thread to be servicing multiple requests at once and to
         switch between them. Note that this is per-thread, not per-process, and
         so is usable in multithreaded programs.
    
         The setting of this field is inherited across fork and exec.
    
     (5) A new keyctl function (KEYCTL_ASSUME_AUTHORITY) has been added that
         permits a thread to assume the authority to deal with an uninstantiated
         key. Assumption is only permitted if the authorisation key associated
         with the uninstantiated key is somewhere in the thread's keyrings.
    
         This function can also clear the assumption.
    
     (6) A new magic key specifier has been added to refer to the currently
         assumed authorisation key (KEY_SPEC_REQKEY_AUTH_KEY).
    
     (7) Instantiation will only proceed if the appropriate authorisation key is
         assumed first. The assumed authorisation key is discarded if
         instantiation is successful.
    
     (8) key_validate() is moved from the file of request_key functions to the
         file of permissions functions.
    
     (9) The documentation is updated.
    
    From: <Valdis.Kletnieks@vt.edu>
    
        Build fix.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: Alexander Zangerl <az@bond.edu.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 20bd70749104..78eb92ae4d94 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -771,6 +771,7 @@ struct task_struct {
 	unsigned keep_capabilities:1;
 	struct user_struct *user;
 #ifdef CONFIG_KEYS
+	struct key *request_key_auth;	/* assumed request_key authority */
 	struct key *thread_keyring;	/* keyring private to this thread */
 	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 #endif

commit d4829cd5b4bd1ea58ba1bebad44d562f4027c290
Author: Paul E. McKenney <paulmck@us.ibm.com>
Date:   Sun Jan 8 01:01:39 2006 -0800

    [PATCH] remove get_task_struct_rcu()
    
    The latest set of signal-RCU patches does not use get_task_struct_rcu().
    Attached is a patch that removes it.
    
    Signed-off-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a6af77e9b4cf..20bd70749104 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -878,18 +878,6 @@ extern void free_task(struct task_struct *tsk);
 extern void __put_task_struct(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
-static inline int get_task_struct_rcu(struct task_struct *t)
-{
-	int oldusage;
-
-	do {
-		oldusage = atomic_read(&t->usage);
-		if (oldusage == 0)
-			return 0;
-	} while (cmpxchg(&t->usage.counter, oldusage, oldusage+1) != oldusage);
-	return 1;
-}
-
 extern void __put_task_struct_cb(struct rcu_head *rhp);
 
 static inline void put_task_struct(struct task_struct *t)

commit e56d090310d7625ecb43a1eeebd479f04affb48b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 8 01:01:37 2006 -0800

    [PATCH] RCU signal handling
    
    RCU tasklist_lock and RCU signal handling: send signals RCU-read-locked
    instead of tasklist_lock read-locked.  This is a scalability improvement on
    SMP and a preemption-latency improvement under PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: William Irwin <wli@holomorphy.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a74662077d60..a6af77e9b4cf 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,6 +34,7 @@
 #include <linux/percpu.h>
 #include <linux/topology.h>
 #include <linux/seccomp.h>
+#include <linux/rcupdate.h>
 
 #include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
 
@@ -350,8 +351,16 @@ struct sighand_struct {
 	atomic_t		count;
 	struct k_sigaction	action[_NSIG];
 	spinlock_t		siglock;
+	struct rcu_head		rcu;
 };
 
+extern void sighand_free_cb(struct rcu_head *rhp);
+
+static inline void sighand_free(struct sighand_struct *sp)
+{
+	call_rcu(&sp->rcu, sighand_free_cb);
+}
+
 /*
  * NOTE! "signal_struct" does not have it's own
  * locking, because a shared signal_struct always
@@ -844,6 +853,7 @@ struct task_struct {
 	int cpuset_mems_generation;
 #endif
 	atomic_t fs_excl;	/* holding fs exclusive resources */
+	struct rcu_head rcu;
 };
 
 static inline pid_t process_group(struct task_struct *tsk)
@@ -867,8 +877,26 @@ static inline int pid_alive(struct task_struct *p)
 extern void free_task(struct task_struct *tsk);
 extern void __put_task_struct(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
-#define put_task_struct(tsk) \
-do { if (atomic_dec_and_test(&(tsk)->usage)) __put_task_struct(tsk); } while(0)
+
+static inline int get_task_struct_rcu(struct task_struct *t)
+{
+	int oldusage;
+
+	do {
+		oldusage = atomic_read(&t->usage);
+		if (oldusage == 0)
+			return 0;
+	} while (cmpxchg(&t->usage.counter, oldusage, oldusage+1) != oldusage);
+	return 1;
+}
+
+extern void __put_task_struct_cb(struct rcu_head *rhp);
+
+static inline void put_task_struct(struct task_struct *t)
+{
+	if (atomic_dec_and_test(&t->usage))
+		call_rcu(&t->rcu, __put_task_struct_cb);
+}
 
 /*
  * Per process flags

commit 930d915252edda7042c944ed3c30194a2f9fe163
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun Jan 8 01:00:47 2006 -0800

    [PATCH] Swap Migration V5: PF_SWAPWRITE to allow writing to swap
    
    Add PF_SWAPWRITE to control a processes permission to write to swap.
    
    - Use PF_SWAPWRITE in may_write_to_queue() instead of checking for kswapd
      and pdflush
    
    - Set PF_SWAPWRITE flag for kswapd and pdflush
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7da33619d5d0..a74662077d60 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -895,6 +895,7 @@ do { if (atomic_dec_and_test(&(tsk)->usage)) __put_task_struct(tsk); } while(0)
 #define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
 #define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
 #define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
+#define PF_SWAPWRITE	0x01000000	/* Allowed to write to swap */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit d3cb487149bd706aa6aeb02042332a450978dc1c
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Fri Jan 6 00:11:20 2006 -0800

    [PATCH] atomic_long_t & include/asm-generic/atomic.h V2
    
    Several counters already have the need to use 64 atomic variables on 64 bit
    platforms (see mm_counter_t in sched.h).  We have to do ugly ifdefs to fall
    back to 32 bit atomic on 32 bit platforms.
    
    The VM statistics patch that I am working on will also make more extensive
    use of atomic64.
    
    This patch introduces a new type atomic_long_t by providing definitions in
    asm-generic/atomic.h that works similar to the c "long" type.  Its 32 bits
    on 32 bit platforms and 64 bits on 64 bit platforms.
    
    Also cleans up the determination of the mm_counter_t in sched.h.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b0ad6f30679e..7da33619d5d0 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -254,25 +254,12 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
  * The mm counters are not protected by its page_table_lock,
  * so must be incremented atomically.
  */
-#ifdef ATOMIC64_INIT
-#define set_mm_counter(mm, member, value) atomic64_set(&(mm)->_##member, value)
-#define get_mm_counter(mm, member) ((unsigned long)atomic64_read(&(mm)->_##member))
-#define add_mm_counter(mm, member, value) atomic64_add(value, &(mm)->_##member)
-#define inc_mm_counter(mm, member) atomic64_inc(&(mm)->_##member)
-#define dec_mm_counter(mm, member) atomic64_dec(&(mm)->_##member)
-typedef atomic64_t mm_counter_t;
-#else /* !ATOMIC64_INIT */
-/*
- * The counters wrap back to 0 at 2^32 * PAGE_SIZE,
- * that is, at 16TB if using 4kB page size.
- */
-#define set_mm_counter(mm, member, value) atomic_set(&(mm)->_##member, value)
-#define get_mm_counter(mm, member) ((unsigned long)atomic_read(&(mm)->_##member))
-#define add_mm_counter(mm, member, value) atomic_add(value, &(mm)->_##member)
-#define inc_mm_counter(mm, member) atomic_inc(&(mm)->_##member)
-#define dec_mm_counter(mm, member) atomic_dec(&(mm)->_##member)
-typedef atomic_t mm_counter_t;
-#endif /* !ATOMIC64_INIT */
+#define set_mm_counter(mm, member, value) atomic_long_set(&(mm)->_##member, value)
+#define get_mm_counter(mm, member) ((unsigned long)atomic_long_read(&(mm)->_##member))
+#define add_mm_counter(mm, member, value) atomic_long_add(value, &(mm)->_##member)
+#define inc_mm_counter(mm, member) atomic_long_inc(&(mm)->_##member)
+#define dec_mm_counter(mm, member) atomic_long_dec(&(mm)->_##member)
+typedef atomic_long_t mm_counter_t;
 
 #else  /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
 /*

commit a9d9baa1e819b2f92f9cfa5240f766c535e636a6
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Mon Nov 28 13:43:46 2005 -0800

    [PATCH] clean up lock_cpu_hotplug() in cpufreq
    
    There are some callers in cpufreq hotplug notify path that the lowest
    function calls lock_cpu_hotplug().  The lock is already held during
    cpu_up() and cpu_down() calls when the notify calls are broadcast to
    registered clients.
    
    Ideally if possible, we could disable_preempt() at the highest caller and
    make sure we dont sleep in the path down in cpufreq->driver_target() calls
    but the calls are so intertwined and cumbersome to cleanup.
    
    Hence we consistently use lock_cpu_hotplug() and unlock_cpu_hotplug() in
    all places.
    
     - Removed export of cpucontrol semaphore and made it static.
     - removed explicit uses of up/down with lock_cpu_hotplug()
       so we can keep track of the the callers in same thread context and
       just keep refcounts without calling a down() that causes a deadlock.
     - Removed current_in_hotplug() uses
     - Removed PF_HOTPLUG_CPU in sched.h introduced for the current_in_hotplug()
       temporary workaround.
    
    Tested with insmod of cpufreq_stat.ko, and logical online/offline
    to make sure we dont have any hang situations.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Cc: Zwane Mwaikambo <zwane@linuxpower.ca>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: "Siddha, Suresh B" <suresh.b.siddha@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2038bd27b041..b0ad6f30679e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -908,7 +908,6 @@ do { if (atomic_dec_and_test(&(tsk)->usage)) __put_task_struct(tsk); } while(0)
 #define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
 #define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
 #define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
-#define PF_HOTPLUG_CPU	0x01000000	/* Currently performing CPU hotplug */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 20dcae32439384b6863c626bb3b2a09bed65b33e
Author: Zach Brown <zach.brown@oracle.com>
Date:   Sun Nov 13 16:07:33 2005 -0800

    [PATCH] aio: remove kioctx from mm_struct
    
    Sync iocbs have a life cycle that don't need a kioctx.  Their retrying, if
    any, is done in the context of their owner who has allocated them on the
    stack.
    
    The sole user of a sync iocb's ctx reference was aio_complete() checking for
    an elevated iocb ref count that could never happen.  No path which grabs an
    iocb ref has access to sync iocbs.
    
    If we were to implement sync iocb cancelation it would be done by the owner of
    the iocb using its on-stack reference.
    
    Removing this chunk from aio_complete allows us to remove the entire kioctx
    instance from mm_struct, reducing its size by a third.  On a i386 testing box
    the slab size went from 768 to 504 bytes and from 5 to 8 per page.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>
    Acked-by: Benjamin LaHaise <bcrl@kvack.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 41df81395719..2038bd27b041 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -357,7 +357,6 @@ struct mm_struct {
 	/* aio bits */
 	rwlock_t		ioctx_list_lock;
 	struct kioctx		*ioctx_list;
-	struct kioctx		default_kioctx;
 };
 
 struct sighand_struct {

commit f037360f2ed111fe89a8f5cb6ba351f4e9934e53
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:57 2005 -0800

    [PATCH] m68k: thread_info header cleanup
    
    a) in smp_lock.h #include of sched.h and spinlock.h moved under #ifdef
       CONFIG_LOCK_KERNEL.
    
    b) interrupt.h now explicitly pulls sched.h (not via smp_lock.h from
       hardirq.h as it used to)
    
    c) in three more places we need changes to compensate for (a) - one place
       in arch/sparc needs string.h now, hardirq.h needs forward declaration of
       task_struct and preempt.h needs direct include of thread_info.h.
    
    d) thread_info-related helpers in sched.h and thread_info.h put under
       ifndef __HAVE_THREAD_FUNCTIONS.  Obviously safe.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index e4681256e43e..41df81395719 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1233,6 +1233,8 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
+#ifndef __HAVE_THREAD_FUNCTIONS
+
 #define task_thread_info(task) (task)->thread_info
 
 static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
@@ -1246,6 +1248,8 @@ static inline unsigned long *end_of_stack(struct task_struct *p)
 	return (unsigned long *)(p->thread_info + 1);
 }
 
+#endif
+
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit 10ebffde3d3916026974352b7900e44afe2b243f
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:56 2005 -0800

    [PATCH] m68k: introduce setup_thread_stack() and end_of_stack()
    
    encapsulates the rest of arch-dependent operations with thread_info access.
    Two new helpers - setup_thread_stack() and end_of_stack().  For normal case
    the former consists of copying thread_info of parent to new thread_info and
    the latter returns pointer immediately past the end of thread_info.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index f8650314ba2f..e4681256e43e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1235,6 +1235,17 @@ static inline void task_unlock(struct task_struct *p)
 
 #define task_thread_info(task) (task)->thread_info
 
+static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
+{
+	*task_thread_info(p) = *task_thread_info(org);
+	task_thread_info(p)->task = p;
+}
+
+static inline unsigned long *end_of_stack(struct task_struct *p)
+{
+	return (unsigned long *)(p->thread_info + 1);
+}
+
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */

commit a1261f54611ec4ad6a7ab7080f86747e3ac3685b
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:55 2005 -0800

    [PATCH] m68k: introduce task_thread_info
    
    new helper - task_thread_info(task).  On platforms that have thread_info
    allocated separately (i.e.  in default case) it simply returns
    task->thread_info.  m68k wants (and for good reasons) to embed its thread_info
    into task_struct.  So it will (in later patch) have task_thread_info() of its
    own.  For now we just add a macro for generic case and convert existing
    instances of its body in core kernel to uses of new macro.  Obviously safe -
    all normal architectures get the same preprocessor output they used to get.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2bbf968b23d9..f8650314ba2f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1233,32 +1233,34 @@ static inline void task_unlock(struct task_struct *p)
 	spin_unlock(&p->alloc_lock);
 }
 
+#define task_thread_info(task) (task)->thread_info
+
 /* set thread flags in other task's structures
  * - see asm/thread_info.h for TIF_xxxx flags available
  */
 static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
-	set_ti_thread_flag(tsk->thread_info,flag);
+	set_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
 static inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
-	clear_ti_thread_flag(tsk->thread_info,flag);
+	clear_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
 static inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
-	return test_and_set_ti_thread_flag(tsk->thread_info,flag);
+	return test_and_set_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
 static inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
-	return test_and_clear_ti_thread_flag(tsk->thread_info,flag);
+	return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
 static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
 {
-	return test_ti_thread_flag(tsk->thread_info,flag);
+	return test_ti_thread_flag(task_thread_info(tsk), flag);
 }
 
 static inline void set_tsk_need_resched(struct task_struct *tsk)
@@ -1329,12 +1331,12 @@ extern void signal_wake_up(struct task_struct *t, int resume_stopped);
 
 static inline unsigned int task_cpu(const struct task_struct *p)
 {
-	return p->thread_info->cpu;
+	return task_thread_info(p)->cpu;
 }
 
 static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 {
-	p->thread_info->cpu = cpu;
+	task_thread_info(p)->cpu = cpu;
 }
 
 #else

commit 90d45d17f3e68608ac7ba8fc3d7acce022a19c8e
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Tue Nov 8 21:34:24 2005 -0800

    [PATCH] cpu hotplug: fix locking in cpufreq drivers
    
    When calling target drivers to set frequency, we take cpucontrol lock.
    When we modified the code to accomodate CPU hotplug, there was an attempt
    to take a double lock of cpucontrol leading to a deadlock.  Since the
    current thread context is already holding the cpucontrol lock, we dont need
    to make another attempt to acquire it.
    
    Now we leave a trace in current->flags indicating current thread already is
    under cpucontrol lock held, so we dont attempt to do this another time.
    
    Thanks to Andrew Morton for the beating:-)
    
    From: Brice Goglin <Brice.Goglin@ens-lyon.org>
    
      Build fix
    
    (akpm: this patch is still unpleasant.  Ashok continues to look for a cleaner
    solution, doesn't he?  ;))
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Brice Goglin <Brice.Goglin@ens-lyon.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 03b68a7b4b82..2bbf968b23d9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -909,6 +909,7 @@ do { if (atomic_dec_and_test(&(tsk)->usage)) __put_task_struct(tsk); } while(0)
 #define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
 #define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
 #define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
+#define PF_HOTPLUG_CPU	0x01000000	/* Currently performing CPU hotplug */
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other

commit 621d31219d9a788bda924a0613048053f3f5f211
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Oct 30 15:03:45 2005 -0800

    [PATCH] cleanup the usage of SEND_SIG_xxx constants
    
    This patch simplifies some checks for magic siginfo values.  It should not
    change the behaviour in any way.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 41285a0e7258..03b68a7b4b82 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1084,6 +1084,11 @@ extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned lon
 #define SEND_SIG_PRIV	((struct siginfo *) 1)
 #define SEND_SIG_FORCED	((struct siginfo *) 2)
 
+static inline int is_si_special(const struct siginfo *info)
+{
+	return info <= SEND_SIG_FORCED;
+}
+
 /* True if we are on the alternate signal stack.  */
 
 static inline int on_sig_stack(unsigned long sp)

commit 4098f9918e068e51fed1727f6ba80efcec372378
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Oct 30 15:03:21 2005 -0800

    [PATCH] sched: hardcode non-smp set_cpus_allowed
    
    Simplify the UP (1 CPU) implementatin of set_cpus_allowed.
    
    The one CPU is hardcoded to be cpu 0 - so just test for that bit, and avoid
    having to pick up the cpu_online_map.
    
    Also, unexport cpu_online_map: it was only needed for set_cpus_allowed().
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b2d2dc14f0b9..41285a0e7258 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -940,7 +940,7 @@ extern int set_cpus_allowed(task_t *p, cpumask_t new_mask);
 #else
 static inline int set_cpus_allowed(task_t *p, cpumask_t new_mask)
 {
-	if (!cpus_intersects(new_mask, cpu_online_map))
+	if (!cpu_isset(0, new_mask))
 		return -EINVAL;
 	return 0;
 }

commit 053199edf54f685e7dea765b60d4d5e9070dadec
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Oct 30 15:02:30 2005 -0800

    [PATCH] cpusets: dual semaphore locking overhaul
    
    Overhaul cpuset locking.  Replace single semaphore with two semaphores.
    
    The suggestion to use two locks was made by Roman Zippel.
    
    Both locks are global.  Code that wants to modify cpusets must first
    acquire the exclusive manage_sem, which allows them read-only access to
    cpusets, and holds off other would-be modifiers.  Before making actual
    changes, the second semaphore, callback_sem must be acquired as well.  Code
    that needs only to query cpusets must acquire callback_sem, which is also a
    global exclusive lock.
    
    The earlier problems with double tripping are avoided, because it is
    allowed for holders of manage_sem to nest the second callback_sem lock, and
    only callback_sem is needed by code called from within __alloc_pages(),
    where the double tripping had been possible.
    
    This is not quite the same as a normal read/write semaphore, because
    obtaining read-only access with intent to change must hold off other such
    attempts, while allowing read-only access w/o such intention.  Changing
    cpusets involves several related checks and changes, which must be done
    while allowing read-only queries (to avoid the double trip), but while
    ensuring nothing changes (holding off other would be modifiers.)
    
    This overhaul of cpuset locking also makes careful use of task_lock() to
    guard access to the task->cpuset pointer, closing a couple of race
    conditions noticed while reading this code (thanks, Roman).  I've never
    seen these races fail in any use or test.
    
    See further the comments in the code.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c30bc308ef1..b2d2dc14f0b9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1211,7 +1211,7 @@ extern void unhash_process(struct task_struct *p);
 /*
  * Protects ->fs, ->files, ->mm, ->ptrace, ->group_info, ->comm, keyring
  * subscriptions and synchronises with wait4().  Also used in procfs.  Also
- * pins the final release of task.io_context.
+ * pins the final release of task.io_context.  Also protects ->cpuset.
  *
  * Nests both inside and outside of read_lock(&tasklist_lock).
  * It must not be nested with write_lock_irq(&tasklist_lock),

commit f412ac08c9861b4791af0145934c22f1458686da
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:41 2005 -0700

    [PATCH] mm: fix rss and mmlist locking
    
    A couple of oddities were guarded by page_table_lock, no longer properly
    guarded when that is split.
    
    The mm_counters of file_rss and anon_rss: make those an atomic_t, or an
    atomic64_t if the architecture supports it, in such a case.  Definitions by
    courtesy of Christoph Lameter: who spent considerable effort on more scalable
    ways of counting, but found insufficient benefit in practice.
    
    And adding an mm with swap to the mmlist for swapoff: the list is well-
    guarded by its own lock, but the list_empty check now has to be repeated
    inside it.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 292cb57ce38f..1c30bc308ef1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -249,13 +249,47 @@ arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 extern void arch_unmap_area(struct mm_struct *, unsigned long);
 extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 
+#if NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS
+/*
+ * The mm counters are not protected by its page_table_lock,
+ * so must be incremented atomically.
+ */
+#ifdef ATOMIC64_INIT
+#define set_mm_counter(mm, member, value) atomic64_set(&(mm)->_##member, value)
+#define get_mm_counter(mm, member) ((unsigned long)atomic64_read(&(mm)->_##member))
+#define add_mm_counter(mm, member, value) atomic64_add(value, &(mm)->_##member)
+#define inc_mm_counter(mm, member) atomic64_inc(&(mm)->_##member)
+#define dec_mm_counter(mm, member) atomic64_dec(&(mm)->_##member)
+typedef atomic64_t mm_counter_t;
+#else /* !ATOMIC64_INIT */
+/*
+ * The counters wrap back to 0 at 2^32 * PAGE_SIZE,
+ * that is, at 16TB if using 4kB page size.
+ */
+#define set_mm_counter(mm, member, value) atomic_set(&(mm)->_##member, value)
+#define get_mm_counter(mm, member) ((unsigned long)atomic_read(&(mm)->_##member))
+#define add_mm_counter(mm, member, value) atomic_add(value, &(mm)->_##member)
+#define inc_mm_counter(mm, member) atomic_inc(&(mm)->_##member)
+#define dec_mm_counter(mm, member) atomic_dec(&(mm)->_##member)
+typedef atomic_t mm_counter_t;
+#endif /* !ATOMIC64_INIT */
+
+#else  /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
+/*
+ * The mm counters are protected by its page_table_lock,
+ * so can be incremented directly.
+ */
 #define set_mm_counter(mm, member, value) (mm)->_##member = (value)
 #define get_mm_counter(mm, member) ((mm)->_##member)
 #define add_mm_counter(mm, member, value) (mm)->_##member += (value)
 #define inc_mm_counter(mm, member) (mm)->_##member++
 #define dec_mm_counter(mm, member) (mm)->_##member--
-#define get_mm_rss(mm) ((mm)->_file_rss + (mm)->_anon_rss)
+typedef unsigned long mm_counter_t;
+
+#endif /* NR_CPUS < CONFIG_SPLIT_PTLOCK_CPUS */
 
+#define get_mm_rss(mm)					\
+	(get_mm_counter(mm, file_rss) + get_mm_counter(mm, anon_rss))
 #define update_hiwater_rss(mm)	do {			\
 	unsigned long _rss = get_mm_rss(mm);		\
 	if ((mm)->hiwater_rss < _rss)			\
@@ -266,8 +300,6 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 		(mm)->hiwater_vm = (mm)->total_vm;	\
 } while (0)
 
-typedef unsigned long mm_counter_t;
-
 struct mm_struct {
 	struct vm_area_struct * mmap;		/* list of VMAs */
 	struct rb_root mm_rb;
@@ -291,7 +323,9 @@ struct mm_struct {
 						 * by mmlist_lock
 						 */
 
-	/* Special counters protected by the page_table_lock */
+	/* Special counters, in some configurations protected by the
+	 * page_table_lock, in other configurations by being atomic.
+	 */
 	mm_counter_t _file_rss;
 	mm_counter_t _anon_rss;
 

commit f449952bc8bde7fbc73c6d20dff92b627a21f8b9
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:19 2005 -0700

    [PATCH] mm: mm_struct hiwaters moved
    
    Slight and timid rearrangement of mm_struct: hiwater_rss and hiwater_vm were
    tacked on the end, but it seems better to keep them near _file_rss, _anon_rss
    and total_vm, in the same cacheline on those arches verified.
    
    There are likely to be more profitable rearrangements, but less obvious (is it
    good or bad that saved_auxv[AT_VECTOR_SIZE] isolates cpu_vm_mask and context
    from many others?), needing serious instrumentation.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a9c0b7d26303..292cb57ce38f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -291,16 +291,19 @@ struct mm_struct {
 						 * by mmlist_lock
 						 */
 
-	unsigned long start_code, end_code, start_data, end_data;
-	unsigned long start_brk, brk, start_stack;
-	unsigned long arg_start, arg_end, env_start, env_end;
-	unsigned long total_vm, locked_vm, shared_vm;
-	unsigned long exec_vm, stack_vm, reserved_vm, def_flags, nr_ptes;
-
 	/* Special counters protected by the page_table_lock */
 	mm_counter_t _file_rss;
 	mm_counter_t _anon_rss;
 
+	unsigned long hiwater_rss;	/* High-watermark of RSS usage */
+	unsigned long hiwater_vm;	/* High-water virtual memory usage */
+
+	unsigned long total_vm, locked_vm, shared_vm, exec_vm;
+	unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;
+	unsigned long start_code, end_code, start_data, end_data;
+	unsigned long start_brk, brk, start_stack;
+	unsigned long arg_start, arg_end, env_start, env_end;
+
 	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */
 
 	unsigned dumpable:2;
@@ -320,11 +323,7 @@ struct mm_struct {
 	/* aio bits */
 	rwlock_t		ioctx_list_lock;
 	struct kioctx		*ioctx_list;
-
 	struct kioctx		default_kioctx;
-
-	unsigned long hiwater_rss;	/* High-water RSS usage */
-	unsigned long hiwater_vm;	/* High-water virtual memory usage */
 };
 
 struct sighand_struct {

commit 365e9c87a982c03d0af3886e29d877f581b59611
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:18 2005 -0700

    [PATCH] mm: update_hiwaters just in time
    
    update_mem_hiwater has attracted various criticisms, in particular from those
    concerned with mm scalability.  Originally it was called whenever rss or
    total_vm got raised.  Then many of those callsites were replaced by a timer
    tick call from account_system_time.  Now Frank van Maarseveen reports that to
    be found inadequate.  How about this?  Works for Frank.
    
    Replace update_mem_hiwater, a poor combination of two unrelated ops, by macros
    update_hiwater_rss and update_hiwater_vm.  Don't attempt to keep
    mm->hiwater_rss up to date at timer tick, nor every time we raise rss (usually
    by 1): those are hot paths.  Do the opposite, update only when about to lower
    rss (usually by many), or just before final accounting in do_exit.  Handle
    mm->hiwater_vm in the same way, though it's much less of an issue.  Demand
    that whoever collects these hiwater statistics do the work of taking the
    maximum with rss or total_vm.
    
    And there has been no collector of these hiwater statistics in the tree.  The
    new convention needs an example, so match Frank's usage by adding a VmPeak
    line above VmSize to /proc/<pid>/status, and also a VmHWM line above VmRSS
    (High-Water-Mark or High-Water-Memory).
    
    There was a particular anomaly during mremap move, that hiwater_vm might be
    captured too high.  A fleeting such anomaly remains, but it's quickly
    corrected now, whereas before it would stick.
    
    What locking?  None: if the app is racy then these statistics will be racy,
    it's not worth any overhead to make them exact.  But whenever it suits,
    hiwater_vm is updated under exclusive mmap_sem, and hiwater_rss under
    page_table_lock (for now) or with preemption disabled (later on): without
    going to any trouble, minimize the time between reading current values and
    updating, to minimize those occasions when a racing thread bumps a count up
    and back down in between.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index afcaac66cbd5..a9c0b7d26303 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -256,6 +256,16 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #define dec_mm_counter(mm, member) (mm)->_##member--
 #define get_mm_rss(mm) ((mm)->_file_rss + (mm)->_anon_rss)
 
+#define update_hiwater_rss(mm)	do {			\
+	unsigned long _rss = get_mm_rss(mm);		\
+	if ((mm)->hiwater_rss < _rss)			\
+		(mm)->hiwater_rss = _rss;		\
+} while (0)
+#define update_hiwater_vm(mm)	do {			\
+	if ((mm)->hiwater_vm < (mm)->total_vm)		\
+		(mm)->hiwater_vm = (mm)->total_vm;	\
+} while (0)
+
 typedef unsigned long mm_counter_t;
 
 struct mm_struct {

commit 4294621f41a85497019fae64341aa5351a1921b7
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:05 2005 -0700

    [PATCH] mm: rss = file_rss + anon_rss
    
    I was lazy when we added anon_rss, and chose to change as few places as
    possible.  So currently each anonymous page has to be counted twice, in rss
    and in anon_rss.  Which won't be so good if those are atomic counts in some
    configurations.
    
    Change that around: keep file_rss and anon_rss separately, and add them
    together (with get_mm_rss macro) when the total is needed - reading two
    atomics is much cheaper than updating two atomics.  And update anon_rss
    upfront, typically in memory.c, not tucked away in page_add_anon_rmap.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 27519df0f987..afcaac66cbd5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -254,6 +254,8 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 #define add_mm_counter(mm, member, value) (mm)->_##member += (value)
 #define inc_mm_counter(mm, member) (mm)->_##member++
 #define dec_mm_counter(mm, member) (mm)->_##member--
+#define get_mm_rss(mm) ((mm)->_file_rss + (mm)->_anon_rss)
+
 typedef unsigned long mm_counter_t;
 
 struct mm_struct {
@@ -286,7 +288,7 @@ struct mm_struct {
 	unsigned long exec_vm, stack_vm, reserved_vm, def_flags, nr_ptes;
 
 	/* Special counters protected by the page_table_lock */
-	mm_counter_t _rss;
+	mm_counter_t _file_rss;
 	mm_counter_t _anon_rss;
 
 	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

commit 46113830a18847cff8da73005e57bc49c2f95a56
Author: Harald Welte <laforge@gnumonks.org>
Date:   Mon Oct 10 19:44:29 2005 +0200

    [PATCH] Fix signal sending in usbdevio on async URB completion
    
    If a process issues an URB from userspace and (starts to) terminate
    before the URB comes back, we run into the issue described above.  This
    is because the urb saves a pointer to "current" when it is posted to the
    device, but there's no guarantee that this pointer is still valid
    afterwards.
    
    In fact, there are three separate issues:
    
    1) the pointer to "current" can become invalid, since the task could be
       completely gone when the URB completion comes back from the device.
    
    2) Even if the saved task pointer is still pointing to a valid task_struct,
       task_struct->sighand could have gone meanwhile.
    
    3) Even if the process is perfectly fine, permissions may have changed,
       and we can no longer send it a signal.
    
    So what we do instead, is to save the PID and uid's of the process, and
    introduce a new kill_proc_info_as_uid() function.
    
    Signed-off-by: Harald Welte <laforge@gnumonks.org>
    [ Fixed up types and added symbol exports ]
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c3ba31f210a9..27519df0f987 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1018,6 +1018,7 @@ extern int force_sig_info(int, struct siginfo *, struct task_struct *);
 extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
 extern int kill_pg_info(int, struct siginfo *, pid_t);
 extern int kill_proc_info(int, struct siginfo *, pid_t);
+extern int kill_proc_info_as_uid(int, struct siginfo *, pid_t, uid_t, uid_t);
 extern void do_notify_parent(struct task_struct *, int);
 extern void force_sig(int, struct task_struct *);
 extern void force_sig_specific(int, struct task_struct *);

commit 4a8342d233a39ee582e9f7260e12d2f5fd194a05
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Sep 29 15:18:21 2005 -0700

    Revert task flag re-ordering, add comments
    
    Roland points out that the flags end up having non-obvious dependencies
    elsewhere, so revert aa55a08687059aa169d10a313c41f238c2070488 and add
    some comments about why things are as they are.
    
    We'll just have to fix up the broken comparisons. Roland has a patch.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index afe6c61f13e5..c3ba31f210a9 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -107,14 +107,26 @@ extern unsigned long nr_iowait(void);
 
 #include <asm/processor.h>
 
+/*
+ * Task state bitmask. NOTE! These bits are also
+ * encoded in fs/proc/array.c: get_task_state().
+ *
+ * We have two separate sets of flags: task->state
+ * is about runnability, while task->exit_state are
+ * about the task exiting. Confusing, but this way
+ * modifying one set can't modify the other one by
+ * mistake.
+ */
 #define TASK_RUNNING		0
 #define TASK_INTERRUPTIBLE	1
 #define TASK_UNINTERRUPTIBLE	2
-#define TASK_NONINTERACTIVE	4
-#define TASK_STOPPED		8
-#define TASK_TRACED		16
-#define EXIT_ZOMBIE		32
-#define EXIT_DEAD		64
+#define TASK_STOPPED		4
+#define TASK_TRACED		8
+/* in tsk->exit_state */
+#define EXIT_ZOMBIE		16
+#define EXIT_DEAD		32
+/* in tsk->state again */
+#define TASK_NONINTERACTIVE	64
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit aa55a08687059aa169d10a313c41f238c2070488
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Sep 29 19:58:53 2005 +0400

    [PATCH] fix TASK_STOPPED vs TASK_NONINTERACTIVE interaction
    
    do_signal_stop:
    
            for_each_thread(t) {
                    if (t->state < TASK_STOPPED)
                            ++sig->group_stop_count;
            }
    
    However, TASK_NONINTERACTIVE > TASK_STOPPED, so this loop will not
    count TASK_INTERRUPTIBLE | TASK_NONINTERACTIVE threads.
    
    See also wait_task_stopped(), which checks ->state > TASK_STOPPED.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    
    [ We really probably should always use the appropriate bitmasks to test
      task states, not do it like this. Using something like
    
            #define TASK_RUNNABLE (TASK_RUNNING | TASK_INTERRUPTIBLE | \
                                    TASK_UNINTERRUPTIBLE | TASK_NONINTERACTIVE)
    
      and then doing "if (task->state & TASK_RUNNABLE)" or similar. But the
      ordering of the task states is historical, and keeping the ordering
      does make sense regardless. ]
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 49e617fa0f66..afe6c61f13e5 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -110,11 +110,11 @@ extern unsigned long nr_iowait(void);
 #define TASK_RUNNING		0
 #define TASK_INTERRUPTIBLE	1
 #define TASK_UNINTERRUPTIBLE	2
-#define TASK_STOPPED		4
-#define TASK_TRACED		8
-#define EXIT_ZOMBIE		16
-#define EXIT_DEAD		32
-#define TASK_NONINTERACTIVE	64
+#define TASK_NONINTERACTIVE	4
+#define TASK_STOPPED		8
+#define TASK_TRACED		16
+#define EXIT_ZOMBIE		32
+#define EXIT_DEAD		64
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit 498d0c5711094b0e1fd93f5355d270ccebdec706
Author: Andrew Morton <akpm@osdl.org>
Date:   Tue Sep 13 01:25:14 2005 -0700

    [PATCH] set_current_state() commentary
    
    Explain the mysteries of set_current_state().
    
    Quoth Linus:
    
     The scheduler itself never needs the memory barrier at all.
    
     The barrier is needed only if the user itself ends up testing some other
     thing afterwards, ie if you have
    
            set_process_state(TASK_INTERRUPTIBLE);
            if (still_need_to_sleep())
                    schedule();
    
     then the "still_need_to_sleep()" thing may test flags and wakeup events,
     and then you _may_ want to (and often do) make sure that the write of
     TASK_INTERRUPTIBLE is serialized wrt the reads of any wakeup data (since
     the wakeup may have happened on another CPU).
    
     So the comment is somewhat wrong. We don't really _care_ whether the state
     propagates out to other CPU's since all of our actions are purely local,
     and there is nothing we do that is conditional on any other CPU: we're
     going to sleep unconditionally, and the scheduler only cares about _our_
     state, not about somebody elses state.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 38c8654aaa96..49e617fa0f66 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -121,6 +121,17 @@ extern unsigned long nr_iowait(void);
 #define set_task_state(tsk, state_value)		\
 	set_mb((tsk)->state, (state_value))
 
+/*
+ * set_current_state() includes a barrier so that the write of current->state
+ * is correctly serialised wrt the caller's subsequent test of whether to
+ * actually sleep:
+ *
+ *	set_current_state(TASK_UNINTERRUPTIBLE);
+ *	if (do_i_need_to_sleep())
+ *		schedule();
+ *
+ * If the caller does not need such serialisation then use __set_current_state()
+ */
 #define __set_current_state(state_value)			\
 	do { current->state = (state_value); } while (0)
 #define set_current_state(state_value)		\

commit b3426599af9524104be6938bcb1fcaab314781c7
Author: Paul Jackson <pj@sgi.com>
Date:   Mon Sep 12 04:30:30 2005 -0700

    [PATCH] cpuset semaphore depth check optimize
    
    Optimize the deadlock avoidance check on the global cpuset
    semaphore cpuset_sem.  Instead of adding a depth counter to the
    task struct of each task, rather just two words are enough, one
    to store the depth and the other the current cpuset_sem holder.
    
    Thanks to Nikita Danilov for the idea.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    
    [ We may want to change this further, but at least it's now
      a totally internal decision to the cpusets code ]
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ed3bb19d1337..38c8654aaa96 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -785,7 +785,6 @@ struct task_struct {
 	short il_next;
 #endif
 #ifdef CONFIG_CPUSETS
-	short cpuset_sem_nest_depth;
 	struct cpuset *cpuset;
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;

commit a2a979821b6ab75a4f143cfaa1c4672cc259ec10
Author: Keith Owens <kaos@sgi.com>
Date:   Sun Sep 11 17:19:06 2005 +1000

    [PATCH] MCA/INIT: scheduler hooks
    
    Scheduler hooks to see/change which process is deemed to be on a cpu.
    
    Signed-off-by: Keith Owens <kaos@sgi.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4b83cb230006..ed3bb19d1337 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -904,6 +904,8 @@ extern int task_curr(const task_t *p);
 extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
 extern task_t *idle_task(int cpu);
+extern task_t *curr_task(int cpu);
+extern void set_curr_task(int cpu, task_t *p);
 
 void yield(void);
 

commit 64ed93a268bc18fa6f72f61420d0e0022c5e38d1
Author: Nishanth Aravamudan <nacc@us.ibm.com>
Date:   Sat Sep 10 00:27:21 2005 -0700

    [PATCH] add schedule_timeout_{,un}interruptible() interfaces
    
    Add schedule_timeout_{,un}interruptible() interfaces so that
    schedule_timeout() callers don't have to worry about forgetting to add the
    set_current_state() call beforehand.
    
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ac70f845b5b1..4b83cb230006 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -203,6 +203,8 @@ extern int in_sched_functions(unsigned long addr);
 
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long FASTCALL(schedule_timeout(signed long timeout));
+extern signed long schedule_timeout_interruptible(signed long timeout);
+extern signed long schedule_timeout_uninterruptible(signed long timeout);
 asmlinkage void schedule(void);
 
 struct namespace;

commit d79fc0fc6645b0cf5cd980da76942ca6d6300fa4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 10 00:26:12 2005 -0700

    [PATCH] sched: TASK_NONINTERACTIVE
    
    This patch implements a task state bit (TASK_NONINTERACTIVE), which can be
    used by blocking points to mark the task's wait as "non-interactive".  This
    does not mean the task will be considered a CPU-hog - the wait will simply
    not have an effect on the waiting task's priority - positive or negative
    alike.  Right now only pipe_wait() will make use of it, because it's a
    common source of not-so-interactive waits (kernel compilation jobs, etc.).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8a1fcfe80fc7..ac70f845b5b1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -114,6 +114,7 @@ extern unsigned long nr_iowait(void);
 #define TASK_TRACED		8
 #define EXIT_ZOMBIE		16
 #define EXIT_DEAD		32
+#define TASK_NONINTERACTIVE	64
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)

commit 4247bdc60048018b98f71228b45cfbc5f5270c86
Author: Paul Jackson <pj@sgi.com>
Date:   Sat Sep 10 00:26:06 2005 -0700

    [PATCH] cpuset semaphore depth check deadlock fix
    
    The cpusets-formalize-intermediate-gfp_kernel-containment patch
    has a deadlock problem.
    
    This patch was part of a set of four patches to make more
    extensive use of the cpuset 'mem_exclusive' attribute to
    manage kernel GFP_KERNEL memory allocations and to constrain
    the out-of-memory (oom) killer.
    
    A task that is changing cpusets in particular ways on a system
    when it is very short of free memory could double trip over
    the global cpuset_sem semaphore (get the lock and then deadlock
    trying to get it again).
    
    The second attempt to get cpuset_sem would be in the routine
    cpuset_zone_allowed().  This was discovered by code inspection.
    I can not reproduce the problem except with an artifically
    hacked kernel and a specialized stress test.
    
    In real life you cannot hit this unless you are manipulating
    cpusets, and are very unlikely to hit it unless you are rapidly
    modifying cpusets on a memory tight system.  Even then it would
    be a rare occurence.
    
    If you did hit it, the task double tripping over cpuset_sem
    would deadlock in the kernel, and any other task also trying
    to manipulate cpusets would deadlock there too, on cpuset_sem.
    Your batch manager would be wedged solid (if it was cpuset
    savvy), but classic Unix shells and utilities would work well
    enough to reboot the system.
    
    The unusual condition that led to this bug is that unlike most
    semaphores, cpuset_sem _can_ be acquired while in the page
    allocation code, when __alloc_pages() calls cpuset_zone_allowed.
    So it easy to mistakenly perform the following sequence:
      1) task makes system call to alter a cpuset
      2) take cpuset_sem
      3) try to allocate memory
      4) memory allocator, via cpuset_zone_allowed, trys to take cpuset_sem
      5) deadlock
    
    The reason that this is not a serious bug for most users
    is that almost all calls to allocate memory don't require
    taking cpuset_sem.  Only some code paths off the beaten
    track require taking cpuset_sem -- which is good.  Taking
    a global semaphore on the main code path for allocating
    memory would not scale well.
    
    This patch fixes this deadlock by wrapping the up() and down()
    calls on cpuset_sem in kernel/cpuset.c with code that tracks
    the nesting depth of the current task on that semaphore, and
    only does the real down() if the task doesn't hold the lock
    already, and only does the real up() if the nesting depth
    (number of unmatched downs) is exactly one.
    
    The previous required use of refresh_mems(), anytime that
    the cpuset_sem semaphore was acquired and the code executed
    while holding that semaphore might try to allocate memory, is
    no longer required.  Two refresh_mems() calls were removed
    thanks to this.  This is a good change, as failing to get
    all the necessary refresh_mems() calls placed was a primary
    source of bugs in this cpuset code.  The only remaining call
    to refresh_mems() is made while doing a memory allocation,
    if certain task memory placement data needs to be updated
    from its cpuset, due to the cpuset having been changed behind
    the tasks back.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c551e6a1447e..8a1fcfe80fc7 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -782,6 +782,7 @@ struct task_struct {
 	short il_next;
 #endif
 #ifdef CONFIG_CPUSETS
+	short cpuset_sem_nest_depth;
 	struct cpuset *cpuset;
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;

commit 383f2835eb9afb723af71850037b2f074ac9db60
Author: Chen, Kenneth W <kenneth.w.chen@intel.com>
Date:   Fri Sep 9 13:02:02 2005 -0700

    [PATCH] Prefetch kernel stacks to speed up context switch
    
    For architecture like ia64, the switch stack structure is fairly large
    (currently 528 bytes).  For context switch intensive application, we found
    that significant amount of cache misses occurs in switch_to() function.
    The following patch adds a hook in the schedule() function to prefetch
    switch stack structure as soon as 'next' task is determined.  This allows
    maximum overlap in prefetch cache lines for that structure.
    
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ea1b5f32ec5c..c551e6a1447e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -604,6 +604,11 @@ extern int groups_search(struct group_info *group_info, gid_t grp);
 #define GROUP_AT(gi, i) \
     ((gi)->blocks[(i)/NGROUPS_PER_BLOCK][(i)%NGROUPS_PER_BLOCK])
 
+#ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
+extern void prefetch_stack(struct task_struct*);
+#else
+static inline void prefetch_stack(struct task_struct *t) { }
+#endif
 
 struct audit_context;		/* See audit.c */
 struct mempolicy;

commit 9c1cfda20a508b181bdda8c0045f7c0c333880a5
Author: John Hawkes <hawkes@sgi.com>
Date:   Tue Sep 6 15:18:14 2005 -0700

    [PATCH] cpusets: Move the ia64 domain setup code to the generic code
    
    Signed-off-by: John Hawkes <hawkes@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b5a22ea80045..ea1b5f32ec5c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -564,13 +564,6 @@ struct sched_domain {
 
 extern void partition_sched_domains(cpumask_t *partition1,
 				    cpumask_t *partition2);
-#ifdef ARCH_HAS_SCHED_DOMAIN
-/* Useful helpers that arch setup code may use. Defined in kernel/sched.c */
-extern cpumask_t cpu_isolated_map;
-extern void init_sched_build_groups(struct sched_group groups[],
-	                        cpumask_t span, int (*group_fn)(int cpu));
-extern void cpu_attach_domain(struct sched_domain *sd, int cpu);
-#endif /* ARCH_HAS_SCHED_DOMAIN */
 #endif /* CONFIG_SMP */
 
 

commit 36d57ac4a818cb4aa3edbdf63ad2ebc31106f925
Author: H. J. Lu <hjl@lucon.org>
Date:   Tue Sep 6 15:16:49 2005 -0700

    [PATCH] auxiliary vector cleanups
    
    The size of auxiliary vector is fixed at 42 in linux/sched.h.  But it isn't
    very obvious when looking at linux/elf.h.  This patch adds AT_VECTOR_SIZE
    so that we can change it if necessary when a new vector is added.
    
    Because of include file ordering problems, doing this necessitated the
    extraction of the AT_* symbols into a standalone header file.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5fb31bede103..b5a22ea80045 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -35,6 +35,8 @@
 #include <linux/topology.h>
 #include <linux/seccomp.h>
 
+#include <linux/auxvec.h>	/* For AT_VECTOR_SIZE */
+
 struct exec_domain;
 
 /*
@@ -261,7 +263,7 @@ struct mm_struct {
 	mm_counter_t _rss;
 	mm_counter_t _anon_rss;
 
-	unsigned long saved_auxv[42]; /* for /proc/PID/auxv */
+	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */
 
 	unsigned dumpable:2;
 	cpumask_t cpu_vm_mask;

commit 8446f1d391f3d27e6bf9c43d4cbcdac0ca720417
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 6 15:16:27 2005 -0700

    [PATCH] detect soft lockups
    
    This patch adds a new kernel debug feature: CONFIG_DETECT_SOFTLOCKUP.
    
    When enabled then per-CPU watchdog threads are started, which try to run
    once per second.  If they get delayed for more than 10 seconds then a
    callback from the timer interrupt detects this condition and prints out a
    warning message and a stack dump (once per lockup incident).  The feature
    is otherwise non-intrusive, it doesnt try to unlock the box in any way, it
    only gets the debug info out, automatically, and on all CPUs affected by
    the lockup.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-Off-By: Matthias Urlichs <smurf@smurf.noris.de>
    Signed-off-by: Richard Purdie <rpurdie@rpsys.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dec5827c7742..5fb31bede103 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -176,6 +176,23 @@ extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
 
+#ifdef CONFIG_DETECT_SOFTLOCKUP
+extern void softlockup_tick(struct pt_regs *regs);
+extern void spawn_softlockup_task(void);
+extern void touch_softlockup_watchdog(void);
+#else
+static inline void softlockup_tick(struct pt_regs *regs)
+{
+}
+static inline void spawn_softlockup_task(void)
+{
+}
+static inline void touch_softlockup_watchdog(void)
+{
+}
+#endif
+
+
 /* Attach to any functions which should be ignored in wchan output. */
 #define __sched		__attribute__((__section__(".sched.text")))
 /* Is this address in the __sched functions? */

commit 0eeca28300df110bd6ed54b31193c83b87921443
Author: Robert Love <rml@novell.com>
Date:   Tue Jul 12 17:06:03 2005 -0400

    [PATCH] inotify
    
    inotify is intended to correct the deficiencies of dnotify, particularly
    its inability to scale and its terrible user interface:
    
            * dnotify requires the opening of one fd per each directory
              that you intend to watch. This quickly results in too many
              open files and pins removable media, preventing unmount.
            * dnotify is directory-based. You only learn about changes to
              directories. Sure, a change to a file in a directory affects
              the directory, but you are then forced to keep a cache of
              stat structures.
            * dnotify's interface to user-space is awful.  Signals?
    
    inotify provides a more usable, simple, powerful solution to file change
    notification:
    
            * inotify's interface is a system call that returns a fd, not SIGIO.
              You get a single fd, which is select()-able.
            * inotify has an event that says "the filesystem that the item
              you were watching is on was unmounted."
            * inotify can watch directories or files.
    
    Inotify is currently used by Beagle (a desktop search infrastructure),
    Gamin (a FAM replacement), and other projects.
    
    See Documentation/filesystems/inotify.txt.
    
    Signed-off-by: Robert Love <rml@novell.com>
    Cc: John McCutchan <ttb@tentacle.dhs.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff48815bd3a2..dec5827c7742 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -410,6 +410,10 @@ struct user_struct {
 	atomic_t processes;	/* How many processes does this user have? */
 	atomic_t files;		/* How many open files does this user have? */
 	atomic_t sigpending;	/* How many pending signals does this user have? */
+#ifdef CONFIG_INOTIFY
+	atomic_t inotify_watches; /* How many inotify watches does this user have? */
+	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
+#endif
 	/* protected by mq_lock	*/
 	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
 	unsigned long locked_shm; /* How many pages of mlocked shm ? */

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9530b1903160..ff48815bd3a2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -608,6 +608,8 @@ struct task_struct {
 	struct list_head run_list;
 	prio_array_t *array;
 
+	unsigned short ioprio;
+
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;
 	unsigned long long sched_time; /* sched_clock time spent running */
@@ -763,6 +765,7 @@ struct task_struct {
 	nodemask_t mems_allowed;
 	int cpuset_mems_generation;
 #endif
+	atomic_t fs_excl;	/* holding fs exclusive resources */
 };
 
 static inline pid_t process_group(struct task_struct *tsk)
@@ -1112,7 +1115,8 @@ extern void unhash_process(struct task_struct *p);
 
 /*
  * Protects ->fs, ->files, ->mm, ->ptrace, ->group_info, ->comm, keyring
- * subscriptions and synchronises with wait4().  Also used in procfs.
+ * subscriptions and synchronises with wait4().  Also used in procfs.  Also
+ * pins the final release of task.io_context.
  *
  * Nests both inside and outside of read_lock(&tasklist_lock).
  * It must not be nested with write_lock_irq(&tasklist_lock),

commit 2031d0f586839bc68f35bcf8580b18947f8491d4
Merge: 98e7f29418a4 3e1d1d28d99d
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Jun 25 17:16:53 2005 -0700

    Merge Christoph's freeze cleanup patch

commit 3e1d1d28d99dabe63c64f7f40f1ca1d646de1f73
Author: Christoph Lameter <christoph@lameter.com>
Date:   Fri Jun 24 23:13:50 2005 -0700

    [PATCH] Cleanup patch for process freezing
    
    1. Establish a simple API for process freezing defined in linux/include/sched.h:
    
       frozen(process)              Check for frozen process
       freezing(process)            Check if a process is being frozen
       freeze(process)              Tell a process to freeze (go to refrigerator)
       thaw_process(process)        Restart process
       frozen_process(process)      Process is frozen now
    
    2. Remove all references to PF_FREEZE and PF_FROZEN from all
       kernel sources except sched.h
    
    3. Fix numerous locations where try_to_freeze is manually done by a driver
    
    4. Remove the argument that is no longer necessary from two function calls.
    
    5. Some whitespace cleanup
    
    6. Clear potential race in refrigerator (provides an open window of PF_FREEZE
       cleared before setting PF_FROZEN, recalc_sigpending does not check
       PF_FROZEN).
    
    This patch does not address the problem of freeze_processes() violating the rule
    that a task may only modify its own flags by setting PF_FREEZE. This is not clean
    in an SMP environment. freeze(process) is therefore not SMP safe!
    
    Signed-off-by: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c69682b0444..e7fd09b0557f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1245,33 +1245,78 @@ extern void normalize_rt_tasks(void);
 
 #endif
 
-/* try_to_freeze
- *
- * Checks whether we need to enter the refrigerator
- * and returns 1 if we did so.
- */
 #ifdef CONFIG_PM
-extern void refrigerator(unsigned long);
+/*
+ * Check if a process has been frozen
+ */
+static inline int frozen(struct task_struct *p)
+{
+	return p->flags & PF_FROZEN;
+}
+
+/*
+ * Check if there is a request to freeze a process
+ */
+static inline int freezing(struct task_struct *p)
+{
+	return p->flags & PF_FREEZE;
+}
+
+/*
+ * Request that a process be frozen
+ * FIXME: SMP problem. We may not modify other process' flags!
+ */
+static inline void freeze(struct task_struct *p)
+{
+	p->flags |= PF_FREEZE;
+}
+
+/*
+ * Wake up a frozen process
+ */
+static inline int thaw_process(struct task_struct *p)
+{
+	if (frozen(p)) {
+		p->flags &= ~PF_FROZEN;
+		wake_up_process(p);
+		return 1;
+	}
+	return 0;
+}
+
+/*
+ * freezing is complete, mark process as frozen
+ */
+static inline void frozen_process(struct task_struct *p)
+{
+	p->flags = (p->flags & ~PF_FREEZE) | PF_FROZEN;
+}
+
+extern void refrigerator(void);
 extern int freeze_processes(void);
 extern void thaw_processes(void);
 
-static inline int try_to_freeze(unsigned long refrigerator_flags)
+static inline int try_to_freeze(void)
 {
-	if (unlikely(current->flags & PF_FREEZE)) {
-		refrigerator(refrigerator_flags);
+	if (freezing(current)) {
+		refrigerator();
 		return 1;
 	} else
 		return 0;
 }
 #else
-static inline void refrigerator(unsigned long flag) {}
+static inline int frozen(struct task_struct *p) { return 0; }
+static inline int freezing(struct task_struct *p) { return 0; }
+static inline void freeze(struct task_struct *p) { BUG(); }
+static inline int thaw_process(struct task_struct *p) { return 1; }
+static inline void frozen_process(struct task_struct *p) { BUG(); }
+
+static inline void refrigerator(void) {}
 static inline int freeze_processes(void) { BUG(); return 0; }
 static inline void thaw_processes(void) {}
 
-static inline int try_to_freeze(unsigned long refrigerator_flags)
-{
-	return 0;
-}
+static inline int try_to_freeze(void) { return 0; }
+
 #endif /* CONFIG_PM */
 #endif /* __KERNEL__ */
 

commit 1a20ff27ef75d866730ee796acd811a925af762f
Author: Dinakar Guniguntala <dino@in.ibm.com>
Date:   Sat Jun 25 14:57:33 2005 -0700

    [PATCH] Dynamic sched domains: sched changes
    
    The following patches add dynamic sched domains functionality that was
    extensively discussed on lkml and lse-tech.  I would like to see this added to
    -mm
    
    o The main advantage with this feature is that it ensures that the scheduler
      load balacing code only balances against the cpus that are in the sched
      domain as defined by an exclusive cpuset and not all of the cpus in the
      system. This removes any overhead due to load balancing code trying to
      pull tasks outside of the cpu exclusive cpuset only to be prevented by
      the tasks' cpus_allowed mask.
    o cpu exclusive cpusets are useful for servers running orthogonal
      workloads such as RT applications requiring low latency and HPC
      applications that are throughput sensitive
    
    o It provides a new API partition_sched_domains in sched.c
      that makes dynamic sched domains possible.
    o cpu_exclusive cpusets sets are now associated with a sched domain.
      Which means that the users can dynamically modify the sched domains
      through the cpuset file system interface
    o ia64 sched domain code has been updated to support this feature as well
    o Currently, this does not support hotplug. (However some of my tests
      indicate hotplug+preempt is currently broken)
    o I have tested it extensively on x86.
    o This should have very minimal impact on performance as none of
      the fast paths are affected
    
    Signed-off-by: Dinakar Guniguntala <dino@in.ibm.com>
    Acked-by: Paul Jackson <pj@sgi.com>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Matthew Dobson <colpatch@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index edb2c69a8873..98c109e4f43d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -539,6 +539,8 @@ struct sched_domain {
 #endif
 };
 
+extern void partition_sched_domains(cpumask_t *partition1,
+				    cpumask_t *partition2);
 #ifdef ARCH_HAS_SCHED_DOMAIN
 /* Useful helpers that arch setup code may use. Defined in kernel/sched.c */
 extern cpumask_t cpu_isolated_map;

commit 476d139c218e44e045e4bc6d4cc02b010b343939
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:29 2005 -0700

    [PATCH] sched: consolidate sbe sbf
    
    Consolidate balance-on-exec with balance-on-fork.  This is made easy by the
    sched-domains RCU patches.
    
    As well as the general goodness of code reduction, this allows the runqueues
    to be unlocked during balance-on-fork.
    
    schedstats is a problem.  Maybe just have balance-on-event instead of
    distinguishing fork and exec?
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index d27be9337425..edb2c69a8873 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -930,7 +930,7 @@ extern void FASTCALL(wake_up_new_task(struct task_struct * tsk,
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void FASTCALL(sched_fork(task_t * p));
+extern void FASTCALL(sched_fork(task_t * p, int clone_flags));
 extern void FASTCALL(sched_exit(task_t * p));
 
 extern int in_group_p(gid_t);

commit 4866cde064afbb6c2a488c265e696879de616daa
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:23 2005 -0700

    [PATCH] sched: cleanup context switch locking
    
    Instead of requiring architecture code to interact with the scheduler's
    locking implementation, provide a couple of defines that can be used by the
    architecture to request runqueue unlocked context switches, and ask for
    interrupts to be enabled over the context switch.
    
    Also replaces the "switch_lock" used by these architectures with an oncpu
    flag (note, not a potentially slow bitflag).  This eliminates one bus
    locked memory operation when context switching, and simplifies the
    task_running function.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 36a10781c3f3..d27be9337425 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -368,6 +368,11 @@ struct signal_struct {
 #endif
 };
 
+/* Context switch must be unlocked if interrupts are to be enabled */
+#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+# define __ARCH_WANT_UNLOCKED_CTXSW
+#endif
+
 /*
  * Bits in flags field of signal_struct.
  */
@@ -594,6 +599,9 @@ struct task_struct {
 
 	int lock_depth;		/* BKL lock depth */
 
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+	int oncpu;
+#endif
 	int prio, static_prio;
 	struct list_head run_list;
 	prio_array_t *array;
@@ -716,8 +724,6 @@ struct task_struct {
 	spinlock_t alloc_lock;
 /* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
 	spinlock_t proc_lock;
-/* context-switch lock */
-	spinlock_t switch_lock;
 
 /* journalling filesystem info */
 	void *journal_info;

commit 68767a0ae428801649d510d9a65bb71feed44dd1
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:20 2005 -0700

    [PATCH] sched: schedstats update for balance on fork
    
    Add SCHEDSTAT statistics for sched-balance-fork.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 613491d3a875..36a10781c3f3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -517,10 +517,16 @@ struct sched_domain {
 	unsigned long alb_failed;
 	unsigned long alb_pushed;
 
-	/* sched_balance_exec() stats */
-	unsigned long sbe_attempts;
+	/* SD_BALANCE_EXEC stats */
+	unsigned long sbe_cnt;
+	unsigned long sbe_balanced;
 	unsigned long sbe_pushed;
 
+	/* SD_BALANCE_FORK stats */
+	unsigned long sbf_cnt;
+	unsigned long sbf_balanced;
+	unsigned long sbf_pushed;
+
 	/* try_to_wake_up() stats */
 	unsigned long ttwu_wake_remote;
 	unsigned long ttwu_move_affine;

commit 147cbb4bbe991452698f0772d8292f22825710ba
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:19 2005 -0700

    [PATCH] sched: balance on fork
    
    Reimplement the balance on exec balancing to be sched-domains aware.  Use this
    to also do balance on fork balancing.  Make x86_64 do balance on fork over the
    NUMA domain.
    
    The problem that the non sched domains aware blancing became apparent on dual
    core, multi socket opterons.  What we want is for the new tasks to be sent to
    a different socket, but more often than not, we would first load up our
    sibling core, or fill two cores of a single remote socket before selecting a
    new one.
    
    This gives large improvements to STREAM on such systems.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 664981ac1fb6..613491d3a875 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -460,10 +460,11 @@ enum idle_type
 #define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */
 #define SD_BALANCE_NEWIDLE	2	/* Balance when about to become idle */
 #define SD_BALANCE_EXEC		4	/* Balance on exec */
-#define SD_WAKE_IDLE		8	/* Wake to idle CPU on task wakeup */
-#define SD_WAKE_AFFINE		16	/* Wake task to waking CPU */
-#define SD_WAKE_BALANCE		32	/* Perform balancing at task wakeup */
-#define SD_SHARE_CPUPOWER	64	/* Domain members share cpu power */
+#define SD_BALANCE_FORK		8	/* Balance on fork, clone */
+#define SD_WAKE_IDLE		16	/* Wake to idle CPU on task wakeup */
+#define SD_WAKE_AFFINE		32	/* Wake task to waking CPU */
+#define SD_WAKE_BALANCE		64	/* Perform balancing at task wakeup */
+#define SD_SHARE_CPUPOWER	128	/* Domain members share cpu power */
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
@@ -492,6 +493,7 @@ struct sched_domain {
 	unsigned int idle_idx;
 	unsigned int newidle_idx;
 	unsigned int wake_idx;
+	unsigned int forkexec_idx;
 	int flags;			/* See SD_* */
 
 	/* Runtime fields. */

commit 7897986bad8f6cd50d6149345aca7f6480f49464
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:13 2005 -0700

    [PATCH] sched: balance timers
    
    Do CPU load averaging over a number of different intervals.  Allow each
    interval to be chosen by sending a parameter to source_load and target_load.
    0 is instantaneous, idx > 0 returns a decaying average with the most recent
    sample weighted at 2^(idx-1).  To a maximum of 3 (could be easily increased).
    
    So generally a higher number will result in more conservative balancing.
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 2c69682b0444..664981ac1fb6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -488,6 +488,10 @@ struct sched_domain {
 	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
 	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
+	unsigned int busy_idx;
+	unsigned int idle_idx;
+	unsigned int newidle_idx;
+	unsigned int wake_idx;
 	int flags;			/* See SD_* */
 
 	/* Runtime fields. */

commit 3e30148c3d524a9c1c63ca28261bc24c457eb07a
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 23 22:00:56 2005 -0700

    [PATCH] Keys: Make request-key create an authorisation key
    
    The attached patch makes the following changes:
    
     (1) There's a new special key type called ".request_key_auth".
    
         This is an authorisation key for when one process requests a key and
         another process is started to construct it. This type of key cannot be
         created by the user; nor can it be requested by kernel services.
    
         Authorisation keys hold two references:
    
         (a) Each refers to a key being constructed. When the key being
             constructed is instantiated the authorisation key is revoked,
             rendering it of no further use.
    
         (b) The "authorising process". This is either:
    
             (i) the process that called request_key(), or:
    
             (ii) if the process that called request_key() itself had an
                  authorisation key in its session keyring, then the authorising
                  process referred to by that authorisation key will also be
                  referred to by the new authorisation key.
    
             This means that the process that initiated a chain of key requests
             will authorise the lot of them, and will, by default, wind up with
             the keys obtained from them in its keyrings.
    
     (2) request_key() creates an authorisation key which is then passed to
         /sbin/request-key in as part of a new session keyring.
    
     (3) When request_key() is searching for a key to hand back to the caller, if
         it comes across an authorisation key in the session keyring of the
         calling process, it will also search the keyrings of the process
         specified therein and it will use the specified process's credentials
         (fsuid, fsgid, groups) to do that rather than the calling process's
         credentials.
    
         This allows a process started by /sbin/request-key to find keys belonging
         to the authorising process.
    
     (4) A key can be read, even if the process executing KEYCTL_READ doesn't have
         direct read or search permission if that key is contained within the
         keyrings of a process specified by an authorisation key found within the
         calling process's session keyring, and is searchable using the
         credentials of the authorising process.
    
         This allows a process started by /sbin/request-key to read keys belonging
         to the authorising process.
    
     (5) The magic KEY_SPEC_*_KEYRING key IDs when passed to KEYCTL_INSTANTIATE or
         KEYCTL_NEGATE will specify a keyring of the authorising process, rather
         than the process doing the instantiation.
    
     (6) One of the process keyrings can be nominated as the default to which
         request_key() should attach new keys if not otherwise specified. This is
         done with KEYCTL_SET_REQKEY_KEYRING and one of the KEY_REQKEY_DEFL_*
         constants. The current setting can also be read using this call.
    
     (7) request_key() is partially interruptible. If it is waiting for another
         process to finish constructing a key, it can be interrupted. This permits
         a request-key cycle to be broken without recourse to rebooting.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-Off-By: Benoit Boissinot <benoit.boissinot@ens-lyon.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 901742f92389..2c69682b0444 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -561,9 +561,10 @@ struct group_info {
 		groups_free(group_info); \
 } while (0)
 
-struct group_info *groups_alloc(int gidsetsize);
-void groups_free(struct group_info *group_info);
-int set_current_groups(struct group_info *group_info);
+extern struct group_info *groups_alloc(int gidsetsize);
+extern void groups_free(struct group_info *group_info);
+extern int set_current_groups(struct group_info *group_info);
+extern int groups_search(struct group_info *group_info, gid_t grp);
 /* access the groups "array" with this macro */
 #define GROUP_AT(gi, i) \
     ((gi)->blocks[(i)/NGROUPS_PER_BLOCK][(i)%NGROUPS_PER_BLOCK])
@@ -660,6 +661,7 @@ struct task_struct {
 	struct user_struct *user;
 #ifdef CONFIG_KEYS
 	struct key *thread_keyring;	/* keyring private to this thread */
+	unsigned char jit_keyring;	/* default keyring to attach requested keys to */
 #endif
 	int oomkilladj; /* OOM kill score adjustment (bit shift). */
 	char comm[TASK_COMM_LEN]; /* executable name excluding path

commit d6e711448137ca3301512cec41a2c2ce852b3d0a
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Thu Jun 23 00:09:43 2005 -0700

    [PATCH] setuid core dump
    
    Add a new `suid_dumpable' sysctl:
    
    This value can be used to query and set the core dump mode for setuid
    or otherwise protected/tainted binaries. The modes are
    
    0 - (default) - traditional behaviour.  Any process which has changed
        privilege levels or is execute only will not be dumped
    
    1 - (debug) - all processes dump core when possible.  The core dump is
        owned by the current user and no security is applied.  This is intended
        for system debugging situations only.  Ptrace is unchecked.
    
    2 - (suidsafe) - any binary which normally would not be dumped is dumped
        readable by root only.  This allows the end user to remove such a dump but
        not access it directly.  For security reasons core dumps in this mode will
        not overwrite one another or other files.  This mode is appropriate when
        adminstrators are attempting to debug problems in a normal environment.
    
    (akpm:
    
    > > +EXPORT_SYMBOL(suid_dumpable);
    >
    > EXPORT_SYMBOL_GPL?
    
    No problem to me.
    
    > >     if (current->euid == current->uid && current->egid == current->gid)
    > >             current->mm->dumpable = 1;
    >
    > Should this be SUID_DUMP_USER?
    
    Actually the feedback I had from last time was that the SUID_ defines
    should go because its clearer to follow the numbers. They can go
    everywhere (and there are lots of places where dumpable is tested/used
    as a bool in untouched code)
    
    > Maybe this should be renamed to `dump_policy' or something.  Doing that
    > would help us catch any code which isn't using the #defines, too.
    
    Fair comment. The patch was designed to be easy to maintain for Red Hat
    rather than for merging. Changing that field would create a gigantic
    diff because it is used all over the place.
    
    )
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index b58afd97a180..901742f92389 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -246,7 +246,7 @@ struct mm_struct {
 
 	unsigned long saved_auxv[42]; /* for /proc/PID/auxv */
 
-	unsigned dumpable:1;
+	unsigned dumpable:2;
 	cpumask_t cpu_vm_mask;
 
 	/* Architecture-specific MM context */

commit 1363c3cd8603a913a27e2995dccbd70d5312d8e6
Author: Wolfgang Wander <wwc@rentec.com>
Date:   Tue Jun 21 17:14:49 2005 -0700

    [PATCH] Avoiding mmap fragmentation
    
    Ingo recently introduced a great speedup for allocating new mmaps using the
    free_area_cache pointer which boosts the specweb SSL benchmark by 4-5% and
    causes huge performance increases in thread creation.
    
    The downside of this patch is that it does lead to fragmentation in the
    mmap-ed areas (visible via /proc/self/maps), such that some applications
    that work fine under 2.4 kernels quickly run out of memory on any 2.6
    kernel.
    
    The problem is twofold:
    
      1) the free_area_cache is used to continue a search for memory where
         the last search ended.  Before the change new areas were always
         searched from the base address on.
    
         So now new small areas are cluttering holes of all sizes
         throughout the whole mmap-able region whereas before small holes
         tended to close holes near the base leaving holes far from the base
         large and available for larger requests.
    
      2) the free_area_cache also is set to the location of the last
         munmap-ed area so in scenarios where we allocate e.g.  five regions of
         1K each, then free regions 4 2 3 in this order the next request for 1K
         will be placed in the position of the old region 3, whereas before we
         appended it to the still active region 1, placing it at the location
         of the old region 2.  Before we had 1 free region of 2K, now we only
         get two free regions of 1K -> fragmentation.
    
    The patch addresses thes issues by introducing yet another cache descriptor
    cached_hole_size that contains the largest known hole size below the
    current free_area_cache.  If a new request comes in the size is compared
    against the cached_hole_size and if the request can be filled with a hole
    below free_area_cache the search is started from the base instead.
    
    The results look promising: Whereas 2.6.12-rc4 fragments quickly and my
    (earlier posted) leakme.c test program terminates after 50000+ iterations
    with 96 distinct and fragmented maps in /proc/self/maps it performs nicely
    (as expected) with thread creation, Ingo's test_str02 with 20000 threads
    requires 0.7s system time.
    
    Taking out Ingo's patch (un-patch available per request) by basically
    deleting all mentions of free_area_cache from the kernel and starting the
    search for new memory always at the respective bases we observe: leakme
    terminates successfully with 11 distinctive hardly fragmented areas in
    /proc/self/maps but thread creating is gringdingly slow: 30+s(!) system
    time for Ingo's test_str02 with 20000 threads.
    
    Now - drumroll ;-) the appended patch works fine with leakme: it ends with
    only 7 distinct areas in /proc/self/maps and also thread creation seems
    sufficiently fast with 0.71s for 20000 threads.
    
    Signed-off-by: Wolfgang Wander <wwc@rentec.com>
    Credit-to: "Richard Purdie" <rpurdie@rpsys.net>
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu> (partly)
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4dbb109022f3..b58afd97a180 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -201,8 +201,8 @@ extern unsigned long
 arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
 			  unsigned long len, unsigned long pgoff,
 			  unsigned long flags);
-extern void arch_unmap_area(struct vm_area_struct *area);
-extern void arch_unmap_area_topdown(struct vm_area_struct *area);
+extern void arch_unmap_area(struct mm_struct *, unsigned long);
+extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 
 #define set_mm_counter(mm, member, value) (mm)->_##member = (value)
 #define get_mm_counter(mm, member) ((mm)->_##member)
@@ -218,9 +218,10 @@ struct mm_struct {
 	unsigned long (*get_unmapped_area) (struct file *filp,
 				unsigned long addr, unsigned long len,
 				unsigned long pgoff, unsigned long flags);
-	void (*unmap_area) (struct vm_area_struct *area);
-	unsigned long mmap_base;		/* base of mmap area */
-	unsigned long free_area_cache;		/* first hole */
+	void (*unmap_area) (struct mm_struct *mm, unsigned long addr);
+        unsigned long mmap_base;		/* base of mmap area */
+        unsigned long cached_hole_size;         /* if non-zero, the largest hole below free_area_cache */
+	unsigned long free_area_cache;		/* first hole of size cached_hole_size or larger */
 	pgd_t * pgd;
 	atomic_t mm_users;			/* How many users with user space? */
 	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */

commit 3677209239ed71d2654e73eecfab1dbec2af11a9
Author: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
Date:   Thu May 5 16:16:12 2005 -0700

    [PATCH] comments on locking of task->comm
    
    Add some comments about task->comm, to explain what it is near its definition
    and provide some important pointers to its uses.
    
    Signed-off-by: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5f868a588581..4dbb109022f3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -578,7 +578,7 @@ struct task_struct {
 	unsigned long flags;	/* per process flags, defined below */
 	unsigned long ptrace;
 
-	int lock_depth;		/* Lock depth */
+	int lock_depth;		/* BKL lock depth */
 
 	int prio, static_prio;
 	struct list_head run_list;
@@ -661,7 +661,10 @@ struct task_struct {
 	struct key *thread_keyring;	/* keyring private to this thread */
 #endif
 	int oomkilladj; /* OOM kill score adjustment (bit shift). */
-	char comm[TASK_COMM_LEN];
+	char comm[TASK_COMM_LEN]; /* executable name excluding path
+				     - access with [gs]et_task_comm (which lock
+				       it with task_lock())
+				     - initialized normally by flush_old_exec */
 /* file system info */
 	int link_count, total_link_count;
 /* ipc stuff */

commit 408b664a7d394a5e4315fbd14aca49b042cb2b08
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sun May 1 08:59:29 2005 -0700

    [PATCH] make lots of things static
    
    Another large rollup of various patches from Adrian which make things static
    where they were needlessly exported.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 8960f99ea128..5f868a588581 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1012,7 +1012,6 @@ extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, struct
 extern void flush_thread(void);
 extern void exit_thread(void);
 
-extern void exit_mm(struct task_struct *);
 extern void exit_files(struct task_struct *);
 extern void exit_signal(struct task_struct *);
 extern void __exit_signal(struct task_struct *);

commit e43379f10b42194b8a6e1de342cfb44463c0f6da
Author: Matt Mackall <mpm@selenic.com>
Date:   Sun May 1 08:59:00 2005 -0700

    [PATCH] nice and rt-prio rlimits
    
    Add a pair of rlimits for allowing non-root tasks to raise nice and rt
    priorities. Defaults to traditional behavior. Originally written by
    Chris Wright.
    
    The patch implements a simple rlimit ceiling for the RT (and nice) priorities
    a task can set.  The rlimit defaults to 0, meaning no change in behavior by
    default.  A value of 50 means RT priority levels 1-50 are allowed.  A value of
    100 means all 99 privilege levels from 1 to 99 are allowed.  CAP_SYS_NICE is
    blanket permission.
    
    (akpm: see http://www.uwsg.iu.edu/hypermail/linux/kernel/0503.1/1921.html for
    tips on integrating this with PAM).
    
    Signed-off-by: Matt Mackall <mpm@selenic.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1cced971232c..8960f99ea128 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -845,6 +845,7 @@ extern void sched_idle_next(void);
 extern void set_user_nice(task_t *p, long nice);
 extern int task_prio(const task_t *p);
 extern int task_nice(const task_t *p);
+extern int can_nice(const task_t *p, const int nice);
 extern int task_curr(const task_t *p);
 extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);

commit 6c46ada700568897165409e618ed584683838b49
Author: Coywolf Qi Hunt <coywolf@lovecn.org>
Date:   Sat Apr 16 15:26:01 2005 -0700

    [PATCH] reparent_to_init cleanup
    
    This patch hides reparent_to_init().  reparent_to_init() should only be
    called by daemonize().
    
    Signed-off-by: Coywolf Qi Hunt <coywolf@lovecn.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index a30e91f40da6..1cced971232c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1021,7 +1021,6 @@ extern void exit_itimers(struct signal_struct *);
 
 extern NORET_TYPE void do_group_exit(int);
 
-extern void reparent_to_init(void);
 extern void daemonize(const char *, ...);
 extern int allow_signal(int);
 extern int disallow_signal(int);

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/sched.h b/include/linux/sched.h
new file mode 100644
index 000000000000..a30e91f40da6
--- /dev/null
+++ b/include/linux/sched.h
@@ -0,0 +1,1273 @@
+#ifndef _LINUX_SCHED_H
+#define _LINUX_SCHED_H
+
+#include <asm/param.h>	/* for HZ */
+
+#include <linux/config.h>
+#include <linux/capability.h>
+#include <linux/threads.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/timex.h>
+#include <linux/jiffies.h>
+#include <linux/rbtree.h>
+#include <linux/thread_info.h>
+#include <linux/cpumask.h>
+#include <linux/errno.h>
+#include <linux/nodemask.h>
+
+#include <asm/system.h>
+#include <asm/semaphore.h>
+#include <asm/page.h>
+#include <asm/ptrace.h>
+#include <asm/mmu.h>
+#include <asm/cputime.h>
+
+#include <linux/smp.h>
+#include <linux/sem.h>
+#include <linux/signal.h>
+#include <linux/securebits.h>
+#include <linux/fs_struct.h>
+#include <linux/compiler.h>
+#include <linux/completion.h>
+#include <linux/pid.h>
+#include <linux/percpu.h>
+#include <linux/topology.h>
+#include <linux/seccomp.h>
+
+struct exec_domain;
+
+/*
+ * cloning flags:
+ */
+#define CSIGNAL		0x000000ff	/* signal mask to be sent at exit */
+#define CLONE_VM	0x00000100	/* set if VM shared between processes */
+#define CLONE_FS	0x00000200	/* set if fs info shared between processes */
+#define CLONE_FILES	0x00000400	/* set if open files shared between processes */
+#define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
+#define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
+#define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
+#define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
+#define CLONE_THREAD	0x00010000	/* Same thread group? */
+#define CLONE_NEWNS	0x00020000	/* New namespace group? */
+#define CLONE_SYSVSEM	0x00040000	/* share system V SEM_UNDO semantics */
+#define CLONE_SETTLS	0x00080000	/* create a new TLS for the child */
+#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */
+#define CLONE_CHILD_CLEARTID	0x00200000	/* clear the TID in the child */
+#define CLONE_DETACHED		0x00400000	/* Unused, ignored */
+#define CLONE_UNTRACED		0x00800000	/* set if the tracing process can't force CLONE_PTRACE on this clone */
+#define CLONE_CHILD_SETTID	0x01000000	/* set the TID in the child */
+#define CLONE_STOPPED		0x02000000	/* Start in stopped state */
+
+/*
+ * List of flags we want to share for kernel threads,
+ * if only because they are not used by them anyway.
+ */
+#define CLONE_KERNEL	(CLONE_FS | CLONE_FILES | CLONE_SIGHAND)
+
+/*
+ * These are the constant used to fake the fixed-point load-average
+ * counting. Some notes:
+ *  - 11 bit fractions expand to 22 bits by the multiplies: this gives
+ *    a load-average precision of 10 bits integer + 11 bits fractional
+ *  - if you want to count load-averages more often, you need more
+ *    precision, or rounding will get you. With 2-second counting freq,
+ *    the EXP_n values would be 1981, 2034 and 2043 if still using only
+ *    11 bit fractions.
+ */
+extern unsigned long avenrun[];		/* Load averages */
+
+#define FSHIFT		11		/* nr of bits of precision */
+#define FIXED_1		(1<<FSHIFT)	/* 1.0 as fixed-point */
+#define LOAD_FREQ	(5*HZ)		/* 5 sec intervals */
+#define EXP_1		1884		/* 1/exp(5sec/1min) as fixed-point */
+#define EXP_5		2014		/* 1/exp(5sec/5min) */
+#define EXP_15		2037		/* 1/exp(5sec/15min) */
+
+#define CALC_LOAD(load,exp,n) \
+	load *= exp; \
+	load += n*(FIXED_1-exp); \
+	load >>= FSHIFT;
+
+extern unsigned long total_forks;
+extern int nr_threads;
+extern int last_pid;
+DECLARE_PER_CPU(unsigned long, process_counts);
+extern int nr_processes(void);
+extern unsigned long nr_running(void);
+extern unsigned long nr_uninterruptible(void);
+extern unsigned long nr_iowait(void);
+
+#include <linux/time.h>
+#include <linux/param.h>
+#include <linux/resource.h>
+#include <linux/timer.h>
+
+#include <asm/processor.h>
+
+#define TASK_RUNNING		0
+#define TASK_INTERRUPTIBLE	1
+#define TASK_UNINTERRUPTIBLE	2
+#define TASK_STOPPED		4
+#define TASK_TRACED		8
+#define EXIT_ZOMBIE		16
+#define EXIT_DEAD		32
+
+#define __set_task_state(tsk, state_value)		\
+	do { (tsk)->state = (state_value); } while (0)
+#define set_task_state(tsk, state_value)		\
+	set_mb((tsk)->state, (state_value))
+
+#define __set_current_state(state_value)			\
+	do { current->state = (state_value); } while (0)
+#define set_current_state(state_value)		\
+	set_mb(current->state, (state_value))
+
+/* Task command name length */
+#define TASK_COMM_LEN 16
+
+/*
+ * Scheduling policies
+ */
+#define SCHED_NORMAL		0
+#define SCHED_FIFO		1
+#define SCHED_RR		2
+
+struct sched_param {
+	int sched_priority;
+};
+
+#ifdef __KERNEL__
+
+#include <linux/spinlock.h>
+
+/*
+ * This serializes "schedule()" and also protects
+ * the run-queue from deletions/modifications (but
+ * _adding_ to the beginning of the run-queue has
+ * a separate lock).
+ */
+extern rwlock_t tasklist_lock;
+extern spinlock_t mmlist_lock;
+
+typedef struct task_struct task_t;
+
+extern void sched_init(void);
+extern void sched_init_smp(void);
+extern void init_idle(task_t *idle, int cpu);
+
+extern cpumask_t nohz_cpu_mask;
+
+extern void show_state(void);
+extern void show_regs(struct pt_regs *);
+
+/*
+ * TASK is a pointer to the task whose backtrace we want to see (or NULL for current
+ * task), SP is the stack pointer of the first frame that should be shown in the back
+ * trace (or NULL if the entire call-chain of the task should be shown).
+ */
+extern void show_stack(struct task_struct *task, unsigned long *sp);
+
+void io_schedule(void);
+long io_schedule_timeout(long timeout);
+
+extern void cpu_init (void);
+extern void trap_init(void);
+extern void update_process_times(int user);
+extern void scheduler_tick(void);
+
+/* Attach to any functions which should be ignored in wchan output. */
+#define __sched		__attribute__((__section__(".sched.text")))
+/* Is this address in the __sched functions? */
+extern int in_sched_functions(unsigned long addr);
+
+#define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
+extern signed long FASTCALL(schedule_timeout(signed long timeout));
+asmlinkage void schedule(void);
+
+struct namespace;
+
+/* Maximum number of active map areas.. This is a random (large) number */
+#define DEFAULT_MAX_MAP_COUNT	65536
+
+extern int sysctl_max_map_count;
+
+#include <linux/aio.h>
+
+extern unsigned long
+arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
+		       unsigned long, unsigned long);
+extern unsigned long
+arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
+			  unsigned long len, unsigned long pgoff,
+			  unsigned long flags);
+extern void arch_unmap_area(struct vm_area_struct *area);
+extern void arch_unmap_area_topdown(struct vm_area_struct *area);
+
+#define set_mm_counter(mm, member, value) (mm)->_##member = (value)
+#define get_mm_counter(mm, member) ((mm)->_##member)
+#define add_mm_counter(mm, member, value) (mm)->_##member += (value)
+#define inc_mm_counter(mm, member) (mm)->_##member++
+#define dec_mm_counter(mm, member) (mm)->_##member--
+typedef unsigned long mm_counter_t;
+
+struct mm_struct {
+	struct vm_area_struct * mmap;		/* list of VMAs */
+	struct rb_root mm_rb;
+	struct vm_area_struct * mmap_cache;	/* last find_vma result */
+	unsigned long (*get_unmapped_area) (struct file *filp,
+				unsigned long addr, unsigned long len,
+				unsigned long pgoff, unsigned long flags);
+	void (*unmap_area) (struct vm_area_struct *area);
+	unsigned long mmap_base;		/* base of mmap area */
+	unsigned long free_area_cache;		/* first hole */
+	pgd_t * pgd;
+	atomic_t mm_users;			/* How many users with user space? */
+	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
+	int map_count;				/* number of VMAs */
+	struct rw_semaphore mmap_sem;
+	spinlock_t page_table_lock;		/* Protects page tables and some counters */
+
+	struct list_head mmlist;		/* List of maybe swapped mm's.  These are globally strung
+						 * together off init_mm.mmlist, and are protected
+						 * by mmlist_lock
+						 */
+
+	unsigned long start_code, end_code, start_data, end_data;
+	unsigned long start_brk, brk, start_stack;
+	unsigned long arg_start, arg_end, env_start, env_end;
+	unsigned long total_vm, locked_vm, shared_vm;
+	unsigned long exec_vm, stack_vm, reserved_vm, def_flags, nr_ptes;
+
+	/* Special counters protected by the page_table_lock */
+	mm_counter_t _rss;
+	mm_counter_t _anon_rss;
+
+	unsigned long saved_auxv[42]; /* for /proc/PID/auxv */
+
+	unsigned dumpable:1;
+	cpumask_t cpu_vm_mask;
+
+	/* Architecture-specific MM context */
+	mm_context_t context;
+
+	/* Token based thrashing protection. */
+	unsigned long swap_token_time;
+	char recent_pagein;
+
+	/* coredumping support */
+	int core_waiters;
+	struct completion *core_startup_done, core_done;
+
+	/* aio bits */
+	rwlock_t		ioctx_list_lock;
+	struct kioctx		*ioctx_list;
+
+	struct kioctx		default_kioctx;
+
+	unsigned long hiwater_rss;	/* High-water RSS usage */
+	unsigned long hiwater_vm;	/* High-water virtual memory usage */
+};
+
+struct sighand_struct {
+	atomic_t		count;
+	struct k_sigaction	action[_NSIG];
+	spinlock_t		siglock;
+};
+
+/*
+ * NOTE! "signal_struct" does not have it's own
+ * locking, because a shared signal_struct always
+ * implies a shared sighand_struct, so locking
+ * sighand_struct is always a proper superset of
+ * the locking of signal_struct.
+ */
+struct signal_struct {
+	atomic_t		count;
+	atomic_t		live;
+
+	wait_queue_head_t	wait_chldexit;	/* for wait4() */
+
+	/* current thread group signal load-balancing target: */
+	task_t			*curr_target;
+
+	/* shared signal handling: */
+	struct sigpending	shared_pending;
+
+	/* thread group exit support */
+	int			group_exit_code;
+	/* overloaded:
+	 * - notify group_exit_task when ->count is equal to notify_count
+	 * - everyone except group_exit_task is stopped during signal delivery
+	 *   of fatal signals, group_exit_task processes the signal.
+	 */
+	struct task_struct	*group_exit_task;
+	int			notify_count;
+
+	/* thread group stop support, overloads group_exit_code too */
+	int			group_stop_count;
+	unsigned int		flags; /* see SIGNAL_* flags below */
+
+	/* POSIX.1b Interval Timers */
+	struct list_head posix_timers;
+
+	/* ITIMER_REAL timer for the process */
+	struct timer_list real_timer;
+	unsigned long it_real_value, it_real_incr;
+
+	/* ITIMER_PROF and ITIMER_VIRTUAL timers for the process */
+	cputime_t it_prof_expires, it_virt_expires;
+	cputime_t it_prof_incr, it_virt_incr;
+
+	/* job control IDs */
+	pid_t pgrp;
+	pid_t tty_old_pgrp;
+	pid_t session;
+	/* boolean value for session group leader */
+	int leader;
+
+	struct tty_struct *tty; /* NULL if no tty */
+
+	/*
+	 * Cumulative resource counters for dead threads in the group,
+	 * and for reaped dead child processes forked by this group.
+	 * Live threads maintain their own counters and add to these
+	 * in __exit_signal, except for the group leader.
+	 */
+	cputime_t utime, stime, cutime, cstime;
+	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
+	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
+
+	/*
+	 * Cumulative ns of scheduled CPU time for dead threads in the
+	 * group, not including a zombie group leader.  (This only differs
+	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
+	 * other than jiffies.)
+	 */
+	unsigned long long sched_time;
+
+	/*
+	 * We don't bother to synchronize most readers of this at all,
+	 * because there is no reader checking a limit that actually needs
+	 * to get both rlim_cur and rlim_max atomically, and either one
+	 * alone is a single word that can safely be read normally.
+	 * getrlimit/setrlimit use task_lock(current->group_leader) to
+	 * protect this instead of the siglock, because they really
+	 * have no need to disable irqs.
+	 */
+	struct rlimit rlim[RLIM_NLIMITS];
+
+	struct list_head cpu_timers[3];
+
+	/* keep the process-shared keyrings here so that they do the right
+	 * thing in threads created with CLONE_THREAD */
+#ifdef CONFIG_KEYS
+	struct key *session_keyring;	/* keyring inherited over fork */
+	struct key *process_keyring;	/* keyring private to this process */
+#endif
+};
+
+/*
+ * Bits in flags field of signal_struct.
+ */
+#define SIGNAL_STOP_STOPPED	0x00000001 /* job control stop in effect */
+#define SIGNAL_STOP_DEQUEUED	0x00000002 /* stop signal dequeued */
+#define SIGNAL_STOP_CONTINUED	0x00000004 /* SIGCONT since WCONTINUED reap */
+#define SIGNAL_GROUP_EXIT	0x00000008 /* group exit in progress */
+
+
+/*
+ * Priority of a process goes from 0..MAX_PRIO-1, valid RT
+ * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL tasks are
+ * in the range MAX_RT_PRIO..MAX_PRIO-1. Priority values
+ * are inverted: lower p->prio value means higher priority.
+ *
+ * The MAX_USER_RT_PRIO value allows the actual maximum
+ * RT priority to be separate from the value exported to
+ * user-space.  This allows kernel threads to set their
+ * priority to a value higher than any user task. Note:
+ * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
+ */
+
+#define MAX_USER_RT_PRIO	100
+#define MAX_RT_PRIO		MAX_USER_RT_PRIO
+
+#define MAX_PRIO		(MAX_RT_PRIO + 40)
+
+#define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
+
+/*
+ * Some day this will be a full-fledged user tracking system..
+ */
+struct user_struct {
+	atomic_t __count;	/* reference count */
+	atomic_t processes;	/* How many processes does this user have? */
+	atomic_t files;		/* How many open files does this user have? */
+	atomic_t sigpending;	/* How many pending signals does this user have? */
+	/* protected by mq_lock	*/
+	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
+	unsigned long locked_shm; /* How many pages of mlocked shm ? */
+
+#ifdef CONFIG_KEYS
+	struct key *uid_keyring;	/* UID specific keyring */
+	struct key *session_keyring;	/* UID's default session keyring */
+#endif
+
+	/* Hash table maintenance information */
+	struct list_head uidhash_list;
+	uid_t uid;
+};
+
+extern struct user_struct *find_user(uid_t);
+
+extern struct user_struct root_user;
+#define INIT_USER (&root_user)
+
+typedef struct prio_array prio_array_t;
+struct backing_dev_info;
+struct reclaim_state;
+
+#ifdef CONFIG_SCHEDSTATS
+struct sched_info {
+	/* cumulative counters */
+	unsigned long	cpu_time,	/* time spent on the cpu */
+			run_delay,	/* time spent waiting on a runqueue */
+			pcnt;		/* # of timeslices run on this cpu */
+
+	/* timestamps */
+	unsigned long	last_arrival,	/* when we last ran on a cpu */
+			last_queued;	/* when we were last queued to run */
+};
+
+extern struct file_operations proc_schedstat_operations;
+#endif
+
+enum idle_type
+{
+	SCHED_IDLE,
+	NOT_IDLE,
+	NEWLY_IDLE,
+	MAX_IDLE_TYPES
+};
+
+/*
+ * sched-domains (multiprocessor balancing) declarations:
+ */
+#ifdef CONFIG_SMP
+#define SCHED_LOAD_SCALE	128UL	/* increase resolution of load */
+
+#define SD_LOAD_BALANCE		1	/* Do load balancing on this domain. */
+#define SD_BALANCE_NEWIDLE	2	/* Balance when about to become idle */
+#define SD_BALANCE_EXEC		4	/* Balance on exec */
+#define SD_WAKE_IDLE		8	/* Wake to idle CPU on task wakeup */
+#define SD_WAKE_AFFINE		16	/* Wake task to waking CPU */
+#define SD_WAKE_BALANCE		32	/* Perform balancing at task wakeup */
+#define SD_SHARE_CPUPOWER	64	/* Domain members share cpu power */
+
+struct sched_group {
+	struct sched_group *next;	/* Must be a circular list */
+	cpumask_t cpumask;
+
+	/*
+	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
+	 * single CPU. This is read only (except for setup, hotplug CPU).
+	 */
+	unsigned long cpu_power;
+};
+
+struct sched_domain {
+	/* These fields must be setup */
+	struct sched_domain *parent;	/* top domain must be null terminated */
+	struct sched_group *groups;	/* the balancing groups of the domain */
+	cpumask_t span;			/* span of all CPUs in this domain */
+	unsigned long min_interval;	/* Minimum balance interval ms */
+	unsigned long max_interval;	/* Maximum balance interval ms */
+	unsigned int busy_factor;	/* less balancing by factor if busy */
+	unsigned int imbalance_pct;	/* No balance until over watermark */
+	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
+	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
+	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
+	int flags;			/* See SD_* */
+
+	/* Runtime fields. */
+	unsigned long last_balance;	/* init to jiffies. units in jiffies */
+	unsigned int balance_interval;	/* initialise to 1. units in ms. */
+	unsigned int nr_balance_failed; /* initialise to 0 */
+
+#ifdef CONFIG_SCHEDSTATS
+	/* load_balance() stats */
+	unsigned long lb_cnt[MAX_IDLE_TYPES];
+	unsigned long lb_failed[MAX_IDLE_TYPES];
+	unsigned long lb_balanced[MAX_IDLE_TYPES];
+	unsigned long lb_imbalance[MAX_IDLE_TYPES];
+	unsigned long lb_gained[MAX_IDLE_TYPES];
+	unsigned long lb_hot_gained[MAX_IDLE_TYPES];
+	unsigned long lb_nobusyg[MAX_IDLE_TYPES];
+	unsigned long lb_nobusyq[MAX_IDLE_TYPES];
+
+	/* Active load balancing */
+	unsigned long alb_cnt;
+	unsigned long alb_failed;
+	unsigned long alb_pushed;
+
+	/* sched_balance_exec() stats */
+	unsigned long sbe_attempts;
+	unsigned long sbe_pushed;
+
+	/* try_to_wake_up() stats */
+	unsigned long ttwu_wake_remote;
+	unsigned long ttwu_move_affine;
+	unsigned long ttwu_move_balance;
+#endif
+};
+
+#ifdef ARCH_HAS_SCHED_DOMAIN
+/* Useful helpers that arch setup code may use. Defined in kernel/sched.c */
+extern cpumask_t cpu_isolated_map;
+extern void init_sched_build_groups(struct sched_group groups[],
+	                        cpumask_t span, int (*group_fn)(int cpu));
+extern void cpu_attach_domain(struct sched_domain *sd, int cpu);
+#endif /* ARCH_HAS_SCHED_DOMAIN */
+#endif /* CONFIG_SMP */
+
+
+struct io_context;			/* See blkdev.h */
+void exit_io_context(void);
+struct cpuset;
+
+#define NGROUPS_SMALL		32
+#define NGROUPS_PER_BLOCK	((int)(PAGE_SIZE / sizeof(gid_t)))
+struct group_info {
+	int ngroups;
+	atomic_t usage;
+	gid_t small_block[NGROUPS_SMALL];
+	int nblocks;
+	gid_t *blocks[0];
+};
+
+/*
+ * get_group_info() must be called with the owning task locked (via task_lock())
+ * when task != current.  The reason being that the vast majority of callers are
+ * looking at current->group_info, which can not be changed except by the
+ * current task.  Changing current->group_info requires the task lock, too.
+ */
+#define get_group_info(group_info) do { \
+	atomic_inc(&(group_info)->usage); \
+} while (0)
+
+#define put_group_info(group_info) do { \
+	if (atomic_dec_and_test(&(group_info)->usage)) \
+		groups_free(group_info); \
+} while (0)
+
+struct group_info *groups_alloc(int gidsetsize);
+void groups_free(struct group_info *group_info);
+int set_current_groups(struct group_info *group_info);
+/* access the groups "array" with this macro */
+#define GROUP_AT(gi, i) \
+    ((gi)->blocks[(i)/NGROUPS_PER_BLOCK][(i)%NGROUPS_PER_BLOCK])
+
+
+struct audit_context;		/* See audit.c */
+struct mempolicy;
+
+struct task_struct {
+	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
+	struct thread_info *thread_info;
+	atomic_t usage;
+	unsigned long flags;	/* per process flags, defined below */
+	unsigned long ptrace;
+
+	int lock_depth;		/* Lock depth */
+
+	int prio, static_prio;
+	struct list_head run_list;
+	prio_array_t *array;
+
+	unsigned long sleep_avg;
+	unsigned long long timestamp, last_ran;
+	unsigned long long sched_time; /* sched_clock time spent running */
+	int activated;
+
+	unsigned long policy;
+	cpumask_t cpus_allowed;
+	unsigned int time_slice, first_time_slice;
+
+#ifdef CONFIG_SCHEDSTATS
+	struct sched_info sched_info;
+#endif
+
+	struct list_head tasks;
+	/*
+	 * ptrace_list/ptrace_children forms the list of my children
+	 * that were stolen by a ptracer.
+	 */
+	struct list_head ptrace_children;
+	struct list_head ptrace_list;
+
+	struct mm_struct *mm, *active_mm;
+
+/* task state */
+	struct linux_binfmt *binfmt;
+	long exit_state;
+	int exit_code, exit_signal;
+	int pdeath_signal;  /*  The signal sent when the parent dies  */
+	/* ??? */
+	unsigned long personality;
+	unsigned did_exec:1;
+	pid_t pid;
+	pid_t tgid;
+	/* 
+	 * pointers to (original) parent process, youngest child, younger sibling,
+	 * older sibling, respectively.  (p->father can be replaced with 
+	 * p->parent->pid)
+	 */
+	struct task_struct *real_parent; /* real parent process (when being debugged) */
+	struct task_struct *parent;	/* parent process */
+	/*
+	 * children/sibling forms the list of my children plus the
+	 * tasks I'm ptracing.
+	 */
+	struct list_head children;	/* list of my children */
+	struct list_head sibling;	/* linkage in my parent's children list */
+	struct task_struct *group_leader;	/* threadgroup leader */
+
+	/* PID/PID hash table linkage. */
+	struct pid pids[PIDTYPE_MAX];
+
+	struct completion *vfork_done;		/* for vfork() */
+	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
+	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
+
+	unsigned long rt_priority;
+	cputime_t utime, stime;
+	unsigned long nvcsw, nivcsw; /* context switch counts */
+	struct timespec start_time;
+/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
+	unsigned long min_flt, maj_flt;
+
+  	cputime_t it_prof_expires, it_virt_expires;
+	unsigned long long it_sched_expires;
+	struct list_head cpu_timers[3];
+
+/* process credentials */
+	uid_t uid,euid,suid,fsuid;
+	gid_t gid,egid,sgid,fsgid;
+	struct group_info *group_info;
+	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted;
+	unsigned keep_capabilities:1;
+	struct user_struct *user;
+#ifdef CONFIG_KEYS
+	struct key *thread_keyring;	/* keyring private to this thread */
+#endif
+	int oomkilladj; /* OOM kill score adjustment (bit shift). */
+	char comm[TASK_COMM_LEN];
+/* file system info */
+	int link_count, total_link_count;
+/* ipc stuff */
+	struct sysv_sem sysvsem;
+/* CPU-specific state of this task */
+	struct thread_struct thread;
+/* filesystem information */
+	struct fs_struct *fs;
+/* open file information */
+	struct files_struct *files;
+/* namespace */
+	struct namespace *namespace;
+/* signal handlers */
+	struct signal_struct *signal;
+	struct sighand_struct *sighand;
+
+	sigset_t blocked, real_blocked;
+	struct sigpending pending;
+
+	unsigned long sas_ss_sp;
+	size_t sas_ss_size;
+	int (*notifier)(void *priv);
+	void *notifier_data;
+	sigset_t *notifier_mask;
+	
+	void *security;
+	struct audit_context *audit_context;
+	seccomp_t seccomp;
+
+/* Thread group tracking */
+   	u32 parent_exec_id;
+   	u32 self_exec_id;
+/* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
+	spinlock_t alloc_lock;
+/* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
+	spinlock_t proc_lock;
+/* context-switch lock */
+	spinlock_t switch_lock;
+
+/* journalling filesystem info */
+	void *journal_info;
+
+/* VM state */
+	struct reclaim_state *reclaim_state;
+
+	struct dentry *proc_dentry;
+	struct backing_dev_info *backing_dev_info;
+
+	struct io_context *io_context;
+
+	unsigned long ptrace_message;
+	siginfo_t *last_siginfo; /* For ptrace use.  */
+/*
+ * current io wait handle: wait queue entry to use for io waits
+ * If this thread is processing aio, this points at the waitqueue
+ * inside the currently handled kiocb. It may be NULL (i.e. default
+ * to a stack based synchronous wait) if its doing sync IO.
+ */
+	wait_queue_t *io_wait;
+/* i/o counters(bytes read/written, #syscalls */
+	u64 rchar, wchar, syscr, syscw;
+#if defined(CONFIG_BSD_PROCESS_ACCT)
+	u64 acct_rss_mem1;	/* accumulated rss usage */
+	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
+	clock_t acct_stimexpd;	/* clock_t-converted stime since last update */
+#endif
+#ifdef CONFIG_NUMA
+  	struct mempolicy *mempolicy;
+	short il_next;
+#endif
+#ifdef CONFIG_CPUSETS
+	struct cpuset *cpuset;
+	nodemask_t mems_allowed;
+	int cpuset_mems_generation;
+#endif
+};
+
+static inline pid_t process_group(struct task_struct *tsk)
+{
+	return tsk->signal->pgrp;
+}
+
+/**
+ * pid_alive - check that a task structure is not stale
+ * @p: Task structure to be checked.
+ *
+ * Test if a process is not yet dead (at most zombie state)
+ * If pid_alive fails, then pointers within the task structure
+ * can be stale and must not be dereferenced.
+ */
+static inline int pid_alive(struct task_struct *p)
+{
+	return p->pids[PIDTYPE_PID].nr != 0;
+}
+
+extern void free_task(struct task_struct *tsk);
+extern void __put_task_struct(struct task_struct *tsk);
+#define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
+#define put_task_struct(tsk) \
+do { if (atomic_dec_and_test(&(tsk)->usage)) __put_task_struct(tsk); } while(0)
+
+/*
+ * Per process flags
+ */
+#define PF_ALIGNWARN	0x00000001	/* Print alignment warning msgs */
+					/* Not implemented yet, only for 486*/
+#define PF_STARTING	0x00000002	/* being created */
+#define PF_EXITING	0x00000004	/* getting shut down */
+#define PF_DEAD		0x00000008	/* Dead */
+#define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
+#define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
+#define PF_DUMPCORE	0x00000200	/* dumped core */
+#define PF_SIGNALED	0x00000400	/* killed by a signal */
+#define PF_MEMALLOC	0x00000800	/* Allocating memory */
+#define PF_FLUSHER	0x00001000	/* responsible for disk writeback */
+#define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
+#define PF_FREEZE	0x00004000	/* this task is being frozen for suspend now */
+#define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
+#define PF_FROZEN	0x00010000	/* frozen for system suspend */
+#define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
+#define PF_KSWAPD	0x00040000	/* I am kswapd */
+#define PF_SWAPOFF	0x00080000	/* I am in swapoff */
+#define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
+#define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
+#define PF_BORROWED_MM	0x00400000	/* I am a kthread doing use_mm */
+#define PF_RANDOMIZE	0x00800000	/* randomize virtual address space */
+
+/*
+ * Only the _current_ task can read/write to tsk->flags, but other
+ * tasks can access tsk->flags in readonly mode for example
+ * with tsk_used_math (like during threaded core dumping).
+ * There is however an exception to this rule during ptrace
+ * or during fork: the ptracer task is allowed to write to the
+ * child->flags of its traced child (same goes for fork, the parent
+ * can write to the child->flags), because we're guaranteed the
+ * child is not running and in turn not changing child->flags
+ * at the same time the parent does it.
+ */
+#define clear_stopped_child_used_math(child) do { (child)->flags &= ~PF_USED_MATH; } while (0)
+#define set_stopped_child_used_math(child) do { (child)->flags |= PF_USED_MATH; } while (0)
+#define clear_used_math() clear_stopped_child_used_math(current)
+#define set_used_math() set_stopped_child_used_math(current)
+#define conditional_stopped_child_used_math(condition, child) \
+	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)
+#define conditional_used_math(condition) \
+	conditional_stopped_child_used_math(condition, current)
+#define copy_to_stopped_child_used_math(child) \
+	do { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)
+/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */
+#define tsk_used_math(p) ((p)->flags & PF_USED_MATH)
+#define used_math() tsk_used_math(current)
+
+#ifdef CONFIG_SMP
+extern int set_cpus_allowed(task_t *p, cpumask_t new_mask);
+#else
+static inline int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+{
+	if (!cpus_intersects(new_mask, cpu_online_map))
+		return -EINVAL;
+	return 0;
+}
+#endif
+
+extern unsigned long long sched_clock(void);
+extern unsigned long long current_sched_time(const task_t *current_task);
+
+/* sched_exec is called by processes performing an exec */
+#ifdef CONFIG_SMP
+extern void sched_exec(void);
+#else
+#define sched_exec()   {}
+#endif
+
+#ifdef CONFIG_HOTPLUG_CPU
+extern void idle_task_exit(void);
+#else
+static inline void idle_task_exit(void) {}
+#endif
+
+extern void sched_idle_next(void);
+extern void set_user_nice(task_t *p, long nice);
+extern int task_prio(const task_t *p);
+extern int task_nice(const task_t *p);
+extern int task_curr(const task_t *p);
+extern int idle_cpu(int cpu);
+extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
+extern task_t *idle_task(int cpu);
+
+void yield(void);
+
+/*
+ * The default (Linux) execution domain.
+ */
+extern struct exec_domain	default_exec_domain;
+
+union thread_union {
+	struct thread_info thread_info;
+	unsigned long stack[THREAD_SIZE/sizeof(long)];
+};
+
+#ifndef __HAVE_ARCH_KSTACK_END
+static inline int kstack_end(void *addr)
+{
+	/* Reliable end of stack detection:
+	 * Some APM bios versions misalign the stack
+	 */
+	return !(((unsigned long)addr+sizeof(void*)-1) & (THREAD_SIZE-sizeof(void*)));
+}
+#endif
+
+extern union thread_union init_thread_union;
+extern struct task_struct init_task;
+
+extern struct   mm_struct init_mm;
+
+#define find_task_by_pid(nr)	find_task_by_pid_type(PIDTYPE_PID, nr)
+extern struct task_struct *find_task_by_pid_type(int type, int pid);
+extern void set_special_pids(pid_t session, pid_t pgrp);
+extern void __set_special_pids(pid_t session, pid_t pgrp);
+
+/* per-UID process charging. */
+extern struct user_struct * alloc_uid(uid_t);
+static inline struct user_struct *get_uid(struct user_struct *u)
+{
+	atomic_inc(&u->__count);
+	return u;
+}
+extern void free_uid(struct user_struct *);
+extern void switch_uid(struct user_struct *);
+
+#include <asm/current.h>
+
+extern void do_timer(struct pt_regs *);
+
+extern int FASTCALL(wake_up_state(struct task_struct * tsk, unsigned int state));
+extern int FASTCALL(wake_up_process(struct task_struct * tsk));
+extern void FASTCALL(wake_up_new_task(struct task_struct * tsk,
+						unsigned long clone_flags));
+#ifdef CONFIG_SMP
+ extern void kick_process(struct task_struct *tsk);
+#else
+ static inline void kick_process(struct task_struct *tsk) { }
+#endif
+extern void FASTCALL(sched_fork(task_t * p));
+extern void FASTCALL(sched_exit(task_t * p));
+
+extern int in_group_p(gid_t);
+extern int in_egroup_p(gid_t);
+
+extern void proc_caches_init(void);
+extern void flush_signals(struct task_struct *);
+extern void flush_signal_handlers(struct task_struct *, int force_default);
+extern int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info);
+
+static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
+{
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&tsk->sighand->siglock, flags);
+	ret = dequeue_signal(tsk, mask, info);
+	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
+
+	return ret;
+}	
+
+extern void block_all_signals(int (*notifier)(void *priv), void *priv,
+			      sigset_t *mask);
+extern void unblock_all_signals(void);
+extern void release_task(struct task_struct * p);
+extern int send_sig_info(int, struct siginfo *, struct task_struct *);
+extern int send_group_sig_info(int, struct siginfo *, struct task_struct *);
+extern int force_sigsegv(int, struct task_struct *);
+extern int force_sig_info(int, struct siginfo *, struct task_struct *);
+extern int __kill_pg_info(int sig, struct siginfo *info, pid_t pgrp);
+extern int kill_pg_info(int, struct siginfo *, pid_t);
+extern int kill_proc_info(int, struct siginfo *, pid_t);
+extern void do_notify_parent(struct task_struct *, int);
+extern void force_sig(int, struct task_struct *);
+extern void force_sig_specific(int, struct task_struct *);
+extern int send_sig(int, struct task_struct *, int);
+extern void zap_other_threads(struct task_struct *p);
+extern int kill_pg(pid_t, int, int);
+extern int kill_sl(pid_t, int, int);
+extern int kill_proc(pid_t, int, int);
+extern struct sigqueue *sigqueue_alloc(void);
+extern void sigqueue_free(struct sigqueue *);
+extern int send_sigqueue(int, struct sigqueue *,  struct task_struct *);
+extern int send_group_sigqueue(int, struct sigqueue *,  struct task_struct *);
+extern int do_sigaction(int, const struct k_sigaction *, struct k_sigaction *);
+extern int do_sigaltstack(const stack_t __user *, stack_t __user *, unsigned long);
+
+/* These can be the second arg to send_sig_info/send_group_sig_info.  */
+#define SEND_SIG_NOINFO ((struct siginfo *) 0)
+#define SEND_SIG_PRIV	((struct siginfo *) 1)
+#define SEND_SIG_FORCED	((struct siginfo *) 2)
+
+/* True if we are on the alternate signal stack.  */
+
+static inline int on_sig_stack(unsigned long sp)
+{
+	return (sp - current->sas_ss_sp < current->sas_ss_size);
+}
+
+static inline int sas_ss_flags(unsigned long sp)
+{
+	return (current->sas_ss_size == 0 ? SS_DISABLE
+		: on_sig_stack(sp) ? SS_ONSTACK : 0);
+}
+
+
+#ifdef CONFIG_SECURITY
+/* code is in security.c */
+extern int capable(int cap);
+#else
+static inline int capable(int cap)
+{
+	if (cap_raised(current->cap_effective, cap)) {
+		current->flags |= PF_SUPERPRIV;
+		return 1;
+	}
+	return 0;
+}
+#endif
+
+/*
+ * Routines for handling mm_structs
+ */
+extern struct mm_struct * mm_alloc(void);
+
+/* mmdrop drops the mm and the page tables */
+extern void FASTCALL(__mmdrop(struct mm_struct *));
+static inline void mmdrop(struct mm_struct * mm)
+{
+	if (atomic_dec_and_test(&mm->mm_count))
+		__mmdrop(mm);
+}
+
+/* mmput gets rid of the mappings and all user-space */
+extern void mmput(struct mm_struct *);
+/* Grab a reference to a task's mm, if it is not already going away */
+extern struct mm_struct *get_task_mm(struct task_struct *task);
+/* Remove the current tasks stale references to the old mm_struct */
+extern void mm_release(struct task_struct *, struct mm_struct *);
+
+extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, struct task_struct *, struct pt_regs *);
+extern void flush_thread(void);
+extern void exit_thread(void);
+
+extern void exit_mm(struct task_struct *);
+extern void exit_files(struct task_struct *);
+extern void exit_signal(struct task_struct *);
+extern void __exit_signal(struct task_struct *);
+extern void exit_sighand(struct task_struct *);
+extern void __exit_sighand(struct task_struct *);
+extern void exit_itimers(struct signal_struct *);
+
+extern NORET_TYPE void do_group_exit(int);
+
+extern void reparent_to_init(void);
+extern void daemonize(const char *, ...);
+extern int allow_signal(int);
+extern int disallow_signal(int);
+extern task_t *child_reaper;
+
+extern int do_execve(char *, char __user * __user *, char __user * __user *, struct pt_regs *);
+extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
+task_t *fork_idle(int);
+
+extern void set_task_comm(struct task_struct *tsk, char *from);
+extern void get_task_comm(char *to, struct task_struct *tsk);
+
+#ifdef CONFIG_SMP
+extern void wait_task_inactive(task_t * p);
+#else
+#define wait_task_inactive(p)	do { } while (0)
+#endif
+
+#define remove_parent(p)	list_del_init(&(p)->sibling)
+#define add_parent(p, parent)	list_add_tail(&(p)->sibling,&(parent)->children)
+
+#define REMOVE_LINKS(p) do {					\
+	if (thread_group_leader(p))				\
+		list_del_init(&(p)->tasks);			\
+	remove_parent(p);					\
+	} while (0)
+
+#define SET_LINKS(p) do {					\
+	if (thread_group_leader(p))				\
+		list_add_tail(&(p)->tasks,&init_task.tasks);	\
+	add_parent(p, (p)->parent);				\
+	} while (0)
+
+#define next_task(p)	list_entry((p)->tasks.next, struct task_struct, tasks)
+#define prev_task(p)	list_entry((p)->tasks.prev, struct task_struct, tasks)
+
+#define for_each_process(p) \
+	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
+
+/*
+ * Careful: do_each_thread/while_each_thread is a double loop so
+ *          'break' will not work as expected - use goto instead.
+ */
+#define do_each_thread(g, t) \
+	for (g = t = &init_task ; (g = t = next_task(g)) != &init_task ; ) do
+
+#define while_each_thread(g, t) \
+	while ((t = next_thread(t)) != g)
+
+extern task_t * FASTCALL(next_thread(const task_t *p));
+
+#define thread_group_leader(p)	(p->pid == p->tgid)
+
+static inline int thread_group_empty(task_t *p)
+{
+	return list_empty(&p->pids[PIDTYPE_TGID].pid_list);
+}
+
+#define delay_group_leader(p) \
+		(thread_group_leader(p) && !thread_group_empty(p))
+
+extern void unhash_process(struct task_struct *p);
+
+/*
+ * Protects ->fs, ->files, ->mm, ->ptrace, ->group_info, ->comm, keyring
+ * subscriptions and synchronises with wait4().  Also used in procfs.
+ *
+ * Nests both inside and outside of read_lock(&tasklist_lock).
+ * It must not be nested with write_lock_irq(&tasklist_lock),
+ * neither inside nor outside.
+ */
+static inline void task_lock(struct task_struct *p)
+{
+	spin_lock(&p->alloc_lock);
+}
+
+static inline void task_unlock(struct task_struct *p)
+{
+	spin_unlock(&p->alloc_lock);
+}
+
+/* set thread flags in other task's structures
+ * - see asm/thread_info.h for TIF_xxxx flags available
+ */
+static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)
+{
+	set_ti_thread_flag(tsk->thread_info,flag);
+}
+
+static inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)
+{
+	clear_ti_thread_flag(tsk->thread_info,flag);
+}
+
+static inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)
+{
+	return test_and_set_ti_thread_flag(tsk->thread_info,flag);
+}
+
+static inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)
+{
+	return test_and_clear_ti_thread_flag(tsk->thread_info,flag);
+}
+
+static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
+{
+	return test_ti_thread_flag(tsk->thread_info,flag);
+}
+
+static inline void set_tsk_need_resched(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
+}
+
+static inline void clear_tsk_need_resched(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
+}
+
+static inline int signal_pending(struct task_struct *p)
+{
+	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
+}
+  
+static inline int need_resched(void)
+{
+	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
+}
+
+/*
+ * cond_resched() and cond_resched_lock(): latency reduction via
+ * explicit rescheduling in places that are safe. The return
+ * value indicates whether a reschedule was done in fact.
+ * cond_resched_lock() will drop the spinlock before scheduling,
+ * cond_resched_softirq() will enable bhs before scheduling.
+ */
+extern int cond_resched(void);
+extern int cond_resched_lock(spinlock_t * lock);
+extern int cond_resched_softirq(void);
+
+/*
+ * Does a critical section need to be broken due to another
+ * task waiting?:
+ */
+#if defined(CONFIG_PREEMPT) && defined(CONFIG_SMP)
+# define need_lockbreak(lock) ((lock)->break_lock)
+#else
+# define need_lockbreak(lock) 0
+#endif
+
+/*
+ * Does a critical section need to be broken due to another
+ * task waiting or preemption being signalled:
+ */
+static inline int lock_need_resched(spinlock_t *lock)
+{
+	if (need_lockbreak(lock) || need_resched())
+		return 1;
+	return 0;
+}
+
+/* Reevaluate whether the task has signals pending delivery.
+   This is required every time the blocked sigset_t changes.
+   callers must hold sighand->siglock.  */
+
+extern FASTCALL(void recalc_sigpending_tsk(struct task_struct *t));
+extern void recalc_sigpending(void);
+
+extern void signal_wake_up(struct task_struct *t, int resume_stopped);
+
+/*
+ * Wrappers for p->thread_info->cpu access. No-op on UP.
+ */
+#ifdef CONFIG_SMP
+
+static inline unsigned int task_cpu(const struct task_struct *p)
+{
+	return p->thread_info->cpu;
+}
+
+static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
+{
+	p->thread_info->cpu = cpu;
+}
+
+#else
+
+static inline unsigned int task_cpu(const struct task_struct *p)
+{
+	return 0;
+}
+
+static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
+{
+}
+
+#endif /* CONFIG_SMP */
+
+#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT
+extern void arch_pick_mmap_layout(struct mm_struct *mm);
+#else
+static inline void arch_pick_mmap_layout(struct mm_struct *mm)
+{
+	mm->mmap_base = TASK_UNMAPPED_BASE;
+	mm->get_unmapped_area = arch_get_unmapped_area;
+	mm->unmap_area = arch_unmap_area;
+}
+#endif
+
+extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
+extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
+
+#ifdef CONFIG_MAGIC_SYSRQ
+
+extern void normalize_rt_tasks(void);
+
+#endif
+
+/* try_to_freeze
+ *
+ * Checks whether we need to enter the refrigerator
+ * and returns 1 if we did so.
+ */
+#ifdef CONFIG_PM
+extern void refrigerator(unsigned long);
+extern int freeze_processes(void);
+extern void thaw_processes(void);
+
+static inline int try_to_freeze(unsigned long refrigerator_flags)
+{
+	if (unlikely(current->flags & PF_FREEZE)) {
+		refrigerator(refrigerator_flags);
+		return 1;
+	} else
+		return 0;
+}
+#else
+static inline void refrigerator(unsigned long flag) {}
+static inline int freeze_processes(void) { BUG(); return 0; }
+static inline void thaw_processes(void) {}
+
+static inline int try_to_freeze(unsigned long refrigerator_flags)
+{
+	return 0;
+}
+#endif /* CONFIG_PM */
+#endif /* __KERNEL__ */
+
+#endif
