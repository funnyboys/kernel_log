commit 873e65bc09078e56eaa51af2c9c60da2fad6fdbf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:15 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 167
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation version 2 of the license this program
      is distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation inc 59 temple place suite 330 boston ma 02111
      1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 83 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070034.021731668@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 06a66327333d..c066fec5b74b 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -1,20 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Macros for manipulating and testing flags related to a
  * pageblock_nr_pages number of pages.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation version 2 of the License
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
- *
  * Copyright (C) IBM Corporation, 2006
  *
  * Original author, Mel Gorman

commit 125b860b251ad226b1384b6db06be37485127f69
Author: Pingfan Liu <kernelfans@gmail.com>
Date:   Fri Dec 28 00:38:43 2018 -0800

    mm/pageblock: throw compile error if pageblock_bits cannot hold MIGRATE_TYPES
    
    Currently, NR_PAGEBLOCK_BITS and MIGRATE_TYPES are not associated by code.
    If someone adds extra migrate type, then he may forget to enlarge the
    NR_PAGEBLOCK_BITS.  Hence it requires some way to fix.
    
    NR_PAGEBLOCK_BITS depends on MIGRATE_TYPES, while these macro spread on
    two different .h file with reverse dependency, it is a little hard to
    refer to MIGRATE_TYPES in pageblock-flag.h.  This patch tries to remind
    such relation in compiling-time.
    
    Link: http://lkml.kernel.org/r/1544508709-11358-1-git-send-email-kernelfans@gmail.com
    Signed-off-by: Pingfan Liu <kernelfans@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 9132c5cb41f1..06a66327333d 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -25,10 +25,11 @@
 
 #include <linux/types.h>
 
+#define PB_migratetype_bits 3
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
 	PB_migrate,
-	PB_migrate_end = PB_migrate + 3 - 1,
+	PB_migrate_end = PB_migrate + PB_migratetype_bits - 1,
 			/* 3 bits required for migrate types */
 	PB_migrate_skip,/* If set the block is skipped by compaction */
 

commit 21dc7e023611fbcf8e38f255731bcf3cc38e7638
Author: David Rientjes <rientjes@google.com>
Date:   Fri Nov 17 15:26:30 2017 -0800

    mm, compaction: persistently skip hugetlbfs pageblocks
    
    It is pointless to migrate hugetlb memory as part of memory compaction
    if the hugetlb size is equal to the pageblock order.  No defragmentation
    is occurring in this condition.
    
    It is also pointless to for the freeing scanner to scan a pageblock
    where a hugetlb page is pinned.  Unconditionally skip these pageblocks,
    and do so peristently so that they are not rescanned until it is
    observed that these hugepages are no longer pinned.
    
    It would also be possible to do this by involving the hugetlb subsystem
    in marking pageblocks to no longer be skipped when they hugetlb pages
    are freed.  This is a simple solution that doesn't involve any
    additional subsystems in pageblock skip manipulation.
    
    [rientjes@google.com: fix build]
      Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1708201734390.117182@chino.kir.corp.google.com
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1708151639130.106658@chino.kir.corp.google.com
    Signed-off-by: David Rientjes <rientjes@google.com>
    Tested-by: Michal Hocko <mhocko@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index e942558b3585..9132c5cb41f1 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -96,6 +96,17 @@ void set_pfnblock_flags_mask(struct page *page,
 #define set_pageblock_skip(page) \
 			set_pageblock_flags_group(page, 1, PB_migrate_skip,  \
 							PB_migrate_skip)
+#else
+static inline bool get_pageblock_skip(struct page *page)
+{
+	return false;
+}
+static inline void clear_pageblock_skip(struct page *page)
+{
+}
+static inline void set_pageblock_skip(struct page *page)
+{
+}
 #endif /* CONFIG_COMPACTION */
 
 #endif	/* PAGEBLOCK_FLAGS_H */

commit d00181b96eb86c914cb327d1de974a1b71366e1b
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Nov 6 16:29:57 2015 -0800

    mm: use 'unsigned int' for page order
    
    Let's try to be consistent about data type of page order.
    
    [sfr@canb.auug.org.au: fix build (type of pageblock_order)]
    [hughd@google.com: some configs end up with MAX_ORDER and pageblock_order having different types]
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 2baeee12f48e..e942558b3585 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -44,7 +44,7 @@ enum pageblock_bits {
 #ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE
 
 /* Huge page sizes are variable */
-extern int pageblock_order;
+extern unsigned int pageblock_order;
 
 #else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */
 

commit dc4b0caff24d9b2918e9f27bc65499ee63187eba
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:17 2014 -0700

    mm: page_alloc: reduce number of times page_to_pfn is called
    
    In the free path we calculate page_to_pfn multiple times. Reduce that.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index c08730c10c7a..2baeee12f48e 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -65,33 +65,26 @@ extern int pageblock_order;
 /* Forward declaration */
 struct page;
 
-unsigned long get_pageblock_flags_mask(struct page *page,
+unsigned long get_pfnblock_flags_mask(struct page *page,
+				unsigned long pfn,
 				unsigned long end_bitidx,
 				unsigned long mask);
-void set_pageblock_flags_mask(struct page *page,
+
+void set_pfnblock_flags_mask(struct page *page,
 				unsigned long flags,
+				unsigned long pfn,
 				unsigned long end_bitidx,
 				unsigned long mask);
 
 /* Declarations for getting and setting flags. See mm/page_alloc.c */
-static inline unsigned long get_pageblock_flags_group(struct page *page,
-					int start_bitidx, int end_bitidx)
-{
-	unsigned long nr_flag_bits = end_bitidx - start_bitidx + 1;
-	unsigned long mask = (1 << nr_flag_bits) - 1;
-
-	return get_pageblock_flags_mask(page, end_bitidx, mask);
-}
-
-static inline void set_pageblock_flags_group(struct page *page,
-					unsigned long flags,
-					int start_bitidx, int end_bitidx)
-{
-	unsigned long nr_flag_bits = end_bitidx - start_bitidx + 1;
-	unsigned long mask = (1 << nr_flag_bits) - 1;
-
-	set_pageblock_flags_mask(page, flags, end_bitidx, mask);
-}
+#define get_pageblock_flags_group(page, start_bitidx, end_bitidx) \
+	get_pfnblock_flags_mask(page, page_to_pfn(page),		\
+			end_bitidx,					\
+			(1 << (end_bitidx - start_bitidx + 1)) - 1)
+#define set_pageblock_flags_group(page, flags, start_bitidx, end_bitidx) \
+	set_pfnblock_flags_mask(page, flags, page_to_pfn(page),		\
+			end_bitidx,					\
+			(1 << (end_bitidx - start_bitidx + 1)) - 1)
 
 #ifdef CONFIG_COMPACTION
 #define get_pageblock_skip(page) \

commit e58469bafd0524e848c3733bc3918d854595e20f
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:16 2014 -0700

    mm: page_alloc: use word-based accesses for get/set pageblock bitmaps
    
    The test_bit operations in get/set pageblock flags are expensive.  This
    patch reads the bitmap on a word basis and use shifts and masks to isolate
    the bits of interest.  Similarly masks are used to set a local copy of the
    bitmap and then use cmpxchg to update the bitmap if there have been no
    other changes made in parallel.
    
    In a test running dd onto tmpfs the overhead of the pageblock-related
    functions went from 1.27% in profiles to 0.5%.
    
    In addition to the performance benefits, this patch closes races that are
    possible between:
    
    a) get_ and set_pageblock_migratetype(), where get_pageblock_migratetype()
       reads part of the bits before and other part of the bits after
       set_pageblock_migratetype() has updated them.
    
    b) set_pageblock_migratetype() and set_pageblock_skip(), where the non-atomic
       read-modify-update set bit operation in set_pageblock_skip() will cause
       lost updates to some bits changed in the set_pageblock_migratetype().
    
    Joonsoo Kim first reported the case a) via code inspection.  Vlastimil
    Babka's testing with a debug patch showed that either a) or b) occurs
    roughly once per mmtests' stress-highalloc benchmark (although not
    necessarily in the same pageblock).  Furthermore during development of
    unrelated compaction patches, it was observed that frequent calls to
    {start,undo}_isolate_page_range() the race occurs several thousands of
    times and has resulted in NULL pointer dereferences in move_freepages()
    and free_one_page() in places where free_list[migratetype] is
    manipulated by e.g.  list_move().  Further debugging confirmed that
    migratetype had invalid value of 6, causing out of bounds access to the
    free_list array.
    
    That confirmed that the race exist, although it may be extremely rare,
    and currently only fatal where page isolation is performed due to
    memory hot remove.  Races on pageblocks being updated by
    set_pageblock_migratetype(), where both old and new migratetype are
    lower MIGRATE_RESERVE, currently cannot result in an invalid value
    being observed, although theoretically they may still lead to
    unexpected creation or destruction of MIGRATE_RESERVE pageblocks.
    Furthermore, things could get suddenly worse when memory isolation is
    used more, or when new migratetypes are added.
    
    After this patch, the race has no longer been observed in testing.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reported-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reported-and-tested-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 2ee8cd2466b5..c08730c10c7a 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -30,9 +30,12 @@ enum pageblock_bits {
 	PB_migrate,
 	PB_migrate_end = PB_migrate + 3 - 1,
 			/* 3 bits required for migrate types */
-#ifdef CONFIG_COMPACTION
 	PB_migrate_skip,/* If set the block is skipped by compaction */
-#endif /* CONFIG_COMPACTION */
+
+	/*
+	 * Assume the bits will always align on a word. If this assumption
+	 * changes then get/set pageblock needs updating.
+	 */
 	NR_PAGEBLOCK_BITS
 };
 
@@ -62,11 +65,33 @@ extern int pageblock_order;
 /* Forward declaration */
 struct page;
 
+unsigned long get_pageblock_flags_mask(struct page *page,
+				unsigned long end_bitidx,
+				unsigned long mask);
+void set_pageblock_flags_mask(struct page *page,
+				unsigned long flags,
+				unsigned long end_bitidx,
+				unsigned long mask);
+
 /* Declarations for getting and setting flags. See mm/page_alloc.c */
-unsigned long get_pageblock_flags_group(struct page *page,
-					int start_bitidx, int end_bitidx);
-void set_pageblock_flags_group(struct page *page, unsigned long flags,
-					int start_bitidx, int end_bitidx);
+static inline unsigned long get_pageblock_flags_group(struct page *page,
+					int start_bitidx, int end_bitidx)
+{
+	unsigned long nr_flag_bits = end_bitidx - start_bitidx + 1;
+	unsigned long mask = (1 << nr_flag_bits) - 1;
+
+	return get_pageblock_flags_mask(page, end_bitidx, mask);
+}
+
+static inline void set_pageblock_flags_group(struct page *page,
+					unsigned long flags,
+					int start_bitidx, int end_bitidx)
+{
+	unsigned long nr_flag_bits = end_bitidx - start_bitidx + 1;
+	unsigned long mask = (1 << nr_flag_bits) - 1;
+
+	set_pageblock_flags_mask(page, flags, end_bitidx, mask);
+}
 
 #ifdef CONFIG_COMPACTION
 #define get_pageblock_skip(page) \

commit 4c42efa266030f32227f35fabbdd986fcf8f2ec4
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Wed Jul 3 15:02:41 2013 -0700

    mm/pageblock: remove get/set_pageblock_flags
    
    get_pageblock_flags and set_pageblock_flags are not used any more, this
    patch removes them.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index be655e4a2a75..2ee8cd2466b5 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -80,10 +80,4 @@ void set_pageblock_flags_group(struct page *page, unsigned long flags,
 							PB_migrate_skip)
 #endif /* CONFIG_COMPACTION */
 
-#define get_pageblock_flags(page) \
-			get_pageblock_flags_group(page, 0, PB_migrate_end)
-#define set_pageblock_flags(page, flags) \
-			set_pageblock_flags_group(page, flags,	\
-						  0, PB_migrate_end)
-
 #endif	/* PAGEBLOCK_FLAGS_H */

commit 627260595ca6abcb16d68a3732bac6b547e112d6
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Wed Oct 10 15:53:55 2012 -0700

    mm: compaction: fix bit ranges in {get,clear,set}_pageblock_skip()
    
    {get,clear,set}_pageblock_skip() use incorrect bit ranges (please compare
    to bit ranges used by {get,set}_pageblock_flags() used for migration
    types) and can overwrite pageblock migratetype of the next pageblock in
    the bitmap.
    
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Thierry Reding <thierry.reding@avionic-design.de>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index eed27f4f4c3e..be655e4a2a75 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -71,13 +71,13 @@ void set_pageblock_flags_group(struct page *page, unsigned long flags,
 #ifdef CONFIG_COMPACTION
 #define get_pageblock_skip(page) \
 			get_pageblock_flags_group(page, PB_migrate_skip,     \
-							PB_migrate_skip + 1)
+							PB_migrate_skip)
 #define clear_pageblock_skip(page) \
 			set_pageblock_flags_group(page, 0, PB_migrate_skip,  \
-							PB_migrate_skip + 1)
+							PB_migrate_skip)
 #define set_pageblock_skip(page) \
 			set_pageblock_flags_group(page, 1, PB_migrate_skip,  \
-							PB_migrate_skip + 1)
+							PB_migrate_skip)
 #endif /* CONFIG_COMPACTION */
 
 #define get_pageblock_flags(page) \

commit bb13ffeb9f6bfeb301443994dfbf29f91117dfb3
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 8 16:32:41 2012 -0700

    mm: compaction: cache if a pageblock was scanned and no pages were isolated
    
    When compaction was implemented it was known that scanning could
    potentially be excessive.  The ideal was that a counter be maintained for
    each pageblock but maintaining this information would incur a severe
    penalty due to a shared writable cache line.  It has reached the point
    where the scanning costs are a serious problem, particularly on
    long-lived systems where a large process starts and allocates a large
    number of THPs at the same time.
    
    Instead of using a shared counter, this patch adds another bit to the
    pageblock flags called PG_migrate_skip.  If a pageblock is scanned by
    either migrate or free scanner and 0 pages were isolated, the pageblock is
    marked to be skipped in the future.  When scanning, this bit is checked
    before any scanning takes place and the block skipped if set.
    
    The main difficulty with a patch like this is "when to ignore the cached
    information?" If it's ignored too often, the scanning rates will still be
    excessive.  If the information is too stale then allocations will fail
    that might have otherwise succeeded.  In this patch
    
    o CMA always ignores the information
    o If the migrate and free scanner meet then the cached information will
      be discarded if it's at least 5 seconds since the last time the cache
      was discarded
    o If there are a large number of allocation failures, discard the cache.
    
    The time-based heuristic is very clumsy but there are few choices for a
    better event.  Depending solely on multiple allocation failures still
    allows excessive scanning when THP allocations are failing in quick
    succession due to memory pressure.  Waiting until memory pressure is
    relieved would cause compaction to continually fail instead of using
    reclaim/compaction to try allocate the page.  The time-based mechanism is
    clumsy but a better option is not obvious.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Richard Davies <richard@arachsys.com>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Rafael Aquini <aquini@redhat.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 19ef95d293ae..eed27f4f4c3e 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -30,6 +30,9 @@ enum pageblock_bits {
 	PB_migrate,
 	PB_migrate_end = PB_migrate + 3 - 1,
 			/* 3 bits required for migrate types */
+#ifdef CONFIG_COMPACTION
+	PB_migrate_skip,/* If set the block is skipped by compaction */
+#endif /* CONFIG_COMPACTION */
 	NR_PAGEBLOCK_BITS
 };
 
@@ -65,10 +68,22 @@ unsigned long get_pageblock_flags_group(struct page *page,
 void set_pageblock_flags_group(struct page *page, unsigned long flags,
 					int start_bitidx, int end_bitidx);
 
+#ifdef CONFIG_COMPACTION
+#define get_pageblock_skip(page) \
+			get_pageblock_flags_group(page, PB_migrate_skip,     \
+							PB_migrate_skip + 1)
+#define clear_pageblock_skip(page) \
+			set_pageblock_flags_group(page, 0, PB_migrate_skip,  \
+							PB_migrate_skip + 1)
+#define set_pageblock_skip(page) \
+			set_pageblock_flags_group(page, 1, PB_migrate_skip,  \
+							PB_migrate_skip + 1)
+#endif /* CONFIG_COMPACTION */
+
 #define get_pageblock_flags(page) \
-			get_pageblock_flags_group(page, 0, NR_PAGEBLOCK_BITS-1)
+			get_pageblock_flags_group(page, 0, PB_migrate_end)
 #define set_pageblock_flags(page, flags) \
 			set_pageblock_flags_group(page, flags,	\
-						  0, NR_PAGEBLOCK_BITS-1)
+						  0, PB_migrate_end)
 
 #endif	/* PAGEBLOCK_FLAGS_H */

commit f19e77a3dc884510dba740caa6dee126b7d40156
Author: zeal <zealcook@gmail.com>
Date:   Tue Oct 26 14:21:27 2010 -0700

    include/linux/pageblock-flags.h: fix set_pageblock_flags() macro definiton
    
    The presently-unused macro was missing one parameter.
    
    Signed-off-by: zeal <zealcook@gmail.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index e8c06122be36..19ef95d293ae 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -67,7 +67,8 @@ void set_pageblock_flags_group(struct page *page, unsigned long flags,
 
 #define get_pageblock_flags(page) \
 			get_pageblock_flags_group(page, 0, NR_PAGEBLOCK_BITS-1)
-#define set_pageblock_flags(page) \
-			set_pageblock_flags_group(page, 0, NR_PAGEBLOCK_BITS-1)
+#define set_pageblock_flags(page, flags) \
+			set_pageblock_flags_group(page, flags,	\
+						  0, NR_PAGEBLOCK_BITS-1)
 
 #endif	/* PAGEBLOCK_FLAGS_H */

commit c801ed3860fe2f84281d4cae4c3e6ca87e81e890
Author: Paul Jackson <pj@sgi.com>
Date:   Wed May 14 08:15:23 2008 -0700

    x86 boot: simplify pageblock_bits enum declaration
    
    The use of #defines with '##' pre-processor concatenation is a useful
    way to form several symbol names with a common pattern.  But when there
    is just a single name obtained from that #define, it's just obfuscation.
    Better to just write the plain symbol name, as is.
    
    The following patch is a result of my wasting ten minutes looking through
    the kernel to figure out what 'PB_migrate_end' meant, and forgetting what
    I came to do, by the time I figured out that the #define PB_range macro
    defined it.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index e875905f7b12..e8c06122be36 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -25,13 +25,11 @@
 
 #include <linux/types.h>
 
-/* Macro to aid the definition of ranges of bits */
-#define PB_range(name, required_bits) \
-	name, name ## _end = (name + required_bits) - 1
-
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
-	PB_range(PB_migrate, 3), /* 3 bits required for migrate types */
+	PB_migrate,
+	PB_migrate_end = PB_migrate + 3 - 1,
+			/* 3 bits required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit a5d76b54a3f3a40385d7f76069a2feac9f1bad63
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:11 2007 -0700

    memory unplug: page isolation
    
    Implement generic chunk-of-pages isolation method by using page grouping ops.
    
    This patch add MIGRATE_ISOLATE to MIGRATE_TYPES. By this
     - MIGRATE_TYPES increases.
     - bitmap for migratetype is enlarged.
    
    pages of MIGRATE_ISOLATE migratetype will not be allocated even if it is free.
    By this, you can isolated *freed* pages from users. How-to-free pages is not
    a purpose of this patch. You may use reclaim and migrate codes to free pages.
    
    If start_isolate_page_range(start,end) is called,
     - migratetype of the range turns to be MIGRATE_ISOLATE  if
       its type is MIGRATE_MOVABLE. (*) this check can be updated if other
       memory reclaiming works make progress.
     - MIGRATE_ISOLATE is not on migratetype fallback list.
     - All free pages and will-be-freed pages are isolated.
    To check all pages in the range are isolated or not,  use test_pages_isolated(),
    To cancel isolation, use undo_isolate_page_range().
    
    Changes V6 -> V7
     - removed unnecessary #ifdef
    
    There are HOLES_IN_ZONE handling codes...I'm glad if we can remove them..
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 9ea130829fb4..e875905f7b12 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -31,7 +31,7 @@
 
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
-	PB_range(PB_migrate, 2), /* 2 bits required for migrate types */
+	PB_range(PB_migrate, 3), /* 3 bits required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit d9c2340052278d8eb2ffb16b0484f8f794def4de
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:26:01 2007 -0700

    Do not depend on MAX_ORDER when grouping pages by mobility
    
    Currently mobility grouping works at the MAX_ORDER_NR_PAGES level.  This makes
    sense for the majority of users where this is also the huge page size.
    However, on platforms like ia64 where the huge page size is runtime
    configurable it is desirable to group at a lower order.  On x86_64 and
    occasionally on x86, the hugepage size may not always be MAX_ORDER_NR_PAGES.
    
    This patch groups pages together based on the value of HUGETLB_PAGE_ORDER.  It
    uses a compile-time constant if possible and a variable where the huge page
    size is runtime configurable.
    
    It is assumed that grouping should be done at the lowest sensible order and
    that the user would not want to override this.  If this is not true,
    page_block order could be forced to a variable initialised via a boot-time
    kernel parameter.
    
    One potential issue with this patch is that IA64 now parses hugepagesz with
    early_param() instead of __setup().  __setup() is called after the memory
    allocator has been initialised and the pageblock bitmaps already setup.  In
    tests on one IA64 there did not seem to be any problem with using
    early_param() and in fact may be more correct as it guarantees the parameter
    is handled before the parsing of hugepages=.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 5456da6b4ade..9ea130829fb4 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -1,6 +1,6 @@
 /*
  * Macros for manipulating and testing flags related to a
- * MAX_ORDER_NR_PAGES block of pages.
+ * pageblock_nr_pages number of pages.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -35,6 +35,29 @@ enum pageblock_bits {
 	NR_PAGEBLOCK_BITS
 };
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE
+
+/* Huge page sizes are variable */
+extern int pageblock_order;
+
+#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */
+
+/* Huge pages are a constant size */
+#define pageblock_order		HUGETLB_PAGE_ORDER
+
+#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */
+
+#else /* CONFIG_HUGETLB_PAGE */
+
+/* If huge pages are not used, group by MAX_ORDER_NR_PAGES */
+#define pageblock_order		(MAX_ORDER-1)
+
+#endif /* CONFIG_HUGETLB_PAGE */
+
+#define pageblock_nr_pages	(1UL << pageblock_order)
+
 /* Forward declaration */
 struct page;
 

commit 64c5e135bf5a2a7f0ededb3435a31adbe0202f0c
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:59 2007 -0700

    don't group high order atomic allocations
    
    Grouping high-order atomic allocations together was intended to allow
    bursty users of atomic allocations to work such as e1000 in situations
    where their preallocated buffers were depleted.  This did not work in at
    least one case with a wireless network adapter needing order-1 allocations
    frequently.  To resolve that, the free pages used for min_free_kbytes were
    moved to separate contiguous blocks with the patch
    bias-the-location-of-pages-freed-for-min_free_kbytes-in-the-same-max_order_nr_pages-blocks.
    
    It is felt that keeping the free pages in the same contiguous blocks should
    be sufficient for bursty short-lived high-order atomic allocations to
    succeed, maybe even with the e1000.  Even if there is a failure, increasing
    the value of min_free_kbytes will free pages as contiguous bloks in
    contrast to the standard buddy allocator which makes no attempt to keep the
    minimum number of free pages contiguous.
    
    This patch backs out grouping high order atomic allocations together to
    determine if it is really needed or not.  If a new report comes in about
    high-order atomic allocations failing, the feature can be reintroduced to
    determine if it fixes the problem or not.  As a side-effect, this patch
    reduces by 1 the number of bits required to track the mobility type of
    pages within a MAX_ORDER_NR_PAGES block.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index fa3b1001894b..5456da6b4ade 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -31,7 +31,7 @@
 
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
-	PB_range(PB_migrate, 3), /* 3 bits required for migrate types */
+	PB_range(PB_migrate, 2), /* 2 bits required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit 56fd56b868f19385c50af8941a4c78df433b2d32
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:58 2007 -0700

    Bias the location of pages freed for min_free_kbytes in the same MAX_ORDER_NR_PAGES blocks
    
    The standard buddy allocator always favours the smallest block of pages.
    The effect of this is that the pages free to satisfy min_free_kbytes tends
    to be preserved since boot time at the same location of memory ffor a very
    long time and as a contiguous block.  When an administrator sets the
    reserve at 16384 at boot time, it tends to be the same MAX_ORDER blocks
    that remain free.  This allows the occasional high atomic allocation to
    succeed up until the point the blocks are split.  In practice, it is
    difficult to split these blocks but when they do split, the benefit of
    having min_free_kbytes for contiguous blocks disappears.  Additionally,
    increasing min_free_kbytes once the system has been running for some time
    has no guarantee of creating contiguous blocks.
    
    On the other hand, CONFIG_PAGE_GROUP_BY_MOBILITY favours splitting large
    blocks when there are no free pages of the appropriate type available.  A
    side-effect of this is that all blocks in memory tends to be used up and
    the contiguous free blocks from boot time are not preserved like in the
    vanilla allocator.  This can cause a problem if a new caller is unwilling
    to reclaim or does not reclaim for long enough.
    
    A failure scenario was found for a wireless network device allocating
    order-1 atomic allocations but the allocations were not intense or frequent
    enough for a whole block of pages to be preserved for MIGRATE_HIGHALLOC.
    This was reproduced on a desktop by booting with mem=256mb, forcing the
    driver to allocate at order-1, running a bittorrent client (downloading a
    debian ISO) and building a kernel with -j2.
    
    This patch addresses the problem on the desktop machine booted with
    mem=256mb.  It works by setting aside a reserve of MAX_ORDER_NR_PAGES
    blocks, the number of which depends on the value of min_free_kbytes.  These
    blocks are only fallen back to when there is no other free pages.  Then the
    smallest possible page is used just like the normal buddy allocator instead
    of the largest possible page to preserve contiguous pages The pages in free
    lists in the reserve blocks are never taken for another migrate type.  The
    results is that even if min_free_kbytes is set to a low value, contiguous
    blocks will be preserved in the MIGRATE_RESERVE blocks.
    
    This works better than the vanilla allocator because if min_free_kbytes is
    increased, a new reserve block will be chosen based on the location of
    reclaimable pages and the block will free up as contiguous pages.  In the
    vanilla allocator, no effort is made to target a block of pages to free as
    contiguous pages and min_free_kbytes pages are scattered randomly.
    
    This effect has been observed on the test machine.  min_free_kbytes was set
    initially low but it was kept as a contiguous free block within
    MIGRATE_RESERVE.  min_free_kbytes was then set to a higher value and over a
    period of time, the free blocks were within the reserve and coalescing.
    How long it takes to free up depends on how quickly LRU is rotating.
    Amusingly, this means that more activity will free the blocks faster.
    
    This mechanism potentially replaces MIGRATE_HIGHALLOC as it may be more
    effective than grouping contiguous free pages together.  It all depends on
    whether the number of active atomic high allocations exceeds
    min_free_kbytes or not.  If the number of active allocations exceeds
    min_free_kbytes, it's worth it but maybe in that situation, min_free_kbytes
    should be set higher.  Once there are no more reports of allocation
    failures, a patch will be submitted that backs out MIGRATE_HIGHALLOC and
    see if the reports stay missing.
    
    Credit to Mariusz Kozlowski for discovering the problem, describing the
    failure scenario and testing patches and scenarios.
    
    [akpm@linux-foundation.org: cleanups]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 5456da6b4ade..fa3b1001894b 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -31,7 +31,7 @@
 
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
-	PB_range(PB_migrate, 2), /* 2 bits required for migrate types */
+	PB_range(PB_migrate, 3), /* 3 bits required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit e12ba74d8ff3e2f73a583500d7095e406df4d093
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:52 2007 -0700

    Group short-lived and reclaimable kernel allocations
    
    This patch marks a number of allocations that are either short-lived such as
    network buffers or are reclaimable such as inode allocations.  When something
    like updatedb is called, long-lived and unmovable kernel allocations tend to
    be spread throughout the address space which increases fragmentation.
    
    This patch groups these allocations together as much as possible by adding a
    new MIGRATE_TYPE.  The MIGRATE_RECLAIMABLE type is for allocations that can be
    reclaimed on demand, but not moved.  i.e.  they can be migrated by deleting
    them and re-reading the information from elsewhere.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 3619d52a425c..5456da6b4ade 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -31,7 +31,7 @@
 
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
-	PB_range(PB_migrate, 1), /* 1 bit required for migrate types */
+	PB_range(PB_migrate, 2), /* 2 bits required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit b2a0ac8875a0a3b9f0739b60526f8c5977d2200f
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:48 2007 -0700

    Split the free lists for movable and unmovable allocations
    
    This patch adds the core of the fragmentation reduction strategy.  It works by
    grouping pages together based on their ability to migrate or be reclaimed.
    Basically, it works by breaking the list in zone->free_area list into
    MIGRATE_TYPES number of lists.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
index 96b623f9b4db..3619d52a425c 100644
--- a/include/linux/pageblock-flags.h
+++ b/include/linux/pageblock-flags.h
@@ -31,6 +31,7 @@
 
 /* Bit indices that affect a whole block of pages */
 enum pageblock_bits {
+	PB_range(PB_migrate, 1), /* 1 bit required for migrate types */
 	NR_PAGEBLOCK_BITS
 };
 

commit 835c134ec4dd755e5c4470af566db226d1e96742
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Oct 16 01:25:47 2007 -0700

    Add a bitmap that is used to track flags affecting a block of pages
    
    Here is the latest revision of the anti-fragmentation patches.  Of particular
    note in this version is special treatment of high-order atomic allocations.
    Care is taken to group them together and avoid grouping pages of other types
    near them.  Artifical tests imply that it works.  I'm trying to get the
    hardware together that would allow setting up of a "real" test.  If anyone
    already has a setup and test that can trigger the atomic-allocation problem,
    I'd appreciate a test of these patches and a report.  The second major change
    is that these patches will apply cleanly with patches that implement
    anti-fragmentation through zones.
    
    kernbench shows effectively no performance difference varying between -0.2%
    and +2% on a variety of test machines.  Success rates for huge page allocation
    are dramatically increased.  For example, on a ppc64 machine, the vanilla
    kernel was only able to allocate 1% of memory as a hugepage and this was due
    to a single hugepage reserved as min_free_kbytes.  With these patches applied,
    17% was allocatable as superpages.  With reclaim-related fixes from Andy
    Whitcroft, it was 40% and further reclaim-related improvements should increase
    this further.
    
    Changelog Since V28
    o Group high-order atomic allocations together
    o It is no longer required to set min_free_kbytes to 10% of memory. A value
      of 16384 in most cases will be sufficient
    o Now applied with zone-based anti-fragmentation
    o Fix incorrect VM_BUG_ON within buffered_rmqueue()
    o Reorder the stack so later patches do not back out work from earlier patches
    o Fix bug were journal pages were being treated as movable
    o Bias placement of non-movable pages to lower PFNs
    o More agressive clustering of reclaimable pages in reactions to workloads
      like updatedb that flood the size of inode caches
    
    Changelog Since V27
    
    o Renamed anti-fragmentation to Page Clustering. Anti-fragmentation was giving
      the mistaken impression that it was the 100% solution for high order
      allocations. Instead, it greatly increases the chances high-order
      allocations will succeed and lays the foundation for defragmentation and
      memory hot-remove to work properly
    o Redefine page groupings based on ability to migrate or reclaim instead of
      basing on reclaimability alone
    o Get rid of spurious inits
    o Per-cpu lists are no longer split up per-type. Instead the per-cpu list is
      searched for a page of the appropriate type
    o Added more explanation commentary
    o Fix up bug in pageblock code where bitmap was used before being initalised
    
    Changelog Since V26
    o Fix double init of lists in setup_pageset
    
    Changelog Since V25
    o Fix loop order of for_each_rclmtype_order so that order of loop matches args
    o gfpflags_to_rclmtype uses gfp_t instead of unsigned long
    o Rename get_pageblock_type() to get_page_rclmtype()
    o Fix alignment problem in move_freepages()
    o Add mechanism for assigning flags to blocks of pages instead of page->flags
    o On fallback, do not examine the preferred list of free pages a second time
    
    The purpose of these patches is to reduce external fragmentation by grouping
    pages of related types together.  When pages are migrated (or reclaimed under
    memory pressure), large contiguous pages will be freed.
    
    This patch works by categorising allocations by their ability to migrate;
    
    Movable - The pages may be moved with the page migration mechanism. These are
            generally userspace pages.
    
    Reclaimable - These are allocations for some kernel caches that are
            reclaimable or allocations that are known to be very short-lived.
    
    Unmovable - These are pages that are allocated by the kernel that
            are not trivially reclaimed. For example, the memory allocated for a
            loaded module would be in this category. By default, allocations are
            considered to be of this type
    
    HighAtomic - These are high-order allocations belonging to callers that
            cannot sleep or perform any IO. In practice, this is restricted to
            jumbo frame allocation for network receive. It is assumed that the
            allocations are short-lived
    
    Instead of having one MAX_ORDER-sized array of free lists in struct free_area,
    there is one for each type of reclaimability.  Once a 2^MAX_ORDER block of
    pages is split for a type of allocation, it is added to the free-lists for
    that type, in effect reserving it.  Hence, over time, pages of the different
    types can be clustered together.
    
    When the preferred freelists are expired, the largest possible block is taken
    from an alternative list.  Buddies that are split from that large block are
    placed on the preferred allocation-type freelists to mitigate fragmentation.
    
    This implementation gives best-effort for low fragmentation in all zones.
    Ideally, min_free_kbytes needs to be set to a value equal to 4 * (1 <<
    (MAX_ORDER-1)) pages in most cases.  This would be 16384 on x86 and x86_64 for
    example.
    
    Our tests show that about 60-70% of physical memory can be allocated on a
    desktop after a few days uptime.  In benchmarks and stress tests, we are
    finding that 80% of memory is available as contiguous blocks at the end of the
    test.  To compare, a standard kernel was getting < 1% of memory as large pages
    on a desktop and about 8-12% of memory as large pages at the end of stress
    tests.
    
    Following this email are 12 patches that implement thie page grouping feature.
     The first patch introduces a mechanism for storing flags related to a whole
    block of pages.  Then allocations are split between movable and all other
    allocations.  Following that are patches to deal with per-cpu pages and make
    the mechanism configurable.  The next patch moves free pages between lists
    when partially allocated blocks are used for pages of another migrate type.
    The second last patch groups reclaimable kernel allocations such as inode
    caches together.  The final patch related to groupings keeps high-order atomic
    allocations.
    
    The last two patches are more concerned with control of fragmentation.  The
    second last patch biases placement of non-movable allocations towards the
    start of memory.  This is with a view of supporting memory hot-remove of DIMMs
    with higher PFNs in the future.  The biasing could be enforced a lot heavier
    but it would cost.  The last patch agressively clusters reclaimable pages like
    inode caches together.
    
    The fragmentation reduction strategy needs to track if pages within a block
    can be moved or reclaimed so that pages are freed to the appropriate list.
    This patch adds a bitmap for flags affecting a whole a MAX_ORDER block of
    pages.
    
    In non-SPARSEMEM configurations, the bitmap is stored in the struct zone and
    allocated during initialisation.  SPARSEMEM statically allocates the bitmap in
    a struct mem_section so that bitmaps do not have to be resized during memory
    hotadd.  This wastes a small amount of memory per unused section (usually
    sizeof(unsigned long)) but the complexity of dynamically allocating the memory
    is quite high.
    
    Additional credit to Andy Whitcroft who reviewed up an earlier implementation
    of the mechanism an suggested how to make it a *lot* cleaner.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/pageblock-flags.h b/include/linux/pageblock-flags.h
new file mode 100644
index 000000000000..96b623f9b4db
--- /dev/null
+++ b/include/linux/pageblock-flags.h
@@ -0,0 +1,51 @@
+/*
+ * Macros for manipulating and testing flags related to a
+ * MAX_ORDER_NR_PAGES block of pages.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation version 2 of the License
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) IBM Corporation, 2006
+ *
+ * Original author, Mel Gorman
+ * Major cleanups and reduction of bit operations, Andy Whitcroft
+ */
+#ifndef PAGEBLOCK_FLAGS_H
+#define PAGEBLOCK_FLAGS_H
+
+#include <linux/types.h>
+
+/* Macro to aid the definition of ranges of bits */
+#define PB_range(name, required_bits) \
+	name, name ## _end = (name + required_bits) - 1
+
+/* Bit indices that affect a whole block of pages */
+enum pageblock_bits {
+	NR_PAGEBLOCK_BITS
+};
+
+/* Forward declaration */
+struct page;
+
+/* Declarations for getting and setting flags. See mm/page_alloc.c */
+unsigned long get_pageblock_flags_group(struct page *page,
+					int start_bitidx, int end_bitidx);
+void set_pageblock_flags_group(struct page *page, unsigned long flags,
+					int start_bitidx, int end_bitidx);
+
+#define get_pageblock_flags(page) \
+			get_pageblock_flags_group(page, 0, NR_PAGEBLOCK_BITS-1)
+#define set_pageblock_flags(page) \
+			set_pageblock_flags_group(page, 0, NR_PAGEBLOCK_BITS-1)
+
+#endif	/* PAGEBLOCK_FLAGS_H */
