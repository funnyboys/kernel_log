commit 710da3c8ea7dfbd327920afd3831d8c82c42789d
Author: Juri Lelli <juri.lelli@redhat.com>
Date:   Fri Jul 19 16:00:00 2019 +0200

    sched/core: Prevent race condition between cpuset and __sched_setscheduler()
    
    No synchronisation mechanism exists between the cpuset subsystem and
    calls to function __sched_setscheduler(). As such, it is possible that
    new root domains are created on the cpuset side while a deadline
    acceptance test is carried out in __sched_setscheduler(), leading to a
    potential oversell of CPU bandwidth.
    
    Grab cpuset_rwsem read lock from core scheduler, so to prevent
    situations such as the one described above from happening.
    
    The only exception is normalize_rt_tasks() which needs to work under
    tasklist_lock and can't therefore grab cpuset_rwsem. We are fine with
    this, as this function is only called by sysrq and, if that gets
    triggered, DEADLINE guarantees are already gone out of the window
    anyway.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: lizefan@huawei.com
    Cc: longman@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: mathieu.poirier@linaro.org
    Cc: rostedt@goodmis.org
    Cc: tj@kernel.org
    Cc: tommaso.cucinotta@santannapisa.it
    Link: https://lkml.kernel.org/r/20190719140000.31694-9-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 7f1478c26a33..04c20de66afc 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -55,6 +55,8 @@ extern void cpuset_init_smp(void);
 extern void cpuset_force_rebuild(void);
 extern void cpuset_update_active_cpus(void);
 extern void cpuset_wait_for_hotplug(void);
+extern void cpuset_read_lock(void);
+extern void cpuset_read_unlock(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -176,6 +178,9 @@ static inline void cpuset_update_active_cpus(void)
 
 static inline void cpuset_wait_for_hotplug(void) { }
 
+static inline void cpuset_read_lock(void) { }
+static inline void cpuset_read_unlock(void) { }
+
 static inline void cpuset_cpus_allowed(struct task_struct *p,
 				       struct cpumask *mask)
 {

commit d74b27d63a8bebe2fe634944e4ebdc7b10db7a39
Author: Juri Lelli <juri.lelli@redhat.com>
Date:   Fri Jul 19 15:59:58 2019 +0200

    cgroup/cpuset: Change cpuset_rwsem and hotplug lock order
    
    cpuset_rwsem is going to be acquired from sched_setscheduler() with a
    following patch. There are however paths (e.g., spawn_ksoftirqd) in
    which sched_scheduler() is eventually called while holding hotplug lock;
    this creates a dependecy between hotplug lock (to be always acquired
    first) and cpuset_rwsem (to be always acquired after hotplug lock).
    
    Fix paths which currently take the two locks in the wrong order (after
    a following patch is applied).
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: lizefan@huawei.com
    Cc: longman@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: mathieu.poirier@linaro.org
    Cc: rostedt@goodmis.org
    Cc: tj@kernel.org
    Cc: tommaso.cucinotta@santannapisa.it
    Link: https://lkml.kernel.org/r/20190719140000.31694-7-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 934633a05d20..7f1478c26a33 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -40,14 +40,14 @@ static inline bool cpusets_enabled(void)
 
 static inline void cpuset_inc(void)
 {
-	static_branch_inc(&cpusets_pre_enable_key);
-	static_branch_inc(&cpusets_enabled_key);
+	static_branch_inc_cpuslocked(&cpusets_pre_enable_key);
+	static_branch_inc_cpuslocked(&cpusets_enabled_key);
 }
 
 static inline void cpuset_dec(void)
 {
-	static_branch_dec(&cpusets_enabled_key);
-	static_branch_dec(&cpusets_pre_enable_key);
+	static_branch_dec_cpuslocked(&cpusets_enabled_key);
+	static_branch_dec_cpuslocked(&cpusets_pre_enable_key);
 }
 
 extern int cpuset_init(void);

commit 77ef80c65ab72e57cfc273b2dd1d48a282b75146
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Tue Feb 6 15:41:24 2018 -0800

    kernel/cpuset: current_cpuset_is_being_rebound can be boolean
    
    Make current_cpuset_is_being_rebound return bool due to this particular
    function only using either one or zero as its return value.
    
    No functional change.
    
    Link: http://lkml.kernel.org/r/1513266622-15860-4-git-send-email-baiyaowei@cmss.chinamobile.com
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 1b8e41597ef5..934633a05d20 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -112,7 +112,7 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return task_spread_slab(current);
 }
 
-extern int current_cpuset_is_being_rebound(void);
+extern bool current_cpuset_is_being_rebound(void);
 
 extern void rebuild_sched_domains(void);
 
@@ -247,9 +247,9 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return 0;
 }
 
-static inline int current_cpuset_is_being_rebound(void)
+static inline bool current_cpuset_is_being_rebound(void)
 {
-	return 0;
+	return false;
 }
 
 static inline void rebuild_sched_domains(void)

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index a1e6a33a4b03..1b8e41597ef5 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_CPUSET_H
 #define _LINUX_CPUSET_H
 /*

commit 50e76632339d4655859523a39249dd95ee5e93e7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 7 11:13:38 2017 +0200

    sched/cpuset/pm: Fix cpuset vs. suspend-resume bugs
    
    Cpusets vs. suspend-resume is _completely_ broken. And it got noticed
    because it now resulted in non-cpuset usage breaking too.
    
    On suspend cpuset_cpu_inactive() doesn't call into
    cpuset_update_active_cpus() because it doesn't want to move tasks about,
    there is no need, all tasks are frozen and won't run again until after
    we've resumed everything.
    
    But this means that when we finally do call into
    cpuset_update_active_cpus() after resuming the last frozen cpu in
    cpuset_cpu_active(), the top_cpuset will not have any difference with
    the cpu_active_mask and this it will not in fact do _anything_.
    
    So the cpuset configuration will not be restored. This was largely
    hidden because we would unconditionally create identity domains and
    mobile users would not in fact use cpusets much. And servers what do use
    cpusets tend to not suspend-resume much.
    
    An addition problem is that we'd not in fact wait for the cpuset work to
    finish before resuming the tasks, allowing spurious migrations outside
    of the specified domains.
    
    Fix the rebuild by introducing cpuset_force_rebuild() and fix the
    ordering with cpuset_wait_for_hotplug().
    
    Reported-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: deb7aa308ea2 ("cpuset: reorganize CPU / memory hotplug handling")
    Link: http://lkml.kernel.org/r/20170907091338.orwxrqkbfkki3c24@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index e74655d941b7..a1e6a33a4b03 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -51,7 +51,9 @@ static inline void cpuset_dec(void)
 
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
+extern void cpuset_force_rebuild(void);
 extern void cpuset_update_active_cpus(void);
+extern void cpuset_wait_for_hotplug(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -164,11 +166,15 @@ static inline bool cpusets_enabled(void) { return false; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
+static inline void cpuset_force_rebuild(void) { }
+
 static inline void cpuset_update_active_cpus(void)
 {
 	partition_sched_domains(1, NULL, NULL);
 }
 
+static inline void cpuset_wait_for_hotplug(void) { }
+
 static inline void cpuset_cpus_allowed(struct task_struct *p,
 				       struct cpumask *mask)
 {

commit be040bea9085a9c2b1700c9e60888777baeb96d5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Aug 1 17:24:06 2017 +0200

    cpuset: Make nr_cpusets private
    
    Any use of key->enabled (that is static_key_enabled and static_key_count)
    outside jump_label_lock should handle its own serialization.  In the case
    of cpusets_enabled_key, the key is always incremented/decremented under
    cpuset_mutex, and hence the same rule applies to nr_cpusets.  The rule
    *is* respected currently, but the mutex is static so nr_cpusets should
    be static too.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1501601046-35683-4-git-send-email-pbonzini@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 898cfe2eeb42..e74655d941b7 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -37,12 +37,6 @@ static inline bool cpusets_enabled(void)
 	return static_branch_unlikely(&cpusets_enabled_key);
 }
 
-static inline int nr_cpusets(void)
-{
-	/* jump label reference count + the top-level cpuset */
-	return static_key_count(&cpusets_enabled_key.key) + 1;
-}
-
 static inline void cpuset_inc(void)
 {
 	static_branch_inc(&cpusets_pre_enable_key);

commit 89affbf5d9ebb15c6460596822e8857ea2f9e735
Author: Dima Zavin <dmitriyz@waymo.com>
Date:   Wed Aug 2 13:32:18 2017 -0700

    cpuset: fix a deadlock due to incomplete patching of cpusets_enabled()
    
    In codepaths that use the begin/retry interface for reading
    mems_allowed_seq with irqs disabled, there exists a race condition that
    stalls the patch process after only modifying a subset of the
    static_branch call sites.
    
    This problem manifested itself as a deadlock in the slub allocator,
    inside get_any_partial.  The loop reads mems_allowed_seq value (via
    read_mems_allowed_begin), performs the defrag operation, and then
    verifies the consistency of mem_allowed via the read_mems_allowed_retry
    and the cookie returned by xxx_begin.
    
    The issue here is that both begin and retry first check if cpusets are
    enabled via cpusets_enabled() static branch.  This branch can be
    rewritted dynamically (via cpuset_inc) if a new cpuset is created.  The
    x86 jump label code fully synchronizes across all CPUs for every entry
    it rewrites.  If it rewrites only one of the callsites (specifically the
    one in read_mems_allowed_retry) and then waits for the
    smp_call_function(do_sync_core) to complete while a CPU is inside the
    begin/retry section with IRQs off and the mems_allowed value is changed,
    we can hang.
    
    This is because begin() will always return 0 (since it wasn't patched
    yet) while retry() will test the 0 against the actual value of the seq
    counter.
    
    The fix is to use two different static keys: one for begin
    (pre_enable_key) and one for retry (enable_key).  In cpuset_inc(), we
    first bump the pre_enable key to ensure that cpuset_mems_allowed_begin()
    always return a valid seqcount if are enabling cpusets.  Similarly, when
    disabling cpusets via cpuset_dec(), we first ensure that callers of
    cpuset_mems_allowed_retry() will start ignoring the seqcount value
    before we let cpuset_mems_allowed_begin() return 0.
    
    The relevant stack traces of the two stuck threads:
    
      CPU: 1 PID: 1415 Comm: mkdir Tainted: G L  4.9.36-00104-g540c51286237 #4
      Hardware name: Default string Default string/Hardware, BIOS 4.29.1-20170526215256 05/26/2017
      task: ffff8817f9c28000 task.stack: ffffc9000ffa4000
      RIP: smp_call_function_many+0x1f9/0x260
      Call Trace:
        smp_call_function+0x3b/0x70
        on_each_cpu+0x2f/0x90
        text_poke_bp+0x87/0xd0
        arch_jump_label_transform+0x93/0x100
        __jump_label_update+0x77/0x90
        jump_label_update+0xaa/0xc0
        static_key_slow_inc+0x9e/0xb0
        cpuset_css_online+0x70/0x2e0
        online_css+0x2c/0xa0
        cgroup_apply_control_enable+0x27f/0x3d0
        cgroup_mkdir+0x2b7/0x420
        kernfs_iop_mkdir+0x5a/0x80
        vfs_mkdir+0xf6/0x1a0
        SyS_mkdir+0xb7/0xe0
        entry_SYSCALL_64_fastpath+0x18/0xad
    
      ...
    
      CPU: 2 PID: 1 Comm: init Tainted: G L  4.9.36-00104-g540c51286237 #4
      Hardware name: Default string Default string/Hardware, BIOS 4.29.1-20170526215256 05/26/2017
      task: ffff8818087c0000 task.stack: ffffc90000030000
      RIP: int3+0x39/0x70
      Call Trace:
        <#DB> ? ___slab_alloc+0x28b/0x5a0
        <EOE> ? copy_process.part.40+0xf7/0x1de0
        __slab_alloc.isra.80+0x54/0x90
        copy_process.part.40+0xf7/0x1de0
        copy_process.part.40+0xf7/0x1de0
        kmem_cache_alloc_node+0x8a/0x280
        copy_process.part.40+0xf7/0x1de0
        _do_fork+0xe7/0x6c0
        _raw_spin_unlock_irq+0x2d/0x60
        trace_hardirqs_on_caller+0x136/0x1d0
        entry_SYSCALL_64_fastpath+0x5/0xad
        do_syscall_64+0x27/0x350
        SyS_clone+0x19/0x20
        do_syscall_64+0x60/0x350
        entry_SYSCALL64_slow_path+0x25/0x25
    
    Link: http://lkml.kernel.org/r/20170731040113.14197-1-dmitriyz@waymo.com
    Fixes: 46e700abc44c ("mm, page_alloc: remove unnecessary taking of a seqlock when cpusets are disabled")
    Signed-off-by: Dima Zavin <dmitriyz@waymo.com>
    Reported-by: Cliff Spradlin <cspradlin@waymo.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 119a3f9604b0..898cfe2eeb42 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -18,6 +18,19 @@
 
 #ifdef CONFIG_CPUSETS
 
+/*
+ * Static branch rewrites can happen in an arbitrary order for a given
+ * key. In code paths where we need to loop with read_mems_allowed_begin() and
+ * read_mems_allowed_retry() to get a consistent view of mems_allowed, we need
+ * to ensure that begin() always gets rewritten before retry() in the
+ * disabled -> enabled transition. If not, then if local irqs are disabled
+ * around the loop, we can deadlock since retry() would always be
+ * comparing the latest value of the mems_allowed seqcount against 0 as
+ * begin() still would see cpusets_enabled() as false. The enabled -> disabled
+ * transition should happen in reverse order for the same reasons (want to stop
+ * looking at real value of mems_allowed.sequence in retry() first).
+ */
+extern struct static_key_false cpusets_pre_enable_key;
 extern struct static_key_false cpusets_enabled_key;
 static inline bool cpusets_enabled(void)
 {
@@ -32,12 +45,14 @@ static inline int nr_cpusets(void)
 
 static inline void cpuset_inc(void)
 {
+	static_branch_inc(&cpusets_pre_enable_key);
 	static_branch_inc(&cpusets_enabled_key);
 }
 
 static inline void cpuset_dec(void)
 {
 	static_branch_dec(&cpusets_enabled_key);
+	static_branch_dec(&cpusets_pre_enable_key);
 }
 
 extern int cpuset_init(void);
@@ -115,7 +130,7 @@ extern void cpuset_print_current_mems_allowed(void);
  */
 static inline unsigned int read_mems_allowed_begin(void)
 {
-	if (!cpusets_enabled())
+	if (!static_branch_unlikely(&cpusets_pre_enable_key))
 		return 0;
 
 	return read_seqcount_begin(&current->mems_allowed_seq);
@@ -129,7 +144,7 @@ static inline unsigned int read_mems_allowed_begin(void)
  */
 static inline bool read_mems_allowed_retry(unsigned int seq)
 {
-	if (!cpusets_enabled())
+	if (!static_branch_unlikely(&cpusets_enabled_key))
 		return false;
 
 	return read_seqcount_retry(&current->mems_allowed_seq, seq);

commit 30e03acda5fd9c77ec9bf8b3c5def9540c6b0486
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Sun Apr 9 07:36:14 2017 +0600

    cpuset: Remove cpuset_update_active_cpus()'s parameter.
    
    In cpuset_update_active_cpus(), cpu_online isn't used anymore. Remove
    it.
    
    Signed-off-by: Rakib Mullick<rakib.mullick@gmail.com>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 611fce58d670..119a3f9604b0 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -42,7 +42,7 @@ static inline void cpuset_dec(void)
 
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
-extern void cpuset_update_active_cpus(bool cpu_online);
+extern void cpuset_update_active_cpus(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -155,7 +155,7 @@ static inline bool cpusets_enabled(void) { return false; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
-static inline void cpuset_update_active_cpus(bool cpu_online)
+static inline void cpuset_update_active_cpus(void)
 {
 	partition_sched_domains(1, NULL, NULL);
 }

commit f719ff9bcee2a422647790f12d53d3755f47c727
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 10:57:33 2017 +0100

    sched/headers: Prepare to move the task_lock()/unlock() APIs to <linux/sched/task.h>
    
    But first update the code that uses these facilities with the
    new header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index c608c39cb161..611fce58d670 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -10,6 +10,7 @@
 
 #include <linux/sched.h>
 #include <linux/sched/topology.h>
+#include <linux/sched/task.h>
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
 #include <linux/mm.h>

commit 105ab3d8ce7269887d24d224054677125e18037c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/topology.h>
    
    We are going to split <linux/sched/topology.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/topology.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index bfc204e70338..c608c39cb161 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -9,6 +9,7 @@
  */
 
 #include <linux/sched.h>
+#include <linux/sched/topology.h>
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
 #include <linux/mm.h>

commit 002f290627c27068087f6204baec7a334e5a3b48
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu May 19 17:14:30 2016 -0700

    cpuset: use static key better and convert to new API
    
    An important function for cpusets is cpuset_node_allowed(), which
    optimizes on the fact if there's a single root CPU set, it must be
    trivially allowed.  But the check "nr_cpusets() <= 1" doesn't use the
    cpusets_enabled_key static key the right way where static keys eliminate
    branching overhead with jump labels.
    
    This patch converts it so that static key is used properly.  It's also
    switched to the new static key API and the checking functions are
    converted to return bool instead of int.  We also provide a new variant
    __cpuset_zone_allowed() which expects that the static key check was
    already done and they key was enabled.  This is needed for
    get_page_from_freelist() where we want to also avoid the relatively
    slower check when ALLOC_CPUSET is not set in alloc_flags.
    
    The impact on the page allocator microbenchmark is less than expected
    but the cleanup in itself is worthwhile.
    
                                                 4.6.0-rc2                  4.6.0-rc2
                                           multcheck-v1r20               cpuset-v1r20
      Min      alloc-odr0-1               348.00 (  0.00%)           348.00 (  0.00%)
      Min      alloc-odr0-2               254.00 (  0.00%)           254.00 (  0.00%)
      Min      alloc-odr0-4               213.00 (  0.00%)           213.00 (  0.00%)
      Min      alloc-odr0-8               186.00 (  0.00%)           183.00 (  1.61%)
      Min      alloc-odr0-16              173.00 (  0.00%)           171.00 (  1.16%)
      Min      alloc-odr0-32              166.00 (  0.00%)           163.00 (  1.81%)
      Min      alloc-odr0-64              162.00 (  0.00%)           159.00 (  1.85%)
      Min      alloc-odr0-128             160.00 (  0.00%)           157.00 (  1.88%)
      Min      alloc-odr0-256             169.00 (  0.00%)           166.00 (  1.78%)
      Min      alloc-odr0-512             180.00 (  0.00%)           180.00 (  0.00%)
      Min      alloc-odr0-1024            188.00 (  0.00%)           187.00 (  0.53%)
      Min      alloc-odr0-2048            194.00 (  0.00%)           193.00 (  0.52%)
      Min      alloc-odr0-4096            199.00 (  0.00%)           198.00 (  0.50%)
      Min      alloc-odr0-8192            202.00 (  0.00%)           201.00 (  0.50%)
      Min      alloc-odr0-16384           203.00 (  0.00%)           202.00 (  0.49%)
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 85a868ccb493..bfc204e70338 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -16,26 +16,26 @@
 
 #ifdef CONFIG_CPUSETS
 
-extern struct static_key cpusets_enabled_key;
+extern struct static_key_false cpusets_enabled_key;
 static inline bool cpusets_enabled(void)
 {
-	return static_key_false(&cpusets_enabled_key);
+	return static_branch_unlikely(&cpusets_enabled_key);
 }
 
 static inline int nr_cpusets(void)
 {
 	/* jump label reference count + the top-level cpuset */
-	return static_key_count(&cpusets_enabled_key) + 1;
+	return static_key_count(&cpusets_enabled_key.key) + 1;
 }
 
 static inline void cpuset_inc(void)
 {
-	static_key_slow_inc(&cpusets_enabled_key);
+	static_branch_inc(&cpusets_enabled_key);
 }
 
 static inline void cpuset_dec(void)
 {
-	static_key_slow_dec(&cpusets_enabled_key);
+	static_branch_dec(&cpusets_enabled_key);
 }
 
 extern int cpuset_init(void);
@@ -48,16 +48,25 @@ extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
 int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);
 
-extern int __cpuset_node_allowed(int node, gfp_t gfp_mask);
+extern bool __cpuset_node_allowed(int node, gfp_t gfp_mask);
 
-static inline int cpuset_node_allowed(int node, gfp_t gfp_mask)
+static inline bool cpuset_node_allowed(int node, gfp_t gfp_mask)
 {
-	return nr_cpusets() <= 1 || __cpuset_node_allowed(node, gfp_mask);
+	if (cpusets_enabled())
+		return __cpuset_node_allowed(node, gfp_mask);
+	return true;
 }
 
-static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+static inline bool __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
 {
-	return cpuset_node_allowed(zone_to_nid(z), gfp_mask);
+	return __cpuset_node_allowed(zone_to_nid(z), gfp_mask);
+}
+
+static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+{
+	if (cpusets_enabled())
+		return __cpuset_zone_allowed(z, gfp_mask);
+	return true;
 }
 
 extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
@@ -172,14 +181,19 @@ static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
 	return 1;
 }
 
-static inline int cpuset_node_allowed(int node, gfp_t gfp_mask)
+static inline bool cpuset_node_allowed(int node, gfp_t gfp_mask)
 {
-	return 1;
+	return true;
 }
 
-static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+static inline bool __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
 {
-	return 1;
+	return true;
+}
+
+static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+{
+	return true;
 }
 
 static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,

commit 5cf1cacb49aee39c3e02ae87068fc3c6430659b0
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Apr 21 19:06:48 2016 -0400

    cgroup, cpuset: replace cpuset_post_attach_flush() with cgroup_subsys->post_attach callback
    
    Since e93ad19d0564 ("cpuset: make mm migration asynchronous"), cpuset
    kicks off asynchronous NUMA node migration if necessary during task
    migration and flushes it from cpuset_post_attach_flush() which is
    called at the end of __cgroup_procs_write().  This is to avoid
    performing migration with cgroup_threadgroup_rwsem write-locked which
    can lead to deadlock through dependency on kworker creation.
    
    memcg has a similar issue with charge moving, so let's convert it to
    an official callback rather than the current one-off cpuset specific
    function.  This patch adds cgroup_subsys->post_attach callback and
    makes cpuset register cpuset_post_attach_flush() as its ->post_attach.
    
    The conversion is mostly one-to-one except that the new callback is
    called under cgroup_mutex.  This is to guarantee that no other
    migration operations are started before ->post_attach callbacks are
    finished.  cgroup_mutex is one of the outermost mutex in the system
    and has never been and shouldn't be a problem.  We can add specialized
    synchronization around __cgroup_procs_write() but I don't think
    there's any noticeable benefit.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: <stable@vger.kernel.org> # 4.4+ prerequisite for the next patch

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index fea160ee5803..85a868ccb493 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -137,8 +137,6 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 	task_unlock(current);
 }
 
-extern void cpuset_post_attach_flush(void);
-
 #else /* !CONFIG_CPUSETS */
 
 static inline bool cpusets_enabled(void) { return false; }
@@ -245,10 +243,6 @@ static inline bool read_mems_allowed_retry(unsigned int seq)
 	return false;
 }
 
-static inline void cpuset_post_attach_flush(void)
-{
-}
-
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit e93ad19d05648397ef3bcb838d26aec06c245dc0
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 19 12:18:41 2016 -0500

    cpuset: make mm migration asynchronous
    
    If "cpuset.memory_migrate" is set, when a process is moved from one
    cpuset to another with a different memory node mask, pages in used by
    the process are migrated to the new set of nodes.  This was performed
    synchronously in the ->attach() callback, which is synchronized
    against process management.  Recently, the synchronization was changed
    from per-process rwsem to global percpu rwsem for simplicity and
    optimization.
    
    Combined with the synchronous mm migration, this led to deadlocks
    because mm migration could schedule a work item which may in turn try
    to create a new worker blocking on the process management lock held
    from cgroup process migration path.
    
    This heavy an operation shouldn't be performed synchronously from that
    deep inside cgroup migration in the first place.  This patch punts the
    actual migration to an ordered workqueue and updates cgroup process
    migration and cpuset config update paths to flush the workqueue after
    all locks are released.  This way, the operations still seem
    synchronous to userland without entangling mm migration with process
    management synchronization.  CPU hotplug can also invoke mm migration
    but there's no reason for it to wait for mm migrations and thus
    doesn't synchronize against their completions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org # v4.4+

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 85a868ccb493..fea160ee5803 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -137,6 +137,8 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 	task_unlock(current);
 }
 
+extern void cpuset_post_attach_flush(void);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline bool cpusets_enabled(void) { return false; }
@@ -243,6 +245,10 @@ static inline bool read_mems_allowed_retry(unsigned int seq)
 	return false;
 }
 
+static inline void cpuset_post_attach_flush(void)
+{
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit 46e700abc44ce215acb4341d9702ce3972eda571
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:15 2015 -0800

    mm, page_alloc: remove unnecessary taking of a seqlock when cpusets are disabled
    
    There is a seqcounter that protects against spurious allocation failures
    when a task is changing the allowed nodes in a cpuset.  There is no need
    to check the seqcounter until a cpuset exists.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 5a1311942358..85a868ccb493 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -104,6 +104,9 @@ extern void cpuset_print_current_mems_allowed(void);
  */
 static inline unsigned int read_mems_allowed_begin(void)
 {
+	if (!cpusets_enabled())
+		return 0;
+
 	return read_seqcount_begin(&current->mems_allowed_seq);
 }
 
@@ -115,6 +118,9 @@ static inline unsigned int read_mems_allowed_begin(void)
  */
 static inline bool read_mems_allowed_retry(unsigned int seq)
 {
+	if (!cpusets_enabled())
+		return false;
+
 	return read_seqcount_retry(&current->mems_allowed_seq, seq);
 }
 

commit da39da3a54fed88e29024f2f1f6cd7357cd03a44
Author: David Rientjes <rientjes@google.com>
Date:   Thu Nov 5 18:48:05 2015 -0800

    mm, oom: remove task_lock protecting comm printing
    
    The oom killer takes task_lock() in a couple of places solely to protect
    printing the task's comm.
    
    A process's comm, including current's comm, may change due to
    /proc/pid/comm or PR_SET_NAME.
    
    The comm will always be NULL-terminated, so the worst race scenario would
    only be during update.  We can tolerate a comm being printed that is in
    the middle of an update to avoid taking the lock.
    
    Other locations in the kernel have already dropped task_lock() when
    printing comm, so this is consistent.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 1b357997cac5..5a1311942358 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -93,7 +93,7 @@ extern int current_cpuset_is_being_rebound(void);
 
 extern void rebuild_sched_domains(void);
 
-extern void cpuset_print_task_mems_allowed(struct task_struct *p);
+extern void cpuset_print_current_mems_allowed(void);
 
 /*
  * read_mems_allowed_begin is required when making decisions involving
@@ -219,7 +219,7 @@ static inline void rebuild_sched_domains(void)
 	partition_sched_domains(1, NULL, NULL);
 }
 
-static inline void cpuset_print_task_mems_allowed(struct task_struct *p)
+static inline void cpuset_print_current_mems_allowed(void)
 {
 }
 

commit 344736f29b359790facd0b7a521e367f1715c11c
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Mon Oct 20 15:50:30 2014 +0400

    cpuset: simplify cpuset_node_allowed API
    
    Current cpuset API for checking if a zone/node is allowed to allocate
    from looks rather awkward. We have hardwall and softwall versions of
    cpuset_node_allowed with the softwall version doing literally the same
    as the hardwall version if __GFP_HARDWALL is passed to it in gfp flags.
    If it isn't, the softwall version may check the given node against the
    enclosing hardwall cpuset, which it needs to take the callback lock to
    do.
    
    Such a distinction was introduced by commit 02a0e53d8227 ("cpuset:
    rework cpuset_zone_allowed api"). Before, we had the only version with
    the __GFP_HARDWALL flag determining its behavior. The purpose of the
    commit was to avoid sleep-in-atomic bugs when someone would mistakenly
    call the function without the __GFP_HARDWALL flag for an atomic
    allocation. The suffixes introduced were intended to make the callers
    think before using the function.
    
    However, since the callback lock was converted from mutex to spinlock by
    the previous patch, the softwall check function cannot sleep, and these
    precautions are no longer necessary.
    
    So let's simplify the API back to the single check.
    
    Suggested-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 2f073db7392e..1b357997cac5 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -48,29 +48,16 @@ extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
 int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);
 
-extern int __cpuset_node_allowed_softwall(int node, gfp_t gfp_mask);
-extern int __cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask);
+extern int __cpuset_node_allowed(int node, gfp_t gfp_mask);
 
-static inline int cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
+static inline int cpuset_node_allowed(int node, gfp_t gfp_mask)
 {
-	return nr_cpusets() <= 1 ||
-		__cpuset_node_allowed_softwall(node, gfp_mask);
+	return nr_cpusets() <= 1 || __cpuset_node_allowed(node, gfp_mask);
 }
 
-static inline int cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)
+static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
 {
-	return nr_cpusets() <= 1 ||
-		__cpuset_node_allowed_hardwall(node, gfp_mask);
-}
-
-static inline int cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
-{
-	return cpuset_node_allowed_softwall(zone_to_nid(z), gfp_mask);
-}
-
-static inline int cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
-{
-	return cpuset_node_allowed_hardwall(zone_to_nid(z), gfp_mask);
+	return cpuset_node_allowed(zone_to_nid(z), gfp_mask);
 }
 
 extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
@@ -179,22 +166,12 @@ static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
 	return 1;
 }
 
-static inline int cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
-{
-	return 1;
-}
-
-static inline int cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)
-{
-	return 1;
-}
-
-static inline int cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
+static inline int cpuset_node_allowed(int node, gfp_t gfp_mask)
 {
 	return 1;
 }
 
-static inline int cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
+static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
 {
 	return 1;
 }

commit b211e9d7c861bdb37b86d6384da9edfb80949ceb
Merge: d9428f09763d e756c7b69860
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 10 07:24:40 2014 -0400

    Merge branch 'for-3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Nothing too interesting.  Just a handful of cleanup patches"
    
    * 'for-3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      Revert "cgroup: remove redundant variable in cgroup_mount()"
      cgroup: remove redundant variable in cgroup_mount()
      cgroup: fix missing unlock in cgroup_release_agent()
      cgroup: remove CGRP_RELEASABLE flag
      perf/cgroup: Remove perf_put_cgroup()
      cgroup: remove redundant check in cgroup_ino()
      cpuset: simplify proc_cpuset_show()
      cgroup: simplify proc_cgroup_show()
      cgroup: use a per-cgroup work for release agent
      cgroup: remove bogus comments
      cgroup: remove redundant code in cgroup_rmdir()
      cgroup: remove some useless forward declarations
      cgroup: fix a typo in comment.

commit 2ad654bc5e2b211e92f66da1d819e47d79a866f0
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 25 09:41:02 2014 +0800

    cpuset: PF_SPREAD_PAGE and PF_SPREAD_SLAB should be atomic flags
    
    When we change cpuset.memory_spread_{page,slab}, cpuset will flip
    PF_SPREAD_{PAGE,SLAB} bit of tsk->flags for each task in that cpuset.
    This should be done using atomic bitops, but currently we don't,
    which is broken.
    
    Tetsuo reported a hard-to-reproduce kernel crash on RHEL6, which happened
    when one thread tried to clear PF_USED_MATH while at the same time another
    thread tried to flip PF_SPREAD_PAGE/PF_SPREAD_SLAB. They both operate on
    the same task.
    
    Here's the full report:
    https://lkml.org/lkml/2014/9/19/230
    
    To fix this, we make PF_SPREAD_PAGE and PF_SPREAD_SLAB atomic flags.
    
    v4:
    - updated mm/slab.c. (Fengguang Wu)
    - updated Documentation.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Kees Cook <keescook@chromium.org>
    Fixes: 950592f7b991 ("cpusets: update tasks' page/slab spread flags in time")
    Cc: <stable@vger.kernel.org> # 2.6.31+
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index ade2390ffe92..6e39c9bb0dae 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -93,12 +93,12 @@ extern int cpuset_slab_spread_node(void);
 
 static inline int cpuset_do_page_mem_spread(void)
 {
-	return current->flags & PF_SPREAD_PAGE;
+	return task_spread_page(current);
 }
 
 static inline int cpuset_do_slab_mem_spread(void)
 {
-	return current->flags & PF_SPREAD_SLAB;
+	return task_spread_slab(current);
 }
 
 extern int current_cpuset_is_being_rebound(void);

commit 52de4779f201758ddcf37360f09a16895756e708
Author: Zefan Li <lizefan@huawei.com>
Date:   Thu Sep 18 16:03:36 2014 +0800

    cpuset: simplify proc_cpuset_show()
    
    Use the ONE macro instead of REG, and we can simplify proc_cpuset_show().
    
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index ade2390ffe92..0d4e0675b318 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -86,7 +86,8 @@ extern void __cpuset_memory_pressure_bump(void);
 
 extern void cpuset_task_status_allowed(struct seq_file *m,
 					struct task_struct *task);
-extern int proc_cpuset_show(struct seq_file *, void *);
+extern int proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,
+			    struct pid *pid, struct task_struct *tsk);
 
 extern int cpuset_mem_spread_node(void);
 extern int cpuset_slab_spread_node(void);

commit 664eeddeef6539247691197c1ac124d4aa872ab6
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Jun 4 16:10:08 2014 -0700

    mm: page_alloc: use jump labels to avoid checking number_of_cpusets
    
    If cpusets are not in use then we still check a global variable on every
    page allocation.  Use jump labels to avoid the overhead.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index b19d3dc2e651..ade2390ffe92 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -12,10 +12,31 @@
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
 #include <linux/mm.h>
+#include <linux/jump_label.h>
 
 #ifdef CONFIG_CPUSETS
 
-extern int number_of_cpusets;	/* How many cpusets are defined in system? */
+extern struct static_key cpusets_enabled_key;
+static inline bool cpusets_enabled(void)
+{
+	return static_key_false(&cpusets_enabled_key);
+}
+
+static inline int nr_cpusets(void)
+{
+	/* jump label reference count + the top-level cpuset */
+	return static_key_count(&cpusets_enabled_key) + 1;
+}
+
+static inline void cpuset_inc(void)
+{
+	static_key_slow_inc(&cpusets_enabled_key);
+}
+
+static inline void cpuset_dec(void)
+{
+	static_key_slow_dec(&cpusets_enabled_key);
+}
 
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
@@ -32,13 +53,13 @@ extern int __cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask);
 
 static inline int cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
 {
-	return number_of_cpusets <= 1 ||
+	return nr_cpusets() <= 1 ||
 		__cpuset_node_allowed_softwall(node, gfp_mask);
 }
 
 static inline int cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)
 {
-	return number_of_cpusets <= 1 ||
+	return nr_cpusets() <= 1 ||
 		__cpuset_node_allowed_hardwall(node, gfp_mask);
 }
 
@@ -124,6 +145,8 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 
 #else /* !CONFIG_CPUSETS */
 
+static inline bool cpusets_enabled(void) { return false; }
+
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 

commit d26914d11751b23ca2e8747725f2cae10c2f2c1b
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Apr 3 14:47:24 2014 -0700

    mm: optimize put_mems_allowed() usage
    
    Since put_mems_allowed() is strictly optional, its a seqcount retry, we
    don't need to evaluate the function if the allocation was in fact
    successful, saving a smp_rmb some loads and comparisons on some relative
    fast-paths.
    
    Since the naming, get/put_mems_allowed() does suggest a mandatory
    pairing, rename the interface, as suggested by Mel, to resemble the
    seqcount interface.
    
    This gives us: read_mems_allowed_begin() and read_mems_allowed_retry(),
    where it is important to note that the return value of the latter call
    is inverted from its previous incarnation.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 3fe661fe96d1..b19d3dc2e651 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -87,25 +87,26 @@ extern void rebuild_sched_domains(void);
 extern void cpuset_print_task_mems_allowed(struct task_struct *p);
 
 /*
- * get_mems_allowed is required when making decisions involving mems_allowed
- * such as during page allocation. mems_allowed can be updated in parallel
- * and depending on the new value an operation can fail potentially causing
- * process failure. A retry loop with get_mems_allowed and put_mems_allowed
- * prevents these artificial failures.
+ * read_mems_allowed_begin is required when making decisions involving
+ * mems_allowed such as during page allocation. mems_allowed can be updated in
+ * parallel and depending on the new value an operation can fail potentially
+ * causing process failure. A retry loop with read_mems_allowed_begin and
+ * read_mems_allowed_retry prevents these artificial failures.
  */
-static inline unsigned int get_mems_allowed(void)
+static inline unsigned int read_mems_allowed_begin(void)
 {
 	return read_seqcount_begin(&current->mems_allowed_seq);
 }
 
 /*
- * If this returns false, the operation that took place after get_mems_allowed
- * may have failed. It is up to the caller to retry the operation if
+ * If this returns true, the operation that took place after
+ * read_mems_allowed_begin may have failed artificially due to a concurrent
+ * update of mems_allowed. It is up to the caller to retry the operation if
  * appropriate.
  */
-static inline bool put_mems_allowed(unsigned int seq)
+static inline bool read_mems_allowed_retry(unsigned int seq)
 {
-	return !read_seqcount_retry(&current->mems_allowed_seq, seq);
+	return read_seqcount_retry(&current->mems_allowed_seq, seq);
 }
 
 static inline void set_mems_allowed(nodemask_t nodemask)
@@ -225,14 +226,14 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 {
 }
 
-static inline unsigned int get_mems_allowed(void)
+static inline unsigned int read_mems_allowed_begin(void)
 {
 	return 0;
 }
 
-static inline bool put_mems_allowed(unsigned int seq)
+static inline bool read_mems_allowed_retry(unsigned int seq)
 {
-	return true;
+	return false;
 }
 
 #endif /* !CONFIG_CPUSETS */

commit db751fe3ea6880ff5ac5abe60cb7b80deb5a4140
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Oct 7 15:52:00 2013 -0700

    cpuset: Fix potential deadlock w/ set_mems_allowed
    
    After adding lockdep support to seqlock/seqcount structures,
    I started seeing the following warning:
    
    [    1.070907] ======================================================
    [    1.072015] [ INFO: SOFTIRQ-safe -> SOFTIRQ-unsafe lock order detected ]
    [    1.073181] 3.11.0+ #67 Not tainted
    [    1.073801] ------------------------------------------------------
    [    1.074882] kworker/u4:2/708 [HC0[0]:SC0[0]:HE0:SE1] is trying to acquire:
    [    1.076088]  (&p->mems_allowed_seq){+.+...}, at: [<ffffffff81187d7f>] new_slab+0x5f/0x280
    [    1.077572]
    [    1.077572] and this task is already holding:
    [    1.078593]  (&(&q->__queue_lock)->rlock){..-...}, at: [<ffffffff81339f03>] blk_execute_rq_nowait+0x53/0xf0
    [    1.080042] which would create a new lock dependency:
    [    1.080042]  (&(&q->__queue_lock)->rlock){..-...} -> (&p->mems_allowed_seq){+.+...}
    [    1.080042]
    [    1.080042] but this new dependency connects a SOFTIRQ-irq-safe lock:
    [    1.080042]  (&(&q->__queue_lock)->rlock){..-...}
    [    1.080042] ... which became SOFTIRQ-irq-safe at:
    [    1.080042]   [<ffffffff810ec179>] __lock_acquire+0x5b9/0x1db0
    [    1.080042]   [<ffffffff810edfe5>] lock_acquire+0x95/0x130
    [    1.080042]   [<ffffffff818968a1>] _raw_spin_lock+0x41/0x80
    [    1.080042]   [<ffffffff81560c9e>] scsi_device_unbusy+0x7e/0xd0
    [    1.080042]   [<ffffffff8155a612>] scsi_finish_command+0x32/0xf0
    [    1.080042]   [<ffffffff81560e91>] scsi_softirq_done+0xa1/0x130
    [    1.080042]   [<ffffffff8133b0f3>] blk_done_softirq+0x73/0x90
    [    1.080042]   [<ffffffff81095dc0>] __do_softirq+0x110/0x2f0
    [    1.080042]   [<ffffffff81095fcd>] run_ksoftirqd+0x2d/0x60
    [    1.080042]   [<ffffffff810bc506>] smpboot_thread_fn+0x156/0x1e0
    [    1.080042]   [<ffffffff810b3916>] kthread+0xd6/0xe0
    [    1.080042]   [<ffffffff818980ac>] ret_from_fork+0x7c/0xb0
    [    1.080042]
    [    1.080042] to a SOFTIRQ-irq-unsafe lock:
    [    1.080042]  (&p->mems_allowed_seq){+.+...}
    [    1.080042] ... which became SOFTIRQ-irq-unsafe at:
    [    1.080042] ...  [<ffffffff810ec1d3>] __lock_acquire+0x613/0x1db0
    [    1.080042]   [<ffffffff810edfe5>] lock_acquire+0x95/0x130
    [    1.080042]   [<ffffffff810b3df2>] kthreadd+0x82/0x180
    [    1.080042]   [<ffffffff818980ac>] ret_from_fork+0x7c/0xb0
    [    1.080042]
    [    1.080042] other info that might help us debug this:
    [    1.080042]
    [    1.080042]  Possible interrupt unsafe locking scenario:
    [    1.080042]
    [    1.080042]        CPU0                    CPU1
    [    1.080042]        ----                    ----
    [    1.080042]   lock(&p->mems_allowed_seq);
    [    1.080042]                                local_irq_disable();
    [    1.080042]                                lock(&(&q->__queue_lock)->rlock);
    [    1.080042]                                lock(&p->mems_allowed_seq);
    [    1.080042]   <Interrupt>
    [    1.080042]     lock(&(&q->__queue_lock)->rlock);
    [    1.080042]
    [    1.080042]  *** DEADLOCK ***
    
    The issue stems from the kthreadd() function calling set_mems_allowed
    with irqs enabled. While its possibly unlikely for the actual deadlock
    to trigger, a fix is fairly simple: disable irqs before taking the
    mems_allowed_seq lock.
    
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/1381186321-4906-4-git-send-email-john.stultz@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index cc1b01cf2035..3fe661fe96d1 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -110,10 +110,14 @@ static inline bool put_mems_allowed(unsigned int seq)
 
 static inline void set_mems_allowed(nodemask_t nodemask)
 {
+	unsigned long flags;
+
 	task_lock(current);
+	local_irq_save(flags);
 	write_seqcount_begin(&current->mems_allowed_seq);
 	current->mems_allowed = nodemask;
 	write_seqcount_end(&current->mems_allowed_seq);
+	local_irq_restore(flags);
 	task_unlock(current);
 }
 

commit 20b4fb485227404329e41ad15588afad3df23050
Merge: b9394d8a657c ac3e3c5b1164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 17:51:54 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull VFS updates from Al Viro,
    
    Misc cleanups all over the place, mainly wrt /proc interfaces (switch
    create_proc_entry to proc_create(), get rid of the deprecated
    create_proc_read_entry() in favor of using proc_create_data() and
    seq_file etc).
    
    7kloc removed.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (204 commits)
      don't bother with deferred freeing of fdtables
      proc: Move non-public stuff from linux/proc_fs.h to fs/proc/internal.h
      proc: Make the PROC_I() and PDE() macros internal to procfs
      proc: Supply a function to remove a proc entry by PDE
      take cgroup_open() and cpuset_open() to fs/proc/base.c
      ppc: Clean up scanlog
      ppc: Clean up rtas_flash driver somewhat
      hostap: proc: Use remove_proc_subtree()
      drm: proc: Use remove_proc_subtree()
      drm: proc: Use minor->index to label things, not PDE->name
      drm: Constify drm_proc_list[]
      zoran: Don't print proc_dir_entry data in debug
      reiserfs: Don't access the proc_dir_entry in r_open(), r_start() r_show()
      proc: Supply an accessor for getting the data from a PDE's parent
      airo: Use remove_proc_subtree()
      rtl8192u: Don't need to save device proc dir PDE
      rtl8187se: Use a dir under /proc/net/r8180/
      proc: Add proc_mkdir_data()
      proc: Move some bits from linux/proc_fs.h to linux/{of.h,signal.h,tty.h}
      proc: Move PDE_NET() to fs/proc/proc_net.c
      ...

commit 8d8b97ba499cb69fccb5fd9f2b439e3265fc3f27
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Apr 19 23:11:24 2013 -0400

    take cgroup_open() and cpuset_open() to fs/proc/base.c
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 8c8a60d29407..22b637c5ecae 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -64,10 +64,9 @@ extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
 extern int cpuset_memory_pressure_enabled;
 extern void __cpuset_memory_pressure_bump(void);
 
-extern const struct file_operations proc_cpuset_operations;
-struct seq_file;
 extern void cpuset_task_status_allowed(struct seq_file *m,
 					struct task_struct *task);
+extern int proc_cpuset_show(struct seq_file *, void *);
 
 extern int cpuset_mem_spread_node(void);
 extern int cpuset_slab_spread_node(void);

commit ff794dea52eaaa09017efea688a1d7f92ab0818e
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 11:37:56 2013 +0800

    cpuset: remove include of cgroup.h from cpuset.h
    
    We don't need to include cgroup.h in cpuset.h.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 8c8a60d29407..ccd1de8ad822 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -11,7 +11,6 @@
 #include <linux/sched.h>
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
-#include <linux/cgroup.h>
 #include <linux/mm.h>
 
 #ifdef CONFIG_CPUSETS

commit 38d7bee9d24adf4c95676a3dc902827c72930ebb
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 12 13:51:24 2012 -0800

    cpuset: use N_MEMORY instead N_HIGH_MEMORY
    
    N_HIGH_MEMORY stands for the nodes that has normal or high memory.
    N_MEMORY stands for the nodes that has any memory.
    
    The code here need to handle with the nodes which have memory, we should
    use N_MEMORY instead.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Hillf Danton <dhillf@gmail.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Lin Feng <linfeng@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 838320fc3d1d..8c8a60d29407 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -144,7 +144,7 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 	return node_possible_map;
 }
 
-#define cpuset_current_mems_allowed (node_states[N_HIGH_MEMORY])
+#define cpuset_current_mems_allowed (node_states[N_MEMORY])
 static inline void cpuset_init_current_mems_allowed(void) {}
 
 static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)

commit 7ddf96b02fe8dd441f452deef879040def5f7b34
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:55 2012 +0530

    cpusets, hotplug: Restructure functions that are invoked during hotplug
    
    Separate out the cpuset related handling for CPU/Memory online/offline.
    This also helps us exploit the most obvious and basic level of optimization
    that any notification mechanism (CPU/Mem online/offline) has to offer us:
    "We *know* why we have been invoked. So stop pretending that we are lost,
    and do only the necessary amount of processing!".
    
    And while at it, rename scan_for_empty_cpusets() to
    scan_cpusets_upon_hotplug(), which is more appropriate considering how
    it is restructured.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141650.3692.48637.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 668f66baac7b..838320fc3d1d 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -20,7 +20,7 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
-extern void cpuset_update_active_cpus(void);
+extern void cpuset_update_active_cpus(bool cpu_online);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -124,7 +124,7 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
-static inline void cpuset_update_active_cpus(void)
+static inline void cpuset_update_active_cpus(bool cpu_online)
 {
 	partition_sched_domains(1, NULL, NULL);
 }

commit 7fda0412c5f7afdd1a5ff518f98dee5157266d8a
Merge: 6b8212a313da 160594e99dbb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 14:46:05 2012 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpusets: Remove an unused variable
      sched/rt: Improve pick_next_highest_task_rt()
      sched: Fix select_fallback_rq() vs cpu_active/cpu_online
      sched/x86/smp: Do not enable IRQs over calibrate_delay()
      sched: Fix compiler warning about declared inline after use
      MAINTAINERS: Update email address for SCHEDULER and PERF EVENTS

commit 2baab4e90495ebc9826c93f79d74d6e60a828d24
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Mar 20 15:57:01 2012 +0100

    sched: Fix select_fallback_rq() vs cpu_active/cpu_online
    
    Commit 5fbd036b55 ("sched: Cleanup cpu_active madness"), which was
    supposed to finally sort the cpu_active mess, instead uncovered more.
    
    Since CPU_STARTING is ran before setting the cpu online, there's a
    (small) window where the cpu has active,!online.
    
    If during this time there's a wakeup of a task that used to reside on
    that cpu select_task_rq() will use select_fallback_rq() to compute an
    alternative cpu to run on since we find !online.
    
    select_fallback_rq() however will compute the new cpu against
    cpu_active, this means that it can return the same cpu it started out
    with, the !online one, since that cpu is in fact marked active.
    
    This results in us trying to scheduling a task on an offline cpu and
    triggering a WARN in the IPI code.
    
    The solution proposed by Chuansheng Liu of setting cpu_active in
    set_cpu_online() is buggy, firstly not all archs actually use
    set_cpu_online(), secondly, not all archs call set_cpu_online() with
    IRQs disabled, this means we would introduce either the same race or
    the race from fd8a7de17 ("x86: cpu-hotplug: Prevent softirq wakeup on
    wrong CPU") -- albeit much narrower.
    
    [ By setting online first and active later we have a window of
      online,!active, fresh and bound kthreads have task_cpu() of 0 and
      since cpu0 isn't in tsk_cpus_allowed() we end up in
      select_fallback_rq() which excludes !active, resulting in a reset
      of ->cpus_allowed and the thread running all over the place. ]
    
    The solution is to re-work select_fallback_rq() to require active
    _and_ online. This makes the active,!online case work as expected,
    OTOH archs running CPU_STARTING after setting online are now
    vulnerable to the issue from fd8a7de17 -- these are alpha and
    blackfin.
    
    Reported-by: Chuansheng Liu <chuansheng.liu@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: linux-alpha@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-hubqk1i10o4dpvlm06gq7v6j@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index e9eaec522655..e0ffaf061ab7 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -22,7 +22,7 @@ extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_update_active_cpus(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
-extern int cpuset_cpus_allowed_fallback(struct task_struct *p);
+extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -144,10 +144,8 @@ static inline void cpuset_cpus_allowed(struct task_struct *p,
 	cpumask_copy(mask, cpu_possible_mask);
 }
 
-static inline int cpuset_cpus_allowed_fallback(struct task_struct *p)
+static inline void cpuset_cpus_allowed_fallback(struct task_struct *p)
 {
-	do_set_cpus_allowed(p, cpu_possible_mask);
-	return cpumask_any(cpu_active_mask);
 }
 
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)

commit cc9a6c8776615f9c194ccf0b63a0aa5628235545
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Mar 21 16:34:11 2012 -0700

    cpuset: mm: reduce large amounts of memory barrier related damage v3
    
    Commit c0ff7453bb5c ("cpuset,mm: fix no node to alloc memory when
    changing cpuset's mems") wins a super prize for the largest number of
    memory barriers entered into fast paths for one commit.
    
    [get|put]_mems_allowed is incredibly heavy with pairs of full memory
    barriers inserted into a number of hot paths.  This was detected while
    investigating at large page allocator slowdown introduced some time
    after 2.6.32.  The largest portion of this overhead was shown by
    oprofile to be at an mfence introduced by this commit into the page
    allocator hot path.
    
    For extra style points, the commit introduced the use of yield() in an
    implementation of what looks like a spinning mutex.
    
    This patch replaces the full memory barriers on both read and write
    sides with a sequence counter with just read barriers on the fast path
    side.  This is much cheaper on some architectures, including x86.  The
    main bulk of the patch is the retry logic if the nodemask changes in a
    manner that can cause a false failure.
    
    While updating the nodemask, a check is made to see if a false failure
    is a risk.  If it is, the sequence number gets bumped and parallel
    allocators will briefly stall while the nodemask update takes place.
    
    In a page fault test microbenchmark, oprofile samples from
    __alloc_pages_nodemask went from 4.53% of all samples to 1.15%.  The
    actual results were
    
                                 3.3.0-rc3          3.3.0-rc3
                                 rc3-vanilla        nobarrier-v2r1
        Clients   1 UserTime       0.07 (  0.00%)   0.08 (-14.19%)
        Clients   2 UserTime       0.07 (  0.00%)   0.07 (  2.72%)
        Clients   4 UserTime       0.08 (  0.00%)   0.07 (  3.29%)
        Clients   1 SysTime        0.70 (  0.00%)   0.65 (  6.65%)
        Clients   2 SysTime        0.85 (  0.00%)   0.82 (  3.65%)
        Clients   4 SysTime        1.41 (  0.00%)   1.41 (  0.32%)
        Clients   1 WallTime       0.77 (  0.00%)   0.74 (  4.19%)
        Clients   2 WallTime       0.47 (  0.00%)   0.45 (  3.73%)
        Clients   4 WallTime       0.38 (  0.00%)   0.37 (  1.58%)
        Clients   1 Flt/sec/cpu  497620.28 (  0.00%) 520294.53 (  4.56%)
        Clients   2 Flt/sec/cpu  414639.05 (  0.00%) 429882.01 (  3.68%)
        Clients   4 Flt/sec/cpu  257959.16 (  0.00%) 258761.48 (  0.31%)
        Clients   1 Flt/sec      495161.39 (  0.00%) 517292.87 (  4.47%)
        Clients   2 Flt/sec      820325.95 (  0.00%) 850289.77 (  3.65%)
        Clients   4 Flt/sec      1020068.93 (  0.00%) 1022674.06 (  0.26%)
        MMTests Statistics: duration
        Sys Time Running Test (seconds)             135.68    132.17
        User+Sys Time Running Test (seconds)         164.2    160.13
        Total Elapsed Time (seconds)                123.46    120.87
    
    The overall improvement is small but the System CPU time is much
    improved and roughly in correlation to what oprofile reported (these
    performance figures are without profiling so skew is expected).  The
    actual number of page faults is noticeably improved.
    
    For benchmarks like kernel builds, the overall benefit is marginal but
    the system CPU time is slightly reduced.
    
    To test the actual bug the commit fixed I opened two terminals.  The
    first ran within a cpuset and continually ran a small program that
    faulted 100M of anonymous data.  In a second window, the nodemask of the
    cpuset was continually randomised in a loop.
    
    Without the commit, the program would fail every so often (usually
    within 10 seconds) and obviously with the commit everything worked fine.
    With this patch applied, it also worked fine so the fix should be
    functionally equivalent.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index e9eaec522655..7a7e5fd2a277 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -89,42 +89,33 @@ extern void rebuild_sched_domains(void);
 extern void cpuset_print_task_mems_allowed(struct task_struct *p);
 
 /*
- * reading current mems_allowed and mempolicy in the fastpath must protected
- * by get_mems_allowed()
+ * get_mems_allowed is required when making decisions involving mems_allowed
+ * such as during page allocation. mems_allowed can be updated in parallel
+ * and depending on the new value an operation can fail potentially causing
+ * process failure. A retry loop with get_mems_allowed and put_mems_allowed
+ * prevents these artificial failures.
  */
-static inline void get_mems_allowed(void)
+static inline unsigned int get_mems_allowed(void)
 {
-	current->mems_allowed_change_disable++;
-
-	/*
-	 * ensure that reading mems_allowed and mempolicy happens after the
-	 * update of ->mems_allowed_change_disable.
-	 *
-	 * the write-side task finds ->mems_allowed_change_disable is not 0,
-	 * and knows the read-side task is reading mems_allowed or mempolicy,
-	 * so it will clear old bits lazily.
-	 */
-	smp_mb();
+	return read_seqcount_begin(&current->mems_allowed_seq);
 }
 
-static inline void put_mems_allowed(void)
+/*
+ * If this returns false, the operation that took place after get_mems_allowed
+ * may have failed. It is up to the caller to retry the operation if
+ * appropriate.
+ */
+static inline bool put_mems_allowed(unsigned int seq)
 {
-	/*
-	 * ensure that reading mems_allowed and mempolicy before reducing
-	 * mems_allowed_change_disable.
-	 *
-	 * the write-side task will know that the read-side task is still
-	 * reading mems_allowed or mempolicy, don't clears old bits in the
-	 * nodemask.
-	 */
-	smp_mb();
-	--ACCESS_ONCE(current->mems_allowed_change_disable);
+	return !read_seqcount_retry(&current->mems_allowed_seq, seq);
 }
 
 static inline void set_mems_allowed(nodemask_t nodemask)
 {
 	task_lock(current);
+	write_seqcount_begin(&current->mems_allowed_seq);
 	current->mems_allowed = nodemask;
+	write_seqcount_end(&current->mems_allowed_seq);
 	task_unlock(current);
 }
 
@@ -234,12 +225,14 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 {
 }
 
-static inline void get_mems_allowed(void)
+static inline unsigned int get_mems_allowed(void)
 {
+	return 0;
 }
 
-static inline void put_mems_allowed(void)
+static inline bool put_mems_allowed(unsigned int seq)
 {
+	return true;
 }
 
 #endif /* !CONFIG_CPUSETS */

commit 1e1b6c511d1b23cb7c3b619d82fc7bd9f620565d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu May 19 15:08:58 2011 +0900

    cpuset: Fix cpuset_cpus_allowed_fallback(), don't update tsk->rt.nr_cpus_allowed
    
    The rule is, we have to update tsk->rt.nr_cpus_allowed if we change
    tsk->cpus_allowed. Otherwise RT scheduler may confuse.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4DD4B3FA.5060901@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index f20eb8f16025..e9eaec522655 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -146,7 +146,7 @@ static inline void cpuset_cpus_allowed(struct task_struct *p,
 
 static inline int cpuset_cpus_allowed_fallback(struct task_struct *p)
 {
-	cpumask_copy(&p->cpus_allowed, cpu_possible_mask);
+	do_set_cpus_allowed(p, cpu_possible_mask);
 	return cpumask_any(cpu_active_mask);
 }
 

commit 3a101d0548e925ab16ca6aaa8cf4f767d322ddb0
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 8 21:40:36 2010 +0200

    sched: adjust when cpu_active and cpuset configurations are updated during cpu on/offlining
    
    Currently, when a cpu goes down, cpu_active is cleared before
    CPU_DOWN_PREPARE starts and cpuset configuration is updated from a
    default priority cpu notifier.  When a cpu is coming up, it's set
    before CPU_ONLINE but cpuset configuration again is updated from the
    same cpu notifier.
    
    For cpu notifiers, this presents an inconsistent state.  Threads which
    a CPU_DOWN_PREPARE notifier expects to be bound to the CPU can be
    migrated to other cpus because the cpu is no more inactive.
    
    Fix it by updating cpu_active in the highest priority cpu notifier and
    cpuset configuration in the second highest when a cpu is coming up.
    Down path is updated similarly.  This guarantees that all other cpu
    notifiers see consistent cpu_active and cpuset configuration.
    
    cpuset_track_online_cpus() notifier is converted to
    cpuset_update_active_cpus() which just updates the configuration and
    now called from cpuset_cpu_[in]active() notifiers registered from
    sched_init_smp().  If cpuset is disabled, cpuset_update_active_cpus()
    degenerates into partition_sched_domains() making separate notifier
    for !CONFIG_CPUSETS unnecessary.
    
    This problem is triggered by cmwq.  During CPU_DOWN_PREPARE, hotplug
    callback creates a kthread and kthread_bind()s it to the target cpu,
    and the thread is expected to run on that cpu.
    
    * Ingo's test discovered __cpuinit/exit markups were incorrect.
      Fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Menage <menage@google.com>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 457ed765a116..f20eb8f16025 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -20,6 +20,7 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
+extern void cpuset_update_active_cpus(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern int cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -132,6 +133,11 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
+static inline void cpuset_update_active_cpus(void)
+{
+	partition_sched_domains(1, NULL, NULL);
+}
+
 static inline void cpuset_cpus_allowed(struct task_struct *p,
 				       struct cpumask *mask)
 {

commit 6adef3ebe570bcde67fd6c16101451ddde5712b5
Author: Jack Steiner <steiner@sgi.com>
Date:   Wed May 26 14:42:49 2010 -0700

    cpusets: new round-robin rotor for SLAB allocations
    
    We have observed several workloads running on multi-node systems where
    memory is assigned unevenly across the nodes in the system.  There are
    numerous reasons for this but one is the round-robin rotor in
    cpuset_mem_spread_node().
    
    For example, a simple test that writes a multi-page file will allocate
    pages on nodes 0 2 4 6 ...  Odd nodes are skipped.  (Sometimes it
    allocates on odd nodes & skips even nodes).
    
    An example is shown below.  The program "lfile" writes a file consisting
    of 10 pages.  The program then mmaps the file & uses get_mempolicy(...,
    MPOL_F_NODE) to determine the nodes where the file pages were allocated.
    The output is shown below:
    
            # ./lfile
             allocated on nodes: 2 4 6 0 1 2 6 0 2
    
    There is a single rotor that is used for allocating both file pages & slab
    pages.  Writing the file allocates both a data page & a slab page
    (buffer_head).  This advances the RR rotor 2 nodes for each page
    allocated.
    
    A quick confirmation seems to confirm this is the cause of the uneven
    allocation:
    
            # echo 0 >/dev/cpuset/memory_spread_slab
            # ./lfile
             allocated on nodes: 6 7 8 9 0 1 2 3 4 5
    
    This patch introduces a second rotor that is used for slab allocations.
    
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Paul Menage <menage@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 20b51cab6593..457ed765a116 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -69,6 +69,7 @@ extern void cpuset_task_status_allowed(struct seq_file *m,
 					struct task_struct *task);
 
 extern int cpuset_mem_spread_node(void);
+extern int cpuset_slab_spread_node(void);
 
 static inline int cpuset_do_page_mem_spread(void)
 {
@@ -194,6 +195,11 @@ static inline int cpuset_mem_spread_node(void)
 	return 0;
 }
 
+static inline int cpuset_slab_spread_node(void)
+{
+	return 0;
+}
+
 static inline int cpuset_do_page_mem_spread(void)
 {
 	return 0;

commit c0ff7453bb5c7c98e0885fb94279f2571946f280
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon May 24 14:32:08 2010 -0700

    cpuset,mm: fix no node to alloc memory when changing cpuset's mems
    
    Before applying this patch, cpuset updates task->mems_allowed and
    mempolicy by setting all new bits in the nodemask first, and clearing all
    old unallowed bits later.  But in the way, the allocator may find that
    there is no node to alloc memory.
    
    The reason is that cpuset rebinds the task's mempolicy, it cleans the
    nodes which the allocater can alloc pages on, for example:
    
    (mpol: mempolicy)
            task1                   task1's mpol    task2
            alloc page              1
              alloc on node0? NO    1
                                    1               change mems from 1 to 0
                                    1               rebind task1's mpol
                                    0-1               set new bits
                                    0                 clear disallowed bits
              alloc on node1? NO    0
              ...
            can't alloc page
              goto oom
    
    This patch fixes this problem by expanding the nodes range first(set newly
    allowed bits) and shrink it lazily(clear newly disallowed bits).  So we
    use a variable to tell the write-side task that read-side task is reading
    nodemask, and the write-side task clears newly disallowed nodes after
    read-side task ends the current memory allocation.
    
    [akpm@linux-foundation.org: fix spello]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Paul Menage <menage@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index a73454aec333..20b51cab6593 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -86,9 +86,44 @@ extern void rebuild_sched_domains(void);
 
 extern void cpuset_print_task_mems_allowed(struct task_struct *p);
 
+/*
+ * reading current mems_allowed and mempolicy in the fastpath must protected
+ * by get_mems_allowed()
+ */
+static inline void get_mems_allowed(void)
+{
+	current->mems_allowed_change_disable++;
+
+	/*
+	 * ensure that reading mems_allowed and mempolicy happens after the
+	 * update of ->mems_allowed_change_disable.
+	 *
+	 * the write-side task finds ->mems_allowed_change_disable is not 0,
+	 * and knows the read-side task is reading mems_allowed or mempolicy,
+	 * so it will clear old bits lazily.
+	 */
+	smp_mb();
+}
+
+static inline void put_mems_allowed(void)
+{
+	/*
+	 * ensure that reading mems_allowed and mempolicy before reducing
+	 * mems_allowed_change_disable.
+	 *
+	 * the write-side task will know that the read-side task is still
+	 * reading mems_allowed or mempolicy, don't clears old bits in the
+	 * nodemask.
+	 */
+	smp_mb();
+	--ACCESS_ONCE(current->mems_allowed_change_disable);
+}
+
 static inline void set_mems_allowed(nodemask_t nodemask)
 {
+	task_lock(current);
 	current->mems_allowed = nodemask;
+	task_unlock(current);
 }
 
 #else /* !CONFIG_CPUSETS */
@@ -187,6 +222,14 @@ static inline void set_mems_allowed(nodemask_t nodemask)
 {
 }
 
+static inline void get_mems_allowed(void)
+{
+}
+
+static inline void put_mems_allowed(void)
+{
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit 9084bb8246ea935b98320554229e2f371f7f52fa
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 15 10:10:27 2010 +0100

    sched: Make select_fallback_rq() cpuset friendly
    
    Introduce cpuset_cpus_allowed_fallback() helper to fix the cpuset problems
    with select_fallback_rq(). It can be called from any context and can't use
    any cpuset locks including task_lock(). It is called when the task doesn't
    have online cpus in ->cpus_allowed but ttwu/etc must be able to find a
    suitable cpu.
    
    I am not proud of this patch. Everything which needs such a fat comment
    can't be good even if correct. But I'd prefer to not change the locking
    rules in the code I hardly understand, and in any case I believe this
    simple change make the code much more correct compared to deadlocks we
    currently have.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100315091027.GA9155@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index eeaaee746bee..a73454aec333 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -21,6 +21,7 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
+extern int cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -101,6 +102,12 @@ static inline void cpuset_cpus_allowed(struct task_struct *p,
 	cpumask_copy(mask, cpu_possible_mask);
 }
 
+static inline int cpuset_cpus_allowed_fallback(struct task_struct *p)
+{
+	cpumask_copy(&p->cpus_allowed, cpu_possible_mask);
+	return cpumask_any(cpu_active_mask);
+}
+
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 {
 	return node_possible_map;

commit 897f0b3c3ff40b443c84e271bef19bd6ae885195
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 15 10:10:03 2010 +0100

    sched: Kill the broken and deadlockable cpuset_lock/cpuset_cpus_allowed_locked code
    
    This patch just states the fact the cpusets/cpuhotplug interaction is
    broken and removes the deadlockable code which only pretends to work.
    
    - cpuset_lock() doesn't really work. It is needed for
      cpuset_cpus_allowed_locked() but we can't take this lock in
      try_to_wake_up()->select_fallback_rq() path.
    
    - cpuset_lock() is deadlockable. Suppose that a task T bound to CPU takes
      callback_mutex. If cpu_down(CPU) happens before T drops callback_mutex
      stop_machine() preempts T, then migration_call(CPU_DEAD) tries to take
      cpuset_lock() and hangs forever because CPU is already dead and thus
      T can't be scheduled.
    
    - cpuset_cpus_allowed_locked() is deadlockable too. It takes task_lock()
      which is not irq-safe, but try_to_wake_up() can be called from irq.
    
    Kill them, and change select_fallback_rq() to use cpu_possible_mask, like
    we currently do without CONFIG_CPUSETS.
    
    Also, with or without this patch, with or without CONFIG_CPUSETS, the
    callers of select_fallback_rq() can race with each other or with
    set_cpus_allowed() pathes.
    
    The subsequent patches try to to fix these problems.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100315091003.GA9123@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index a5740fc4d04b..eeaaee746bee 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -21,8 +21,6 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
-extern void cpuset_cpus_allowed_locked(struct task_struct *p,
-				       struct cpumask *mask);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -69,9 +67,6 @@ struct seq_file;
 extern void cpuset_task_status_allowed(struct seq_file *m,
 					struct task_struct *task);
 
-extern void cpuset_lock(void);
-extern void cpuset_unlock(void);
-
 extern int cpuset_mem_spread_node(void);
 
 static inline int cpuset_do_page_mem_spread(void)
@@ -105,11 +100,6 @@ static inline void cpuset_cpus_allowed(struct task_struct *p,
 {
 	cpumask_copy(mask, cpu_possible_mask);
 }
-static inline void cpuset_cpus_allowed_locked(struct task_struct *p,
-					      struct cpumask *mask)
-{
-	cpumask_copy(mask, cpu_possible_mask);
-}
 
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 {
@@ -157,9 +147,6 @@ static inline void cpuset_task_status_allowed(struct seq_file *m,
 {
 }
 
-static inline void cpuset_lock(void) {}
-static inline void cpuset_unlock(void) {}
-
 static inline int cpuset_mem_spread_node(void)
 {
 	return 0;

commit 58568d2a8215cb6f55caf2332017d7bdff954e1c
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Jun 16 15:31:49 2009 -0700

    cpuset,mm: update tasks' mems_allowed in time
    
    Fix allocating page cache/slab object on the unallowed node when memory
    spread is set by updating tasks' mems_allowed after its cpuset's mems is
    changed.
    
    In order to update tasks' mems_allowed in time, we must modify the code of
    memory policy.  Because the memory policy is applied in the process's
    context originally.  After applying this patch, one task directly
    manipulates anothers mems_allowed, and we use alloc_lock in the
    task_struct to protect mems_allowed and memory policy of the task.
    
    But in the fast path, we didn't use lock to protect them, because adding a
    lock may lead to performance regression.  But if we don't add a lock,the
    task might see no nodes when changing cpuset's mems_allowed to some
    non-overlapping set.  In order to avoid it, we set all new allowed nodes,
    then clear newly disallowed ones.
    
    [lee.schermerhorn@hp.com:
      The rework of mpol_new() to extract the adjusting of the node mask to
      apply cpuset and mpol flags "context" breaks set_mempolicy() and mbind()
      with MPOL_PREFERRED and a NULL nodemask--i.e., explicit local
      allocation.  Fix this by adding the check for MPOL_PREFERRED and empty
      node mask to mpol_new_mpolicy().
    
      Remove the now unneeded 'nodes = NULL' from mpol_new().
    
      Note that mpol_new_mempolicy() is always called with a non-NULL
      'nodes' parameter now that it has been removed from mpol_new().
      Therefore, we don't need to test nodes for NULL before testing it for
      'empty'.  However, just to be extra paranoid, add a VM_BUG_ON() to
      verify this assumption.]
    [lee.schermerhorn@hp.com:
    
      I don't think the function name 'mpol_new_mempolicy' is descriptive
      enough to differentiate it from mpol_new().
    
      This function applies cpuset set context, usually constraining nodes
      to those allowed by the cpuset.  However, when the 'RELATIVE_NODES flag
      is set, it also translates the nodes.  So I settled on
      'mpol_set_nodemask()', because the comment block for mpol_new() mentions
      that we need to call this function to "set nodes".
    
      Some additional minor line length, whitespace and typo cleanup.]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Paul Menage <menage@google.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 05ea1dd7d681..a5740fc4d04b 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -18,7 +18,6 @@
 
 extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 
-extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
@@ -27,7 +26,6 @@ extern void cpuset_cpus_allowed_locked(struct task_struct *p,
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
-void cpuset_update_task_memory_state(void);
 int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);
 
 extern int __cpuset_node_allowed_softwall(int node, gfp_t gfp_mask);
@@ -92,9 +90,13 @@ extern void rebuild_sched_domains(void);
 
 extern void cpuset_print_task_mems_allowed(struct task_struct *p);
 
+static inline void set_mems_allowed(nodemask_t nodemask)
+{
+	current->mems_allowed = nodemask;
+}
+
 #else /* !CONFIG_CPUSETS */
 
-static inline int cpuset_init_early(void) { return 0; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
@@ -116,7 +118,6 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 
 #define cpuset_current_mems_allowed (node_states[N_HIGH_MEMORY])
 static inline void cpuset_init_current_mems_allowed(void) {}
-static inline void cpuset_update_task_memory_state(void) {}
 
 static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
 {
@@ -188,6 +189,10 @@ static inline void cpuset_print_task_mems_allowed(struct task_struct *p)
 {
 }
 
+static inline void set_mems_allowed(nodemask_t nodemask)
+{
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit a1bc5a4eee990a1f290735c8694d0aebdad095fa
Author: David Rientjes <rientjes@google.com>
Date:   Thu Apr 2 16:57:54 2009 -0700

    cpusets: replace zone allowed functions with node allowed
    
    The cpuset_zone_allowed() variants are actually only a function of the
    zone's node.
    
    Cc: Paul Menage <menage@google.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 2e0d79678deb..05ea1dd7d681 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -12,6 +12,7 @@
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
 #include <linux/cgroup.h>
+#include <linux/mm.h>
 
 #ifdef CONFIG_CPUSETS
 
@@ -29,19 +30,29 @@ void cpuset_init_current_mems_allowed(void);
 void cpuset_update_task_memory_state(void);
 int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);
 
-extern int __cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask);
-extern int __cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask);
+extern int __cpuset_node_allowed_softwall(int node, gfp_t gfp_mask);
+extern int __cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask);
 
-static int inline cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
+static inline int cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
 {
 	return number_of_cpusets <= 1 ||
-		__cpuset_zone_allowed_softwall(z, gfp_mask);
+		__cpuset_node_allowed_softwall(node, gfp_mask);
 }
 
-static int inline cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
+static inline int cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)
 {
 	return number_of_cpusets <= 1 ||
-		__cpuset_zone_allowed_hardwall(z, gfp_mask);
+		__cpuset_node_allowed_hardwall(node, gfp_mask);
+}
+
+static inline int cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
+{
+	return cpuset_node_allowed_softwall(zone_to_nid(z), gfp_mask);
+}
+
+static inline int cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
+{
+	return cpuset_node_allowed_hardwall(zone_to_nid(z), gfp_mask);
 }
 
 extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
@@ -112,6 +123,16 @@ static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
 	return 1;
 }
 
+static inline int cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)
+{
+	return 1;
+}
+
+static inline int cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)
+{
+	return 1;
+}
+
 static inline int cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
 {
 	return 1;

commit aa85ea5b89c36c51200d795dd788139bd9b8cf50
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Mar 30 22:05:15 2009 -0600

    cpumask: use new cpumask_ functions in core code.
    
    Impact: cleanup
    
    Time to clean up remaining laggards using the old cpu_ functions.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Trond.Myklebust@netapp.com

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 90c6074a36ca..2e0d79678deb 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -90,12 +90,12 @@ static inline void cpuset_init_smp(void) {}
 static inline void cpuset_cpus_allowed(struct task_struct *p,
 				       struct cpumask *mask)
 {
-	*mask = cpu_possible_map;
+	cpumask_copy(mask, cpu_possible_mask);
 }
 static inline void cpuset_cpus_allowed_locked(struct task_struct *p,
 					      struct cpumask *mask)
 {
-	*mask = cpu_possible_map;
+	cpumask_copy(mask, cpu_possible_mask);
 }
 
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)

commit 6af866af34a96fed24a55979a78b6f73bd4e8e87
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Jan 7 18:08:45 2009 -0800

    cpuset: remove remaining pointers to cpumask_t
    
    Impact: cleanups, use new cpumask API
    
    Final trivial cleanups: mainly s/cpumask_t/struct cpumask
    
    Note there is a FIXME in generate_sched_domains(). A future patch will
    change struct cpumask *doms to struct cpumask *doms[].
    (I suppose Rusty will do this.)
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Mike Travis <travis@sgi.com>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 51ea2bdea0f9..90c6074a36ca 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -20,8 +20,9 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
-extern void cpuset_cpus_allowed(struct task_struct *p, cpumask_t *mask);
-extern void cpuset_cpus_allowed_locked(struct task_struct *p, cpumask_t *mask);
+extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
+extern void cpuset_cpus_allowed_locked(struct task_struct *p,
+				       struct cpumask *mask);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -86,12 +87,13 @@ static inline int cpuset_init_early(void) { return 0; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
-static inline void cpuset_cpus_allowed(struct task_struct *p, cpumask_t *mask)
+static inline void cpuset_cpus_allowed(struct task_struct *p,
+				       struct cpumask *mask)
 {
 	*mask = cpu_possible_map;
 }
 static inline void cpuset_cpus_allowed_locked(struct task_struct *p,
-								cpumask_t *mask)
+					      struct cpumask *mask)
 {
 	*mask = cpu_possible_map;
 }

commit 75aa199410359dc5fbcf9025ff7af98a9d20f0d5
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jan 6 14:39:01 2009 -0800

    oom: print triggering task's cpuset and mems allowed
    
    When cpusets are enabled, it's necessary to print the triggering task's
    set of allowable nodes so the subsequently printed meminfo can be
    interpreted correctly.
    
    We also print the task's cpuset name for informational purposes.
    
    [rientjes@google.com: task lock current before dereferencing cpuset]
    Cc: Paul Menage <menage@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 8e540d32c9fe..51ea2bdea0f9 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -78,6 +78,8 @@ extern int current_cpuset_is_being_rebound(void);
 
 extern void rebuild_sched_domains(void);
 
+extern void cpuset_print_task_mems_allowed(struct task_struct *p);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
@@ -159,6 +161,10 @@ static inline void rebuild_sched_domains(void)
 	partition_sched_domains(1, NULL, NULL);
 }
 
+static inline void cpuset_print_task_mems_allowed(struct task_struct *p)
+{
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit f481891fdc49d3d1b8a9674a1825d183069a805f
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Nov 19 15:36:30 2008 -0800

    cpuset: update top cpuset's mems after adding a node
    
    After adding a node into the machine, top cpuset's mems isn't updated.
    
    By reviewing the code, we found that the update function
    
      cpuset_track_online_nodes()
    
    was invoked after node_states[N_ONLINE] changes.  It is wrong because
    N_ONLINE just means node has pgdat, and if node has/added memory, we use
    N_HIGH_MEMORY.  So, We should invoke the update function after
    node_states[N_HIGH_MEMORY] changes, just like its commit says.
    
    This patch fixes it.  And we use notifier of memory hotplug instead of
    direct calling of cpuset_track_online_nodes().
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Paul Menage <menage@google.com
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 2691926fb506..8e540d32c9fe 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -74,8 +74,6 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return current->flags & PF_SPREAD_SLAB;
 }
 
-extern void cpuset_track_online_nodes(void);
-
 extern int current_cpuset_is_being_rebound(void);
 
 extern void rebuild_sched_domains(void);
@@ -151,8 +149,6 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return 0;
 }
 
-static inline void cpuset_track_online_nodes(void) {}
-
 static inline int current_cpuset_is_being_rebound(void)
 {
 	return 0;

commit dfb512ec4834116124da61d6c1ee10fd0aa32bd6
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Fri Aug 29 13:11:41 2008 -0700

    sched: arch_reinit_sched_domains() must destroy domains to force rebuild
    
    What I realized recently is that calling rebuild_sched_domains() in
    arch_reinit_sched_domains() by itself is not enough when cpusets are enabled.
    partition_sched_domains() code is trying to avoid unnecessary domain rebuilds
    and will not actually rebuild anything if new domain masks match the old ones.
    
    What this means is that doing
         echo 1 > /sys/devices/system/cpu/sched_mc_power_savings
    on a system with cpusets enabled will not take affect untill something changes
    in the cpuset setup (ie new sets created or deleted).
    
    This patch fixes restore correct behaviour where domains must be rebuilt in
    order to enable MC powersaving flags.
    
    Test on quad-core Core2 box with both CONFIG_CPUSETS and !CONFIG_CPUSETS.
    Also tested on dual-core Core2 laptop. Lockdep is happy and things are working
    as expected.
    
    Signed-off-by: Max Krasnyansky <maxk@qualcomm.com>
    Tested-by: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index e8f450c499b0..2691926fb506 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -160,7 +160,7 @@ static inline int current_cpuset_is_being_rebound(void)
 
 static inline void rebuild_sched_domains(void)
 {
-	partition_sched_domains(0, NULL, NULL);
+	partition_sched_domains(1, NULL, NULL);
 }
 
 #endif /* !CONFIG_CPUSETS */

commit e761b7725234276a802322549cee5255305a0930
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Tue Jul 15 04:43:49 2008 -0700

    cpu hotplug, sched: Introduce cpu_active_map and redo sched domain managment (take 2)
    
    This is based on Linus' idea of creating cpu_active_map that prevents
    scheduler load balancer from migrating tasks to the cpu that is going
    down.
    
    It allows us to simplify domain management code and avoid unecessary
    domain rebuilds during cpu hotplug event handling.
    
    Please ignore the cpusets part for now. It needs some more work in order
    to avoid crazy lock nesting. Although I did simplfy and unify domain
    reinitialization logic. We now simply call partition_sched_domains() in
    all the cases. This means that we're using exact same code paths as in
    cpusets case and hence the test below cover cpusets too.
    Cpuset changes to make rebuild_sched_domains() callable from various
    contexts are in the separate patch (right next after this one).
    
    This not only boots but also easily handles
            while true; do make clean; make -j 8; done
    and
            while true; do on-off-cpu 1; done
    at the same time.
    (on-off-cpu 1 simple does echo 0/1 > /sys/.../cpu1/online thing).
    
    Suprisingly the box (dual-core Core2) is quite usable. In fact I'm typing
    this on right now in gnome-terminal and things are moving just fine.
    
    Also this is running with most of the debug features enabled (lockdep,
    mutex, etc) no BUG_ONs or lockdep complaints so far.
    
    I believe I addressed all of the Dmitry's comments for original Linus'
    version. I changed both fair and rt balancer to mask out non-active cpus.
    And replaced cpu_is_offline() with !cpu_active() in the main scheduler
    code where it made sense (to me).
    
    Signed-off-by: Max Krasnyanskiy <maxk@qualcomm.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Gregory Haskins <ghaskins@novell.com>
    Cc: dmitry.adamushko@gmail.com
    Cc: pj@sgi.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 038578362b47..e8f450c499b0 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -78,6 +78,8 @@ extern void cpuset_track_online_nodes(void);
 
 extern int current_cpuset_is_being_rebound(void);
 
+extern void rebuild_sched_domains(void);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
@@ -156,6 +158,11 @@ static inline int current_cpuset_is_being_rebound(void)
 	return 0;
 }
 
+static inline void rebuild_sched_domains(void)
+{
+	partition_sched_domains(0, NULL, NULL);
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit 19770b32609b6bf97a3dece2529089494cbfc549
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:18 2008 -0700

    mm: filter based on a nodemask as well as a gfp_mask
    
    The MPOL_BIND policy creates a zonelist that is used for allocations
    controlled by that mempolicy.  As the per-node zonelist is already being
    filtered based on a zone id, this patch adds a version of __alloc_pages() that
    takes a nodemask for further filtering.  This eliminates the need for
    MPOL_BIND to create a custom zonelist.
    
    A positive benefit of this is that allocations using MPOL_BIND now use the
    local node's distance-ordered zonelist instead of a custom node-id-ordered
    zonelist.  I.e., pages will be allocated from the closest allowed node with
    available memory.
    
    [Lee.Schermerhorn@hp.com: Mempolicy: update stale documentation and comments]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask]
    [Lee.Schermerhorn@hp.com: Mempolicy: make dequeue_huge_page_vma() obey MPOL_BIND nodemask rework]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 726761e24003..038578362b47 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -26,7 +26,7 @@ extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_task_memory_state(void);
-int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
+int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);
 
 extern int __cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask);
 extern int __cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask);
@@ -103,7 +103,7 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_task_memory_state(void) {}
 
-static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
+static inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
 {
 	return 1;
 }

commit f9a86fcbbb1e5542eabf45c9144ac4b6330861a4
Author: Mike Travis <travis@sgi.com>
Date:   Fri Apr 4 18:11:07 2008 -0700

    cpuset: modify cpuset_set_cpus_allowed to use cpumask pointer
    
      * Modify cpuset_cpus_allowed to return the currently allowed cpuset
        via a pointer argument instead of as the function return value.
    
      * Use new set_cpus_allowed_ptr function.
    
      * Cleanup CPU_MASK_ALL and NODE_MASK_ALL uses.
    
    Depends on:
            [sched-devel]: sched: add new set_cpus_allowed_ptr function
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 0a26be353cb3..726761e24003 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -20,8 +20,8 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
-extern cpumask_t cpuset_cpus_allowed(struct task_struct *p);
-extern cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p);
+extern void cpuset_cpus_allowed(struct task_struct *p, cpumask_t *mask);
+extern void cpuset_cpus_allowed_locked(struct task_struct *p, cpumask_t *mask);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -84,13 +84,14 @@ static inline int cpuset_init_early(void) { return 0; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 
-static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
+static inline void cpuset_cpus_allowed(struct task_struct *p, cpumask_t *mask)
 {
-	return cpu_possible_map;
+	*mask = cpu_possible_map;
 }
-static inline cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p)
+static inline void cpuset_cpus_allowed_locked(struct task_struct *p,
+								cpumask_t *mask)
 {
-	return cpu_possible_map;
+	*mask = cpu_possible_map;
 }
 
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)

commit 31f1de46b90ad360a16e7af3e277d104961df923
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Feb 12 13:30:22 2008 +0900

    mempolicy: silently restrict nodemask to allowed nodes
    
    Kosaki Motohito noted that "numactl --interleave=all ..." failed in the
    presence of memoryless nodes.  This patch attempts to fix that problem.
    
    Some background:
    
    numactl --interleave=all calls set_mempolicy(2) with a fully populated
    [out to MAXNUMNODES] nodemask.  set_mempolicy() [in do_set_mempolicy()]
    calls contextualize_policy() which requires that the nodemask be a
    subset of the current task's mems_allowed; else EINVAL will be returned.
    
    A task's mems_allowed will always be a subset of node_states[N_HIGH_MEMORY]
    i.e., nodes with memory.  So, a fully populated nodemask will be
    declared invalid if it includes memoryless nodes.
    
      NOTE:  the same thing will occur when running in a cpuset
             with restricted mem_allowed--for the same reason:
             node mask contains dis-allowed nodes.
    
    mbind(2), on the other hand, just masks off any nodes in the nodemask
    that are not included in the caller's mems_allowed.
    
    In each case [mbind() and set_mempolicy()], mpol_check_policy() will
    complain [again, resulting in EINVAL] if the nodemask contains any
    memoryless nodes.  This is somewhat redundant as mpol_new() will remove
    memoryless nodes for interleave policy, as will bind_zonelist()--called
    by mpol_new() for BIND policy.
    
    Proposed fix:
    
    1) modify contextualize_policy logic to:
       a) remember whether the incoming node mask is empty.
       b) if not, restrict the nodemask to allowed nodes, as is
          currently done in-line for mbind().  This guarantees
          that the resulting mask includes only nodes with memory.
    
          NOTE:  this is a [benign, IMO] change in behavior for
                 set_mempolicy().  Dis-allowed nodes will be
                 silently ignored, rather than returning an error.
    
       c) fold this code into mpol_check_policy(), replace 2 calls to
          contextualize_policy() to call mpol_check_policy() directly
          and remove contextualize_policy().
    
    2) In existing mpol_check_policy() logic, after "contextualization":
       a) MPOL_DEFAULT:  require that in coming mask "was_empty"
       b) MPOL_{BIND|INTERLEAVE}:  require that contextualized nodemask
          contains at least one node.
       c) add a case for MPOL_PREFERRED:  if in coming was not empty
          and resulting mask IS empty, user specified invalid nodes.
          Return EINVAL.
       c) remove the now redundant check for memoryless nodes
    
    3) remove the now redundant masking of policy nodes for interleave
       policy from mpol_new().
    
    4) Now that mpol_check_policy() contextualizes the nodemask, remove
       the in-line nodes_and() from sys_mbind().  I believe that this
       restores mbind() to the behavior before the memoryless-nodes
       patch series.  E.g., we'll no longer treat an invalid nodemask
       with MPOL_PREFERRED as local allocation.
    
    [ Patch history:
    
      v1 -> v2:
       - Communicate whether or not incoming node mask was empty to
         mpol_check_policy() for better error checking.
       - As suggested by David Rientjes, remove the now unused
         cpuset_nodes_subset_current_mems_allowed() from cpuset.h
    
      v2 -> v3:
       - As suggested by Kosaki Motohito, fold the "contextualization"
         of policy nodemask into mpol_check_policy().  Looks a little
         cleaner. ]
    
    Signed-off-by:  Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by:  KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Tested-by:      KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by:       David Rientjes <rientjes@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index f8c9a2752f06..0a26be353cb3 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -26,8 +26,6 @@ extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_task_memory_state(void);
-#define cpuset_nodes_subset_current_mems_allowed(nodes) \
-		nodes_subset((nodes), current->mems_allowed)
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
 
 extern int __cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask);
@@ -103,7 +101,6 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 #define cpuset_current_mems_allowed (node_states[N_HIGH_MEMORY])
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_task_memory_state(void) {}
-#define cpuset_nodes_subset_current_mems_allowed(nodes) (1)
 
 static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
 {

commit df5f8314ca30d6a76735748e5ba4ca9809c0f434
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Feb 8 04:18:33 2008 -0800

    proc: seqfile convert proc_pid_status to properly handle pid namespaces
    
    Currently we possibly lookup the pid in the wrong pid namespace.  So
    seq_file convert proc_pid_status which ensures the proper pid namespaces is
    passed in.
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: another build fix]
    [akpm@linux-foundation.org: s390 build fix]
    [akpm@linux-foundation.org: fix task_name() output]
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Andrew Morgan <morgan@kernel.org>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index ecae585ec3da..f8c9a2752f06 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -57,7 +57,9 @@ extern int cpuset_memory_pressure_enabled;
 extern void __cpuset_memory_pressure_bump(void);
 
 extern const struct file_operations proc_cpuset_operations;
-extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
+struct seq_file;
+extern void cpuset_task_status_allowed(struct seq_file *m,
+					struct task_struct *task);
 
 extern void cpuset_lock(void);
 extern void cpuset_unlock(void);
@@ -126,10 +128,9 @@ static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
 
 static inline void cpuset_memory_pressure_bump(void) {}
 
-static inline char *cpuset_task_status_allowed(struct task_struct *task,
-							char *buffer)
+static inline void cpuset_task_status_allowed(struct seq_file *m,
+						struct task_struct *task)
 {
-	return buffer;
 }
 
 static inline void cpuset_lock(void) {}

commit 470fd646444c65a5d062a371f5ec8dcedee61239
Author: Cliff Wickman <cpw@sgi.com>
Date:   Thu Oct 18 23:40:46 2007 -0700

    hotplug cpu: migrate a task within its cpuset
    
    When a cpu is disabled, move_task_off_dead_cpu() is called for tasks that have
    been running on that cpu.
    
    Currently, such a task is migrated:
     1) to any cpu on the same node as the disabled cpu, which is both online
        and among that task's cpus_allowed
     2) to any cpu which is both online and among that task's cpus_allowed
    
    It is typical of a multithreaded application running on a large NUMA system to
    have its tasks confined to a cpuset so as to cluster them near the memory that
    they share.  Furthermore, it is typical to explicitly place such a task on a
    specific cpu in that cpuset.  And in that case the task's cpus_allowed
    includes only a single cpu.
    
    This patch would insert a preference to migrate such a task to some cpu within
    its cpuset (and set its cpus_allowed to its entire cpuset).
    
    With this patch, migrate the task to:
     1) to any cpu on the same node as the disabled cpu, which is both online
        and among that task's cpus_allowed
     2) to any online cpu within the task's cpuset
     3) to any cpu which is both online and among that task's cpus_allowed
    
    In order to do this, move_task_off_dead_cpu() must make a call to
    cpuset_cpus_allowed_locked(), a new subset of cpuset_cpus_allowed(), that will
    not block.  (name change - per Oleg's suggestion)
    
    Calls are made to cpuset_lock() and cpuset_unlock() in migration_call() to set
    the cpuset mutex during the whole migrate_live_tasks() and
    migrate_dead_tasks() procedure.
    
    [akpm@linux-foundation.org: build fix]
    [pj@sgi.com: Fix indentation and spacing]
    Signed-off-by: Cliff Wickman <cpw@sgi.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 31adfde1c95f..ecae585ec3da 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -21,6 +21,7 @@ extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern cpumask_t cpuset_cpus_allowed(struct task_struct *p);
+extern cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
@@ -87,6 +88,10 @@ static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
 {
 	return cpu_possible_map;
 }
+static inline cpumask_t cpuset_cpus_allowed_locked(struct task_struct *p)
+{
+	return cpu_possible_map;
+}
 
 static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 {

commit 8793d854edbc2774943a4b0de3304dc73991159a
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:39 2007 -0700

    Task Control Groups: make cpusets a client of cgroups
    
    Remove the filesystem support logic from the cpusets system and makes cpusets
    a cgroup subsystem
    
    The "cpuset" filesystem becomes a dummy filesystem; attempts to mount it get
    passed through to the cgroup filesystem with the appropriate options to
    emulate the old cpuset filesystem behaviour.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index ea44d2e768a0..31adfde1c95f 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -11,6 +11,7 @@
 #include <linux/sched.h>
 #include <linux/cpumask.h>
 #include <linux/nodemask.h>
+#include <linux/cgroup.h>
 
 #ifdef CONFIG_CPUSETS
 
@@ -19,8 +20,6 @@ extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
-extern void cpuset_fork(struct task_struct *p);
-extern void cpuset_exit(struct task_struct *p);
 extern cpumask_t cpuset_cpus_allowed(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 #define cpuset_current_mems_allowed (current->mems_allowed)
@@ -76,13 +75,13 @@ static inline int cpuset_do_slab_mem_spread(void)
 
 extern void cpuset_track_online_nodes(void);
 
+extern int current_cpuset_is_being_rebound(void);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
-static inline void cpuset_fork(struct task_struct *p) {}
-static inline void cpuset_exit(struct task_struct *p) {}
 
 static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
 {
@@ -148,6 +147,11 @@ static inline int cpuset_do_slab_mem_spread(void)
 
 static inline void cpuset_track_online_nodes(void) {}
 
+static inline int current_cpuset_is_being_rebound(void)
+{
+	return 0;
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit bbe373f2c60b2aa36c3231734a5afc5271a06718
Author: David Rientjes <rientjes@google.com>
Date:   Tue Oct 16 23:25:58 2007 -0700

    oom: compare cpuset mems_allowed instead of exclusive ancestors
    
    Instead of testing for overlap in the memory nodes of the the nearest
    exclusive ancestor of both current and the candidate task, it is better to
    simply test for intersection between the task's mems_allowed in their task
    descriptors.  This does not require taking callback_mutex since it is only
    used as a hint in the badness scoring.
    
    Tasks that do not have an intersection in their mems_allowed with the current
    task are not explicitly restricted from being OOM killed because it is quite
    possible that the candidate task has allocated memory there before and has
    since changed its mems_allowed.
    
    Cc: Andrea Arcangeli <andrea@suse.de>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 9e633ea103ce..ea44d2e768a0 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -45,7 +45,8 @@ static int inline cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
 		__cpuset_zone_allowed_hardwall(z, gfp_mask);
 }
 
-extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
+extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
+					  const struct task_struct *tsk2);
 
 #define cpuset_memory_pressure_bump() 				\
 	do {							\
@@ -113,7 +114,8 @@ static inline int cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
 	return 1;
 }
 
-static inline int cpuset_excl_nodes_overlap(const struct task_struct *p)
+static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
+						 const struct task_struct *tsk2)
 {
 	return 1;
 }

commit 0e1e7c7a739562a321fda07c7cd2a97a7114f8f8
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:25:38 2007 -0700

    Memoryless nodes: Use N_HIGH_MEMORY for cpusets
    
    cpusets try to ensure that any node added to a cpuset's mems_allowed is
    on-line and contains memory.  The assumption was that online nodes contained
    memory.  Thus, it is possible to add memoryless nodes to a cpuset and then add
    tasks to this cpuset.  This results in continuous series of oom-kill and
    apparent system hang.
    
    Change cpusets to use node_states[N_HIGH_MEMORY] [a.k.a.  node_memory_map] in
    place of node_online_map when vetting memories.  Return error if admin
    attempts to write a non-empty mems_allowed node mask containing only
    memoryless-nodes.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Bob Picco <bob.picco@hp.com>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@skynet.ie>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 826b15e914e2..9e633ea103ce 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -93,7 +93,7 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 	return node_possible_map;
 }
 
-#define cpuset_current_mems_allowed (node_online_map)
+#define cpuset_current_mems_allowed (node_states[N_HIGH_MEMORY])
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_task_memory_state(void) {}
 #define cpuset_nodes_subset_current_mems_allowed(nodes) (1)

commit 540473208f8ac71c25a87e1a2670c3c18dd4d6db
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Feb 12 00:55:28 2007 -0800

    [PATCH] mark struct file_operations const 1
    
    Many struct file_operations in the kernel can be "const".  Marking them const
    moves these to the .rodata section, which avoids false sharing with potential
    dirty data.  In addition it'll catch accidental writes at compile time to
    these shared resources.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index fd404416f31c..826b15e914e2 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -55,7 +55,7 @@ extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 extern int cpuset_memory_pressure_enabled;
 extern void __cpuset_memory_pressure_bump(void);
 
-extern struct file_operations proc_cpuset_operations;
+extern const struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
 extern void cpuset_lock(void);

commit 089e34b60033863549fbe561d31ac8c778a20e7f
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Dec 29 16:49:04 2006 -0800

    [PATCH] cpuset procfs warning fix
    
    fs/proc/base.c:1869: warning: initialization discards qualifiers from pointer target type
    fs/proc/base.c:2150: warning: initialization discards qualifiers from pointer target type
    
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 826b15e914e2..fd404416f31c 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -55,7 +55,7 @@ extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 extern int cpuset_memory_pressure_enabled;
 extern void __cpuset_memory_pressure_bump(void);
 
-extern const struct file_operations proc_cpuset_operations;
+extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
 extern void cpuset_lock(void);

commit 02a0e53d8227aff5e62e0433f82c12c1c2805fd6
Author: Paul Jackson <pj@sgi.com>
Date:   Wed Dec 13 00:34:25 2006 -0800

    [PATCH] cpuset: rework cpuset_zone_allowed api
    
    Elaborate the API for calling cpuset_zone_allowed(), so that users have to
    explicitly choose between the two variants:
    
      cpuset_zone_allowed_hardwall()
      cpuset_zone_allowed_softwall()
    
    Until now, whether or not you got the hardwall flavor depended solely on
    whether or not you or'd in the __GFP_HARDWALL gfp flag to the gfp_mask
    argument.
    
    If you didn't specify __GFP_HARDWALL, you implicitly got the softwall
    version.
    
    Unfortunately, this meant that users would end up with the softwall version
    without thinking about it.  Since only the softwall version might sleep,
    this led to bugs with possible sleeping in interrupt context on more than
    one occassion.
    
    The hardwall version requires that the current tasks mems_allowed allows
    the node of the specified zone (or that you're in interrupt or that
    __GFP_THISNODE is set or that you're on a one cpuset system.)
    
    The softwall version, depending on the gfp_mask, might allow a node if it
    was allowed in the nearest enclusing cpuset marked mem_exclusive (which
    requires taking the cpuset lock 'callback_mutex' to evaluate.)
    
    This patch removes the cpuset_zone_allowed() call, and forces the caller to
    explicitly choose between the hardwall and the softwall case.
    
    If the caller wants the gfp_mask to determine this choice, they should (1)
    be sure they can sleep or that __GFP_HARDWALL is set, and (2) invoke the
    cpuset_zone_allowed_softwall() routine.
    
    This adds another 100 or 200 bytes to the kernel text space, due to the few
    lines of nearly duplicate code at the top of both cpuset_zone_allowed_*
    routines.  It should save a few instructions executed for the calls that
    turned into calls of cpuset_zone_allowed_hardwall, thanks to not having to
    set (before the call) then check (within the call) the __GFP_HARDWALL flag.
    
    For the most critical call, from get_page_from_freelist(), the same
    instructions are executed as before -- the old cpuset_zone_allowed()
    routine it used to call is the same code as the
    cpuset_zone_allowed_softwall() routine that it calls now.
    
    Not a perfect win, but seems worth it, to reduce this chance of hitting a
    sleeping with irq off complaint again.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 8821e1f75b44..826b15e914e2 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -30,10 +30,19 @@ void cpuset_update_task_memory_state(void);
 		nodes_subset((nodes), current->mems_allowed)
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
 
-extern int __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
-static int inline cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+extern int __cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask);
+extern int __cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask);
+
+static int inline cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
+{
+	return number_of_cpusets <= 1 ||
+		__cpuset_zone_allowed_softwall(z, gfp_mask);
+}
+
+static int inline cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
 {
-	return number_of_cpusets <= 1 || __cpuset_zone_allowed(z, gfp_mask);
+	return number_of_cpusets <= 1 ||
+		__cpuset_zone_allowed_hardwall(z, gfp_mask);
 }
 
 extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
@@ -94,7 +103,12 @@ static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
 	return 1;
 }
 
-static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+static inline int cpuset_zone_allowed_softwall(struct zone *z, gfp_t gfp_mask)
+{
+	return 1;
+}
+
+static inline int cpuset_zone_allowed_hardwall(struct zone *z, gfp_t gfp_mask)
 {
 	return 1;
 }

commit 15ad7cdcfd76450d4beebc789ec646664238184d
Author: Helge Deller <deller@gmx.de>
Date:   Wed Dec 6 20:40:36 2006 -0800

    [PATCH] struct seq_operations and struct file_operations constification
    
     - move some file_operations structs into the .rodata section
    
     - move static strings from policy_types[] array into the .rodata section
    
     - fix generic seq_operations usages, so that those structs may be defined
       as "const" as well
    
    [akpm@osdl.org: couple of fixes]
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 748d2c996631..8821e1f75b44 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -46,7 +46,7 @@ extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 extern int cpuset_memory_pressure_enabled;
 extern void __cpuset_memory_pressure_bump(void);
 
-extern struct file_operations proc_cpuset_operations;
+extern const struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
 extern void cpuset_lock(void);

commit 9276b1bc96a132f4068fdee00983c532f43d3a26
Author: Paul Jackson <pj@sgi.com>
Date:   Wed Dec 6 20:31:48 2006 -0800

    [PATCH] memory page_alloc zonelist caching speedup
    
    Optimize the critical zonelist scanning for free pages in the kernel memory
    allocator by caching the zones that were found to be full recently, and
    skipping them.
    
    Remembers the zones in a zonelist that were short of free memory in the
    last second.  And it stashes a zone-to-node table in the zonelist struct,
    to optimize that conversion (minimize its cache footprint.)
    
    Recent changes:
    
        This differs in a significant way from a similar patch that I
        posted a week ago.  Now, instead of having a nodemask_t of
        recently full nodes, I have a bitmask of recently full zones.
        This solves a problem that last weeks patch had, which on
        systems with multiple zones per node (such as DMA zone) would
        take seeing any of these zones full as meaning that all zones
        on that node were full.
    
        Also I changed names - from "zonelist faster" to "zonelist cache",
        as that seemed to better convey what we're doing here - caching
        some of the key zonelist state (for faster access.)
    
        See below for some performance benchmark results.  After all that
        discussion with David on why I didn't need them, I went and got
        some ;).  I wanted to verify that I had not hurt the normal case
        of memory allocation noticeably.  At least for my one little
        microbenchmark, I found (1) the normal case wasn't affected, and
        (2) workloads that forced scanning across multiple nodes for
        memory improved up to 10% fewer System CPU cycles and lower
        elapsed clock time ('sys' and 'real').  Good.  See details, below.
    
        I didn't have the logic in get_page_from_freelist() for various
        full nodes and zone reclaim failures correct.  That should be
        fixed up now - notice the new goto labels zonelist_scan,
        this_zone_full, and try_next_zone, in get_page_from_freelist().
    
    There are two reasons I persued this alternative, over some earlier
    proposals that would have focused on optimizing the fake numa
    emulation case by caching the last useful zone:
    
     1) Contrary to what I said before, we (SGI, on large ia64 sn2 systems)
        have seen real customer loads where the cost to scan the zonelist
        was a problem, due to many nodes being full of memory before
        we got to a node we could use.  Or at least, I think we have.
        This was related to me by another engineer, based on experiences
        from some time past.  So this is not guaranteed.  Most likely, though.
    
        The following approach should help such real numa systems just as
        much as it helps fake numa systems, or any combination thereof.
    
     2) The effort to distinguish fake from real numa, using node_distance,
        so that we could cache a fake numa node and optimize choosing
        it over equivalent distance fake nodes, while continuing to
        properly scan all real nodes in distance order, was going to
        require a nasty blob of zonelist and node distance munging.
    
        The following approach has no new dependency on node distances or
        zone sorting.
    
    See comment in the patch below for a description of what it actually does.
    
    Technical details of note (or controversy):
    
     - See the use of "zlc_active" and "did_zlc_setup" below, to delay
       adding any work for this new mechanism until we've looked at the
       first zone in zonelist.  I figured the odds of the first zone
       having the memory we needed were high enough that we should just
       look there, first, then get fancy only if we need to keep looking.
    
     - Some odd hackery was needed to add items to struct zonelist, while
       not tripping up the custom zonelists built by the mm/mempolicy.c
       code for MPOL_BIND.  My usual wordy comments below explain this.
       Search for "MPOL_BIND".
    
     - Some per-node data in the struct zonelist is now modified frequently,
       with no locking.  Multiple CPU cores on a node could hit and mangle
       this data.  The theory is that this is just performance hint data,
       and the memory allocator will work just fine despite any such mangling.
       The fields at risk are the struct 'zonelist_cache' fields 'fullzones'
       (a bitmask) and 'last_full_zap' (unsigned long jiffies).  It should
       all be self correcting after at most a one second delay.
    
     - This still does a linear scan of the same lengths as before.  All
       I've optimized is making the scan faster, not algorithmically
       shorter.  It is now able to scan a compact array of 'unsigned
       short' in the case of many full nodes, so one cache line should
       cover quite a few nodes, rather than each node hitting another
       one or two new and distinct cache lines.
    
     - If both Andi and Nick don't find this too complicated, I will be
       (pleasantly) flabbergasted.
    
     - I removed the comment claiming we only use one cachline's worth of
       zonelist.  We seem, at least in the fake numa case, to have put the
       lie to that claim.
    
     - I pay no attention to the various watermarks and such in this performance
       hint.  A node could be marked full for one watermark, and then skipped
       over when searching for a page using a different watermark.  I think
       that's actually quite ok, as it will tend to slightly increase the
       spreading of memory over other nodes, away from a memory stressed node.
    
    ===============
    
    Performance - some benchmark results and analysis:
    
    This benchmark runs a memory hog program that uses multiple
    threads to touch alot of memory as quickly as it can.
    
    Multiple runs were made, touching 12, 38, 64 or 90 GBytes out of
    the total 96 GBytes on the system, and using 1, 19, 37, or 55
    threads (on a 56 CPU system.)  System, user and real (elapsed)
    timings were recorded for each run, shown in units of seconds,
    in the table below.
    
    Two kernels were tested - 2.6.18-mm3 and the same kernel with
    this zonelist caching patch added.  The table also shows the
    percentage improvement the zonelist caching sys time is over
    (lower than) the stock *-mm kernel.
    
          number     2.6.18-mm3        zonelist-cache    delta (< 0 good)   percent
     GBs    N       ------------       --------------    ----------------   systime
     mem threads   sys user  real     sys  user  real     sys  user  real    better
      12     1     153   24   177     151    24   176      -2     0    -1      1%
      12    19      99   22     8      99    22     8       0     0     0      0%
      12    37     111   25     6     112    25     6       1     0     0     -0%
      12    55     115   25     5     110    23     5      -5    -2     0      4%
      38     1     502   74   576     497    73   570      -5    -1    -6      0%
      38    19     426   78    48     373    76    39     -53    -2    -9     12%
      38    37     544   83    36     547    82    36       3    -1     0     -0%
      38    55     501   77    23     511    80    24      10     3     1     -1%
      64     1     917  125  1042     890   124  1014     -27    -1   -28      2%
      64    19    1118  138   119     965   141   103    -153     3   -16     13%
      64    37    1202  151    94    1136   150    81     -66    -1   -13      5%
      64    55    1118  141    61    1072   140    58     -46    -1    -3      4%
      90     1    1342  177  1519    1275   174  1450     -67    -3   -69      4%
      90    19    2392  199   192    2116   189   176    -276   -10   -16     11%
      90    37    3313  238   175    2972   225   145    -341   -13   -30     10%
      90    55    1948  210   104    1843   213   100    -105     3    -4      5%
    
    Notes:
     1) This test ran a memory hog program that started a specified number N of
        threads, and had each thread allocate and touch 1/N'th of
        the total memory to be used in the test run in a single loop,
        writing a constant word to memory, one store every 4096 bytes.
        Watching this test during some earlier trial runs, I would see
        each of these threads sit down on one CPU and stay there, for
        the remainder of the pass, a different CPU for each thread.
    
     2) The 'real' column is not comparable to the 'sys' or 'user' columns.
        The 'real' column is seconds wall clock time elapsed, from beginning
        to end of that test pass.  The 'sys' and 'user' columns are total
        CPU seconds spent on that test pass.  For a 19 thread test run,
        for example, the sum of 'sys' and 'user' could be up to 19 times the
        number of 'real' elapsed wall clock seconds.
    
     3) Tests were run on a fresh, single-user boot, to minimize the amount
        of memory already in use at the start of the test, and to minimize
        the amount of background activity that might interfere.
    
     4) Tests were done on a 56 CPU, 28 Node system with 96 GBytes of RAM.
    
     5) Notice that the 'real' time gets large for the single thread runs, even
        though the measured 'sys' and 'user' times are modest.  I'm not sure what
        that means - probably something to do with it being slow for one thread to
        be accessing memory along ways away.  Perhaps the fake numa system, running
        ostensibly the same workload, would not show this substantial degradation
        of 'real' time for one thread on many nodes -- lets hope not.
    
     6) The high thread count passes (one thread per CPU - on 55 of 56 CPUs)
        ran quite efficiently, as one might expect.  Each pair of threads needed
        to allocate and touch the memory on the node the two threads shared, a
        pleasantly parallizable workload.
    
     7) The intermediate thread count passes, when asking for alot of memory forcing
        them to go to a few neighboring nodes, improved the most with this zonelist
        caching patch.
    
    Conclusions:
     * This zonelist cache patch probably makes little difference one way or the
       other for most workloads on real numa hardware, if those workloads avoid
       heavy off node allocations.
     * For memory intensive workloads requiring substantial off-node allocations
       on real numa hardware, this patch improves both kernel and elapsed timings
       up to ten per-cent.
     * For fake numa systems, I'm optimistic, but will have to leave that up to
       Rohit Seth to actually test (once I get him a 2.6.18 backport.)
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Rohit Seth <rohitseth@google.com>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Cc: David Rientjes <rientjes@cs.washington.edu>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 4d8adf663681..748d2c996631 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -23,6 +23,7 @@ extern void cpuset_fork(struct task_struct *p);
 extern void cpuset_exit(struct task_struct *p);
 extern cpumask_t cpuset_cpus_allowed(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
+#define cpuset_current_mems_allowed (current->mems_allowed)
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_task_memory_state(void);
 #define cpuset_nodes_subset_current_mems_allowed(nodes) \
@@ -83,6 +84,7 @@ static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
 	return node_possible_map;
 }
 
+#define cpuset_current_mems_allowed (node_online_map)
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_task_memory_state(void) {}
 #define cpuset_nodes_subset_current_mems_allowed(nodes) (1)

commit 38837fc75acb7fa9b0e111b0241fe4fe76c5d4b3
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Sep 29 02:01:16 2006 -0700

    [PATCH] cpuset: top_cpuset tracks hotplug changes to node_online_map
    
    Change the list of memory nodes allowed to tasks in the top (root) nodeset
    to dynamically track what cpus are online, using a call to a cpuset hook
    from the memory hotplug code.  Make this top cpus file read-only.
    
    On systems that have cpusets configured in their kernel, but that aren't
    actively using cpusets (for some distros, this covers the majority of
    systems) all tasks end up in the top cpuset.
    
    If that system does support memory hotplug, then these tasks cannot make
    use of memory nodes that are added after system boot, because the memory
    nodes are not allowed in the top cpuset.  This is a surprising regression
    over earlier kernels that didn't have cpusets enabled.
    
    One key motivation for this change is to remain consistent with the
    behaviour for the top_cpuset's 'cpus', which is also read-only, and which
    automatically tracks the cpu_online_map.
    
    This change also has the minor benefit that it fixes a long standing,
    little noticed, minor bug in cpusets.  The cpuset performance tweak to
    short circuit the cpuset_zone_allowed() check on systems with just a single
    cpuset (see 'number_of_cpusets', in linux/cpuset.h) meant that simply
    changing the 'mems' of the top_cpuset had no affect, even though the change
    (the write system call) appeared to succeed.  With the following change,
    that write to the 'mems' file fails -EACCES, and the 'mems' file stubbornly
    refuses to be changed via user space writes.  Thus no one should be mislead
    into thinking they've changed the top_cpusets's 'mems' when in affect they
    haven't.
    
    In order to keep the behaviour of cpusets consistent between systems
    actively making use of them and systems not using them, this patch changes
    the behaviour of the 'mems' file in the top (root) cpuset, making it read
    only, and making it automatically track the value of node_online_map.  Thus
    tasks in the top cpuset will have automatic use of hot plugged memory nodes
    allowed by their cpuset.
    
    [akpm@osdl.org: build fix]
    [bunk@stusta.de: build fix]
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 9354722a9217..4d8adf663681 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -63,6 +63,8 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return current->flags & PF_SPREAD_SLAB;
 }
 
+extern void cpuset_track_online_nodes(void);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
@@ -126,6 +128,8 @@ static inline int cpuset_do_slab_mem_spread(void)
 	return 0;
 }
 
+static inline void cpuset_track_online_nodes(void) {}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit 825a46af5ac171f9f41f794a0a00165588ba1589
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:03 2006 -0800

    [PATCH] cpuset memory spread basic implementation
    
    This patch provides the implementation and cpuset interface for an alternative
    memory allocation policy that can be applied to certain kinds of memory
    allocations, such as the page cache (file system buffers) and some slab caches
    (such as inode caches).
    
    The policy is called "memory spreading." If enabled, it spreads out these
    kinds of memory allocations over all the nodes allowed to a task, instead of
    preferring to place them on the node where the task is executing.
    
    All other kinds of allocations, including anonymous pages for a tasks stack
    and data regions, are not affected by this policy choice, and continue to be
    allocated preferring the node local to execution, as modified by the NUMA
    mempolicy.
    
    There are two boolean flag files per cpuset that control where the kernel
    allocates pages for the file system buffers and related in kernel data
    structures.  They are called 'memory_spread_page' and 'memory_spread_slab'.
    
    If the per-cpuset boolean flag file 'memory_spread_page' is set, then the
    kernel will spread the file system buffers (page cache) evenly over all the
    nodes that the faulting task is allowed to use, instead of preferring to put
    those pages on the node where the task is running.
    
    If the per-cpuset boolean flag file 'memory_spread_slab' is set, then the
    kernel will spread some file system related slab caches, such as for inodes
    and dentries evenly over all the nodes that the faulting task is allowed to
    use, instead of preferring to put those pages on the node where the task is
    running.
    
    The implementation is simple.  Setting the cpuset flags 'memory_spread_page'
    or 'memory_spread_cache' turns on the per-process flags PF_SPREAD_PAGE or
    PF_SPREAD_SLAB, respectively, for each task that is in the cpuset or
    subsequently joins that cpuset.  In subsequent patches, the page allocation
    calls for the affected page cache and slab caches are modified to perform an
    inline check for these flags, and if set, a call to a new routine
    cpuset_mem_spread_node() returns the node to prefer for the allocation.
    
    The cpuset_mem_spread_node() routine is also simple.  It uses the value of a
    per-task rotor cpuset_mem_spread_rotor to select the next node in the current
    tasks mems_allowed to prefer for the allocation.
    
    This policy can provide substantial improvements for jobs that need to place
    thread local data on the corresponding node, but that need to access large
    file system data sets that need to be spread across the several nodes in the
    jobs cpuset in order to fit.  Without this patch, especially for jobs that
    might have one thread reading in the data set, the memory allocation across
    the nodes in the jobs cpuset can become very uneven.
    
    A couple of Copyright year ranges are updated as well.  And a couple of email
    addresses that can be found in the MAINTAINERS file are removed.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 3bc606927116..9354722a9217 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -4,7 +4,7 @@
  *  cpuset interface
  *
  *  Copyright (C) 2003 BULL SA
- *  Copyright (C) 2004 Silicon Graphics, Inc.
+ *  Copyright (C) 2004-2006 Silicon Graphics, Inc.
  *
  */
 
@@ -51,6 +51,18 @@ extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 extern void cpuset_lock(void);
 extern void cpuset_unlock(void);
 
+extern int cpuset_mem_spread_node(void);
+
+static inline int cpuset_do_page_mem_spread(void)
+{
+	return current->flags & PF_SPREAD_PAGE;
+}
+
+static inline int cpuset_do_slab_mem_spread(void)
+{
+	return current->flags & PF_SPREAD_SLAB;
+}
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
@@ -99,6 +111,21 @@ static inline char *cpuset_task_status_allowed(struct task_struct *task,
 static inline void cpuset_lock(void) {}
 static inline void cpuset_unlock(void) {}
 
+static inline int cpuset_mem_spread_node(void)
+{
+	return 0;
+}
+
+static inline int cpuset_do_page_mem_spread(void)
+{
+	return 0;
+}
+
+static inline int cpuset_do_slab_mem_spread(void)
+{
+	return 0;
+}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit 505970b96e3b7d22177c38e03435a68376628e7a
Author: Paul Jackson <pj@sgi.com>
Date:   Sat Jan 14 13:21:06 2006 -0800

    [PATCH] cpuset oom lock fix
    
    The problem, reported in:
    
      http://bugzilla.kernel.org/show_bug.cgi?id=5859
    
    and by various other email messages and lkml posts is that the cpuset hook
    in the oom (out of memory) code can try to take a cpuset semaphore while
    holding the tasklist_lock (a spinlock).
    
    One must not sleep while holding a spinlock.
    
    The fix seems easy enough - move the cpuset semaphore region outside the
    tasklist_lock region.
    
    This required a few lines of mechanism to implement.  The oom code where
    the locking needs to be changed does not have access to the cpuset locks,
    which are internal to kernel/cpuset.c only.  So I provided a couple more
    cpuset interface routines, available to the rest of the kernel, which
    simple take and drop the lock needed here (cpusets callback_sem).
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index c472f972bd6d..3bc606927116 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -48,6 +48,9 @@ extern void __cpuset_memory_pressure_bump(void);
 extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
+extern void cpuset_lock(void);
+extern void cpuset_unlock(void);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
@@ -93,6 +96,9 @@ static inline char *cpuset_task_status_allowed(struct task_struct *task,
 	return buffer;
 }
 
+static inline void cpuset_lock(void) {}
+static inline void cpuset_unlock(void) {}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */

commit c417f0242ebe578924a30d4e53d35b5059fed4e7
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:02:01 2006 -0800

    [PATCH] cpuset: remove test for null cpuset from alloc code path
    
    Remove a couple of more lines of code from the cpuset hooks in the page
    allocation code path.
    
    There was a check for a NULL cpuset pointer in the routine
    cpuset_update_task_memory_state() that was only needed during system boot,
    after the memory subsystem was initialized, before the cpuset subsystem was
    initialized, to catch a NULL task->cpuset pointer.
    
    Add a cpuset_init_early() routine, just before the mem_init() call in
    init/main.c, that sets up just enough of the init tasks cpuset structure to
    render cpuset_update_task_memory_state() calls harmless.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 34081c168af5..c472f972bd6d 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -16,6 +16,7 @@
 
 extern int number_of_cpusets;	/* How many cpusets are defined in system? */
 
+extern int cpuset_init_early(void);
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_fork(struct task_struct *p);
@@ -49,6 +50,7 @@ extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
 #else /* !CONFIG_CPUSETS */
 
+static inline int cpuset_init_early(void) { return 0; }
 static inline int cpuset_init(void) { return 0; }
 static inline void cpuset_init_smp(void) {}
 static inline void cpuset_fork(struct task_struct *p) {}

commit 202f72d5d1b5c2c084f63ef996c736d208b447b5
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:57 2006 -0800

    [PATCH] cpuset: number_of_cpusets optimization
    
    Easy little optimization hack to avoid actually having to call
    cpuset_zone_allowed() and check mems_allowed, in the main page allocation
    routine, __alloc_pages().  This saves several CPU cycles per page allocation
    on systems not using cpusets.
    
    A counter is updated each time a cpuset is created or removed, and whenever
    there is only one cpuset in the system, it must be the root cpuset, which
    contains all CPUs and all Memory Nodes.  In that case, when the counter is
    one, all allocations are allowed.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 37d2dd7ca3e9..34081c168af5 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -14,6 +14,8 @@
 
 #ifdef CONFIG_CPUSETS
 
+extern int number_of_cpusets;	/* How many cpusets are defined in system? */
+
 extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_fork(struct task_struct *p);
@@ -25,7 +27,13 @@ void cpuset_update_task_memory_state(void);
 #define cpuset_nodes_subset_current_mems_allowed(nodes) \
 		nodes_subset((nodes), current->mems_allowed)
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
-extern int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
+
+extern int __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
+static int inline cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
+{
+	return number_of_cpusets <= 1 || __cpuset_zone_allowed(z, gfp_mask);
+}
+
 extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 
 #define cpuset_memory_pressure_bump() 				\

commit 909d75a3b77bdd8baa9429bad3b69a654d2954ce
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:55 2006 -0800

    [PATCH] cpuset: implement cpuset_mems_allowed
    
    Provide a cpuset_mems_allowed() method, which the sys_migrate_pages() code
    needed, to obtain the mems_allowed vector of a cpuset, and replaced the
    workaround in sys_migrate_pages() to call this new method.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 1feebf16ab08..37d2dd7ca3e9 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -18,7 +18,8 @@ extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_fork(struct task_struct *p);
 extern void cpuset_exit(struct task_struct *p);
-extern cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
+extern cpumask_t cpuset_cpus_allowed(struct task_struct *p);
+extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_task_memory_state(void);
 #define cpuset_nodes_subset_current_mems_allowed(nodes) \
@@ -50,6 +51,11 @@ static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
 	return cpu_possible_map;
 }
 
+static inline nodemask_t cpuset_mems_allowed(struct task_struct *p)
+{
+	return node_possible_map;
+}
+
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_task_memory_state(void) {}
 #define cpuset_nodes_subset_current_mems_allowed(nodes) (1)

commit cf2a473c4089aa41c26f653200673f5a4cc25047
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:54 2006 -0800

    [PATCH] cpuset: combine refresh_mems and update_mems
    
    The important code paths through alloc_pages_current() and alloc_page_vma(),
    by which most kernel page allocations go, both called
    cpuset_update_current_mems_allowed(), which in turn called refresh_mems().
    -Both- of these latter two routines did a tasklock, got the tasks cpuset
    pointer, and checked for out of date cpuset->mems_generation.
    
    That was a silly duplication of code and waste of CPU cycles on an important
    code path.
    
    Consolidated those two routines into a single routine, called
    cpuset_update_task_memory_state(), since it updates more than just
    mems_allowed.
    
    Changed all callers of either routine to call the new consolidated routine.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 736d73801cb6..1feebf16ab08 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -20,7 +20,7 @@ extern void cpuset_fork(struct task_struct *p);
 extern void cpuset_exit(struct task_struct *p);
 extern cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
-void cpuset_update_current_mems_allowed(void);
+void cpuset_update_task_memory_state(void);
 #define cpuset_nodes_subset_current_mems_allowed(nodes) \
 		nodes_subset((nodes), current->mems_allowed)
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
@@ -51,7 +51,7 @@ static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
 }
 
 static inline void cpuset_init_current_mems_allowed(void) {}
-static inline void cpuset_update_current_mems_allowed(void) {}
+static inline void cpuset_update_task_memory_state(void) {}
 #define cpuset_nodes_subset_current_mems_allowed(nodes) (1)
 
 static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)

commit 3e0d98b9f1eb757fc98efc84e74e54a08308aa73
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:49 2006 -0800

    [PATCH] cpuset: memory pressure meter
    
    Provide a simple per-cpuset metric of memory pressure, tracking the -rate-
    that the tasks in a cpuset call try_to_free_pages(), the synchronous
    (direct) memory reclaim code.
    
    This enables batch managers monitoring jobs running in dedicated cpusets to
    efficiently detect what level of memory pressure that job is causing.
    
    This is useful both on tightly managed systems running a wide mix of
    submitted jobs, which may choose to terminate or reprioritize jobs that are
    trying to use more memory than allowed on the nodes assigned them, and with
    tightly coupled, long running, massively parallel scientific computing jobs
    that will dramatically fail to meet required performance goals if they
    start to use more memory than allowed to them.
    
    This patch just provides a very economical way for the batch manager to
    monitor a cpuset for signs of memory pressure.  It's up to the batch
    manager or other user code to decide what to do about it and take action.
    
    ==> Unless this feature is enabled by writing "1" to the special file
        /dev/cpuset/memory_pressure_enabled, the hook in the rebalance
        code of __alloc_pages() for this metric reduces to simply noticing
        that the cpuset_memory_pressure_enabled flag is zero.  So only
        systems that enable this feature will compute the metric.
    
    Why a per-cpuset, running average:
    
        Because this meter is per-cpuset, rather than per-task or mm, the
        system load imposed by a batch scheduler monitoring this metric is
        sharply reduced on large systems, because a scan of the tasklist can be
        avoided on each set of queries.
    
        Because this meter is a running average, instead of an accumulating
        counter, a batch scheduler can detect memory pressure with a single
        read, instead of having to read and accumulate results for a period of
        time.
    
        Because this meter is per-cpuset rather than per-task or mm, the
        batch scheduler can obtain the key information, memory pressure in a
        cpuset, with a single read, rather than having to query and accumulate
        results over all the (dynamically changing) set of tasks in the cpuset.
    
    A per-cpuset simple digital filter (requires a spinlock and 3 words of data
    per-cpuset) is kept, and updated by any task attached to that cpuset, if it
    enters the synchronous (direct) page reclaim code.
    
    A per-cpuset file provides an integer number representing the recent
    (half-life of 10 seconds) rate of direct page reclaims caused by the tasks
    in the cpuset, in units of reclaims attempted per second, times 1000.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 8b21786490ee..736d73801cb6 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -26,6 +26,15 @@ void cpuset_update_current_mems_allowed(void);
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
 extern int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
 extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
+
+#define cpuset_memory_pressure_bump() 				\
+	do {							\
+		if (cpuset_memory_pressure_enabled)		\
+			__cpuset_memory_pressure_bump();	\
+	} while (0)
+extern int cpuset_memory_pressure_enabled;
+extern void __cpuset_memory_pressure_bump(void);
+
 extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
@@ -60,6 +69,8 @@ static inline int cpuset_excl_nodes_overlap(const struct task_struct *p)
 	return 1;
 }
 
+static inline void cpuset_memory_pressure_bump(void) {}
+
 static inline char *cpuset_task_status_allowed(struct task_struct *task,
 							char *buffer)
 {

commit 5966514db662fb24c9bb43226a80106bcffd51f8
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:47 2006 -0800

    [PATCH] cpuset: mempolicy one more nodemask conversion
    
    Finish converting mm/mempolicy.c from bitmaps to nodemasks.  The previous
    conversion had left one routine using bitmaps, since it involved a
    corresponding change to kernel/cpuset.c
    
    Fix that interface by replacing with a simple macro that calls nodes_subset(),
    or if !CONFIG_CPUSET, returns (1).
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Christoph Lameter <christoph@lameter.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 6e2deef96b34..8b21786490ee 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -21,7 +21,8 @@ extern void cpuset_exit(struct task_struct *p);
 extern cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_current_mems_allowed(void);
-void cpuset_restrict_to_mems_allowed(unsigned long *nodes);
+#define cpuset_nodes_subset_current_mems_allowed(nodes) \
+		nodes_subset((nodes), current->mems_allowed)
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
 extern int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
 extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
@@ -42,7 +43,7 @@ static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
 
 static inline void cpuset_init_current_mems_allowed(void) {}
 static inline void cpuset_update_current_mems_allowed(void) {}
-static inline void cpuset_restrict_to_mems_allowed(unsigned long *nodes) {}
+#define cpuset_nodes_subset_current_mems_allowed(nodes) (1)
 
 static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
 {

commit dd0fc66fb33cd610bc1a5db8a5e232d34879b4d7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Oct 7 07:46:04 2005 +0100

    [PATCH] gfp flags annotations - part 1
    
     - added typedef unsigned int __nocast gfp_t;
    
     - replaced __nocast uses for gfp flags with gfp_t - it gives exactly
       the same warnings as far as sparse is concerned, doesn't change
       generated code (from gcc point of view we replaced unsigned int with
       typedef) and documents what's going on far better.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 24062a1dbf61..6e2deef96b34 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -23,7 +23,7 @@ void cpuset_init_current_mems_allowed(void);
 void cpuset_update_current_mems_allowed(void);
 void cpuset_restrict_to_mems_allowed(unsigned long *nodes);
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
-extern int cpuset_zone_allowed(struct zone *z, unsigned int __nocast gfp_mask);
+extern int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask);
 extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
@@ -49,8 +49,7 @@ static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
 	return 1;
 }
 
-static inline int cpuset_zone_allowed(struct zone *z,
-					unsigned int __nocast gfp_mask)
+static inline int cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)
 {
 	return 1;
 }

commit ef08e3b4981aebf2ba9bd7025ef7210e8eec07ce
Author: Paul Jackson <pj@sgi.com>
Date:   Tue Sep 6 15:18:13 2005 -0700

    [PATCH] cpusets: confine oom_killer to mem_exclusive cpuset
    
    Now the real motivation for this cpuset mem_exclusive patch series seems
    trivial.
    
    This patch keeps a task in or under one mem_exclusive cpuset from provoking an
    oom kill of a task under a non-overlapping mem_exclusive cpuset.  Since only
    interrupt and GFP_ATOMIC allocations are allowed to escape mem_exclusive
    containment, there is little to gain from oom killing a task under a
    non-overlapping mem_exclusive cpuset, as almost all kernel and user memory
    allocation must come from disjoint memory nodes.
    
    This patch enables configuring a system so that a runaway job under one
    mem_exclusive cpuset cannot cause the killing of a job in another such cpuset
    that might be using very high compute and memory resources for a prolonged
    time.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 1fe1c3ebad30..24062a1dbf61 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -24,6 +24,7 @@ void cpuset_update_current_mems_allowed(void);
 void cpuset_restrict_to_mems_allowed(unsigned long *nodes);
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
 extern int cpuset_zone_allowed(struct zone *z, unsigned int __nocast gfp_mask);
+extern int cpuset_excl_nodes_overlap(const struct task_struct *p);
 extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
@@ -54,6 +55,11 @@ static inline int cpuset_zone_allowed(struct zone *z,
 	return 1;
 }
 
+static inline int cpuset_excl_nodes_overlap(const struct task_struct *p)
+{
+	return 1;
+}
+
 static inline char *cpuset_task_status_allowed(struct task_struct *task,
 							char *buffer)
 {

commit 9bf2229f8817677127a60c177aefce1badd22d7b
Author: Paul Jackson <pj@sgi.com>
Date:   Tue Sep 6 15:18:12 2005 -0700

    [PATCH] cpusets: formalize intermediate GFP_KERNEL containment
    
    This patch makes use of the previously underutilized cpuset flag
    'mem_exclusive' to provide what amounts to another layer of memory placement
    resolution.  With this patch, there are now the following four layers of
    memory placement available:
    
     1) The whole system (interrupt and GFP_ATOMIC allocations can use this),
     2) The nearest enclosing mem_exclusive cpuset (GFP_KERNEL allocations can use),
     3) The current tasks cpuset (GFP_USER allocations constrained to here), and
     4) Specific node placement, using mbind and set_mempolicy.
    
    These nest - each layer is a subset (same or within) of the previous.
    
    Layer (2) above is new, with this patch.  The call used to check whether a
    zone (its node, actually) is in a cpuset (in its mems_allowed, actually) is
    extended to take a gfp_mask argument, and its logic is extended, in the case
    that __GFP_HARDWALL is not set in the flag bits, to look up the cpuset
    hierarchy for the nearest enclosing mem_exclusive cpuset, to determine if
    placement is allowed.  The definition of GFP_USER, which used to be identical
    to GFP_KERNEL, is changed to also set the __GFP_HARDWALL bit, in the previous
    cpuset_gfp_hardwall_flag patch.
    
    GFP_ATOMIC and GFP_KERNEL allocations will stay within the current tasks
    cpuset, so long as any node therein is not too tight on memory, but will
    escape to the larger layer, if need be.
    
    The intended use is to allow something like a batch manager to handle several
    jobs, each job in its own cpuset, but using common kernel memory for caches
    and such.  Swapper and oom_kill activity is also constrained to Layer (2).  A
    task in or below one mem_exclusive cpuset should not cause swapping on nodes
    in another non-overlapping mem_exclusive cpuset, nor provoke oom_killing of a
    task in another such cpuset.  Heavy use of kernel memory for i/o caching and
    such by one job should not impact the memory available to jobs in other
    non-overlapping mem_exclusive cpusets.
    
    This patch enables providing hardwall, inescapable cpusets for memory
    allocations of each job, while sharing kernel memory allocations between
    several jobs, in an enclosing mem_exclusive cpuset.
    
    Like Dinakar's patch earlier to enable administering sched domains using the
    cpu_exclusive flag, this patch also provides a useful meaning to a cpuset flag
    that had previously done nothing much useful other than restrict what cpuset
    configurations were allowed.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 3438233305a3..1fe1c3ebad30 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -23,7 +23,7 @@ void cpuset_init_current_mems_allowed(void);
 void cpuset_update_current_mems_allowed(void);
 void cpuset_restrict_to_mems_allowed(unsigned long *nodes);
 int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
-int cpuset_zone_allowed(struct zone *z);
+extern int cpuset_zone_allowed(struct zone *z, unsigned int __nocast gfp_mask);
 extern struct file_operations proc_cpuset_operations;
 extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
 
@@ -48,7 +48,8 @@ static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
 	return 1;
 }
 
-static inline int cpuset_zone_allowed(struct zone *z)
+static inline int cpuset_zone_allowed(struct zone *z,
+					unsigned int __nocast gfp_mask)
 {
 	return 1;
 }

commit 9a8488965dc4c42a4a1f84cab907c7d6c5cf1563
Author: Benoit Boissinot <benoit.boissinot@ens-lyon.org>
Date:   Sat Apr 16 15:25:59 2005 -0700

    [PATCH] cpuset: remove function attribute const
    
    gcc-4 warns with
    include/linux/cpuset.h:21: warning: type qualifiers ignored on function
    return type
    
    cpuset_cpus_allowed is declared with const
    extern const cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
    
    First const should be __attribute__((const)), but the gcc manual
    explains that:
    
    "Note that a function that has pointer arguments and examines the data
    pointed to must not be declared const. Likewise, a function that calls a
    non-const function usually must not be const. It does not make sense for
    a const function to return void."
    
    The following patch remove const from the function declaration.
    
    Signed-off-by: Benoit Boissinot <benoit.boissinot@ens-lyon.org>
    Acked-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 2d9a500d994d..3438233305a3 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -18,7 +18,7 @@ extern int cpuset_init(void);
 extern void cpuset_init_smp(void);
 extern void cpuset_fork(struct task_struct *p);
 extern void cpuset_exit(struct task_struct *p);
-extern const cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
+extern cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
 void cpuset_init_current_mems_allowed(void);
 void cpuset_update_current_mems_allowed(void);
 void cpuset_restrict_to_mems_allowed(unsigned long *nodes);

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
new file mode 100644
index 000000000000..2d9a500d994d
--- /dev/null
+++ b/include/linux/cpuset.h
@@ -0,0 +1,64 @@
+#ifndef _LINUX_CPUSET_H
+#define _LINUX_CPUSET_H
+/*
+ *  cpuset interface
+ *
+ *  Copyright (C) 2003 BULL SA
+ *  Copyright (C) 2004 Silicon Graphics, Inc.
+ *
+ */
+
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+#include <linux/nodemask.h>
+
+#ifdef CONFIG_CPUSETS
+
+extern int cpuset_init(void);
+extern void cpuset_init_smp(void);
+extern void cpuset_fork(struct task_struct *p);
+extern void cpuset_exit(struct task_struct *p);
+extern const cpumask_t cpuset_cpus_allowed(const struct task_struct *p);
+void cpuset_init_current_mems_allowed(void);
+void cpuset_update_current_mems_allowed(void);
+void cpuset_restrict_to_mems_allowed(unsigned long *nodes);
+int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl);
+int cpuset_zone_allowed(struct zone *z);
+extern struct file_operations proc_cpuset_operations;
+extern char *cpuset_task_status_allowed(struct task_struct *task, char *buffer);
+
+#else /* !CONFIG_CPUSETS */
+
+static inline int cpuset_init(void) { return 0; }
+static inline void cpuset_init_smp(void) {}
+static inline void cpuset_fork(struct task_struct *p) {}
+static inline void cpuset_exit(struct task_struct *p) {}
+
+static inline cpumask_t cpuset_cpus_allowed(struct task_struct *p)
+{
+	return cpu_possible_map;
+}
+
+static inline void cpuset_init_current_mems_allowed(void) {}
+static inline void cpuset_update_current_mems_allowed(void) {}
+static inline void cpuset_restrict_to_mems_allowed(unsigned long *nodes) {}
+
+static inline int cpuset_zonelist_valid_mems_allowed(struct zonelist *zl)
+{
+	return 1;
+}
+
+static inline int cpuset_zone_allowed(struct zone *z)
+{
+	return 1;
+}
+
+static inline char *cpuset_task_status_allowed(struct task_struct *task,
+							char *buffer)
+{
+	return buffer;
+}
+
+#endif /* !CONFIG_CPUSETS */
+
+#endif /* _LINUX_CPUSET_H */
