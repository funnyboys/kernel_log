commit ee01c4d72adffb7d424535adf630f2955748fa8b
Merge: c444eb564fb1 09587a09ada2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 20:24:15 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
     "More mm/ work, plenty more to come
    
      Subsystems affected by this patch series: slub, memcg, gup, kasan,
      pagealloc, hugetlb, vmscan, tools, mempolicy, memblock, hugetlbfs,
      thp, mmap, kconfig"
    
    * akpm: (131 commits)
      arm64: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      x86: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      riscv: support DEBUG_WX
      mm: add DEBUG_WX support
      drivers/base/memory.c: cache memory blocks in xarray to accelerate lookup
      mm/thp: rename pmd_mknotpresent() as pmd_mkinvalid()
      powerpc/mm: drop platform defined pmd_mknotpresent()
      mm: thp: don't need to drain lru cache when splitting and mlocking THP
      hugetlbfs: get unmapped area below TASK_UNMAPPED_BASE for hugetlbfs
      sparc32: register memory occupied by kernel as memblock.memory
      include/linux/memblock.h: fix minor typo and unclear comment
      mm, mempolicy: fix up gup usage in lookup_node
      tools/vm/page_owner_sort.c: filter out unneeded line
      mm: swap: memcg: fix memcg stats for huge pages
      mm: swap: fix vmstats for huge pages
      mm: vmscan: limit the range of LRU type balancing
      mm: vmscan: reclaim writepage is IO cost
      mm: vmscan: determine anon/file pressure balance at the reclaim root
      mm: balance LRU lists based on relative thrashing
      mm: only count actual rotations as LRU reclaim cost
      ...

commit 96f8bf4fb1dd2656ae3e92326be9ebf003bbfd45
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jun 3 16:03:09 2020 -0700

    mm: vmscan: reclaim writepage is IO cost
    
    The VM tries to balance reclaim pressure between anon and file so as to
    reduce the amount of IO incurred due to the memory shortage.  It already
    counts refaults and swapins, but in addition it should also count
    writepage calls during reclaim.
    
    For swap, this is obvious: it's IO that wouldn't have occurred if the
    anonymous memory hadn't been under memory pressure.  From a relative
    balancing point of view this makes sense as well: even if anon is cold and
    reclaimable, a cache that isn't thrashing may have equally cold pages that
    don't require IO to reclaim.
    
    For file writeback, it's trickier: some of the reclaim writepage IO would
    have likely occurred anyway due to dirty expiration.  But not all of it -
    premature writeback reduces batching and generates additional writes.
    Since the flushers are already woken up by the time the VM starts writing
    cache pages one by one, let's assume that we'e likely causing writes that
    wouldn't have happened without memory pressure.  In addition, the per-page
    cost of IO would have probably been much cheaper if written in larger
    batches from the flusher thread rather than the single-page-writes from
    kswapd.
    
    For our purposes - getting the trend right to accelerate convergence on a
    stable state that doesn't require paging at all - this is sufficiently
    accurate.  If we later wanted to optimize for sustained thrashing, we can
    still refine the measurements.
    
    Count all writepage calls from kswapd as IO cost toward the LRU that the
    page belongs to.
    
    Why do this dynamically?  Don't we know in advance that anon pages require
    IO to reclaim, and so could build in a static bias?
    
    First, scanning is not the same as reclaiming.  If all the anon pages are
    referenced, we may not swap for a while just because we're scanning the
    anon list.  During this time, however, it's important that we age
    anonymous memory and the page cache at the same rate so that their
    hot-cold gradients are comparable.  Everything else being equal, we still
    want to reclaim the coldest memory overall.
    
    Second, we keep copies in swap unless the page changes.  If there is
    swap-backed data that's mostly read (tmpfs file) and has been swapped out
    before, we can reclaim it without incurring additional IO.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Link: http://lkml.kernel.org/r/20200520232525.798933-14-hannes@cmpxchg.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 10cc932e209a..3d12c34cd42a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -26,6 +26,7 @@ struct reclaim_stat {
 	unsigned nr_congested;
 	unsigned nr_writeback;
 	unsigned nr_immediate;
+	unsigned nr_pageout;
 	unsigned nr_activate[2];
 	unsigned nr_ref_keep;
 	unsigned nr_unmap_fail;

commit 1f318a9b0dc3990490e98eef48f21e6f15185781
Author: Jaewon Kim <jaewon31.kim@samsung.com>
Date:   Wed Jun 3 16:01:15 2020 -0700

    mm/vmscan: count layzfree pages and fix nr_isolated_* mismatch
    
    Fix an nr_isolate_* mismatch problem between cma and dirty lazyfree pages.
    
    If try_to_unmap_one is used for reclaim and it detects a dirty lazyfree
    page, then the lazyfree page is changed to a normal anon page having
    SwapBacked by commit 802a3a92ad7a ("mm: reclaim MADV_FREE pages").  Even
    with the change, reclaim context correctly counts isolated files because
    it uses is_file_lru to distinguish file.  And the change to anon is not
    happened if try_to_unmap_one is used for migration.  So migration context
    like compaction also correctly counts isolated files even though it uses
    page_is_file_lru insted of is_file_lru.  Recently page_is_file_cache was
    renamed to page_is_file_lru by commit 9de4f22a60f7 ("mm: code cleanup for
    MADV_FREE").
    
    But the nr_isolate_* mismatch problem happens on cma alloc.  There is
    reclaim_clean_pages_from_list which is being used only by cma.  It was
    introduced by commit 02c6de8d757c ("mm: cma: discard clean pages during
    contiguous allocation instead of migration") to reclaim clean file pages
    without migration.  The cma alloc uses both reclaim_clean_pages_from_list
    and migrate_pages, and it uses page_is_file_lru to count isolated files.
    If there are dirty lazyfree pages allocated from cma memory region, the
    pages are counted as isolated file at the beginging but are counted as
    isolated anon after finished.
    
    Mem-Info:
    Node 0 active_anon:3045904kB inactive_anon:611448kB active_file:14892kB inactive_file:205636kB unevictable:10416kB isolated(anon):0kB isolated(file):37664kB mapped:630216kB dirty:384kB writeback:0kB shmem:42576kB writeback_tmp:0kB unstable:0kB all_unreclaimable? no
    
    Like log above, there were too much isolated files, 37664kB, which
    triggers too_many_isolated in reclaim even when there is no actually
    isolated file in system wide.  It could be reproducible by running two
    programs, writing on MADV_FREE page and doing cma alloc, respectively.
    Although isolated anon is 0, I found that the internal value of isolated
    anon was the negative value of isolated file.
    
    Fix this by compensating the isolated count for both LRU lists.  Count
    non-discarded lazyfree pages in shrink_page_list, then compensate the
    counted number in reclaim_clean_pages_from_list.
    
    Reported-by: Yong-Taek Lee <ytk.lee@samsung.com>
    Suggested-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Jaewon Kim <jaewon31.kim@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Shaohua Li <shli@fb.com>
    Link: http://lkml.kernel.org/r/20200426011718.30246-1-jaewon31.kim@samsung.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 292485f3d24d..10cc932e209a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -29,6 +29,7 @@ struct reclaim_stat {
 	unsigned nr_activate[2];
 	unsigned nr_ref_keep;
 	unsigned nr_unmap_fail;
+	unsigned nr_lazyfree_fail;
 };
 
 enum writeback_stat_item {

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 292485f3d24d..cb507151710f 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -16,8 +16,8 @@ extern int sysctl_stat_interval;
 #define DISABLE_NUMA_STAT   0
 extern int sysctl_vm_numa_stat;
 DECLARE_STATIC_KEY_TRUE(vm_numa_stat_key);
-extern int sysctl_vm_numa_stat_handler(struct ctl_table *table,
-		int write, void __user *buffer, size_t *length, loff_t *ppos);
+int sysctl_vm_numa_stat_handler(struct ctl_table *table, int write,
+		void *buffer, size_t *length, loff_t *ppos);
 #endif
 
 struct reclaim_stat {
@@ -274,8 +274,8 @@ void cpu_vm_stats_fold(int cpu);
 void refresh_zone_stat_thresholds(void);
 
 struct ctl_table;
-int vmstat_refresh(struct ctl_table *, int write,
-		   void __user *buffer, size_t *lenp, loff_t *ppos);
+int vmstat_refresh(struct ctl_table *, int write, void *buffer, size_t *lenp,
+		loff_t *ppos);
 
 void drain_zonestat(struct zone *zone, struct per_cpu_pageset *);
 

commit ebc5d83d04438116c24dcc556b0ab6c8ef64b77e
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed Dec 4 16:49:53 2019 -0800

    mm/memcontrol: use vmstat names for printing statistics
    
    Use common names from vmstat array when possible.  This gives not much
    difference in code size for now, but should help in keeping interfaces
    consistent.
    
      add/remove: 0/2 grow/shrink: 2/0 up/down: 70/-72 (-2)
      Function                                     old     new   delta
      memory_stat_format                           984    1050     +66
      memcg_stat_show                              957     961      +4
      memcg1_event_names                            32       -     -32
      mem_cgroup_lru_names                          40       -     -40
      Total: Before=14485337, After=14485335, chg -0.00%
    
    Link: http://lkml.kernel.org/r/157113012508.453.80391533767219371.stgit@buzz
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index b995d8b680c2..292485f3d24d 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -420,7 +420,7 @@ static inline const char *writeback_stat_name(enum writeback_stat_item item)
 			   item];
 }
 
-#ifdef CONFIG_VM_EVENT_COUNTERS
+#if defined(CONFIG_VM_EVENT_COUNTERS) || defined(CONFIG_MEMCG)
 static inline const char *vm_event_name(enum vm_event_item item)
 {
 	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
@@ -429,6 +429,6 @@ static inline const char *vm_event_name(enum vm_event_item item)
 			   NR_VM_WRITEBACK_STAT_ITEMS +
 			   item];
 }
-#endif /* CONFIG_VM_EVENT_COUNTERS */
+#endif /* CONFIG_VM_EVENT_COUNTERS || CONFIG_MEMCG */
 
 #endif /* _LINUX_VMSTAT_H */

commit 9d7ea9a297e6445d567056f15b469dde13ca4134
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Wed Dec 4 16:49:50 2019 -0800

    mm/vmstat: add helpers to get vmstat item names for each enum type
    
    Statistics in vmstat is combined from counters with different structure,
    but names for them are merged into one array.
    
    This patch adds trivial helpers to get name for each item:
    
      const char *zone_stat_name(enum zone_stat_item item);
      const char *numa_stat_name(enum numa_stat_item item);
      const char *node_stat_name(enum node_stat_item item);
      const char *writeback_stat_name(enum writeback_stat_item item);
      const char *vm_event_name(enum vm_event_item item);
    
    Names for enum writeback_stat_item are folded in the middle of
    vmstat_text so this patch moves declaration into header to calculate
    offset of following items.
    
    Also this patch reuses piece of node stat names for lru list names:
    
      const char *lru_list_name(enum lru_list lru);
    
    This returns common lru list names: "inactive_anon", "active_anon",
    "inactive_file", "active_file", "unevictable".
    
    [khlebnikov@yandex-team.ru: do not use size of vmstat_text as count of /proc/vmstat items]
      Link: http://lkml.kernel.org/r/157152151769.4139.15423465513138349343.stgit@buzz
      Link: https://lore.kernel.org/linux-mm/cd1c42ae-281f-c8a8-70ac-1d01d417b2e1@infradead.org/T/#u
    Link: http://lkml.kernel.org/r/157113012325.453.562783073839432766.stgit@buzz
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: YueHaibing <yuehaibing@huawei.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index bdeda4b079fe..b995d8b680c2 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -31,6 +31,12 @@ struct reclaim_stat {
 	unsigned nr_unmap_fail;
 };
 
+enum writeback_stat_item {
+	NR_DIRTY_THRESHOLD,
+	NR_DIRTY_BG_THRESHOLD,
+	NR_VM_WRITEBACK_STAT_ITEMS,
+};
+
 #ifdef CONFIG_VM_EVENT_COUNTERS
 /*
  * Light weight per cpu counter implementation.
@@ -381,4 +387,48 @@ static inline void __mod_zone_freepage_state(struct zone *zone, int nr_pages,
 
 extern const char * const vmstat_text[];
 
+static inline const char *zone_stat_name(enum zone_stat_item item)
+{
+	return vmstat_text[item];
+}
+
+#ifdef CONFIG_NUMA
+static inline const char *numa_stat_name(enum numa_stat_item item)
+{
+	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
+			   item];
+}
+#endif /* CONFIG_NUMA */
+
+static inline const char *node_stat_name(enum node_stat_item item)
+{
+	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
+			   NR_VM_NUMA_STAT_ITEMS +
+			   item];
+}
+
+static inline const char *lru_list_name(enum lru_list lru)
+{
+	return node_stat_name(NR_LRU_BASE + lru) + 3; // skip "nr_"
+}
+
+static inline const char *writeback_stat_name(enum writeback_stat_item item)
+{
+	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
+			   NR_VM_NUMA_STAT_ITEMS +
+			   NR_VM_NODE_STAT_ITEMS +
+			   item];
+}
+
+#ifdef CONFIG_VM_EVENT_COUNTERS
+static inline const char *vm_event_name(enum vm_event_item item)
+{
+	return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
+			   NR_VM_NUMA_STAT_ITEMS +
+			   NR_VM_NODE_STAT_ITEMS +
+			   NR_VM_WRITEBACK_STAT_ITEMS +
+			   item];
+}
+#endif /* CONFIG_VM_EVENT_COUNTERS */
+
 #endif /* _LINUX_VMSTAT_H */

commit 886cf1901db962cee5f8b82b9b260079a5e8a4eb
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Mon May 13 17:16:51 2019 -0700

    mm: move recent_rotated pages calculation to shrink_inactive_list()
    
    Patch series "mm: Generalize putback functions"]
    
    putback_inactive_pages() and move_active_pages_to_lru() are almost
    similar, so this patchset merges them ina single function.
    
    This patch (of 4):
    
    The patch moves the calculation from putback_inactive_pages() to
    shrink_inactive_list().  This makes putback_inactive_pages() looking more
    similar to move_active_pages_to_lru().
    
    To do that, we account activated pages in reclaim_stat::nr_activate.
    Since a page may change its LRU type from anon to file cache inside
    shrink_page_list() (see ClearPageSwapBacked()), we have to account pages
    for the both types.  So, nr_activate becomes an array.
    
    Previously we used nr_activate to account PGACTIVATE events, but now we
    account them into pgactivate variable (since they are about number of
    pages in general, not about sum of hpage_nr_pages).
    
    Link: http://lkml.kernel.org/r/155290127956.31489.3393586616054413298.stgit@localhost.localdomain
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 2db8d60981fe..bdeda4b079fe 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -26,7 +26,7 @@ struct reclaim_stat {
 	unsigned nr_congested;
 	unsigned nr_writeback;
 	unsigned nr_immediate;
-	unsigned nr_activate;
+	unsigned nr_activate[2];
 	unsigned nr_ref_keep;
 	unsigned nr_unmap_fail;
 };

commit 4918e7625ffa82f388ea70538f0e1df20ea35a54
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:39:27 2018 -0800

    include/linux/vmstat.h: remove unused page state adjustment macro
    
    These four macro are not used anymore.
    
    Just remove them.
    
    Link: http://lkml.kernel.org/r/20181214063211.2290-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index f25cef84b41d..2db8d60981fe 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -239,11 +239,6 @@ extern unsigned long node_page_state(struct pglist_data *pgdat,
 #define node_page_state(node, item) global_node_page_state(item)
 #endif /* CONFIG_NUMA */
 
-#define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
-#define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
-#define add_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, __d)
-#define sub_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, -(__d))
-
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);

commit d51d1e64500fcb48fc6a18c77c965b8f48a175f2
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Apr 10 16:28:07 2018 -0700

    mm, vmscan, tracing: use pointer to reclaim_stat struct in trace event
    
    The trace event trace_mm_vmscan_lru_shrink_inactive() currently has 12
    parameters! Seven of them are from the reclaim_stat structure.  This
    structure is currently local to mm/vmscan.c.  By moving it to the global
    vmstat.h header, we can also reference it from the vmscan tracepoints.
    In moving it, it brings down the overhead of passing so many arguments
    to the trace event.  In the future, we may limit the number of arguments
    that a trace event may pass (ideally just 6, but more realistically it
    may be 8).
    
    Before this patch, the code to call the trace event is this:
    
     0f 83 aa fe ff ff       jae    ffffffff811e6261 <shrink_inactive_list+0x1e1>
     48 8b 45 a0             mov    -0x60(%rbp),%rax
     45 8b 64 24 20          mov    0x20(%r12),%r12d
     44 8b 6d d4             mov    -0x2c(%rbp),%r13d
     8b 4d d0                mov    -0x30(%rbp),%ecx
     44 8b 75 cc             mov    -0x34(%rbp),%r14d
     44 8b 7d c8             mov    -0x38(%rbp),%r15d
     48 89 45 90             mov    %rax,-0x70(%rbp)
     8b 83 b8 fe ff ff       mov    -0x148(%rbx),%eax
     8b 55 c0                mov    -0x40(%rbp),%edx
     8b 7d c4                mov    -0x3c(%rbp),%edi
     8b 75 b8                mov    -0x48(%rbp),%esi
     89 45 80                mov    %eax,-0x80(%rbp)
     65 ff 05 e4 f7 e2 7e    incl   %gs:0x7ee2f7e4(%rip)        # 15bd0 <__preempt_count>
     48 8b 05 75 5b 13 01    mov    0x1135b75(%rip),%rax        # ffffffff8231bf68 <__tracepoint_mm_vmscan_lru_shrink_inactive+0x28>
     48 85 c0                test   %rax,%rax
     74 72                   je     ffffffff811e646a <shrink_inactive_list+0x3ea>
     48 89 c3                mov    %rax,%rbx
     4c 8b 10                mov    (%rax),%r10
     89 f8                   mov    %edi,%eax
     48 89 85 68 ff ff ff    mov    %rax,-0x98(%rbp)
     89 f0                   mov    %esi,%eax
     48 89 85 60 ff ff ff    mov    %rax,-0xa0(%rbp)
     89 c8                   mov    %ecx,%eax
     48 89 85 78 ff ff ff    mov    %rax,-0x88(%rbp)
     89 d0                   mov    %edx,%eax
     48 89 85 70 ff ff ff    mov    %rax,-0x90(%rbp)
     8b 45 8c                mov    -0x74(%rbp),%eax
     48 8b 7b 08             mov    0x8(%rbx),%rdi
     48 83 c3 18             add    $0x18,%rbx
     50                      push   %rax
     41 54                   push   %r12
     41 55                   push   %r13
     ff b5 78 ff ff ff       pushq  -0x88(%rbp)
     41 56                   push   %r14
     41 57                   push   %r15
     ff b5 70 ff ff ff       pushq  -0x90(%rbp)
     4c 8b 8d 68 ff ff ff    mov    -0x98(%rbp),%r9
     4c 8b 85 60 ff ff ff    mov    -0xa0(%rbp),%r8
     48 8b 4d 98             mov    -0x68(%rbp),%rcx
     48 8b 55 90             mov    -0x70(%rbp),%rdx
     8b 75 80                mov    -0x80(%rbp),%esi
     41 ff d2                callq  *%r10
    
    After the patch:
    
     0f 83 a8 fe ff ff       jae    ffffffff811e626d <shrink_inactive_list+0x1cd>
     8b 9b b8 fe ff ff       mov    -0x148(%rbx),%ebx
     45 8b 64 24 20          mov    0x20(%r12),%r12d
     4c 8b 6d a0             mov    -0x60(%rbp),%r13
     65 ff 05 f5 f7 e2 7e    incl   %gs:0x7ee2f7f5(%rip)        # 15bd0 <__preempt_count>
     4c 8b 35 86 5b 13 01    mov    0x1135b86(%rip),%r14        # ffffffff8231bf68 <__tracepoint_mm_vmscan_lru_shrink_inactive+0x28>
     4d 85 f6                test   %r14,%r14
     74 2a                   je     ffffffff811e6411 <shrink_inactive_list+0x371>
     49 8b 06                mov    (%r14),%rax
     8b 4d 8c                mov    -0x74(%rbp),%ecx
     49 8b 7e 08             mov    0x8(%r14),%rdi
     49 83 c6 18             add    $0x18,%r14
     4c 89 ea                mov    %r13,%rdx
     45 89 e1                mov    %r12d,%r9d
     4c 8d 45 b8             lea    -0x48(%rbp),%r8
     89 de                   mov    %ebx,%esi
     51                      push   %rcx
     48 8b 4d 98             mov    -0x68(%rbp),%rcx
     ff d0                   callq  *%rax
    
    Link: http://lkml.kernel.org/r/2559d7cb-ec60-1200-2362-04fa34fd02bb@fb.com
    Link: http://lkml.kernel.org/r/20180322121003.4177af15@gandalf.local.home
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reported-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index a4c2317d8b9f..f25cef84b41d 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -20,6 +20,17 @@ extern int sysctl_vm_numa_stat_handler(struct ctl_table *table,
 		int write, void __user *buffer, size_t *length, loff_t *ppos);
 #endif
 
+struct reclaim_stat {
+	unsigned nr_dirty;
+	unsigned nr_unqueued_dirty;
+	unsigned nr_congested;
+	unsigned nr_writeback;
+	unsigned nr_immediate;
+	unsigned nr_activate;
+	unsigned nr_ref_keep;
+	unsigned nr_unmap_fail;
+};
+
 #ifdef CONFIG_VM_EVENT_COUNTERS
 /*
  * Light weight per cpu counter implementation.

commit a4ef87684108e5fef38cf289ee360f9b87a53cfd
Author: Jan Kara <jack@suse.cz>
Date:   Wed Jan 31 16:17:06 2018 -0800

    mm: remove unused pgdat_reclaimable_pages()
    
    Remove unused function pgdat_reclaimable_pages() and
    node_page_state_snapshot() which becomes unused as well.
    
    Link: http://lkml.kernel.org/r/20171122094416.26019-1-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 1779c9817b39..a4c2317d8b9f 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -216,23 +216,6 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 	return x;
 }
 
-static inline unsigned long node_page_state_snapshot(pg_data_t *pgdat,
-					enum node_stat_item item)
-{
-	long x = atomic_long_read(&pgdat->vm_stat[item]);
-
-#ifdef CONFIG_SMP
-	int cpu;
-	for_each_online_cpu(cpu)
-		x += per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->vm_node_stat_diff[item];
-
-	if (x < 0)
-		x = 0;
-#endif
-	return x;
-}
-
-
 #ifdef CONFIG_NUMA
 extern void __inc_numa_state(struct zone *zone, enum numa_stat_item item);
 extern unsigned long sum_zone_node_page_state(int node,

commit 4518085e127dff97e74f74a8780d7564e273bec8
Author: Kemi Wang <kemi.wang@intel.com>
Date:   Wed Nov 15 17:38:22 2017 -0800

    mm, sysctl: make NUMA stats configurable
    
    This is the second step which introduces a tunable interface that allow
    numa stats configurable for optimizing zone_statistics(), as suggested
    by Dave Hansen and Ying Huang.
    
    =========================================================================
    
    When page allocation performance becomes a bottleneck and you can
    tolerate some possible tool breakage and decreased numa counter
    precision, you can do:
    
            echo 0 > /proc/sys/vm/numa_stat
    
    In this case, numa counter update is ignored.  We can see about
    *4.8%*(185->176) drop of cpu cycles per single page allocation and
    reclaim on Jesper's page_bench01 (single thread) and *8.1%*(343->315)
    drop of cpu cycles per single page allocation and reclaim on Jesper's
    page_bench03 (88 threads) running on a 2-Socket Broadwell-based server
    (88 threads, 126G memory).
    
    Benchmark link provided by Jesper D Brouer (increase loop times to
    10000000):
    
      https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/bench
    
    =========================================================================
    
    When page allocation performance is not a bottleneck and you want all
    tooling to work, you can do:
    
            echo 1 > /proc/sys/vm/numa_stat
    
    This is system default setting.
    
    Many thanks to Michal Hocko, Dave Hansen, Ying Huang and Vlastimil Babka
    for comments to help improve the original patch.
    
    [keescook@chromium.org: make sure mutex is a global static]
      Link: http://lkml.kernel.org/r/20171107213809.GA4314@beast
    Link: http://lkml.kernel.org/r/1508290927-8518-1-git-send-email-kemi.wang@intel.com
    Signed-off-by: Kemi Wang <kemi.wang@intel.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Suggested-by: Ying Huang <ying.huang@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: "Luis R . Rodriguez" <mcgrof@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 1e0cb72e0598..1779c9817b39 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -7,9 +7,19 @@
 #include <linux/mmzone.h>
 #include <linux/vm_event_item.h>
 #include <linux/atomic.h>
+#include <linux/static_key.h>
 
 extern int sysctl_stat_interval;
 
+#ifdef CONFIG_NUMA
+#define ENABLE_NUMA_STAT   1
+#define DISABLE_NUMA_STAT   0
+extern int sysctl_vm_numa_stat;
+DECLARE_STATIC_KEY_TRUE(vm_numa_stat_key);
+extern int sysctl_vm_numa_stat_handler(struct ctl_table *table,
+		int write, void __user *buffer, size_t *length, loff_t *ppos);
+#endif
+
 #ifdef CONFIG_VM_EVENT_COUNTERS
 /*
  * Light weight per cpu counter implementation.

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index ade7cb5f1359..1e0cb72e0598 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_VMSTAT_H
 #define _LINUX_VMSTAT_H
 

commit 638032224ed762a29baca1fc37f1168efc2554ae
Author: Kemi Wang <kemi.wang@intel.com>
Date:   Fri Sep 8 16:12:55 2017 -0700

    mm: consider the number in local CPUs when reading NUMA stats
    
    To avoid deviation, the per cpu number of NUMA stats in
    vm_numa_stat_diff[] is included when a user *reads* the NUMA stats.
    
    Since NUMA stats does not be read by users frequently, and kernel does not
    need it to make a decision, it will not be a problem to make the readers
    more expensive.
    
    Link: http://lkml.kernel.org/r/1503568801-21305-4-git-send-email-kemi.wang@intel.com
    Signed-off-by: Kemi Wang <kemi.wang@intel.com>
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Ying Huang <ying.huang@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 9ac82e29948f..ade7cb5f1359 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -125,10 +125,14 @@ static inline unsigned long global_numa_state(enum numa_stat_item item)
 	return x;
 }
 
-static inline unsigned long zone_numa_state(struct zone *zone,
+static inline unsigned long zone_numa_state_snapshot(struct zone *zone,
 					enum numa_stat_item item)
 {
 	long x = atomic_long_read(&zone->vm_numa_stat[item]);
+	int cpu;
+
+	for_each_online_cpu(cpu)
+		x += per_cpu_ptr(zone->pageset, cpu)->vm_numa_stat_diff[item];
 
 	return x;
 }

commit 3a321d2a3dde812142e06ab5c2f062ed860182a5
Author: Kemi Wang <kemi.wang@intel.com>
Date:   Fri Sep 8 16:12:48 2017 -0700

    mm: change the call sites of numa statistics items
    
    Patch series "Separate NUMA statistics from zone statistics", v2.
    
    Each page allocation updates a set of per-zone statistics with a call to
    zone_statistics().  As discussed in 2017 MM summit, these are a
    substantial source of overhead in the page allocator and are very rarely
    consumed.  This significant overhead in cache bouncing caused by zone
    counters (NUMA associated counters) update in parallel in multi-threaded
    page allocation (pointed out by Dave Hansen).
    
    A link to the MM summit slides:
      http://people.netfilter.org/hawk/presentations/MM-summit2017/MM-summit2017-JesperBrouer.pdf
    
    To mitigate this overhead, this patchset separates NUMA statistics from
    zone statistics framework, and update NUMA counter threshold to a fixed
    size of MAX_U16 - 2, as a small threshold greatly increases the update
    frequency of the global counter from local per cpu counter (suggested by
    Ying Huang).  The rationality is that these statistics counters don't
    need to be read often, unlike other VM counters, so it's not a problem
    to use a large threshold and make readers more expensive.
    
    With this patchset, we see 31.3% drop of CPU cycles(537-->369, see
    below) for per single page allocation and reclaim on Jesper's
    page_bench03 benchmark.  Meanwhile, this patchset keeps the same style
    of virtual memory statistics with little end-user-visible effects (only
    move the numa stats to show behind zone page stats, see the first patch
    for details).
    
    I did an experiment of single page allocation and reclaim concurrently
    using Jesper's page_bench03 benchmark on a 2-Socket Broadwell-based
    server (88 processors with 126G memory) with different size of threshold
    of pcp counter.
    
    Benchmark provided by Jesper D Brouer(increase loop times to 10000000):
      https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm/bench
    
       Threshold   CPU cycles    Throughput(88 threads)
          32        799         241760478
          64        640         301628829
          125       537         358906028 <==> system by default
          256       468         412397590
          512       428         450550704
          4096      399         482520943
          20000     394         489009617
          30000     395         488017817
          65533     369(-31.3%) 521661345(+45.3%) <==> with this patchset
          N/A       342(-36.3%) 562900157(+56.8%) <==> disable zone_statistics
    
    This patch (of 3):
    
    In this patch, NUMA statistics is separated from zone statistics
    framework, all the call sites of NUMA stats are changed to use
    numa-stats-specific functions, it does not have any functionality change
    except that the number of NUMA stats is shown behind zone page stats
    when users *read* the zone info.
    
    E.g. cat /proc/zoneinfo
        ***Base***                           ***With this patch***
    nr_free_pages 3976                         nr_free_pages 3976
    nr_zone_inactive_anon 0                    nr_zone_inactive_anon 0
    nr_zone_active_anon 0                      nr_zone_active_anon 0
    nr_zone_inactive_file 0                    nr_zone_inactive_file 0
    nr_zone_active_file 0                      nr_zone_active_file 0
    nr_zone_unevictable 0                      nr_zone_unevictable 0
    nr_zone_write_pending 0                    nr_zone_write_pending 0
    nr_mlock     0                             nr_mlock     0
    nr_page_table_pages 0                      nr_page_table_pages 0
    nr_kernel_stack 0                          nr_kernel_stack 0
    nr_bounce    0                             nr_bounce    0
    nr_zspages   0                             nr_zspages   0
    numa_hit 0                                *nr_free_cma  0*
    numa_miss 0                                numa_hit     0
    numa_foreign 0                             numa_miss    0
    numa_interleave 0                          numa_foreign 0
    numa_local   0                             numa_interleave 0
    numa_other   0                             numa_local   0
    *nr_free_cma 0*                            numa_other 0
        ...                                        ...
    vm stats threshold: 10                     vm stats threshold: 10
        ...                                        ...
    
    The next patch updates the numa stats counter size and threshold.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/1503568801-21305-2-git-send-email-kemi.wang@intel.com
    Signed-off-by: Kemi Wang <kemi.wang@intel.com>
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Ying Huang <ying.huang@intel.com>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 97e11ab573f0..9ac82e29948f 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -107,8 +107,33 @@ static inline void vm_events_fold_cpu(int cpu)
  * Zone and node-based page accounting with per cpu differentials.
  */
 extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];
+extern atomic_long_t vm_numa_stat[NR_VM_NUMA_STAT_ITEMS];
 extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];
 
+#ifdef CONFIG_NUMA
+static inline void zone_numa_state_add(long x, struct zone *zone,
+				 enum numa_stat_item item)
+{
+	atomic_long_add(x, &zone->vm_numa_stat[item]);
+	atomic_long_add(x, &vm_numa_stat[item]);
+}
+
+static inline unsigned long global_numa_state(enum numa_stat_item item)
+{
+	long x = atomic_long_read(&vm_numa_stat[item]);
+
+	return x;
+}
+
+static inline unsigned long zone_numa_state(struct zone *zone,
+					enum numa_stat_item item)
+{
+	long x = atomic_long_read(&zone->vm_numa_stat[item]);
+
+	return x;
+}
+#endif /* CONFIG_NUMA */
+
 static inline void zone_page_state_add(long x, struct zone *zone,
 				 enum zone_stat_item item)
 {
@@ -194,8 +219,10 @@ static inline unsigned long node_page_state_snapshot(pg_data_t *pgdat,
 
 
 #ifdef CONFIG_NUMA
+extern void __inc_numa_state(struct zone *zone, enum numa_stat_item item);
 extern unsigned long sum_zone_node_page_state(int node,
-						enum zone_stat_item item);
+					      enum zone_stat_item item);
+extern unsigned long sum_zone_numa_state(int node, enum numa_stat_item item);
 extern unsigned long node_page_state(struct pglist_data *pgdat,
 						enum node_stat_item item);
 #else

commit c41f012ade0b95b0a6e25c7150673e0554736165
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 6 16:23:36 2017 -0700

    mm: rename global_page_state to global_zone_page_state
    
    global_page_state is error prone as a recent bug report pointed out [1].
    It only returns proper values for zone based counters as the enum it
    gets suggests.  We already have global_node_page_state so let's rename
    global_page_state to global_zone_page_state to be more explicit here.
    All existing users seems to be correct:
    
    $ git grep "global_page_state(NR_" | sed 's@.*(\(NR_[A-Z_]*\)).*@\1@' | sort | uniq -c
          2 NR_BOUNCE
          2 NR_FREE_CMA_PAGES
         11 NR_FREE_PAGES
          1 NR_KERNEL_STACK_KB
          1 NR_MLOCK
          2 NR_PAGETABLE
    
    This patch shouldn't introduce any functional change.
    
    [1] http://lkml.kernel.org/r/201707260628.v6Q6SmaS030814@www262.sakura.ne.jp
    
    Link: http://lkml.kernel.org/r/20170801134256.5400-2-hannes@cmpxchg.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index b3d85f30d424..97e11ab573f0 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -123,7 +123,7 @@ static inline void node_page_state_add(long x, struct pglist_data *pgdat,
 	atomic_long_add(x, &vm_node_stat[item]);
 }
 
-static inline unsigned long global_page_state(enum zone_stat_item item)
+static inline unsigned long global_zone_page_state(enum zone_stat_item item)
 {
 	long x = atomic_long_read(&vm_zone_stat[item]);
 #ifdef CONFIG_SMP
@@ -199,7 +199,7 @@ extern unsigned long sum_zone_node_page_state(int node,
 extern unsigned long node_page_state(struct pglist_data *pgdat,
 						enum node_stat_item item);
 #else
-#define sum_zone_node_page_state(node, item) global_page_state(item)
+#define sum_zone_node_page_state(node, item) global_zone_page_state(item)
 #define node_page_state(node, item) global_node_page_state(item)
 #endif /* CONFIG_NUMA */
 

commit 00f3ca2c2d6635d85108571c4dd9a29088668662
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jul 6 15:40:52 2017 -0700

    mm: memcontrol: per-lruvec stats infrastructure
    
    lruvecs are at the intersection of the NUMA node and memcg, which is the
    scope for most paging activity.
    
    Introduce a convenient accounting infrastructure that maintains
    statistics per node, per memcg, and the lruvec itself.
    
    Then convert over accounting sites for statistics that are already
    tracked in both nodes and memcgs and can be easily switched.
    
    [hannes@cmpxchg.org: fix crash in the new cgroup stat keeping code]
      Link: http://lkml.kernel.org/r/20170531171450.GA10481@cmpxchg.org
    [hannes@cmpxchg.org: don't track uncharged pages at all
      Link: http://lkml.kernel.org/r/20170605175254.GA8547@cmpxchg.org
    [hannes@cmpxchg.org: add missing free_percpu()]
      Link: http://lkml.kernel.org/r/20170605175354.GB8547@cmpxchg.org
    [linux@roeck-us.net: hexagon: fix build error caused by include file order]
      Link: http://lkml.kernel.org/r/20170617153721.GA4382@roeck-us.net
    Link: http://lkml.kernel.org/r/20170530181724.27197-6-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 613771909b6e..b3d85f30d424 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -3,7 +3,6 @@
 
 #include <linux/types.h>
 #include <linux/percpu.h>
-#include <linux/mm.h>
 #include <linux/mmzone.h>
 #include <linux/vm_event_item.h>
 #include <linux/atomic.h>

commit 16709d1de1954475356a65848f80a01581b4903c
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:56 2016 -0700

    mm: vmstat: replace __count_zone_vm_events with a zone id equivalent
    
    This is partially a preparation patch for more vmstat work but it also
    has the slight advantage that __count_zid_vm_events is cheaper to
    calculate than __count_zone_vm_events().
    
    Link: http://lkml.kernel.org/r/1467970510-21195-32-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 6b7975cd98aa..613771909b6e 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -101,9 +101,8 @@ static inline void vm_events_fold_cpu(int cpu)
 #define count_vm_vmacache_event(x) do {} while (0)
 #endif
 
-#define __count_zone_vm_events(item, zone, delta) \
-		__count_vm_events(item##_NORMAL - ZONE_NORMAL + \
-		zone_idx(zone), delta)
+#define __count_zid_vm_events(item, zid, delta) \
+	__count_vm_events(item##_NORMAL - ZONE_NORMAL + zid, delta)
 
 /*
  * Zone and node-based page accounting with per cpu differentials.

commit 1e6b10857f91685c60c341703ece4ae9bb775cf3
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:46:08 2016 -0700

    mm, workingset: make working set detection node-aware
    
    Working set and refault detection is still zone-based, fix it.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-16-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index fee321c98550..6b7975cd98aa 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -227,7 +227,6 @@ void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);
 void inc_node_page_state(struct page *, enum node_stat_item);
 void dec_node_page_state(struct page *, enum node_stat_item);
 
-extern void inc_zone_state(struct zone *, enum zone_stat_item);
 extern void inc_node_state(struct pglist_data *, enum node_stat_item);
 extern void __inc_zone_state(struct zone *, enum zone_stat_item);
 extern void __inc_node_state(struct pglist_data *, enum node_stat_item);

commit 599d0c954f91d0689c9bb421b5bc04ea02437a41
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:31 2016 -0700

    mm, vmscan: move LRU lists to node
    
    This moves the LRU lists from the zone to the node and related data such
    as counters, tracing, congestion tracking and writeback tracking.
    
    Unfortunately, due to reclaim and compaction retry logic, it is
    necessary to account for the number of LRU pages on both zone and node
    logic.  Most reclaim logic is based on the node counters but the retry
    logic uses the zone counters which do not distinguish inactive and
    active sizes.  It would be possible to leave the LRU counters on a
    per-zone basis but it's a heavier calculation across multiple cache
    lines that is much more frequent than the retry checks.
    
    Other than the LRU counters, this is mostly a mechanical patch but note
    that it introduces a number of anomalies.  For example, the scans are
    per-zone but using per-node counters.  We also mark a node as congested
    when a zone is congested.  This causes weird problems that are fixed
    later but is easier to review.
    
    In the event that there is excessive overhead on 32-bit systems due to
    the nodes being on LRU then there are two potential solutions
    
    1. Long-term isolation of highmem pages when reclaim is lowmem
    
       When pages are skipped, they are immediately added back onto the LRU
       list. If lowmem reclaim persisted for long periods of time, the same
       highmem pages get continually scanned. The idea would be that lowmem
       keeps those pages on a separate list until a reclaim for highmem pages
       arrives that splices the highmem pages back onto the LRU. It potentially
       could be implemented similar to the UNEVICTABLE list.
    
       That would reduce the skip rate with the potential corner case is that
       highmem pages have to be scanned and reclaimed to free lowmem slab pages.
    
    2. Linear scan lowmem pages if the initial LRU shrink fails
    
       This will break LRU ordering but may be preferable and faster during
       memory pressure than skipping LRU pages.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-4-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d1744aa3ab9c..fee321c98550 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -178,6 +178,23 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 	return x;
 }
 
+static inline unsigned long node_page_state_snapshot(pg_data_t *pgdat,
+					enum node_stat_item item)
+{
+	long x = atomic_long_read(&pgdat->vm_stat[item]);
+
+#ifdef CONFIG_SMP
+	int cpu;
+	for_each_online_cpu(cpu)
+		x += per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->vm_node_stat_diff[item];
+
+	if (x < 0)
+		x = 0;
+#endif
+	return x;
+}
+
+
 #ifdef CONFIG_NUMA
 extern unsigned long sum_zone_node_page_state(int node,
 						enum zone_stat_item item);

commit 75ef7184053989118d3814c558a9af62e7376a58
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu Jul 28 15:45:24 2016 -0700

    mm, vmstat: add infrastructure for per-node vmstats
    
    Patchset: "Move LRU page reclaim from zones to nodes v9"
    
    This series moves LRUs from the zones to the node.  While this is a
    current rebase, the test results were based on mmotm as of June 23rd.
    Conceptually, this series is simple but there are a lot of details.
    Some of the broad motivations for this are;
    
    1. The residency of a page partially depends on what zone the page was
       allocated from.  This is partially combatted by the fair zone allocation
       policy but that is a partial solution that introduces overhead in the
       page allocator paths.
    
    2. Currently, reclaim on node 0 behaves slightly different to node 1. For
       example, direct reclaim scans in zonelist order and reclaims even if
       the zone is over the high watermark regardless of the age of pages
       in that LRU. Kswapd on the other hand starts reclaim on the highest
       unbalanced zone. A difference in distribution of file/anon pages due
       to when they were allocated results can result in a difference in
       again. While the fair zone allocation policy mitigates some of the
       problems here, the page reclaim results on a multi-zone node will
       always be different to a single-zone node.
       it was scheduled on as a result.
    
    3. kswapd and the page allocator scan zones in the opposite order to
       avoid interfering with each other but it's sensitive to timing.  This
       mitigates the page allocator using pages that were allocated very recently
       in the ideal case but it's sensitive to timing. When kswapd is allocating
       from lower zones then it's great but during the rebalancing of the highest
       zone, the page allocator and kswapd interfere with each other. It's worse
       if the highest zone is small and difficult to balance.
    
    4. slab shrinkers are node-based which makes it harder to identify the exact
       relationship between slab reclaim and LRU reclaim.
    
    The reason we have zone-based reclaim is that we used to have
    large highmem zones in common configurations and it was necessary
    to quickly find ZONE_NORMAL pages for reclaim. Today, this is much
    less of a concern as machines with lots of memory will (or should) use
    64-bit kernels. Combinations of 32-bit hardware and 64-bit hardware are
    rare. Machines that do use highmem should have relatively low highmem:lowmem
    ratios than we worried about in the past.
    
    Conceptually, moving to node LRUs should be easier to understand. The
    page allocator plays fewer tricks to game reclaim and reclaim behaves
    similarly on all nodes.
    
    The series has been tested on a 16 core UMA machine and a 2-socket 48
    core NUMA machine. The UMA results are presented in most cases as the NUMA
    machine behaved similarly.
    
    pagealloc
    ---------
    
    This is a microbenchmark that shows the benefit of removing the fair zone
    allocation policy. It was tested uip to order-4 but only orders 0 and 1 are
    shown as the other orders were comparable.
    
                                               4.7.0-rc4                  4.7.0-rc4
                                          mmotm-20160623                 nodelru-v9
    Min      total-odr0-1               490.00 (  0.00%)           457.00 (  6.73%)
    Min      total-odr0-2               347.00 (  0.00%)           329.00 (  5.19%)
    Min      total-odr0-4               288.00 (  0.00%)           273.00 (  5.21%)
    Min      total-odr0-8               251.00 (  0.00%)           239.00 (  4.78%)
    Min      total-odr0-16              234.00 (  0.00%)           222.00 (  5.13%)
    Min      total-odr0-32              223.00 (  0.00%)           211.00 (  5.38%)
    Min      total-odr0-64              217.00 (  0.00%)           208.00 (  4.15%)
    Min      total-odr0-128             214.00 (  0.00%)           204.00 (  4.67%)
    Min      total-odr0-256             250.00 (  0.00%)           230.00 (  8.00%)
    Min      total-odr0-512             271.00 (  0.00%)           269.00 (  0.74%)
    Min      total-odr0-1024            291.00 (  0.00%)           282.00 (  3.09%)
    Min      total-odr0-2048            303.00 (  0.00%)           296.00 (  2.31%)
    Min      total-odr0-4096            311.00 (  0.00%)           309.00 (  0.64%)
    Min      total-odr0-8192            316.00 (  0.00%)           314.00 (  0.63%)
    Min      total-odr0-16384           317.00 (  0.00%)           315.00 (  0.63%)
    Min      total-odr1-1               742.00 (  0.00%)           712.00 (  4.04%)
    Min      total-odr1-2               562.00 (  0.00%)           530.00 (  5.69%)
    Min      total-odr1-4               457.00 (  0.00%)           433.00 (  5.25%)
    Min      total-odr1-8               411.00 (  0.00%)           381.00 (  7.30%)
    Min      total-odr1-16              381.00 (  0.00%)           356.00 (  6.56%)
    Min      total-odr1-32              372.00 (  0.00%)           346.00 (  6.99%)
    Min      total-odr1-64              372.00 (  0.00%)           343.00 (  7.80%)
    Min      total-odr1-128             375.00 (  0.00%)           351.00 (  6.40%)
    Min      total-odr1-256             379.00 (  0.00%)           351.00 (  7.39%)
    Min      total-odr1-512             385.00 (  0.00%)           355.00 (  7.79%)
    Min      total-odr1-1024            386.00 (  0.00%)           358.00 (  7.25%)
    Min      total-odr1-2048            390.00 (  0.00%)           362.00 (  7.18%)
    Min      total-odr1-4096            390.00 (  0.00%)           362.00 (  7.18%)
    Min      total-odr1-8192            388.00 (  0.00%)           363.00 (  6.44%)
    
    This shows a steady improvement throughout. The primary benefit is from
    reduced system CPU usage which is obvious from the overall times;
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623nodelru-v8
    User          189.19      191.80
    System       2604.45     2533.56
    Elapsed      2855.30     2786.39
    
    The vmstats also showed that the fair zone allocation policy was definitely
    removed as can be seen here;
    
                                 4.7.0-rc3   4.7.0-rc3
                             mmotm-20160623 nodelru-v8
    DMA32 allocs               28794729769           0
    Normal allocs              48432501431 77227309877
    Movable allocs                       0           0
    
    tiobench on ext4
    ----------------
    
    tiobench is a benchmark that artifically benefits if old pages remain resident
    while new pages get reclaimed. The fair zone allocation policy mitigates this
    problem so pages age fairly. While the benchmark has problems, it is important
    that tiobench performance remains constant as it implies that page aging
    problems that the fair zone allocation policy fixes are not re-introduced.
    
                                             4.7.0-rc4             4.7.0-rc4
                                        mmotm-20160623            nodelru-v9
    Min      PotentialReadSpeed        89.65 (  0.00%)       90.21 (  0.62%)
    Min      SeqRead-MB/sec-1          82.68 (  0.00%)       82.01 ( -0.81%)
    Min      SeqRead-MB/sec-2          72.76 (  0.00%)       72.07 ( -0.95%)
    Min      SeqRead-MB/sec-4          75.13 (  0.00%)       74.92 ( -0.28%)
    Min      SeqRead-MB/sec-8          64.91 (  0.00%)       65.19 (  0.43%)
    Min      SeqRead-MB/sec-16         62.24 (  0.00%)       62.22 ( -0.03%)
    Min      RandRead-MB/sec-1          0.88 (  0.00%)        0.88 (  0.00%)
    Min      RandRead-MB/sec-2          0.95 (  0.00%)        0.92 ( -3.16%)
    Min      RandRead-MB/sec-4          1.43 (  0.00%)        1.34 ( -6.29%)
    Min      RandRead-MB/sec-8          1.61 (  0.00%)        1.60 ( -0.62%)
    Min      RandRead-MB/sec-16         1.80 (  0.00%)        1.90 (  5.56%)
    Min      SeqWrite-MB/sec-1         76.41 (  0.00%)       76.85 (  0.58%)
    Min      SeqWrite-MB/sec-2         74.11 (  0.00%)       73.54 ( -0.77%)
    Min      SeqWrite-MB/sec-4         80.05 (  0.00%)       80.13 (  0.10%)
    Min      SeqWrite-MB/sec-8         72.88 (  0.00%)       73.20 (  0.44%)
    Min      SeqWrite-MB/sec-16        75.91 (  0.00%)       76.44 (  0.70%)
    Min      RandWrite-MB/sec-1         1.18 (  0.00%)        1.14 ( -3.39%)
    Min      RandWrite-MB/sec-2         1.02 (  0.00%)        1.03 (  0.98%)
    Min      RandWrite-MB/sec-4         1.05 (  0.00%)        0.98 ( -6.67%)
    Min      RandWrite-MB/sec-8         0.89 (  0.00%)        0.92 (  3.37%)
    Min      RandWrite-MB/sec-16        0.92 (  0.00%)        0.93 (  1.09%)
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623 approx-v9
    User          645.72      525.90
    System        403.85      331.75
    Elapsed      6795.36     6783.67
    
    This shows that the series has little or not impact on tiobench which is
    desirable and a reduction in system CPU usage. It indicates that the fair
    zone allocation policy was removed in a manner that didn't reintroduce
    one class of page aging bug. There were only minor differences in overall
    reclaim activity
    
                                 4.7.0-rc4   4.7.0-rc4
                              mmotm-20160623nodelru-v8
    Minor Faults                    645838      647465
    Major Faults                       573         640
    Swap Ins                             0           0
    Swap Outs                            0           0
    DMA allocs                           0           0
    DMA32 allocs                  46041453    44190646
    Normal allocs                 78053072    79887245
    Movable allocs                       0           0
    Allocation stalls                   24          67
    Stall zone DMA                       0           0
    Stall zone DMA32                     0           0
    Stall zone Normal                    0           2
    Stall zone HighMem                   0           0
    Stall zone Movable                   0          65
    Direct pages scanned             10969       30609
    Kswapd pages scanned          93375144    93492094
    Kswapd pages reclaimed        93372243    93489370
    Direct pages reclaimed           10969       30609
    Kswapd efficiency                  99%         99%
    Kswapd velocity              13741.015   13781.934
    Direct efficiency                 100%        100%
    Direct velocity                  1.614       4.512
    Percentage direct scans             0%          0%
    
    kswapd activity was roughly comparable. There were differences in direct
    reclaim activity but negligible in the context of the overall workload
    (velocity of 4 pages per second with the patches applied, 1.6 pages per
    second in the baseline kernel).
    
    pgbench read-only large configuration on ext4
    ---------------------------------------------
    
    pgbench is a database benchmark that can be sensitive to page reclaim
    decisions. This also checks if removing the fair zone allocation policy
    is safe
    
    pgbench Transactions
                            4.7.0-rc4             4.7.0-rc4
                       mmotm-20160623            nodelru-v8
    Hmean    1       188.26 (  0.00%)      189.78 (  0.81%)
    Hmean    5       330.66 (  0.00%)      328.69 ( -0.59%)
    Hmean    12      370.32 (  0.00%)      380.72 (  2.81%)
    Hmean    21      368.89 (  0.00%)      369.00 (  0.03%)
    Hmean    30      382.14 (  0.00%)      360.89 ( -5.56%)
    Hmean    32      428.87 (  0.00%)      432.96 (  0.95%)
    
    Negligible differences again. As with tiobench, overall reclaim activity
    was comparable.
    
    bonnie++ on ext4
    ----------------
    
    No interesting performance difference, negligible differences on reclaim
    stats.
    
    paralleldd on ext4
    ------------------
    
    This workload uses varying numbers of dd instances to read large amounts of
    data from disk.
    
                                   4.7.0-rc3             4.7.0-rc3
                              mmotm-20160623            nodelru-v9
    Amean    Elapsd-1       186.04 (  0.00%)      189.41 ( -1.82%)
    Amean    Elapsd-3       192.27 (  0.00%)      191.38 (  0.46%)
    Amean    Elapsd-5       185.21 (  0.00%)      182.75 (  1.33%)
    Amean    Elapsd-7       183.71 (  0.00%)      182.11 (  0.87%)
    Amean    Elapsd-12      180.96 (  0.00%)      181.58 ( -0.35%)
    Amean    Elapsd-16      181.36 (  0.00%)      183.72 ( -1.30%)
    
               4.7.0-rc4   4.7.0-rc4
            mmotm-20160623 nodelru-v9
    User         1548.01     1552.44
    System       8609.71     8515.08
    Elapsed      3587.10     3594.54
    
    There is little or no change in performance but some drop in system CPU usage.
    
                                 4.7.0-rc3   4.7.0-rc3
                            mmotm-20160623  nodelru-v9
    Minor Faults                    362662      367360
    Major Faults                      1204        1143
    Swap Ins                            22           0
    Swap Outs                         2855        1029
    DMA allocs                           0           0
    DMA32 allocs                  31409797    28837521
    Normal allocs                 46611853    49231282
    Movable allocs                       0           0
    Direct pages scanned                 0           0
    Kswapd pages scanned          40845270    40869088
    Kswapd pages reclaimed        40830976    40855294
    Direct pages reclaimed               0           0
    Kswapd efficiency                  99%         99%
    Kswapd velocity              11386.711   11369.769
    Direct efficiency                 100%        100%
    Direct velocity                  0.000       0.000
    Percentage direct scans             0%          0%
    Page writes by reclaim            2855        1029
    Page writes file                     0           0
    Page writes anon                  2855        1029
    Page reclaim immediate             771        1628
    Sector Reads                 293312636   293536360
    Sector Writes                 18213568    18186480
    Page rescued immediate               0           0
    Slabs scanned                   128257      132747
    Direct inode steals                181          56
    Kswapd inode steals                 59        1131
    
    It basically shows that kswapd was active at roughly the same rate in
    both kernels. There was also comparable slab scanning activity and direct
    reclaim was avoided in both cases. There appears to be a large difference
    in numbers of inodes reclaimed but the workload has few active inodes and
    is likely a timing artifact.
    
    stutter
    -------
    
    stutter simulates a simple workload. One part uses a lot of anonymous
    memory, a second measures mmap latency and a third copies a large file.
    The primary metric is checking for mmap latency.
    
    stutter
                                 4.7.0-rc4             4.7.0-rc4
                            mmotm-20160623            nodelru-v8
    Min         mmap     16.6283 (  0.00%)     13.4258 ( 19.26%)
    1st-qrtle   mmap     54.7570 (  0.00%)     34.9121 ( 36.24%)
    2nd-qrtle   mmap     57.3163 (  0.00%)     46.1147 ( 19.54%)
    3rd-qrtle   mmap     58.9976 (  0.00%)     47.1882 ( 20.02%)
    Max-90%     mmap     59.7433 (  0.00%)     47.4453 ( 20.58%)
    Max-93%     mmap     60.1298 (  0.00%)     47.6037 ( 20.83%)
    Max-95%     mmap     73.4112 (  0.00%)     82.8719 (-12.89%)
    Max-99%     mmap     92.8542 (  0.00%)     88.8870 (  4.27%)
    Max         mmap   1440.6569 (  0.00%)    121.4201 ( 91.57%)
    Mean        mmap     59.3493 (  0.00%)     42.2991 ( 28.73%)
    Best99%Mean mmap     57.2121 (  0.00%)     41.8207 ( 26.90%)
    Best95%Mean mmap     55.9113 (  0.00%)     39.9620 ( 28.53%)
    Best90%Mean mmap     55.6199 (  0.00%)     39.3124 ( 29.32%)
    Best50%Mean mmap     53.2183 (  0.00%)     33.1307 ( 37.75%)
    Best10%Mean mmap     45.9842 (  0.00%)     20.4040 ( 55.63%)
    Best5%Mean  mmap     43.2256 (  0.00%)     17.9654 ( 58.44%)
    Best1%Mean  mmap     32.9388 (  0.00%)     16.6875 ( 49.34%)
    
    This shows a number of improvements with the worst-case outlier greatly
    improved.
    
    Some of the vmstats are interesting
    
                                 4.7.0-rc4   4.7.0-rc4
                              mmotm-20160623nodelru-v8
    Swap Ins                           163         502
    Swap Outs                            0           0
    DMA allocs                           0           0
    DMA32 allocs                 618719206  1381662383
    Normal allocs                891235743   564138421
    Movable allocs                       0           0
    Allocation stalls                 2603           1
    Direct pages scanned            216787           2
    Kswapd pages scanned          50719775    41778378
    Kswapd pages reclaimed        41541765    41777639
    Direct pages reclaimed          209159           0
    Kswapd efficiency                  81%         99%
    Kswapd velocity              16859.554   14329.059
    Direct efficiency                  96%          0%
    Direct velocity                 72.061       0.001
    Percentage direct scans             0%          0%
    Page writes by reclaim         6215049           0
    Page writes file               6215049           0
    Page writes anon                     0           0
    Page reclaim immediate           70673          90
    Sector Reads                  81940800    81680456
    Sector Writes                100158984    98816036
    Page rescued immediate               0           0
    Slabs scanned                  1366954       22683
    
    While this is not guaranteed in all cases, this particular test showed
    a large reduction in direct reclaim activity. It's also worth noting
    that no page writes were issued from reclaim context.
    
    This series is not without its hazards. There are at least three areas
    that I'm concerned with even though I could not reproduce any problems in
    that area.
    
    1. Reclaim/compaction is going to be affected because the amount of reclaim is
       no longer targetted at a specific zone. Compaction works on a per-zone basis
       so there is no guarantee that reclaiming a few THP's worth page pages will
       have a positive impact on compaction success rates.
    
    2. The Slab/LRU reclaim ratio is affected because the frequency the shrinkers
       are called is now different. This may or may not be a problem but if it
       is, it'll be because shrinkers are not called enough and some balancing
       is required.
    
    3. The anon/file reclaim ratio may be affected. Pages about to be dirtied are
       distributed between zones and the fair zone allocation policy used to do
       something very similar for anon. The distribution is now different but not
       necessarily in any way that matters but it's still worth bearing in mind.
    
    VM statistic counters for reclaim decisions are zone-based.  If the kernel
    is to reclaim on a per-node basis then we need to track per-node
    statistics but there is no infrastructure for that.  The most notable
    change is that the old node_page_state is renamed to
    sum_zone_node_page_state.  The new node_page_state takes a pglist_data and
    uses per-node stats but none exist yet.  There is some renaming such as
    vm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming
    of mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical
    patch with no functional change.  There is a lot of similarity between the
    node and zone helpers which is unfortunate but there was no obvious way of
    reusing the code and maintaining type safety.
    
    Link: http://lkml.kernel.org/r/1467970510-21195-2-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d2da8e053210..d1744aa3ab9c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -106,20 +106,38 @@ static inline void vm_events_fold_cpu(int cpu)
 		zone_idx(zone), delta)
 
 /*
- * Zone based page accounting with per cpu differentials.
+ * Zone and node-based page accounting with per cpu differentials.
  */
-extern atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
+extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];
+extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];
 
 static inline void zone_page_state_add(long x, struct zone *zone,
 				 enum zone_stat_item item)
 {
 	atomic_long_add(x, &zone->vm_stat[item]);
-	atomic_long_add(x, &vm_stat[item]);
+	atomic_long_add(x, &vm_zone_stat[item]);
+}
+
+static inline void node_page_state_add(long x, struct pglist_data *pgdat,
+				 enum node_stat_item item)
+{
+	atomic_long_add(x, &pgdat->vm_stat[item]);
+	atomic_long_add(x, &vm_node_stat[item]);
 }
 
 static inline unsigned long global_page_state(enum zone_stat_item item)
 {
-	long x = atomic_long_read(&vm_stat[item]);
+	long x = atomic_long_read(&vm_zone_stat[item]);
+#ifdef CONFIG_SMP
+	if (x < 0)
+		x = 0;
+#endif
+	return x;
+}
+
+static inline unsigned long global_node_page_state(enum node_stat_item item)
+{
+	long x = atomic_long_read(&vm_node_stat[item]);
 #ifdef CONFIG_SMP
 	if (x < 0)
 		x = 0;
@@ -161,31 +179,44 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 }
 
 #ifdef CONFIG_NUMA
-
-extern unsigned long node_page_state(int node, enum zone_stat_item item);
-
+extern unsigned long sum_zone_node_page_state(int node,
+						enum zone_stat_item item);
+extern unsigned long node_page_state(struct pglist_data *pgdat,
+						enum node_stat_item item);
 #else
-
-#define node_page_state(node, item) global_page_state(item)
-
+#define sum_zone_node_page_state(node, item) global_page_state(item)
+#define node_page_state(node, item) global_node_page_state(item)
 #endif /* CONFIG_NUMA */
 
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
+#define add_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, __d)
+#define sub_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, -(__d))
 
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);
 void __dec_zone_page_state(struct page *, enum zone_stat_item);
 
+void __mod_node_page_state(struct pglist_data *, enum node_stat_item item, long);
+void __inc_node_page_state(struct page *, enum node_stat_item);
+void __dec_node_page_state(struct page *, enum node_stat_item);
+
 void mod_zone_page_state(struct zone *, enum zone_stat_item, long);
 void inc_zone_page_state(struct page *, enum zone_stat_item);
 void dec_zone_page_state(struct page *, enum zone_stat_item);
 
+void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);
+void inc_node_page_state(struct page *, enum node_stat_item);
+void dec_node_page_state(struct page *, enum node_stat_item);
+
 extern void inc_zone_state(struct zone *, enum zone_stat_item);
+extern void inc_node_state(struct pglist_data *, enum node_stat_item);
 extern void __inc_zone_state(struct zone *, enum zone_stat_item);
+extern void __inc_node_state(struct pglist_data *, enum node_stat_item);
 extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
+extern void __dec_node_state(struct pglist_data *, enum node_stat_item);
 
 void quiet_vmstat(void);
 void cpu_vm_stats_fold(int cpu);
@@ -213,16 +244,34 @@ static inline void __mod_zone_page_state(struct zone *zone,
 	zone_page_state_add(delta, zone, item);
 }
 
+static inline void __mod_node_page_state(struct pglist_data *pgdat,
+			enum node_stat_item item, int delta)
+{
+	node_page_state_add(delta, pgdat, item);
+}
+
 static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_inc(&zone->vm_stat[item]);
-	atomic_long_inc(&vm_stat[item]);
+	atomic_long_inc(&vm_zone_stat[item]);
+}
+
+static inline void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
+{
+	atomic_long_inc(&pgdat->vm_stat[item]);
+	atomic_long_inc(&vm_node_stat[item]);
 }
 
 static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_dec(&zone->vm_stat[item]);
-	atomic_long_dec(&vm_stat[item]);
+	atomic_long_dec(&vm_zone_stat[item]);
+}
+
+static inline void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
+{
+	atomic_long_dec(&pgdat->vm_stat[item]);
+	atomic_long_dec(&vm_node_stat[item]);
 }
 
 static inline void __inc_zone_page_state(struct page *page,
@@ -231,12 +280,26 @@ static inline void __inc_zone_page_state(struct page *page,
 	__inc_zone_state(page_zone(page), item);
 }
 
+static inline void __inc_node_page_state(struct page *page,
+			enum node_stat_item item)
+{
+	__inc_node_state(page_pgdat(page), item);
+}
+
+
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
 	__dec_zone_state(page_zone(page), item);
 }
 
+static inline void __dec_node_page_state(struct page *page,
+			enum node_stat_item item)
+{
+	__dec_node_state(page_pgdat(page), item);
+}
+
+
 /*
  * We only use atomic operations to update counters. So there is no need to
  * disable interrupts.
@@ -245,7 +308,12 @@ static inline void __dec_zone_page_state(struct page *page,
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
+#define inc_node_page_state __inc_node_page_state
+#define dec_node_page_state __dec_node_page_state
+#define mod_node_page_state __mod_node_page_state
+
 #define inc_zone_state __inc_zone_state
+#define inc_node_state __inc_node_state
 #define dec_zone_state __dec_zone_state
 
 #define set_pgdat_percpu_threshold(pgdat, callback) { }

commit 060e74173f292fb3e0398b3dca8765568d195ff1
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Thu May 19 17:13:27 2016 -0700

    mm, page_alloc: inline zone_statistics
    
    zone_statistics has one call-site but it's a public function.  Make it
    static and inline.
    
    The performance difference on a page allocator microbenchmark is;
    
                                                 4.6.0-rc2                  4.6.0-rc2
                                          statbranch-v1r20           statinline-v1r20
      Min      alloc-odr0-1               419.00 (  0.00%)           412.00 (  1.67%)
      Min      alloc-odr0-2               305.00 (  0.00%)           301.00 (  1.31%)
      Min      alloc-odr0-4               250.00 (  0.00%)           247.00 (  1.20%)
      Min      alloc-odr0-8               219.00 (  0.00%)           215.00 (  1.83%)
      Min      alloc-odr0-16              203.00 (  0.00%)           199.00 (  1.97%)
      Min      alloc-odr0-32              195.00 (  0.00%)           191.00 (  2.05%)
      Min      alloc-odr0-64              191.00 (  0.00%)           187.00 (  2.09%)
      Min      alloc-odr0-128             189.00 (  0.00%)           185.00 (  2.12%)
      Min      alloc-odr0-256             198.00 (  0.00%)           193.00 (  2.53%)
      Min      alloc-odr0-512             210.00 (  0.00%)           207.00 (  1.43%)
      Min      alloc-odr0-1024            216.00 (  0.00%)           213.00 (  1.39%)
      Min      alloc-odr0-2048            221.00 (  0.00%)           220.00 (  0.45%)
      Min      alloc-odr0-4096            227.00 (  0.00%)           226.00 (  0.44%)
      Min      alloc-odr0-8192            232.00 (  0.00%)           229.00 (  1.29%)
      Min      alloc-odr0-16384           232.00 (  0.00%)           229.00 (  1.29%)
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 02fce415b3d9..d2da8e053210 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -163,12 +163,10 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 #ifdef CONFIG_NUMA
 
 extern unsigned long node_page_state(int node, enum zone_stat_item item);
-extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 
 #else
 
 #define node_page_state(node, item) global_page_state(item)
-#define zone_statistics(_zl, _z, gfp) do { } while (0)
 
 #endif /* CONFIG_NUMA */
 

commit 52b6f46bc163eef17ecba4cd552beeafe2b24453
Author: Hugh Dickins <hughd@google.com>
Date:   Thu May 19 17:12:50 2016 -0700

    mm: /proc/sys/vm/stat_refresh to force vmstat update
    
    Provide /proc/sys/vm/stat_refresh to force an immediate update of
    per-cpu into global vmstats: useful to avoid a sleep(2) or whatever
    before checking counts when testing.  Originally added to work around a
    bug which left counts stranded indefinitely on a cpu going idle (an
    inaccuracy magnified when small below-batch numbers represent "huge"
    amounts of memory), but I believe that bug is now fixed: nonetheless,
    this is still a useful knob.
    
    Its schedule_on_each_cpu() is probably too expensive just to fold into
    reading /proc/meminfo itself: give this mode 0600 to prevent abuse.
    Allow a write or a read to do the same: nothing to read, but "grep -h
    Shmem /proc/sys/vm/stat_refresh /proc/meminfo" is convenient.  Oh, and
    since global_page_state() itself is careful to disguise any underflow as
    0, hack in an "Invalid argument" and pr_warn() if a counter is negative
    after the refresh - this helped to fix a misaccounting of
    NR_ISOLATED_FILE in my migration code.
    
    But on recent kernels, I find that NR_ALLOC_BATCH and NR_PAGES_SCANNED
    often go negative some of the time.  I have not yet worked out why, but
    have no evidence that it's actually harmful.  Punt for the moment by
    just ignoring the anomaly on those.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Yang Shi <yang.shi@linaro.org>
    Cc: Ning Qu <quning@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 73fae8c4a5fb..02fce415b3d9 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -193,6 +193,10 @@ void quiet_vmstat(void);
 void cpu_vm_stats_fold(int cpu);
 void refresh_zone_stat_thresholds(void);
 
+struct ctl_table;
+int vmstat_refresh(struct ctl_table *, int write,
+		   void __user *buffer, size_t *lenp, loff_t *ppos);
+
 void drain_zonestat(struct zone *zone, struct per_cpu_pageset *);
 
 int calculate_pressure_threshold(struct zone *zone);

commit 0eb77e9880321915322d42913c3b53241739c8aa
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 14 15:21:40 2016 -0800

    vmstat: make vmstat_updater deferrable again and shut down on idle
    
    Currently the vmstat updater is not deferrable as a result of commit
    ba4877b9ca51 ("vmstat: do not use deferrable delayed work for
    vmstat_update").  This in turn can cause multiple interruptions of the
    applications because the vmstat updater may run at
    
    Make vmstate_update deferrable again and provide a function that folds
    the differentials when the processor is going to idle mode thus
    addressing the issue of the above commit in a clean way.
    
    Note that the shepherd thread will continue scanning the differentials
    from another processor and will reenable the vmstat workers if it
    detects any changes.
    
    Fixes: ba4877b9ca51 ("vmstat: do not use deferrable delayed work for vmstat_update")
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 3e5d9075960f..73fae8c4a5fb 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -189,6 +189,7 @@ extern void __inc_zone_state(struct zone *, enum zone_stat_item);
 extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
+void quiet_vmstat(void);
 void cpu_vm_stats_fold(int cpu);
 void refresh_zone_stat_thresholds(void);
 
@@ -249,6 +250,7 @@ static inline void __dec_zone_page_state(struct page *page,
 
 static inline void refresh_zone_stat_thresholds(void) { }
 static inline void cpu_vm_stats_fold(int cpu) { }
+static inline void quiet_vmstat(void) { }
 
 static inline void drain_zonestat(struct zone *zone,
 			struct per_cpu_pageset *pset) { }

commit 6cdb18ad98a49f7e9b95d538a0614cde827404b8
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Dec 29 14:54:32 2015 -0800

    mm/vmstat: fix overflow in mod_zone_page_state()
    
    mod_zone_page_state() takes a "delta" integer argument.  delta contains
    the number of pages that should be added or subtracted from a struct
    zone's vm_stat field.
    
    If a zone is larger than 8TB this will cause overflows.  E.g.  for a
    zone with a size slightly larger than 8TB the line
    
        mod_zone_page_state(zone, NR_ALLOC_BATCH, zone->managed_pages);
    
    in mm/page_alloc.c:free_area_init_core() will result in a negative
    result for the NR_ALLOC_BATCH entry within the zone's vm_stat, since 8TB
    contain 0x8xxxxxxx pages which will be sign extended to a negative
    value.
    
    Fix this by changing the delta argument to long type.
    
    This could fix an early boot problem seen on s390, where we have a 9TB
    system with only one node.  ZONE_DMA contains 2GB and ZONE_NORMAL the
    rest.  The system is trying to allocate a GFP_DMA page but ZONE_DMA is
    completely empty, so it tries to reclaim pages in an endless loop.
    
    This was seen on a heavily patched 3.10 kernel.  One possible
    explaination seem to be the overflows caused by mod_zone_page_state().
    Unfortunately I did not have the chance to verify that this patch
    actually fixes the problem, since I don't have access to the system
    right now.  However the overflow problem does exist anyway.
    
    Given the description that a system with slightly less than 8TB does
    work, this seems to be a candidate for the observed problem.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 5dbc8b0ee567..3e5d9075960f 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -176,11 +176,11 @@ extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
 
 #ifdef CONFIG_SMP
-void __mod_zone_page_state(struct zone *, enum zone_stat_item item, int);
+void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);
 void __dec_zone_page_state(struct page *, enum zone_stat_item);
 
-void mod_zone_page_state(struct zone *, enum zone_stat_item, int);
+void mod_zone_page_state(struct zone *, enum zone_stat_item, long);
 void inc_zone_page_state(struct page *, enum zone_stat_item);
 void dec_zone_page_state(struct page *, enum zone_stat_item);
 
@@ -205,7 +205,7 @@ void set_pgdat_percpu_threshold(pg_data_t *pgdat,
  * The functions directly modify the zone and global counters.
  */
 static inline void __mod_zone_page_state(struct zone *zone,
-			enum zone_stat_item item, int delta)
+			enum zone_stat_item item, long delta)
 {
 	zone_page_state_add(delta, zone, item);
 }

commit 5ba97bf9d8754bd984e81c1061fcda682681939e
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Thu Nov 5 18:50:40 2015 -0800

    mm: remove refresh_cpu_vm_stats() definition for !SMP kernel
    
    refresh_cpu_vm_stats(int cpu) is no longer referenced by !SMP kernel
    since Linux 3.12.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 49dfe40b3673..5dbc8b0ee567 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -247,7 +247,6 @@ static inline void __dec_zone_page_state(struct page *page,
 
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
 
-static inline void refresh_cpu_vm_stats(int cpu) { }
 static inline void refresh_zone_stat_thresholds(void) { }
 static inline void cpu_vm_stats_fold(int cpu) { }
 

commit c2d42c16ad83006a706d83e51a7268db04af733a
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Nov 5 18:48:43 2015 -0800

    mm/vmstat.c: uninline node_page_state()
    
    With x86_64 (config http://ozlabs.org/~akpm/config-akpm2.txt) and old gcc
    (4.4.4), drivers/base/node.c:node_read_meminfo() is using 2344 bytes of
    stack.  Uninlining node_page_state() reduces this to 440 bytes.
    
    The stack consumption issue is fixed by newer gcc (4.8.4) however with
    that compiler this patch reduces the node.o text size from 7314 bytes to
    4578.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 82e7db7f7100..49dfe40b3673 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -161,30 +161,8 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 }
 
 #ifdef CONFIG_NUMA
-/*
- * Determine the per node value of a stat item. This function
- * is called frequently in a NUMA machine, so try to be as
- * frugal as possible.
- */
-static inline unsigned long node_page_state(int node,
-				 enum zone_stat_item item)
-{
-	struct zone *zones = NODE_DATA(node)->node_zones;
-
-	return
-#ifdef CONFIG_ZONE_DMA
-		zone_page_state(&zones[ZONE_DMA], item) +
-#endif
-#ifdef CONFIG_ZONE_DMA32
-		zone_page_state(&zones[ZONE_DMA32], item) +
-#endif
-#ifdef CONFIG_HIGHMEM
-		zone_page_state(&zones[ZONE_HIGHMEM], item) +
-#endif
-		zone_page_state(&zones[ZONE_NORMAL], item) +
-		zone_page_state(&zones[ZONE_MOVABLE], item);
-}
 
+extern unsigned long node_page_state(int node, enum zone_stat_item item);
 extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 
 #else

commit 4f115147ff802267d0aa41e361c5aa5bd933d896
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Wed Jun 4 16:06:46 2014 -0700

    mm,vmacache: add debug data
    
    Introduce a CONFIG_DEBUG_VM_VMACACHE option to enable counting the cache
    hit rate -- exported in /proc/vmstat.
    
    Any updates to the caching scheme needs this kind of data, thus it can
    save some work re-implementing the counting all the time.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 45c9cd1daf7a..82e7db7f7100 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -95,6 +95,12 @@ static inline void vm_events_fold_cpu(int cpu)
 #define count_vm_tlb_events(x, y) do { (void)(y); } while (0)
 #endif
 
+#ifdef CONFIG_DEBUG_VM_VMACACHE
+#define count_vm_vmacache_event(x) count_vm_event(x)
+#else
+#define count_vm_vmacache_event(x) do {} while (0)
+#endif
+
 #define __count_zone_vm_events(item, zone, delta) \
 		__count_vm_events(item##_NORMAL - ZONE_NORMAL + \
 		zone_idx(zone), delta)

commit 293b6a4c875c3b49853bff7de99954f49f59aa75
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Apr 7 15:39:43 2014 -0700

    vmstat: use raw_cpu_ops to avoid false positives on preemption checks
    
    vm counters are allowed to be racy.  Use raw_cpu_ops to avoid the
    local_irq_disable overhead and to avoid preemption checks which will be
    added to the __this_cpu operations.
    
    [akpm@linux-foundation.org: Add comment.  Again.]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Reported-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index ea4476157e00..45c9cd1daf7a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -27,9 +27,13 @@ struct vm_event_state {
 
 DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
 
+/*
+ * vm counters are allowed to be racy. Use raw_cpu_ops to avoid the
+ * local_irq_disable overhead.
+ */
 static inline void __count_vm_event(enum vm_event_item item)
 {
-	__this_cpu_inc(vm_event_states.event[item]);
+	raw_cpu_inc(vm_event_states.event[item]);
 }
 
 static inline void count_vm_event(enum vm_event_item item)
@@ -39,7 +43,7 @@ static inline void count_vm_event(enum vm_event_item item)
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
-	__this_cpu_add(vm_event_states.event[item], delta);
+	raw_cpu_add(vm_event_states.event[item], delta);
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)

commit 6a3ed2123a78de22a9e2b2855068a8d89f8e14f4
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Apr 3 14:47:34 2014 -0700

    mm: vmstat: fix UP zone state accounting
    
    Summary:
    
    The VM maintains cached filesystem pages on two types of lists.  One
    list holds the pages recently faulted into the cache, the other list
    holds pages that have been referenced repeatedly on that first list.
    The idea is to prefer reclaiming young pages over those that have shown
    to benefit from caching in the past.  We call the recently used list
    "inactive list" and the frequently used list "active list".
    
    Currently, the VM aims for a 1:1 ratio between the lists, which is the
    "perfect" trade-off between the ability to *protect* frequently used
    pages and the ability to *detect* frequently used pages.  This means
    that working set changes bigger than half of cache memory go undetected
    and thrash indefinitely, whereas working sets bigger than half of cache
    memory are unprotected against used-once streams that don't even need
    caching.
    
    This happens on file servers and media streaming servers, where the
    popular files and file sections change over time.  Even though the
    individual files might be smaller than half of memory, concurrent access
    to many of them may still result in their inter-reference distance being
    greater than half of memory.  It's also been reported as a problem on
    database workloads that switch back and forth between tables that are
    bigger than half of memory.  In these cases the VM never recognizes the
    new working set and will for the remainder of the workload thrash disk
    data which could easily live in memory.
    
    Historically, every reclaim scan of the inactive list also took a
    smaller number of pages from the tail of the active list and moved them
    to the head of the inactive list.  This model gave established working
    sets more gracetime in the face of temporary use-once streams, but
    ultimately was not significantly better than a FIFO policy and still
    thrashed cache based on eviction speed, rather than actual demand for
    cache.
    
    This series solves the problem by maintaining a history of pages evicted
    from the inactive list, enabling the VM to detect frequently used pages
    regardless of inactive list size and facilitate working set transitions.
    
    Tests:
    
    The reported database workload is easily demonstrated on a 8G machine
    with two filesets a 6G.  This fio workload operates on one set first,
    then switches to the other.  The VM should obviously always cache the
    set that the workload is currently using.
    
    This test is based on a problem encountered by Citus Data customers:
      http://citusdata.com/blog/72-linux-memory-manager-and-your-big-data
    
    unpatched:
      db1: READ: io=98304MB, aggrb=885559KB/s, minb=885559KB/s, maxb=885559KB/s, mint= 113672msec, maxt= 113672msec
      db2: READ: io=98304MB, aggrb= 66169KB/s, minb= 66169KB/s, maxb= 66169KB/s, mint=1521302msec, maxt=1521302msec
      sdb: ios=835750/4, merge=2/1, ticks=4659739/60016, in_queue=4719203, util=98.92%
    
      real    27m15.541s
      user    0m19.059s
      sys     0m51.459s
    
    patched:
      db1: READ: io=98304MB, aggrb=877783KB/s, minb=877783KB/s, maxb=877783KB/s, mint=114679msec, maxt=114679msec
      db2: READ: io=98304MB, aggrb=397449KB/s, minb=397449KB/s, maxb=397449KB/s, mint=253273msec, maxt=253273msec
      sdb: ios=170587/4, merge=2/1, ticks=954910/61123, in_queue=1015923, util=90.40%
    
      real    6m8.630s
      user    0m14.714s
      sys     0m31.233s
    
    As can be seen, the unpatched kernel simply never adapts to the
    workingset change and db2 is stuck indefinitely with secondary storage
    speed.  The patched kernel needs 2-3 iterations over db2 before it
    replaces db1 and reaches full memory speed.  Given the unbounded
    negative affect of the existing VM behavior, these patches should be
    considered correctness fixes rather than performance optimizations.
    
    Another test resembles a fileserver or streaming server workload, where
    data in excess of memory size is accessed at different frequencies.
    There is very hot data accessed at a high frequency.  Machines should be
    fitted so that the hot set of such a workload can be fully cached or all
    bets are off.  Then there is a very big (compared to available memory)
    set of data that is used-once or at a very low frequency; this is what
    drives the inactive list and does not really benefit from caching.
    Lastly, there is a big set of warm data in between that is accessed at
    medium frequencies and benefits from caching the pages between the first
    and last streamer of each burst.
    
    unpatched:
       hot: READ: io=128000MB, aggrb=160693KB/s, minb=160693KB/s, maxb=160693KB/s, mint=815665msec, maxt=815665msec
      warm: READ: io= 81920MB, aggrb=109853KB/s, minb= 27463KB/s, maxb= 29244KB/s, mint=717110msec, maxt=763617msec
      cold: READ: io= 30720MB, aggrb= 35245KB/s, minb= 35245KB/s, maxb= 35245KB/s, mint=892530msec, maxt=892530msec
       sdb: ios=797960/4, merge=11763/1, ticks=4307910/796, in_queue=4308380, util=100.00%
    
    patched:
       hot: READ: io=128000MB, aggrb=160678KB/s, minb=160678KB/s, maxb=160678KB/s, mint=815740msec, maxt=815740msec
      warm: READ: io= 81920MB, aggrb=147747KB/s, minb= 36936KB/s, maxb= 40960KB/s, mint=512000msec, maxt=567767msec
      cold: READ: io= 30720MB, aggrb= 40960KB/s, minb= 40960KB/s, maxb= 40960KB/s, mint=768000msec, maxt=768000msec
       sdb: ios=596514/4, merge=9341/1, ticks=2395362/997, in_queue=2396484, util=79.18%
    
    In both kernels, the hot set is propagated to the active list and then
    served from cache.
    
    In both kernels, the beginning of the warm set is propagated to the
    active list as well, but in the unpatched case the active list
    eventually takes up half of memory and no new pages from the warm set
    get activated, despite repeated access, and despite most of the active
    list soon being stale.  The patched kernel on the other hand detects the
    thrashing and manages to keep this cache window rolling through the data
    set.  This frees up enough IO bandwidth that the cold set is served at
    full speed as well and disk utilization even drops by 20%.
    
    For reference, this same test was performed with the traditional
    demotion mechanism, where deactivation is coupled to inactive list
    reclaim.  However, this had the same outcome as the unpatched kernel:
    while the warm set does indeed get activated continuously, it is forced
    out of the active list by inactive list pressure, which is dictated
    primarily by the unrelated cold set.  The warm set is evicted before
    subsequent streamers can benefit from it, even though there would be
    enough space available to cache the pages of interest.
    
    Costs:
    
    Page reclaim used to shrink the radix trees but now the tree nodes are
    reused for shadow entries, where the cost depends heavily on the page
    cache access patterns.  However, with workloads that maintain spatial or
    temporal locality, the shadow entries are either refaulted quickly or
    reclaimed along with the inode object itself.  Workloads that will
    experience a memory cost increase are those that don't really benefit
    from caching in the first place.
    
    A more predictable alternative would be a fixed-cost separate pool of
    shadow entries, but this would incur relatively higher memory cost for
    well-behaved workloads at the benefit of cornercases.  It would also
    make the shadow entry lookup more costly compared to storing them
    directly in the cache structure.
    
    Future:
    
    To simplify the merging process, this patch set is implementing thrash
    detection on a global per-zone level only for now, but the design is
    such that it can be extended to memory cgroups as well.  All we need to
    do is store the unique cgroup ID along the node and zone identifier
    inside the eviction cookie to identify the lruvec.
    
    Right now we have a fixed ratio (50:50) between inactive and active list
    but we already have complaints about working sets exceeding half of
    memory being pushed out of the cache by simple streaming in the
    background.  Ultimately, we want to adjust this ratio and allow for a
    much smaller inactive list.  These patches are an essential step in this
    direction because they decouple the VMs ability to detect working set
    changes from the inactive list size.  This would allow us to base the
    inactive list size on the combined readahead window size for example and
    potentially protect a much bigger working set.
    
    It's also a big step towards activating pages with a reuse distance
    larger than memory, as long as they are the most frequently used pages
    in the workload.  This will require knowing more about the access
    frequency of active pages than what we measure right now, so it's also
    deferred in this series.
    
    Another possibility of having thrashing information would be to revisit
    the idea of local reclaim in the form of zero-config memory control
    groups.  Instead of having allocating tasks go straight to global
    reclaim, they could try to reclaim the pages in the memcg they are part
    of first as long as the group is not thrashing.  This would allow a user
    to drop e.g.  a back-up job in an otherwise unconfigured memcg and it
    would only inflate (and possibly do global reclaim) until it has enough
    memory to do proper readahead.  But once it reaches that point and stops
    thrashing it would just recycle its own used-once pages without kicking
    out the cache of any other tasks in the system more than necessary.
    
    This patch (of 10):
    
    Fengguang Wu's build testing spotted problems with inc_zone_state() and
    dec_zone_state() on UP configurations in out-of-tree patches.
    
    inc_zone_state() is declared but not defined, dec_zone_state() is
    missing entirely.
    
    Just like with *_zone_page_state(), they can be defined like their
    preemption-unsafe counterparts on UP.
    
    [akpm@linux-foundation.org: make it build]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Metin Doslu <metin@citusdata.com>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Ozgun Erdogan <ozgun@citusdata.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Roman Gushchin <klamm@yandex-team.ru>
    Cc: Ryan Mallon <rmallon@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 67ce70c8279b..ea4476157e00 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -187,8 +187,6 @@ extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
 
-extern void inc_zone_state(struct zone *, enum zone_stat_item);
-
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, int);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);
@@ -230,18 +228,18 @@ static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 	atomic_long_inc(&vm_stat[item]);
 }
 
-static inline void __inc_zone_page_state(struct page *page,
-			enum zone_stat_item item)
-{
-	__inc_zone_state(page_zone(page), item);
-}
-
 static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_dec(&zone->vm_stat[item]);
 	atomic_long_dec(&vm_stat[item]);
 }
 
+static inline void __inc_zone_page_state(struct page *page,
+			enum zone_stat_item item)
+{
+	__inc_zone_state(page_zone(page), item);
+}
+
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
@@ -256,6 +254,9 @@ static inline void __dec_zone_page_state(struct page *page,
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
+#define inc_zone_state __inc_zone_state
+#define dec_zone_state __dec_zone_state
+
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
 
 static inline void refresh_cpu_vm_stats(int cpu) { }

commit a3b072cd180c12e8fe0ece9487b9065808327640
Merge: 75a1ba5b2c52 081cd62a010f
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Fri Feb 7 11:27:30 2014 -0800

    Merge tag 'efi-urgent' into x86/urgent
    
     * Avoid WARN_ON() when mapping BGRT on Baytrail (EFI 32-bit).
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit a1c3bfb2f67ef766de03f1f56bdfff9c8595ab14
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jan 29 14:05:41 2014 -0800

    mm/page-writeback.c: do not count anon pages as dirtyable memory
    
    The VM is currently heavily tuned to avoid swapping.  Whether that is
    good or bad is a separate discussion, but as long as the VM won't swap
    to make room for dirty cache, we can not consider anonymous pages when
    calculating the amount of dirtyable memory, the baseline to which
    dirty_background_ratio and dirty_ratio are applied.
    
    A simple workload that occupies a significant size (40+%, depending on
    memory layout, storage speeds etc.) of memory with anon/tmpfs pages and
    uses the remainder for a streaming writer demonstrates this problem.  In
    that case, the actual cache pages are a small fraction of what is
    considered dirtyable overall, which results in an relatively large
    portion of the cache pages to be dirtied.  As kswapd starts rotating
    these, random tasks enter direct reclaim and stall on IO.
    
    Only consider free pages and file pages dirtyable.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Tejun Heo <tj@kernel.org>
    Tested-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e4b948080d20..a67b38415768 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -142,8 +142,6 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 	return x;
 }
 
-extern unsigned long global_reclaimable_pages(void);
-
 #ifdef CONFIG_NUMA
 /*
  * Determine the per node value of a stat item. This function

commit ec65993443736a5091b68e80ff1734548944a4b8
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jan 21 14:33:16 2014 -0800

    mm, x86: Account for TLB flushes only when debugging
    
    Bisection between 3.11 and 3.12 fingered commit 9824cf97 ("mm:
    vmstats: tlb flush counters") to cause overhead problems.
    
    The counters are undeniably useful but how often do we really
    need to debug TLB flush related issues?  It does not justify
    taking the penalty everywhere so make it a debugging option.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Tested-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-XzxjntugxuwpxXhcrxqqh53b@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e4b948080d20..80ebba9c2e87 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -83,6 +83,14 @@ static inline void vm_events_fold_cpu(int cpu)
 #define count_vm_numa_events(x, y) do { (void)(y); } while (0)
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_DEBUG_TLBFLUSH
+#define count_vm_tlb_event(x)	   count_vm_event(x)
+#define count_vm_tlb_events(x, y)  count_vm_events(x, y)
+#else
+#define count_vm_tlb_event(x)     do {} while (0)
+#define count_vm_tlb_events(x, y) do { (void)(y); } while (0)
+#endif
+
 #define __count_zone_vm_events(item, zone, delta) \
 		__count_vm_events(item##_NORMAL - ZONE_NORMAL + \
 		zone_idx(zone), delta)

commit 6e543d5780e36ff5ee56c44d7e2e30db3457a7ed
Author: Lisa Du <cldu@marvell.com>
Date:   Wed Sep 11 14:22:36 2013 -0700

    mm: vmscan: fix do_try_to_free_pages() livelock
    
    This patch is based on KOSAKI's work and I add a little more description,
    please refer https://lkml.org/lkml/2012/6/14/74.
    
    Currently, I found system can enter a state that there are lots of free
    pages in a zone but only order-0 and order-1 pages which means the zone is
    heavily fragmented, then high order allocation could make direct reclaim
    path's long stall(ex, 60 seconds) especially in no swap and no compaciton
    enviroment.  This problem happened on v3.4, but it seems issue still lives
    in current tree, the reason is do_try_to_free_pages enter live lock:
    
    kswapd will go to sleep if the zones have been fully scanned and are still
    not balanced.  As kswapd thinks there's little point trying all over again
    to avoid infinite loop.  Instead it changes order from high-order to
    0-order because kswapd think order-0 is the most important.  Look at
    73ce02e9 in detail.  If watermarks are ok, kswapd will go back to sleep
    and may leave zone->all_unreclaimable =3D 0.  It assume high-order users
    can still perform direct reclaim if they wish.
    
    Direct reclaim continue to reclaim for a high order which is not a
    COSTLY_ORDER without oom-killer until kswapd turn on
    zone->all_unreclaimble= .  This is because to avoid too early oom-kill.
    So it means direct_reclaim depends on kswapd to break this loop.
    
    In worst case, direct-reclaim may continue to page reclaim forever when
    kswapd sleeps forever until someone like watchdog detect and finally kill
    the process.  As described in:
    http://thread.gmane.org/gmane.linux.kernel.mm/103737
    
    We can't turn on zone->all_unreclaimable from direct reclaim path because
    direct reclaim path don't take any lock and this way is racy.  Thus this
    patch removes zone->all_unreclaimable field completely and recalculates
    zone reclaimable state every time.
    
    Note: we can't take the idea that direct-reclaim see zone->pages_scanned
    directly and kswapd continue to use zone->all_unreclaimable.  Because, it
    is racy.  commit 929bea7c71 (vmscan: all_unreclaimable() use
    zone->all_unreclaimable as a name) describes the detail.
    
    [akpm@linux-foundation.org: uninline zone_reclaimable_pages() and zone_reclaimable()]
    Cc: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: Nick Piggin <npiggin@gmail.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Bob Liu <lliubbo@gmail.com>
    Cc: Neil Zhang <zhangwm@marvell.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Lisa Du <cldu@marvell.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 502767f4e4d4..e4b948080d20 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -143,7 +143,6 @@ static inline unsigned long zone_page_state_snapshot(struct zone *zone,
 }
 
 extern unsigned long global_reclaimable_pages(void);
-extern unsigned long zone_reclaimable_pages(struct zone *zone);
 
 #ifdef CONFIG_NUMA
 /*

commit 2bb921e526656556e68f99f5f15a4a1bf2691844
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Sep 11 14:21:30 2013 -0700

    vmstat: create separate function to fold per cpu diffs into local counters
    
    The main idea behind this patchset is to reduce the vmstat update overhead
    by avoiding interrupt enable/disable and the use of per cpu atomics.
    
    This patch (of 3):
    
    It is better to have a separate folding function because
    refresh_cpu_vm_stats() also does other things like expire pages in the
    page allocator caches.
    
    If we have a separate function then refresh_cpu_vm_stats() is only called
    from the local cpu which allows additional optimizations.
    
    The folding function is only called when a cpu is being downed and
    therefore no other processor will be accessing the counters.  Also
    simplifies synchronization.
    
    [akpm@linux-foundation.org: fix UP build]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    CC: Tejun Heo <tj@kernel.org>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index c586679b6fef..502767f4e4d4 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -198,7 +198,7 @@ extern void __inc_zone_state(struct zone *, enum zone_stat_item);
 extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
-void refresh_cpu_vm_stats(int);
+void cpu_vm_stats_fold(int cpu);
 void refresh_zone_stat_thresholds(void);
 
 void drain_zonestat(struct zone *zone, struct per_cpu_pageset *);
@@ -255,6 +255,7 @@ static inline void __dec_zone_page_state(struct page *page,
 
 static inline void refresh_cpu_vm_stats(int cpu) { }
 static inline void refresh_zone_stat_thresholds(void) { }
+static inline void cpu_vm_stats_fold(int cpu) { }
 
 static inline void drain_zonestat(struct zone *zone,
 			struct per_cpu_pageset *pset) { }

commit f1cb08798e2497238b28f377bd131426f0b9835d
Author: Yijing Wang <wangyijing@huawei.com>
Date:   Mon Apr 29 15:08:14 2013 -0700

    mm: remove CONFIG_HOTPLUG ifdefs
    
    CONFIG_HOTPLUG is going away as an option, cleanup CONFIG_HOTPLUG
    ifdefs in mm files.
    
    Signed-off-by: Yijing Wang <wangyijing@huawei.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 5fd71a7d0dfd..c586679b6fef 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -48,13 +48,8 @@ static inline void count_vm_events(enum vm_event_item item, long delta)
 }
 
 extern void all_vm_events(unsigned long *);
-#ifdef CONFIG_HOTPLUG
+
 extern void vm_events_fold_cpu(int cpu);
-#else
-static inline void vm_events_fold_cpu(int cpu)
-{
-}
-#endif
 
 #else
 

commit 3c0ff4689630b280704666833e9539d84cddc373
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Feb 22 16:34:29 2013 -0800

    mm: numa: handle side-effects in count_vm_numa_events() for !CONFIG_NUMA_BALANCING
    
    The current definitions for count_vm_numa_events() is wrong for
    !CONFIG_NUMA_BALANCING as the following would miss the side-effect.
    
            count_vm_numa_events(NUMA_FOO, bar++);
    
    There are no such users of count_vm_numa_events() but this patch fixes
    it as it is a potential pitfall.  Ideally both would be converted to
    static inline but NUMA_PTE_UPDATES is not defined if
    !CONFIG_NUMA_BALANCING and creating dummy constants just to have a
    static inline would be similarly clumsy.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Simon Jeons <simon.jeons@gmail.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index a13291f7da88..5fd71a7d0dfd 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -85,7 +85,7 @@ static inline void vm_events_fold_cpu(int cpu)
 #define count_vm_numa_events(x, y) count_vm_events(x, y)
 #else
 #define count_vm_numa_event(x) do {} while (0)
-#define count_vm_numa_events(x, y) do {} while (0)
+#define count_vm_numa_events(x, y) do { (void)(y); } while (0)
 #endif /* CONFIG_NUMA_BALANCING */
 
 #define __count_zone_vm_events(item, zone, delta) \

commit 03c5a6e16322c997bf8f264851bfa3f532ad515f
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Nov 2 14:52:48 2012 +0000

    mm: numa: Add pte updates, hinting and migration stats
    
    It is tricky to quantify the basic cost of automatic NUMA placement in a
    meaningful manner. This patch adds some vmstats that can be used as part
    of a basic costing model.
    
    u    = basic unit = sizeof(void *)
    Ca   = cost of struct page access = sizeof(struct page) / u
    Cpte = Cost PTE access = Ca
    Cupdate = Cost PTE update = (2 * Cpte) + (2 * Wlock)
            where Cpte is incurred twice for a read and a write and Wlock
            is a constant representing the cost of taking or releasing a
            lock
    Cnumahint = Cost of a minor page fault = some high constant e.g. 1000
    Cpagerw = Cost to read or write a full page = Ca + PAGE_SIZE/u
    Ci = Cost of page isolation = Ca + Wi
            where Wi is a constant that should reflect the approximate cost
            of the locking operation
    Cpagecopy = Cpagerw + (Cpagerw * Wnuma) + Ci + (Ci * Wnuma)
            where Wnuma is the approximate NUMA factor. 1 is local. 1.2
            would imply that remote accesses are 20% more expensive
    
    Balancing cost = Cpte * numa_pte_updates +
                    Cnumahint * numa_hint_faults +
                    Ci * numa_pages_migrated +
                    Cpagecopy * numa_pages_migrated
    
    Note that numa_pages_migrated is used as a measure of how many pages
    were isolated even though it would miss pages that failed to migrate. A
    vmstat counter could have been added for it but the isolation cost is
    pretty marginal in comparison to the overall cost so it seemed overkill.
    
    The ideal way to measure automatic placement benefit would be to count
    the number of remote accesses versus local accesses and do something like
    
            benefit = (remote_accesses_before - remove_access_after) * Wnuma
    
    but the information is not readily available. As a workload converges, the
    expection would be that the number of remote numa hints would reduce to 0.
    
            convergence = numa_hint_faults_local / numa_hint_faults
                    where this is measured for the last N number of
                    numa hints recorded. When the workload is fully
                    converged the value is 1.
    
    This can measure if the placement policy is converging and how fast it is
    doing it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 92a86b2cce33..a13291f7da88 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -80,6 +80,14 @@ static inline void vm_events_fold_cpu(int cpu)
 
 #endif /* CONFIG_VM_EVENT_COUNTERS */
 
+#ifdef CONFIG_NUMA_BALANCING
+#define count_vm_numa_event(x)     count_vm_event(x)
+#define count_vm_numa_events(x, y) count_vm_events(x, y)
+#else
+#define count_vm_numa_event(x) do {} while (0)
+#define count_vm_numa_events(x, y) do {} while (0)
+#endif /* CONFIG_NUMA_BALANCING */
+
 #define __count_zone_vm_events(item, zone, delta) \
 		__count_vm_events(item##_NORMAL - ZONE_NORMAL + \
 		zone_idx(zone), delta)

commit 5a883813845a2bb5ed2bd8c9240736c0740b156f
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:33:39 2012 -0700

    memory-hotplug: fix zone stat mismatch
    
    During memory-hotplug, I found NR_ISOLATED_[ANON|FILE] are increasing,
    causing the kernel to hang.  When the system doesn't have enough free
    pages, it enters reclaim but never reclaim any pages due to
    too_many_isolated()==true and loops forever.
    
    The cause is that when we do memory-hotadd after memory-remove,
    __zone_pcp_update() clears a zone's ZONE_STAT_ITEMS in setup_pageset()
    although the vm_stat_diff of all CPUs still have values.
    
    In addtion, when we offline all pages of the zone, we reset them in
    zone_pcp_reset without draining so we loss some zone stat item.
    
    Reviewed-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index a5bb15018b5c..92a86b2cce33 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -198,6 +198,8 @@ extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 void refresh_cpu_vm_stats(int);
 void refresh_zone_stat_thresholds(void);
 
+void drain_zonestat(struct zone *zone, struct per_cpu_pageset *);
+
 int calculate_pressure_threshold(struct zone *zone);
 int calculate_normal_threshold(struct zone *zone);
 void set_pgdat_percpu_threshold(pg_data_t *pgdat,
@@ -251,6 +253,8 @@ static inline void __dec_zone_page_state(struct page *page,
 static inline void refresh_cpu_vm_stats(int cpu) { }
 static inline void refresh_zone_stat_thresholds(void) { }
 
+static inline void drain_zonestat(struct zone *zone,
+			struct per_cpu_pageset *pset) { }
 #endif		/* CONFIG_SMP */
 
 static inline void __mod_zone_freepage_state(struct zone *zone, int nr_pages,

commit d1ce749a0db12202b711d1aba1d29e823034648d
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Mon Oct 8 16:32:02 2012 -0700

    cma: count free CMA pages
    
    Add NR_FREE_CMA_PAGES counter to be later used for checking watermark in
    __zone_watermark_ok().  For simplicity and to avoid #ifdef hell make this
    counter always available (not only when CONFIG_CMA=y).
    
    [akpm@linux-foundation.org: use conventional migratetype naming]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index ad2cfd53dadc..a5bb15018b5c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -253,6 +253,14 @@ static inline void refresh_zone_stat_thresholds(void) { }
 
 #endif		/* CONFIG_SMP */
 
+static inline void __mod_zone_freepage_state(struct zone *zone, int nr_pages,
+					     int migratetype)
+{
+	__mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
+	if (is_migrate_cma(migratetype))
+		__mod_zone_page_state(zone, NR_FREE_CMA_PAGES, nr_pages);
+}
+
 extern const char * const vmstat_text[];
 
 #endif /* _LINUX_VMSTAT_H */

commit 6527af5d1bea219d64095a5e30c1b1e0868aae16
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:46:16 2012 -0700

    mm: remove redundant initialization
    
    pg_data_t is zeroed before reaching free_area_init_core(), so remove the
    now unnecessary initializations.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 65efb92da996..ad2cfd53dadc 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -179,11 +179,6 @@ extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
 
-static inline void zap_zone_vm_stats(struct zone *zone)
-{
-	memset(zone->vm_stat, 0, sizeof(zone->vm_stat));
-}
-
 extern void inc_zone_state(struct zone *, enum zone_stat_item);
 
 #ifdef CONFIG_SMP

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index bcd942fa611c..65efb92da996 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -6,7 +6,7 @@
 #include <linux/mm.h>
 #include <linux/mmzone.h>
 #include <linux/vm_event_item.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 
 extern int sysctl_stat_interval;
 

commit f042e707ee671e4beb5389abeb9a1819a2cf5532
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu May 26 16:25:24 2011 -0700

    mm: move enum vm_event_item into a standalone header file
    
    enums are problematic because they cannot be forward-declared:
    
      akpm2:/home/akpm> cat t.c
    
      enum foo;
    
      static inline void bar(enum foo f)
      {
      }
      akpm2:/home/akpm> gcc -c t.c
      t.c:4: error: parameter 1 ('f') has incomplete type
    
    So move the enum's definition into a standalone header file which can be used
    wherever its definition is needed.
    
    Cc: Ying Han <yinghan@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 51359837511a..bcd942fa611c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -5,69 +5,9 @@
 #include <linux/percpu.h>
 #include <linux/mm.h>
 #include <linux/mmzone.h>
+#include <linux/vm_event_item.h>
 #include <asm/atomic.h>
 
-#ifdef CONFIG_ZONE_DMA
-#define DMA_ZONE(xx) xx##_DMA,
-#else
-#define DMA_ZONE(xx)
-#endif
-
-#ifdef CONFIG_ZONE_DMA32
-#define DMA32_ZONE(xx) xx##_DMA32,
-#else
-#define DMA32_ZONE(xx)
-#endif
-
-#ifdef CONFIG_HIGHMEM
-#define HIGHMEM_ZONE(xx) , xx##_HIGH
-#else
-#define HIGHMEM_ZONE(xx)
-#endif
-
-
-#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
-
-enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
-		FOR_ALL_ZONES(PGALLOC),
-		PGFREE, PGACTIVATE, PGDEACTIVATE,
-		PGFAULT, PGMAJFAULT,
-		FOR_ALL_ZONES(PGREFILL),
-		FOR_ALL_ZONES(PGSTEAL),
-		FOR_ALL_ZONES(PGSCAN_KSWAPD),
-		FOR_ALL_ZONES(PGSCAN_DIRECT),
-#ifdef CONFIG_NUMA
-		PGSCAN_ZONE_RECLAIM_FAILED,
-#endif
-		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
-		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
-		KSWAPD_SKIP_CONGESTION_WAIT,
-		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
-#ifdef CONFIG_COMPACTION
-		COMPACTBLOCKS, COMPACTPAGES, COMPACTPAGEFAILED,
-		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
-#endif
-#ifdef CONFIG_HUGETLB_PAGE
-		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
-#endif
-		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
-		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
-		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
-		UNEVICTABLE_PGMLOCKED,
-		UNEVICTABLE_PGMUNLOCKED,
-		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
-		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
-		UNEVICTABLE_MLOCKFREED,
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
-		THP_FAULT_ALLOC,
-		THP_FAULT_FALLBACK,
-		THP_COLLAPSE_ALLOC,
-		THP_COLLAPSE_ALLOC_FAILED,
-		THP_SPLIT,
-#endif
-		NR_VM_EVENT_ITEMS
-};
-
 extern int sysctl_stat_interval;
 
 #ifdef CONFIG_VM_EVENT_COUNTERS

commit a6cccdc36c966e51fd969560d870cfd37afbfa9c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue May 24 17:11:33 2011 -0700

    mm, mem-hotplug: update pcp->stat_threshold when memory hotplug occur
    
    Currently, cpu hotplug updates pcp->stat_threshold, but memory hotplug
    doesn't.  There is no reason for this.
    
    [akpm@linux-foundation.org: fix CONFIG_SMP=n build]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e73d1030f2f7..51359837511a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -261,6 +261,7 @@ extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
 void refresh_cpu_vm_stats(int);
+void refresh_zone_stat_thresholds(void);
 
 int calculate_pressure_threshold(struct zone *zone);
 int calculate_normal_threshold(struct zone *zone);
@@ -313,6 +314,8 @@ static inline void __dec_zone_page_state(struct page *page,
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
 
 static inline void refresh_cpu_vm_stats(int cpu) { }
+static inline void refresh_zone_stat_thresholds(void) { }
+
 #endif		/* CONFIG_SMP */
 
 extern const char * const vmstat_text[];

commit fa25c503dfa203b921199ea42c0046c89f2ed49f
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue May 24 17:11:28 2011 -0700

    mm: per-node vmstat: show proper vmstats
    
    commit 2ac390370a ("writeback: add
    /sys/devices/system/node/<node>/vmstat") added vmstat entry.  But
    strangely it only show nr_written and nr_dirtied.
    
            # cat /sys/devices/system/node/node20/vmstat
            nr_written 0
            nr_dirtied 0
    
    Of course, It's not adequate.  With this patch, the vmstat show all vm
    stastics as /proc/vmstat.
    
            # cat /sys/devices/system/node/node0/vmstat
            nr_free_pages 899224
            nr_inactive_anon 201
            nr_active_anon 17380
            nr_inactive_file 31572
            nr_active_file 28277
            nr_unevictable 0
            nr_mlock 0
            nr_anon_pages 17321
            nr_mapped 8640
            nr_file_pages 60107
            nr_dirty 33
            nr_writeback 0
            nr_slab_reclaimable 6850
            nr_slab_unreclaimable 7604
            nr_page_table_pages 3105
            nr_kernel_stack 175
            nr_unstable 0
            nr_bounce 0
            nr_vmscan_write 0
            nr_writeback_temp 0
            nr_isolated_anon 0
            nr_isolated_file 0
            nr_shmem 260
            nr_dirtied 1050
            nr_written 938
            numa_hit 962872
            numa_miss 0
            numa_foreign 0
            numa_interleave 8617
            numa_local 962872
            numa_other 0
            nr_anon_transparent_hugepages 0
    
    [akpm@linux-foundation.org: no externs in .c files]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michael Rubin <mrubin@google.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 2b3831b58aa4..e73d1030f2f7 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -313,6 +313,8 @@ static inline void __dec_zone_page_state(struct page *page,
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
 
 static inline void refresh_cpu_vm_stats(int cpu) { }
-#endif
+#endif		/* CONFIG_SMP */
+
+extern const char * const vmstat_text[];
 
 #endif /* _LINUX_VMSTAT_H */

commit 81ab4201fb7d91d6b0cd9ad5b4b16776e4bed145
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Apr 14 15:22:06 2011 -0700

    mm: add VM counters for transparent hugepages
    
    I found it difficult to make sense of transparent huge pages without
    having any counters for its actions.  Add some counters to vmstat for
    allocation of transparent hugepages and fallback to smaller pages.
    
    Optional patch, but useful for development and understanding the system.
    
    Contains improvements from Andrea Arcangeli and Johannes Weiner
    
    [akpm@linux-foundation.org: coding-style fixes]
    [hannes@cmpxchg.org: fix vmstat_text[] entries]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 461c0119664f..2b3831b58aa4 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -58,6 +58,13 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
 		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
 		UNEVICTABLE_MLOCKFREED,
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+		THP_FAULT_ALLOC,
+		THP_FAULT_FALLBACK,
+		THP_COLLAPSE_ALLOC,
+		THP_COLLAPSE_ALLOC_FAILED,
+		THP_SPLIT,
+#endif
 		NR_VM_EVENT_ITEMS
 };
 

commit 78afd5612deb8268bafc8b6507d72341d5ed9aac
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Mar 22 16:33:12 2011 -0700

    mm: add __GFP_OTHER_NODE flag
    
    Add a new __GFP_OTHER_NODE flag to tell the low level numa statistics in
    zone_statistics() that an allocation is on behalf of another thread.  This
    way the local and remote counters can be still correct, even when
    background daemons like khugepaged are changing memory mappings.
    
    This only affects the accounting, but I think it's worth doing that right
    to avoid confusing users.
    
    I first tried to just pass down the right node, but this required a lot of
    changes to pass down this parameter and at least one addition of a 10th
    argument to a 9 argument function.  Using the flag is a lot less
    intrusive.
    
    Open: should be also used for migration?
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 833e676d6d92..461c0119664f 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -220,12 +220,12 @@ static inline unsigned long node_page_state(int node,
 		zone_page_state(&zones[ZONE_MOVABLE], item);
 }
 
-extern void zone_statistics(struct zone *, struct zone *);
+extern void zone_statistics(struct zone *, struct zone *, gfp_t gfp);
 
 #else
 
 #define node_page_state(node, item) global_page_state(item)
-#define zone_statistics(_zl,_z) do { } while (0)
+#define zone_statistics(_zl, _z, gfp) do { } while (0)
 
 #endif /* CONFIG_NUMA */
 

commit b44129b30652c8771db2265939bb8b463724043d
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:45:43 2011 -0800

    mm: vmstat: use a single setter function and callback for adjusting percpu thresholds
    
    reduce_pgdat_percpu_threshold() and restore_pgdat_percpu_threshold() exist
    to adjust the per-cpu vmstat thresholds while kswapd is awake to avoid
    errors due to counter drift.  The functions duplicate some code so this
    patch replaces them with a single set_pgdat_percpu_threshold() that takes
    a callback function to calculate the desired threshold as a parameter.
    
    [akpm@linux-foundation.org: readability tweak]
    [kosaki.motohiro@jp.fujitsu.com: set_pgdat_percpu_threshold(): don't use for_each_online_cpu]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e4cc21cf5870..833e676d6d92 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -254,8 +254,11 @@ extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
 void refresh_cpu_vm_stats(int);
-void reduce_pgdat_percpu_threshold(pg_data_t *pgdat);
-void restore_pgdat_percpu_threshold(pg_data_t *pgdat);
+
+int calculate_pressure_threshold(struct zone *zone);
+int calculate_normal_threshold(struct zone *zone);
+void set_pgdat_percpu_threshold(pg_data_t *pgdat,
+				int (*calculate_pressure)(struct zone *));
 #else /* CONFIG_SMP */
 
 /*
@@ -300,8 +303,7 @@ static inline void __dec_zone_page_state(struct page *page,
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
-static inline void reduce_pgdat_percpu_threshold(pg_data_t *pgdat) { }
-static inline void restore_pgdat_percpu_threshold(pg_data_t *pgdat) { }
+#define set_pgdat_percpu_threshold(pgdat, callback) { }
 
 static inline void refresh_cpu_vm_stats(int cpu) { }
 #endif

commit 88f5acf88ae6a9778f6d25d0d5d7ec2d57764a97
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Thu Jan 13 15:45:41 2011 -0800

    mm: page allocator: adjust the per-cpu counter threshold when memory is low
    
    Commit aa45484 ("calculate a better estimate of NR_FREE_PAGES when memory
    is low") noted that watermarks were based on the vmstat NR_FREE_PAGES.  To
    avoid synchronization overhead, these counters are maintained on a per-cpu
    basis and drained both periodically and when a threshold is above a
    threshold.  On large CPU systems, the difference between the estimate and
    real value of NR_FREE_PAGES can be very high.  The system can get into a
    case where pages are allocated far below the min watermark potentially
    causing livelock issues.  The commit solved the problem by taking a better
    reading of NR_FREE_PAGES when memory was low.
    
    Unfortately, as reported by Shaohua Li this accurate reading can consume a
    large amount of CPU time on systems with many sockets due to cache line
    bouncing.  This patch takes a different approach.  For large machines
    where counter drift might be unsafe and while kswapd is awake, the per-cpu
    thresholds for the target pgdat are reduced to limit the level of drift to
    what should be a safe level.  This incurs a performance penalty in heavy
    memory pressure by a factor that depends on the workload and the machine
    but the machine should function correctly without accidentally exhausting
    all memory on a node.  There is an additional cost when kswapd wakes and
    sleeps but the event is not expected to be frequent - in Shaohua's test
    case, there was one recorded sleep and wake event at least.
    
    To ensure that kswapd wakes up, a safe version of zone_watermark_ok() is
    introduced that takes a more accurate reading of NR_FREE_PAGES when called
    from wakeup_kswapd, when deciding whether it is really safe to go back to
    sleep in sleeping_prematurely() and when deciding if a zone is really
    balanced or not in balance_pgdat().  We are still using an expensive
    function but limiting how often it is called.
    
    When the test case is reproduced, the time spent in the watermark
    functions is reduced.  The following report is on the percentage of time
    spent cumulatively spent in the functions zone_nr_free_pages(),
    zone_watermark_ok(), __zone_watermark_ok(), zone_watermark_ok_safe(),
    zone_page_state_snapshot(), zone_page_state().
    
    vanilla                      11.6615%
    disable-threshold            0.2584%
    
    David said:
    
    : We had to pull aa454840 "mm: page allocator: calculate a better estimate
    : of NR_FREE_PAGES when memory is low and kswapd is awake" from 2.6.36
    : internally because tests showed that it would cause the machine to stall
    : as the result of heavy kswapd activity.  I merged it back with this fix as
    : it is pending in the -mm tree and it solves the issue we were seeing, so I
    : definitely think this should be pushed to -stable (and I would seriously
    : consider it for 2.6.37 inclusion even at this late date).
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reported-by: Shaohua Li <shaohua.li@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Tested-by: Nicolas Bareil <nico@chdir.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: <stable@kernel.org>         [2.6.37.1, 2.6.36.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index eaaea37b3b75..e4cc21cf5870 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -254,6 +254,8 @@ extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
 void refresh_cpu_vm_stats(int);
+void reduce_pgdat_percpu_threshold(pg_data_t *pgdat);
+void restore_pgdat_percpu_threshold(pg_data_t *pgdat);
 #else /* CONFIG_SMP */
 
 /*
@@ -298,6 +300,9 @@ static inline void __dec_zone_page_state(struct page *page,
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
+static inline void reduce_pgdat_percpu_threshold(pg_data_t *pgdat) { }
+static inline void restore_pgdat_percpu_threshold(pg_data_t *pgdat) { }
+
 static inline void refresh_cpu_vm_stats(int cpu) { }
 #endif
 

commit aa45484031ddee09b06350ab8528bfe5b2c76d1c
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Sep 9 16:38:17 2010 -0700

    mm: page allocator: calculate a better estimate of NR_FREE_PAGES when memory is low and kswapd is awake
    
    Ordinarily watermark checks are based on the vmstat NR_FREE_PAGES as it is
    cheaper than scanning a number of lists.  To avoid synchronization
    overhead, counter deltas are maintained on a per-cpu basis and drained
    both periodically and when the delta is above a threshold.  On large CPU
    systems, the difference between the estimated and real value of
    NR_FREE_PAGES can be very high.  If NR_FREE_PAGES is much higher than
    number of real free page in buddy, the VM can allocate pages below min
    watermark, at worst reducing the real number of pages to zero.  Even if
    the OOM killer kills some victim for freeing memory, it may not free
    memory if the exit path requires a new page resulting in livelock.
    
    This patch introduces a zone_page_state_snapshot() function (courtesy of
    Christoph) that takes a slightly more accurate view of an arbitrary vmstat
    counter.  It is used to read NR_FREE_PAGES while kswapd is awake to avoid
    the watermark being accidentally broken.  The estimate is not perfect and
    may result in cache line bounces but is expected to be lighter than the
    IPI calls necessary to continually drain the per-cpu counters while kswapd
    is awake.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 7f43ccdc1d38..eaaea37b3b75 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -170,6 +170,28 @@ static inline unsigned long zone_page_state(struct zone *zone,
 	return x;
 }
 
+/*
+ * More accurate version that also considers the currently pending
+ * deltas. For that we need to loop over all cpus to find the current
+ * deltas. There is no synchronization so the result cannot be
+ * exactly accurate either.
+ */
+static inline unsigned long zone_page_state_snapshot(struct zone *zone,
+					enum zone_stat_item item)
+{
+	long x = atomic_long_read(&zone->vm_stat[item]);
+
+#ifdef CONFIG_SMP
+	int cpu;
+	for_each_online_cpu(cpu)
+		x += per_cpu_ptr(zone->pageset, cpu)->vm_stat_diff[item];
+
+	if (x < 0)
+		x = 0;
+#endif
+	return x;
+}
+
 extern unsigned long global_reclaimable_pages(void);
 extern unsigned long zone_reclaimable_pages(struct zone *zone);
 

commit 56de7263fcf3eb10c8dcdf8d59a9cec831795f3f
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon May 24 14:32:30 2010 -0700

    mm: compaction: direct compact when a high-order allocation fails
    
    Ordinarily when a high-order allocation fails, direct reclaim is entered
    to free pages to satisfy the allocation.  With this patch, it is
    determined if an allocation failed due to external fragmentation instead
    of low memory and if so, the calling process will compact until a suitable
    page is freed.  Compaction by moving pages in memory is considerably
    cheaper than paging out to disk and works where there are locked pages or
    no swap.  If compaction fails to free a page of a suitable size, then
    reclaim will still occur.
    
    Direct compaction returns as soon as possible.  As each block is
    compacted, it is checked if a suitable page has been freed and if so, it
    returns.
    
    [akpm@linux-foundation.org: Fix build errors]
    [aarcange@redhat.com: fix count_vm_event preempt in memory compaction direct reclaim]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index b421d1b22b62..7f43ccdc1d38 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -45,6 +45,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_COMPACTION
 		COMPACTBLOCKS, COMPACTPAGES, COMPACTPAGEFAILED,
+		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
 #endif
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,

commit 748446bb6b5a9390b546af38ec899c868a9dbcf0
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon May 24 14:32:27 2010 -0700

    mm: compaction: memory compaction core
    
    This patch is the core of a mechanism which compacts memory in a zone by
    relocating movable pages towards the end of the zone.
    
    A single compaction run involves a migration scanner and a free scanner.
    Both scanners operate on pageblock-sized areas in the zone.  The migration
    scanner starts at the bottom of the zone and searches for all movable
    pages within each area, isolating them onto a private list called
    migratelist.  The free scanner starts at the top of the zone and searches
    for suitable areas and consumes the free pages within making them
    available for the migration scanner.  The pages isolated for migration are
    then migrated to the newly isolated free pages.
    
    [aarcange@redhat.com: Fix unsafe optimisation]
    [mel@csn.ul.ie: do not schedule work on other CPUs for compaction]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 117f0dd8ad03..b421d1b22b62 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -43,6 +43,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
 		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+#ifdef CONFIG_COMPACTION
+		COMPACTBLOCKS, COMPACTPAGES, COMPACTPAGEFAILED,
+#endif
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
 #endif

commit 32032df6c2f6c9c6b2ada2ce42322231824f70c2
Merge: 22b737f4c751 c5974b835a90
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 5 09:17:33 2010 +0900

    Merge branch 'master' into percpu
    
    Conflicts:
            arch/powerpc/platforms/pseries/hvCall.S
            include/linux/percpu.h

commit bb3ab596832b920c703d1aea1ce76d69c0f71fb7
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Dec 14 17:58:55 2009 -0800

    vmscan: stop kswapd waiting on congestion when the min watermark is not being met
    
    If reclaim fails to make sufficient progress, the priority is raised.
    Once the priority is higher, kswapd starts waiting on congestion.
    However, if the zone is below the min watermark then kswapd needs to
    continue working without delay as there is a danger of an increased rate
    of GFP_ATOMIC allocation failure.
    
    This patch changes the conditions under which kswapd waits on congestion
    by only going to sleep if the min watermarks are being met.
    
    [mel@csn.ul.ie: add stats to track how relevant the logic is]
    [mel@csn.ul.ie: make kswapd only check its own zones and rename the relevant counters]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index fd5be240c0b7..ee03bba9c5df 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -40,7 +40,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGSCAN_ZONE_RECLAIM_FAILED,
 #endif
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
-		KSWAPD_PREMATURE_FAST, KSWAPD_PREMATURE_SLOW,
+		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
+		KSWAPD_SKIP_CONGESTION_WAIT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,

commit f50de2d3811081957156b5d736778799379c29de
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Dec 14 17:58:53 2009 -0800

    vmscan: have kswapd sleep for a short interval and double check it should be asleep
    
    After kswapd balances all zones in a pgdat, it goes to sleep.  In the
    event of no IO congestion, kswapd can go to sleep very shortly after the
    high watermark was reached.  If there are a constant stream of allocations
    from parallel processes, it can mean that kswapd went to sleep too quickly
    and the high watermark is not being maintained for sufficient length time.
    
    This patch makes kswapd go to sleep as a two-stage process.  It first
    tries to sleep for HZ/10.  If it is woken up by another process or the
    high watermark is no longer met, it's considered a premature sleep and
    kswapd continues work.  Otherwise it goes fully to sleep.
    
    This adds more counters to distinguish between fast and slow breaches of
    watermarks.  A "fast" premature sleep is one where the low watermark was
    hit in a very short time after kswapd going to sleep.  A "slow" premature
    sleep indicates that the high watermark was breached after a very short
    interval.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Frans Pop <elendil@planet.nl>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d85889710f9b..fd5be240c0b7 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -40,6 +40,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGSCAN_ZONE_RECLAIM_FAILED,
 #endif
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
+		KSWAPD_PREMATURE_FAST, KSWAPD_PREMATURE_SLOW,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,

commit dd17c8f72993f9461e9c19250e3f155d6d99df22
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Oct 29 22:34:15 2009 +0900

    percpu: remove per_cpu__ prefix.
    
    Now that the return from alloc_percpu is compatible with the address
    of per-cpu vars, it makes sense to hand around the address of per-cpu
    variables.  To make this sane, we remove the per_cpu__ prefix we used
    created to stop people accidentally using these vars directly.
    
    Now we have sparse, we can use that (next patch).
    
    tj: * Updated to convert stuff which were missed by or added after the
          original patch.
    
        * Kill per_cpu_var() macro.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d85889710f9b..3e489fda11a1 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -76,22 +76,22 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
 
 static inline void __count_vm_event(enum vm_event_item item)
 {
-	__this_cpu_inc(per_cpu_var(vm_event_states).event[item]);
+	__this_cpu_inc(vm_event_states.event[item]);
 }
 
 static inline void count_vm_event(enum vm_event_item item)
 {
-	this_cpu_inc(per_cpu_var(vm_event_states).event[item]);
+	this_cpu_inc(vm_event_states.event[item]);
 }
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
-	__this_cpu_add(per_cpu_var(vm_event_states).event[item], delta);
+	__this_cpu_add(vm_event_states.event[item], delta);
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
 {
-	this_cpu_add(per_cpu_var(vm_event_states).event[item], delta);
+	this_cpu_add(vm_event_states.event[item], delta);
 }
 
 extern void all_vm_events(unsigned long *);

commit 4dac3e98840f11bb2d8d52fd375150c7c1912117
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Sat Oct 3 19:48:23 2009 +0900

    this_cpu: Use this_cpu ops for VM statistics
    
    Using per cpu atomics for the vm statistics reduces their overhead.
    And in the case of x86 we are guaranteed that they will never race even
    in the lax form used for vm statistics.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 2d0f222388a8..d85889710f9b 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -76,24 +76,22 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
 
 static inline void __count_vm_event(enum vm_event_item item)
 {
-	__get_cpu_var(vm_event_states).event[item]++;
+	__this_cpu_inc(per_cpu_var(vm_event_states).event[item]);
 }
 
 static inline void count_vm_event(enum vm_event_item item)
 {
-	get_cpu_var(vm_event_states).event[item]++;
-	put_cpu();
+	this_cpu_inc(per_cpu_var(vm_event_states).event[item]);
 }
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
-	__get_cpu_var(vm_event_states).event[item] += delta;
+	__this_cpu_add(per_cpu_var(vm_event_states).event[item], delta);
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
 {
-	get_cpu_var(vm_event_states).event[item] += delta;
-	put_cpu();
+	this_cpu_add(per_cpu_var(vm_event_states).event[item], delta);
 }
 
 extern void all_vm_events(unsigned long *);

commit adea02a1bea71a508da32c04d715485a1fe62029
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Mon Sep 21 17:01:42 2009 -0700

    mm: count only reclaimable lru pages
    
    global_lru_pages() / zone_lru_pages() can be used in two ways:
    - to estimate max reclaimable pages in determine_dirtyable_memory()
    - to calculate the slab scan ratio
    
    When swap is full or not present, the anon lru lists are not reclaimable
    and also won't be scanned.  So the anon pages shall not be counted in both
    usage scenarios.  Also rename to _reclaimable_pages: now they are counting
    the possibly reclaimable lru pages.
    
    It can greatly (and correctly) increase the slab scan rate under high
    memory pressure (when most file pages have been reclaimed and swap is
    full/absent), thus reduce false OOM kills.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Li, Ming Chun" <macli@brc.ubc.ca>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d7f577f49d16..2d0f222388a8 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -166,15 +166,8 @@ static inline unsigned long zone_page_state(struct zone *zone,
 	return x;
 }
 
-extern unsigned long global_lru_pages(void);
-
-static inline unsigned long zone_lru_pages(struct zone *zone)
-{
-	return (zone_page_state(zone, NR_ACTIVE_ANON)
-		+ zone_page_state(zone, NR_ACTIVE_FILE)
-		+ zone_page_state(zone, NR_INACTIVE_ANON)
-		+ zone_page_state(zone, NR_INACTIVE_FILE));
-}
+extern unsigned long global_reclaimable_pages(void);
+extern unsigned long zone_reclaimable_pages(struct zone *zone);
 
 #ifdef CONFIG_NUMA
 /*

commit 5a2ae913f5229d6e1d4a666f0477350789d5128e
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:01:39 2009 -0700

    mm: remove __{add,sub}_zone_page_state()
    
    __add_zone_page_state() and __sub_zone_page_state() are unused.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 81a97cf8f0a0..d7f577f49d16 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -210,11 +210,6 @@ extern void zone_statistics(struct zone *, struct zone *);
 
 #endif /* CONFIG_NUMA */
 
-#define __add_zone_page_state(__z, __i, __d)	\
-		__mod_zone_page_state(__z, __i, __d)
-#define __sub_zone_page_state(__z, __i, __d)	\
-		__mod_zone_page_state(__z, __i,-(__d))
-
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
 

commit 24cf72518c79cdcda486ed26074ff8151291cf65
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jun 16 15:33:23 2009 -0700

    vmscan: count the number of times zone_reclaim() scans and fails
    
    On NUMA machines, the administrator can configure zone_reclaim_mode that
    is a more targetted form of direct reclaim.  On machines with large NUMA
    distances for example, a zone_reclaim_mode defaults to 1 meaning that
    clean unmapped pages will be reclaimed if the zone watermarks are not
    being met.
    
    There is a heuristic that determines if the scan is worthwhile but it is
    possible that the heuristic will fail and the CPU gets tied up scanning
    uselessly.  Detecting the situation requires some guesswork and
    experimentation so this patch adds a counter "zreclaim_failed" to
    /proc/vmstat.  If during high CPU utilisation this counter is increasing
    rapidly, then the resolution to the problem may be to set
    /proc/sys/vm/zone_reclaim_mode to 0.
    
    [akpm@linux-foundation.org: name things consistently]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index ff4696c6dce3..81a97cf8f0a0 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -36,6 +36,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGSTEAL),
 		FOR_ALL_ZONES(PGSCAN_KSWAPD),
 		FOR_ALL_ZONES(PGSCAN_DIRECT),
+#ifdef CONFIG_NUMA
+		PGSCAN_ZONE_RECLAIM_FAILED,
+#endif
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_HUGETLB_PAGE

commit 6837765963f1723e80ca97b1fae660f3a60d77df
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jun 16 15:32:51 2009 -0700

    mm: remove CONFIG_UNEVICTABLE_LRU config option
    
    Currently, nobody wants to turn UNEVICTABLE_LRU off.  Thus this
    configurability is unnecessary.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Acked-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 524cd1b28ecb..ff4696c6dce3 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -41,7 +41,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
 #endif
-#ifdef CONFIG_UNEVICTABLE_LRU
 		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
 		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
 		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
@@ -50,7 +49,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
 		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
 		UNEVICTABLE_MLOCKFREED,
-#endif
 		NR_VM_EVENT_ITEMS
 };
 

commit 5c9fe6281b75832e8d2555ec8700ea763d9a865e
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 04:19:42 2008 +0400

    proc: move /proc/zoneinfo boilerplate to mm/vmstat.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 7b68c4c1e19c..524cd1b28ecb 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -54,7 +54,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
-extern const struct seq_operations zoneinfo_op;
 extern int sysctl_stat_interval;
 
 #ifdef CONFIG_VM_EVENT_COUNTERS

commit b6aa44ab698c7df9d951d3eb45c4fcb8ba68fb25
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 04:17:48 2008 +0400

    proc: move /proc/vmstat boilerplate to mm/vmstat.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 33ffd89a88ac..7b68c4c1e19c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -55,7 +55,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 };
 
 extern const struct seq_operations zoneinfo_op;
-extern const struct seq_operations vmstat_op;
 extern int sysctl_stat_interval;
 
 #ifdef CONFIG_VM_EVENT_COUNTERS

commit 74e2e8e8ce7b3c0f878a349f9fa6cf2831548eef
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 04:15:36 2008 +0400

    proc: move /proc/pagetypeinfo boilerplate to mm/vmstat.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d4551f206409..33ffd89a88ac 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -54,7 +54,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
-extern const struct seq_operations pagetypeinfo_op;
 extern const struct seq_operations zoneinfo_op;
 extern const struct seq_operations vmstat_op;
 extern int sysctl_stat_interval;

commit 8f32f7e5ac2ed11b0659b6b55af926f3d58ffd9d
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 04:13:52 2008 +0400

    proc: move /proc/buddyinfo boilerplate to mm/vmstat.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 9cd3ab0f554d..d4551f206409 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -54,7 +54,6 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
-extern const struct seq_operations fragmentation_op;
 extern const struct seq_operations pagetypeinfo_op;
 extern const struct seq_operations zoneinfo_op;
 extern const struct seq_operations vmstat_op;

commit 985737cf2ea096ea946aed82c7484d40defc71a8
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Sat Oct 18 20:26:53 2008 -0700

    mlock: count attempts to free mlocked page
    
    Allow free of mlock()ed pages.  This shouldn't happen, but during
    developement, it occasionally did.
    
    This patch allows us to survive that condition, while keeping the
    statistics and events correct for debug.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 05b805020be2..9cd3ab0f554d 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -49,6 +49,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGMUNLOCKED,
 		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
 		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
+		UNEVICTABLE_MLOCKFREED,
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit 5344b7e648980cc2ca613ec03a56a8222ff48820
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:26:51 2008 -0700

    vmstat: mlocked pages statistics
    
    Add NR_MLOCK zone page state, which provides a (conservative) count of
    mlocked pages (actually, the number of mlocked pages moved off the LRU).
    
    Reworked by lts to fit in with the modified mlock page support in the
    Reclaim Scalability series.
    
    [kosaki.motohiro@jp.fujitsu.com: fix incorrect Mlocked field of /proc/meminfo]
    [lee.schermerhorn@hp.com: mlocked-pages: add event counting with statistics]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 135840cd7feb..05b805020be2 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -45,6 +45,10 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
 		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
 		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
+		UNEVICTABLE_PGMLOCKED,
+		UNEVICTABLE_PGMUNLOCKED,
+		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
+		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit bbfd28eee9fbd73e780b19beb3dc562befbb94fa
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Sat Oct 18 20:26:40 2008 -0700

    unevictable lru: add event counting with statistics
    
    Fix to unevictable-lru-page-statistics.patch
    
    Add unevictable lru infrastructure vm events to the statistics patch.
    Rename the "NORECL_" and "noreclaim_" symbols and text strings to
    "UNEVICTABLE_" and "unevictable_", respectively.
    
    Currently, both the infrastructure and the mlocked pages event are
    added by a single patch later in the series.  This makes it difficult
    to add or rework the incremental patches.  The events actually "belong"
    with the stats, so pull them up to here.
    
    Also, restore the event counting to putback_lru_page().  This was removed
    from previous patch in series where it was "misplaced".  The actual events
    weren't defined that early.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index ff5179f2b153..135840cd7feb 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -40,6 +40,11 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
+#endif
+#ifdef CONFIG_UNEVICTABLE_LRU
+		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
+		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
+		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
 #endif
 		NR_VM_EVENT_ITEMS
 };

commit 4f98a2fee8acdb4ac84545df98cccecfd130f8db
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Oct 18 20:26:32 2008 -0700

    vmscan: split LRU lists into anon & file sets
    
    Split the LRU lists in two, one set for pages that are backed by real file
    systems ("file") and one for pages that are backed by memory and swap
    ("anon").  The latter includes tmpfs.
    
    The advantage of doing this is that the VM will not have to scan over lots
    of anonymous pages (which we generally do not want to swap out), just to
    find the page cache pages that it should evict.
    
    This patch has the infrastructure and a basic policy to balance how much
    we scan the anon lists and how much we scan the file lists.  The big
    policy changes are in separate patches.
    
    [lee.schermerhorn@hp.com: collect lru meminfo statistics from correct offset]
    [kosaki.motohiro@jp.fujitsu.com: prevent incorrect oom under split_lru]
    [kosaki.motohiro@jp.fujitsu.com: fix pagevec_move_tail() doesn't treat unevictable page]
    [hugh@veritas.com: memcg swapbacked pages active]
    [hugh@veritas.com: splitlru: BDI_CAP_SWAP_BACKED]
    [akpm@linux-foundation.org: fix /proc/vmstat units]
    [nishimura@mxp.nes.nec.co.jp: memcg: fix handling of shmem migration]
    [kosaki.motohiro@jp.fujitsu.com: adjust Quicklists field of /proc/meminfo]
    [kosaki.motohiro@jp.fujitsu.com: fix style issue of get_scan_ratio()]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 58334d439516..ff5179f2b153 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -159,6 +159,16 @@ static inline unsigned long zone_page_state(struct zone *zone,
 	return x;
 }
 
+extern unsigned long global_lru_pages(void);
+
+static inline unsigned long zone_lru_pages(struct zone *zone)
+{
+	return (zone_page_state(zone, NR_ACTIVE_ANON)
+		+ zone_page_state(zone, NR_ACTIVE_FILE)
+		+ zone_page_state(zone, NR_INACTIVE_ANON)
+		+ zone_page_state(zone, NR_INACTIVE_FILE));
+}
+
 #ifdef CONFIG_NUMA
 /*
  * Determine the per node value of a stat item. This function

commit c748e1340e0de3fa7fed86f8bdf499be9242afff
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Jul 23 21:27:03 2008 -0700

    mm/vmstat.c: proper externs
    
    This patch adds proper extern declarations for five variables in
    include/linux/vmstat.h
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e83b69346d23..58334d439516 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -44,6 +44,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
+extern const struct seq_operations fragmentation_op;
+extern const struct seq_operations pagetypeinfo_op;
+extern const struct seq_operations zoneinfo_op;
+extern const struct seq_operations vmstat_op;
+extern int sysctl_stat_interval;
+
 #ifdef CONFIG_VM_EVENT_COUNTERS
 /*
  * Light weight per cpu counter implementation.

commit 3b1163006332302117b1b2acf226d4014ff46525
Author: Adam Litke <agl@us.ibm.com>
Date:   Mon Apr 28 02:13:06 2008 -0700

    Subject: [PATCH] hugetlb: vmstat events for huge page allocations
    
    Allocating huge pages directly from the buddy allocator is not guaranteed to
    succeed.  Success depends on several factors (such as the amount of physical
    memory available and the level of fragmentation).  With the addition of
    dynamic hugetlb pool resizing, allocations can occur much more frequently.
    For these reasons it is desirable to keep track of huge page allocation
    successes and failures.
    
    Add two new vmstat entries to track huge page allocations that succeed and
    fail.  The presence of the two entries is contingent upon CONFIG_HUGETLB_PAGE
    being enabled.
    
    [akpm@linux-foundation.org: reduced ifdeffery]
    Signed-off-by: Adam Litke <agl@us.ibm.com>
    Signed-off-by: Eric Munson <ebmunson@us.ibm.com>
    Tested-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Andy Whitcroft <apw@shadowen.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index e726b6d46495..e83b69346d23 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -25,6 +25,7 @@
 #define HIGHMEM_ZONE(xx)
 #endif
 
+
 #define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
@@ -37,6 +38,9 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGSCAN_DIRECT),
 		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
 		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+#ifdef CONFIG_HUGETLB_PAGE
+		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
+#endif
 		NR_VM_EVENT_ITEMS
 };
 

commit 18ea7e710d2452fa726814a406779188028cf1bf
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Mon Apr 28 02:12:14 2008 -0700

    mm: remember what the preferred zone is for zone_statistics
    
    On NUMA, zone_statistics() is used to record events like numa hit, miss and
    foreign.  It assumes that the first zone in a zonelist is the preferred zone.
    When multiple zonelists are replaced by one that is filtered, this is no
    longer the case.
    
    This patch records what the preferred zone is rather than assuming the first
    zone in the zonelist is it.  This simplifies the reading of later patches in
    this set.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <clameter@sgi.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 9f1b4b46151e..e726b6d46495 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -174,7 +174,7 @@ static inline unsigned long node_page_state(int node,
 		zone_page_state(&zones[ZONE_MOVABLE], item);
 }
 
-extern void zone_statistics(struct zonelist *, struct zone *);
+extern void zone_statistics(struct zone *, struct zone *);
 
 #else
 

commit 57ce36feb4d1281247755bc445bae77728298955
Author: Uwe Kleine-König <Uwe.Kleine-Koenig@digi.com>
Date:   Mon Feb 25 16:45:03 2008 +0100

    let __dec_zone_page_state use __dec_zone_state
    
    This removes code duplication and makes __dec_zone_page_state look like
    __inc_zone_page_state.
    
    Signed-off-by: Uwe Kleine-König <Uwe.Kleine-Koenig@digi.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 75370ec0923e..9f1b4b46151e 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -246,8 +246,7 @@ static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
-	atomic_long_dec(&page_zone(page)->vm_stat[item]);
-	atomic_long_dec(&vm_stat[item]);
+	__dec_zone_state(page_zone(page), item);
 }
 
 /*

commit 2a1e274acf0b1c192face19a4be7c12d4503eaaf
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jul 17 04:03:12 2007 -0700

    Create the ZONE_MOVABLE zone
    
    The following 8 patches against 2.6.20-mm2 create a zone called ZONE_MOVABLE
    that is only usable by allocations that specify both __GFP_HIGHMEM and
    __GFP_MOVABLE.  This has the effect of keeping all non-movable pages within a
    single memory partition while allowing movable allocations to be satisfied
    from either partition.  The patches may be applied with the list-based
    anti-fragmentation patches that groups pages together based on mobility.
    
    The size of the zone is determined by a kernelcore= parameter specified at
    boot-time.  This specifies how much memory is usable by non-movable
    allocations and the remainder is used for ZONE_MOVABLE.  Any range of pages
    within ZONE_MOVABLE can be released by migrating the pages or by reclaiming.
    
    When selecting a zone to take pages from for ZONE_MOVABLE, there are two
    things to consider.  First, only memory from the highest populated zone is
    used for ZONE_MOVABLE.  On the x86, this is probably going to be ZONE_HIGHMEM
    but it would be ZONE_DMA on ppc64 or possibly ZONE_DMA32 on x86_64.  Second,
    the amount of memory usable by the kernel will be spread evenly throughout
    NUMA nodes where possible.  If the nodes are not of equal size, the amount of
    memory usable by the kernel on some nodes may be greater than others.
    
    By default, the zone is not as useful for hugetlb allocations because they are
    pinned and non-migratable (currently at least).  A sysctl is provided that
    allows huge pages to be allocated from that zone.  This means that the huge
    page pool can be resized to the size of ZONE_MOVABLE during the lifetime of
    the system assuming that pages are not mlocked.  Despite huge pages being
    non-movable, we do not introduce additional external fragmentation of note as
    huge pages are always the largest contiguous block we care about.
    
    Credit goes to Andy Whitcroft for catching a large variety of problems during
    review of the patches.
    
    This patch creates an additional zone, ZONE_MOVABLE.  This zone is only usable
    by allocations which specify both __GFP_HIGHMEM and __GFP_MOVABLE.  Hot-added
    memory continues to be placed in their existing destination as there is no
    mechanism to redirect them to a specific zone.
    
    [y-goto@jp.fujitsu.com: Fix section mismatch of memory hotplug related code]
    [akpm@linux-foundation.org: various fixes]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d9325cf8a134..75370ec0923e 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -25,7 +25,7 @@
 #define HIGHMEM_ZONE(xx)
 #endif
 
-#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx)
+#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
@@ -170,7 +170,8 @@ static inline unsigned long node_page_state(int node,
 #ifdef CONFIG_HIGHMEM
 		zone_page_state(&zones[ZONE_HIGHMEM], item) +
 #endif
-		zone_page_state(&zones[ZONE_NORMAL], item);
+		zone_page_state(&zones[ZONE_NORMAL], item) +
+		zone_page_state(&zones[ZONE_MOVABLE], item);
 }
 
 extern void zone_statistics(struct zonelist *, struct zone *);

commit d1187ed21026fd512b87851d0ca26d9ae16f9059
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 9 02:35:12 2007 -0700

    vmstat: use our own timer events
    
    vmstat is currently using the cache reaper to periodically bring the
    statistics up to date.  The cache reaper does only exists in SLUB as a way to
    provide compatibility with SLAB.  This patch removes the vmstat calls from the
    slab allocators and provides its own handling.
    
    The advantage is also that we can use a different frequency for the updates.
    Refreshing vm stats is a pretty fast job so we can run this every second and
    stagger this by only one tick.  This will lead to some overlap in large
    systems.  F.e a system running at 250 HZ with 1024 processors will have 4 vm
    updates occurring at once.
    
    However, the vm stats update only accesses per node information.  It is only
    necessary to stagger the vm statistics updates per processor in each node.  Vm
    counter updates occurring on distant nodes will not cause cacheline
    contention.
    
    We could implement an alternate approach that runs the first processor on each
    node at the second and then each of the other processor on a node on a
    subsequent tick.  That may be useful to keep a large amount of the second free
    of timer activity.  Maybe the timer folks will have some feedback on this one?
    
    [jirislaby@gmail.com: add missing break]
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index acb1f105870c..d9325cf8a134 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -212,8 +212,6 @@ extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
 void refresh_cpu_vm_stats(int);
-void refresh_vm_stats(void);
-
 #else /* CONFIG_SMP */
 
 /*
@@ -260,7 +258,6 @@ static inline void __dec_zone_page_state(struct page *page,
 #define mod_zone_page_state __mod_zone_page_state
 
 static inline void refresh_cpu_vm_stats(int cpu) { }
-static inline void refresh_vm_stats(void) { }
 #endif
 
 #endif /* _LINUX_VMSTAT_H */

commit 780a065668b1c6ca6a70c7d36b9f6552ea3bb5f5
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Feb 10 01:44:41 2007 -0800

    [PATCH] count_vm_events-warning-fix
    
    - Prevent things like this:
    
            block/ll_rw_blk.c: In function 'submit_bio':
            block/ll_rw_blk.c:3222: warning: unused variable 'count'
    
      inlines are very, very preferable to macros.
    
    - remove unused get_cpu_vm_events() macro
    
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 7ba91f2839fa..acb1f105870c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -7,18 +7,6 @@
 #include <linux/mmzone.h>
 #include <asm/atomic.h>
 
-#ifdef CONFIG_VM_EVENT_COUNTERS
-/*
- * Light weight per cpu counter implementation.
- *
- * Counters should only be incremented.  You need to set EMBEDDED
- * to disable VM_EVENT_COUNTERS.  Things like procps (vmstat,
- * top, etc) use /proc/vmstat and depend on these counters.
- *
- * Counters are handled completely inline. On many platforms the code
- * generated will simply be the increment of a global address.
- */
-
 #ifdef CONFIG_ZONE_DMA
 #define DMA_ZONE(xx) xx##_DMA,
 #else
@@ -52,6 +40,17 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		NR_VM_EVENT_ITEMS
 };
 
+#ifdef CONFIG_VM_EVENT_COUNTERS
+/*
+ * Light weight per cpu counter implementation.
+ *
+ * Counters should only be incremented and no critical kernel component
+ * should rely on the counter values.
+ *
+ * Counters are handled completely inline. On many platforms the code
+ * generated will simply be the increment of a global address.
+ */
+
 struct vm_event_state {
 	unsigned long event[NR_VM_EVENT_ITEMS];
 };
@@ -92,12 +91,24 @@ static inline void vm_events_fold_cpu(int cpu)
 #else
 
 /* Disable counters */
-#define get_cpu_vm_events(e)	0L
-#define count_vm_event(e)	do { } while (0)
-#define count_vm_events(e,d)	do { } while (0)
-#define __count_vm_event(e)	do { } while (0)
-#define __count_vm_events(e,d)	do { } while (0)
-#define vm_events_fold_cpu(x)	do { } while (0)
+static inline void count_vm_event(enum vm_event_item item)
+{
+}
+static inline void count_vm_events(enum vm_event_item item, long delta)
+{
+}
+static inline void __count_vm_event(enum vm_event_item item)
+{
+}
+static inline void __count_vm_events(enum vm_event_item item, long delta)
+{
+}
+static inline void all_vm_events(unsigned long *ret)
+{
+}
+static inline void vm_events_fold_cpu(int cpu)
+{
+}
 
 #endif /* CONFIG_VM_EVENT_COUNTERS */
 

commit 4b51d66989218aad731a721b5b28c79bf5388c09
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:10 2007 -0800

    [PATCH] optional ZONE_DMA: optional ZONE_DMA in the VM
    
    Make ZONE_DMA optional in core code.
    
    - ifdef all code for ZONE_DMA and related definitions following the example
      for ZONE_DMA32 and ZONE_HIGHMEM.
    
    - Without ZONE_DMA, ZONE_HIGHMEM and ZONE_DMA32 we get to a ZONES_SHIFT of
      0.
    
    - Modify the VM statistics to work correctly without a DMA zone.
    
    - Modify slab to not create DMA slabs if there is no ZONE_DMA.
    
    [akpm@osdl.org: cleanup]
    [jdike@addtoit.com: build fix]
    [apw@shadowen.org: Simplify calculation of the number of bits we need for ZONES_SHIFT]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Matthew Wilcox <willy@debian.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 77caf911969c..7ba91f2839fa 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -19,6 +19,12 @@
  * generated will simply be the increment of a global address.
  */
 
+#ifdef CONFIG_ZONE_DMA
+#define DMA_ZONE(xx) xx##_DMA,
+#else
+#define DMA_ZONE(xx)
+#endif
+
 #ifdef CONFIG_ZONE_DMA32
 #define DMA32_ZONE(xx) xx##_DMA32,
 #else
@@ -31,7 +37,7 @@
 #define HIGHMEM_ZONE(xx)
 #endif
 
-#define FOR_ALL_ZONES(xx) xx##_DMA, DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx)
+#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx)
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
@@ -96,7 +102,8 @@ static inline void vm_events_fold_cpu(int cpu)
 #endif /* CONFIG_VM_EVENT_COUNTERS */
 
 #define __count_zone_vm_events(item, zone, delta) \
-			__count_vm_events(item##_DMA + zone_idx(zone), delta)
+		__count_vm_events(item##_NORMAL - ZONE_NORMAL + \
+		zone_idx(zone), delta)
 
 /*
  * Zone based page accounting with per cpu differentials.
@@ -143,14 +150,16 @@ static inline unsigned long node_page_state(int node,
 	struct zone *zones = NODE_DATA(node)->node_zones;
 
 	return
+#ifdef CONFIG_ZONE_DMA
+		zone_page_state(&zones[ZONE_DMA], item) +
+#endif
 #ifdef CONFIG_ZONE_DMA32
 		zone_page_state(&zones[ZONE_DMA32], item) +
 #endif
-		zone_page_state(&zones[ZONE_NORMAL], item) +
 #ifdef CONFIG_HIGHMEM
 		zone_page_state(&zones[ZONE_HIGHMEM], item) +
 #endif
-		zone_page_state(&zones[ZONE_DMA], item);
+		zone_page_state(&zones[ZONE_NORMAL], item);
 }
 
 extern void zone_statistics(struct zonelist *, struct zone *);

commit 96177299416dbccb73b54e6b344260154a445375
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:03 2007 -0800

    [PATCH] Drop free_pages()
    
    nr_free_pages is now a simple access to a global variable.  Make it a macro
    instead of a function.
    
    The nr_free_pages now requires vmstat.h to be included.  There is one
    occurrence in power management where we need to add the include.  Directly
    refrer to global_page_state() there to clarify why the #include was added.
    
    [akpm@osdl.org: arm build fix]
    [akpm@osdl.org: sparc64 build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index c8d55bcc09b9..77caf911969c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -3,6 +3,7 @@
 
 #include <linux/types.h>
 #include <linux/percpu.h>
+#include <linux/mm.h>
 #include <linux/mmzone.h>
 #include <asm/atomic.h>
 

commit c878538598d1e7ab41ecc0de8894e34e2fdef630
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sat Feb 10 01:43:01 2007 -0800

    [PATCH] Use ZVC for inactive and active counts
    
    The determination of the dirty ratio to determine writeback behavior is
    currently based on the number of total pages on the system.
    
    However, not all pages in the system may be dirtied.  Thus the ratio is always
    too low and can never reach 100%.  The ratio may be particularly skewed if
    large hugepage allocations, slab allocations or device driver buffers make
    large sections of memory not available anymore.  In that case we may get into
    a situation in which f.e.  the background writeback ratio of 40% cannot be
    reached anymore which leads to undesired writeback behavior.
    
    This patchset fixes that issue by determining the ratio based on the actual
    pages that may potentially be dirty.  These are the pages on the active and
    the inactive list plus free pages.
    
    The problem with those counts has so far been that it is expensive to
    calculate these because counts from multiple nodes and multiple zones will
    have to be summed up.  This patchset makes these counters ZVC counters.  This
    means that a current sum per zone, per node and for the whole system is always
    available via global variables and not expensive anymore to calculate.
    
    The patchset results in some other good side effects:
    
    - Removal of the various functions that sum up free, active and inactive
      page counts
    
    - Cleanup of the functions that display information via the proc filesystem.
    
    This patch:
    
    The use of a ZVC for nr_inactive and nr_active allows a simplification of some
    counter operations.  More ZVC functionality is used for sums etc in the
    following patches.
    
    [akpm@osdl.org: UP build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 5e9803ed17fc..c8d55bcc09b9 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -186,6 +186,9 @@ void inc_zone_page_state(struct page *, enum zone_stat_item);
 void dec_zone_page_state(struct page *, enum zone_stat_item);
 
 extern void inc_zone_state(struct zone *, enum zone_stat_item);
+extern void __inc_zone_state(struct zone *, enum zone_stat_item);
+extern void dec_zone_state(struct zone *, enum zone_stat_item);
+extern void __dec_zone_state(struct zone *, enum zone_stat_item);
 
 void refresh_cpu_vm_stats(int);
 void refresh_vm_stats(void);
@@ -214,6 +217,12 @@ static inline void __inc_zone_page_state(struct page *page,
 	__inc_zone_state(page_zone(page), item);
 }
 
+static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
+{
+	atomic_long_dec(&zone->vm_stat[item]);
+	atomic_long_dec(&vm_stat[item]);
+}
+
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {

commit e903387f1ebe3a7ddb93cd49c38341d3632df528
Author: Magnus Damm <damm@opensource.se>
Date:   Fri Dec 22 01:08:01 2006 -0800

    [PATCH] fix vm_events_fold_cpu() build breakage
    
    fix vm_events_fold_cpu() build breakage
    
    2.6.20-rc1 does not build properly if CONFIG_VM_EVENT_COUNTERS is set
    and CONFIG_HOTPLUG is unset:
    
      CC      init/version.o
      LD      init/built-in.o
      LD      .tmp_vmlinux1
    mm/built-in.o: In function `page_alloc_cpu_notify':
    page_alloc.c:(.text+0x56eb): undefined reference to `vm_events_fold_cpu'
    make: *** [.tmp_vmlinux1] Error 1
    
    [akpm@osdl.org: cleanup]
    Signed-off-by: Magnus Damm <magnus@valinux.co.jp>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 532360010919..5e9803ed17fc 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -74,7 +74,13 @@ static inline void count_vm_events(enum vm_event_item item, long delta)
 }
 
 extern void all_vm_events(unsigned long *);
+#ifdef CONFIG_HOTPLUG
 extern void vm_events_fold_cpu(int cpu);
+#else
+static inline void vm_events_fold_cpu(int cpu)
+{
+}
+#endif
 
 #else
 

commit 2aea4fb61609ba7ef82f7dc6fca116bda88816e1
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Dec 22 01:06:10 2006 -0800

    [PATCH] CONFIG_VM_EVENT_COUNTER comment decrustify
    
    The VM event counters, enabled by CONFIG_VM_EVENT_COUNTERS, which provides
    VM event counters in /proc/vmstat, has become more essential to
    non-EMBEDDED kernel configurations than they were in the past.  Comments in
    the code and the Kconfig configuration explanation were stale, downplaying
    their role excessively.
    
    Refresh those comments to correctly reflect the current role of VM event
    counters.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Acked-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index c89df55f6e03..532360010919 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -10,8 +10,9 @@
 /*
  * Light weight per cpu counter implementation.
  *
- * Counters should only be incremented and no critical kernel component
- * should rely on the counter values.
+ * Counters should only be incremented.  You need to set EMBEDDED
+ * to disable VM_EVENT_COUNTERS.  Things like procps (vmstat,
+ * top, etc) use /proc/vmstat and depend on these counters.
  *
  * Counters are handled completely inline. On many platforms the code
  * generated will simply be the increment of a global address.

commit 3ca212b813299899d2968aa0a24a797c3746f5ec
Author: Dave Jones <davej@redhat.com>
Date:   Fri Sep 29 02:00:14 2006 -0700

    [PATCH] Remove another config.h
    
    After the asm/ uses of #include <linux/config.h> this one is the next
    biggest source of noise.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 176c7f797339..c89df55f6e03 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -3,7 +3,6 @@
 
 #include <linux/types.h>
 #include <linux/percpu.h>
-#include <linux/config.h>
 #include <linux/mmzone.h>
 #include <asm/atomic.h>
 

commit 27bf71c2a7e596ed34e9bf2d4a5030321a09a1ad
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:15 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: remove display of counters for unconfigured zones
    
    eventcounters: Do not display counters for zones that are not available on an
    arch
    
    Do not define or display counters for the DMA32 and the HIGHMEM zone if such
    zones were not configured.
    
    [akpm@osdl.org: s390 fix]
    [heiko.carstens@de.ibm.com: s390 fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 9c6e62c56ec2..176c7f797339 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -18,7 +18,19 @@
  * generated will simply be the increment of a global address.
  */
 
-#define FOR_ALL_ZONES(x) x##_DMA, x##_DMA32, x##_NORMAL, x##_HIGH
+#ifdef CONFIG_ZONE_DMA32
+#define DMA32_ZONE(xx) xx##_DMA32,
+#else
+#define DMA32_ZONE(xx)
+#endif
+
+#ifdef CONFIG_HIGHMEM
+#define HIGHMEM_ZONE(xx) , xx##_HIGH
+#else
+#define HIGHMEM_ZONE(xx)
+#endif
+
+#define FOR_ALL_ZONES(xx) xx##_DMA, DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx)
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),

commit fb0e7942bdcbbd2f90e61cb4cfa4fa892a873f8a
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:13 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: make ZONE_DMA32 optional
    
    Make ZONE_DMA32 optional
    
    - Add #ifdefs around ZONE_DMA32 specific code and definitions.
    
    - Add CONFIG_ZONE_DMA32 config option and use that for x86_64
      that alone needs this zone.
    
    - Remove the use of CONFIG_DMA_IS_DMA32 and CONFIG_DMA_IS_NORMAL
      for ia64 and fix up the way per node ZVCs are calculated.
    
    - Fall back to prior GFP_ZONEMASK of 0x03 if there is no
      DMA32 zone.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 2d9b1b60798a..9c6e62c56ec2 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -124,12 +124,10 @@ static inline unsigned long node_page_state(int node,
 	struct zone *zones = NODE_DATA(node)->node_zones;
 
 	return
-#ifndef CONFIG_DMA_IS_NORMAL
-#if !defined(CONFIG_DMA_IS_DMA32) && BITS_PER_LONG >= 64
+#ifdef CONFIG_ZONE_DMA32
 		zone_page_state(&zones[ZONE_DMA32], item) +
 #endif
 		zone_page_state(&zones[ZONE_NORMAL], item) +
-#endif
 #ifdef CONFIG_HIGHMEM
 		zone_page_state(&zones[ZONE_HIGHMEM], item) +
 #endif

commit 38cbcdc0a7be69a15462dc49512d43353f34b43b
Author: Jan Blunck <jblunck@suse.de>
Date:   Sat Aug 5 12:14:14 2006 -0700

    [PATCH] fix vmstat per cpu usage
    
    The per cpu variables are used incorrectly in vmstat.h.
    
    Signed-off-by: Jan Blunck <jblunck@suse.de>
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Acked-by: Steve Fox <drfickle@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 1ab806c47514..2d9b1b60798a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -41,23 +41,23 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
 
 static inline void __count_vm_event(enum vm_event_item item)
 {
-	__get_cpu_var(vm_event_states.event[item])++;
+	__get_cpu_var(vm_event_states).event[item]++;
 }
 
 static inline void count_vm_event(enum vm_event_item item)
 {
-	get_cpu_var(vm_event_states.event[item])++;
+	get_cpu_var(vm_event_states).event[item]++;
 	put_cpu();
 }
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
-	__get_cpu_var(vm_event_states.event[item]) += delta;
+	__get_cpu_var(vm_event_states).event[item] += delta;
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
 {
-	get_cpu_var(vm_event_states.event[item]) += delta;
+	get_cpu_var(vm_event_states).event[item] += delta;
 	put_cpu();
 }
 

commit 7f4599e9cd6bca0efc1000359584d1cff68f9f13
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Jul 10 04:44:30 2006 -0700

    [PATCH] ZVC: add __inc_zone_state for !SMP configuration
    
    It turns out that there is a way to build a kernel with NUMA and no SMP.
    In that case we are missing one definition __inc_zone_state.
    
    Provide that missing __inc_zone_state.
    
    (akpm: NUMA && !SMP sounds odd, but I am told "But there is the concept of
    cpuless nodes.  A NUMA system without SMP has a single processor but multiple
    memory nodes.  This used to work before on IA64 (wasn't aware of it, never seen
    anyone with this kind of thing).")
    
    Acked-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index d673b7b15c34..1ab806c47514 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -186,11 +186,16 @@ static inline void __mod_zone_page_state(struct zone *zone,
 	zone_page_state_add(delta, zone, item);
 }
 
+static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
+{
+	atomic_long_inc(&zone->vm_stat[item]);
+	atomic_long_inc(&vm_stat[item]);
+}
+
 static inline void __inc_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
-	atomic_long_inc(&page_zone(page)->vm_stat[item]);
-	atomic_long_inc(&vm_stat[item]);
+	__inc_zone_state(page_zone(page), item);
 }
 
 static inline void __dec_zone_page_state(struct page *page,

commit e45b3b6af09dab2a28a7c88b340d0bcdd173e068
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Jul 10 04:43:50 2006 -0700

    [PATCH] count_vm_events() fix
    
    Dopey bug.  Causes hopelessly-wrong numbers from vmstat(8) and several other
    counters.
    
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 3e0daf54133e..d673b7b15c34 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -57,7 +57,7 @@ static inline void __count_vm_events(enum vm_event_item item, long delta)
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
 {
-	get_cpu_var(vm_event_states.event[item])++;
+	get_cpu_var(vm_event_states.event[item]) += delta;
 	put_cpu();
 }
 

commit f8891e5e1f93a128c3900f82035e8541357896a7
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:45 2006 -0700

    [PATCH] Light weight event counters
    
    The remaining counters in page_state after the zoned VM counter patches
    have been applied are all just for show in /proc/vmstat.  They have no
    essential function for the VM.
    
    We use a simple increment of per cpu variables.  In order to avoid the most
    severe races we disable preempt.  Preempt does not prevent the race between
    an increment and an interrupt handler incrementing the same statistics
    counter.  However, that race is exceedingly rare, we may only loose one
    increment or so and there is no requirement (at least not in kernel) that
    the vm event counters have to be accurate.
    
    In the non preempt case this results in a simple increment for each
    counter.  For many architectures this will be reduced by the compiler to a
    single instruction.  This single instruction is atomic for i386 and x86_64.
     And therefore even the rare race condition in an interrupt is avoided for
    both architectures in most cases.
    
    The patchset also adds an off switch for embedded systems that allows a
    building of linux kernels without these counters.
    
    The implementation of these counters is through inline code that hopefully
    results in only a single instruction increment instruction being emitted
    (i386, x86_64) or in the increment being hidden though instruction
    concurrency (EPIC architectures such as ia64 can get that done).
    
    Benefits:
    - VM event counter operations usually reduce to a single inline instruction
      on i386 and x86_64.
    - No interrupt disable, only preempt disable for the preempt case.
      Preempt disable can also be avoided by moving the counter into a spinlock.
    - Handling is similar to zoned VM counters.
    - Simple and easily extendable.
    - Can be omitted to reduce memory use for embedded use.
    
    References:
    
    RFC http://marc.theaimsgroup.com/?l=linux-kernel&m=113512330605497&w=2
    RFC http://marc.theaimsgroup.com/?l=linux-kernel&m=114988082814934&w=2
    local_t http://marc.theaimsgroup.com/?l=linux-kernel&m=114991748606690&w=2
    V2 http://marc.theaimsgroup.com/?t=115014808400007&r=1&w=2
    V3 http://marc.theaimsgroup.com/?l=linux-kernel&m=115024767022346&w=2
    V4 http://marc.theaimsgroup.com/?l=linux-kernel&m=115047968808926&w=2
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 16173b63ee67..3e0daf54133e 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -7,115 +7,77 @@
 #include <linux/mmzone.h>
 #include <asm/atomic.h>
 
+#ifdef CONFIG_VM_EVENT_COUNTERS
 /*
- * Global page accounting.  One instance per CPU.  Only unsigned longs are
- * allowed.
+ * Light weight per cpu counter implementation.
  *
- * - Fields can be modified with xxx_page_state and xxx_page_state_zone at
- * any time safely (which protects the instance from modification by
- * interrupt.
- * - The __xxx_page_state variants can be used safely when interrupts are
- * disabled.
- * - The __xxx_page_state variants can be used if the field is only
- * modified from process context and protected from preemption, or only
- * modified from interrupt context.  In this case, the field should be
- * commented here.
+ * Counters should only be incremented and no critical kernel component
+ * should rely on the counter values.
+ *
+ * Counters are handled completely inline. On many platforms the code
+ * generated will simply be the increment of a global address.
  */
-struct page_state {
-	unsigned long pgpgin;		/* Disk reads */
-	unsigned long pgpgout;		/* Disk writes */
-	unsigned long pswpin;		/* swap reads */
-	unsigned long pswpout;		/* swap writes */
-
-	unsigned long pgalloc_high;	/* page allocations */
-	unsigned long pgalloc_normal;
-	unsigned long pgalloc_dma32;
-	unsigned long pgalloc_dma;
-
-	unsigned long pgfree;		/* page freeings */
-	unsigned long pgactivate;	/* pages moved inactive->active */
-	unsigned long pgdeactivate;	/* pages moved active->inactive */
-
-	unsigned long pgfault;		/* faults (major+minor) */
-	unsigned long pgmajfault;	/* faults (major only) */
-
-	unsigned long pgrefill_high;	/* inspected in refill_inactive_zone */
-	unsigned long pgrefill_normal;
-	unsigned long pgrefill_dma32;
-	unsigned long pgrefill_dma;
-
-	unsigned long pgsteal_high;	/* total highmem pages reclaimed */
-	unsigned long pgsteal_normal;
-	unsigned long pgsteal_dma32;
-	unsigned long pgsteal_dma;
-
-	unsigned long pgscan_kswapd_high;/* total highmem pages scanned */
-	unsigned long pgscan_kswapd_normal;
-	unsigned long pgscan_kswapd_dma32;
-	unsigned long pgscan_kswapd_dma;
-
-	unsigned long pgscan_direct_high;/* total highmem pages scanned */
-	unsigned long pgscan_direct_normal;
-	unsigned long pgscan_direct_dma32;
-	unsigned long pgscan_direct_dma;
-
-	unsigned long pginodesteal;	/* pages reclaimed via inode freeing */
-	unsigned long slabs_scanned;	/* slab objects scanned */
-	unsigned long kswapd_steal;	/* pages reclaimed by kswapd */
-	unsigned long kswapd_inodesteal;/* reclaimed via kswapd inode freeing */
-	unsigned long pageoutrun;	/* kswapd's calls to page reclaim */
-	unsigned long allocstall;	/* direct reclaim calls */
-
-	unsigned long pgrotated;	/* pages rotated to tail of the LRU */
+
+#define FOR_ALL_ZONES(x) x##_DMA, x##_DMA32, x##_NORMAL, x##_HIGH
+
+enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
+		FOR_ALL_ZONES(PGALLOC),
+		PGFREE, PGACTIVATE, PGDEACTIVATE,
+		PGFAULT, PGMAJFAULT,
+		FOR_ALL_ZONES(PGREFILL),
+		FOR_ALL_ZONES(PGSTEAL),
+		FOR_ALL_ZONES(PGSCAN_KSWAPD),
+		FOR_ALL_ZONES(PGSCAN_DIRECT),
+		PGINODESTEAL, SLABS_SCANNED, KSWAPD_STEAL, KSWAPD_INODESTEAL,
+		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
+		NR_VM_EVENT_ITEMS
+};
+
+struct vm_event_state {
+	unsigned long event[NR_VM_EVENT_ITEMS];
 };
 
-extern void get_full_page_state(struct page_state *ret);
-extern void mod_page_state_offset(unsigned long offset, unsigned long delta);
-extern void __mod_page_state_offset(unsigned long offset, unsigned long delta);
-
-#define mod_page_state(member, delta)	\
-	mod_page_state_offset(offsetof(struct page_state, member), (delta))
-
-#define __mod_page_state(member, delta)	\
-	__mod_page_state_offset(offsetof(struct page_state, member), (delta))
-
-#define inc_page_state(member)		mod_page_state(member, 1UL)
-#define dec_page_state(member)		mod_page_state(member, 0UL - 1)
-#define add_page_state(member,delta)	mod_page_state(member, (delta))
-#define sub_page_state(member,delta)	mod_page_state(member, 0UL - (delta))
-
-#define __inc_page_state(member)	__mod_page_state(member, 1UL)
-#define __dec_page_state(member)	__mod_page_state(member, 0UL - 1)
-#define __add_page_state(member,delta)	__mod_page_state(member, (delta))
-#define __sub_page_state(member,delta)	__mod_page_state(member, 0UL - (delta))
-
-#define page_state(member) (*__page_state(offsetof(struct page_state, member)))
-
-#define state_zone_offset(zone, member)					\
-({									\
-	unsigned offset;						\
-	if (is_highmem(zone))						\
-		offset = offsetof(struct page_state, member##_high);	\
-	else if (is_normal(zone))					\
-		offset = offsetof(struct page_state, member##_normal);	\
-	else if (is_dma32(zone))					\
-		offset = offsetof(struct page_state, member##_dma32);	\
-	else								\
-		offset = offsetof(struct page_state, member##_dma);	\
-	offset;								\
-})
-
-#define __mod_page_state_zone(zone, member, delta)			\
- do {									\
-	__mod_page_state_offset(state_zone_offset(zone, member), (delta)); \
- } while (0)
-
-#define mod_page_state_zone(zone, member, delta)			\
- do {									\
-	mod_page_state_offset(state_zone_offset(zone, member), (delta)); \
- } while (0)
-
-DECLARE_PER_CPU(struct page_state, page_states);
+DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
+
+static inline void __count_vm_event(enum vm_event_item item)
+{
+	__get_cpu_var(vm_event_states.event[item])++;
+}
+
+static inline void count_vm_event(enum vm_event_item item)
+{
+	get_cpu_var(vm_event_states.event[item])++;
+	put_cpu();
+}
+
+static inline void __count_vm_events(enum vm_event_item item, long delta)
+{
+	__get_cpu_var(vm_event_states.event[item]) += delta;
+}
+
+static inline void count_vm_events(enum vm_event_item item, long delta)
+{
+	get_cpu_var(vm_event_states.event[item])++;
+	put_cpu();
+}
+
+extern void all_vm_events(unsigned long *);
+extern void vm_events_fold_cpu(int cpu);
+
+#else
+
+/* Disable counters */
+#define get_cpu_vm_events(e)	0L
+#define count_vm_event(e)	do { } while (0)
+#define count_vm_events(e,d)	do { } while (0)
+#define __count_vm_event(e)	do { } while (0)
+#define __count_vm_events(e,d)	do { } while (0)
+#define vm_events_fold_cpu(x)	do { } while (0)
+
+#endif /* CONFIG_VM_EVENT_COUNTERS */
+
+#define __count_zone_vm_events(item, zone, delta) \
+			__count_vm_events(item##_DMA + zone_idx(zone), delta)
 
 /*
  * Zone based page accounting with per cpu differentials.

commit ca889e6c45e0b112cb2ca9d35afc66297519b5d5
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:44 2006 -0700

    [PATCH] Use Zoned VM Counters for NUMA statistics
    
    The numa statistics are really event counters.  But they are per node and
    so we have had special treatment for these counters through additional
    fields on the pcp structure.  We can now use the per zone nature of the
    zoned VM counters to realize these.
    
    This will shrink the size of the pcp structure on NUMA systems.  We will
    have some room to add additional per zone counters that will all still fit
    in the same cacheline.
    
     Bits   Prior pcp size          Size after patch        We can add
     ------------------------------------------------------------------
     64     128 bytes (16 words)    80 bytes (10 words)     48
     32      76 bytes (19 words)    56 bytes (14 words)     8 (64 byte cacheline)
                                                            72 (128 byte)
    
    Remove the special statistics for numa and replace them with zoned vm
    counters.  This has the side effect that global sums of these events now
    show up in /proc/vmstat.
    
    Also take the opportunity to move the zone_statistics() function from
    page_alloc.c into vmstat.c.
    
    Discussions:
    V2 http://marc.theaimsgroup.com/?t=115048227000002&r=1&w=2
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 5fad1613e7d6..16173b63ee67 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -173,9 +173,15 @@ static inline unsigned long node_page_state(int node,
 #endif
 		zone_page_state(&zones[ZONE_DMA], item);
 }
+
+extern void zone_statistics(struct zonelist *, struct zone *);
+
 #else
+
 #define node_page_state(node, item) global_page_state(item)
-#endif
+#define zone_statistics(_zl,_z) do { } while (0)
+
+#endif /* CONFIG_NUMA */
 
 #define __add_zone_page_state(__z, __i, __d)	\
 		__mod_zone_page_state(__z, __i, __d)
@@ -190,6 +196,8 @@ static inline void zap_zone_vm_stats(struct zone *zone)
 	memset(zone->vm_stat, 0, sizeof(zone->vm_stat));
 }
 
+extern void inc_zone_state(struct zone *, enum zone_stat_item);
+
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, int);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);

commit bab1846a0582f627f5ec22aa2dc5f4f3e82e8176
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Jun 30 01:55:43 2006 -0700

    [PATCH] zoned-vm-counters: remove read_page_state()
    
    No callers.
    
    Cc: Christoph Lameter <clameter@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 5b5b96afc395..5fad1613e7d6 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -70,13 +70,9 @@ struct page_state {
 };
 
 extern void get_full_page_state(struct page_state *ret);
-extern unsigned long read_page_state_offset(unsigned long offset);
 extern void mod_page_state_offset(unsigned long offset, unsigned long delta);
 extern void __mod_page_state_offset(unsigned long offset, unsigned long delta);
 
-#define read_page_state(member) \
-	read_page_state_offset(offsetof(struct page_state, member))
-
 #define mod_page_state(member, delta)	\
 	mod_page_state_offset(offsetof(struct page_state, member), (delta))
 

commit d2c5e30c9a1420902262aa923794d2ae4e0bc391
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:41 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_bounce to per zone counter
    
    Conversion of nr_bounce to a per zone counter
    
    nr_bounce is only used for proc output.  So it could be left as an event
    counter.  However, the event counters may not be accurate and nr_bounce is
    categorizing types of pages in a zone.  So we really need this to also be a
    per zone counter.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 9de2a41c885c..5b5b96afc395 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -67,7 +67,6 @@ struct page_state {
 	unsigned long allocstall;	/* direct reclaim calls */
 
 	unsigned long pgrotated;	/* pages rotated to tail of the LRU */
-	unsigned long nr_bounce;	/* pages for bounce buffers */
 };
 
 extern void get_full_page_state(struct page_state *ret);

commit fd39fc8561be33065306bdac0e30414e1e8ac8e1
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:40 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_unstable to per zone counter
    
    Conversion of nr_unstable to a per zone counter
    
    We need to do some special modifications to the nfs code since there are
    multiple cases of disposition and we need to have a page ref for proper
    accounting.
    
    This converts the last critical page state of the VM and therefore we need to
    remove several functions that were depending on GET_PAGE_STATE_LAST in order
    to make the kernel compile again.  We are only left with event type counters
    in page state.
    
    [akpm@osdl.org: bugfixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 60c2e0382ceb..9de2a41c885c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -22,13 +22,6 @@
  * commented here.
  */
 struct page_state {
-	unsigned long nr_unstable;	/* NFS unstable pages */
-#define GET_PAGE_STATE_LAST nr_unstable
-
-	/*
-	 * The below are zeroed by get_page_state().  Use get_full_page_state()
-	 * to add up all these.
-	 */
 	unsigned long pgpgin;		/* Disk reads */
 	unsigned long pgpgout;		/* Disk writes */
 	unsigned long pswpin;		/* swap reads */
@@ -77,8 +70,6 @@ struct page_state {
 	unsigned long nr_bounce;	/* pages for bounce buffers */
 };
 
-extern void get_page_state(struct page_state *ret);
-extern void get_page_state_node(struct page_state *ret, int node);
 extern void get_full_page_state(struct page_state *ret);
 extern unsigned long read_page_state_offset(unsigned long offset);
 extern void mod_page_state_offset(unsigned long offset, unsigned long delta);

commit ce866b34ae1b7f1ce60234cf65855886ac7e7d30
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:40 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_writeback to per zone counter
    
    Conversion of nr_writeback to per zone counter.
    
    This removes the last page_state counter from arch/i386/mm/pgtable.c so we
    drop the page_state from there.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index b323ea2c6260..60c2e0382ceb 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -22,7 +22,6 @@
  * commented here.
  */
 struct page_state {
-	unsigned long nr_writeback;	/* Pages under writeback */
 	unsigned long nr_unstable;	/* NFS unstable pages */
 #define GET_PAGE_STATE_LAST nr_unstable
 

commit b1e7a8fd854d2f895730e82137400012b509650e
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:39 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_dirty to per zone counter
    
    This makes nr_dirty a per zone counter.  Looping over all processors is
    avoided during writeback state determination.
    
    The counter aggregation for nr_dirty had to be undone in the NFS layer since
    we summed up the page counts from multiple zones.  Someone more familiar with
    NFS should probably review what I have done.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 56220441d7c9..b323ea2c6260 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -22,7 +22,6 @@
  * commented here.
  */
 struct page_state {
-	unsigned long nr_dirty;		/* Dirty writeable pages */
 	unsigned long nr_writeback;	/* Pages under writeback */
 	unsigned long nr_unstable;	/* NFS unstable pages */
 #define GET_PAGE_STATE_LAST nr_unstable

commit df849a1529c106f7460e51479ca78fe07b07dc8c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:38 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_pagetables to per zone counter
    
    Conversion of nr_page_table_pages to a per zone counter
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 4b97381a2937..56220441d7c9 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -25,8 +25,7 @@ struct page_state {
 	unsigned long nr_dirty;		/* Dirty writeable pages */
 	unsigned long nr_writeback;	/* Pages under writeback */
 	unsigned long nr_unstable;	/* NFS unstable pages */
-	unsigned long nr_page_table_pages;/* Pages used for pagetables */
-#define GET_PAGE_STATE_LAST nr_page_table_pages
+#define GET_PAGE_STATE_LAST nr_unstable
 
 	/*
 	 * The below are zeroed by get_page_state().  Use get_full_page_state()

commit 9a865ffa34b6117a5e0b67640a084d8c2e198c93
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:38 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_slab to per zone counter
    
    - Allows reclaim to access counter without looping over processor counts.
    
    - Allows accurate statistics on how many pages are used in a zone by
      the slab. This may become useful to balance slab allocations over
      various zones.
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 8ab8229523e6..4b97381a2937 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -26,8 +26,7 @@ struct page_state {
 	unsigned long nr_writeback;	/* Pages under writeback */
 	unsigned long nr_unstable;	/* NFS unstable pages */
 	unsigned long nr_page_table_pages;/* Pages used for pagetables */
-	unsigned long nr_slab;		/* In slab */
-#define GET_PAGE_STATE_LAST nr_slab
+#define GET_PAGE_STATE_LAST nr_page_table_pages
 
 	/*
 	 * The below are zeroed by get_page_state().  Use get_full_page_state()

commit 65ba55f500a37272985d071c9bbb35256a2f7c14
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:34 2006 -0700

    [PATCH] zoned vm counters: convert nr_mapped to per zone counter
    
    nr_mapped is important because it allows a determination of how many pages of
    a zone are not mapped, which would allow a more efficient means of determining
    when we need to reclaim memory in a zone.
    
    We take the nr_mapped field out of the page state structure and define a new
    per zone counter named NR_FILE_MAPPED (the anonymous pages will be split off
    from NR_MAPPED in the next patch).
    
    We replace the use of nr_mapped in various kernel locations.  This avoids the
    looping over all processors in try_to_free_pages(), writeback, reclaim (swap +
    zone reclaim).
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 3fd5c11e544a..8ab8229523e6 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -26,8 +26,6 @@ struct page_state {
 	unsigned long nr_writeback;	/* Pages under writeback */
 	unsigned long nr_unstable;	/* NFS unstable pages */
 	unsigned long nr_page_table_pages;/* Pages used for pagetables */
-	unsigned long nr_mapped;	/* mapped into pagetables.
-					 * only modified from process context */
 	unsigned long nr_slab;		/* In slab */
 #define GET_PAGE_STATE_LAST nr_slab
 

commit 2244b95a7bcf8d24196f8a3a44187ba5dfff754c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:33 2006 -0700

    [PATCH] zoned vm counters: basic ZVC (zoned vm counter) implementation
    
    Per zone counter infrastructure
    
    The counters that we currently have for the VM are split per processor.  The
    processor however has not much to do with the zone these pages belong to.  We
    cannot tell f.e.  how many ZONE_DMA pages are dirty.
    
    So we are blind to potentially inbalances in the usage of memory in various
    zones.  F.e.  in a NUMA system we cannot tell how many pages are dirty on a
    particular node.  If we knew then we could put measures into the VM to balance
    the use of memory between different zones and different nodes in a NUMA
    system.  For example it would be possible to limit the dirty pages per node so
    that fast local memory is kept available even if a process is dirtying huge
    amounts of pages.
    
    Another example is zone reclaim.  We do not know how many unmapped pages exist
    per zone.  So we just have to try to reclaim.  If it is not working then we
    pause and try again later.  It would be better if we knew when it makes sense
    to reclaim unmapped pages from a zone.  This patchset allows the determination
    of the number of unmapped pages per zone.  We can remove the zone reclaim
    interval with the counters introduced here.
    
    Futhermore the ability to have various usage statistics available will allow
    the development of new NUMA balancing algorithms that may be able to improve
    the decision making in the scheduler of when to move a process to another node
    and hopefully will also enable automatic page migration through a user space
    program that can analyse the memory load distribution and then rebalance
    memory use in order to increase performance.
    
    The counter framework here implements differential counters for each processor
    in struct zone.  The differential counters are consolidated when a threshold
    is exceeded (like done in the current implementation for nr_pageache), when
    slab reaping occurs or when a consolidation function is called.
    
    Consolidation uses atomic operations and accumulates counters per zone in the
    zone structure and also globally in the vm_stat array.  VM functions can
    access the counts by simply indexing a global or zone specific array.
    
    The arrangement of counters in an array also simplifies processing when output
    has to be generated for /proc/*.
    
    Counters can be updated by calling inc/dec_zone_page_state or
    _inc/dec_zone_page_state analogous to *_page_state.  The second group of
    functions can be called if it is known that interrupts are disabled.
    
    Special optimized increment and decrement functions are provided.  These can
    avoid certain checks and use increment or decrement instructions that an
    architecture may provide.
    
    We also add a new CONFIG_DMA_IS_NORMAL that signifies that an architecture can
    do DMA to all memory and therefore ZONE_NORMAL will not be populated.  This is
    only currently set for IA64 SGI SN2 and currently only affects
    node_page_state().  In the best case node_page_state can be reduced to
    retrieving a single counter for the one zone on the node.
    
    [akpm@osdl.org: cleanups]
    [akpm@osdl.org: export vm_stat[] for filesystems]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index 3ca0c1989fc2..3fd5c11e544a 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -3,6 +3,9 @@
 
 #include <linux/types.h>
 #include <linux/percpu.h>
+#include <linux/config.h>
+#include <linux/mmzone.h>
+#include <asm/atomic.h>
 
 /*
  * Global page accounting.  One instance per CPU.  Only unsigned longs are
@@ -134,5 +137,129 @@ extern void __mod_page_state_offset(unsigned long offset, unsigned long delta);
 
 DECLARE_PER_CPU(struct page_state, page_states);
 
-#endif /* _LINUX_VMSTAT_H */
+/*
+ * Zone based page accounting with per cpu differentials.
+ */
+extern atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
+
+static inline void zone_page_state_add(long x, struct zone *zone,
+				 enum zone_stat_item item)
+{
+	atomic_long_add(x, &zone->vm_stat[item]);
+	atomic_long_add(x, &vm_stat[item]);
+}
+
+static inline unsigned long global_page_state(enum zone_stat_item item)
+{
+	long x = atomic_long_read(&vm_stat[item]);
+#ifdef CONFIG_SMP
+	if (x < 0)
+		x = 0;
+#endif
+	return x;
+}
+
+static inline unsigned long zone_page_state(struct zone *zone,
+					enum zone_stat_item item)
+{
+	long x = atomic_long_read(&zone->vm_stat[item]);
+#ifdef CONFIG_SMP
+	if (x < 0)
+		x = 0;
+#endif
+	return x;
+}
+
+#ifdef CONFIG_NUMA
+/*
+ * Determine the per node value of a stat item. This function
+ * is called frequently in a NUMA machine, so try to be as
+ * frugal as possible.
+ */
+static inline unsigned long node_page_state(int node,
+				 enum zone_stat_item item)
+{
+	struct zone *zones = NODE_DATA(node)->node_zones;
+
+	return
+#ifndef CONFIG_DMA_IS_NORMAL
+#if !defined(CONFIG_DMA_IS_DMA32) && BITS_PER_LONG >= 64
+		zone_page_state(&zones[ZONE_DMA32], item) +
+#endif
+		zone_page_state(&zones[ZONE_NORMAL], item) +
+#endif
+#ifdef CONFIG_HIGHMEM
+		zone_page_state(&zones[ZONE_HIGHMEM], item) +
+#endif
+		zone_page_state(&zones[ZONE_DMA], item);
+}
+#else
+#define node_page_state(node, item) global_page_state(item)
+#endif
+
+#define __add_zone_page_state(__z, __i, __d)	\
+		__mod_zone_page_state(__z, __i, __d)
+#define __sub_zone_page_state(__z, __i, __d)	\
+		__mod_zone_page_state(__z, __i,-(__d))
+
+#define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
+#define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
+
+static inline void zap_zone_vm_stats(struct zone *zone)
+{
+	memset(zone->vm_stat, 0, sizeof(zone->vm_stat));
+}
+
+#ifdef CONFIG_SMP
+void __mod_zone_page_state(struct zone *, enum zone_stat_item item, int);
+void __inc_zone_page_state(struct page *, enum zone_stat_item);
+void __dec_zone_page_state(struct page *, enum zone_stat_item);
 
+void mod_zone_page_state(struct zone *, enum zone_stat_item, int);
+void inc_zone_page_state(struct page *, enum zone_stat_item);
+void dec_zone_page_state(struct page *, enum zone_stat_item);
+
+extern void inc_zone_state(struct zone *, enum zone_stat_item);
+
+void refresh_cpu_vm_stats(int);
+void refresh_vm_stats(void);
+
+#else /* CONFIG_SMP */
+
+/*
+ * We do not maintain differentials in a single processor configuration.
+ * The functions directly modify the zone and global counters.
+ */
+static inline void __mod_zone_page_state(struct zone *zone,
+			enum zone_stat_item item, int delta)
+{
+	zone_page_state_add(delta, zone, item);
+}
+
+static inline void __inc_zone_page_state(struct page *page,
+			enum zone_stat_item item)
+{
+	atomic_long_inc(&page_zone(page)->vm_stat[item]);
+	atomic_long_inc(&vm_stat[item]);
+}
+
+static inline void __dec_zone_page_state(struct page *page,
+			enum zone_stat_item item)
+{
+	atomic_long_dec(&page_zone(page)->vm_stat[item]);
+	atomic_long_dec(&vm_stat[item]);
+}
+
+/*
+ * We only use atomic operations to update counters. So there is no need to
+ * disable interrupts.
+ */
+#define inc_zone_page_state __inc_zone_page_state
+#define dec_zone_page_state __dec_zone_page_state
+#define mod_zone_page_state __mod_zone_page_state
+
+static inline void refresh_cpu_vm_stats(int cpu) { }
+static inline void refresh_vm_stats(void) { }
+#endif
+
+#endif /* _LINUX_VMSTAT_H */

commit f6ac2354d791195ca40822b84d73d48a4e8b7f2b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:32 2006 -0700

    [PATCH] zoned vm counters: create vmstat.c/.h from page_alloc.c/.h
    
    NOTE: ZVC are *not* the lightweight event counters.  ZVCs are reliable whereas
    event counters do not need to be.
    
    Zone based VM statistics are necessary to be able to determine what the state
    of memory in one zone is.  In a NUMA system this can be helpful for local
    reclaim and other memory optimizations that may be able to shift VM load in
    order to get more balanced memory use.
    
    It is also useful to know how the computing load affects the memory
    allocations on various zones.  This patchset allows the retrieval of that data
    from userspace.
    
    The patchset introduces a framework for counters that is a cross between the
    existing page_stats --which are simply global counters split per cpu-- and the
    approach of deferred incremental updates implemented for nr_pagecache.
    
    Small per cpu 8 bit counters are added to struct zone.  If the counter exceeds
    certain thresholds then the counters are accumulated in an array of
    atomic_long in the zone and in a global array that sums up all zone values.
    The small 8 bit counters are next to the per cpu page pointers and so they
    will be in high in the cpu cache when pages are allocated and freed.
    
    Access to VM counter information for a zone and for the whole machine is then
    possible by simply indexing an array (Thanks to Nick Piggin for pointing out
    that approach).  The access to the total number of pages of various types does
    no longer require the summing up of all per cpu counters.
    
    Benefits of this patchset right now:
    
    - Ability for UP and SMP configuration to determine how memory
      is balanced between the DMA, NORMAL and HIGHMEM zones.
    
    - loops over all processors are avoided in writeback and
      reclaim paths. We can avoid caching the writeback information
      because the needed information is directly accessible.
    
    - Special handling for nr_pagecache removed.
    
    - zone_reclaim_interval vanishes since VM stats can now determine
      when it is worth to do local reclaim.
    
    - Fast inline per node page state determination.
    
    - Accurate counters in /sys/devices/system/node/node*/meminfo. Current
      counters are counting simply which processor allocated a page somewhere
      and guestimate based on that. So the counters were not useful to show
      the actual distribution of page use on a specific zone.
    
    - The swap_prefetch patch requires per node statistics in order to
      figure out when processors of a node can prefetch. This patch provides
      some of the needed numbers.
    
    - Detailed VM counters available in more /proc and /sys status files.
    
    References to earlier discussions:
    V1 http://marc.theaimsgroup.com/?l=linux-kernel&m=113511649910826&w=2
    V2 http://marc.theaimsgroup.com/?l=linux-kernel&m=114980851924230&w=2
    V3 http://marc.theaimsgroup.com/?l=linux-kernel&m=115014697910351&w=2
    V4 http://marc.theaimsgroup.com/?l=linux-kernel&m=115024767318740&w=2
    
    Performance tests with AIM7 did not show any regressions.  Seems to be a tad
    faster even.  Tested on ia64/NUMA.  Builds fine on i386, SMP / UP.  Includes
    fixes for s390/arm/uml arch code.
    
    This patch:
    
    Move counter code from page_alloc.c/page-flags.h to vmstat.c/h.
    
    Create vmstat.c/vmstat.h by separating the counter code and the proc
    functions.
    
    Move the vm_stat_text array before zoneinfo_show.
    
    [akpm@osdl.org: s390 build fix]
    [akpm@osdl.org: HOTPLUG_CPU build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
new file mode 100644
index 000000000000..3ca0c1989fc2
--- /dev/null
+++ b/include/linux/vmstat.h
@@ -0,0 +1,138 @@
+#ifndef _LINUX_VMSTAT_H
+#define _LINUX_VMSTAT_H
+
+#include <linux/types.h>
+#include <linux/percpu.h>
+
+/*
+ * Global page accounting.  One instance per CPU.  Only unsigned longs are
+ * allowed.
+ *
+ * - Fields can be modified with xxx_page_state and xxx_page_state_zone at
+ * any time safely (which protects the instance from modification by
+ * interrupt.
+ * - The __xxx_page_state variants can be used safely when interrupts are
+ * disabled.
+ * - The __xxx_page_state variants can be used if the field is only
+ * modified from process context and protected from preemption, or only
+ * modified from interrupt context.  In this case, the field should be
+ * commented here.
+ */
+struct page_state {
+	unsigned long nr_dirty;		/* Dirty writeable pages */
+	unsigned long nr_writeback;	/* Pages under writeback */
+	unsigned long nr_unstable;	/* NFS unstable pages */
+	unsigned long nr_page_table_pages;/* Pages used for pagetables */
+	unsigned long nr_mapped;	/* mapped into pagetables.
+					 * only modified from process context */
+	unsigned long nr_slab;		/* In slab */
+#define GET_PAGE_STATE_LAST nr_slab
+
+	/*
+	 * The below are zeroed by get_page_state().  Use get_full_page_state()
+	 * to add up all these.
+	 */
+	unsigned long pgpgin;		/* Disk reads */
+	unsigned long pgpgout;		/* Disk writes */
+	unsigned long pswpin;		/* swap reads */
+	unsigned long pswpout;		/* swap writes */
+
+	unsigned long pgalloc_high;	/* page allocations */
+	unsigned long pgalloc_normal;
+	unsigned long pgalloc_dma32;
+	unsigned long pgalloc_dma;
+
+	unsigned long pgfree;		/* page freeings */
+	unsigned long pgactivate;	/* pages moved inactive->active */
+	unsigned long pgdeactivate;	/* pages moved active->inactive */
+
+	unsigned long pgfault;		/* faults (major+minor) */
+	unsigned long pgmajfault;	/* faults (major only) */
+
+	unsigned long pgrefill_high;	/* inspected in refill_inactive_zone */
+	unsigned long pgrefill_normal;
+	unsigned long pgrefill_dma32;
+	unsigned long pgrefill_dma;
+
+	unsigned long pgsteal_high;	/* total highmem pages reclaimed */
+	unsigned long pgsteal_normal;
+	unsigned long pgsteal_dma32;
+	unsigned long pgsteal_dma;
+
+	unsigned long pgscan_kswapd_high;/* total highmem pages scanned */
+	unsigned long pgscan_kswapd_normal;
+	unsigned long pgscan_kswapd_dma32;
+	unsigned long pgscan_kswapd_dma;
+
+	unsigned long pgscan_direct_high;/* total highmem pages scanned */
+	unsigned long pgscan_direct_normal;
+	unsigned long pgscan_direct_dma32;
+	unsigned long pgscan_direct_dma;
+
+	unsigned long pginodesteal;	/* pages reclaimed via inode freeing */
+	unsigned long slabs_scanned;	/* slab objects scanned */
+	unsigned long kswapd_steal;	/* pages reclaimed by kswapd */
+	unsigned long kswapd_inodesteal;/* reclaimed via kswapd inode freeing */
+	unsigned long pageoutrun;	/* kswapd's calls to page reclaim */
+	unsigned long allocstall;	/* direct reclaim calls */
+
+	unsigned long pgrotated;	/* pages rotated to tail of the LRU */
+	unsigned long nr_bounce;	/* pages for bounce buffers */
+};
+
+extern void get_page_state(struct page_state *ret);
+extern void get_page_state_node(struct page_state *ret, int node);
+extern void get_full_page_state(struct page_state *ret);
+extern unsigned long read_page_state_offset(unsigned long offset);
+extern void mod_page_state_offset(unsigned long offset, unsigned long delta);
+extern void __mod_page_state_offset(unsigned long offset, unsigned long delta);
+
+#define read_page_state(member) \
+	read_page_state_offset(offsetof(struct page_state, member))
+
+#define mod_page_state(member, delta)	\
+	mod_page_state_offset(offsetof(struct page_state, member), (delta))
+
+#define __mod_page_state(member, delta)	\
+	__mod_page_state_offset(offsetof(struct page_state, member), (delta))
+
+#define inc_page_state(member)		mod_page_state(member, 1UL)
+#define dec_page_state(member)		mod_page_state(member, 0UL - 1)
+#define add_page_state(member,delta)	mod_page_state(member, (delta))
+#define sub_page_state(member,delta)	mod_page_state(member, 0UL - (delta))
+
+#define __inc_page_state(member)	__mod_page_state(member, 1UL)
+#define __dec_page_state(member)	__mod_page_state(member, 0UL - 1)
+#define __add_page_state(member,delta)	__mod_page_state(member, (delta))
+#define __sub_page_state(member,delta)	__mod_page_state(member, 0UL - (delta))
+
+#define page_state(member) (*__page_state(offsetof(struct page_state, member)))
+
+#define state_zone_offset(zone, member)					\
+({									\
+	unsigned offset;						\
+	if (is_highmem(zone))						\
+		offset = offsetof(struct page_state, member##_high);	\
+	else if (is_normal(zone))					\
+		offset = offsetof(struct page_state, member##_normal);	\
+	else if (is_dma32(zone))					\
+		offset = offsetof(struct page_state, member##_dma32);	\
+	else								\
+		offset = offsetof(struct page_state, member##_dma);	\
+	offset;								\
+})
+
+#define __mod_page_state_zone(zone, member, delta)			\
+ do {									\
+	__mod_page_state_offset(state_zone_offset(zone, member), (delta)); \
+ } while (0)
+
+#define mod_page_state_zone(zone, member, delta)			\
+ do {									\
+	mod_page_state_offset(state_zone_offset(zone, member), (delta)); \
+ } while (0)
+
+DECLARE_PER_CPU(struct page_state, page_states);
+
+#endif /* _LINUX_VMSTAT_H */
+
