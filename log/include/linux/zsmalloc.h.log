commit 8b136018da7bf49b988a24064fc45c290baffd93
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:50:53 2020 -0700

    mm: rename CONFIG_PGTABLE_MAPPING to CONFIG_ZSMALLOC_PGTABLE_MAPPING
    
    Rename the Kconfig variable to clarify the scope.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-11-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 2219cce81ca4..0fdbf653b173 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -20,7 +20,7 @@
  * zsmalloc mapping modes
  *
  * NOTE: These only make a difference when a mapped object spans pages.
- * They also have no effect when PGTABLE_MAPPING is selected.
+ * They also have no effect when ZSMALLOC_PGTABLE_MAPPING is selected.
  */
 enum zs_mapmode {
 	ZS_MM_RW, /* normal read-write mapping */

commit 010b495e2fa32353d0ef6aa70a8169e5ef617a15
Author: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
Date:   Thu Apr 5 16:24:43 2018 -0700

    zsmalloc: introduce zs_huge_class_size()
    
    Patch series "zsmalloc/zram: drop zram's max_zpage_size", v3.
    
    ZRAM's max_zpage_size is a bad thing.  It forces zsmalloc to store
    normal objects as huge ones, which results in bigger zsmalloc memory
    usage.  Drop it and use actual zsmalloc huge-class value when decide if
    the object is huge or not.
    
    This patch (of 2):
    
    Not every object can be share its zspage with other objects, e.g.  when
    the object is as big as zspage or nearly as big a zspage.  For such
    objects zsmalloc has a so called huge class - every object which belongs
    to huge class consumes the entire zspage (which consists of a physical
    page).  On x86_64, PAGE_SHIFT 12 box, the first non-huge class size is
    3264, so starting down from size 3264, objects can share page(-s) and
    thus minimize memory wastage.
    
    ZRAM, however, has its own statically defined watermark for huge
    objects, namely "3 * PAGE_SIZE / 4 = 3072", and forcibly stores every
    object larger than this watermark (3072) as a PAGE_SIZE object, in other
    words, to a huge class, while zsmalloc can keep some of those objects in
    non-huge classes.  This results in increased memory consumption.
    
    zsmalloc knows better if the object is huge or not.  Introduce
    zs_huge_class_size() function which tells if the given object can be
    stored in one of non-huge classes or not.  This will let us to drop
    ZRAM's huge object watermark and fully rely on zsmalloc when we decide
    if the object is huge.
    
    [sergey.senozhatsky.work@gmail.com: add pool param to zs_huge_class_size()]
      Link: http://lkml.kernel.org/r/20180314081833.1096-2-sergey.senozhatsky@gmail.com
    Link: http://lkml.kernel.org/r/20180306070639.7389-2-sergey.senozhatsky@gmail.com
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 57a8e98f2708..2219cce81ca4 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -47,6 +47,8 @@ void zs_destroy_pool(struct zs_pool *pool);
 unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t flags);
 void zs_free(struct zs_pool *pool, unsigned long obj);
 
+size_t zs_huge_class_size(struct zs_pool *pool);
+
 void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 			enum zs_mapmode mm);
 void zs_unmap_object(struct zs_pool *pool, unsigned long handle);

commit d0d8da2dc49dfdfe1d788eaf4d55eb5d4964d926
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Fri May 20 16:59:48 2016 -0700

    zsmalloc: require GFP in zs_malloc()
    
    Pass GFP flags to zs_malloc() instead of using a fixed mask supplied to
    zs_create_pool(), so we can be more flexible, but, more importantly, we
    need this to switch zram to per-cpu compression streams -- zram will try
    to allocate handle with preemption disabled in a fast path and switch to
    a slow path (using different gfp mask) if the fast one has failed.
    
    Apart from that, this also align zs_malloc() interface with zspool/zbud.
    
    [sergey.senozhatsky@gmail.com: pass GFP flags to zs_malloc() instead of using a fixed mask]
      Link: http://lkml.kernel.org/r/20160429150942.GA637@swordfish
    Link: http://lkml.kernel.org/r/20160429150942.GA637@swordfish
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 34eb16098a33..57a8e98f2708 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -41,10 +41,10 @@ struct zs_pool_stats {
 
 struct zs_pool;
 
-struct zs_pool *zs_create_pool(const char *name, gfp_t flags);
+struct zs_pool *zs_create_pool(const char *name);
 void zs_destroy_pool(struct zs_pool *pool);
 
-unsigned long zs_malloc(struct zs_pool *pool, size_t size);
+unsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t flags);
 void zs_free(struct zs_pool *pool, unsigned long obj);
 
 void *zs_map_object(struct zs_pool *pool, unsigned long handle,

commit 6f3526d6db7cbe8b53e42d6bf0cad2072afcf3fe
Author: Sergey SENOZHATSKY <sergey.senozhatsky@gmail.com>
Date:   Fri Nov 6 16:29:21 2015 -0800

    mm: zsmalloc: constify struct zs_pool name
    
    Constify `struct zs_pool' ->name.
    
    [akpm@inux-foundation.org: constify zpool_create_pool()'s `type' arg also]
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Dan Streetman <ddstreet@ieee.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 6398dfae53f1..34eb16098a33 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -41,7 +41,7 @@ struct zs_pool_stats {
 
 struct zs_pool;
 
-struct zs_pool *zs_create_pool(char *name, gfp_t flags);
+struct zs_pool *zs_create_pool(const char *name, gfp_t flags);
 void zs_destroy_pool(struct zs_pool *pool);
 
 unsigned long zs_malloc(struct zs_pool *pool, size_t size);

commit 860c707dca155a56dfa115ddd6c00959296144a6
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Sep 8 15:04:38 2015 -0700

    zsmalloc: account the number of compacted pages
    
    Compaction returns back to zram the number of migrated objects, which is
    quite uninformative -- we have objects of different sizes so user space
    cannot obtain any valuable data from that number.  Change compaction to
    operate in terms of pages and return back to compaction issuer the
    number of pages that were freed during compaction.  So from now on we
    will export more meaningful value in zram<id>/mm_stat -- the number of
    freed (compacted) pages.
    
    This requires:
     (a) a rename of `num_migrated' to 'pages_compacted'
     (b) a internal API change -- return first_page's fullness_group from
         putback_zspage(), so we know when putback_zspage() did
         free_zspage().  It helps us to account compaction stats correctly.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index ad3d23239043..6398dfae53f1 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -35,8 +35,8 @@ enum zs_mapmode {
 };
 
 struct zs_pool_stats {
-	/* How many objects were migrated */
-	unsigned long num_migrated;
+	/* How many pages were migrated (freed) */
+	unsigned long pages_compacted;
 };
 
 struct zs_pool;

commit 7d3f3938236b4bb878214e6791e76fd8409bdeee
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Sep 8 15:04:35 2015 -0700

    zsmalloc/zram: introduce zs_pool_stats api
    
    `zs_compact_control' accounts the number of migrated objects but it has
    a limited lifespan -- we lose it as soon as zs_compaction() returns back
    to zram.  It worked fine, because (a) zram had it's own counter of
    migrated objects and (b) only zram could trigger compaction.  However,
    this does not work for automatic pool compaction (not issued by zram).
    To account objects migrated during auto-compaction (issued by the
    shrinker) we need to store this number in zs_pool.
    
    Define a new `struct zs_pool_stats' structure to keep zs_pool's stats
    there.  It provides only `num_migrated', as of this writing, but it
    surely can be extended.
    
    A new zsmalloc zs_pool_stats() symbol exports zs_pool's stats back to
    caller.
    
    Use zs_pool_stats() in zram and remove `num_migrated' from zram_stats.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Suggested-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 1338190b5478..ad3d23239043 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -34,6 +34,11 @@ enum zs_mapmode {
 	 */
 };
 
+struct zs_pool_stats {
+	/* How many objects were migrated */
+	unsigned long num_migrated;
+};
+
 struct zs_pool;
 
 struct zs_pool *zs_create_pool(char *name, gfp_t flags);
@@ -49,4 +54,5 @@ void zs_unmap_object(struct zs_pool *pool, unsigned long handle);
 unsigned long zs_get_total_pages(struct zs_pool *pool);
 unsigned long zs_compact(struct zs_pool *pool);
 
+void zs_pool_stats(struct zs_pool *pool, struct zs_pool_stats *stats);
 #endif

commit 312fcae227037619dc858c9ccd362c7b847730a2
Author: Minchan Kim <minchan@kernel.org>
Date:   Wed Apr 15 16:15:30 2015 -0700

    zsmalloc: support compaction
    
    This patch provides core functions for migration of zsmalloc.  Migraion
    policy is simple as follows.
    
    for each size class {
            while {
                    src_page = get zs_page from ZS_ALMOST_EMPTY
                    if (!src_page)
                            break;
                    dst_page = get zs_page from ZS_ALMOST_FULL
                    if (!dst_page)
                            dst_page = get zs_page from ZS_ALMOST_EMPTY
                    if (!dst_page)
                            break;
                    migrate(from src_page, to dst_page);
            }
    }
    
    For migration, we need to identify which objects in zspage are allocated
    to migrate them out.  We could know it by iterating of freed objects in a
    zspage because first_page of zspage keeps free objects singly-linked list
    but it's not efficient.  Instead, this patch adds a tag(ie,
    OBJ_ALLOCATED_TAG) in header of each object(ie, handle) so we could check
    whether the object is allocated easily.
    
    This patch adds another status bit in handle to synchronize between user
    access through zs_map_object and migration.  During migration, we cannot
    move objects user are using due to data coherency between old object and
    new object.
    
    [akpm@linux-foundation.org: zsmalloc.c needs sched.h for cond_resched()]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Juneho Choi <juno.choi@lge.com>
    Cc: Gunho Lee <gunho.lee@lge.com>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: Seth Jennings <sjennings@variantweb.net>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 3283c6a55425..1338190b5478 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -47,5 +47,6 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 void zs_unmap_object(struct zs_pool *pool, unsigned long handle);
 
 unsigned long zs_get_total_pages(struct zs_pool *pool);
+unsigned long zs_compact(struct zs_pool *pool);
 
 #endif

commit 3eba0c6a56c04f2b017b43641a821f1ebfb7fb4c
Author: Ganesh Mahendran <opensource.ganesh@gmail.com>
Date:   Thu Feb 12 15:00:51 2015 -0800

    mm/zpool: add name argument to create zpool
    
    Currently the underlay of zpool: zsmalloc/zbud, do not know who creates
    them.  There is not a method to let zsmalloc/zbud find which caller they
    belong to.
    
    Now we want to add statistics collection in zsmalloc.  We need to name the
    debugfs dir for each pool created.  The way suggested by Minchan Kim is to
    use a name passed by caller(such as zram) to create the zsmalloc pool.
    
        /sys/kernel/debug/zsmalloc/zram0
    
    This patch adds an argument `name' to zs_create_pool() and other related
    functions.
    
    Signed-off-by: Ganesh Mahendran <opensource.ganesh@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Seth Jennings <sjennings@variantweb.net>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index 05c214760977..3283c6a55425 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -36,7 +36,7 @@ enum zs_mapmode {
 
 struct zs_pool;
 
-struct zs_pool *zs_create_pool(gfp_t flags);
+struct zs_pool *zs_create_pool(char *name, gfp_t flags);
 void zs_destroy_pool(struct zs_pool *pool);
 
 unsigned long zs_malloc(struct zs_pool *pool, size_t size);

commit 722cdc17232f0f684011407f7cf3c40d39457971
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Oct 9 15:29:50 2014 -0700

    zsmalloc: change return value unit of zs_get_total_size_bytes
    
    zs_get_total_size_bytes returns a amount of memory zsmalloc consumed with
    *byte unit* but zsmalloc operates *page unit* rather than byte unit so
    let's change the API so benefit we could get is that reduce unnecessary
    overhead (ie, change page unit with byte unit) in zsmalloc.
    
    Since return type is pages, "zs_get_total_pages" is better than
    "zs_get_total_size_bytes".
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reviewed-by: Dan Streetman <ddstreet@ieee.org>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: <juno.choi@lge.com>
    Cc: <seungho1.park@lge.com>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Seth Jennings <sjennings@variantweb.net>
    Cc: David Horner <ds2horner@gmail.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index e44d634e7fb7..05c214760977 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -46,6 +46,6 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 			enum zs_mapmode mm);
 void zs_unmap_object(struct zs_pool *pool, unsigned long handle);
 
-u64 zs_get_total_size_bytes(struct zs_pool *pool);
+unsigned long zs_get_total_pages(struct zs_pool *pool);
 
 #endif

commit 31fc00bb788ffde7d8d861d8b2bba798ab445992
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:55 2014 -0800

    zsmalloc: add copyright
    
    Add my copyright to the zsmalloc source code which I maintain.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
index c2eb174b97ee..e44d634e7fb7 100644
--- a/include/linux/zsmalloc.h
+++ b/include/linux/zsmalloc.h
@@ -2,6 +2,7 @@
  * zsmalloc memory allocator
  *
  * Copyright (C) 2011  Nitin Gupta
+ * Copyright (C) 2012, 2013 Minchan Kim
  *
  * This code is released using a dual license strategy: BSD/GPL
  * You can choose the license that better fits your requirements.

commit bcf1647d0899666f0fb90d176abf63bae22abb7c
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 30 15:45:50 2014 -0800

    zsmalloc: move it under mm
    
    This patch moves zsmalloc under mm directory.
    
    Before that, description will explain why we have needed custom
    allocator.
    
    Zsmalloc is a new slab-based memory allocator for storing compressed
    pages.  It is designed for low fragmentation and high allocation success
    rate on large object, but <= PAGE_SIZE allocations.
    
    zsmalloc differs from the kernel slab allocator in two primary ways to
    achieve these design goals.
    
    zsmalloc never requires high order page allocations to back slabs, or
    "size classes" in zsmalloc terms.  Instead it allows multiple
    single-order pages to be stitched together into a "zspage" which backs
    the slab.  This allows for higher allocation success rate under memory
    pressure.
    
    Also, zsmalloc allows objects to span page boundaries within the zspage.
    This allows for lower fragmentation than could be had with the kernel
    slab allocator for objects between PAGE_SIZE/2 and PAGE_SIZE.  With the
    kernel slab allocator, if a page compresses to 60% of it original size,
    the memory savings gained through compression is lost in fragmentation
    because another object of the same size can't be stored in the leftover
    space.
    
    This ability to span pages results in zsmalloc allocations not being
    directly addressable by the user.  The user is given an
    non-dereferencable handle in response to an allocation request.  That
    handle must be mapped, using zs_map_object(), which returns a pointer to
    the mapped region that can be used.  The mapping is necessary since the
    object data may reside in two different noncontigious pages.
    
    The zsmalloc fulfills the allocation needs for zram perfectly
    
    [sjenning@linux.vnet.ibm.com: borrow Seth's quote]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Nitin Gupta <ngupta@vflare.org>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Luigi Semenzato <semenzato@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Seth Jennings <sjenning@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/zsmalloc.h b/include/linux/zsmalloc.h
new file mode 100644
index 000000000000..c2eb174b97ee
--- /dev/null
+++ b/include/linux/zsmalloc.h
@@ -0,0 +1,50 @@
+/*
+ * zsmalloc memory allocator
+ *
+ * Copyright (C) 2011  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the license that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ */
+
+#ifndef _ZS_MALLOC_H_
+#define _ZS_MALLOC_H_
+
+#include <linux/types.h>
+
+/*
+ * zsmalloc mapping modes
+ *
+ * NOTE: These only make a difference when a mapped object spans pages.
+ * They also have no effect when PGTABLE_MAPPING is selected.
+ */
+enum zs_mapmode {
+	ZS_MM_RW, /* normal read-write mapping */
+	ZS_MM_RO, /* read-only (no copy-out at unmap time) */
+	ZS_MM_WO /* write-only (no copy-in at map time) */
+	/*
+	 * NOTE: ZS_MM_WO should only be used for initializing new
+	 * (uninitialized) allocations.  Partial writes to already
+	 * initialized allocations should use ZS_MM_RW to preserve the
+	 * existing data.
+	 */
+};
+
+struct zs_pool;
+
+struct zs_pool *zs_create_pool(gfp_t flags);
+void zs_destroy_pool(struct zs_pool *pool);
+
+unsigned long zs_malloc(struct zs_pool *pool, size_t size);
+void zs_free(struct zs_pool *pool, unsigned long obj);
+
+void *zs_map_object(struct zs_pool *pool, unsigned long handle,
+			enum zs_mapmode mm);
+void zs_unmap_object(struct zs_pool *pool, unsigned long handle);
+
+u64 zs_get_total_size_bytes(struct zs_pool *pool);
+
+#endif
