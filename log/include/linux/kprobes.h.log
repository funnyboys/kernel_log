commit 8b6ddd10d678bebec32381f71b6b420bafc43ad0
Merge: eede2b9b3fe0 026bb845b0ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 20 13:17:47 2020 -0700

    Merge tag 'trace-v5.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing fixes from Steven Rostedt:
    
     - Have recordmcount work with > 64K sections (to support LTO)
    
     - kprobe RCU fixes
    
     - Correct a kprobe critical section with missing mutex
    
     - Remove redundant arch_disarm_kprobe() call
    
     - Fix lockup when kretprobe triggers within kprobe_flush_task()
    
     - Fix memory leak in fetch_op_data operations
    
     - Fix sleep in atomic in ftrace trace array sample code
    
     - Free up memory on failure in sample trace array code
    
     - Fix incorrect reporting of function_graph fields in format file
    
     - Fix quote within quote parsing in bootconfig
    
     - Fix return value of bootconfig tool
    
     - Add testcases for bootconfig tool
    
     - Fix maybe uninitialized warning in ftrace pid file code
    
     - Remove unused variable in tracing_iter_reset()
    
     - Fix some typos
    
    * tag 'trace-v5.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ftrace: Fix maybe-uninitialized compiler warning
      tools/bootconfig: Add testcase for show-command and quotes test
      tools/bootconfig: Fix to return 0 if succeeded to show the bootconfig
      tools/bootconfig: Fix to use correct quotes for value
      proc/bootconfig: Fix to use correct quotes for value
      tracing: Remove unused event variable in tracing_iter_reset
      tracing/probe: Fix memleak in fetch_op_data operations
      trace: Fix typo in allocate_ftrace_ops()'s comment
      tracing: Make ftrace packed events have align of 1
      sample-trace-array: Remove trace_array 'sample-instance'
      sample-trace-array: Fix sleeping function called from invalid context
      kretprobe: Prevent triggering kretprobe from within kprobe_flush_task
      kprobes: Remove redundant arch_disarm_kprobe() call
      kprobes: Fix to protect kick_kprobe_optimizer() by kprobe_mutex
      kprobes: Use non RCU traversal APIs on kprobe_tables if possible
      kprobes: Suppress the suspicious RCU warning on kprobes
      recordmcount: support >64k sections

commit 9b38cc704e844e41d9cf74e647bff1d249512cb3
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Tue May 12 17:03:18 2020 +0900

    kretprobe: Prevent triggering kretprobe from within kprobe_flush_task
    
    Ziqian reported lockup when adding retprobe on _raw_spin_lock_irqsave.
    My test was also able to trigger lockdep output:
    
     ============================================
     WARNING: possible recursive locking detected
     5.6.0-rc6+ #6 Not tainted
     --------------------------------------------
     sched-messaging/2767 is trying to acquire lock:
     ffffffff9a492798 (&(kretprobe_table_locks[i].lock)){-.-.}, at: kretprobe_hash_lock+0x52/0xa0
    
     but task is already holding lock:
     ffffffff9a491a18 (&(kretprobe_table_locks[i].lock)){-.-.}, at: kretprobe_trampoline+0x0/0x50
    
     other info that might help us debug this:
      Possible unsafe locking scenario:
    
            CPU0
            ----
       lock(&(kretprobe_table_locks[i].lock));
       lock(&(kretprobe_table_locks[i].lock));
    
      *** DEADLOCK ***
    
      May be due to missing lock nesting notation
    
     1 lock held by sched-messaging/2767:
      #0: ffffffff9a491a18 (&(kretprobe_table_locks[i].lock)){-.-.}, at: kretprobe_trampoline+0x0/0x50
    
     stack backtrace:
     CPU: 3 PID: 2767 Comm: sched-messaging Not tainted 5.6.0-rc6+ #6
     Call Trace:
      dump_stack+0x96/0xe0
      __lock_acquire.cold.57+0x173/0x2b7
      ? native_queued_spin_lock_slowpath+0x42b/0x9e0
      ? lockdep_hardirqs_on+0x590/0x590
      ? __lock_acquire+0xf63/0x4030
      lock_acquire+0x15a/0x3d0
      ? kretprobe_hash_lock+0x52/0xa0
      _raw_spin_lock_irqsave+0x36/0x70
      ? kretprobe_hash_lock+0x52/0xa0
      kretprobe_hash_lock+0x52/0xa0
      trampoline_handler+0xf8/0x940
      ? kprobe_fault_handler+0x380/0x380
      ? find_held_lock+0x3a/0x1c0
      kretprobe_trampoline+0x25/0x50
      ? lock_acquired+0x392/0xbc0
      ? _raw_spin_lock_irqsave+0x50/0x70
      ? __get_valid_kprobe+0x1f0/0x1f0
      ? _raw_spin_unlock_irqrestore+0x3b/0x40
      ? finish_task_switch+0x4b9/0x6d0
      ? __switch_to_asm+0x34/0x70
      ? __switch_to_asm+0x40/0x70
    
    The code within the kretprobe handler checks for probe reentrancy,
    so we won't trigger any _raw_spin_lock_irqsave probe in there.
    
    The problem is in outside kprobe_flush_task, where we call:
    
      kprobe_flush_task
        kretprobe_table_lock
          raw_spin_lock_irqsave
            _raw_spin_lock_irqsave
    
    where _raw_spin_lock_irqsave triggers the kretprobe and installs
    kretprobe_trampoline handler on _raw_spin_lock_irqsave return.
    
    The kretprobe_trampoline handler is then executed with already
    locked kretprobe_table_locks, and first thing it does is to
    lock kretprobe_table_locks ;-) the whole lockup path like:
    
      kprobe_flush_task
        kretprobe_table_lock
          raw_spin_lock_irqsave
            _raw_spin_lock_irqsave ---> probe triggered, kretprobe_trampoline installed
    
            ---> kretprobe_table_locks locked
    
            kretprobe_trampoline
              trampoline_handler
                kretprobe_hash_lock(current, &head, &flags);  <--- deadlock
    
    Adding kprobe_busy_begin/end helpers that mark code with fake
    probe installed to prevent triggering of another kprobe within
    this code.
    
    Using these helpers in kprobe_flush_task, so the probe recursion
    protection check is hit and the probe is never set to prevent
    above lockup.
    
    Link: http://lkml.kernel.org/r/158927059835.27680.7011202830041561604.stgit@devnote2
    
    Fixes: ef53d9c5e4da ("kprobes: improve kretprobe scalability with hashed locking")
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "Gustavo A . R . Silva" <gustavoars@kernel.org>
    Cc: Anders Roxell <anders.roxell@linaro.org>
    Cc: "Naveen N . Rao" <naveen.n.rao@linux.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Reported-by: "Ziqian SUN (Zamir)" <zsun@redhat.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 594265bfd390..05ed663e6c7b 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -350,6 +350,10 @@ static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
 	return this_cpu_ptr(&kprobe_ctlblk);
 }
 
+extern struct kprobe kprobe_busy;
+void kprobe_busy_begin(void);
+void kprobe_busy_end(void);
+
 kprobe_opcode_t *kprobe_lookup_name(const char *name, unsigned int offset);
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);

commit 67a862a94d5d55fbfde6942c81f9e51f943f452c
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Thu May 28 09:35:11 2020 -0500

    kprobes: Replace zero-length array with flexible-array
    
    There is a regular need in the kernel to provide a way to declare having a
    dynamically sized set of trailing elements in a structure. Kernel code should
    always use “flexible array members”[1] for these cases. The older style of
    one-element or zero-length arrays should no longer be used[2].
    
    [1] https://en.wikipedia.org/wiki/Flexible_array_member
    [2] https://github.com/KSPP/linux/issues/21
    
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 594265bfd390..e210af75ee38 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -161,7 +161,7 @@ struct kretprobe_instance {
 	kprobe_opcode_t *ret_addr;
 	struct task_struct *task;
 	void *fp;
-	char data[0];
+	char data[];
 };
 
 struct kretprobe_blackpoint {

commit 32927393dc1ccd60fb2bdc05b9e8e88753761469
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 24 08:43:38 2020 +0200

    sysctl: pass kernel pointers to ->proc_handler
    
    Instead of having all the sysctl handlers deal with user pointers, which
    is rather hairy in terms of the BPF interaction, copy the input to and
    from  userspace in common code.  This also means that the strings are
    always NUL-terminated by the common code, making the API a little bit
    safer.
    
    As most handler just pass through the data to one of the common handlers
    a lot of the changes are mechnical.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 04bdaf01112c..594265bfd390 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -312,7 +312,7 @@ DEFINE_INSN_CACHE_OPS(optinsn);
 #ifdef CONFIG_SYSCTL
 extern int sysctl_kprobes_optimization;
 extern int proc_kprobes_optimization_handler(struct ctl_table *table,
-					     int write, void __user *buffer,
+					     int write, void *buffer,
 					     size_t *length, loff_t *ppos);
 #endif
 extern void wait_for_kprobe_optimizer(void);

commit b98cca444d287a63dd96df04af7fb9793567599e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Jul 16 16:28:00 2019 -0700

    mm, kprobes: generalize and rename notify_page_fault() as kprobe_page_fault()
    
    Architectures which support kprobes have very similar boilerplate around
    calling kprobe_fault_handler().  Use a helper function in kprobes.h to
    unify them, based on the x86 code.
    
    This changes the behaviour for other architectures when preemption is
    enabled.  Previously, they would have disabled preemption while calling
    the kprobe handler.  However, preemption would be disabled if this fault
    was due to a kprobe, so we know the fault was not due to a kprobe
    handler and can simply return failure.
    
    This behaviour was introduced in commit a980c0ef9f6d ("x86/kprobes:
    Refactor kprobes_fault() like kprobe_exceptions_notify()")
    
    [anshuman.khandual@arm.com: export kprobe_fault_handler()]
      Link: http://lkml.kernel.org/r/1561133358-8876-1-git-send-email-anshuman.khandual@arm.com
    Link: http://lkml.kernel.org/r/1560420444-25737-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 443d9800ca3f..04bdaf01112c 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -458,4 +458,23 @@ static inline bool is_kprobe_optinsn_slot(unsigned long addr)
 }
 #endif
 
+/* Returns true if kprobes handled the fault */
+static nokprobe_inline bool kprobe_page_fault(struct pt_regs *regs,
+					      unsigned int trap)
+{
+	if (!kprobes_built_in())
+		return false;
+	if (user_mode(regs))
+		return false;
+	/*
+	 * To be potentially processing a kprobe fault and to be allowed
+	 * to call kprobe_running(), we have to be non-preemptible.
+	 */
+	if (preemptible())
+		return false;
+	if (!kprobe_running())
+		return false;
+	return kprobe_fault_handler(regs, trap);
+}
+
 #endif /* _LINUX_KPROBES_H */

commit 1a59d1b8e05ea6ab45f7e18897de1ef0e6bc3da6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:05 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 156
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not write to the free software foundation inc
      59 temple place suite 330 boston ma 02111 1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 1334 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070033.113240726@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 9a897256e481..443d9800ca3f 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -1,23 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 #ifndef _LINUX_KPROBES_H
 #define _LINUX_KPROBES_H
 /*
  *  Kernel Probes (KProbes)
  *  include/linux/kprobes.h
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
- *
  * Copyright (C) IBM Corporation, 2002, 2004
  *
  * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel

commit 3ff9c075cc767b3060bdac12da72fc94dd7da1b8
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sun Feb 24 01:49:52 2019 +0900

    x86/kprobes: Verify stack frame on kretprobe
    
    Verify the stack frame pointer on kretprobe trampoline handler,
    If the stack frame pointer does not match, it skips the wrong
    entry and tries to find correct one.
    
    This can happen if user puts the kretprobe on the function
    which can be used in the path of ftrace user-function call.
    Such functions should not be probed, so this adds a warning
    message that reports which function should be blacklisted.
    
    Tested-by: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/155094059185.6137.15527904013362842072.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 201f0f2683f2..9a897256e481 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -173,6 +173,7 @@ struct kretprobe_instance {
 	struct kretprobe *rp;
 	kprobe_opcode_t *ret_addr;
 	struct task_struct *task;
+	void *fp;
 	char data[0];
 };
 

commit fab940755d1d78377901450b6ee7c77356e06821
Author: Borislav Petkov <bp@suse.de>
Date:   Sun Jan 27 14:03:57 2019 +0100

    x86/hw_breakpoints, kprobes: Remove kprobes ifdeffery
    
    Remove the ifdeffery in the breakpoint parsing arch_build_bp_info() by
    adding a within_kprobe_blacklist() stub for the !CONFIG_KPROBES case.
    
    It is returning true when kprobes are not enabled to mean that any
    address is within the kprobes blacklist on such kernels and thus not
    allow kernel breakpoints on non-kprobes kernels.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Frederic Weisbecker <frederic@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "Naveen N. Rao" <naveen.n.rao@linux.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190127131237.4557-1-bp@alien8.de

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e07e91daaacc..201f0f2683f2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -442,6 +442,11 @@ static inline int enable_kprobe(struct kprobe *kp)
 {
 	return -ENOSYS;
 }
+
+static inline bool within_kprobe_blacklist(unsigned long addr)
+{
+	return true;
+}
 #endif /* CONFIG_KPROBES */
 static inline int disable_kretprobe(struct kretprobe *rp)
 {

commit 312a466155108329c458049dc76a21ad56106960
Merge: 6e54df001ac9 4b1bacab61aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 17:03:51 2018 -0800

    Merge branch 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cleanups from Ingo Molnar:
     "Misc cleanups"
    
    * 'x86-cleanups-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/kprobes: Remove trampoline_handler() prototype
      x86/kernel: Fix more -Wmissing-prototypes warnings
      x86: Fix various typos in comments
      x86/headers: Fix -Wmissing-prototypes warning
      x86/process: Avoid unnecessary NULL check in get_wchan()
      x86/traps: Complete prototype declarations
      x86/mce: Fix -Wmissing-prototypes warnings
      x86/gart: Rewrite early_gart_iommu_check() comment

commit fb1a59fae8baa3f3c69b72a87ff94fc4fa5683ec
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Mon Dec 17 17:20:55 2018 +0900

    kprobes: Blacklist symbols in arch-defined prohibited area
    
    Blacklist symbols in arch-defined probe-prohibited areas.
    With this change, user can see all symbols which are prohibited
    to probe in debugfs.
    
    All archtectures which have custom prohibit areas should define
    its own arch_populate_kprobe_blacklist() function, but unless that,
    all symbols marked __kprobes are blacklisted.
    
    Reported-by: Andrea Righi <righi.andrea@gmail.com>
    Tested-by: Andrea Righi <righi.andrea@gmail.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yonghong Song <yhs@fb.com>
    Link: http://lkml.kernel.org/r/154503485491.26176.15823229545155174796.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e909413e4e38..5da8a1de2187 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -242,10 +242,13 @@ extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 extern bool arch_within_kprobe_blacklist(unsigned long addr);
+extern int arch_populate_kprobe_blacklist(void);
 extern bool arch_kprobe_on_func_entry(unsigned long offset);
 extern bool kprobe_on_func_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset);
 
 extern bool within_kprobe_blacklist(unsigned long addr);
+extern int kprobe_add_ksym_blacklist(unsigned long entry);
+extern int kprobe_add_area_blacklist(unsigned long start, unsigned long end);
 
 struct kprobe_insn_cache {
 	struct mutex mutex;

commit ad3bc25a320742f42b3015115384f5aec69c7ce2
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Dec 5 00:34:56 2018 +0100

    x86/kernel: Fix more -Wmissing-prototypes warnings
    
    ... with the goal of eventually enabling -Wmissing-prototypes by
    default. At least on x86.
    
    Make functions static where possible, otherwise add prototypes or make
    them visible through includes.
    
    asm/trace/ changes courtesy of Steven Rostedt <rostedt@goodmis.org>.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com> # ACPI + cpufreq bits
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Dimitri Sivanich <dimitri.sivanich@hpe.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: "Steven Rostedt (VMware)" <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Cc: linux-acpi@vger.kernel.org

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e909413e4e38..e64b26c81c2f 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -379,6 +379,9 @@ int enable_kprobe(struct kprobe *kp);
 
 void dump_kprobe(struct kprobe *kp);
 
+void *alloc_insn_page(void);
+void free_insn_page(void *page);
+
 #else /* !CONFIG_KPROBES: */
 
 static inline int kprobes_built_in(void)

commit 4de58696de076d9bd2745d1cbe0930635c3f5ac9
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed Jun 20 01:17:15 2018 +0900

    kprobes: Remove jprobe stub API
    
    Remove jprobe stub APIs from linux/kprobes.h since
    the jprobe implementation was completely gone.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linux-arch@vger.kernel.org
    Link: https://lore.kernel.org/lkml/152942503572.15209.1652552217914694917.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index b520baa65682..e909413e4e38 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -63,7 +63,6 @@ struct pt_regs;
 struct kretprobe;
 struct kretprobe_instance;
 typedef int (*kprobe_pre_handler_t) (struct kprobe *, struct pt_regs *);
-typedef int (*kprobe_break_handler_t) (struct kprobe *, struct pt_regs *);
 typedef void (*kprobe_post_handler_t) (struct kprobe *, struct pt_regs *,
 				       unsigned long flags);
 typedef int (*kprobe_fault_handler_t) (struct kprobe *, struct pt_regs *,
@@ -101,12 +100,6 @@ struct kprobe {
 	 */
 	kprobe_fault_handler_t fault_handler;
 
-	/*
-	 * ... called if breakpoint trap occurs in probe handler.
-	 * Return 1 if it handled break, otherwise kernel will see it.
-	 */
-	kprobe_break_handler_t break_handler;
-
 	/* Saved opcode (which has been replaced with breakpoint) */
 	kprobe_opcode_t opcode;
 
@@ -154,24 +147,6 @@ static inline int kprobe_ftrace(struct kprobe *p)
 	return p->flags & KPROBE_FLAG_FTRACE;
 }
 
-/*
- * Special probe type that uses setjmp-longjmp type tricks to resume
- * execution at a specified entry with a matching prototype corresponding
- * to the probed function - a trick to enable arguments to become
- * accessible seamlessly by probe handling logic.
- * Note:
- * Because of the way compilers allocate stack space for local variables
- * etc upfront, regardless of sub-scopes within a function, this mirroring
- * principle currently works only for probes placed on function entry points.
- */
-struct jprobe {
-	struct kprobe kp;
-	void *entry;	/* probe handling code to jump to */
-};
-
-/* For backward compatibility with old code using JPROBE_ENTRY() */
-#define JPROBE_ENTRY(handler)	(handler)
-
 /*
  * Function-return probe -
  * Note:
@@ -436,9 +411,6 @@ static inline void unregister_kprobe(struct kprobe *p)
 static inline void unregister_kprobes(struct kprobe **kps, int num)
 {
 }
-static inline void jprobe_return(void)
-{
-}
 static inline int register_kretprobe(struct kretprobe *rp)
 {
 	return -ENOSYS;
@@ -465,20 +437,6 @@ static inline int enable_kprobe(struct kprobe *kp)
 	return -ENOSYS;
 }
 #endif /* CONFIG_KPROBES */
-static inline int register_jprobe(struct jprobe *p)
-{
-	return -ENOSYS;
-}
-static inline int register_jprobes(struct jprobe **jps, int num)
-{
-	return -ENOSYS;
-}
-static inline void unregister_jprobe(struct jprobe *p)
-{
-}
-static inline void unregister_jprobes(struct jprobe **jps, int num)
-{
-}
 static inline int disable_kretprobe(struct kretprobe *rp)
 {
 	return disable_kprobe(&rp->kp);
@@ -487,14 +445,6 @@ static inline int enable_kretprobe(struct kretprobe *rp)
 {
 	return enable_kprobe(&rp->kp);
 }
-static inline int disable_jprobe(struct jprobe *jp)
-{
-	return -ENOSYS;
-}
-static inline int enable_jprobe(struct jprobe *jp)
-{
-	return -ENOSYS;
-}
 
 #ifndef CONFIG_KPROBES
 static inline bool is_kprobe_insn_slot(unsigned long addr)

commit 5a6cf77f5e35e7af35d36a1e7dc21a42f6412e4f
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed Jun 20 01:05:07 2018 +0900

    kprobes: Remove jprobe API implementation
    
    Remove functionally empty jprobe API implementations and test cases.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linux-arch@vger.kernel.org
    Link: https://lore.kernel.org/lkml/152942430705.15209.2307050500995264322.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 9440a2fc8893..b520baa65682 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -389,9 +389,6 @@ int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
 int register_kprobes(struct kprobe **kps, int num);
 void unregister_kprobes(struct kprobe **kps, int num);
-int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
-int longjmp_break_handler(struct kprobe *, struct pt_regs *);
-void jprobe_return(void);
 unsigned long arch_deref_entry_point(void *);
 
 int register_kretprobe(struct kretprobe *rp);

commit fcdfafcb73be8fa45909327bbddca46fb362a675
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Nov 13 07:59:10 2017 +0100

    kprobes: Don't spam the build log with deprecation warnings
    
    The jprobes APIs are deprecated - but are still in occasional use for code that
    few people seem to care about, so stop generating deprecation warnings.
    
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 56b2e698dbad..9440a2fc8893 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -468,18 +468,18 @@ static inline int enable_kprobe(struct kprobe *kp)
 	return -ENOSYS;
 }
 #endif /* CONFIG_KPROBES */
-static inline int __deprecated register_jprobe(struct jprobe *p)
+static inline int register_jprobe(struct jprobe *p)
 {
 	return -ENOSYS;
 }
-static inline int __deprecated register_jprobes(struct jprobe **jps, int num)
+static inline int register_jprobes(struct jprobe **jps, int num)
 {
 	return -ENOSYS;
 }
-static inline void __deprecated unregister_jprobe(struct jprobe *p)
+static inline void unregister_jprobe(struct jprobe *p)
 {
 }
-static inline void __deprecated unregister_jprobes(struct jprobe **jps, int num)
+static inline void unregister_jprobes(struct jprobe **jps, int num)
 {
 }
 static inline int disable_kretprobe(struct kretprobe *rp)
@@ -490,11 +490,11 @@ static inline int enable_kretprobe(struct kretprobe *rp)
 {
 	return enable_kprobe(&rp->kp);
 }
-static inline int __deprecated disable_jprobe(struct jprobe *jp)
+static inline int disable_jprobe(struct jprobe *jp)
 {
 	return -ENOSYS;
 }
-static inline int __deprecated enable_jprobe(struct jprobe *jp)
+static inline int enable_jprobe(struct jprobe *jp)
 {
 	return -ENOSYS;
 }

commit 590c845930457d25d27dc1fdd964a1ce18ef2d7d
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Fri Oct 6 08:14:37 2017 +0900

    kprobes: Disable the jprobes APIs
    
    Disable the jprobes APIs and comment out the jprobes API function
    code. This is in preparation of removing all jprobes related
    code (including kprobe's break_handler).
    
    Nowadays ftrace and other tracing features are mature enough
    to replace jprobes use-cases. Users can safely use ftrace and
    perf probe etc. for their use cases.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S . Miller <davem@davemloft.net>
    Cc: Ian McDonald <ian.mcdonald@jandi.co.nz>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Hemminger <stephen@networkplumber.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlad Yasevich <vyasevich@gmail.com>
    Link: http://lkml.kernel.org/r/150724527741.5014.15465541485637899227.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index bd2684700b74..56b2e698dbad 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -391,10 +391,6 @@ int register_kprobes(struct kprobe **kps, int num);
 void unregister_kprobes(struct kprobe **kps, int num);
 int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
 int longjmp_break_handler(struct kprobe *, struct pt_regs *);
-int register_jprobe(struct jprobe *p);
-void unregister_jprobe(struct jprobe *p);
-int register_jprobes(struct jprobe **jps, int num);
-void unregister_jprobes(struct jprobe **jps, int num);
 void jprobe_return(void);
 unsigned long arch_deref_entry_point(void *);
 
@@ -443,20 +439,6 @@ static inline void unregister_kprobe(struct kprobe *p)
 static inline void unregister_kprobes(struct kprobe **kps, int num)
 {
 }
-static inline int register_jprobe(struct jprobe *p)
-{
-	return -ENOSYS;
-}
-static inline int register_jprobes(struct jprobe **jps, int num)
-{
-	return -ENOSYS;
-}
-static inline void unregister_jprobe(struct jprobe *p)
-{
-}
-static inline void unregister_jprobes(struct jprobe **jps, int num)
-{
-}
 static inline void jprobe_return(void)
 {
 }
@@ -486,6 +468,20 @@ static inline int enable_kprobe(struct kprobe *kp)
 	return -ENOSYS;
 }
 #endif /* CONFIG_KPROBES */
+static inline int __deprecated register_jprobe(struct jprobe *p)
+{
+	return -ENOSYS;
+}
+static inline int __deprecated register_jprobes(struct jprobe **jps, int num)
+{
+	return -ENOSYS;
+}
+static inline void __deprecated unregister_jprobe(struct jprobe *p)
+{
+}
+static inline void __deprecated unregister_jprobes(struct jprobe **jps, int num)
+{
+}
 static inline int disable_kretprobe(struct kretprobe *rp)
 {
 	return disable_kprobe(&rp->kp);
@@ -494,13 +490,13 @@ static inline int enable_kretprobe(struct kretprobe *rp)
 {
 	return enable_kprobe(&rp->kp);
 }
-static inline int disable_jprobe(struct jprobe *jp)
+static inline int __deprecated disable_jprobe(struct jprobe *jp)
 {
-	return disable_kprobe(&jp->kp);
+	return -ENOSYS;
 }
-static inline int enable_jprobe(struct jprobe *jp)
+static inline int __deprecated enable_jprobe(struct jprobe *jp)
 {
-	return enable_kprobe(&jp->kp);
+	return -ENOSYS;
 }
 
 #ifndef CONFIG_KPROBES

commit 659b957f20c78fd470083c80af5e79eedfb39e5b
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Fri Jul 7 22:37:24 2017 +0530

    kprobes: Rename [arch_]function_offset_within_entry() to [arch_]kprobe_on_func_entry()
    
    Rename function_offset_within_entry() to scope it to kprobe namespace by
    using kprobe_ prefix, and to also simplify it.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Suggested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/3aa6c7e2e4fb6e00f3c24fa306496a66edb558ea.1499443367.git.naveen.n.rao@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 541df0b5b815..bd2684700b74 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -267,8 +267,8 @@ extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 extern bool arch_within_kprobe_blacklist(unsigned long addr);
-extern bool arch_function_offset_within_entry(unsigned long offset);
-extern bool function_offset_within_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset);
+extern bool arch_kprobe_on_func_entry(unsigned long offset);
+extern bool kprobe_on_func_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset);
 
 extern bool within_kprobe_blacklist(unsigned long addr);
 

commit 30e7d894c1478c88d50ce94ddcdbd7f9763d9cdd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 17 10:19:49 2017 +0200

    tracing/kprobes: Enforce kprobes teardown after testing
    
    Enabling the tracer selftest triggers occasionally the warning in
    text_poke(), which warns when the to be modified page is not marked
    reserved.
    
    The reason is that the tracer selftest installs kprobes on functions marked
    __init for testing. These probes are removed after the tests, but that
    removal schedules the delayed kprobes_optimizer work, which will do the
    actual text poke. If the work is executed after the init text is freed,
    then the warning triggers. The bug can be reproduced reliably when the work
    delay is increased.
    
    Flush the optimizer work and wait for the optimizing/unoptimizing lists to
    become empty before returning from the kprobes tracer selftest. That
    ensures that all operations which were queued due to the probes removal
    have completed.
    
    Link: http://lkml.kernel.org/r/20170516094802.76a468bb@gandalf.local.home
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: stable@vger.kernel.org
    Fixes: 6274de498 ("kprobes: Support delayed unoptimizing")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 30f90c1a0aaf..541df0b5b815 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -349,6 +349,9 @@ extern int proc_kprobes_optimization_handler(struct ctl_table *table,
 					     int write, void __user *buffer,
 					     size_t *length, loff_t *ppos);
 #endif
+extern void wait_for_kprobe_optimizer(void);
+#else
+static inline void wait_for_kprobe_optimizer(void) { }
 #endif /* CONFIG_OPTPROBES */
 #ifdef CONFIG_KPROBES_ON_FTRACE
 extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,

commit 7246f60068840847bdcf595be5f0b5ca632736e0
Merge: e579dde654fc 700b7eadd562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 11:36:44 2017 -0700

    Merge tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Larger virtual address space on 64-bit server CPUs. By default we
         use a 128TB virtual address space, but a process can request access
         to the full 512TB by passing a hint to mmap().
    
       - Support for the new Power9 "XIVE" interrupt controller.
    
       - TLB flushing optimisations for the radix MMU on Power9.
    
       - Support for CAPI cards on Power9, using the "Coherent Accelerator
         Interface Architecture 2.0".
    
       - The ability to configure the mmap randomisation limits at build and
         runtime.
    
       - Several small fixes and cleanups to the kprobes code, as well as
         support for KPROBES_ON_FTRACE.
    
       - Major improvements to handling of system reset interrupts,
         correctly treating them as NMIs, giving them a dedicated stack and
         using a new hypervisor call to trigger them, all of which should
         aid debugging and robustness.
    
       - Many fixes and other minor enhancements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Alistair Popple,
      Andrew Donnellan, Aneesh Kumar K.V, Anshuman Khandual, Anton
      Blanchard, Balbir Singh, Ben Hutchings, Benjamin Herrenschmidt,
      Bhupesh Sharma, Chris Packham, Christian Zigotzky, Christophe Leroy,
      Christophe Lombard, Daniel Axtens, David Gibson, Gautham R. Shenoy,
      Gavin Shan, Geert Uytterhoeven, Guilherme G. Piccoli, Hamish Martin,
      Hari Bathini, Kees Cook, Laurent Dufour, Madhavan Srinivasan, Mahesh J
      Salgaonkar, Mahesh Salgaonkar, Masami Hiramatsu, Matt Brown, Matthew
      R. Ochs, Michael Neuling, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Pan Xinhui, Paul Mackerras, Rashmica Gupta, Russell
      Currey, Sukadev Bhattiprolu, Thadeu Lima de Souza Cascardo, Tobin C.
      Harding, Tyrel Datwyler, Uma Krishnan, Vaibhav Jain, Vipin K Parashar,
      Yang Shi"
    
    * tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (214 commits)
      powerpc/64s: Power9 has no LPCR[VRMASD] field so don't set it
      powerpc/powernv: Fix TCE kill on NVLink2
      powerpc/mm/radix: Drop support for CPUs without lockless tlbie
      powerpc/book3s/mce: Move add_taint() later in virtual mode
      powerpc/sysfs: Move #ifdef CONFIG_HOTPLUG_CPU out of the function body
      powerpc/smp: Document irq enable/disable after migrating IRQs
      powerpc/mpc52xx: Don't select user-visible RTAS_PROC
      powerpc/powernv: Document cxl dependency on special case in pnv_eeh_reset()
      powerpc/eeh: Clean up and document event handling functions
      powerpc/eeh: Avoid use after free in eeh_handle_special_event()
      cxl: Mask slice error interrupts after first occurrence
      cxl: Route eeh events to all drivers in cxl_pci_error_detected()
      cxl: Force context lock during EEH flow
      powerpc/64: Allow CONFIG_RELOCATABLE if COMPILE_TEST
      powerpc/xmon: Teach xmon oops about radix vectors
      powerpc/mm/hash: Fix off-by-one in comment about kernel contexts ids
      powerpc/pseries: Enable VFIO
      powerpc/powernv: Fix iommu table size calculation hook for small tables
      powerpc/powernv: Check kzalloc() return value in pnv_pci_table_alloc
      powerpc: Add arch/powerpc/tools directory
      ...

commit 290e3070762ac80e5fc4087d8c4de7e3f1d90aca
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Wed Apr 19 18:21:01 2017 +0530

    powerpc/kprobes: Fix handling of function offsets on ABIv2
    
    commit 239aeba76409 ("perf powerpc: Fix kprobe and kretprobe handling with
    kallsyms on ppc64le") changed how we use the offset field in struct kprobe on
    ABIv2. perf now offsets from the global entry point if an offset is specified
    and otherwise chooses the local entry point.
    
    Fix the same in kernel for kprobe API users. We do this by extending
    kprobe_lookup_name() to accept an additional parameter to indicate the offset
    specified with the kprobe registration. If offset is 0, we return the local
    function entry and return the global entry point otherwise.
    
    With:
      # cd /sys/kernel/debug/tracing/
      # echo "p _do_fork" >> kprobe_events
      # echo "p _do_fork+0x10" >> kprobe_events
    
    before this patch:
      # cat ../kprobes/list
      c0000000000d0748  k  _do_fork+0x8    [DISABLED]
      c0000000000d0758  k  _do_fork+0x18    [DISABLED]
      c0000000000412b0  k  kretprobe_trampoline+0x0    [OPTIMIZED]
    
    and after:
      # cat ../kprobes/list
      c0000000000d04c8  k  _do_fork+0x8    [DISABLED]
      c0000000000d04d0  k  _do_fork+0x10    [DISABLED]
      c0000000000412b0  k  kretprobe_trampoline+0x0    [OPTIMIZED]
    
    Acked-by: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 16f153c84646..1f82a3db00b1 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -379,7 +379,7 @@ static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
 	return this_cpu_ptr(&kprobe_ctlblk);
 }
 
-kprobe_opcode_t *kprobe_lookup_name(const char *name);
+kprobe_opcode_t *kprobe_lookup_name(const char *name, unsigned int offset);
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
 int register_kprobes(struct kprobe **kps, int num);

commit 49e0b4658fe6aab5bf6bfe0738a86c1895930ad1
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Wed Apr 19 18:21:00 2017 +0530

    kprobes: Convert kprobe_lookup_name() to a function
    
    The macro is now pretty long and ugly on powerpc. In the light of further
    changes needed here, convert it to a __weak variant to be over-ridden with a
    nicer looking function.
    
    Suggested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index c328e4f7dcad..16f153c84646 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -379,6 +379,7 @@ static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
 	return this_cpu_ptr(&kprobe_ctlblk);
 }
 
+kprobe_opcode_t *kprobe_lookup_name(const char *name);
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
 int register_kprobes(struct kprobe **kps, int num);

commit 1d585e70905e03e8c19c9aaf523ec246ae6b18a1
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Wed Mar 8 13:56:06 2017 +0530

    trace/kprobes: Fix check for kretprobe offset within function entry
    
    perf specifies an offset from _text and since this offset is fed
    directly into the arch-specific helper, kprobes tracer rejects
    installation of kretprobes through perf. Fix this by looking up the
    actual offset from a function for the specified sym+offset.
    
    Refactor and reuse existing routines to limit code duplication -- we
    repurpose kprobe_addr() for determining final kprobe address and we
    split out the function entry offset determination into a separate
    generic helper.
    
    Before patch:
    
      naveen@ubuntu:~/linux/tools/perf$ sudo ./perf probe -v do_open%return
      probe-definition(0): do_open%return
      symbol:do_open file:(null) line:0 offset:0 return:1 lazy:(null)
      0 arguments
      Looking at the vmlinux_path (8 entries long)
      Using /boot/vmlinux for symbols
      Open Debuginfo file: /boot/vmlinux
      Try to find probe point from debuginfo.
      Matched function: do_open [2d0c7ff]
      Probe point found: do_open+0
      Matched function: do_open [35d76dc]
      found inline addr: 0xc0000000004ba9c4
      Failed to find "do_open%return",
       because do_open is an inlined function and has no return point.
      An error occurred in debuginfo analysis (-22).
      Trying to use symbols.
      Opening /sys/kernel/debug/tracing//README write=0
      Opening /sys/kernel/debug/tracing//kprobe_events write=1
      Writing event: r:probe/do_open _text+4469776
      Failed to write event: Invalid argument
        Error: Failed to add events. Reason: Invalid argument (Code: -22)
      naveen@ubuntu:~/linux/tools/perf$ dmesg | tail
      <snip>
      [   33.568656] Given offset is not valid for return probe.
    
    After patch:
    
      naveen@ubuntu:~/linux/tools/perf$ sudo ./perf probe -v do_open%return
      probe-definition(0): do_open%return
      symbol:do_open file:(null) line:0 offset:0 return:1 lazy:(null)
      0 arguments
      Looking at the vmlinux_path (8 entries long)
      Using /boot/vmlinux for symbols
      Open Debuginfo file: /boot/vmlinux
      Try to find probe point from debuginfo.
      Matched function: do_open [2d0c7d6]
      Probe point found: do_open+0
      Matched function: do_open [35d76b3]
      found inline addr: 0xc0000000004ba9e4
      Failed to find "do_open%return",
       because do_open is an inlined function and has no return point.
      An error occurred in debuginfo analysis (-22).
      Trying to use symbols.
      Opening /sys/kernel/debug/tracing//README write=0
      Opening /sys/kernel/debug/tracing//kprobe_events write=1
      Writing event: r:probe/do_open _text+4469808
      Writing event: r:probe/do_open_1 _text+4956344
      Added new events:
        probe:do_open        (on do_open%return)
        probe:do_open_1      (on do_open%return)
    
      You can now use it in all perf tools, such as:
    
              perf record -e probe:do_open_1 -aR sleep 1
    
      naveen@ubuntu:~/linux/tools/perf$ sudo cat /sys/kernel/debug/kprobes/list
      c000000000041370  k  kretprobe_trampoline+0x0    [OPTIMIZED]
      c0000000004ba0b8  r  do_open+0x8    [DISABLED]
      c000000000443430  r  do_open+0x0    [DISABLED]
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/d8cd1ef420ec22e3643ac332fdabcffc77319a42.1488961018.git.naveen.n.rao@linux.vnet.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 177bdf6c6aeb..47e4da5b4fa2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -268,6 +268,7 @@ extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 extern bool arch_within_kprobe_blacklist(unsigned long addr);
 extern bool arch_function_offset_within_entry(unsigned long offset);
+extern bool function_offset_within_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset);
 
 extern bool within_kprobe_blacklist(unsigned long addr);
 

commit 90ec5e89e393c76e19afc845d8f88a5dc8315919
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Wed Feb 22 19:23:37 2017 +0530

    kretprobes: Ensure probe location is at function entry
    
    kretprobes can be registered by specifying an absolute address or by
    specifying offset to a symbol. However, we need to ensure this falls at
    function entry so as to be able to determine the return address.
    
    Validate the same during kretprobe registration. By default, there
    should not be any offset from a function entry, as determined through a
    kallsyms_lookup(). Introduce arch_function_offset_within_entry() as a
    way for architectures to override this.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/f1583bc4839a3862cfc2acefcc56f9c8837fa2ba.1487770934.git.naveen.n.rao@linux.vnet.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index c328e4f7dcad..177bdf6c6aeb 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -267,6 +267,7 @@ extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 extern bool arch_within_kprobe_blacklist(unsigned long addr);
+extern bool arch_function_offset_within_entry(unsigned long offset);
 
 extern bool within_kprobe_blacklist(unsigned long addr);
 

commit 7d134b2ce639448199052fd573a324f7e7cd5ed8
Author: Luis R. Rodriguez <mcgrof@kernel.org>
Date:   Mon Feb 27 14:26:56 2017 -0800

    kprobes: move kprobe declarations to asm-generic/kprobes.h
    
    Often all is needed is these small helpers, instead of compiler.h or a
    full kprobes.h.  This is important for asm helpers, in fact even some
    asm/kprobes.h make use of these helpers...  instead just keep a generic
    asm file with helpers useful for asm code with the least amount of
    clutter as possible.
    
    Likewise we need now to also address what to do about this file for both
    when architectures have CONFIG_HAVE_KPROBES, and when they do not.  Then
    for when architectures have CONFIG_HAVE_KPROBES but have disabled
    CONFIG_KPROBES.
    
    Right now most asm/kprobes.h do not have guards against CONFIG_KPROBES,
    this means most architecture code cannot include asm/kprobes.h safely.
    Correct this and add guards for architectures missing them.
    Additionally provide architectures that not have kprobes support with
    the default asm-generic solution.  This lets us force asm/kprobes.h on
    the header include/linux/kprobes.h always, but most importantly we can
    now safely include just asm/kprobes.h on architecture code without
    bringing the full kitchen sink of header files.
    
    Two architectures already provided a guard against CONFIG_KPROBES on its
    kprobes.h: sh, arch.  The rest of the architectures needed gaurds added.
    We avoid including any not-needed headers on asm/kprobes.h unless
    kprobes have been enabled.
    
    In a subsequent atomic change we can try now to remove compiler.h from
    include/linux/kprobes.h.
    
    During this sweep I've also identified a few architectures defining a
    common macro needed for both kprobes and ftrace, that of the definition
    of the breakput instruction up.  Some refer to this as
    BREAKPOINT_INSTRUCTION.  This must be kept outside of the #ifdef
    CONFIG_KPROBES guard.
    
    [mcgrof@kernel.org: fix arm64 build]
      Link: http://lkml.kernel.org/r/CAB=NE6X1WMByuARS4mZ1g9+W=LuVBnMDnh_5zyN0CLADaVh=Jw@mail.gmail.com
    [sfr@canb.auug.org.au: fixup for kprobes declarations moving]
      Link: http://lkml.kernel.org/r/20170214165933.13ebd4f4@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170203233139.32682-1-mcgrof@kernel.org
    Signed-off-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 16ddfb8b304a..c328e4f7dcad 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -29,7 +29,7 @@
  *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
  *		<prasanna@in.ibm.com> added function-return probes.
  */
-#include <linux/compiler.h>	/* for __kprobes */
+#include <linux/compiler.h>
 #include <linux/linkage.h>
 #include <linux/list.h>
 #include <linux/notifier.h>
@@ -40,9 +40,9 @@
 #include <linux/rcupdate.h>
 #include <linux/mutex.h>
 #include <linux/ftrace.h>
+#include <asm/kprobes.h>
 
 #ifdef CONFIG_KPROBES
-#include <asm/kprobes.h>
 
 /* kprobe_status settings */
 #define KPROBE_HIT_ACTIVE	0x00000001
@@ -51,6 +51,7 @@
 #define KPROBE_HIT_SSDONE	0x00000008
 
 #else /* CONFIG_KPROBES */
+#include <asm-generic/kprobes.h>
 typedef int kprobe_opcode_t;
 struct arch_specific_insn {
 	int dummy;
@@ -509,18 +510,4 @@ static inline bool is_kprobe_optinsn_slot(unsigned long addr)
 }
 #endif
 
-#ifdef CONFIG_KPROBES
-/*
- * Blacklist ganerating macro. Specify functions which is not probed
- * by using this macro.
- */
-#define __NOKPROBE_SYMBOL(fname)			\
-static unsigned long __used				\
-	__attribute__((section("_kprobe_blacklist")))	\
-	_kbl_addr_##fname = (unsigned long)fname;
-#define NOKPROBE_SYMBOL(fname)	__NOKPROBE_SYMBOL(fname)
-#else
-#define NOKPROBE_SYMBOL(fname)
-#endif
-
 #endif /* _LINUX_KPROBES_H */

commit 5b485629ba0d5d027880769ff467c587b24b4bde
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sun Jan 8 23:58:09 2017 +0900

    kprobes, extable: Identify kprobes trampolines as kernel text area
    
    Improve __kernel_text_address()/kernel_text_address() to return
    true if the given address is on a kprobe's instruction slot
    trampoline.
    
    This can help stacktraces to determine the address is on a
    text area or not.
    
    To implement this atomically in is_kprobe_*_slot(), also change
    the insn_cache page list to an RCU list.
    
    This changes timings a bit (it delays page freeing to the RCU garbage
    collection phase), but none of that is in the hot path.
    
    Note: this change can add small overhead to stack unwinders because
    it adds 2 additional checks to __kernel_text_address(). However, the
    impact should be very small, because kprobe_insn_pages list has 1 entry
    per 256 probes(on x86, on arm/arm64 it will be 1024 probes),
    and kprobe_optinsn_pages has 1 entry per 32 probes(on x86).
    In most use cases, the number of kprobe events may be less
    than 20, which means that is_kprobe_*_slot() will check just one entry.
    
    Tested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/148388747896.6869.6354262871751682264.stgit@devbox
    [ Improved the changelog and coding style. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 8f6849084248..16ddfb8b304a 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -278,9 +278,13 @@ struct kprobe_insn_cache {
 	int nr_garbage;
 };
 
+#ifdef __ARCH_WANT_KPROBES_INSN_SLOT
 extern kprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c);
 extern void __free_insn_slot(struct kprobe_insn_cache *c,
 			     kprobe_opcode_t *slot, int dirty);
+/* sleep-less address checking routine  */
+extern bool __is_insn_slot_addr(struct kprobe_insn_cache *c,
+				unsigned long addr);
 
 #define DEFINE_INSN_CACHE_OPS(__name)					\
 extern struct kprobe_insn_cache kprobe_##__name##_slots;		\
@@ -294,6 +298,18 @@ static inline void free_##__name##_slot(kprobe_opcode_t *slot, int dirty)\
 {									\
 	__free_insn_slot(&kprobe_##__name##_slots, slot, dirty);	\
 }									\
+									\
+static inline bool is_kprobe_##__name##_slot(unsigned long addr)	\
+{									\
+	return __is_insn_slot_addr(&kprobe_##__name##_slots, addr);	\
+}
+#else /* __ARCH_WANT_KPROBES_INSN_SLOT */
+#define DEFINE_INSN_CACHE_OPS(__name)					\
+static inline bool is_kprobe_##__name##_slot(unsigned long addr)	\
+{									\
+	return 0;							\
+}
+#endif
 
 DEFINE_INSN_CACHE_OPS(insn);
 
@@ -330,7 +346,6 @@ extern int proc_kprobes_optimization_handler(struct ctl_table *table,
 					     int write, void __user *buffer,
 					     size_t *length, loff_t *ppos);
 #endif
-
 #endif /* CONFIG_OPTPROBES */
 #ifdef CONFIG_KPROBES_ON_FTRACE
 extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,
@@ -481,6 +496,19 @@ static inline int enable_jprobe(struct jprobe *jp)
 	return enable_kprobe(&jp->kp);
 }
 
+#ifndef CONFIG_KPROBES
+static inline bool is_kprobe_insn_slot(unsigned long addr)
+{
+	return false;
+}
+#endif
+#ifndef CONFIG_OPTPROBES
+static inline bool is_kprobe_optinsn_slot(unsigned long addr)
+{
+	return false;
+}
+#endif
+
 #ifdef CONFIG_KPROBES
 /*
  * Blacklist ganerating macro. Specify functions which is not probed

commit e5779e8e12299f77c2421a707855d8d124171d85
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 30 20:32:40 2015 -0700

    perf/x86/hw_breakpoints: Disallow kernel breakpoints unless kprobe-safe
    
    Code on the kprobe blacklist doesn't want unexpected int3
    exceptions. It probably doesn't want unexpected debug exceptions
    either. Be safe: disallow breakpoints in nokprobes code.
    
    On non-CONFIG_KPROBES kernels, there is no kprobe blacklist.  In
    that case, disallow kernel breakpoints entirely.
    
    It will be particularly important to keep hw breakpoints out of the
    entry and NMI code once we move debug exceptions off the IST stack.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e14b152af99640448d895e3c2a8c2d5ee19a1325.1438312874.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 1ab54754a86d..8f6849084248 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -267,6 +267,8 @@ extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 extern bool arch_within_kprobe_blacklist(unsigned long addr);
 
+extern bool within_kprobe_blacklist(unsigned long addr);
+
 struct kprobe_insn_cache {
 	struct mutex mutex;
 	void *(*alloc)(void);	/* allocate insn page */

commit cbf6ab52add20b845f903decc973afbd5463c527
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Mon Jan 5 19:29:32 2015 +0800

    kprobes: Pass the original kprobe for preparing optimized kprobe
    
    Pass the original kprobe for preparing an optimized kprobe arch-dep
    part, since for some architecture (e.g. ARM32) requires the information
    in original kprobe.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Jon Medhurst <tixy@linaro.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 5297f9fa0ef2..1ab54754a86d 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -308,7 +308,8 @@ struct optimized_kprobe {
 /* Architecture dependent functions for direct jump optimization */
 extern int arch_prepared_optinsn(struct arch_optimized_insn *optinsn);
 extern int arch_check_optimized_kprobe(struct optimized_kprobe *op);
-extern int arch_prepare_optimized_kprobe(struct optimized_kprobe *op);
+extern int arch_prepare_optimized_kprobe(struct optimized_kprobe *op,
+					 struct kprobe *orig);
 extern void arch_remove_optimized_kprobe(struct optimized_kprobe *op);
 extern void arch_optimize_kprobes(struct list_head *oplist);
 extern void arch_unoptimize_kprobes(struct list_head *oplist,

commit f7f242ff004499e0904d3664713dfba01f24c408
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Oct 15 12:17:34 2014 +0200

    kprobes: introduce weak arch_check_ftrace_location() helper function
    
    Introduce weak arch_check_ftrace_location() helper function which
    architectures can override in order to implement handling of kprobes
    on function tracer call sites on their own, without depending on
    common code or implementing the KPROBES_ON_FTRACE feature.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index f7296e57d614..5297f9fa0ef2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -335,6 +335,7 @@ extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,
 extern int arch_prepare_kprobe_ftrace(struct kprobe *p);
 #endif
 
+int arch_check_ftrace_location(struct kprobe *p);
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);

commit 3737a12761636ebde0f09ef49daebb8eed18cc8a
Merge: c29deef32e36 82b897782d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:18:49 2014 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more perf updates from Ingo Molnar:
     "A second round of perf updates:
    
       - wide reaching kprobes sanitization and robustization, with the hope
         of fixing all 'probe this function crashes the kernel' bugs, by
         Masami Hiramatsu.
    
       - uprobes updates from Oleg Nesterov: tmpfs support, corner case
         fixes and robustization work.
    
       - perf tooling updates and fixes from Jiri Olsa, Namhyung Ki, Arnaldo
         et al:
            * Add support to accumulate hist periods (Namhyung Kim)
            * various fixes, refactorings and enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      perf: Differentiate exec() and non-exec() comm events
      perf: Fix perf_event_comm() vs. exec() assumption
      uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates
      perf/documentation: Add description for conditional branch filter
      perf/x86: Add conditional branch filtering support
      perf/tool: Add conditional branch filter 'cond' to perf record
      perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
      uprobes: Teach copy_insn() to support tmpfs
      uprobes: Shift ->readpage check from __copy_insn() to uprobe_register()
      perf/x86: Use common PMU interrupt disabled code
      perf/ARM: Use common PMU interrupt disabled code
      perf: Disable sampled events if no PMU interrupt
      perf: Fix use after free in perf_remove_from_context()
      perf tools: Fix 'make help' message error
      perf record: Fix poll return value propagation
      perf tools: Move elide bool into perf_hpp_fmt struct
      perf tools: Remove elide setup for SORT_MODE__MEMORY mode
      perf tools: Fix "==" into "=" in ui_browser__warning assignment
      perf tools: Allow overriding sysfs and proc finding with env var
      perf tools: Consider header files outside perf directory in tags target
      ...

commit bdffd893a0e9c431304142d12d9a0a21d365c502
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Apr 29 14:17:40 2014 -0500

    tracing: Replace __get_cpu_var uses with this_cpu_ptr
    
    Replace uses of &__get_cpu_var for address calculation with this_cpu_ptr.
    
    Link: http://lkml.kernel.org/p/alpine.DEB.2.10.1404291415560.18364@gentwo.org
    
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 925eaf28fca9..7bd2ad01e39c 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -355,7 +355,7 @@ static inline void reset_current_kprobe(void)
 
 static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
 {
-	return (&__get_cpu_var(kprobe_ctlblk));
+	return this_cpu_ptr(&kprobe_ctlblk);
 }
 
 int register_kprobe(struct kprobe *p);

commit 376e242429bf8539ef39a080ac113c8799840b13
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:05 2014 +0900

    kprobes: Introduce NOKPROBE_SYMBOL() macro to maintain kprobes blacklist
    
    Introduce NOKPROBE_SYMBOL() macro which builds a kprobes
    blacklist at kernel build time.
    
    The usage of this macro is similar to EXPORT_SYMBOL(),
    placed after the function definition:
    
      NOKPROBE_SYMBOL(function);
    
    Since this macro will inhibit inlining of static/inline
    functions, this patch also introduces a nokprobe_inline macro
    for static/inline functions. In this case, we must use
    NOKPROBE_SYMBOL() for the inline function caller.
    
    When CONFIG_KPROBES=y, the macro stores the given function
    address in the "_kprobe_blacklist" section.
    
    Since the data structures are not fully initialized by the
    macro (because there is no "size" information),  those
    are re-initialized at boot time by using kallsyms.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Link: http://lkml.kernel.org/r/20140417081705.26341.96719.stgit@ltc230.yrl.intra.hitachi.co.jp
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jan-Simon Möller <dl9pf@gmx.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-sparse@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index cdf9251f8249..e059507c465d 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -205,10 +205,10 @@ struct kretprobe_blackpoint {
 	void *addr;
 };
 
-struct kprobe_blackpoint {
-	const char *name;
+struct kprobe_blacklist_entry {
+	struct list_head list;
 	unsigned long start_addr;
-	unsigned long range;
+	unsigned long end_addr;
 };
 
 #ifdef CONFIG_KPROBES
@@ -477,4 +477,18 @@ static inline int enable_jprobe(struct jprobe *jp)
 	return enable_kprobe(&jp->kp);
 }
 
+#ifdef CONFIG_KPROBES
+/*
+ * Blacklist ganerating macro. Specify functions which is not probed
+ * by using this macro.
+ */
+#define __NOKPROBE_SYMBOL(fname)			\
+static unsigned long __used				\
+	__attribute__((section("_kprobe_blacklist")))	\
+	_kbl_addr_##fname = (unsigned long)fname;
+#define NOKPROBE_SYMBOL(fname)	__NOKPROBE_SYMBOL(fname)
+#else
+#define NOKPROBE_SYMBOL(fname)
+#endif
+
 #endif /* _LINUX_KPROBES_H */

commit be8f274323c26ddc7e6fd6c44254b7abcdbe6389
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:16:58 2014 +0900

    kprobes: Prohibit probing on .entry.text code
    
    .entry.text is a code area which is used for interrupt/syscall
    entries, which includes many sensitive code.
    Thus, it is better to prohibit probing on all of such code
    instead of a part of that.
    Since some symbols are already registered on kprobe blacklist,
    this also removes them from the blacklist.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jan Kiszka <jan.kiszka@siemens.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Jonathan Lebon <jlebon@redhat.com>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Link: http://lkml.kernel.org/r/20140417081658.26341.57354.stgit@ltc230.yrl.intra.hitachi.co.jp
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 925eaf28fca9..cdf9251f8249 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -265,6 +265,7 @@ extern void arch_disarm_kprobe(struct kprobe *p);
 extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
+extern bool arch_within_kprobe_blacklist(unsigned long addr);
 
 struct kprobe_insn_cache {
 	struct mutex mutex;

commit af96397de8600232effbff43dc8b4ca20ddc02b1
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Sep 11 14:24:13 2013 -0700

    kprobes: allow to specify custom allocator for insn caches
    
    The current two insn slot caches both use module_alloc/module_free to
    allocate and free insn slot cache pages.
    
    For s390 this is not sufficient since there is the need to allocate insn
    slots that are either within the vmalloc module area or within dma memory.
    
    Therefore add a mechanism which allows to specify an own allocator for an
    own insn slot cache.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 077f65321b5e..925eaf28fca9 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -268,6 +268,8 @@ extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
 struct kprobe_insn_cache {
 	struct mutex mutex;
+	void *(*alloc)(void);	/* allocate insn page */
+	void (*free)(void *);	/* free insn page */
 	struct list_head pages; /* list of kprobe_insn_page */
 	size_t insn_size;	/* size of instruction slot */
 	int nr_garbage;

commit c802d64a356b5cf349121ac4c5e005f037ce548d
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Sep 11 14:24:11 2013 -0700

    kprobes: unify insn caches
    
    The current kpropes insn caches allocate memory areas for insn slots
    with module_alloc().  The assumption is that the kernel image and module
    area are both within the same +/- 2GB memory area.
    
    This however is not true for s390 where the kernel image resides within
    the first 2GB (DMA memory area), but the module area is far away in the
    vmalloc area, usually somewhere close below the 4TB area.
    
    For new pc relative instructions s390 needs insn slots that are within
    +/- 2GB of each area.  That way we can patch displacements of
    pc-relative instructions within the insn slots just like x86 and
    powerpc.
    
    The module area works already with the normal insn slot allocator,
    however there is currently no way to get insn slots that are within the
    first 2GB on s390 (aka DMA area).
    
    Therefore this patch set modifies the kprobes insn slot cache code in
    order to allow to specify a custom allocator for the insn slot cache
    pages.  In addition architecure can now have private insn slot caches
    withhout the need to modify common code.
    
    Patch 1 unifies and simplifies the current insn and optinsn caches
            implementation. This is a preparation which allows to add more
            insn caches in a simple way.
    
    Patch 2 adds the possibility to specify a custom allocator.
    
    Patch 3 makes s390 use the new insn slot mechanisms and adds support for
            pc-relative instructions with long displacements.
    
    This patch (of 3):
    
    The two insn caches (insn, and optinsn) each have an own mutex and
    alloc/free functions (get_[opt]insn_slot() / free_[opt]insn_slot()).
    
    Since there is the need for yet another insn cache which satifies dma
    allocations on s390, unify and simplify the current implementation:
    
    - Move the per insn cache mutex into struct kprobe_insn_cache.
    - Move the alloc/free functions to kprobe.h so they are simply
      wrappers for the generic __get_insn_slot/__free_insn_slot functions.
      The implementation is done with a DEFINE_INSN_CACHE_OPS() macro
      which provides the alloc/free functions for each cache if needed.
    - move the struct kprobe_insn_cache to kprobe.h which allows to generate
      architecture specific insn slot caches outside of the core kprobes
      code.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index ca1d27a0d6a6..077f65321b5e 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -264,10 +264,34 @@ extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
 extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
-extern kprobe_opcode_t *get_insn_slot(void);
-extern void free_insn_slot(kprobe_opcode_t *slot, int dirty);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
+struct kprobe_insn_cache {
+	struct mutex mutex;
+	struct list_head pages; /* list of kprobe_insn_page */
+	size_t insn_size;	/* size of instruction slot */
+	int nr_garbage;
+};
+
+extern kprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c);
+extern void __free_insn_slot(struct kprobe_insn_cache *c,
+			     kprobe_opcode_t *slot, int dirty);
+
+#define DEFINE_INSN_CACHE_OPS(__name)					\
+extern struct kprobe_insn_cache kprobe_##__name##_slots;		\
+									\
+static inline kprobe_opcode_t *get_##__name##_slot(void)		\
+{									\
+	return __get_insn_slot(&kprobe_##__name##_slots);		\
+}									\
+									\
+static inline void free_##__name##_slot(kprobe_opcode_t *slot, int dirty)\
+{									\
+	__free_insn_slot(&kprobe_##__name##_slots, slot, dirty);	\
+}									\
+
+DEFINE_INSN_CACHE_OPS(insn);
+
 #ifdef CONFIG_OPTPROBES
 /*
  * Internal structure for direct jump optimized probe
@@ -287,13 +311,13 @@ extern void arch_optimize_kprobes(struct list_head *oplist);
 extern void arch_unoptimize_kprobes(struct list_head *oplist,
 				    struct list_head *done_list);
 extern void arch_unoptimize_kprobe(struct optimized_kprobe *op);
-extern kprobe_opcode_t *get_optinsn_slot(void);
-extern void free_optinsn_slot(kprobe_opcode_t *slot, int dirty);
 extern int arch_within_optimized_kprobe(struct optimized_kprobe *op,
 					unsigned long addr);
 
 extern void opt_pre_handler(struct kprobe *p, struct pt_regs *regs);
 
+DEFINE_INSN_CACHE_OPS(optinsn);
+
 #ifdef CONFIG_SYSCTL
 extern int sysctl_kprobes_optimization;
 extern int proc_kprobes_optimization_handler(struct ctl_table *table,

commit 324670b620ab1ed00ba160e435686bd2ae4a59ce
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 4 19:40:50 2013 +0900

    kprobes: Move __kprobes definition into compiler.h
    
    Currently, __kprobes is defined in linux/kprobes.h which
    is too big to be included in small or basic headers
    that want to make use of this simple attribute.
    
    So move __kprobes definition into linux/compiler.h
    in which other compiler attributes are defined.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Timo Juhani Lindfors <timo.lindfors@iki.fi>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Nadia Yvette Chambers <nyc@holomorphy.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20130404104049.21071.20908.stgit@mhiramat-M0-7522
    [ Improved the attribute explanation a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 4b6ef4d33cc2..ca1d27a0d6a6 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -29,6 +29,7 @@
  *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
  *		<prasanna@in.ibm.com> added function-return probes.
  */
+#include <linux/compiler.h>	/* for __kprobes */
 #include <linux/linkage.h>
 #include <linux/list.h>
 #include <linux/notifier.h>
@@ -49,16 +50,11 @@
 #define KPROBE_REENTER		0x00000004
 #define KPROBE_HIT_SSDONE	0x00000008
 
-/* Attach to insert probes on any functions which should be ignored*/
-#define __kprobes	__attribute__((__section__(".kprobes.text")))
-
 #else /* CONFIG_KPROBES */
 typedef int kprobe_opcode_t;
 struct arch_specific_insn {
 	int dummy;
 };
-#define __kprobes
-
 #endif /* CONFIG_KPROBES */
 
 struct kprobe;

commit e7dbfe349d12eabb7783b117e0c115f6f3d9ef9e
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Sep 28 17:15:20 2012 +0900

    kprobes/x86: Move ftrace-based kprobe code into kprobes-ftrace.c
    
    Split ftrace-based kprobes code from kprobes, and introduce
    CONFIG_(HAVE_)KPROBES_ON_FTRACE Kconfig flags.
    For the cleanup reason, this also moves kprobe_ftrace check
    into skip_singlestep.
    
    Link: http://lkml.kernel.org/r/20120928081520.3560.25624.stgit@ltc138.sdl.hitachi.co.jp
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 23755ba42abc..4b6ef4d33cc2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -49,16 +49,6 @@
 #define KPROBE_REENTER		0x00000004
 #define KPROBE_HIT_SSDONE	0x00000008
 
-/*
- * If function tracer is enabled and the arch supports full
- * passing of pt_regs to function tracing, then kprobes can
- * optimize on top of function tracing.
- */
-#if defined(CONFIG_FUNCTION_TRACER) && defined(ARCH_SUPPORTS_FTRACE_SAVE_REGS) \
-	&& defined(ARCH_SUPPORTS_KPROBES_ON_FTRACE)
-# define KPROBES_CAN_USE_FTRACE
-#endif
-
 /* Attach to insert probes on any functions which should be ignored*/
 #define __kprobes	__attribute__((__section__(".kprobes.text")))
 
@@ -316,7 +306,7 @@ extern int proc_kprobes_optimization_handler(struct ctl_table *table,
 #endif
 
 #endif /* CONFIG_OPTPROBES */
-#ifdef KPROBES_CAN_USE_FTRACE
+#ifdef CONFIG_KPROBES_ON_FTRACE
 extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,
 				  struct ftrace_ops *ops, struct pt_regs *regs);
 extern int arch_prepare_kprobe_ftrace(struct kprobe *p);

commit e52538965119319447c0800c534da73142c27be2
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Tue Jun 5 19:28:38 2012 +0900

    kprobes/x86: ftrace based optimization for x86
    
    Add function tracer based kprobe optimization support
    handlers on x86. This allows kprobes to use function
    tracer for probing on mcount call.
    
    Link: http://lkml.kernel.org/r/20120605102838.27845.26317.stgit@localhost.localdomain
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    
    [ Updated to new port of ftrace save regs functions ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index aa0d05e852e3..23755ba42abc 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -318,7 +318,7 @@ extern int proc_kprobes_optimization_handler(struct ctl_table *table,
 #endif /* CONFIG_OPTPROBES */
 #ifdef KPROBES_CAN_USE_FTRACE
 extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,
-				  struct pt_regs *regs);
+				  struct ftrace_ops *ops, struct pt_regs *regs);
 extern int arch_prepare_kprobe_ftrace(struct kprobe *p);
 #endif
 

commit ae6aa16fdc163afe6b04b6c073ad4ddd4663c03b
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Tue Jun 5 19:28:32 2012 +0900

    kprobes: introduce ftrace based optimization
    
    Introduce function trace based kprobes optimization.
    
    With using ftrace optimization, kprobes on the mcount calling
    address, use ftrace's mcount call instead of breakpoint.
    Furthermore, this optimization works with preemptive kernel
    not like as current jump-based optimization. Of cource,
    this feature works only if the probe is on mcount call.
    
    Only if kprobe.break_handler is set, that probe is not
    optimized with ftrace (nor put on ftrace). The reason why this
    limitation comes is that this break_handler may be used only
    from jprobes which changes ip address (for fetching the function
    arguments), but function tracer ignores modified ip address.
    
    Changes in v2:
     - Fix ftrace_ops registering right after setting its filter.
     - Unregister ftrace_ops if there is no kprobe using.
     - Remove notrace dependency from __kprobes macro.
    
    Link: http://lkml.kernel.org/r/20120605102832.27845.63461.stgit@localhost.localdomain
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index b6e1f8c00577..aa0d05e852e3 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -38,6 +38,7 @@
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
 #include <linux/mutex.h>
+#include <linux/ftrace.h>
 
 #ifdef CONFIG_KPROBES
 #include <asm/kprobes.h>
@@ -48,14 +49,26 @@
 #define KPROBE_REENTER		0x00000004
 #define KPROBE_HIT_SSDONE	0x00000008
 
+/*
+ * If function tracer is enabled and the arch supports full
+ * passing of pt_regs to function tracing, then kprobes can
+ * optimize on top of function tracing.
+ */
+#if defined(CONFIG_FUNCTION_TRACER) && defined(ARCH_SUPPORTS_FTRACE_SAVE_REGS) \
+	&& defined(ARCH_SUPPORTS_KPROBES_ON_FTRACE)
+# define KPROBES_CAN_USE_FTRACE
+#endif
+
 /* Attach to insert probes on any functions which should be ignored*/
 #define __kprobes	__attribute__((__section__(".kprobes.text")))
+
 #else /* CONFIG_KPROBES */
 typedef int kprobe_opcode_t;
 struct arch_specific_insn {
 	int dummy;
 };
 #define __kprobes
+
 #endif /* CONFIG_KPROBES */
 
 struct kprobe;
@@ -128,6 +141,7 @@ struct kprobe {
 				   * NOTE:
 				   * this flag is only for optimized_kprobe.
 				   */
+#define KPROBE_FLAG_FTRACE	8 /* probe is using ftrace */
 
 /* Has this kprobe gone ? */
 static inline int kprobe_gone(struct kprobe *p)
@@ -146,6 +160,13 @@ static inline int kprobe_optimized(struct kprobe *p)
 {
 	return p->flags & KPROBE_FLAG_OPTIMIZED;
 }
+
+/* Is this kprobe uses ftrace ? */
+static inline int kprobe_ftrace(struct kprobe *p)
+{
+	return p->flags & KPROBE_FLAG_FTRACE;
+}
+
 /*
  * Special probe type that uses setjmp-longjmp type tricks to resume
  * execution at a specified entry with a matching prototype corresponding
@@ -295,6 +316,12 @@ extern int proc_kprobes_optimization_handler(struct ctl_table *table,
 #endif
 
 #endif /* CONFIG_OPTPROBES */
+#ifdef KPROBES_CAN_USE_FTRACE
+extern void kprobe_ftrace_handler(unsigned long ip, unsigned long parent_ip,
+				  struct pt_regs *regs);
+extern int arch_prepare_kprobe_ftrace(struct kprobe *p);
+#endif
+
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index dce6e4dbeda7..b6e1f8c00577 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -33,6 +33,7 @@
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/bug.h>
 #include <linux/percpu.h>
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>

commit ec484608c5885931c432e99ecfd2772288cd993c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 16:09:17 2009 +0200

    locking, kprobes: Annotate the hash locks and kretprobe.lock as raw
    
    The kprobe locks can be taken in atomic context and therefore
    cannot be preempted on -rt - annotate it.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index dd7c12e875bc..dce6e4dbeda7 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -181,7 +181,7 @@ struct kretprobe {
 	int nmissed;
 	size_t data_size;
 	struct hlist_head free_instances;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 };
 
 struct kretprobe_instance {

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit b76834bc1b6db0a0923eed85c81b1113021b0612
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Dec 6 11:16:25 2010 -0600

    kprobes: Use this_cpu_ops
    
    Use this_cpu ops in various places to optimize per cpu data access.
    
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e7d1b2e0070d..0c251e9f0507 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -303,12 +303,12 @@ struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 /* kprobe_running() will just return the current_kprobe on this CPU */
 static inline struct kprobe *kprobe_running(void)
 {
-	return (__get_cpu_var(current_kprobe));
+	return (__this_cpu_read(current_kprobe));
 }
 
 static inline void reset_current_kprobe(void)
 {
-	__get_cpu_var(current_kprobe) = NULL;
+	__this_cpu_write(current_kprobe, NULL);
 }
 
 static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)

commit f984ba4eb575e4a27ed28a76d4126d2aa9233c32
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Dec 3 18:54:34 2010 +0900

    kprobes: Use text_poke_smp_batch for unoptimizing
    
    Use text_poke_smp_batch() on unoptimization path for reducing
    the number of stop_machine() issues. If the number of
    unoptimizing probes is more than MAX_OPTIMIZE_PROBES(=256),
    kprobes unoptimizes first MAX_OPTIMIZE_PROBES probes and kicks
    optimizer for remaining probes.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: 2nddept-manager@sdl.hitachi.co.jp
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20101203095434.2961.22657.stgit@ltc236.sdl.hitachi.co.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index fe157ba6aa0e..b78edb58ee66 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -276,6 +276,8 @@ extern int arch_check_optimized_kprobe(struct optimized_kprobe *op);
 extern int arch_prepare_optimized_kprobe(struct optimized_kprobe *op);
 extern void arch_remove_optimized_kprobe(struct optimized_kprobe *op);
 extern void arch_optimize_kprobes(struct list_head *oplist);
+extern void arch_unoptimize_kprobes(struct list_head *oplist,
+				    struct list_head *done_list);
 extern void arch_unoptimize_kprobe(struct optimized_kprobe *op);
 extern kprobe_opcode_t *get_optinsn_slot(void);
 extern void free_optinsn_slot(kprobe_opcode_t *slot, int dirty);

commit cd7ebe2298ff1c3112232878678ce5fe6be8a15b
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Fri Dec 3 18:54:28 2010 +0900

    kprobes: Use text_poke_smp_batch for optimizing
    
    Use text_poke_smp_batch() in optimization path for reducing
    the number of stop_machine() issues. If the number of optimizing
    probes is more than MAX_OPTIMIZE_PROBES(=256), kprobes optimizes
    first MAX_OPTIMIZE_PROBES probes and kicks optimizer for
    remaining probes.
    
    Changes in v5:
    - Use kick_kprobe_optimizer() instead of directly calling
      schedule_delayed_work().
    - Rescheduling optimizer outside of kprobe mutex lock.
    
    Changes in v2:
    - Allocate code buffer and parameters in arch_init_kprobes()
      instead of using static arraies.
    - Merge previous max optimization limit patch into this patch.
      So, this patch introduces upper limit of optimization at
      once.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: 2nddept-manager@sdl.hitachi.co.jp
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20101203095428.2961.8994.stgit@ltc236.sdl.hitachi.co.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e7d1b2e0070d..fe157ba6aa0e 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -275,7 +275,7 @@ extern int arch_prepared_optinsn(struct arch_optimized_insn *optinsn);
 extern int arch_check_optimized_kprobe(struct optimized_kprobe *op);
 extern int arch_prepare_optimized_kprobe(struct optimized_kprobe *op);
 extern void arch_remove_optimized_kprobe(struct optimized_kprobe *op);
-extern int  arch_optimize_kprobe(struct optimized_kprobe *op);
+extern void arch_optimize_kprobes(struct list_head *oplist);
 extern void arch_unoptimize_kprobe(struct optimized_kprobe *op);
 extern kprobe_opcode_t *get_optinsn_slot(void);
 extern void free_optinsn_slot(kprobe_opcode_t *slot, int dirty);

commit b2be84df99ebc93599c69e931a3c4a5105abfabc
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Feb 25 08:34:15 2010 -0500

    kprobes: Jump optimization sysctl interface
    
    Add /proc/sys/debug/kprobes-optimization sysctl which enables
    and disables kprobes jump optimization on the fly for debugging.
    
    Changes in v7:
     - Remove ctl_name = CTL_UNNUMBERED for upstream compatibility.
    
    Changes in v6:
    - Update comments and coding style.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: systemtap <systemtap@sources.redhat.com>
    Cc: DLE <dle-develop@lists.sourceforge.net>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Anders Kaseorg <andersk@ksplice.com>
    Cc: Tim Abbott <tabbott@ksplice.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Mathieu Desnoyers <compudj@krystal.dyndns.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    LKML-Reference: <20100225133415.6725.8274.stgit@localhost6.localdomain6>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index aed1f95c582f..e7d1b2e0070d 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -283,6 +283,14 @@ extern int arch_within_optimized_kprobe(struct optimized_kprobe *op,
 					unsigned long addr);
 
 extern void opt_pre_handler(struct kprobe *p, struct pt_regs *regs);
+
+#ifdef CONFIG_SYSCTL
+extern int sysctl_kprobes_optimization;
+extern int proc_kprobes_optimization_handler(struct ctl_table *table,
+					     int write, void __user *buffer,
+					     size_t *length, loff_t *ppos);
+#endif
+
 #endif /* CONFIG_OPTPROBES */
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */

commit afd66255b9a48f5851326ddae50e2203fbf71dc9
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Feb 25 08:34:07 2010 -0500

    kprobes: Introduce kprobes jump optimization
    
    Introduce kprobes jump optimization arch-independent parts.
    Kprobes uses breakpoint instruction for interrupting execution
    flow, on some architectures, it can be replaced by a jump
    instruction and interruption emulation code. This gains kprobs'
    performance drastically.
    
    To enable this feature, set CONFIG_OPTPROBES=y (default y if the
    arch supports OPTPROBE).
    
    Changes in v9:
     - Fix a bug to optimize probe when enabling.
     - Check nearby probes can be optimize/unoptimize when disarming/arming
       kprobes, instead of registering/unregistering. This will help
       kprobe-tracer because most of probes on it are usually disabled.
    
    Changes in v6:
     - Cleanup coding style for readability.
     - Add comments around get/put_online_cpus().
    
    Changes in v5:
     - Use get_online_cpus()/put_online_cpus() for avoiding text_mutex
       deadlock.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: systemtap <systemtap@sources.redhat.com>
    Cc: DLE <dle-develop@lists.sourceforge.net>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Anders Kaseorg <andersk@ksplice.com>
    Cc: Tim Abbott <tabbott@ksplice.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Mathieu Desnoyers <compudj@krystal.dyndns.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    LKML-Reference: <20100225133407.6725.81992.stgit@localhost6.localdomain6>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 1b672f74a32f..aed1f95c582f 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -122,6 +122,11 @@ struct kprobe {
 /* Kprobe status flags */
 #define KPROBE_FLAG_GONE	1 /* breakpoint has already gone */
 #define KPROBE_FLAG_DISABLED	2 /* probe is temporarily disabled */
+#define KPROBE_FLAG_OPTIMIZED	4 /*
+				   * probe is really optimized.
+				   * NOTE:
+				   * this flag is only for optimized_kprobe.
+				   */
 
 /* Has this kprobe gone ? */
 static inline int kprobe_gone(struct kprobe *p)
@@ -134,6 +139,12 @@ static inline int kprobe_disabled(struct kprobe *p)
 {
 	return p->flags & (KPROBE_FLAG_DISABLED | KPROBE_FLAG_GONE);
 }
+
+/* Is this kprobe really running optimized path ? */
+static inline int kprobe_optimized(struct kprobe *p)
+{
+	return p->flags & KPROBE_FLAG_OPTIMIZED;
+}
 /*
  * Special probe type that uses setjmp-longjmp type tricks to resume
  * execution at a specified entry with a matching prototype corresponding
@@ -249,6 +260,31 @@ extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot, int dirty);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
+#ifdef CONFIG_OPTPROBES
+/*
+ * Internal structure for direct jump optimized probe
+ */
+struct optimized_kprobe {
+	struct kprobe kp;
+	struct list_head list;	/* list for optimizing queue */
+	struct arch_optimized_insn optinsn;
+};
+
+/* Architecture dependent functions for direct jump optimization */
+extern int arch_prepared_optinsn(struct arch_optimized_insn *optinsn);
+extern int arch_check_optimized_kprobe(struct optimized_kprobe *op);
+extern int arch_prepare_optimized_kprobe(struct optimized_kprobe *op);
+extern void arch_remove_optimized_kprobe(struct optimized_kprobe *op);
+extern int  arch_optimize_kprobe(struct optimized_kprobe *op);
+extern void arch_unoptimize_kprobe(struct optimized_kprobe *op);
+extern kprobe_opcode_t *get_optinsn_slot(void);
+extern void free_optinsn_slot(kprobe_opcode_t *slot, int dirty);
+extern int arch_within_optimized_kprobe(struct optimized_kprobe *op,
+					unsigned long addr);
+
+extern void opt_pre_handler(struct kprobe *p, struct pt_regs *regs);
+#endif /* CONFIG_OPTPROBES */
+
 /* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);
 void kretprobe_hash_lock(struct task_struct *tsk,

commit d7a4b414eed51f1653bb05ebe84122bf9a7ae18b
Merge: 1f0ab4097646 a724eada8c2a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 23 23:08:43 2009 +0200

    Merge commit 'linus/master' into tracing/kprobes
    
    Conflicts:
            kernel/trace/Makefile
            kernel/trace/trace.h
            kernel/trace/trace_event_types.h
            kernel/trace/trace_export.c
    
    Merge reason:
            Sync with latest significant tracing core changes.

commit fe832a3a48737b24f95fab202b1c67fb588b071d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Sep 15 23:51:31 2009 -0400

    tracing: remove notrace from __kprobes annotation
    
    When ftrace had issues with NMIs, it was needed to annotate all
    the areas that kprobes had issues with notrace. Now that ftrace is
    NMI safe, the functions that limit ftrace from tracing are just a
    small few.
    
    Kprobes is too big of a set for ftrace not to trace. Remove the
    coupling.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index bcd9c07848be..3a46b7b7abb2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -48,13 +48,13 @@
 #define KPROBE_HIT_SSDONE	0x00000008
 
 /* Attach to insert probes on any functions which should be ignored*/
-#define __kprobes	__attribute__((__section__(".kprobes.text"))) notrace
+#define __kprobes	__attribute__((__section__(".kprobes.text")))
 #else /* CONFIG_KPROBES */
 typedef int kprobe_opcode_t;
 struct arch_specific_insn {
 	int dummy;
 };
-#define __kprobes	notrace
+#define __kprobes
 #endif /* CONFIG_KPROBES */
 
 struct kprobe;

commit 24851d2447830e6cba4c4b641cb73e713f312373
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 26 23:38:30 2009 +0200

    tracing/kprobes: Dump the culprit kprobe in case of kprobe recursion
    
    Kprobes can enter into a probing recursion, ie: a kprobe that does an
    endless loop because one of its core mechanism function used during
    probing is also probed itself.
    
    This patch helps pinpointing the kprobe that raised such recursion
    by dumping it and raising a BUG instead of a warning (we also disarm
    the kprobe to try avoiding recursion in BUG itself). Having a BUG
    instead of a warning stops the stacktrace in the right place and
    doesn't pollute the logs with hundreds of traces that eventually end
    up in a stack overflow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index bcd9c07848be..87eb79c9dd60 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -296,6 +296,8 @@ void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 int disable_kprobe(struct kprobe *kp);
 int enable_kprobe(struct kprobe *kp);
 
+void dump_kprobe(struct kprobe *kp);
+
 #else /* !CONFIG_KPROBES: */
 
 static inline int kprobes_built_in(void)

commit 8f9b15286a8ea49e997e845d02d357ed33ebd090
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 6 19:01:02 2009 -0700

    kprobes: support kretprobe and jprobe per-probe disabling
    
    Add disable/enable_kretprobe() and disable/enable_jprobe().
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 1071cfddddc9..bcd9c07848be 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -371,4 +371,21 @@ static inline int enable_kprobe(struct kprobe *kp)
 	return -ENOSYS;
 }
 #endif /* CONFIG_KPROBES */
+static inline int disable_kretprobe(struct kretprobe *rp)
+{
+	return disable_kprobe(&rp->kp);
+}
+static inline int enable_kretprobe(struct kretprobe *rp)
+{
+	return enable_kprobe(&rp->kp);
+}
+static inline int disable_jprobe(struct jprobe *jp)
+{
+	return disable_kprobe(&jp->kp);
+}
+static inline int enable_jprobe(struct jprobe *jp)
+{
+	return enable_kprobe(&jp->kp);
+}
+
 #endif /* _LINUX_KPROBES_H */

commit de5bd88d5a5cce3cacea904d3503e5ebdb3852a2
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 6 19:01:02 2009 -0700

    kprobes: support per-kprobe disabling
    
    Add disable_kprobe() and enable_kprobe() to disable/enable kprobes
    temporarily.
    
    disable_kprobe() asynchronously disables probe handlers of specified
    kprobe.  So, after calling it, some handlers can be called at a while.
    enable_kprobe() enables specified kprobe.
    
    aggr_pre_handler and aggr_post_handler check disabled probes.  On the
    other hand aggr_break_handler and aggr_fault_handler don't check it
    because these handlers will be called while executing pre or post handlers
    and usually those help error handling.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 39826a678364..1071cfddddc9 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -112,18 +112,28 @@ struct kprobe {
 	/* copy of the original instruction */
 	struct arch_specific_insn ainsn;
 
-	/* Indicates various status flags.  Protected by kprobe_mutex. */
+	/*
+	 * Indicates various status flags.
+	 * Protected by kprobe_mutex after this kprobe is registered.
+	 */
 	u32 flags;
 };
 
 /* Kprobe status flags */
 #define KPROBE_FLAG_GONE	1 /* breakpoint has already gone */
+#define KPROBE_FLAG_DISABLED	2 /* probe is temporarily disabled */
 
+/* Has this kprobe gone ? */
 static inline int kprobe_gone(struct kprobe *p)
 {
 	return p->flags & KPROBE_FLAG_GONE;
 }
 
+/* Is this kprobe disabled ? */
+static inline int kprobe_disabled(struct kprobe *p)
+{
+	return p->flags & (KPROBE_FLAG_DISABLED | KPROBE_FLAG_GONE);
+}
 /*
  * Special probe type that uses setjmp-longjmp type tricks to resume
  * execution at a specified entry with a matching prototype corresponding
@@ -283,6 +293,9 @@ void unregister_kretprobes(struct kretprobe **rps, int num);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 
+int disable_kprobe(struct kprobe *kp);
+int enable_kprobe(struct kprobe *kp);
+
 #else /* !CONFIG_KPROBES: */
 
 static inline int kprobes_built_in(void)
@@ -349,5 +362,13 @@ static inline void unregister_kretprobes(struct kretprobe **rps, int num)
 static inline void kprobe_flush_task(struct task_struct *tk)
 {
 }
+static inline int disable_kprobe(struct kprobe *kp)
+{
+	return -ENOSYS;
+}
+static inline int enable_kprobe(struct kprobe *kp)
+{
+	return -ENOSYS;
+}
 #endif /* CONFIG_KPROBES */
 #endif /* _LINUX_KPROBES_H */

commit cc00e9cfe0e5c4c31057c722e49fdf2c76dd5953
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 6 19:01:00 2009 -0700

    kprobes: cleanup comment style in kprobes.h
    
    Fix comment style in kprobes.h.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 2ec6cc14a114..39826a678364 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -94,12 +94,16 @@ struct kprobe {
 	/* Called after addr is executed, unless... */
 	kprobe_post_handler_t post_handler;
 
-	/* ... called if executing addr causes a fault (eg. page fault).
-	 * Return 1 if it handled fault, otherwise kernel will see it. */
+	/*
+	 * ... called if executing addr causes a fault (eg. page fault).
+	 * Return 1 if it handled fault, otherwise kernel will see it.
+	 */
 	kprobe_fault_handler_t fault_handler;
 
-	/* ... called if breakpoint trap occurs in probe handler.
-	 * Return 1 if it handled break, otherwise kernel will see it. */
+	/*
+	 * ... called if breakpoint trap occurs in probe handler.
+	 * Return 1 if it handled break, otherwise kernel will see it.
+	 */
 	kprobe_break_handler_t break_handler;
 
 	/* Saved opcode (which has been replaced with breakpoint) */

commit b18018126f422f5b706fd750373425e10e84b486
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 20 22:42:57 2009 +0100

    x86, mm, kprobes: fault.c, simplify notify_page_fault()
    
    Impact: cleanup
    
    Remove an #ifdef from notify_page_fault(). The function still
    compiles to nothing in the !CONFIG_KPROBES case.
    
    Introduce kprobes_built_in() and kprobe_fault_handler() helpers
    to allow this - they returns 0 if !CONFIG_KPROBES.
    
    No code changed:
    
       text    data     bss     dec     hex filename
       4618      32      24    4674    1242 fault.o.before
       4618      32      24    4674    1242 fault.o.after
    
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 32851eef48f0..2ec6cc14a114 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -182,6 +182,14 @@ struct kprobe_blackpoint {
 DECLARE_PER_CPU(struct kprobe *, current_kprobe);
 DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
 
+/*
+ * For #ifdef avoidance:
+ */
+static inline int kprobes_built_in(void)
+{
+	return 1;
+}
+
 #ifdef CONFIG_KRETPROBES
 extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
 				   struct pt_regs *regs);
@@ -271,8 +279,16 @@ void unregister_kretprobes(struct kretprobe **rps, int num);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 
-#else /* CONFIG_KPROBES */
+#else /* !CONFIG_KPROBES: */
 
+static inline int kprobes_built_in(void)
+{
+	return 0;
+}
+static inline int kprobe_fault_handler(struct pt_regs *regs, int trapnr)
+{
+	return 0;
+}
 static inline struct kprobe *get_kprobe(void *addr)
 {
 	return NULL;
@@ -329,5 +345,5 @@ static inline void unregister_kretprobes(struct kretprobe **rps, int num)
 static inline void kprobe_flush_task(struct task_struct *tk)
 {
 }
-#endif				/* CONFIG_KPROBES */
-#endif				/* _LINUX_KPROBES_H */
+#endif /* CONFIG_KPROBES */
+#endif /* _LINUX_KPROBES_H */

commit dc19835df6c47ff676ad6c98722d5e529db5d74c
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Jan 29 14:25:08 2009 -0800

    kprobes: fix module compilation error with CONFIG_KPROBES=n
    
    Define kprobes related data structures even if CONFIG_KPROBES is not set.
    This fixes compilation errors which occur if CONFIG_KPROBES is not set, in
    kprobe using modules.
    
    [akpm@linux-foundation.org: fix build for non-kprobes-supporting architectures]
    Reviewed-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index d6ea19e314bb..32851eef48f0 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -49,6 +49,13 @@
 
 /* Attach to insert probes on any functions which should be ignored*/
 #define __kprobes	__attribute__((__section__(".kprobes.text"))) notrace
+#else /* CONFIG_KPROBES */
+typedef int kprobe_opcode_t;
+struct arch_specific_insn {
+	int dummy;
+};
+#define __kprobes	notrace
+#endif /* CONFIG_KPROBES */
 
 struct kprobe;
 struct pt_regs;
@@ -131,23 +138,6 @@ struct jprobe {
 /* For backward compatibility with old code using JPROBE_ENTRY() */
 #define JPROBE_ENTRY(handler)	(handler)
 
-DECLARE_PER_CPU(struct kprobe *, current_kprobe);
-DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
-
-#ifdef CONFIG_KRETPROBES
-extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
-				   struct pt_regs *regs);
-extern int arch_trampoline_kprobe(struct kprobe *p);
-#else /* CONFIG_KRETPROBES */
-static inline void arch_prepare_kretprobe(struct kretprobe *rp,
-					struct pt_regs *regs)
-{
-}
-static inline int arch_trampoline_kprobe(struct kprobe *p)
-{
-	return 0;
-}
-#endif /* CONFIG_KRETPROBES */
 /*
  * Function-return probe -
  * Note:
@@ -188,6 +178,25 @@ struct kprobe_blackpoint {
 	unsigned long range;
 };
 
+#ifdef CONFIG_KPROBES
+DECLARE_PER_CPU(struct kprobe *, current_kprobe);
+DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+
+#ifdef CONFIG_KRETPROBES
+extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
+				   struct pt_regs *regs);
+extern int arch_trampoline_kprobe(struct kprobe *p);
+#else /* CONFIG_KRETPROBES */
+static inline void arch_prepare_kretprobe(struct kretprobe *rp,
+					struct pt_regs *regs)
+{
+}
+static inline int arch_trampoline_kprobe(struct kprobe *p)
+{
+	return 0;
+}
+#endif /* CONFIG_KRETPROBES */
+
 extern struct kretprobe_blackpoint kretprobe_blacklist[];
 
 static inline void kretprobe_assert(struct kretprobe_instance *ri,
@@ -264,10 +273,6 @@ void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 
 #else /* CONFIG_KPROBES */
 
-#define __kprobes	notrace
-struct jprobe;
-struct kretprobe;
-
 static inline struct kprobe *get_kprobe(void *addr)
 {
 	return NULL;

commit e8386a0cb22f4a2d439384212c494ad0bda848fe
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Jan 6 14:41:52 2009 -0800

    kprobes: support probing module __exit function
    
    Allows kprobes to probe __exit routine.  This adds flags member to struct
    kprobe.  When module is freed(kprobes hooks module_notifier to get this
    event), kprobes which probe the functions in that module are set to "Gone"
    flag to the flags member.  These "Gone" probes are never be enabled.
    Users can check the GONE flag through debugfs.
    
    This also removes mod_refcounted, because we couldn't free a module if
    kprobe incremented the refcount of that module.
    
    [akpm@linux-foundation.org: document some locking]
    [mhiramat@redhat.com: bugfix: pass aggr_kprobe to arch_remove_kprobe]
    [mhiramat@redhat.com: bugfix: release old_p's insn_slot before error return]
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index b93e44ce2284..d6ea19e314bb 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -69,9 +69,6 @@ struct kprobe {
 	/* list of kprobes for multi-handler support */
 	struct list_head list;
 
-	/* Indicates that the corresponding module has been ref counted */
-	unsigned int mod_refcounted;
-
 	/*count the number of times this probe was temporarily disarmed */
 	unsigned long nmissed;
 
@@ -103,8 +100,19 @@ struct kprobe {
 
 	/* copy of the original instruction */
 	struct arch_specific_insn ainsn;
+
+	/* Indicates various status flags.  Protected by kprobe_mutex. */
+	u32 flags;
 };
 
+/* Kprobe status flags */
+#define KPROBE_FLAG_GONE	1 /* breakpoint has already gone */
+
+static inline int kprobe_gone(struct kprobe *p)
+{
+	return p->flags & KPROBE_FLAG_GONE;
+}
+
 /*
  * Special probe type that uses setjmp-longjmp type tricks to resume
  * execution at a specified entry with a matching prototype corresponding

commit 129415607845d4daea11ddcba706005c69dcb942
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Jan 6 14:41:50 2009 -0800

    kprobes: add kprobe_insn_mutex and cleanup arch_remove_kprobe()
    
    Add kprobe_insn_mutex for protecting kprobe_insn_pages hlist, and remove
    kprobe_mutex from architecture dependent code.
    
    This allows us to call arch_remove_kprobe() (and free_insn_slot) while
    holding kprobe_mutex.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 497b1d1f7a05..b93e44ce2284 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -201,7 +201,6 @@ static inline int init_test_probes(void)
 }
 #endif /* CONFIG_KPROBES_SANITY_TEST */
 
-extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);

commit 36dcd67ae994fece615b7c700958d215e884b9ae
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 29 12:00:59 2008 +0200

    ftrace: ignore functions that cannot be kprobe-ed
    
    kprobes already has an extensive list of annotations for functions
    that should not be instrumented. Add notrace annotations to these
    functions as well.
    
    This is particularly useful for functions called by the NMI path.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 0be7795655fa..497b1d1f7a05 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -29,6 +29,7 @@
  *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
  *		<prasanna@in.ibm.com> added function-return probes.
  */
+#include <linux/linkage.h>
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
@@ -47,7 +48,7 @@
 #define KPROBE_HIT_SSDONE	0x00000008
 
 /* Attach to insert probes on any functions which should be ignored*/
-#define __kprobes	__attribute__((__section__(".kprobes.text")))
+#define __kprobes	__attribute__((__section__(".kprobes.text"))) notrace
 
 struct kprobe;
 struct pt_regs;
@@ -256,7 +257,7 @@ void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 
 #else /* CONFIG_KPROBES */
 
-#define __kprobes	/**/
+#define __kprobes	notrace
 struct jprobe;
 struct kretprobe;
 

commit ef53d9c5e4da147ecaa43c44c5e5945eb83970a2
Author: Srinivasa D S <srinivasa@in.ibm.com>
Date:   Fri Jul 25 01:46:04 2008 -0700

    kprobes: improve kretprobe scalability with hashed locking
    
    Currently list of kretprobe instances are stored in kretprobe object (as
    used_instances,free_instances) and in kretprobe hash table.  We have one
    global kretprobe lock to serialise the access to these lists.  This causes
    only one kretprobe handler to execute at a time.  Hence affects system
    performance, particularly on SMP systems and when return probe is set on
    lot of functions (like on all systemcalls).
    
    Solution proposed here gives fine-grain locks that performs better on SMP
    system compared to present kretprobe implementation.
    
    Solution:
    
     1) Instead of having one global lock to protect kretprobe instances
        present in kretprobe object and kretprobe hash table.  We will have
        two locks, one lock for protecting kretprobe hash table and another
        lock for kretporbe object.
    
     2) We hold lock present in kretprobe object while we modify kretprobe
        instance in kretprobe object and we hold per-hash-list lock while
        modifying kretprobe instances present in that hash list.  To prevent
        deadlock, we never grab a per-hash-list lock while holding a kretprobe
        lock.
    
     3) We can remove used_instances from struct kretprobe, as we can
        track used instances of kretprobe instances using kretprobe hash
        table.
    
    Time duration for kernel compilation ("make -j 8") on a 8-way ppc64 system
    with return probes set on all systemcalls looks like this.
    
    cacheline              non-cacheline             Un-patched kernel
    aligned patch          aligned patch
    ===============================================================================
    real    9m46.784s       9m54.412s                  10m2.450s
    user    40m5.715s       40m7.142s                  40m4.273s
    sys     2m57.754s       2m58.583s                  3m17.430s
    ===========================================================
    
    Time duration for kernel compilation ("make -j 8) on the same system, when
    kernel is not probed.
    =========================
    real    9m26.389s
    user    40m8.775s
    sys     2m7.283s
    =========================
    
    Signed-off-by: Srinivasa DS <srinivasa@in.ibm.com>
    Signed-off-by: Jim Keniston <jkenisto@us.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 04a3556bdea6..0be7795655fa 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -157,11 +157,10 @@ struct kretprobe {
 	int nmissed;
 	size_t data_size;
 	struct hlist_head free_instances;
-	struct hlist_head used_instances;
+	spinlock_t lock;
 };
 
 struct kretprobe_instance {
-	struct hlist_node uflist; /* either on free list or used list */
 	struct hlist_node hlist;
 	struct kretprobe *rp;
 	kprobe_opcode_t *ret_addr;
@@ -201,7 +200,6 @@ static inline int init_test_probes(void)
 }
 #endif /* CONFIG_KPROBES_SANITY_TEST */
 
-extern spinlock_t kretprobe_lock;
 extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
@@ -214,6 +212,9 @@ extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);
+void kretprobe_hash_lock(struct task_struct *tsk,
+			 struct hlist_head **head, unsigned long *flags);
+void kretprobe_hash_unlock(struct task_struct *tsk, unsigned long *flags);
 struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 
 /* kprobe_running() will just return the current_kprobe on this CPU */

commit 785656a41f9a9c0e843a23d1ae05d900b5158f8f
Author: Abhishek Sagar <sagar.abhishek@gmail.com>
Date:   Sat Jun 21 23:47:39 2008 +0530

    kprobes: enable clean usage of get_kprobe
    
    Allow clean use of get_kprobe() outside of core kprobe code. Ftrace makes use
    of get_kprobe to identify probes installed on mcount call-sites.
    
    Signed-off-by: Abhishek Sagar <sagar.abhishek@gmail.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: jkenisto@us.ibm.com
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 1036631ff4fa..04a3556bdea6 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -259,6 +259,10 @@ void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 struct jprobe;
 struct kretprobe;
 
+static inline struct kprobe *get_kprobe(void *addr)
+{
+	return NULL;
+}
 static inline struct kprobe *kprobe_running(void)
 {
 	return NULL;

commit 26b31c1908e02a316edfba08080373342e662c14
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 28 02:14:29 2008 -0700

    kprobes: add (un)register_jprobes for batch registration
    
    Introduce unregister_/register_jprobes() for jprobe batch registration.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 94c855a236ae..1036631ff4fa 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -240,6 +240,8 @@ int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
 int longjmp_break_handler(struct kprobe *, struct pt_regs *);
 int register_jprobe(struct jprobe *p);
 void unregister_jprobe(struct jprobe *p);
+int register_jprobes(struct jprobe **jps, int num);
+void unregister_jprobes(struct jprobe **jps, int num);
 void jprobe_return(void);
 unsigned long arch_deref_entry_point(void *);
 
@@ -279,9 +281,16 @@ static inline int register_jprobe(struct jprobe *p)
 {
 	return -ENOSYS;
 }
+static inline int register_jprobes(struct jprobe **jps, int num)
+{
+	return -ENOSYS;
+}
 static inline void unregister_jprobe(struct jprobe *p)
 {
 }
+static inline void unregister_jprobes(struct jprobe **jps, int num)
+{
+}
 static inline void jprobe_return(void)
 {
 }

commit 4a296e07c3a410c09b9155da4c2fa84a07964f38
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 28 02:14:29 2008 -0700

    kprobes: add (un)register_kretprobes for batch registration
    
    Introduce unregister_/register_kretprobes() for kretprobe batch registration.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 2ba7df645a84..94c855a236ae 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -245,6 +245,8 @@ unsigned long arch_deref_entry_point(void *);
 
 int register_kretprobe(struct kretprobe *rp);
 void unregister_kretprobe(struct kretprobe *rp);
+int register_kretprobes(struct kretprobe **rps, int num);
+void unregister_kretprobes(struct kretprobe **rps, int num);
 
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
@@ -287,9 +289,16 @@ static inline int register_kretprobe(struct kretprobe *rp)
 {
 	return -ENOSYS;
 }
+static inline int register_kretprobes(struct kretprobe **rps, int num)
+{
+	return -ENOSYS;
+}
 static inline void unregister_kretprobe(struct kretprobe *rp)
 {
 }
+static inline void unregister_kretprobes(struct kretprobe **rps, int num)
+{
+}
 static inline void kprobe_flush_task(struct task_struct *tk)
 {
 }

commit 9861668f747895608cea425f8457989d8dd2edf2
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 28 02:14:28 2008 -0700

    kprobes: add (un)register_kprobes for batch registration
    
    Introduce unregister_/register_kprobes() for kprobe batch registration.  This
    can reduce waiting time for synchronized_sched() when a lot of probes have to
    be unregistered at once.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index cd507ab4fed7..2ba7df645a84 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -234,6 +234,8 @@ static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
 
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
+int register_kprobes(struct kprobe **kps, int num);
+void unregister_kprobes(struct kprobe **kps, int num);
 int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
 int longjmp_break_handler(struct kprobe *, struct pt_regs *);
 int register_jprobe(struct jprobe *p);
@@ -261,9 +263,16 @@ static inline int register_kprobe(struct kprobe *p)
 {
 	return -ENOSYS;
 }
+static inline int register_kprobes(struct kprobe **kps, int num)
+{
+	return -ENOSYS;
+}
 static inline void unregister_kprobe(struct kprobe *p)
 {
 }
+static inline void unregister_kprobes(struct kprobe **kps, int num)
+{
+}
 static inline int register_jprobe(struct jprobe *p)
 {
 	return -ENOSYS;

commit 3d8d996e0ca5b4093203d3f050b0f70b5c949ae8
Author: Srinivasa Ds <srinivasa@in.ibm.com>
Date:   Mon Apr 28 02:14:26 2008 -0700

    kprobes: prevent probing of preempt_schedule()
    
    Prohibit users from probing preempt_schedule().  One way of prohibiting the
    user from probing functions is by marking such functions with __kprobes.  But
    this method doesn't work for those functions, which are already marked to
    different section like preempt_schedule() (belongs to __sched section).  So we
    use blacklist approach to refuse user from probing these functions.
    
    In blacklist approach we populate the blacklisted function's starting address
    and its size in kprobe_blacklist structure.  Then we verify the user specified
    address against start and end of the blacklisted function.  So any attempt to
    register probe on blacklisted functions will be rejected.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Srinivasa DS <srinivasa@in.ibm.com>
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 0f28486f6360..cd507ab4fed7 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -173,6 +173,13 @@ struct kretprobe_blackpoint {
 	const char *name;
 	void *addr;
 };
+
+struct kprobe_blackpoint {
+	const char *name;
+	unsigned long start_addr;
+	unsigned long range;
+};
+
 extern struct kretprobe_blackpoint kretprobe_blacklist[];
 
 static inline void kretprobe_assert(struct kretprobe_instance *ri,

commit 9edddaa200df18e08fe0cf21036e8ae467b1363c
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Tue Mar 4 14:28:37 2008 -0800

    Kprobes: indicate kretprobe support in Kconfig
    
    Add CONFIG_HAVE_KRETPROBES to the arch/<arch>/Kconfig file for relevant
    architectures with kprobes support.  This facilitates easy handling of
    in-kernel modules (like samples/kprobes/kretprobe_example.c) that depend on
    kretprobes being present in the kernel.
    
    Thanks to Sam Ravnborg for helping make the patch more lean.
    
    Per Mathieu's suggestion, added CONFIG_KRETPROBES and fixed up dependencies.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 4a6ce82ba039..0f28486f6360 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -125,11 +125,11 @@ struct jprobe {
 DECLARE_PER_CPU(struct kprobe *, current_kprobe);
 DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
 
-#ifdef ARCH_SUPPORTS_KRETPROBES
+#ifdef CONFIG_KRETPROBES
 extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
 				   struct pt_regs *regs);
 extern int arch_trampoline_kprobe(struct kprobe *p);
-#else /* ARCH_SUPPORTS_KRETPROBES */
+#else /* CONFIG_KRETPROBES */
 static inline void arch_prepare_kretprobe(struct kretprobe *rp,
 					struct pt_regs *regs)
 {
@@ -138,7 +138,7 @@ static inline int arch_trampoline_kprobe(struct kprobe *p)
 {
 	return 0;
 }
-#endif /* ARCH_SUPPORTS_KRETPROBES */
+#endif /* CONFIG_KRETPROBES */
 /*
  * Function-return probe -
  * Note:

commit f47cd9b553aaada602449204513b5a5b29cba263
Author: Abhishek Sagar <sagar.abhishek@gmail.com>
Date:   Wed Feb 6 01:38:22 2008 -0800

    kprobes: kretprobe user entry-handler
    
    Provide support to add an optional user defined callback to be run at
    function entry of a kretprobe'd function.  Also modify the kprobe smoke
    tests to include an entry-handler during the kretprobe sanity test.
    
    Signed-off-by: Abhishek Sagar <sagar.abhishek@gmail.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Acked-by: Jim Keniston <jkenisto@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 6168c0a44172..4a6ce82ba039 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -152,8 +152,10 @@ static inline int arch_trampoline_kprobe(struct kprobe *p)
 struct kretprobe {
 	struct kprobe kp;
 	kretprobe_handler_t handler;
+	kretprobe_handler_t entry_handler;
 	int maxactive;
 	int nmissed;
+	size_t data_size;
 	struct hlist_head free_instances;
 	struct hlist_head used_instances;
 };
@@ -164,6 +166,7 @@ struct kretprobe_instance {
 	struct kretprobe *rp;
 	kprobe_opcode_t *ret_addr;
 	struct task_struct *task;
+	char data[0];
 };
 
 struct kretprobe_blackpoint {

commit 8c1c9356429741a82ff176d0f3400fb9e06b2a30
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Wed Jan 30 13:32:53 2008 +0100

    x86: kprobes: add kprobes smoke tests that run on boot
    
    Here is a quick and naive smoke test for kprobes. This is intended to
    just verify if some unrelated change broke the *probes subsystem. It is
    self contained, architecture agnostic and isn't of any great use by itself.
    
    This needs to be built in the kernel and runs a basic set of tests to
    verify if kprobes, jprobes and kretprobes run fine on the kernel. In case
    of an error, it'll print out a message with a "BUG" prefix.
    
    This is a start; we intend to add more tests to this bucket over time.
    
    Thanks to Jim Keniston and Masami Hiramatsu for comments and suggestions.
    
    Tested on x86 (32/64) and powerpc.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 81891581e89b..6168c0a44172 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -182,6 +182,15 @@ static inline void kretprobe_assert(struct kretprobe_instance *ri,
 	}
 }
 
+#ifdef CONFIG_KPROBES_SANITY_TEST
+extern int init_test_probes(void);
+#else
+static inline int init_test_probes(void)
+{
+	return 0;
+}
+#endif /* CONFIG_KPROBES_SANITY_TEST */
+
 extern spinlock_t kretprobe_lock;
 extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
@@ -227,6 +236,7 @@ void unregister_kretprobe(struct kretprobe *rp);
 
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
+
 #else /* CONFIG_KPROBES */
 
 #define __kprobes	/**/

commit f438d914b220051d4cbc65cbc5d98e163c85c93b
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Tue Oct 16 01:27:49 2007 -0700

    kprobes: support kretprobe blacklist
    
    Introduce architecture dependent kretprobe blacklists to prohibit users
    from inserting return probes on the function in which kprobes can be
    inserted but kretprobes can not.
    
    This patch also removes "__kprobes" mark from "__switch_to" on x86_64 and
    registers "__switch_to" to the blacklist on x86-64, because that mark is to
    prohibit user from inserting only kretprobe.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 51464d12a4e5..81891581e89b 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -166,6 +166,12 @@ struct kretprobe_instance {
 	struct task_struct *task;
 };
 
+struct kretprobe_blackpoint {
+	const char *name;
+	void *addr;
+};
+extern struct kretprobe_blackpoint kretprobe_blacklist[];
+
 static inline void kretprobe_assert(struct kretprobe_instance *ri,
 	unsigned long orig_ret_address, unsigned long trampoline_address)
 {

commit 3d7e33825d8799115dd2495c9944badd3272a623
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jul 19 01:48:11 2007 -0700

    jprobes: make jprobes a little safer for users
    
    I realise jprobes are a razor-blades-included type of interface, but that
    doesn't mean we can't try and make them safer to use.  This guy I know once
    wrote code like this:
    
    struct jprobe jp = { .kp.symbol_name = "foo", .entry = "jprobe_foo" };
    
    And then his kernel exploded. Oops.
    
    This patch adds an arch hook, arch_deref_entry_point() (I don't like it
    either) which takes the void * in a struct jprobe, and gives back the text
    address that it represents.
    
    We can then use that in register_jprobe() to check that the entry point we're
    passed is actually in the kernel text, rather than just some random value.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index bd892850c94a..51464d12a4e5 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -214,6 +214,7 @@ int longjmp_break_handler(struct kprobe *, struct pt_regs *);
 int register_jprobe(struct jprobe *p);
 void unregister_jprobe(struct jprobe *p);
 void jprobe_return(void);
+unsigned long arch_deref_entry_point(void *);
 
 int register_kretprobe(struct kretprobe *rp);
 void unregister_kretprobe(struct kretprobe *rp);

commit 9e367d859297b9377d65574f538cf52730e9eda8
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jul 19 01:48:10 2007 -0700

    jprobes: remove JPROBE_ENTRY()
    
    AFAICT now that jprobe.entry is a void *, JPROBE_ENTRY doesn't do anything
    useful - so remove it ..
    
    I've left a do-nothing version so that out-of-tree jprobes code will still
    compile without modifications.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index f4e53b71d23f..bd892850c94a 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -119,6 +119,9 @@ struct jprobe {
 	void *entry;	/* probe handling code to jump to */
 };
 
+/* For backward compatibility with old code using JPROBE_ENTRY() */
+#define JPROBE_ENTRY(handler)	(handler)
+
 DECLARE_PER_CPU(struct kprobe *, current_kprobe);
 DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
 

commit 81eae375eceba481ca4c605d42913871f093f6d5
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jul 19 01:48:09 2007 -0700

    jprobes: make struct jprobe.entry a void *
    
    Currently jprobe.entry is a kprobe_opcode_t *, but that's a lie.  On some
    platforms it doesn't point to an opcode at all, it points to a function
    descriptor.
    
    It's really a pointer to something that the arch code can turn into a function
    entry point.  And that's what actually happens, none of the generic code ever
    looks at jprobe.entry, it's only ever dereferenced by arch code.
    
    So just make it a void *.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 23adf6075ae4..f4e53b71d23f 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -116,7 +116,7 @@ struct kprobe {
  */
 struct jprobe {
 	struct kprobe kp;
-	kprobe_opcode_t *entry;	/* probe handling code to jump to */
+	void *entry;	/* probe handling code to jump to */
 };
 
 DECLARE_PER_CPU(struct kprobe *, current_kprobe);

commit bf8f6e5b3e51ee0c64c2d1350c70198ddc8ad3f7
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Tue May 8 00:34:16 2007 -0700

    Kprobes: The ON/OFF knob thru debugfs
    
    This patch provides a debugfs knob to turn kprobes on/off
    
    o A new file /debug/kprobes/enabled indicates if kprobes is enabled or
      not (default enabled)
    o Echoing 0 to this file will disarm all installed probes
    o Any new probe registration when disabled will register the probe but
      not arm it. A message will be printed out in such a case.
    o When a value 1 is echoed to the file, all probes (including ones
      registered in the intervening period) will be enabled
    o Unregistration will happen irrespective of whether probes are globally
      enabled or not.
    o Update Documentation/kprobes.txt to reflect these changes. While there
      also update the doc to make it current.
    
    We are also looking at providing sysrq key support to tie to the disabling
    feature provided by this patch.
    
    [akpm@linux-foundation.org: Use bool like a bool!]
    [akpm@linux-foundation.org: add printk facility levels]
    [cornelia.huck@de.ibm.com: Add the missing arch_trampoline_kprobe() for s390]
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Srinivasa DS <srinivasa@in.ibm.com>
    Signed-off-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 6fc623e41fd8..23adf6075ae4 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -125,11 +125,16 @@ DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
 #ifdef ARCH_SUPPORTS_KRETPROBES
 extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
 				   struct pt_regs *regs);
+extern int arch_trampoline_kprobe(struct kprobe *p);
 #else /* ARCH_SUPPORTS_KRETPROBES */
 static inline void arch_prepare_kretprobe(struct kretprobe *rp,
 					struct pt_regs *regs)
 {
 }
+static inline int arch_trampoline_kprobe(struct kprobe *p)
+{
+	return 0;
+}
 #endif /* ARCH_SUPPORTS_KRETPROBES */
 /*
  * Function-return probe -

commit 4c4308cb93450989846ac49faeb6dab943e7657e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 8 00:34:14 2007 -0700

    kprobes: kretprobes simplifications
    
     - consolidate duplicate code in all arch_prepare_kretprobe instances
       into common code
     - replace various odd helpers that use hlist_for_each_entry to get
       the first elemenet of a list with either a hlist_for_each_entry_save
       or an opencoded access to the first element in the caller
     - inline add_rp_inst into it's only remaining caller
     - use kretprobe_inst_table_head instead of opencoding it
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 850ee871e353..6fc623e41fd8 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -123,7 +123,8 @@ DECLARE_PER_CPU(struct kprobe *, current_kprobe);
 DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
 
 #ifdef ARCH_SUPPORTS_KRETPROBES
-extern void arch_prepare_kretprobe(struct kretprobe *rp, struct pt_regs *regs);
+extern void arch_prepare_kretprobe(struct kretprobe_instance *ri,
+				   struct pt_regs *regs);
 #else /* ARCH_SUPPORTS_KRETPROBES */
 static inline void arch_prepare_kretprobe(struct kretprobe *rp,
 					struct pt_regs *regs)
@@ -209,8 +210,6 @@ void jprobe_return(void);
 int register_kretprobe(struct kretprobe *rp);
 void unregister_kretprobe(struct kretprobe *rp);
 
-struct kretprobe_instance *get_free_rp_inst(struct kretprobe *rp);
-void add_rp_inst(struct kretprobe_instance *ri);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 #else /* CONFIG_KPROBES */

commit 0f95b7fc839bc3272b1bf2325d8748a649bd3534
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Tue May 8 00:28:27 2007 -0700

    Kprobes: print details of kretprobe on assertion failure
    
    In certain cases like when the real return address can't be found or when
    the number of tracked calls to a kretprobed function is less than the
    number of returns, we may not be able to find the correct return address
    after processing a kretprobe.  Currently we just do a BUG_ON, but no
    information is provided about the actual failing kretprobe.
    
    Print out details of the kretprobe before calling BUG().
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Maneesh Soni <maneesh@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index f26460700379..850ee871e353 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -157,6 +157,16 @@ struct kretprobe_instance {
 	struct task_struct *task;
 };
 
+static inline void kretprobe_assert(struct kretprobe_instance *ri,
+	unsigned long orig_ret_address, unsigned long trampoline_address)
+{
+	if (!orig_ret_address || (orig_ret_address == trampoline_address)) {
+		printk("kretprobe BUG!: Processing kretprobe %p @ %p\n",
+				ri->rp, ri->rp->kp.addr);
+		BUG();
+	}
+}
+
 extern spinlock_t kretprobe_lock;
 extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);

commit 9b3af29bf33bfe08c604769632799d27d56ae103
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Tue May 8 00:26:23 2007 -0700

    Kprobes: Make kprobe.symbol_name const
    
    Kprobes doesn't scribble the kprobe.symbol_name field.  Its only set by the
    module when registering the probe.  Modules that exercise good hygiene
    using the "const" qualifier will see warnings...
    
            warning: assignment discards qualifiers from pointer target type
    
    Make struct kprobe.symbol_name const char *
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Jim Keniston <jkenisto@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 769be39b9681..f26460700379 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -78,7 +78,7 @@ struct kprobe {
 	kprobe_opcode_t *addr;
 
 	/* Allow user to indicate symbol name of the probe point */
-	char *symbol_name;
+	const char *symbol_name;
 
 	/* Offset into the symbol */
 	unsigned int offset;

commit b4c6c34a530b4d1c626f4ac0a884e0a9b849378c
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Wed Dec 6 20:38:11 2006 -0800

    [PATCH] kprobes: enable booster on the preemptible kernel
    
    When we are unregistering a kprobe-booster, we can't release its
    instruction buffer immediately on the preemptive kernel, because some
    processes might be preempted on the buffer.  The freeze_processes() and
    thaw_processes() functions can clean most of processes up from the buffer.
    There are still some non-frozen threads who have the PF_NOFREEZE flag.  If
    those threads are sleeping (not preempted) at the known place outside the
    buffer, we can ensure safety of freeing.
    
    However, the processing of this check routine takes a long time.  So, this
    patch introduces the garbage collection mechanism of insn_slot.  It also
    introduces the "dirty" flag to free_insn_slot because of efficiency.
    
    The "clean" instruction slots (dirty flag is cleared) are released
    immediately.  But the "dirty" slots which are used by boosted kprobes, are
    marked as garbages.  collect_garbage_slots() will be invoked to release
    "dirty" slots if there are more than INSNS_PER_PAGE garbage slots or if
    there are no unused slots.
    
    Cc: "Keshavamurthy, Anil S" <anil.s.keshavamurthy@intel.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "bibo,mao" <bibo.mao@intel.com>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Yumiko Sugita <yumiko.sugita.yf@hitachi.com>
    Cc: Satoshi Oshima <soshima@redhat.com>
    Cc: Hideo Aoki <haoki@redhat.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index ac4c0559f751..769be39b9681 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -165,7 +165,7 @@ extern void arch_disarm_kprobe(struct kprobe *p);
 extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
-extern void free_insn_slot(kprobe_opcode_t *slot);
+extern void free_insn_slot(kprobe_opcode_t *slot, int dirty);
 extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */

commit 99219a3fbc2dcf2eaa954f7b2ac27299fd7894cd
Author: bibo,mao <bibo.mao@intel.com>
Date:   Mon Oct 2 02:17:35 2006 -0700

    [PATCH] kretprobe spinlock deadlock patch
    
    kprobe_flush_task() possibly calls kfree function during holding
    kretprobe_lock spinlock, if kfree function is probed by kretprobe that will
    incur spinlock deadlock.  This patch moves kfree function out scope of
    kretprobe_lock.
    
    Signed-off-by: bibo, mao <bibo.mao@intel.com>
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index a5c5a0cb0d5c..ac4c0559f751 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -202,7 +202,7 @@ void unregister_kretprobe(struct kretprobe *rp);
 struct kretprobe_instance *get_free_rp_inst(struct kretprobe *rp);
 void add_rp_inst(struct kretprobe_instance *ri);
 void kprobe_flush_task(struct task_struct *tk);
-void recycle_rp_inst(struct kretprobe_instance *ri);
+void recycle_rp_inst(struct kretprobe_instance *ri, struct hlist_head *head);
 #else /* CONFIG_KPROBES */
 
 #define __kprobes	/**/

commit 3a872d89baae821a0f6e2c1055d4b47650661137
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Oct 2 02:17:30 2006 -0700

    [PATCH] Kprobes: Make kprobe modules more portable
    
    In an effort to make kprobe modules more portable, here is a patch that:
    
    o Introduces the "symbol_name" field to struct kprobe.
      The symbol->address resolution now happens in the kernel in an
      architecture agnostic manner. 64-bit powerpc users no longer have
      to specify the ".symbols"
    o Introduces the "offset" field to struct kprobe to allow a user to
      specify an offset into a symbol.
    o The legacy mechanism of specifying the kprobe.addr is still supported.
      However, if both the kprobe.addr and kprobe.symbol_name are specified,
      probe registration fails with an -EINVAL.
    o The symbol resolution code uses kallsyms_lookup_name(). So
      CONFIG_KPROBES now depends on CONFIG_KALLSYMS
    o Apparantly kprobe modules were the only legitimate out-of-tree user of
      the kallsyms_lookup_name() EXPORT. Now that the symbol resolution
      happens in-kernel, remove the EXPORT as suggested by Christoph Hellwig
    o Modify tcp_probe.c that uses the kprobe interface so as to make it
      work on multiple platforms (in its earlier form, the code wouldn't
      work, say, on powerpc)
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 8bf6702da2a0..a5c5a0cb0d5c 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -77,6 +77,12 @@ struct kprobe {
 	/* location of the probe point */
 	kprobe_opcode_t *addr;
 
+	/* Allow user to indicate symbol name of the probe point */
+	char *symbol_name;
+
+	/* Offset into the symbol */
+	unsigned int offset;
+
 	/* Called before addr is executed. */
 	kprobe_pre_handler_t pre_handler;
 

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 778adc0fa640..8bf6702da2a0 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -29,7 +29,6 @@
  *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
  *		<prasanna@in.ibm.com> added function-return probes.
  */
-#include <linux/config.h>
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>

commit 7a7d1cf95408863a657035701606b13644c9f55e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 23 03:00:35 2006 -0800

    [PATCH] sem2mutex: kprobes
    
    Semaphore to mutex conversion.
    
    The conversion was generated via scripts, and the result was validated
    automatically via a script as well.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 669756bc20a2..778adc0fa640 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -36,6 +36,7 @@
 #include <linux/percpu.h>
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
+#include <linux/mutex.h>
 
 #ifdef CONFIG_KPROBES
 #include <asm/kprobes.h>
@@ -152,7 +153,7 @@ struct kretprobe_instance {
 };
 
 extern spinlock_t kretprobe_lock;
-extern struct semaphore kprobe_mutex;
+extern struct mutex kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);

commit df019b1d8b893d0f0ee5a9b0f71486f0892561ae
Author: Keshavamurthy Anil S <anil.s.keshavamurthy@intel.com>
Date:   Wed Jan 11 12:17:41 2006 -0800

    [PATCH] kprobes: fix unloading of self probed module
    
    When a kprobes modules is written in such a way that probes are inserted on
    itself, then unload of that moudle was not possible due to reference
    couning on the same module.
    
    The below patch makes a check and incrementes the module refcount only if
    it is not a self probed module.
    
    We need to allow modules to probe themself for kprobes performance
    measurements
    
    This patch has been tested on several x86_64, ppc64 and IA64 architectures.
    
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 10005bc92a31..669756bc20a2 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -68,6 +68,9 @@ struct kprobe {
 	/* list of kprobes for multi-handler support */
 	struct list_head list;
 
+	/* Indicates that the corresponding module has been ref counted */
+	unsigned int mod_refcounted;
+
 	/*count the number of times this probe was temporarily disarmed */
 	unsigned long nmissed;
 

commit 0498b63504f818e5ab39c818cd6f7b41319a1187
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Jan 9 20:52:46 2006 -0800

    [PATCH] kprobes: fix build breakage
    
    The following patch (against 2.6.15-rc5-mm3) fixes a kprobes build break
    due to changes introduced in the kprobe locking in 2.6.15-rc5-mm3.  In
    addition, the patch reverts back the open-coding of kprobe_mutex.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Acked-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 59bf240cdb9d..10005bc92a31 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -149,6 +149,7 @@ struct kretprobe_instance {
 };
 
 extern spinlock_t kretprobe_lock;
+extern struct semaphore kprobe_mutex;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);

commit e597c2984c64609c6e1e1ac803f00f7550705860
Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
Date:   Mon Jan 9 20:52:45 2006 -0800

    [PATCH] kprobes: arch_remove_kprobe
    
    Currently arch_remove_kprobes() is only implemented/required for x86_64 and
    powerpc.  All other architecture like IA64, i386 and sparc64 implementes a
    dummy function which is being called from arch independent kprobes.c file.
    
    This patch removes the dummy functions and replaces it with
    #define arch_remove_kprobe(p, s)        do { } while(0)
    
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index ad6e4fe970fd..59bf240cdb9d 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -152,7 +152,6 @@ extern spinlock_t kretprobe_lock;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
-extern void arch_remove_kprobe(struct kprobe *p);
 extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);

commit 49a2a1b83ba6fa40c41968d6a28ba16e7ed0c3f7
Author: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
Date:   Mon Jan 9 20:52:43 2006 -0800

    [PATCH] kprobes: changed from using spinlock to mutex
    
    Since Kprobes runtime exception handlers is now lock free as this code path is
    now using RCU to walk through the list, there is no need for the
    register/unregister{_kprobe} to use spin_{lock/unlock}_isr{save/restore}.  The
    serialization during registration/unregistration is now possible using just a
    mutex.
    
    In the above process, this patch also fixes a minor memory leak for x86_64 and
    powerpc.
    
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index c03f2dc933de..ad6e4fe970fd 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -150,7 +150,6 @@ struct kretprobe_instance {
 
 extern spinlock_t kretprobe_lock;
 extern int arch_prepare_kprobe(struct kprobe *p);
-extern void arch_copy_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
 extern void arch_remove_kprobe(struct kprobe *p);

commit bf8d5c52c3b6b27061e3b7d779057fd9a6cac164
Author: Keshavamurthy Anil S <anil.s.keshavamurthy@intel.com>
Date:   Mon Dec 12 00:37:34 2005 -0800

    [PATCH] kprobes: increment kprobe missed count for multiprobes
    
    When multiple probes are registered at the same address and if due to some
    recursion (probe getting triggered within a probe handler), we skip calling
    pre_handlers and just increment nmissed field.
    
    The below patch make sure it walks the list for multiple probes case.
    Without the below patch we get incorrect results of nmissed count for
    multiple probe case.
    
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 434ecfd7cf48..c03f2dc933de 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -158,6 +158,7 @@ extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot);
+extern void kprobes_inc_nmissed_count(struct kprobe *p);
 
 /* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);

commit 00d7c05ab168c10f9b520e07400923267bc04419
Author: Keshavamurthy Anil S <anil.s.keshavamurthy@intel.com>
Date:   Mon Dec 12 00:37:33 2005 -0800

    [PATCH] kprobes: no probes on critical path
    
    For Kprobes critical path is the path from debug break exception handler
    till the control reaches kprobes exception code.  No probes can be
    supported in this path as we will end up in recursion.
    
    This patch prevents this by moving the below function to safe __kprobes
    section onto which no probes can be inserted.
    
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e373c4a9de53..434ecfd7cf48 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -37,6 +37,7 @@
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
 
+#ifdef CONFIG_KPROBES
 #include <asm/kprobes.h>
 
 /* kprobe_status settings */
@@ -147,7 +148,6 @@ struct kretprobe_instance {
 	struct task_struct *task;
 };
 
-#ifdef CONFIG_KPROBES
 extern spinlock_t kretprobe_lock;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_copy_kprobe(struct kprobe *p);
@@ -195,6 +195,11 @@ void add_rp_inst(struct kretprobe_instance *ri);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri);
 #else /* CONFIG_KPROBES */
+
+#define __kprobes	/**/
+struct jprobe;
+struct kretprobe;
+
 static inline struct kprobe *kprobe_running(void)
 {
 	return NULL;

commit d217d5450f11d8c907c0458d175b0dc999b4d06d
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Nov 7 01:00:14 2005 -0800

    [PATCH] Kprobes: preempt_disable/enable() simplification
    
    Reorganize the preempt_disable/enable calls to eliminate the extra preempt
    depth.  Changes based on Paul McKenney's review suggestions for the kprobes
    RCU changeset.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index cff281cf70cf..e373c4a9de53 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -159,7 +159,7 @@ extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot);
 
-/* Get the kprobe at this addr (if any) - called under a rcu_read_lock() */
+/* Get the kprobe at this addr (if any) - called with preemption disabled */
 struct kprobe *get_kprobe(void *addr);
 struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 

commit 3516a46042508a495fac13c2e73530d936ebe015
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Nov 7 01:00:13 2005 -0800

    [PATCH] Kprobes: Use RCU for (un)register synchronization - base changes
    
    Changes to the base kprobes infrastructure to use RCU for synchronization
    during kprobe registration and unregistration.  These changes coupled with the
    arch kprobe changes (next in series):
    
    a. serialize registration and unregistration of kprobes.
    b. enable lockless execution of handlers. Handlers can now run in parallel.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 6720305a31e8..cff281cf70cf 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -34,6 +34,8 @@
 #include <linux/notifier.h>
 #include <linux/smp.h>
 #include <linux/percpu.h>
+#include <linux/spinlock.h>
+#include <linux/rcupdate.h>
 
 #include <asm/kprobes.h>
 
@@ -146,10 +148,7 @@ struct kretprobe_instance {
 };
 
 #ifdef CONFIG_KPROBES
-/* Locks kprobe: irq must be disabled */
-void lock_kprobes(void);
-void unlock_kprobes(void);
-
+extern spinlock_t kretprobe_lock;
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_copy_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
@@ -160,7 +159,7 @@ extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot);
 
-/* Get the kprobe at this addr (if any).  Must have called lock_kprobes */
+/* Get the kprobe at this addr (if any) - called under a rcu_read_lock() */
 struct kprobe *get_kprobe(void *addr);
 struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 

commit e65845235c8120be63001fc1a4ac00c819194bbe
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Nov 7 01:00:07 2005 -0800

    [PATCH] Kprobes: Track kprobe on a per_cpu basis - base changes
    
    Changes to the base kprobe infrastructure to track kprobe execution on a
    per-cpu basis.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e30afdca7917..6720305a31e8 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -33,6 +33,7 @@
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/percpu.h>
 
 #include <asm/kprobes.h>
 
@@ -106,6 +107,9 @@ struct jprobe {
 	kprobe_opcode_t *entry;	/* probe handling code to jump to */
 };
 
+DECLARE_PER_CPU(struct kprobe *, current_kprobe);
+DECLARE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+
 #ifdef ARCH_SUPPORTS_KRETPROBES
 extern void arch_prepare_kretprobe(struct kretprobe *rp, struct pt_regs *regs);
 #else /* ARCH_SUPPORTS_KRETPROBES */
@@ -146,13 +150,6 @@ struct kretprobe_instance {
 void lock_kprobes(void);
 void unlock_kprobes(void);
 
-/* kprobe running now on this CPU? */
-static inline int kprobe_running(void)
-{
-	extern unsigned int kprobe_cpu;
-	return kprobe_cpu == smp_processor_id();
-}
-
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_copy_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
@@ -167,6 +164,22 @@ extern void free_insn_slot(kprobe_opcode_t *slot);
 struct kprobe *get_kprobe(void *addr);
 struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 
+/* kprobe_running() will just return the current_kprobe on this CPU */
+static inline struct kprobe *kprobe_running(void)
+{
+	return (__get_cpu_var(current_kprobe));
+}
+
+static inline void reset_current_kprobe(void)
+{
+	__get_cpu_var(current_kprobe) = NULL;
+}
+
+static inline struct kprobe_ctlblk *get_kprobe_ctlblk(void)
+{
+	return (&__get_cpu_var(kprobe_ctlblk));
+}
+
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
 int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
@@ -183,9 +196,9 @@ void add_rp_inst(struct kretprobe_instance *ri);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri);
 #else /* CONFIG_KPROBES */
-static inline int kprobe_running(void)
+static inline struct kprobe *kprobe_running(void)
 {
-	return 0;
+	return NULL;
 }
 static inline int register_kprobe(struct kprobe *p)
 {

commit d0aaff9796c3310326d10da44fc0faed352a1d29
Author: Prasanna S Panchamukhi <prasanna@in.ibm.com>
Date:   Tue Sep 6 15:19:26 2005 -0700

    [PATCH] Kprobes: prevent possible race conditions generic
    
    There are possible race conditions if probes are placed on routines within the
    kprobes files and routines used by the kprobes.  For example if you put probe
    on get_kprobe() routines, the system can hang while inserting probes on any
    routine such as do_fork().  Because while inserting probes on do_fork(),
    register_kprobes() routine grabs the kprobes spin lock and executes
    get_kprobe() routine and to handle probe of get_kprobe(), kprobes_handler()
    gets executed and tries to grab kprobes spin lock, and spins forever.  This
    patch avoids such possible race conditions by preventing probes on routines
    within the kprobes file and routines used by kprobes.
    
    I have modified the patches as per Andi Kleen's suggestion to move kprobes
    routines and other routines used by kprobes to a seperate section
    .kprobes.text.
    
    Also moved page fault and exception handlers, general protection fault to
    .kprobes.text section.
    
    These patches have been tested on i386, x86_64 and ppc64 architectures, also
    compiled on ia64 and sparc64 architectures.
    
    Signed-off-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index e050fc2d4c26..e30afdca7917 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -42,6 +42,9 @@
 #define KPROBE_REENTER		0x00000004
 #define KPROBE_HIT_SSDONE	0x00000008
 
+/* Attach to insert probes on any functions which should be ignored*/
+#define __kprobes	__attribute__((__section__(".kprobes.text")))
+
 struct kprobe;
 struct pt_regs;
 struct kretprobe;

commit 6772926bef3c9f0ec761b39e5702535471fff70b
Author: Rusty Lynch <rusty.lynch@intel.com>
Date:   Tue Jul 5 18:54:50 2005 -0700

    [PATCH] kprobes: fix namespace problem and sparc64 build
    
    The following renames arch_init, a kprobes function for performing any
    architecture specific initialization, to arch_init_kprobes in order to
    cleanup the namespace.
    
    Also, this patch adds arch_init_kprobes to sparc64 to fix the sparc64 kprobes
    build from the last return probe patch.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index b7a194c4362a..e050fc2d4c26 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -155,7 +155,7 @@ extern void arch_copy_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
 extern void arch_remove_kprobe(struct kprobe *p);
-extern int arch_init(void);
+extern int arch_init_kprobes(void);
 extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot);

commit 802eae7c800fb7f583e6c06afa363585af2bef00
Author: Rusty Lynch <rusty.lynch@intel.com>
Date:   Mon Jun 27 15:17:08 2005 -0700

    [PATCH] Return probe redesign: architecture independent changes
    
    The following is the second version of the function return probe patches
    I sent out earlier this week.  Changes since my last submission include:
    
    * Fix in ppc64 code removing an unneeded call to re-enable preemption
    * Fix a build problem in ia64 when kprobes was turned off
    * Added another BUG_ON check to each of the architecture trampoline
      handlers
    
    My initial patch description ==>
    
     From my experiences with adding return probes to x86_64 and ia64, and the
    feedback on LKML to those patches, I think we can simplify the design
    for return probes.
    
    The following patch tweaks the original design such that:
    
    * Instead of storing the stack address in the return probe instance, the
      task pointer is stored.  This gives us all we need in order to:
        - find the correct return probe instance when we enter the trampoline
          (even if we are recursing)
        - find all left-over return probe instances when the task is going away
    
      This has the side effect of simplifying the implementation since more
      work can be done in kernel/kprobes.c since architecture specific knowledge
      of the stack layout is no longer required.  Specifically, we no longer have:
            - arch_get_kprobe_task()
            - arch_kprobe_flush_task()
            - get_rp_inst_tsk()
            - get_rp_inst()
            - trampoline_post_handler() <see next bullet>
    
    * Instead of splitting the return probe handling and cleanup logic across
      the pre and post trampoline handlers, all the work is pushed into the
      pre function (trampoline_probe_handler), and then we skip single stepping
      the original function.  In this case the original instruction to be single
      stepped was just a NOP, and we can do without the extra interruption.
    
    The new flow of events to having a return probe handler execute when a target
    function exits is:
    
    * At system initialization time, a kprobe is inserted at the beginning of
      kretprobe_trampoline.  kernel/kprobes.c use to handle this on it's own,
      but ia64 needed to do this a little differently (i.e. a function pointer
      is really a pointer to a structure containing the instruction pointer and
      a global pointer), so I added the notion of arch_init(), so that
      kernel/kprobes.c:init_kprobes() now allows architecture specific
      initialization by calling arch_init() before exiting.  Each architecture
      now registers a kprobe on it's own trampoline function.
    
    * register_kretprobe() will insert a kprobe at the beginning of the targeted
      function with the kprobe pre_handler set to arch_prepare_kretprobe
      (still no change)
    
    * When the target function is entered, the kprobe is fired, calling
      arch_prepare_kretprobe (still no change)
    
    * In arch_prepare_kretprobe() we try to get a free instance and if one is
      available then we fill out the instance with a pointer to the return probe,
      the original return address, and a pointer to the task structure (instead
      of the stack address.)  Just like before we change the return address
      to the trampoline function and mark the instance as used.
    
      If multiple return probes are registered for a given target function,
      then arch_prepare_kretprobe() will get called multiple times for the same
      task (since our kprobe implementation is able to handle multiple kprobes
      at the same address.)  Past the first call to arch_prepare_kretprobe,
      we end up with the original address stored in the return probe instance
      pointing to our trampoline function. (This is a significant difference
      from the original arch_prepare_kretprobe design.)
    
    * Target function executes like normal and then returns to kretprobe_trampoline.
    
    * kprobe inserted on the first instruction of kretprobe_trampoline is fired
      and calls trampoline_probe_handler() (no change here)
    
    * trampoline_probe_handler() consumes each of the instances associated with
      the current task by calling the registered handler function and marking
      the instance as unused until an instance is found that has a return address
      different then the trampoline function.
    
      (change similar to my previous ia64 RFC)
    
    * If the task is killed with some left-over return probe instances (meaning
      that a target function was entered, but never returned), then we just
      free any instances associated with the task.  (Not much different other
      then we can handle this without calling architecture specific functions.)
    
      There is a known problem that this patch does not yet solve where
      registering a return probe flush_old_exec or flush_thread will put us
      in a bad state.  Most likely the best way to handle this is to not allow
      registering return probes on these two functions.
    
      (Significant change)
    
    This patch series applies to the 2.6.12-rc6-mm1 kernel, and provides:
      * kernel/kprobes.c changes
      * i386 patch of existing return probes implementation
      * x86_64 patch of existing return probe implementation
      * ia64 implementation
      * ppc64 implementation (provided by Ananth)
    
    This patch implements the architecture independant changes for a reworking
    of the kprobes based function return probes design. Changes include:
    
      * Removing functions for querying a return probe instance off a stack address
      * Removing the stack_addr field from the kretprobe_instance definition,
        and adding a task pointer
      * Adding architecture specific initialization via arch_init()
      * Removing extern definitions for the architecture trampoline functions
        (this isn't needed anymore since the architecture handles the
         initialization of the kprobe in the return probe trampoline function.)
    
    Signed-off-by: Rusty Lynch <rusty.lynch@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index d304d4579856..b7a194c4362a 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -104,33 +104,12 @@ struct jprobe {
 };
 
 #ifdef ARCH_SUPPORTS_KRETPROBES
-extern int trampoline_probe_handler(struct kprobe *p, struct pt_regs *regs);
-extern void trampoline_post_handler(struct kprobe *p, struct pt_regs *regs,
-							unsigned long flags);
-extern struct task_struct *arch_get_kprobe_task(void *ptr);
 extern void arch_prepare_kretprobe(struct kretprobe *rp, struct pt_regs *regs);
-extern void arch_kprobe_flush_task(struct task_struct *tk);
 #else /* ARCH_SUPPORTS_KRETPROBES */
-static inline void kretprobe_trampoline(void)
-{
-}
-static inline int trampoline_probe_handler(struct kprobe *p,
-						struct pt_regs *regs)
-{
-	return 0;
-}
-static inline void trampoline_post_handler(struct kprobe *p,
-				struct pt_regs *regs, unsigned long flags)
-{
-}
 static inline void arch_prepare_kretprobe(struct kretprobe *rp,
 					struct pt_regs *regs)
 {
 }
-static inline void arch_kprobe_flush_task(struct task_struct *tk)
-{
-}
-#define arch_get_kprobe_task(ptr) ((struct task_struct *)NULL)
 #endif /* ARCH_SUPPORTS_KRETPROBES */
 /*
  * Function-return probe -
@@ -155,8 +134,8 @@ struct kretprobe_instance {
 	struct hlist_node uflist; /* either on free list or used list */
 	struct hlist_node hlist;
 	struct kretprobe *rp;
-	void *ret_addr;
-	void *stack_addr;
+	kprobe_opcode_t *ret_addr;
+	struct task_struct *task;
 };
 
 #ifdef CONFIG_KPROBES
@@ -176,6 +155,7 @@ extern void arch_copy_kprobe(struct kprobe *p);
 extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
 extern void arch_remove_kprobe(struct kprobe *p);
+extern int arch_init(void);
 extern void show_registers(struct pt_regs *regs);
 extern kprobe_opcode_t *get_insn_slot(void);
 extern void free_insn_slot(kprobe_opcode_t *slot);
@@ -196,8 +176,6 @@ int register_kretprobe(struct kretprobe *rp);
 void unregister_kretprobe(struct kretprobe *rp);
 
 struct kretprobe_instance *get_free_rp_inst(struct kretprobe *rp);
-struct kretprobe_instance *get_rp_inst(void *sara);
-struct kretprobe_instance *get_rp_inst_tsk(struct task_struct *tk);
 void add_rp_inst(struct kretprobe_instance *ri);
 void kprobe_flush_task(struct task_struct *tk);
 void recycle_rp_inst(struct kretprobe_instance *ri);

commit 9ec4b1f356b3bad928ae8e2aa9caebfa737d52df
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Mon Jun 27 15:17:01 2005 -0700

    [PATCH] kprobes: fix single-step out of line - take2
    
    Now that PPC64 has no-execute support, here is a second try to fix the
    single step out of line during kprobe execution.  Kprobes on x86_64 already
    solved this problem by allocating an executable page and using it as the
    scratch area for stepping out of line.  Reuse that.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 5e1a7b0d7b3f..d304d4579856 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -177,6 +177,8 @@ extern void arch_arm_kprobe(struct kprobe *p);
 extern void arch_disarm_kprobe(struct kprobe *p);
 extern void arch_remove_kprobe(struct kprobe *p);
 extern void show_registers(struct pt_regs *regs);
+extern kprobe_opcode_t *get_insn_slot(void);
+extern void free_insn_slot(kprobe_opcode_t *slot);
 
 /* Get the kprobe at this addr (if any).  Must have called lock_kprobes */
 struct kprobe *get_kprobe(void *addr);

commit ea32c65cc2d2294c04e9f81d0578a6f51febfdbf
Author: Prasanna S Panchamukhi <prasanna@in.ibm.com>
Date:   Thu Jun 23 00:09:36 2005 -0700

    [PATCH] kprobes: Temporary disarming of reentrant probe
    
    In situations where a kprobes handler calls a routine which has a probe on it,
    then kprobes_handler() disarms the new probe forever.  This patch removes the
    above limitation by temporarily disarming the new probe.  When the another
    probe hits while handling the old probe, the kprobes_handler() saves previous
    kprobes state and handles the new probe without calling the new kprobes
    registered handlers.  kprobe_post_handler() restores back the previous kprobes
    state and the normal execution continues.
    
    However on x86_64 architecture, re-rentrancy is provided only through
    pre_handler().  If a routine having probe is referenced through
    post_handler(), then the probes on that routine are disarmed forever, since
    the exception stack is gets changed after the processor single steps the
    instruction of the new probe.
    
    This patch includes generic changes to support temporary disarming on
    reentrancy of probes.
    
    Signed-of-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 461391decc46..5e1a7b0d7b3f 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -36,6 +36,12 @@
 
 #include <asm/kprobes.h>
 
+/* kprobe_status settings */
+#define KPROBE_HIT_ACTIVE	0x00000001
+#define KPROBE_HIT_SS		0x00000002
+#define KPROBE_REENTER		0x00000004
+#define KPROBE_HIT_SSDONE	0x00000008
+
 struct kprobe;
 struct pt_regs;
 struct kretprobe;
@@ -55,6 +61,9 @@ struct kprobe {
 	/* list of kprobes for multi-handler support */
 	struct list_head list;
 
+	/*count the number of times this probe was temporarily disarmed */
+	unsigned long nmissed;
+
 	/* location of the probe point */
 	kprobe_opcode_t *addr;
 

commit 0aa55e4d7db822059fe8132fe9f2b7773c48216c
Author: Hien Nguyen <hien@us.ibm.com>
Date:   Thu Jun 23 00:09:26 2005 -0700

    [PATCH] kprobes: moves lock-unlock to non-arch kprobe_flush_task
    
    This patch moves the lock/unlock of the arch specific kprobe_flush_task()
    to the non-arch specific kprobe_flusk_task().
    
    Signed-off-by: Hien Nguyen <hien@us.ibm.com>
    Acked-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 0f90466fb8b0..461391decc46 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -33,7 +33,6 @@
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
-#include <linux/spinlock.h>
 
 #include <asm/kprobes.h>
 
@@ -101,7 +100,7 @@ extern void trampoline_post_handler(struct kprobe *p, struct pt_regs *regs,
 							unsigned long flags);
 extern struct task_struct *arch_get_kprobe_task(void *ptr);
 extern void arch_prepare_kretprobe(struct kretprobe *rp, struct pt_regs *regs);
-extern void arch_kprobe_flush_task(struct task_struct *tk, spinlock_t *kp_lock);
+extern void arch_kprobe_flush_task(struct task_struct *tk);
 #else /* ARCH_SUPPORTS_KRETPROBES */
 static inline void kretprobe_trampoline(void)
 {

commit 7e1048b11c5afe79aac46a42e3ccec86b8365c6d
Author: Rusty Lynch <rusty.lynch@intel.com>
Date:   Thu Jun 23 00:09:25 2005 -0700

    [PATCH] Move kprobe [dis]arming into arch specific code
    
    The architecture independent code of the current kprobes implementation is
    arming and disarming kprobes at registration time.  The problem is that the
    code is assuming that arming and disarming is a just done by a simple write
    of some magic value to an address.  This is problematic for ia64 where our
    instructions look more like structures, and we can not insert break points
    by just doing something like:
    
    *p->addr = BREAKPOINT_INSTRUCTION;
    
    The following patch to 2.6.12-rc4-mm2 adds two new architecture dependent
    functions:
    
         * void arch_arm_kprobe(struct kprobe *p)
         * void arch_disarm_kprobe(struct kprobe *p)
    
    and then adds the new functions for each of the architectures that already
    implement kprobes (spar64/ppc64/i386/x86_64).
    
    I thought arch_[dis]arm_kprobe was the most descriptive of what was really
    happening, but each of the architectures already had a disarm_kprobe()
    function that was really a "disarm and do some other clean-up items as
    needed when you stumble across a recursive kprobe." So...  I took the
    liberty of changing the code that was calling disarm_kprobe() to call
    arch_disarm_kprobe(), and then do the cleanup in the block of code dealing
    with the recursive kprobe case.
    
    So far this patch as been tested on i386, x86_64, and ppc64, but still
    needs to be tested in sparc64.
    
    Signed-off-by: Rusty Lynch <rusty.lynch@intel.com>
    Signed-off-by: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index fba39f87efec..0f90466fb8b0 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -165,6 +165,8 @@ static inline int kprobe_running(void)
 
 extern int arch_prepare_kprobe(struct kprobe *p);
 extern void arch_copy_kprobe(struct kprobe *p);
+extern void arch_arm_kprobe(struct kprobe *p);
+extern void arch_disarm_kprobe(struct kprobe *p);
 extern void arch_remove_kprobe(struct kprobe *p);
 extern void show_registers(struct pt_regs *regs);
 

commit b94cce926b2b902b79380ccba370d6f9f2980de0
Author: Hien Nguyen <hien@us.ibm.com>
Date:   Thu Jun 23 00:09:19 2005 -0700

    [PATCH] kprobes: function-return probes
    
    This patch adds function-return probes to kprobes for the i386
    architecture.  This enables you to establish a handler to be run when a
    function returns.
    
    1. API
    
    Two new functions are added to kprobes:
    
            int register_kretprobe(struct kretprobe *rp);
            void unregister_kretprobe(struct kretprobe *rp);
    
    2. Registration and unregistration
    
    2.1 Register
    
      To register a function-return probe, the user populates the following
      fields in a kretprobe object and calls register_kretprobe() with the
      kretprobe address as an argument:
    
      kp.addr - the function's address
    
      handler - this function is run after the ret instruction executes, but
      before control returns to the return address in the caller.
    
      maxactive - The maximum number of instances of the probed function that
      can be active concurrently.  For example, if the function is non-
      recursive and is called with a spinlock or mutex held, maxactive = 1
      should be enough.  If the function is non-recursive and can never
      relinquish the CPU (e.g., via a semaphore or preemption), NR_CPUS should
      be enough.  maxactive is used to determine how many kretprobe_instance
      objects to allocate for this particular probed function.  If maxactive <=
      0, it is set to a default value (if CONFIG_PREEMPT maxactive=max(10, 2 *
      NR_CPUS) else maxactive=NR_CPUS)
    
      For example:
    
        struct kretprobe rp;
        rp.kp.addr = /* entrypoint address */
        rp.handler = /*return probe handler */
        rp.maxactive = /* e.g., 1 or NR_CPUS or 0, see the above explanation */
        register_kretprobe(&rp);
    
      The following field may also be of interest:
    
      nmissed - Initialized to zero when the function-return probe is
      registered, and incremented every time the probed function is entered but
      there is no kretprobe_instance object available for establishing the
      function-return probe (i.e., because maxactive was set too low).
    
    2.2 Unregister
    
      To unregiter a function-return probe, the user calls
      unregister_kretprobe() with the same kretprobe object as registered
      previously.  If a probed function is running when the return probe is
      unregistered, the function will return as expected, but the handler won't
      be run.
    
    3. Limitations
    
    3.1 This patch supports only the i386 architecture, but patches for
        x86_64 and ppc64 are anticipated soon.
    
    3.2 Return probes operates by replacing the return address in the stack
        (or in a known register, such as the lr register for ppc).  This may
        cause __builtin_return_address(0), when invoked from the return-probed
        function, to return the address of the return-probes trampoline.
    
    3.3 This implementation uses the "Multiprobes at an address" feature in
        2.6.12-rc3-mm3.
    
    3.4 Due to a limitation in multi-probes, you cannot currently establish
        a return probe and a jprobe on the same function.  A patch to remove
        this limitation is being tested.
    
    This feature is required by SystemTap (http://sourceware.org/systemtap),
    and reflects ideas contributed by several SystemTap developers, including
    Will Cohen and Ananth Mavinakayanahalli.
    
    Signed-off-by: Hien Nguyen <hien@us.ibm.com>
    Signed-off-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Frederik Deweerdt <frederik.deweerdt@laposte.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index 99ddba5a4e00..fba39f87efec 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -25,21 +25,31 @@
  *		Rusty Russell).
  * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes
  *		interface to access function arguments.
+ * 2005-May	Hien Nguyen <hien@us.ibm.com> and Jim Keniston
+ *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
+ *		<prasanna@in.ibm.com> added function-return probes.
  */
 #include <linux/config.h>
 #include <linux/list.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/spinlock.h>
+
 #include <asm/kprobes.h>
 
 struct kprobe;
 struct pt_regs;
+struct kretprobe;
+struct kretprobe_instance;
 typedef int (*kprobe_pre_handler_t) (struct kprobe *, struct pt_regs *);
 typedef int (*kprobe_break_handler_t) (struct kprobe *, struct pt_regs *);
 typedef void (*kprobe_post_handler_t) (struct kprobe *, struct pt_regs *,
 				       unsigned long flags);
 typedef int (*kprobe_fault_handler_t) (struct kprobe *, struct pt_regs *,
 				       int trapnr);
+typedef int (*kretprobe_handler_t) (struct kretprobe_instance *,
+				    struct pt_regs *);
+
 struct kprobe {
 	struct hlist_node hlist;
 
@@ -85,6 +95,62 @@ struct jprobe {
 	kprobe_opcode_t *entry;	/* probe handling code to jump to */
 };
 
+#ifdef ARCH_SUPPORTS_KRETPROBES
+extern int trampoline_probe_handler(struct kprobe *p, struct pt_regs *regs);
+extern void trampoline_post_handler(struct kprobe *p, struct pt_regs *regs,
+							unsigned long flags);
+extern struct task_struct *arch_get_kprobe_task(void *ptr);
+extern void arch_prepare_kretprobe(struct kretprobe *rp, struct pt_regs *regs);
+extern void arch_kprobe_flush_task(struct task_struct *tk, spinlock_t *kp_lock);
+#else /* ARCH_SUPPORTS_KRETPROBES */
+static inline void kretprobe_trampoline(void)
+{
+}
+static inline int trampoline_probe_handler(struct kprobe *p,
+						struct pt_regs *regs)
+{
+	return 0;
+}
+static inline void trampoline_post_handler(struct kprobe *p,
+				struct pt_regs *regs, unsigned long flags)
+{
+}
+static inline void arch_prepare_kretprobe(struct kretprobe *rp,
+					struct pt_regs *regs)
+{
+}
+static inline void arch_kprobe_flush_task(struct task_struct *tk)
+{
+}
+#define arch_get_kprobe_task(ptr) ((struct task_struct *)NULL)
+#endif /* ARCH_SUPPORTS_KRETPROBES */
+/*
+ * Function-return probe -
+ * Note:
+ * User needs to provide a handler function, and initialize maxactive.
+ * maxactive - The maximum number of instances of the probed function that
+ * can be active concurrently.
+ * nmissed - tracks the number of times the probed function's return was
+ * ignored, due to maxactive being too low.
+ *
+ */
+struct kretprobe {
+	struct kprobe kp;
+	kretprobe_handler_t handler;
+	int maxactive;
+	int nmissed;
+	struct hlist_head free_instances;
+	struct hlist_head used_instances;
+};
+
+struct kretprobe_instance {
+	struct hlist_node uflist; /* either on free list or used list */
+	struct hlist_node hlist;
+	struct kretprobe *rp;
+	void *ret_addr;
+	void *stack_addr;
+};
+
 #ifdef CONFIG_KPROBES
 /* Locks kprobe: irq must be disabled */
 void lock_kprobes(void);
@@ -104,6 +170,7 @@ extern void show_registers(struct pt_regs *regs);
 
 /* Get the kprobe at this addr (if any).  Must have called lock_kprobes */
 struct kprobe *get_kprobe(void *addr);
+struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);
 
 int register_kprobe(struct kprobe *p);
 void unregister_kprobe(struct kprobe *p);
@@ -113,7 +180,16 @@ int register_jprobe(struct jprobe *p);
 void unregister_jprobe(struct jprobe *p);
 void jprobe_return(void);
 
-#else
+int register_kretprobe(struct kretprobe *rp);
+void unregister_kretprobe(struct kretprobe *rp);
+
+struct kretprobe_instance *get_free_rp_inst(struct kretprobe *rp);
+struct kretprobe_instance *get_rp_inst(void *sara);
+struct kretprobe_instance *get_rp_inst_tsk(struct task_struct *tk);
+void add_rp_inst(struct kretprobe_instance *ri);
+void kprobe_flush_task(struct task_struct *tk);
+void recycle_rp_inst(struct kretprobe_instance *ri);
+#else /* CONFIG_KPROBES */
 static inline int kprobe_running(void)
 {
 	return 0;
@@ -135,5 +211,15 @@ static inline void unregister_jprobe(struct jprobe *p)
 static inline void jprobe_return(void)
 {
 }
-#endif
+static inline int register_kretprobe(struct kretprobe *rp)
+{
+	return -ENOSYS;
+}
+static inline void unregister_kretprobe(struct kretprobe *rp)
+{
+}
+static inline void kprobe_flush_task(struct task_struct *tk)
+{
+}
+#endif				/* CONFIG_KPROBES */
 #endif				/* _LINUX_KPROBES_H */

commit 64f562c6df3cfc5d1b2b4bdbcb7951457df9c237
Author: Ananth N Mavinakayanahalli <amavin@redhat.com>
Date:   Thu May 5 16:15:42 2005 -0700

    [PATCH] kprobes: Allow multiple kprobes at the same address
    
    Allow registration of multiple kprobes at an address in an architecture
    agnostic way.  Corresponding handlers will be invoked in a sequence.  But,
    a kprobe and a jprobe can't (yet) co-exist at the same address.
    
    Signed-off-by: Ananth N Mavinakayanahalli <amavin@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
index f20c163de4f5..99ddba5a4e00 100644
--- a/include/linux/kprobes.h
+++ b/include/linux/kprobes.h
@@ -43,6 +43,9 @@ typedef int (*kprobe_fault_handler_t) (struct kprobe *, struct pt_regs *,
 struct kprobe {
 	struct hlist_node hlist;
 
+	/* list of kprobes for multi-handler support */
+	struct list_head list;
+
 	/* location of the probe point */
 	kprobe_opcode_t *addr;
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/kprobes.h b/include/linux/kprobes.h
new file mode 100644
index 000000000000..f20c163de4f5
--- /dev/null
+++ b/include/linux/kprobes.h
@@ -0,0 +1,136 @@
+#ifndef _LINUX_KPROBES_H
+#define _LINUX_KPROBES_H
+/*
+ *  Kernel Probes (KProbes)
+ *  include/linux/kprobes.h
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) IBM Corporation, 2002, 2004
+ *
+ * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel
+ *		Probes initial implementation ( includes suggestions from
+ *		Rusty Russell).
+ * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes
+ *		interface to access function arguments.
+ */
+#include <linux/config.h>
+#include <linux/list.h>
+#include <linux/notifier.h>
+#include <linux/smp.h>
+#include <asm/kprobes.h>
+
+struct kprobe;
+struct pt_regs;
+typedef int (*kprobe_pre_handler_t) (struct kprobe *, struct pt_regs *);
+typedef int (*kprobe_break_handler_t) (struct kprobe *, struct pt_regs *);
+typedef void (*kprobe_post_handler_t) (struct kprobe *, struct pt_regs *,
+				       unsigned long flags);
+typedef int (*kprobe_fault_handler_t) (struct kprobe *, struct pt_regs *,
+				       int trapnr);
+struct kprobe {
+	struct hlist_node hlist;
+
+	/* location of the probe point */
+	kprobe_opcode_t *addr;
+
+	/* Called before addr is executed. */
+	kprobe_pre_handler_t pre_handler;
+
+	/* Called after addr is executed, unless... */
+	kprobe_post_handler_t post_handler;
+
+	/* ... called if executing addr causes a fault (eg. page fault).
+	 * Return 1 if it handled fault, otherwise kernel will see it. */
+	kprobe_fault_handler_t fault_handler;
+
+	/* ... called if breakpoint trap occurs in probe handler.
+	 * Return 1 if it handled break, otherwise kernel will see it. */
+	kprobe_break_handler_t break_handler;
+
+	/* Saved opcode (which has been replaced with breakpoint) */
+	kprobe_opcode_t opcode;
+
+	/* copy of the original instruction */
+	struct arch_specific_insn ainsn;
+};
+
+/*
+ * Special probe type that uses setjmp-longjmp type tricks to resume
+ * execution at a specified entry with a matching prototype corresponding
+ * to the probed function - a trick to enable arguments to become
+ * accessible seamlessly by probe handling logic.
+ * Note:
+ * Because of the way compilers allocate stack space for local variables
+ * etc upfront, regardless of sub-scopes within a function, this mirroring
+ * principle currently works only for probes placed on function entry points.
+ */
+struct jprobe {
+	struct kprobe kp;
+	kprobe_opcode_t *entry;	/* probe handling code to jump to */
+};
+
+#ifdef CONFIG_KPROBES
+/* Locks kprobe: irq must be disabled */
+void lock_kprobes(void);
+void unlock_kprobes(void);
+
+/* kprobe running now on this CPU? */
+static inline int kprobe_running(void)
+{
+	extern unsigned int kprobe_cpu;
+	return kprobe_cpu == smp_processor_id();
+}
+
+extern int arch_prepare_kprobe(struct kprobe *p);
+extern void arch_copy_kprobe(struct kprobe *p);
+extern void arch_remove_kprobe(struct kprobe *p);
+extern void show_registers(struct pt_regs *regs);
+
+/* Get the kprobe at this addr (if any).  Must have called lock_kprobes */
+struct kprobe *get_kprobe(void *addr);
+
+int register_kprobe(struct kprobe *p);
+void unregister_kprobe(struct kprobe *p);
+int setjmp_pre_handler(struct kprobe *, struct pt_regs *);
+int longjmp_break_handler(struct kprobe *, struct pt_regs *);
+int register_jprobe(struct jprobe *p);
+void unregister_jprobe(struct jprobe *p);
+void jprobe_return(void);
+
+#else
+static inline int kprobe_running(void)
+{
+	return 0;
+}
+static inline int register_kprobe(struct kprobe *p)
+{
+	return -ENOSYS;
+}
+static inline void unregister_kprobe(struct kprobe *p)
+{
+}
+static inline int register_jprobe(struct jprobe *p)
+{
+	return -ENOSYS;
+}
+static inline void unregister_jprobe(struct jprobe *p)
+{
+}
+static inline void jprobe_return(void)
+{
+}
+#endif
+#endif				/* _LINUX_KPROBES_H */
