commit 3aa91625007807bfca4155df1867a5c924a08662
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 29 15:03:56 2020 +0200

    dma-mapping: Add a new dma_need_sync API
    
    Add a new API to check if calls to dma_sync_single_for_{device,cpu} are
    required for a given DMA streaming mapping.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200629130359.2690853-2-hch@lst.de

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 78f677cf45ab..a33ed3954ed4 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -461,6 +461,7 @@ int dma_set_mask(struct device *dev, u64 mask);
 int dma_set_coherent_mask(struct device *dev, u64 mask);
 u64 dma_get_required_mask(struct device *dev);
 size_t dma_max_mapping_size(struct device *dev);
+bool dma_need_sync(struct device *dev, dma_addr_t dma_addr);
 unsigned long dma_get_merge_boundary(struct device *dev);
 #else /* CONFIG_HAS_DMA */
 static inline dma_addr_t dma_map_page_attrs(struct device *dev,
@@ -571,6 +572,10 @@ static inline size_t dma_max_mapping_size(struct device *dev)
 {
 	return 0;
 }
+static inline bool dma_need_sync(struct device *dev, dma_addr_t dma_addr)
+{
+	return false;
+}
 static inline unsigned long dma_get_merge_boundary(struct device *dev)
 {
 	return 0;

commit 6f2dc3d335457d9c815be9f4fd3dc8eff92fcef7
Merge: 1ee18de92927 48530d9fab0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 6 11:55:53 2020 -0700

    Merge tag 'dma-mapping-5.8-2' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping helpers from Christoph Hellwig:
     "These were in a separate stable branch so that various media and drm
      trees could pull the in for bug fixes, but looking at linux-next that
      hasn't actually happened yet. Still sending the APIs to you in the
      hope that these bug fixes get picked up for 5.8 in one way or another.
    
      Summary:
    
       - add DMA mapping helpers for struct sg_table (Marek Szyprowski)"
    
    * tag 'dma-mapping-5.8-2' of git://git.infradead.org/users/hch/dma-mapping:
      iommu: add generic helper for mapping sgtable objects
      scatterlist: add generic wrappers for iterating over sgtable objects
      dma-mapping: add generic helpers for mapping sgtable objects

commit d9d200bcebc1f6e56f0178cbb8db9953e8cc9a11
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed May 13 15:32:08 2020 +0200

    dma-mapping: add generic helpers for mapping sgtable objects
    
    struct sg_table is a common structure used for describing a memory
    buffer. It consists of a scatterlist with memory pages and DMA addresses
    (sgl entry), as well as the number of scatterlist entries: CPU pages
    (orig_nents entry) and DMA mapped pages (nents entry).
    
    It turned out that it was a common mistake to misuse nents and orig_nents
    entries, calling DMA-mapping functions with a wrong number of entries or
    ignoring the number of mapped entries returned by the dma_map_sg
    function.
    
    To avoid such issues, let's introduce a common wrappers operating
    directly on the struct sg_table objects, which take care of the proper
    use of the nents and orig_nents entries.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 330ad58fbf4d..936e30b86cd3 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -609,6 +609,86 @@ static inline void dma_sync_single_range_for_device(struct device *dev,
 	return dma_sync_single_for_device(dev, addr + offset, size, dir);
 }
 
+/**
+ * dma_map_sgtable - Map the given buffer for DMA
+ * @dev:	The device for which to perform the DMA operation
+ * @sgt:	The sg_table object describing the buffer
+ * @dir:	DMA direction
+ * @attrs:	Optional DMA attributes for the map operation
+ *
+ * Maps a buffer described by a scatterlist stored in the given sg_table
+ * object for the @dir DMA operation by the @dev device. After success the
+ * ownership for the buffer is transferred to the DMA domain.  One has to
+ * call dma_sync_sgtable_for_cpu() or dma_unmap_sgtable() to move the
+ * ownership of the buffer back to the CPU domain before touching the
+ * buffer by the CPU.
+ *
+ * Returns 0 on success or -EINVAL on error during mapping the buffer.
+ */
+static inline int dma_map_sgtable(struct device *dev, struct sg_table *sgt,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	int nents;
+
+	nents = dma_map_sg_attrs(dev, sgt->sgl, sgt->orig_nents, dir, attrs);
+	if (nents <= 0)
+		return -EINVAL;
+	sgt->nents = nents;
+	return 0;
+}
+
+/**
+ * dma_unmap_sgtable - Unmap the given buffer for DMA
+ * @dev:	The device for which to perform the DMA operation
+ * @sgt:	The sg_table object describing the buffer
+ * @dir:	DMA direction
+ * @attrs:	Optional DMA attributes for the unmap operation
+ *
+ * Unmaps a buffer described by a scatterlist stored in the given sg_table
+ * object for the @dir DMA operation by the @dev device. After this function
+ * the ownership of the buffer is transferred back to the CPU domain.
+ */
+static inline void dma_unmap_sgtable(struct device *dev, struct sg_table *sgt,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	dma_unmap_sg_attrs(dev, sgt->sgl, sgt->orig_nents, dir, attrs);
+}
+
+/**
+ * dma_sync_sgtable_for_cpu - Synchronize the given buffer for CPU access
+ * @dev:	The device for which to perform the DMA operation
+ * @sgt:	The sg_table object describing the buffer
+ * @dir:	DMA direction
+ *
+ * Performs the needed cache synchronization and moves the ownership of the
+ * buffer back to the CPU domain, so it is safe to perform any access to it
+ * by the CPU. Before doing any further DMA operations, one has to transfer
+ * the ownership of the buffer back to the DMA domain by calling the
+ * dma_sync_sgtable_for_device().
+ */
+static inline void dma_sync_sgtable_for_cpu(struct device *dev,
+		struct sg_table *sgt, enum dma_data_direction dir)
+{
+	dma_sync_sg_for_cpu(dev, sgt->sgl, sgt->orig_nents, dir);
+}
+
+/**
+ * dma_sync_sgtable_for_device - Synchronize the given buffer for DMA
+ * @dev:	The device for which to perform the DMA operation
+ * @sgt:	The sg_table object describing the buffer
+ * @dir:	DMA direction
+ *
+ * Performs the needed cache synchronization and moves the ownership of the
+ * buffer back to the DMA domain, so it is safe to perform the DMA operation.
+ * Once finished, one has to call dma_sync_sgtable_for_cpu() or
+ * dma_unmap_sgtable().
+ */
+static inline void dma_sync_sgtable_for_device(struct device *dev,
+		struct sg_table *sgt, enum dma_data_direction dir)
+{
+	dma_sync_sg_for_device(dev, sgt->sgl, sgt->orig_nents, dir);
+}
+
 #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
 #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
 #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)

commit c84dc6e68a1d2464e050d9694be4e4ff49e32bfd
Author: David Rientjes <rientjes@google.com>
Date:   Tue Apr 14 17:04:55 2020 -0700

    dma-pool: add additional coherent pools to map to gfp mask
    
    The single atomic pool is allocated from the lowest zone possible since
    it is guaranteed to be applicable for any DMA allocation.
    
    Devices may allocate through the DMA API but not have a strict reliance
    on GFP_DMA memory.  Since the atomic pool will be used for all
    non-blockable allocations, returning all memory from ZONE_DMA may
    unnecessarily deplete the zone.
    
    Provision for multiple atomic pools that will map to the optimal gfp
    mask of the device.
    
    When allocating non-blockable memory, determine the optimal gfp mask of
    the device and use the appropriate atomic pool.
    
    The coherent DMA mask will remain the same between allocation and free
    and, thus, memory will be freed to the same atomic pool it was allocated
    from.
    
    __dma_atomic_pool_init() will be changed to return struct gen_pool *
    later once dynamic expansion is added.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 330ad58fbf4d..b43116a6405d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -630,9 +630,9 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 			pgprot_t prot, const void *caller);
 void dma_common_free_remap(void *cpu_addr, size_t size);
 
-bool dma_in_atomic_pool(void *start, size_t size);
-void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
-bool dma_free_from_pool(void *start, size_t size);
+void *dma_alloc_from_pool(struct device *dev, size_t size,
+			  struct page **ret_page, gfp_t flags);
+bool dma_free_from_pool(struct device *dev, void *start, size_t size);
 
 int
 dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,

commit 81b6b96475ac7a4ebfceae9f16fb3758327adbfe
Merge: a308a7102215 a7ba70f1787f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 28 11:16:43 2019 -0800

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux; tag 'dma-mapping-5.5' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - improve dma-debug scalability (Eric Dumazet)
    
     - tiny dma-debug cleanup (Dan Carpenter)
    
     - check for vmap memory in dma_map_single (Kees Cook)
    
     - check for dma_addr_t overflows in dma-direct when using DMA offsets
       (Nicolas Saenz Julienne)
    
     - switch the x86 sta2x11 SOC to use more generic DMA code (Nicolas
       Saenz Julienne)
    
     - fix arm-nommu dma-ranges handling (Vladimir Murzin)
    
     - use __initdata in CMA (Shyam Saini)
    
     - replace the bus dma mask with a limit (Nicolas Saenz Julienne)
    
     - merge the remapping helpers into the main dma-direct flow (me)
    
     - switch xtensa to the generic dma remap handling (me)
    
     - various cleanups around dma_capable (me)
    
     - remove unused dev arguments to various dma-noncoherent helpers (me)
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux:
    
    * tag 'dma-mapping-5.5' of git://git.infradead.org/users/hch/dma-mapping: (22 commits)
      dma-mapping: treat dev->bus_dma_mask as a DMA limit
      dma-direct: exclude dma_direct_map_resource from the min_low_pfn check
      dma-direct: don't check swiotlb=force in dma_direct_map_resource
      dma-debug: clean up put_hash_bucket()
      powerpc: remove support for NULL dev in __phys_to_dma / __dma_to_phys
      dma-direct: avoid a forward declaration for phys_to_dma
      dma-direct: unify the dma_capable definitions
      dma-mapping: drop the dev argument to arch_sync_dma_for_*
      x86/PCI: sta2x11: use default DMA address translation
      dma-direct: check for overflows on 32 bit DMA addresses
      dma-debug: increase HASH_SIZE
      dma-debug: reorder struct dma_debug_entry fields
      xtensa: use the generic uncached segment support
      dma-mapping: merge the generic remapping helpers into dma-direct
      dma-direct: provide mmap and get_sgtable method overrides
      dma-direct: remove the dma_handle argument to __dma_direct_alloc_pages
      dma-direct: remove __dma_direct_free_pages
      usb: core: Remove redundant vmap checks
      kernel: dma-contiguous: mark CMA parameters __initdata/__initconst
      dma-debug: add a schedule point in debug_dma_dump_mappings()
      ...

commit a7ba70f1787f977f970cd116076c6fce4b9e01cc
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Thu Nov 21 10:26:44 2019 +0100

    dma-mapping: treat dev->bus_dma_mask as a DMA limit
    
    Using a mask to represent bus DMA constraints has a set of limitations.
    The biggest one being it can only hold a power of two (minus one). The
    DMA mapping code is already aware of this and treats dev->bus_dma_mask
    as a limit. This quirk is already used by some architectures although
    still rare.
    
    With the introduction of the Raspberry Pi 4 we've found a new contender
    for the use of bus DMA limits, as its PCIe bus can only address the
    lower 3GB of memory (of a total of 4GB). This is impossible to represent
    with a mask. To make things worse the device-tree code rounds non power
    of two bus DMA limits to the next power of two, which is unacceptable in
    this case.
    
    In the light of this, rename dev->bus_dma_mask to dev->bus_dma_limit all
    over the tree and treat it as such. Note that dev->bus_dma_limit should
    contain the higher accessible DMA address.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4d450672b7d6..c4d8741264bd 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -697,7 +697,7 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
  */
 static inline bool dma_addressing_limited(struct device *dev)
 {
-	return min_not_zero(dma_get_mask(dev), dev->bus_dma_mask) <
+	return min_not_zero(dma_get_mask(dev), dev->bus_dma_limit) <
 			    dma_get_required_mask(dev);
 }
 

commit 7283fff8b524b2f27438429aca458b232f5c5c8a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 13 08:32:13 2019 +0100

    dma-mapping: remove the DMA_ATTR_WRITE_BARRIER flag
    
    This flag is not implemented by any backend and only set by the ib_umem
    module in a single instance.
    
    Link: https://lore.kernel.org/r/20191113073214.9514-2-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4a1c4fca475a..8023071d6903 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -15,11 +15,8 @@
 /**
  * List of possible attributes associated with a DMA mapping. The semantics
  * of each attribute should be defined in Documentation/DMA-attributes.txt.
- *
- * DMA_ATTR_WRITE_BARRIER: DMA to a memory region with this attribute
- * forces all pending DMA writes to complete.
  */
-#define DMA_ATTR_WRITE_BARRIER		(1UL << 0)
+
 /*
  * DMA_ATTR_WEAK_ORDERING: Specifies that reads and writes to the mapping
  * may be weakly ordered, that is that reads and writes may pass each other.

commit 4544b9f25e70eae9f70a243de0cc802aa5c8cb69
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:34:22 2019 -0700

    dma-mapping: Add vmap checks to dma_map_single()
    
    As we've seen from USB and other areas[1], we need to always do runtime
    checks for DMA operating on memory regions that might be remapped. This
    adds vmap checks (similar to those already in USB but missing in other
    places) into dma_map_single() so all callers benefit from the checking.
    
    [1] https://git.kernel.org/linus/3840c5b78803b2b6cc1ff820100a74a092c40cbb
    
    Suggested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    [hch: fixed the printk message]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 10918c55003f..4d450672b7d6 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -583,6 +583,10 @@ static inline unsigned long dma_get_merge_boundary(struct device *dev)
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 		size_t size, enum dma_data_direction dir, unsigned long attrs)
 {
+	/* DMA must never operate on areas that might be remapped. */
+	if (dev_WARN_ONCE(dev, is_vmalloc_addr(ptr),
+			  "rejecting DMA map of vmalloc memory\n"))
+		return DMA_MAPPING_ERROR;
 	debug_dma_map_single(dev, ptr, size);
 	return dma_map_page_attrs(dev, virt_to_page(ptr), offset_in_page(ptr),
 			size, dir, attrs);

commit a445e940ea686fc60475564009821010eb213be3
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Wed Oct 30 10:13:13 2019 +0000

    dma-mapping: fix handling of dma-ranges for reserved memory (again)
    
    Daniele reported that issue previously fixed in c41f9ea998f3
    ("drivers: dma-coherent: Account dma_pfn_offset when used with device
    tree") reappear shortly after 43fc509c3efb ("dma-coherent: introduce
    interface for default DMA pool") where fix was accidentally dropped.
    
    Lets put fix back in place and respect dma-ranges for reserved memory.
    
    Fixes: 43fc509c3efb ("dma-coherent: introduce interface for default DMA pool")
    
    Reported-by: Daniele Alessandrelli <daniele.alessandrelli@gmail.com>
    Tested-by: Daniele Alessandrelli <daniele.alessandrelli@gmail.com>
    Tested-by: Alexandre Torgue <alexandre.torgue@st.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4a1c4fca475a..10918c55003f 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -162,7 +162,7 @@ int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr);
 int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,
 			    void *cpu_addr, size_t size, int *ret);
 
-void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle);
+void *dma_alloc_from_global_coherent(struct device *dev, ssize_t size, dma_addr_t *dma_handle);
 int dma_release_from_global_coherent(int order, void *vaddr);
 int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,
 				  size_t size, int *ret);
@@ -172,7 +172,7 @@ int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,
 #define dma_release_from_dev_coherent(dev, order, vaddr) (0)
 #define dma_mmap_from_dev_coherent(dev, vma, vaddr, order, ret) (0)
 
-static inline void *dma_alloc_from_global_coherent(ssize_t size,
+static inline void *dma_alloc_from_global_coherent(struct device *dev, ssize_t size,
 						   dma_addr_t *dma_handle)
 {
 	return NULL;

commit 671df189537883f36cf9c7d4f9495bfac0f86627
Merge: c9fe5630dae1 c7d9eccb3c1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 19 13:27:23 2019 -0700

    Merge tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - add dma-mapping and block layer helpers to take care of IOMMU merging
       for mmc plus subsequent fixups (Yoshihiro Shimoda)
    
     - rework handling of the pgprot bits for remapping (me)
    
     - take care of the dma direct infrastructure for swiotlb-xen (me)
    
     - improve the dma noncoherent remapping infrastructure (me)
    
     - better defaults for ->mmap, ->get_sgtable and ->get_required_mask
       (me)
    
     - cleanup mmaping of coherent DMA allocations (me)
    
     - various misc cleanups (Andy Shevchenko, me)
    
    * tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping: (41 commits)
      mmc: renesas_sdhi_internal_dmac: Add MMC_CAP2_MERGE_CAPABLE
      mmc: queue: Fix bigger segments usage
      arm64: use asm-generic/dma-mapping.h
      swiotlb-xen: merge xen_unmap_single into xen_swiotlb_unmap_page
      swiotlb-xen: simplify cache maintainance
      swiotlb-xen: use the same foreign page check everywhere
      swiotlb-xen: remove xen_swiotlb_dma_mmap and xen_swiotlb_dma_get_sgtable
      xen: remove the exports for xen_{create,destroy}_contiguous_region
      xen/arm: remove xen_dma_ops
      xen/arm: simplify dma_cache_maint
      xen/arm: use dev_is_dma_coherent
      xen/arm: consolidate page-coherent.h
      xen/arm: use dma-noncoherent.h calls for xen-swiotlb cache maintainance
      arm: remove wrappers for the generic dma remap helpers
      dma-mapping: introduce a dma_common_find_pages helper
      dma-mapping: always use VM_DMA_COHERENT for generic DMA remap
      vmalloc: lift the arm flag for coherent mappings to common code
      dma-mapping: provide a better default ->get_required_mask
      dma-mapping: remove the dma_declare_coherent_memory export
      remoteproc: don't allow modular build
      ...

commit 5cf4537975bbd5691b9ddd015d540bb92f61e322
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 3 09:14:31 2019 +0200

    dma-mapping: introduce a dma_common_find_pages helper
    
    A helper to find the backing page array based on a virtual address.
    This also ensures we do the same vm_flags check everywhere instead
    of slightly different or missing ones in a few places.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 86223bc24d82..746fa5d6850c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -626,6 +626,7 @@ extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 		unsigned long attrs);
 
+struct page **dma_common_find_pages(void *cpu_addr);
 void *dma_common_contiguous_remap(struct page *page, size_t size,
 			pgprot_t prot, const void *caller);
 

commit 512317401f6a337e617ec284d20dec5fa3a951ec
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 30 08:51:01 2019 +0200

    dma-mapping: always use VM_DMA_COHERENT for generic DMA remap
    
    Currently the generic dma remap allocator gets a vm_flags passed by
    the caller that is a little confusing.  We just introduced a generic
    vmalloc-level flag to identify the dma coherent allocations, so use
    that everywhere and remove the now pointless argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 80063b0fdea8..86223bc24d82 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -627,13 +627,11 @@ extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 		unsigned long attrs);
 
 void *dma_common_contiguous_remap(struct page *page, size_t size,
-			unsigned long vm_flags,
 			pgprot_t prot, const void *caller);
 
 void *dma_common_pages_remap(struct page **pages, size_t size,
-			unsigned long vm_flags, pgprot_t prot,
-			const void *caller);
-void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
+			pgprot_t prot, const void *caller);
+void dma_common_free_remap(void *cpu_addr, size_t size);
 
 bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);

commit 1fa0682448acd5198f79c1d28ee1292a27ae406d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 30 08:46:57 2019 +0200

    dma-mapping: remove dma_release_declared_memory
    
    This function is entirely unused given that declared memory is
    generally provided by platform setup code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 41d4e91b0982..80063b0fdea8 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -764,7 +764,6 @@ static inline int dma_get_cache_alignment(void)
 #ifdef CONFIG_DMA_DECLARE_COHERENT
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 				dma_addr_t device_addr, size_t size);
-void dma_release_declared_memory(struct device *dev);
 #else
 static inline int
 dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
@@ -772,11 +771,6 @@ dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 {
 	return -ENOSYS;
 }
-
-static inline void
-dma_release_declared_memory(struct device *dev)
-{
-}
 #endif /* CONFIG_DMA_DECLARE_COHERENT */
 
 static inline void *dmam_alloc_coherent(struct device *dev, size_t size,

commit 212836a9929f0c91214a8a1879e6e41be0e26a6f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jul 26 08:58:36 2019 +0200

    dma-mapping: remove dma_{alloc,free,mmap}_writecombine
    
    We can already use DMA_ATTR_WRITE_COMBINE or the _wc prefixed version,
    so remove the third way of doing things.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Reviewed-by: Tomi Valkeinen <tomi.valkeinen@ti.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 10dc9ac3cccb..41d4e91b0982 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -796,9 +796,6 @@ static inline void *dma_alloc_wc(struct device *dev, size_t size,
 
 	return dma_alloc_attrs(dev, size, dma_addr, gfp, attrs);
 }
-#ifndef dma_alloc_writecombine
-#define dma_alloc_writecombine dma_alloc_wc
-#endif
 
 static inline void dma_free_wc(struct device *dev, size_t size,
 			       void *cpu_addr, dma_addr_t dma_addr)
@@ -806,9 +803,6 @@ static inline void dma_free_wc(struct device *dev, size_t size,
 	return dma_free_attrs(dev, size, cpu_addr, dma_addr,
 			      DMA_ATTR_WRITE_COMBINE);
 }
-#ifndef dma_free_writecombine
-#define dma_free_writecombine dma_free_wc
-#endif
 
 static inline int dma_mmap_wc(struct device *dev,
 			      struct vm_area_struct *vma,
@@ -818,9 +812,6 @@ static inline int dma_mmap_wc(struct device *dev,
 	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size,
 			      DMA_ATTR_WRITE_COMBINE);
 }
-#ifndef dma_mmap_writecombine
-#define dma_mmap_writecombine dma_mmap_wc
-#endif
 
 #ifdef CONFIG_NEED_DMA_MAP_STATE
 #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME

commit e29ccc188f3dae1cb66f59e10e01e0f150642a54
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 3 13:31:25 2019 +0300

    dma-mapping: add a dma_can_mmap helper
    
    Add a helper to check if DMA allocations for a specific device can be
    mapped to userspace using dma_mmap_*.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index de41a4f0b9f6..10dc9ac3cccb 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -463,6 +463,7 @@ int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
 int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
 		unsigned long attrs);
+bool dma_can_mmap(struct device *dev);
 int dma_supported(struct device *dev, u64 mask);
 int dma_set_mask(struct device *dev, u64 mask);
 int dma_set_coherent_mask(struct device *dev, u64 mask);
@@ -554,6 +555,10 @@ static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
 {
 	return -ENXIO;
 }
+static inline bool dma_can_mmap(struct device *dev)
+{
+	return false;
+}
 static inline int dma_supported(struct device *dev, u64 mask)
 {
 	return 0;

commit 6ba99411b858bd70bae966633561e698cd6de38c
Author: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
Date:   Wed Aug 28 21:35:40 2019 +0900

    dma-mapping: introduce dma_get_merge_boundary()
    
    This patch adds a new DMA API "dma_get_merge_boundary". This function
    returns the DMA merge boundary if the DMA layer can merge the segments.
    This patch also adds the implementation for a new dma_map_ops pointer.
    
    Signed-off-by: Yoshihiro Shimoda <yoshihiro.shimoda.uh@renesas.com>
    Reviewed-by: Simon Horman <horms+renesas@verge.net.au>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 48ebe8295987..de41a4f0b9f6 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -131,6 +131,7 @@ struct dma_map_ops {
 	int (*dma_supported)(struct device *dev, u64 mask);
 	u64 (*get_required_mask)(struct device *dev);
 	size_t (*max_mapping_size)(struct device *dev);
+	unsigned long (*get_merge_boundary)(struct device *dev);
 };
 
 #define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
@@ -467,6 +468,7 @@ int dma_set_mask(struct device *dev, u64 mask);
 int dma_set_coherent_mask(struct device *dev, u64 mask);
 u64 dma_get_required_mask(struct device *dev);
 size_t dma_max_mapping_size(struct device *dev);
+unsigned long dma_get_merge_boundary(struct device *dev);
 #else /* CONFIG_HAS_DMA */
 static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 		struct page *page, size_t offset, size_t size,
@@ -572,6 +574,10 @@ static inline size_t dma_max_mapping_size(struct device *dev)
 {
 	return 0;
 }
+static inline unsigned long dma_get_merge_boundary(struct device *dev)
+{
+	return 0;
+}
 #endif /* CONFIG_HAS_DMA */
 
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,

commit 8e3a68fb55e00e0760bd8023883e064f1f93c62d
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 3 12:42:15 2019 +0300

    dma-mapping: make dma_atomic_pool_init self-contained
    
    The memory allocated for the atomic pool needs to have the same
    mapping attributes that we use for remapping, so use
    pgprot_dmacoherent instead of open coding it.  Also deduct a
    suitable zone to allocate the memory from based on the presence
    of the DMA zones.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f7d1eea32c78..48ebe8295987 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -624,7 +624,6 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 			const void *caller);
 void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
 
-int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
 bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 bool dma_free_from_pool(void *start, size_t size);

commit bd5defaee872da9b81e3c72045eb6794445cd2e6
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 16 08:24:34 2019 +0200

    dma-mapping: remove is_device_dma_capable
    
    No users left.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Link: https://lore.kernel.org/r/20190816062435.881-6-hch@lst.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f7d1eea32c78..14702e2d6fa8 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -149,11 +149,6 @@ static inline int valid_dma_direction(int dma_direction)
 		(dma_direction == DMA_FROM_DEVICE));
 }
 
-static inline int is_device_dma_capable(struct device *dev)
-{
-	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
-}
-
 #ifdef CONFIG_DMA_DECLARE_COHERENT
 /*
  * These three functions are only for dma allocator.

commit 06532750010e06dd4b6d69983773677df7fc5291
Author: Eric Auger <eric.auger@redhat.com>
Date:   Mon Jul 22 18:51:49 2019 +0200

    dma-mapping: use dma_get_mask in dma_addressing_limited
    
    We currently have cases where the dma_addressing_limited() gets
    called with dma_mask unset. This causes a NULL pointer dereference.
    
    Use dma_get_mask() accessor to prevent the crash.
    
    Fixes: b866455423e0 ("dma-mapping: add a dma_addressing_limited helper")
    Signed-off-by: Eric Auger <eric.auger@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e11b115dd0e4..f7d1eea32c78 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -689,8 +689,8 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
  */
 static inline bool dma_addressing_limited(struct device *dev)
 {
-	return min_not_zero(*dev->dma_mask, dev->bus_dma_mask) <
-		dma_get_required_mask(dev);
+	return min_not_zero(dma_get_mask(dev), dev->bus_dma_mask) <
+			    dma_get_required_mask(dev);
 }
 
 #ifdef CONFIG_ARCH_HAS_SETUP_DMA_OPS

commit b866455423e040813f113d8b87e8297778ee2014
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 16 21:59:11 2019 +0200

    dma-mapping: add a dma_addressing_limited helper
    
    This helper returns if the device has issues addressing all present
    memory in the system.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8d13e28a8e07..e11b115dd0e4 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -679,6 +679,20 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 	return dma_set_mask_and_coherent(dev, mask);
 }
 
+/**
+ * dma_addressing_limited - return if the device is addressing limited
+ * @dev:	device to check
+ *
+ * Return %true if the devices DMA mask is too small to address all memory in
+ * the system, else %false.  Lack of addressing bits is the prime reason for
+ * bounce buffering, but might not be the only one.
+ */
+static inline bool dma_addressing_limited(struct device *dev)
+{
+	return min_not_zero(*dev->dma_mask, dev->bus_dma_mask) <
+		dma_get_required_mask(dev);
+}
+
 #ifdef CONFIG_ARCH_HAS_SETUP_DMA_OPS
 void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 		const struct iommu_ops *iommu, bool coherent);

commit 79a986721decf428ba539e6e2c941c987acce655
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 25 11:20:42 2019 +0200

    dma-mapping: remove dma_max_pfn
    
    These days, the DMA mapping code must bounce buffers for any unsupported
    address. If the driver needs to optimize for natively supported ranges,
    then it should use dma_get_required_mask.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Marc Gonzalez <marc.w.gonzalez@free.fr>
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 6309a721394b..8d13e28a8e07 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -729,13 +729,6 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 	return -EIO;
 }
 
-#ifndef dma_max_pfn
-static inline unsigned long dma_max_pfn(struct device *dev)
-{
-	return (*dev->dma_mask >> PAGE_SHIFT) + dev->dma_pfn_offset;
-}
-#endif
-
 static inline int dma_get_cache_alignment(void)
 {
 #ifdef ARCH_DMA_MINALIGN

commit d7e02a931235de0779d44c6f8d211df0eca304b8
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 13 18:45:21 2019 +0100

    dma-mapping: remove leftover NULL device support
    
    Most dma_map_ops implementations already had some issues with a NULL
    device, or did simply crash if one was fed to them.  Now that we have
    cleaned up all the obvious offenders we can stop to pretend we
    support this mode.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 75e60be91e5f..6309a721394b 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -267,9 +267,9 @@ size_t dma_direct_max_mapping_size(struct device *dev);
 
 static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	if (dev && dev->dma_ops)
+	if (dev->dma_ops)
 		return dev->dma_ops;
-	return get_arch_dma_ops(dev ? dev->bus : NULL);
+	return get_arch_dma_ops(dev->bus);
 }
 
 static inline void set_dma_ops(struct device *dev,
@@ -650,7 +650,7 @@ static inline void dma_free_coherent(struct device *dev, size_t size,
 
 static inline u64 dma_get_mask(struct device *dev)
 {
-	if (dev && dev->dma_mask && *dev->dma_mask)
+	if (dev->dma_mask && *dev->dma_mask)
 		return *dev->dma_mask;
 	return DMA_BIT_MASK(32);
 }

commit 45ba8d5d061b13494c2a7a7652d51b9da3d9e77a
Merge: bb97be23db2a cfdbb4ed31aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 12:47:57 2019 -0700

    Merge tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost
    
    Pull virtio updates from Michael Tsirkin:
     "Several fixes, most notably fix for virtio on swiotlb systems"
    
    * tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost:
      vhost: silence an unused-variable warning
      virtio: hint if callbacks surprisingly might sleep
      virtio-ccw: wire up ->bus_name callback
      s390/virtio: handle find on invalid queue gracefully
      virtio-ccw: diag 500 may return a negative cookie
      virtio_balloon: remove the unnecessary 0-initialization
      virtio-balloon: improve update_balloon_size_func
      virtio-blk: Consider virtio_max_dma_size() for maximum segment size
      virtio: Introduce virtio_max_dma_size()
      dma: Introduce dma_max_mapping_size()
      swiotlb: Add is_swiotlb_active() function
      swiotlb: Introduce swiotlb_max_mapping_size()

commit 133d624b1cee16906134e92d5befb843b58bcf31
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu Feb 7 12:59:15 2019 +0100

    dma: Introduce dma_max_mapping_size()
    
    The function returns the maximum size that can be mapped
    using DMA-API functions. The patch also adds the
    implementation for direct DMA and a new dma_map_ops pointer
    so that other implementations can expose their limit.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f6ded992c183..5b21f14802e1 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -130,6 +130,7 @@ struct dma_map_ops {
 			enum dma_data_direction direction);
 	int (*dma_supported)(struct device *dev, u64 mask);
 	u64 (*get_required_mask)(struct device *dev);
+	size_t (*max_mapping_size)(struct device *dev);
 };
 
 #define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
@@ -257,6 +258,8 @@ static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
 }
 #endif
 
+size_t dma_direct_max_mapping_size(struct device *dev);
+
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 
@@ -460,6 +463,7 @@ int dma_supported(struct device *dev, u64 mask);
 int dma_set_mask(struct device *dev, u64 mask);
 int dma_set_coherent_mask(struct device *dev, u64 mask);
 u64 dma_get_required_mask(struct device *dev);
+size_t dma_max_mapping_size(struct device *dev);
 #else /* CONFIG_HAS_DMA */
 static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 		struct page *page, size_t offset, size_t size,
@@ -561,6 +565,10 @@ static inline u64 dma_get_required_mask(struct device *dev)
 {
 	return 0;
 }
+static inline size_t dma_max_mapping_size(struct device *dev)
+{
+	return 0;
+}
 #endif /* CONFIG_HAS_DMA */
 
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,

commit 82c5de0ab8dbd6035223ad69e76bd8a88a0a9399
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 25 13:29:54 2018 +0100

    dma-mapping: remove the DMA_MEMORY_EXCLUSIVE flag
    
    All users of dma_declare_coherent want their allocations to be
    exclusive, so default to exclusive allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d29faadf6ef2..70ad15758a70 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -736,17 +736,14 @@ static inline int dma_get_cache_alignment(void)
 	return 1;
 }
 
-/* flags for the coherent memory api */
-#define DMA_MEMORY_EXCLUSIVE		0x01
-
 #ifdef CONFIG_DMA_DECLARE_COHERENT
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
-				dma_addr_t device_addr, size_t size, int flags);
+				dma_addr_t device_addr, size_t size);
 void dma_release_declared_memory(struct device *dev);
 #else
 static inline int
 dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
-			    dma_addr_t device_addr, size_t size, int flags)
+			    dma_addr_t device_addr, size_t size)
 {
 	return -ENOSYS;
 }

commit 91a6fda95cb67c94b887355690d1923a7eb6f630
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 25 17:27:14 2018 +0100

    dma-mapping: remove dma_mark_declared_memory_occupied
    
    This API is not used anywhere, so remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e29441b8b3b7..d29faadf6ef2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -743,8 +743,6 @@ static inline int dma_get_cache_alignment(void)
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 				dma_addr_t device_addr, size_t size, int flags);
 void dma_release_declared_memory(struct device *dev);
-void *dma_mark_declared_memory_occupied(struct device *dev,
-					dma_addr_t device_addr, size_t size);
 #else
 static inline int
 dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
@@ -757,13 +755,6 @@ static inline void
 dma_release_declared_memory(struct device *dev)
 {
 }
-
-static inline void *
-dma_mark_declared_memory_occupied(struct device *dev,
-				  dma_addr_t device_addr, size_t size)
-{
-	return ERR_PTR(-EBUSY);
-}
 #endif /* CONFIG_DMA_DECLARE_COHERENT */
 
 static inline void *dmam_alloc_coherent(struct device *dev, size_t size,

commit ff4c25f26a71b79c70ea03b3935a1297439a8a85
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Feb 3 20:12:02 2019 +0100

    dma-mapping: improve selection of dma_declare_coherent availability
    
    This API is primarily used through DT entries, but two architectures
    and two drivers call it directly.  So instead of selecting the config
    symbol for random architectures pull it in implicitly for the actual
    users.  Also rename the Kconfig option to describe the feature better.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS
    Acked-by: Lee Jones <lee.jones@linaro.org>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4210c5c1dd21..e29441b8b3b7 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -153,7 +153,7 @@ static inline int is_device_dma_capable(struct device *dev)
 	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
 }
 
-#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
+#ifdef CONFIG_DMA_DECLARE_COHERENT
 /*
  * These three functions are only for dma allocator.
  * Don't use them in device drivers.
@@ -192,7 +192,7 @@ static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,
 {
 	return 0;
 }
-#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
+#endif /* CONFIG_DMA_DECLARE_COHERENT */
 
 static inline bool dma_is_direct(const struct dma_map_ops *ops)
 {
@@ -739,7 +739,7 @@ static inline int dma_get_cache_alignment(void)
 /* flags for the coherent memory api */
 #define DMA_MEMORY_EXCLUSIVE		0x01
 
-#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
+#ifdef CONFIG_DMA_DECLARE_COHERENT
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 				dma_addr_t device_addr, size_t size, int flags);
 void dma_release_declared_memory(struct device *dev);
@@ -764,7 +764,7 @@ dma_mark_declared_memory_occupied(struct device *dev,
 {
 	return ERR_PTR(-EBUSY);
 }
-#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
+#endif /* CONFIG_DMA_DECLARE_COHERENT */
 
 static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)

commit dc2acded38957dfa6b7b7e0203b4b8cb8d818ce6
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 21 22:14:44 2018 +0100

    dma-mapping: add a kconfig symbol for arch_teardown_dma_ops availability
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2b20d60e6158..4210c5c1dd21 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -681,9 +681,13 @@ static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
 }
 #endif /* CONFIG_ARCH_HAS_SETUP_DMA_OPS */
 
-#ifndef arch_teardown_dma_ops
-static inline void arch_teardown_dma_ops(struct device *dev) { }
-#endif
+#ifdef CONFIG_ARCH_HAS_TEARDOWN_DMA_OPS
+void arch_teardown_dma_ops(struct device *dev);
+#else
+static inline void arch_teardown_dma_ops(struct device *dev)
+{
+}
+#endif /* CONFIG_ARCH_HAS_TEARDOWN_DMA_OPS */
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)
 {

commit 347cb6af8710b72cf9685fdc09d07873cf42d51f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jan 7 13:36:20 2019 -0500

    dma-mapping: add a kconfig symbol for arch_setup_dma_ops availability
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS
    Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index b904d55247ab..2b20d60e6158 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -671,11 +671,15 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 	return dma_set_mask_and_coherent(dev, mask);
 }
 
-#ifndef arch_setup_dma_ops
+#ifdef CONFIG_ARCH_HAS_SETUP_DMA_OPS
+void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+		const struct iommu_ops *iommu, bool coherent);
+#else
 static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
-				      u64 size, const struct iommu_ops *iommu,
-				      bool coherent) { }
-#endif
+		u64 size, const struct iommu_ops *iommu, bool coherent)
+{
+}
+#endif /* CONFIG_ARCH_HAS_SETUP_DMA_OPS */
 
 #ifndef arch_teardown_dma_ops
 static inline void arch_teardown_dma_ops(struct device *dev) { }

commit 645386dfe6307dbb28f10a4513792a59beda0efa
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 4 17:17:53 2019 +0100

    dma-mapping: don't BUG when calling dma_map_resource on RAM
    
    Use WARN_ON_ONCE to print a stack trace and return a proper error
    code instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 9842085e6774..b904d55247ab 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -353,7 +353,8 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 	BUG_ON(!valid_dma_direction(dir));
 
 	/* Don't allow RAM to be mapped */
-	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
+	if (WARN_ON_ONCE(pfn_valid(PHYS_PFN(phys_addr))))
+		return DMA_MAPPING_ERROR;
 
 	if (dma_is_direct(ops))
 		addr = dma_direct_map_resource(dev, phys_addr, size, dir, attrs);

commit cfced786969c2a3e1bca45d7055a00311d93ae6c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 4 18:20:05 2019 +0100

    dma-mapping: remove the default map_resource implementation
    
    Instead provide a proper implementation in the direct mapping code, and
    also wire it up for arm and powerpc, leaving an error return for all the
    IOMMU or virtual mapping instances for which we'd have to wire up an
    actual implementation
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f6ded992c183..9842085e6774 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -208,6 +208,8 @@ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		unsigned long attrs);
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs);
+dma_addr_t dma_direct_map_resource(struct device *dev, phys_addr_t paddr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
 
 #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
     defined(CONFIG_SWIOTLB)
@@ -346,19 +348,19 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 					  unsigned long attrs)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
-	dma_addr_t addr;
+	dma_addr_t addr = DMA_MAPPING_ERROR;
 
 	BUG_ON(!valid_dma_direction(dir));
 
 	/* Don't allow RAM to be mapped */
 	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
 
-	addr = phys_addr;
-	if (ops && ops->map_resource)
+	if (dma_is_direct(ops))
+		addr = dma_direct_map_resource(dev, phys_addr, size, dir, attrs);
+	else if (ops->map_resource)
 		addr = ops->map_resource(dev, phys_addr, size, dir, attrs);
 
 	debug_dma_map_resource(dev, phys_addr, size, dir, addr);
-
 	return addr;
 }
 
@@ -369,7 +371,7 @@ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops && ops->unmap_resource)
+	if (!dma_is_direct(ops) && ops->unmap_resource)
 		ops->unmap_resource(dev, addr, size, dir, attrs);
 	debug_dma_unmap_resource(dev, addr, size, dir);
 }

commit dfd32cad146e3624970eee9329e99d2c6ef751b3
Author: Luis Chamberlain <mcgrof@kernel.org>
Date:   Wed Dec 19 12:30:34 2018 -0800

    dma-mapping: remove dma_zalloc_coherent()
    
    dma_zalloc_coherent() is no longer needed as it has no users because
    dma_alloc_coherent() already zeroes out memory for us.
    
    The Coccinelle grammar rule that used to check for dma_alloc_coherent()
    + memset() is modified so that it just tells the user that the memset is
    not needed anymore.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Luis Chamberlain <mcgrof@kernel.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index cef2127e1d70..f6ded992c183 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -717,15 +717,6 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #endif
 
-/*
- * Please always use dma_alloc_coherent instead as it already zeroes the memory!
- */
-static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
-					dma_addr_t *dma_handle, gfp_t flag)
-{
-	return dma_alloc_coherent(dev, size, dma_handle, flag);
-}
-
 static inline int dma_get_cache_alignment(void)
 {
 #ifdef ARCH_DMA_MINALIGN

commit ed6ccf10f24bdfc1955bc8b976ddedc370fc3869
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 26 07:52:13 2018 +0100

    dma-mapping: properly stub out the DMA API for !CONFIG_HAS_DMA
    
    This avoids link failures in drivers using the DMA API, when they
    are compiled for user mode Linux with CONFIG_COMPILE_TEST=y.
    
    Fixes: 356da6d0cd ("dma-mapping: bypass indirect calls for dma-direct")
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 937c2a949fca..cef2127e1d70 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -194,33 +194,6 @@ static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
-#ifdef CONFIG_HAS_DMA
-#include <asm/dma-mapping.h>
-static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
-{
-	if (dev && dev->dma_ops)
-		return dev->dma_ops;
-	return get_arch_dma_ops(dev ? dev->bus : NULL);
-}
-
-static inline void set_dma_ops(struct device *dev,
-			       const struct dma_map_ops *dma_ops)
-{
-	dev->dma_ops = dma_ops;
-}
-#else
-/*
- * Define the dma api to allow compilation of dma dependent code.
- * Code that depends on the dma-mapping API needs to set 'depends on HAS_DMA'
- * in its Kconfig, unless it already depends on <something> || COMPILE_TEST,
- * where <something> guarantuees the availability of the dma-mapping API.
- */
-static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
-{
-	return NULL;
-}
-#endif
-
 static inline bool dma_is_direct(const struct dma_map_ops *ops)
 {
 	return likely(!ops);
@@ -284,6 +257,22 @@ static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
 }
 #endif
 
+#ifdef CONFIG_HAS_DMA
+#include <asm/dma-mapping.h>
+
+static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
+{
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
+	return get_arch_dma_ops(dev ? dev->bus : NULL);
+}
+
+static inline void set_dma_ops(struct device *dev,
+			       const struct dma_map_ops *dma_ops)
+{
+	dev->dma_ops = dma_ops;
+}
+
 static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 		struct page *page, size_t offset, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -399,13 +388,6 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
 }
 
-static inline void dma_sync_single_range_for_cpu(struct device *dev,
-		dma_addr_t addr, unsigned long offset, size_t size,
-		enum dma_data_direction dir)
-{
-	return dma_sync_single_for_cpu(dev, addr + offset, size, dir);
-}
-
 static inline void dma_sync_single_for_device(struct device *dev,
 					      dma_addr_t addr, size_t size,
 					      enum dma_data_direction dir)
@@ -420,13 +402,6 @@ static inline void dma_sync_single_for_device(struct device *dev,
 	debug_dma_sync_single_for_device(dev, addr, size, dir);
 }
 
-static inline void dma_sync_single_range_for_device(struct device *dev,
-		dma_addr_t addr, unsigned long offset, size_t size,
-		enum dma_data_direction dir)
-{
-	return dma_sync_single_for_device(dev, addr + offset, size, dir);
-}
-
 static inline void
 dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 		    int nelems, enum dma_data_direction dir)
@@ -456,6 +431,138 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 
 }
 
+static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	debug_dma_mapping_error(dev, dma_addr);
+
+	if (dma_addr == DMA_MAPPING_ERROR)
+		return -ENOMEM;
+	return 0;
+}
+
+void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t flag, unsigned long attrs);
+void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle, unsigned long attrs);
+void *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t gfp, unsigned long attrs);
+void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
+		dma_addr_t dma_handle);
+void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		enum dma_data_direction dir);
+int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
+int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
+int dma_supported(struct device *dev, u64 mask);
+int dma_set_mask(struct device *dev, u64 mask);
+int dma_set_coherent_mask(struct device *dev, u64 mask);
+u64 dma_get_required_mask(struct device *dev);
+#else /* CONFIG_HAS_DMA */
+static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+		struct page *page, size_t offset, size_t size,
+		enum dma_data_direction dir, unsigned long attrs)
+{
+	return DMA_MAPPING_ERROR;
+}
+static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+}
+static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
+		int nents, enum dma_data_direction dir, unsigned long attrs)
+{
+	return 0;
+}
+static inline void dma_unmap_sg_attrs(struct device *dev,
+		struct scatterlist *sg, int nents, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+}
+static inline dma_addr_t dma_map_resource(struct device *dev,
+		phys_addr_t phys_addr, size_t size, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+	return DMA_MAPPING_ERROR;
+}
+static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+}
+static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+{
+}
+static inline void dma_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sg, int nelems, enum dma_data_direction dir)
+{
+}
+static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	return -ENOMEM;
+}
+static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t flag, unsigned long attrs)
+{
+	return NULL;
+}
+static void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle, unsigned long attrs)
+{
+}
+static inline void *dmam_alloc_attrs(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
+{
+	return NULL;
+}
+static inline void dmam_free_coherent(struct device *dev, size_t size,
+		void *vaddr, dma_addr_t dma_handle)
+{
+}
+static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		enum dma_data_direction dir)
+{
+}
+static inline int dma_get_sgtable_attrs(struct device *dev,
+		struct sg_table *sgt, void *cpu_addr, dma_addr_t dma_addr,
+		size_t size, unsigned long attrs)
+{
+	return -ENXIO;
+}
+static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs)
+{
+	return -ENXIO;
+}
+static inline int dma_supported(struct device *dev, u64 mask)
+{
+	return 0;
+}
+static inline int dma_set_mask(struct device *dev, u64 mask)
+{
+	return -EIO;
+}
+static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
+{
+	return -EIO;
+}
+static inline u64 dma_get_required_mask(struct device *dev)
+{
+	return 0;
+}
+#endif /* CONFIG_HAS_DMA */
+
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 		size_t size, enum dma_data_direction dir, unsigned long attrs)
 {
@@ -470,15 +577,28 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 	return dma_unmap_page_attrs(dev, addr, size, dir, attrs);
 }
 
+static inline void dma_sync_single_range_for_cpu(struct device *dev,
+		dma_addr_t addr, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
+{
+	return dma_sync_single_for_cpu(dev, addr + offset, size, dir);
+}
+
+static inline void dma_sync_single_range_for_device(struct device *dev,
+		dma_addr_t addr, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
+{
+	return dma_sync_single_for_device(dev, addr + offset, size, dir);
+}
+
 #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
 #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
 #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
 #define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
 #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
 #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
-
-void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
-		enum dma_data_direction dir);
+#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
+#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
@@ -498,25 +618,10 @@ bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 bool dma_free_from_pool(void *start, size_t size);
 
-int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
-		void *cpu_addr, dma_addr_t dma_addr, size_t size,
-		unsigned long attrs);
-#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
-
 int
 dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 		dma_addr_t dma_addr, size_t size, unsigned long attrs);
 
-int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
-		void *cpu_addr, dma_addr_t dma_addr, size_t size,
-		unsigned long attrs);
-#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
-
-void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
-		gfp_t flag, unsigned long attrs);
-void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
-		dma_addr_t dma_handle, unsigned long attrs);
-
 static inline void *dma_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
 {
@@ -531,18 +636,6 @@ static inline void dma_free_coherent(struct device *dev, size_t size,
 	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
 }
 
-static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
-{
-	debug_dma_mapping_error(dev, dma_addr);
-
-	if (dma_addr == DMA_MAPPING_ERROR)
-		return -ENOMEM;
-	return 0;
-}
-
-int dma_supported(struct device *dev, u64 mask);
-int dma_set_mask(struct device *dev, u64 mask);
-int dma_set_coherent_mask(struct device *dev, u64 mask);
 
 static inline u64 dma_get_mask(struct device *dev)
 {
@@ -575,8 +668,6 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 	return dma_set_mask_and_coherent(dev, mask);
 }
 
-extern u64 dma_get_required_mask(struct device *dev);
-
 #ifndef arch_setup_dma_ops
 static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
 				      u64 size, const struct iommu_ops *iommu,
@@ -673,24 +764,6 @@ dma_mark_declared_memory_occupied(struct device *dev,
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
-/*
- * Managed DMA API
- */
-#ifdef CONFIG_HAS_DMA
-extern void *dmam_alloc_attrs(struct device *dev, size_t size,
-				 dma_addr_t *dma_handle, gfp_t gfp,
-				 unsigned long attrs);
-extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
-			       dma_addr_t dma_handle);
-#else /* !CONFIG_HAS_DMA */
-static inline void *dmam_alloc_attrs(struct device *dev, size_t size,
-					dma_addr_t *dma_handle, gfp_t gfp,
-					unsigned long attrs)
-{ return NULL; }
-static inline void dmam_free_coherent(struct device *dev, size_t size,
-				      void *vaddr, dma_addr_t dma_handle) { }
-#endif /* !CONFIG_HAS_DMA */
-
 static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
 {

commit 4788ba5792cc1368ba4867e1488dc168b4fe97b7
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 26 07:51:44 2018 +0100

    dma-mapping: remove dmam_{declare,release}_coherent_memory
    
    These functions have never been used.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index fa2ebe8ad4d0..937c2a949fca 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -691,25 +691,6 @@ static inline void dmam_free_coherent(struct device *dev, size_t size,
 				      void *vaddr, dma_addr_t dma_handle) { }
 #endif /* !CONFIG_HAS_DMA */
 
-#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
-extern int dmam_declare_coherent_memory(struct device *dev,
-					phys_addr_t phys_addr,
-					dma_addr_t device_addr, size_t size,
-					int flags);
-extern void dmam_release_declared_memory(struct device *dev);
-#else /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
-static inline int dmam_declare_coherent_memory(struct device *dev,
-				phys_addr_t phys_addr, dma_addr_t device_addr,
-				size_t size, gfp_t gfp)
-{
-	return 0;
-}
-
-static inline void dmam_release_declared_memory(struct device *dev)
-{
-}
-#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
-
 static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
 {

commit d7076f07840851bbe57cb21ba052d6a4a9b1efa9
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 25 17:44:19 2018 +0100

    dma-mapping: implement dmam_alloc_coherent using dmam_alloc_attrs
    
    dmam_alloc_coherent is just the default no-flags case of
    dmam_alloc_attrs, so take advantage of this similar to the non-managed
    version.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0452a8be2789..fa2ebe8ad4d0 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -677,21 +677,20 @@ dma_mark_declared_memory_occupied(struct device *dev,
  * Managed DMA API
  */
 #ifdef CONFIG_HAS_DMA
-extern void *dmam_alloc_coherent(struct device *dev, size_t size,
-				 dma_addr_t *dma_handle, gfp_t gfp);
+extern void *dmam_alloc_attrs(struct device *dev, size_t size,
+				 dma_addr_t *dma_handle, gfp_t gfp,
+				 unsigned long attrs);
 extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
 			       dma_addr_t dma_handle);
 #else /* !CONFIG_HAS_DMA */
-static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
-					dma_addr_t *dma_handle, gfp_t gfp)
+static inline void *dmam_alloc_attrs(struct device *dev, size_t size,
+					dma_addr_t *dma_handle, gfp_t gfp,
+					unsigned long attrs)
 { return NULL; }
 static inline void dmam_free_coherent(struct device *dev, size_t size,
 				      void *vaddr, dma_addr_t dma_handle) { }
 #endif /* !CONFIG_HAS_DMA */
 
-extern void *dmam_alloc_attrs(struct device *dev, size_t size,
-			      dma_addr_t *dma_handle, gfp_t gfp,
-			      unsigned long attrs);
 #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 extern int dmam_declare_coherent_memory(struct device *dev,
 					phys_addr_t phys_addr,
@@ -711,6 +710,13 @@ static inline void dmam_release_declared_memory(struct device *dev)
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
+static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t gfp)
+{
+	return dmam_alloc_attrs(dev, size, dma_handle, gfp,
+			(gfp & __GFP_NOWARN) ? DMA_ATTR_NO_WARN : 0);
+}
+
 static inline void *dma_alloc_wc(struct device *dev, size_t size,
 				 dma_addr_t *dma_addr, gfp_t gfp)
 {

commit 2e05ea5cdc1ac55d9ef678ed5ea6c38acf7fd2a3
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 25 08:50:35 2018 +0100

    dma-mapping: implement dma_map_single_attrs using dma_map_page_attrs
    
    And also switch the way we implement the unmap side around to stay
    consistent.  This ensures dma-debug works again because it records which
    function we used for mapping to ensure it is also used for unmapping,
    and also reduces further code duplication.  Last but not least this
    also officially allows calling dma_sync_single_* for mappings created
    using dma_map_page, which is perfectly fine given that the sync calls
    only take a dma_addr_t, but not a virtual address or struct page.
    
    Fixes: 7f0fee242e ("dma-mapping: merge dma_unmap_page_attrs and dma_unmap_single_attrs")
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: LABBE Corentin <clabbe.montjoie@gmail.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ba521d5506c9..0452a8be2789 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -284,32 +284,25 @@ static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
 }
 #endif
 
-static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
-					      size_t size,
-					      enum dma_data_direction dir,
-					      unsigned long attrs)
+static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+		struct page *page, size_t offset, size_t size,
+		enum dma_data_direction dir, unsigned long attrs)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
-	debug_dma_map_single(dev, ptr, size);
 	if (dma_is_direct(ops))
-		addr = dma_direct_map_page(dev, virt_to_page(ptr),
-				offset_in_page(ptr), size, dir, attrs);
+		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
 	else
-		addr = ops->map_page(dev, virt_to_page(ptr),
-				offset_in_page(ptr), size, dir, attrs);
-	debug_dma_map_page(dev, virt_to_page(ptr),
-			   offset_in_page(ptr), size,
-			   dir, addr, true);
+		addr = ops->map_page(dev, page, offset, size, dir, attrs);
+	debug_dma_map_page(dev, page, offset, size, dir, addr);
+
 	return addr;
 }
 
-static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
-					  size_t size,
-					  enum dma_data_direction dir,
-					  unsigned long attrs)
+static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
@@ -318,13 +311,7 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 		dma_direct_unmap_page(dev, addr, size, dir, attrs);
 	else if (ops->unmap_page)
 		ops->unmap_page(dev, addr, size, dir, attrs);
-	debug_dma_unmap_page(dev, addr, size, dir, true);
-}
-
-static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
-		size_t size, enum dma_data_direction dir, unsigned long attrs)
-{
-	return dma_unmap_single_attrs(dev, addr, size, dir, attrs);
+	debug_dma_unmap_page(dev, addr, size, dir);
 }
 
 /*
@@ -363,25 +350,6 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
 		ops->unmap_sg(dev, sg, nents, dir, attrs);
 }
 
-static inline dma_addr_t dma_map_page_attrs(struct device *dev,
-					    struct page *page,
-					    size_t offset, size_t size,
-					    enum dma_data_direction dir,
-					    unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	dma_addr_t addr;
-
-	BUG_ON(!valid_dma_direction(dir));
-	if (dma_is_direct(ops))
-		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
-	else
-		addr = ops->map_page(dev, page, offset, size, dir, attrs);
-	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
-
-	return addr;
-}
-
 static inline dma_addr_t dma_map_resource(struct device *dev,
 					  phys_addr_t phys_addr,
 					  size_t size,
@@ -488,6 +456,20 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 
 }
 
+static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+	debug_dma_map_single(dev, ptr, size);
+	return dma_map_page_attrs(dev, virt_to_page(ptr), offset_in_page(ptr),
+			size, dir, attrs);
+}
+
+static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+	return dma_unmap_page_attrs(dev, addr, size, dir, attrs);
+}
+
 #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
 #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
 #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)

commit af7ddd8a627c62a835524b3f5b471edbbbcce025
Merge: fe2b0cdabcd9 8b1cce9f5832
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 28 14:12:21 2018 -0800

    Merge tag 'dma-mapping-4.21' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull DMA mapping updates from Christoph Hellwig:
     "A huge update this time, but a lot of that is just consolidating or
      removing code:
    
       - provide a common DMA_MAPPING_ERROR definition and avoid indirect
         calls for dma_map_* error checking
    
       - use direct calls for the DMA direct mapping case, avoiding huge
         retpoline overhead for high performance workloads
    
       - merge the swiotlb dma_map_ops into dma-direct
    
       - provide a generic remapping DMA consistent allocator for
         architectures that have devices that perform DMA that is not cache
         coherent. Based on the existing arm64 implementation and also used
         for csky now.
    
       - improve the dma-debug infrastructure, including dynamic allocation
         of entries (Robin Murphy)
    
       - default to providing chaining scatterlist everywhere, with opt-outs
         for the few architectures (alpha, parisc, most arm32 variants) that
         can't cope with it
    
       - misc sparc32 dma-related cleanups
    
       - remove the dma_mark_clean arch hook used by swiotlb on ia64 and
         replace it with the generic noncoherent infrastructure
    
       - fix the return type of dma_set_max_seg_size (Niklas Sderlund)
    
       - move the dummy dma ops for not DMA capable devices from arm64 to
         common code (Robin Murphy)
    
       - ensure dma_alloc_coherent returns zeroed memory to avoid kernel
         data leaks through userspace. We already did this for most common
         architectures, but this ensures we do it everywhere.
         dma_zalloc_coherent has been deprecated and can hopefully be
         removed after -rc1 with a coccinelle script"
    
    * tag 'dma-mapping-4.21' of git://git.infradead.org/users/hch/dma-mapping: (73 commits)
      dma-mapping: fix inverted logic in dma_supported
      dma-mapping: deprecate dma_zalloc_coherent
      dma-mapping: zero memory returned from dma_alloc_*
      sparc/iommu: fix ->map_sg return value
      sparc/io-unit: fix ->map_sg return value
      arm64: default to the direct mapping in get_arch_dma_ops
      PCI: Remove unused attr variable in pci_dma_configure
      ia64: only select ARCH_HAS_DMA_COHERENT_TO_PFN if swiotlb is enabled
      dma-mapping: bypass indirect calls for dma-direct
      vmd: use the proper dma_* APIs instead of direct methods calls
      dma-direct: merge swiotlb_dma_ops into the dma_direct code
      dma-direct: use dma_direct_map_page to implement dma_direct_map_sg
      dma-direct: improve addressability error reporting
      swiotlb: remove dma_mark_clean
      swiotlb: remove SWIOTLB_MAP_ERROR
      ACPI / scan: Refactor _CCA enforcement
      dma-mapping: factor out dummy DMA ops
      dma-mapping: always build the direct mapping code
      dma-mapping: move dma_cache_sync out of line
      dma-mapping: move various slow path functions out of line
      ...

commit 0cd60eb1a7b5421e92a3489e1829bfb2243e21e1
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Dec 22 09:21:08 2018 +0100

    dma-mapping: fix flags in dma_alloc_wc
    
    We really need the writecombine flag in dma_alloc_wc, fix a stupid
    oversight.
    
    Fixes: 7ed1d91a9e ("dma-mapping: translate __GFP_NOFAIL to DMA_ATTR_NO_WARN")
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 15bd41447025..d327bdd53716 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -796,7 +796,7 @@ static inline void dmam_release_declared_memory(struct device *dev)
 static inline void *dma_alloc_wc(struct device *dev, size_t size,
 				 dma_addr_t *dma_addr, gfp_t gfp)
 {
-	unsigned long attrs = DMA_ATTR_NO_WARN;
+	unsigned long attrs = DMA_ATTR_WRITE_COMBINE;
 
 	if (gfp & __GFP_NOWARN)
 		attrs |= DMA_ATTR_NO_WARN;

commit 06d4dd2f2ce1cdb625f77c0676d5af6ba310c01d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 14 09:15:02 2018 +0100

    dma-mapping: deprecate dma_zalloc_coherent
    
    We now always return zeroed memory from dma_alloc_coherent.  Note that
    simply passing GFP_ZERO to dma_alloc_coherent wasn't always doing the
    right thing to start with given that various allocators are not backed
    by the page allocator and thus would ignore GFP_ZERO.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f422aec0f53c..a52c6409bdc2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -644,12 +644,13 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #endif
 
+/*
+ * Please always use dma_alloc_coherent instead as it already zeroes the memory!
+ */
 static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
 					dma_addr_t *dma_handle, gfp_t flag)
 {
-	void *ret = dma_alloc_coherent(dev, size, dma_handle,
-				       flag | __GFP_ZERO);
-	return ret;
+	return dma_alloc_coherent(dev, size, dma_handle, flag);
 }
 
 static inline int dma_get_cache_alignment(void)

commit 356da6d0cde3323236977fce54c1f9612a742036
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 13:39:32 2018 -0800

    dma-mapping: bypass indirect calls for dma-direct
    
    Avoid expensive indirect calls in the fast path DMA mapping
    operations by directly calling the dma_direct_* ops if we are using
    the directly mapped DMA operations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 269ee27fc3d9..f422aec0f53c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -134,7 +134,6 @@ struct dma_map_ops {
 
 #define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
 
-extern const struct dma_map_ops dma_direct_ops;
 extern const struct dma_map_ops dma_virt_ops;
 extern const struct dma_map_ops dma_dummy_ops;
 
@@ -222,6 +221,69 @@ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 }
 #endif
 
+static inline bool dma_is_direct(const struct dma_map_ops *ops)
+{
+	return likely(!ops);
+}
+
+/*
+ * All the dma_direct_* declarations are here just for the indirect call bypass,
+ * and must not be used directly drivers!
+ */
+dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size, enum dma_data_direction dir,
+		unsigned long attrs);
+int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+		enum dma_data_direction dir, unsigned long attrs);
+
+#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+    defined(CONFIG_SWIOTLB)
+void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+#else
+static inline void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+{
+}
+#endif
+
+#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+    defined(CONFIG_SWIOTLB)
+void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+		int nents, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+#else
+static inline void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+}
+static inline void dma_direct_unmap_sg(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+}
+static inline void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+{
+}
+#endif
+
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 					      size_t size,
 					      enum dma_data_direction dir,
@@ -232,9 +294,12 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_map_single(dev, ptr, size);
-	addr = ops->map_page(dev, virt_to_page(ptr),
-			     offset_in_page(ptr), size,
-			     dir, attrs);
+	if (dma_is_direct(ops))
+		addr = dma_direct_map_page(dev, virt_to_page(ptr),
+				offset_in_page(ptr), size, dir, attrs);
+	else
+		addr = ops->map_page(dev, virt_to_page(ptr),
+				offset_in_page(ptr), size, dir, attrs);
 	debug_dma_map_page(dev, virt_to_page(ptr),
 			   offset_in_page(ptr), size,
 			   dir, addr, true);
@@ -249,7 +314,9 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->unmap_page)
+	if (dma_is_direct(ops))
+		dma_direct_unmap_page(dev, addr, size, dir, attrs);
+	else if (ops->unmap_page)
 		ops->unmap_page(dev, addr, size, dir, attrs);
 	debug_dma_unmap_page(dev, addr, size, dir, true);
 }
@@ -272,7 +339,10 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 	int ents;
 
 	BUG_ON(!valid_dma_direction(dir));
-	ents = ops->map_sg(dev, sg, nents, dir, attrs);
+	if (dma_is_direct(ops))
+		ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
+	else
+		ents = ops->map_sg(dev, sg, nents, dir, attrs);
 	BUG_ON(ents < 0);
 	debug_dma_map_sg(dev, sg, nents, ents, dir);
 
@@ -287,7 +357,9 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_unmap_sg(dev, sg, nents, dir);
-	if (ops->unmap_sg)
+	if (dma_is_direct(ops))
+		dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
+	else if (ops->unmap_sg)
 		ops->unmap_sg(dev, sg, nents, dir, attrs);
 }
 
@@ -301,7 +373,10 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
-	addr = ops->map_page(dev, page, offset, size, dir, attrs);
+	if (dma_is_direct(ops))
+		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
+	else
+		addr = ops->map_page(dev, page, offset, size, dir, attrs);
 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 
 	return addr;
@@ -322,7 +397,7 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
 
 	addr = phys_addr;
-	if (ops->map_resource)
+	if (ops && ops->map_resource)
 		addr = ops->map_resource(dev, phys_addr, size, dir, attrs);
 
 	debug_dma_map_resource(dev, phys_addr, size, dir, addr);
@@ -337,7 +412,7 @@ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->unmap_resource)
+	if (ops && ops->unmap_resource)
 		ops->unmap_resource(dev, addr, size, dir, attrs);
 	debug_dma_unmap_resource(dev, addr, size, dir);
 }
@@ -349,7 +424,9 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_cpu)
+	if (dma_is_direct(ops))
+		dma_direct_sync_single_for_cpu(dev, addr, size, dir);
+	else if (ops->sync_single_for_cpu)
 		ops->sync_single_for_cpu(dev, addr, size, dir);
 	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
 }
@@ -368,7 +445,9 @@ static inline void dma_sync_single_for_device(struct device *dev,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_device)
+	if (dma_is_direct(ops))
+		dma_direct_sync_single_for_device(dev, addr, size, dir);
+	else if (ops->sync_single_for_device)
 		ops->sync_single_for_device(dev, addr, size, dir);
 	debug_dma_sync_single_for_device(dev, addr, size, dir);
 }
@@ -387,7 +466,9 @@ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_sg_for_cpu)
+	if (dma_is_direct(ops))
+		dma_direct_sync_sg_for_cpu(dev, sg, nelems, dir);
+	else if (ops->sync_sg_for_cpu)
 		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
 	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
 }
@@ -399,7 +480,9 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_sg_for_device)
+	if (dma_is_direct(ops))
+		dma_direct_sync_sg_for_device(dev, sg, nelems, dir);
+	else if (ops->sync_sg_for_device)
 		ops->sync_sg_for_device(dev, sg, nelems, dir);
 	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
 

commit 90ac706e98fcb24fb0b0a259558987f33cc2f0f6
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Thu Dec 6 13:14:44 2018 -0800

    dma-mapping: factor out dummy DMA ops
    
    The dummy DMA ops are currently used by arm64 for any device which has
    an invalid ACPI description and is thus barred from using DMA due to not
    knowing whether is is cache-coherent or not. Factor these out into
    general dma-mapping code so that they can be referenced from other
    common code paths. In the process, we can prune all the optional
    callbacks which just do the same thing as the default behaviour, and
    fill in .map_resource for completeness.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    [hch: moved to a separate source file]
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0f0078490df4..269ee27fc3d9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -136,6 +136,7 @@ struct dma_map_ops {
 
 extern const struct dma_map_ops dma_direct_ops;
 extern const struct dma_map_ops dma_virt_ops;
+extern const struct dma_map_ops dma_dummy_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 

commit 8ddbe5943c0b1259b5ddb6dc1729863433fc256c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 12:47:50 2018 -0800

    dma-mapping: move dma_cache_sync out of line
    
    This isn't exactly a slow path routine, but it is not super critical
    either, and moving it out of line will help to keep the include chain
    clean for the following DMA indirection bypass work.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0bbce52606c2..0f0078490df4 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -411,16 +411,8 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
 #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
 
-static inline void
-dma_cache_sync(struct device *dev, void *vaddr, size_t size,
-		enum dma_data_direction dir)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!valid_dma_direction(dir));
-	if (ops->cache_sync)
-		ops->cache_sync(dev, vaddr, size, dir);
-}
+void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		enum dma_data_direction dir);
 
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,

commit 7249c1a52df9967cd23550f3dc24fb6ca43cdc6a
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 12:43:30 2018 -0800

    dma-mapping: move various slow path functions out of line
    
    There is no need to have all setup and coherent allocation / freeing
    routines inline.  Move them out of line to keep the implemeation
    nicely encapsulated and save some kernel text size.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 3b431cc58794..0bbce52606c2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -440,107 +440,24 @@ bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 bool dma_free_from_pool(void *start, size_t size);
 
-/**
- * dma_mmap_attrs - map a coherent DMA allocation into user space
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @vma: vm_area_struct describing requested user mapping
- * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
- * @handle: device-view address returned from dma_alloc_attrs
- * @size: size of memory originally requested in dma_alloc_attrs
- * @attrs: attributes of mapping properties requested in dma_alloc_attrs
- *
- * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
- * into user space.  The coherent DMA buffer must not be freed by the
- * driver until the user space mapping has been released.
- */
-static inline int
-dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
-	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-	if (ops->mmap)
-		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-}
-
+int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 
 int
 dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 		dma_addr_t dma_addr, size_t size, unsigned long attrs);
 
-static inline int
-dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
-		      dma_addr_t dma_addr, size_t size,
-		      unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-	if (ops->get_sgtable)
-		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
-					attrs);
-	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
-			attrs);
-}
-
+int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 
-#ifndef arch_dma_alloc_attrs
-#define arch_dma_alloc_attrs(dev)	(true)
-#endif
-
-static inline void *dma_alloc_attrs(struct device *dev, size_t size,
-				       dma_addr_t *dma_handle, gfp_t flag,
-				       unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	void *cpu_addr;
-
-	BUG_ON(!ops);
-	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
-
-	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
-		return cpu_addr;
-
-	/* let the implementation decide on the zone to allocate from: */
-	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
-
-	if (!arch_dma_alloc_attrs(&dev))
-		return NULL;
-	if (!ops->alloc)
-		return NULL;
-
-	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
-	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
-	return cpu_addr;
-}
-
-static inline void dma_free_attrs(struct device *dev, size_t size,
-				     void *cpu_addr, dma_addr_t dma_handle,
-				     unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!ops);
-
-	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
-		return;
-	/*
-	 * On non-coherent platforms which implement DMA-coherent buffers via
-	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
-	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
-	 * sleep on some machines, and b) an indication that the driver is
-	 * probably misusing the coherent API anyway.
-	 */
-	WARN_ON(irqs_disabled());
-
-	if (!ops->free || !cpu_addr)
-		return;
-
-	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
-	ops->free(dev, size, cpu_addr, dma_handle, attrs);
-}
+void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t flag, unsigned long attrs);
+void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle, unsigned long attrs);
 
 static inline void *dma_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
@@ -565,35 +482,9 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
-static inline void dma_check_mask(struct device *dev, u64 mask)
-{
-	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
-		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
-}
-
-static inline int dma_supported(struct device *dev, u64 mask)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	if (!ops)
-		return 0;
-	if (!ops->dma_supported)
-		return 1;
-	return ops->dma_supported(dev, mask);
-}
-
-#ifndef HAVE_ARCH_DMA_SET_MASK
-static inline int dma_set_mask(struct device *dev, u64 mask)
-{
-	if (!dev->dma_mask || !dma_supported(dev, mask))
-		return -EIO;
-
-	dma_check_mask(dev, mask);
-
-	*dev->dma_mask = mask;
-	return 0;
-}
-#endif
+int dma_supported(struct device *dev, u64 mask);
+int dma_set_mask(struct device *dev, u64 mask);
+int dma_set_coherent_mask(struct device *dev, u64 mask);
 
 static inline u64 dma_get_mask(struct device *dev)
 {
@@ -602,21 +493,6 @@ static inline u64 dma_get_mask(struct device *dev)
 	return DMA_BIT_MASK(32);
 }
 
-#ifdef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
-int dma_set_coherent_mask(struct device *dev, u64 mask);
-#else
-static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
-{
-	if (!dma_supported(dev, mask))
-		return -EIO;
-
-	dma_check_mask(dev, mask);
-
-	dev->coherent_dma_mask = mask;
-	return 0;
-}
-#endif
-
 /*
  * Set both the DMA mask and the coherent DMA mask to the same thing.
  * Note that we don't check the return value from dma_set_coherent_mask()

commit 7f0fee242e899f2eb42fd9e72bcfc3cb59aad1ce
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 12:24:27 2018 -0800

    dma-mapping: merge dma_unmap_page_attrs and dma_unmap_single_attrs
    
    The two functions are exactly the same, so don't bother implementing
    them twice.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8916499d2805..3b431cc58794 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -253,6 +253,12 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 	debug_dma_unmap_page(dev, addr, size, dir, true);
 }
 
+static inline void dma_unmap_page_attrs(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+	return dma_unmap_single_attrs(dev, addr, size, dir, attrs);
+}
+
 /*
  * dma_maps_sg_attrs returns 0 on error and > 0 on success.
  * It should never return a value < 0.
@@ -300,19 +306,6 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 	return addr;
 }
 
-static inline void dma_unmap_page_attrs(struct device *dev,
-					dma_addr_t addr, size_t size,
-					enum dma_data_direction dir,
-					unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!valid_dma_direction(dir));
-	if (ops->unmap_page)
-		ops->unmap_page(dev, addr, size, dir, attrs);
-	debug_dma_unmap_page(dev, addr, size, dir, false);
-}
-
 static inline dma_addr_t dma_map_resource(struct device *dev,
 					  phys_addr_t phys_addr,
 					  size_t size,

commit 8d59b5f2a44611d7327a2a14b36090d692186f60
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Dec 3 14:58:59 2018 +0100

    dma-mapping: simplify the dma_sync_single_range_for_{cpu,device} implementation
    
    We can just call the regular calls after adding offset the the address instead
    of reimplementing them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 7799c2b27849..8916499d2805 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -360,6 +360,13 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
 }
 
+static inline void dma_sync_single_range_for_cpu(struct device *dev,
+		dma_addr_t addr, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
+{
+	return dma_sync_single_for_cpu(dev, addr + offset, size, dir);
+}
+
 static inline void dma_sync_single_for_device(struct device *dev,
 					      dma_addr_t addr, size_t size,
 					      enum dma_data_direction dir)
@@ -372,32 +379,11 @@ static inline void dma_sync_single_for_device(struct device *dev,
 	debug_dma_sync_single_for_device(dev, addr, size, dir);
 }
 
-static inline void dma_sync_single_range_for_cpu(struct device *dev,
-						 dma_addr_t addr,
-						 unsigned long offset,
-						 size_t size,
-						 enum dma_data_direction dir)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_cpu)
-		ops->sync_single_for_cpu(dev, addr + offset, size, dir);
-	debug_dma_sync_single_range_for_cpu(dev, addr, offset, size, dir);
-}
-
 static inline void dma_sync_single_range_for_device(struct device *dev,
-						    dma_addr_t addr,
-						    unsigned long offset,
-						    size_t size,
-						    enum dma_data_direction dir)
+		dma_addr_t addr, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
 {
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_device)
-		ops->sync_single_for_device(dev, addr + offset, size, dir);
-	debug_dma_sync_single_range_for_device(dev, addr, offset, size, dir);
+	return dma_sync_single_for_device(dev, addr + offset, size, dir);
 }
 
 static inline void

commit b14b9d25a3c707c85e7e31e15766a71365b52ab7
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 30 10:59:37 2018 +0100

    dma-mapping: return an error code from dma_mapping_error
    
    Currently dma_mapping_error returns a boolean as int, with 1 meaning
    error.  This is rather unusual and many callers have to convert it to
    errno value.  The callers are highly inconsistent with error codes
    ranging from -ENOMEM over -EIO, -EINVAL and -EFAULT ranging to -EAGAIN.
    Return -ENOMEM which seems to be what the largest number of callers
    convert it to, and which also matches the typical error case where
    we are out of resources.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f4ac26d5294a..7799c2b27849 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -582,7 +582,7 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	debug_dma_mapping_error(dev, dma_addr);
 
 	if (dma_addr == DMA_MAPPING_ERROR)
-		return 1;
+		return -ENOMEM;
 	return 0;
 }
 

commit 68c9ac1d1fd51233cfac15484c6153b90aaa4ca4
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 4 14:33:24 2018 -0800

    dma-mapping: remove the mapping_error dma_map_ops method
    
    No users left except for vmd which just forwards it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f89d277cc8ed..f4ac26d5294a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -128,7 +128,6 @@ struct dma_map_ops {
 				   enum dma_data_direction dir);
 	void (*cache_sync)(struct device *dev, void *vaddr, size_t size,
 			enum dma_data_direction direction);
-	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
 	u64 (*get_required_mask)(struct device *dev);
 };
@@ -580,12 +579,8 @@ static inline void dma_free_coherent(struct device *dev, size_t size,
 
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
 	debug_dma_mapping_error(dev, dma_addr);
 
-	if (ops->mapping_error)
-		return ops->mapping_error(dev, dma_addr);
 	if (dma_addr == DMA_MAPPING_ERROR)
 		return 1;
 	return 0;

commit 42ee3cae0ed38b6c04038bf851ea2496da2135bb
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 21 18:52:35 2018 +0100

    dma-mapping: provide a generic DMA_MAPPING_ERROR
    
    Error handling of the dma_map_single and dma_map_page APIs is a little
    problematic at the moment, in that we use different encodings in the
    returned dma_addr_t to indicate an error.  That means we require an
    additional indirect call to figure out if a dma mapping call returned
    an error, and a lot of boilerplate code to implement these semantics.
    
    Instead return the maximum addressable value as the error.  As long
    as we don't allow mapping single-byte ranges with single-byte alignment
    this value can never be a valid return.  Additionaly if drivers do
    not check the return value from the dma_map* routines this values means
    they will generally not be pointed to actual memory.
    
    Once the default value is added here we can start removing the
    various mapping_error methods and just rely on this generic check.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 1a0edcde7d14..f89d277cc8ed 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -133,6 +133,8 @@ struct dma_map_ops {
 	u64 (*get_required_mask)(struct device *dev);
 };
 
+#define DMA_MAPPING_ERROR		(~(dma_addr_t)0)
+
 extern const struct dma_map_ops dma_direct_ops;
 extern const struct dma_map_ops dma_virt_ops;
 
@@ -581,8 +583,11 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	debug_dma_mapping_error(dev, dma_addr);
+
 	if (ops->mapping_error)
 		return ops->mapping_error(dev, dma_addr);
+	if (dma_addr == DMA_MAPPING_ERROR)
+		return 1;
 	return 0;
 }
 

commit 0c3b3171ceccb8830c2bb5adff1b4e9b204c1450
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 4 20:29:28 2018 +0100

    dma-mapping: move the arm64 noncoherent alloc/free support to common code
    
    The arm64 codebase to implement coherent dma allocation for architectures
    with non-coherent DMA is a good start for a generic implementation, given
    that is uses the generic remap helpers, provides the atomic pool for
    allocations that can't sleep and still is realtively simple and well
    tested.  Move it to kernel/dma and allow architectures to opt into it
    using a config symbol.  Architectures just need to provide a new
    arch_dma_prep_coherent helper to writeback an invalidate the caches
    for any memory that gets remapped for uncached access.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0f81c713f6e9..1a0edcde7d14 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -455,6 +455,11 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 			const void *caller);
 void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
 
+int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot);
+bool dma_in_atomic_pool(void *start, size_t size);
+void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
+bool dma_free_from_pool(void *start, size_t size);
+
 /**
  * dma_mmap_attrs - map a coherent DMA allocation into user space
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit c9d76d0655c06b8c1f944e46c4fd9e9cf4b331c0
Author: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
Date:   Wed Aug 29 23:29:21 2018 +0200

    dma-mapping: fix return type of dma_set_max_seg_size()
    
    The function dma_set_max_seg_size() can return either 0 on success or
    -EIO on error. Change its return type from unsigned int to int to
    capture this.
    
    Signed-off-by: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
    Reviewed-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 15bd41447025..0f81c713f6e9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -676,8 +676,7 @@ static inline unsigned int dma_get_max_seg_size(struct device *dev)
 	return SZ_64K;
 }
 
-static inline unsigned int dma_set_max_seg_size(struct device *dev,
-						unsigned int size)
+static inline int dma_set_max_seg_size(struct device *dev, unsigned int size)
 {
 	if (dev->dma_parms) {
 		dev->dma_parms->max_segment_size = size;

commit 7ed1d91a9ed61afe438ba51cbf49cb567ab7dca8
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 24 13:06:58 2018 +0200

    dma-mapping: translate __GFP_NOFAIL to DMA_ATTR_NO_WARN
    
    This allows all dma_map_ops instances to entirely rely on
    DMA_ATTR_NO_WARN going forward.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 547a48bcfa3d..15bd41447025 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -558,9 +558,11 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 }
 
 static inline void *dma_alloc_coherent(struct device *dev, size_t size,
-		dma_addr_t *dma_handle, gfp_t flag)
+		dma_addr_t *dma_handle, gfp_t gfp)
 {
-	return dma_alloc_attrs(dev, size, dma_handle, flag, 0);
+
+	return dma_alloc_attrs(dev, size, dma_handle, gfp,
+			(gfp & __GFP_NOWARN) ? DMA_ATTR_NO_WARN : 0);
 }
 
 static inline void dma_free_coherent(struct device *dev, size_t size,
@@ -794,8 +796,12 @@ static inline void dmam_release_declared_memory(struct device *dev)
 static inline void *dma_alloc_wc(struct device *dev, size_t size,
 				 dma_addr_t *dma_addr, gfp_t gfp)
 {
-	return dma_alloc_attrs(dev, size, dma_addr, gfp,
-			       DMA_ATTR_WRITE_COMBINE);
+	unsigned long attrs = DMA_ATTR_NO_WARN;
+
+	if (gfp & __GFP_NOWARN)
+		attrs |= DMA_ATTR_NO_WARN;
+
+	return dma_alloc_attrs(dev, size, dma_addr, gfp, attrs);
 }
 #ifndef dma_alloc_writecombine
 #define dma_alloc_writecombine dma_alloc_wc

commit 99c65fa7c59ff558e70db8aa61bbdece5d3a9588
Author: Stephen Boyd <swboyd@chromium.org>
Date:   Mon Oct 8 00:20:07 2018 -0700

    dma-debug: Check for drivers mapping invalid addresses in dma_map_single()
    
    I recently debugged a DMA mapping oops where a driver was trying to map
    a buffer returned from request_firmware() with dma_map_single(). Memory
    returned from request_firmware() is mapped into the vmalloc region and
    this isn't a valid region to map with dma_map_single() per the DMA
    documentation's "What memory is DMA'able?" section.
    
    Unfortunately, we don't really check that in the DMA debugging code, so
    enabling DMA debugging doesn't help catch this problem. Let's add a new
    DMA debug function to check for a vmalloc address or an invalid virtual
    address and print a warning if this happens. This makes it a little
    easier to debug these sorts of problems, instead of seeing odd behavior
    or crashes when drivers attempt to map the vmalloc space for DMA.
    
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Stephen Boyd <swboyd@chromium.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 562af6b45f23..547a48bcfa3d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -229,6 +229,7 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
+	debug_dma_map_single(dev, ptr, size);
 	addr = ops->map_page(dev, virt_to_page(ptr),
 			     offset_in_page(ptr), size,
 			     dir, attrs);

commit c6d4381220a0087ce19dbf6984d92c451bd6b364
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 6 19:27:24 2018 -0400

    dma-mapping: make the get_required_mask method available unconditionally
    
    This save some duplication for ia64, and makes the interface more
    general.  In the long run we want each dma_map_ops instance to fill this
    out, but this will take a little more prep work.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d23fc45c8208..562af6b45f23 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -130,9 +130,7 @@ struct dma_map_ops {
 			enum dma_data_direction direction);
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
-#ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
 	u64 (*get_required_mask)(struct device *dev);
-#endif
 };
 
 extern const struct dma_map_ops dma_direct_ops;

commit 1a0afc14b5da329765d6ecd4a79f546b9363ad8c
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 25 13:16:55 2018 -0700

    Revert "dma-mapping: clear dev->dma_ops in arch_teardown_dma_ops"
    
    This reverts commit 46053c73685411915d3de50c5a0045beef32806b.
    
    This change breaks architectures setting up dma_ops in their own magic
    way and not using arch_setup_dma_ops, so revert it.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index bd81e74cca7b..d23fc45c8208 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -665,10 +665,7 @@ static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
 #endif
 
 #ifndef arch_teardown_dma_ops
-static inline void arch_teardown_dma_ops(struct device *dev)
-{
-	dev->dma_ops = NULL;
-}
+static inline void arch_teardown_dma_ops(struct device *dev) { }
 #endif
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit 9406a49fd1f4379409ed87b29fdaa259b0441912
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Aug 23 09:39:38 2018 +0200

    dma-mapping: support non-coherent devices in dma_common_get_sgtable
    
    We can use the arch_dma_coherent_to_pfn hook to provide a ->get_sgtable
    implementation.  Note that this isn't an endorsement of this interface
    (which is a horrible bad idea), but it is required to move arm64 over
    to the generic code without a loss of functionality.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index c3378d4e0d57..bd81e74cca7b 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -483,8 +483,8 @@ dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 
 int
-dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
-		       void *cpu_addr, dma_addr_t dma_addr, size_t size);
+dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,
+		dma_addr_t dma_addr, size_t size, unsigned long attrs);
 
 static inline int
 dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
@@ -496,7 +496,8 @@ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 	if (ops->get_sgtable)
 		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
 					attrs);
-	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size);
+	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+			attrs);
 }
 
 #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)

commit 58b0440663ec11372befb8ead0ee7099d8878590
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 11 08:55:28 2018 +0200

    dma-mapping: consolidate the dma mmap implementations
    
    The only functional differences (modulo a few missing fixes in the arch
    code) is that architectures without coherent caches need a hook to
    convert a virtual or dma address into a pfn, given that we don't have
    the kernel linear mapping available for the otherwise easy virt_to_page
    call.  As a side effect we can support mmap of the per-device coherent
    area even on architectures not providing the callback, and we make
    previous dangerous default methods dma_common_mmap actually save for
    non-coherent architectures by rejecting it without the right helper.
    
    In addition to that we need a hook so that some architectures can
    override the protection bits when mmaping a dma coherent allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS parts

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8f2001181cd1..c3378d4e0d57 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -444,7 +444,8 @@ dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 }
 
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
-			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 
 void *dma_common_contiguous_remap(struct page *page, size_t size,
 			unsigned long vm_flags,
@@ -476,7 +477,7 @@ dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 	BUG_ON(!ops);
 	if (ops->mmap)
 		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
+	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
 }
 
 #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)

commit bc3ec75de5452db59b683487867ba562b950708a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Sep 8 11:22:43 2018 +0200

    dma-mapping: merge direct and noncoherent ops
    
    All the cache maintainance is already stubbed out when not enabled,
    but merging the two allows us to nicely handle the case where
    cache maintainance is required for some devices, but not others.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS parts

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index eafd6f318e78..8f2001181cd1 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -136,7 +136,6 @@ struct dma_map_ops {
 };
 
 extern const struct dma_map_ops dma_direct_ops;
-extern const struct dma_map_ops dma_noncoherent_ops;
 extern const struct dma_map_ops dma_virt_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))

commit 46053c73685411915d3de50c5a0045beef32806b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 24 10:29:02 2018 +0200

    dma-mapping: clear dev->dma_ops in arch_teardown_dma_ops
    
    There is no reason to leave the per-device dma_ops around when
    deconfiguring a device, so move this code from arm64 into the
    common code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 1423b69f3cc9..eafd6f318e78 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -664,7 +664,10 @@ static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
 #endif
 
 #ifndef arch_teardown_dma_ops
-static inline void arch_teardown_dma_ops(struct device *dev) { }
+static inline void arch_teardown_dma_ops(struct device *dev)
+{
+	dev->dma_ops = NULL;
+}
 #endif
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit dc3c05504d38849f77149cb962caeaedd1efa127
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 24 10:28:18 2018 +0200

    dma-mapping: remove dma_deconfigure
    
    This goes through a lot of hooks just to call arch_teardown_dma_ops.
    Replace it with a direct call instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 1c6c7c09bcf2..1423b69f3cc9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -753,12 +753,6 @@ dma_mark_declared_memory_occupied(struct device *dev,
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
-#ifdef CONFIG_HAS_DMA
-void dma_deconfigure(struct device *dev);
-#else
-static inline void dma_deconfigure(struct device *dev) {}
-#endif
-
 /*
  * Managed DMA API
  */

commit ccf640f4c9988653ef884672381b03b9be247bec
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 24 09:40:24 2018 +0200

    dma-mapping: remove dma_configure
    
    There is no good reason for this indirection given that the method
    always exists.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 1db6a6b46d0d..1c6c7c09bcf2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -754,14 +754,8 @@ dma_mark_declared_memory_occupied(struct device *dev,
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
 #ifdef CONFIG_HAS_DMA
-int dma_configure(struct device *dev);
 void dma_deconfigure(struct device *dev);
 #else
-static inline int dma_configure(struct device *dev)
-{
-	return 0;
-}
-
 static inline void dma_deconfigure(struct device *dev) {}
 #endif
 

commit d27fb99f62af7b79c542d161aa5155ed57271ddc
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jul 23 22:42:48 2018 +0100

    dma-mapping: relax warning for per-device areas
    
    The reasons why dma_free_attrs() should not be called from IRQ context
    are not necessarily obvious and somewhat buried in the development
    history, so let's start by documenting the warning itself to help anyone
    who does happen to hit it and wonder what the deal is.
    
    However, this check turns out to be slightly over-restrictive for the
    way that per-device memory has been spliced into the general API, since
    for that case we know that dma_declare_coherent_memory() has created an
    appropriate CPU mapping for the entire area and nothing dynamic should
    be happening. Given that the usage model for per-device memory is often
    more akin to streaming DMA than 'real' coherent DMA (e.g. allocating and
    freeing space to copy short-lived packets in and out), it is also
    somewhat more reasonable for those operations to happen in IRQ handlers
    for such devices.
    
    Therefore, let's move the irqs_disabled() check down past the per-device
    area hook, so that that gets a chance to resolve the request before we
    reach definite "you're doing it wrong" territory.
    
    Reported-by: Fredrik Noring <noring@nocrew.org>
    Tested-by: Fredrik Noring <noring@nocrew.org>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f9cc309507d9..1db6a6b46d0d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -538,10 +538,17 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!ops);
-	WARN_ON(irqs_disabled());
 
 	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
 		return;
+	/*
+	 * On non-coherent platforms which implement DMA-coherent buffers via
+	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
+	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
+	 * sleep on some machines, and b) an indication that the driver is
+	 * probably misusing the coherent API anyway.
+	 */
+	WARN_ON(irqs_disabled());
 
 	if (!ops->free || !cpu_addr)
 		return;

commit 0ead51c3fbd15a4bc91e984f1b18b5c9422fbb02
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 28 12:47:57 2018 +0200

    x86/pci-dma: switch the VIA 32-bit DMA quirk to use the struct device flag
    
    Instead of globally disabling > 32bit DMA using the arch_dma_supported
    hook walk the PCI bus under the actually affected bridge and mark every
    device with the dma_32bit_limit flag.  This also gets rid of the
    arch_dma_supported hook entirely.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 15e4b4e9cc20..f9cc309507d9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -572,14 +572,6 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
-/*
- * This is a hack for the legacy x86 forbid_dac and iommu_sac_force. Please
- * don't use this in new code.
- */
-#ifndef arch_dma_supported
-#define arch_dma_supported(dev, mask)	(1)
-#endif
-
 static inline void dma_check_mask(struct device *dev, u64 mask)
 {
 	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
@@ -592,9 +584,6 @@ static inline int dma_supported(struct device *dev, u64 mask)
 
 	if (!ops)
 		return 0;
-	if (!arch_dma_supported(dev, mask))
-		return 0;
-
 	if (!ops->dma_supported)
 		return 1;
 	return ops->dma_supported(dev, mask);

commit 884571f0de7b02bb784be3a5c870eabce62cdaeb
Author: Huaisheng Ye <yehs1@lenovo.com>
Date:   Fri May 25 13:00:00 2018 +0800

    dma-mapping: remove unused gfp_t parameter to arch_dma_alloc_attrs
    
    Signed-off-by: Huaisheng Ye <yehs1@lenovo.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4be070df5fc5..15e4b4e9cc20 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -502,7 +502,7 @@ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 
 #ifndef arch_dma_alloc_attrs
-#define arch_dma_alloc_attrs(dev, flag)	(true)
+#define arch_dma_alloc_attrs(dev)	(true)
 #endif
 
 static inline void *dma_alloc_attrs(struct device *dev, size_t size,
@@ -521,7 +521,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 	/* let the implementation decide on the zone to allocate from: */
 	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
 
-	if (!arch_dma_alloc_attrs(&dev, &flag))
+	if (!arch_dma_alloc_attrs(&dev))
 		return NULL;
 	if (!ops->alloc)
 		return NULL;

commit 782e6769c0df744e773dc2acff71c974b3bba4e9
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 16 15:24:51 2018 +0200

    dma-mapping: provide a generic dma-noncoherent implementation
    
    Add a new dma_map_ops implementation that uses dma-direct for the
    address mapping of streaming mappings, and which requires arch-specific
    implemenations of coherent allocate/free.
    
    Architectures have to provide flushing helpers to ownership trasnfers
    to the device and/or CPU, and can provide optional implementations of
    the coherent mmap functionality, and the cache_flush routines for
    non-coherent long term allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Alexey Brodkin <abrodkin@synopsys.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 25a9a2b04f78..4be070df5fc5 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -136,6 +136,7 @@ struct dma_map_ops {
 };
 
 extern const struct dma_map_ops dma_direct_ops;
+extern const struct dma_map_ops dma_noncoherent_ops;
 extern const struct dma_map_ops dma_virt_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))

commit f616ab59c294b6ea6efa94f6139ea3eda2f52be0
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed May 9 06:53:49 2018 +0200

    dma-mapping: move the NEED_DMA_MAP_STATE config symbol to lib/Kconfig
    
    This way we have one central definition of it, and user can select it as
    needed.  Note that we now also always select it when CONFIG_DMA_API_DEBUG
    is select, which fixes some incorrect checks in a few network drivers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index a6d4ac8b81e4..25a9a2b04f78 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -838,7 +838,7 @@ static inline int dma_mmap_wc(struct device *dev,
 #define dma_mmap_writecombine dma_mmap_wc
 #endif
 
-#if defined(CONFIG_NEED_DMA_MAP_STATE) || defined(CONFIG_DMA_API_DEBUG)
+#ifdef CONFIG_NEED_DMA_MAP_STATE
 #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME
 #define DEFINE_DMA_UNMAP_LEN(LEN_NAME)          __u32 LEN_NAME
 #define dma_unmap_addr(PTR, ADDR_NAME)           ((PTR)->ADDR_NAME)

commit 325ef1857fff8b2049322921e19421b6c5ad74e5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Apr 12 09:33:30 2018 +0200

    PCI: remove PCI_DMA_BUS_IS_PHYS
    
    This was used by the ide, scsi and networking code in the past to
    determine if they should bounce payloads.  Now that the dma mapping
    always have to support dma to all physical memory (thanks to swiotlb
    for non-iommu systems) there is no need to this crude hack any more.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Palmer Dabbelt <palmer@sifive.com> (for riscv)
    Reviewed-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f8ab1c0f589e..a6d4ac8b81e4 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -133,7 +133,6 @@ struct dma_map_ops {
 #ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
 	u64 (*get_required_mask)(struct device *dev);
 #endif
-	int is_phys;
 };
 
 extern const struct dma_map_ops dma_direct_ops;

commit 652ede37cec0bf7313927ecf4cebf0e79a0619f2
Merge: 1b2951dd99af 1f674e16f9ce
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 5 10:23:24 2018 -0700

    Merge tag 'dma-mapping-4.17' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
     "Very light this round as the interesting dma mapping changes went
      through the x86 tree.
    
      This just provides proper stubs for architectures not supporting dma
      (Geert Uytterhoeven)"
    
    * tag 'dma-mapping-4.17' of git://git.infradead.org/users/hch/dma-mapping:
      usb: gadget: Add NO_DMA dummies for DMA mapping API
      scsi: Add NO_DMA dummies for SCSI DMA mapping API
      mm: Add NO_DMA dummies for DMA pool API
      dma-coherent: Add NO_DMA dummies for managed DMA API
      dma-mapping: Convert NO_DMA get_dma_ops() into a real dummy

commit e89f5b37015309a8bdf0b21d08007580b92f92a4
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 28 15:35:35 2018 +0200

    dma-mapping: Don't clear GFP_ZERO in dma_alloc_attrs
    
    Revert the clearing of __GFP_ZERO in dma_alloc_attrs and move it to
    dma_direct_alloc for now.  While most common architectures always zero dma
    cohereny allocations (and x86 did so since day one) this is not documented
    and at least arc and s390 do not zero without the explicit __GFP_ZERO
    argument.
    
    Fixes: 57bf5a8963f8 ("dma-mapping: clear harmful GFP_* flags in common code")
    Reported-by: Evgeniy Didin <Evgeniy.Didin@synopsys.com>
    Reported-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Evgeniy Didin <Evgeniy.Didin@synopsys.com>
    Cc: iommu@lists.linux-foundation.org
    Link: https://lkml.kernel.org/r/20180328133535.17302-2-hch@lst.de

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index eb9eab4ecd6d..12fedcba9a9a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -518,12 +518,8 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 		return cpu_addr;
 
-	/*
-	 * Let the implementation decide on the zone to allocate from, and
-	 * decide on the way of zeroing the memory given that the memory
-	 * returned should always be zeroed.
-	 */
-	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM | __GFP_ZERO);
+	/* let the implementation decide on the zone to allocate from: */
+	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
 
 	if (!arch_dma_alloc_attrs(&dev, &flag))
 		return NULL;

commit ab642e952f80c66c5592f0e2c35588843a813df8
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Fri Mar 16 14:25:41 2018 +0100

    dma-coherent: Add NO_DMA dummies for managed DMA API
    
    Add dummies for dmam_{alloc,free}_coherent(), to allow compile-testing
    if NO_DMA=y.
    
    This prevents the following from showing up later:
    
        ERROR: "dmam_alloc_coherent" [drivers/net/ethernet/arc/arc_emac.ko] undefined!
        ERROR: "dmam_free_coherent" [drivers/net/ethernet/apm/xgene/xgene-enet.ko] undefined!
        ERROR: "dmam_alloc_coherent" [drivers/net/ethernet/apm/xgene/xgene-enet.ko] undefined!
        ERROR: "dmam_alloc_coherent" [drivers/mtd/nand/hisi504_nand.ko] undefined!
        ERROR: "dmam_alloc_coherent" [drivers/mmc/host/dw_mmc.ko] undefined!
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: Mark Brown <broonie@kernel.org>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 5ea7eec83c0f..94f41846b933 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -776,10 +776,19 @@ static inline void dma_deconfigure(struct device *dev) {}
 /*
  * Managed DMA API
  */
+#ifdef CONFIG_HAS_DMA
 extern void *dmam_alloc_coherent(struct device *dev, size_t size,
 				 dma_addr_t *dma_handle, gfp_t gfp);
 extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
 			       dma_addr_t dma_handle);
+#else /* !CONFIG_HAS_DMA */
+static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
+					dma_addr_t *dma_handle, gfp_t gfp)
+{ return NULL; }
+static inline void dmam_free_coherent(struct device *dev, size_t size,
+				      void *vaddr, dma_addr_t dma_handle) { }
+#endif /* !CONFIG_HAS_DMA */
+
 extern void *dmam_alloc_attrs(struct device *dev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t gfp,
 			      unsigned long attrs);

commit f29ab49b5388b2f829cf99859bc5f8ad8ec4d06a
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Fri Mar 16 14:25:40 2018 +0100

    dma-mapping: Convert NO_DMA get_dma_ops() into a real dummy
    
    If NO_DMA=y, get_dma_ops() returns a reference to the
    non-existing symbol bad_dma_ops, thus causing a link failure if it is
    ever used.
    
    Make get_dma_ops() return NULL instead, to avoid the link failure.
    This allows to improve compile-testing, and limits the need to keep on
    sprinkling dependencies on HAS_DMA all over the place.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: Mark Brown <broonie@kernel.org>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index eb9eab4ecd6d..5ea7eec83c0f 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -212,14 +212,14 @@ static inline void set_dma_ops(struct device *dev,
 }
 #else
 /*
- * Define the dma api to allow compilation but not linking of
- * dma dependent code.  Code that depends on the dma-mapping
- * API needs to set 'depends on HAS_DMA' in its Kconfig
+ * Define the dma api to allow compilation of dma dependent code.
+ * Code that depends on the dma-mapping API needs to set 'depends on HAS_DMA'
+ * in its Kconfig, unless it already depends on <something> || COMPILE_TEST,
+ * where <something> guarantuees the availability of the dma-mapping API.
  */
-extern const struct dma_map_ops bad_dma_ops;
 static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	return &bad_dma_ops;
+	return NULL;
 }
 #endif
 

commit ecc2dc55ce79945c2e0a04977706a99dc4848229
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Feb 10 09:43:49 2018 +0100

    dma-mapping: fix a comment typo
    
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 34fe8463d10e..eb9eab4ecd6d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -578,7 +578,7 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 
 /*
  * This is a hack for the legacy x86 forbid_dac and iommu_sac_force. Please
- * don't use this is new code.
+ * don't use this in new code.
  */
 #ifndef arch_dma_supported
 #define arch_dma_supported(dev, mask)	(1)

commit 002e67454f61bb67d8071ac4d0cacb86a01d18e0
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 9 16:30:23 2018 +0100

    dma-direct: rename dma_noop to dma_direct
    
    The trivial direct mapping implementation already does a virtual to
    physical translation which isn't strictly a noop, and will soon learn
    to do non-direct but linear physical to dma translations through the
    device offset and a few small tricks.  Rename it to a better fitting
    name.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 46542ad9d709..34fe8463d10e 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -136,7 +136,7 @@ struct dma_map_ops {
 	int is_phys;
 };
 
-extern const struct dma_map_ops dma_noop_ops;
+extern const struct dma_map_ops dma_direct_ops;
 extern const struct dma_map_ops dma_virt_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))

commit cea9d03c822cf1b8e90cc4fc51be6d248fb5d776
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Dec 23 11:01:41 2017 +0100

    dma-mapping: add an arch_dma_supported hook
    
    To implement the x86 forbid_dac and iommu_sac_force we want an arch hook
    so that it can apply the global options across all dma_map_ops
    implementations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d036e78b9ae9..46542ad9d709 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -576,6 +576,14 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
+/*
+ * This is a hack for the legacy x86 forbid_dac and iommu_sac_force. Please
+ * don't use this is new code.
+ */
+#ifndef arch_dma_supported
+#define arch_dma_supported(dev, mask)	(1)
+#endif
+
 static inline void dma_check_mask(struct device *dev, u64 mask)
 {
 	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
@@ -588,6 +596,9 @@ static inline int dma_supported(struct device *dev, u64 mask)
 
 	if (!ops)
 		return 0;
+	if (!arch_dma_supported(dev, mask))
+		return 0;
+
 	if (!ops->dma_supported)
 		return 1;
 	return ops->dma_supported(dev, mask);

commit 57bf5a8963f80fb3828c46c3e3a5b2dd790e09a7
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 22 16:05:15 2017 +0100

    dma-mapping: clear harmful GFP_* flags in common code
    
    Lift the code from x86 so that we behave consistently.  In the future we
    should probably warn if any of these is set.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 16add66d5b68..d036e78b9ae9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -518,6 +518,13 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 		return cpu_addr;
 
+	/*
+	 * Let the implementation decide on the zone to allocate from, and
+	 * decide on the way of zeroing the memory given that the memory
+	 * returned should always be zeroed.
+	 */
+	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM | __GFP_ZERO);
+
 	if (!arch_dma_alloc_attrs(&dev, &flag))
 		return NULL;
 	if (!ops->alloc)

commit 205e1b7f51e4af2643eb1d61a6503e415cd1e014
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 22 14:50:47 2017 +0100

    dma-mapping: warn when there is no coherent_dma_mask
    
    These days all devices should have a DMA coherent mask, and most dma_ops
    implementations rely on that fact.  But just to be sure add an assert to
    ring the warning bell if that is not the case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d84951865be7..16add66d5b68 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -513,6 +513,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 	void *cpu_addr;
 
 	BUG_ON(!ops);
+	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
 
 	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 		return cpu_addr;

commit a41ef1e455a9796be8cb986f0616f52453ac8e4b
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 30 07:32:51 2017 -0800

    dma-mapping: take dma_pfn_offset into account in dma_max_pfn
    
    This makes sure the generic version can be used with architectures /
    devices that have a DMA offset in the direct mapping.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 81ed9b2d84dc..d84951865be7 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -692,7 +692,7 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 #ifndef dma_max_pfn
 static inline unsigned long dma_max_pfn(struct device *dev)
 {
-	return *dev->dma_mask >> PAGE_SHIFT;
+	return (*dev->dma_mask >> PAGE_SHIFT) + dev->dma_pfn_offset;
 }
 #endif
 

commit 13231caccedffcceff1580b78e6d99d8b0a43404
Merge: e6cdd80a8342 c2e8fbf908af
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 5 10:31:32 2017 -0800

    Merge tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI fixes from James Bottomley:
     "A bunch of fixes for aacraid, a set of coherency fixes that only
      affect non-coherent platforms and one coccinelle detected null check
      after use"
    
    * tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:
      scsi: libsas: align sata_device's rps_resp on a cacheline
      scsi: use dma_get_cache_alignment() as minimum DMA alignment
      scsi: dma-mapping: always provide dma_get_cache_alignment
      scsi: ufs: ufshcd: fix potential NULL pointer dereference in ufshcd_config_vreg
      scsi: aacraid: Prevent crash in case of free interrupt during scsi EH path
      scsi: aacraid: Perform initialization reset only once
      scsi: aacraid: Check for PCI state of device in a generic way

commit 860dd4424f344400b491b212ee4acb3a358ba9d9
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 21 14:23:37 2017 +0100

    scsi: dma-mapping: always provide dma_get_cache_alignment
    
    Provide the dummy version of dma_get_cache_alignment that always returns
    1 even if CONFIG_HAS_DMA is not set, so that drivers and subsystems can
    use it without ifdefs.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index eee1499db396..29cfd18360be 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -710,7 +710,6 @@ static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
 	return ret;
 }
 
-#ifdef CONFIG_HAS_DMA
 static inline int dma_get_cache_alignment(void)
 {
 #ifdef ARCH_DMA_MINALIGN
@@ -718,7 +717,6 @@ static inline int dma_get_cache_alignment(void)
 #endif
 	return 1;
 }
-#endif
 
 /* flags for the coherent memory api */
 #define DMA_MEMORY_EXCLUSIVE		0x01

commit 4950276672fce5c241857540f8561c440663673d
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:51 2017 -0800

    kmemcheck: remove annotations
    
    Patch series "kmemcheck: kill kmemcheck", v2.
    
    As discussed at LSF/MM, kill kmemcheck.
    
    KASan is a replacement that is able to work without the limitation of
    kmemcheck (single CPU, slow).  KASan is already upstream.
    
    We are also not aware of any users of kmemcheck (or users who don't
    consider KASan as a suitable replacement).
    
    The only objection was that since KASAN wasn't supported by all GCC
    versions provided by distros at that time we should hold off for 2
    years, and try again.
    
    Now that 2 years have passed, and all distros provide gcc that supports
    KASAN, kill kmemcheck again for the very same reasons.
    
    This patch (of 4):
    
    Remove kmemcheck annotations, and calls to kmemcheck from the kernel.
    
    [alexander.levin@verizon.com: correctly remove kmemcheck call from dma_map_sg_attrs]
      Link: http://lkml.kernel.org/r/20171012192151.26531-1-alexander.levin@verizon.com
    Link: http://lkml.kernel.org/r/20171007030159.22241-2-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index eee1499db396..e8f8e8fb244d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -9,7 +9,6 @@
 #include <linux/dma-debug.h>
 #include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
-#include <linux/kmemcheck.h>
 #include <linux/bug.h>
 #include <linux/mem_encrypt.h>
 
@@ -232,7 +231,6 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
-	kmemcheck_mark_initialized(ptr, size);
 	BUG_ON(!valid_dma_direction(dir));
 	addr = ops->map_page(dev, virt_to_page(ptr),
 			     offset_in_page(ptr), size,
@@ -265,11 +263,8 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 				   unsigned long attrs)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
-	int i, ents;
-	struct scatterlist *s;
+	int ents;
 
-	for_each_sg(sg, s, nents, i)
-		kmemcheck_mark_initialized(sg_virt(s), s->length);
 	BUG_ON(!valid_dma_direction(dir));
 	ents = ops->map_sg(dev, sg, nents, dir, attrs);
 	BUG_ON(ents < 0);
@@ -299,7 +294,6 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
-	kmemcheck_mark_initialized(page_address(page) + offset, size);
 	BUG_ON(!valid_dma_direction(dir));
 	addr = ops->map_page(dev, page, offset, size, dir, attrs);
 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);

commit e37e0ee0190034a059c9faea8adfb4982fb24ddd
Merge: 23c258763ba9 c9eb6172c328
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 16:54:12 2017 -0800

    Merge tag 'dma-mapping-4.15' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - turn dma_cache_sync into a dma_map_ops instance and remove
       implementation that purely are dead because the architecture doesn't
       support noncoherent allocations
    
     - add a flag for busses that need DMA configuration (Robin Murphy)
    
    * tag 'dma-mapping-4.15' of git://git.infradead.org/users/hch/dma-mapping:
      dma-mapping: turn dma_cache_sync into a dma_map_ops method
      sh: make dma_cache_sync a no-op
      xtensa: make dma_cache_sync a no-op
      unicore32: make dma_cache_sync a no-op
      powerpc: make dma_cache_sync a no-op
      mn10300: make dma_cache_sync a no-op
      microblaze: make dma_cache_sync a no-op
      ia64: make dma_cache_sync a no-op
      frv: make dma_cache_sync a no-op
      x86: make dma_cache_sync a no-op
      floppy: consolidate the dummy fd_cacheflush definition
      drivers: flag buses which demand DMA configuration

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 29ce9815da87..7653ea66874d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_DMA_MAPPING_H
 #define _LINUX_DMA_MAPPING_H
 

commit c9eb6172c328dde7e14812f94f8da87b691e41b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Aug 27 10:37:15 2017 +0200

    dma-mapping: turn dma_cache_sync into a dma_map_ops method
    
    After we removed all the dead wood it turns out only two architectures
    actually implement dma_cache_sync as a real op: mips and parisc.  Add
    a cache_sync method to struct dma_map_ops and implement it for the
    mips defualt DMA ops, and the parisc pa11 ops.
    
    Note that arm, arc and openrisc support DMA_ATTR_NON_CONSISTENT, but
    never provided a functional dma_cache_sync implementations, which
    seems somewhat odd.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 29ce9815da87..028a375d240d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -126,6 +126,8 @@ struct dma_map_ops {
 	void (*sync_sg_for_device)(struct device *dev,
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction dir);
+	void (*cache_sync)(struct device *dev, void *vaddr, size_t size,
+			enum dma_data_direction direction);
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
 #ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
@@ -436,6 +438,17 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
 #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
 
+static inline void
+dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		enum dma_data_direction dir)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->cache_sync)
+		ops->cache_sync(dev, vaddr, size, dir);
+}
+
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 

commit 52269718dc2cf2585d7a2828f31d46ef46e68000
Merge: ae71948f398e d35b0996fef3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 12 13:30:06 2017 -0700

    Merge tag 'dma-mapping-4.14' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - removal of the old dma_alloc_noncoherent interface
    
     - remove unused flags to dma_declare_coherent_memory
    
     - restrict OF DMA configuration to specific physical busses
    
     - use the iommu mailing list for dma-mapping questions and patches
    
    * tag 'dma-mapping-4.14' of git://git.infradead.org/users/hch/dma-mapping:
      dma-coherent: fix dma_declare_coherent_memory() logic error
      ARM: imx: mx31moboard: Remove unused 'dma' variable
      dma-coherent: remove an unused variable
      MAINTAINERS: use the iommu list for the dma-mapping subsystem
      dma-coherent: remove the DMA_MEMORY_MAP and DMA_MEMORY_IO flags
      dma-coherent: remove the DMA_MEMORY_INCLUDES_CHILDREN flag
      of: restrict DMA configuration
      dma-mapping: remove dma_alloc_noncoherent and dma_free_noncoherent
      i825xx: switch to switch to dma_alloc_attrs
      au1000_eth: switch to dma_alloc_attrs
      sgiseeq: switch to dma_alloc_attrs
      dma-mapping: reduce dma_mapping_error inline bloat

commit 2436bdcda53ff4abb7897c87fa29ef3de8055344
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 25 17:13:09 2017 +0200

    dma-coherent: remove the DMA_MEMORY_MAP and DMA_MEMORY_IO flags
    
    DMA_MEMORY_IO was never used in the tree, so remove it.  That means there is
    no need for the DMA_MEMORY_MAP flag either now, so remove it as well and
    change dma_declare_coherent_memory to return a normal errno value.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
     Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index abf369b11f54..b7dd11c02a45 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -694,9 +694,7 @@ static inline int dma_get_cache_alignment(void)
 #endif
 
 /* flags for the coherent memory api */
-#define	DMA_MEMORY_MAP			0x01
-#define DMA_MEMORY_IO			0x02
-#define DMA_MEMORY_EXCLUSIVE		0x04
+#define DMA_MEMORY_EXCLUSIVE		0x01
 
 #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
@@ -709,7 +707,7 @@ static inline int
 dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 			    dma_addr_t device_addr, size_t size, int flags)
 {
-	return 0;
+	return -ENOSYS;
 }
 
 static inline void

commit b32dbc1e0bf0eaa8bdd725491b361d5fc2f57a85
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 25 17:12:18 2017 +0200

    dma-coherent: remove the DMA_MEMORY_INCLUDES_CHILDREN flag
    
    This flag was never implemented or used.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4c98cc96971f..abf369b11f54 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -696,8 +696,7 @@ static inline int dma_get_cache_alignment(void)
 /* flags for the coherent memory api */
 #define	DMA_MEMORY_MAP			0x01
 #define DMA_MEMORY_IO			0x02
-#define DMA_MEMORY_INCLUDES_CHILDREN	0x04
-#define DMA_MEMORY_EXCLUSIVE		0x08
+#define DMA_MEMORY_EXCLUSIVE		0x04
 
 #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,

commit 2fd523c57e520899e05ec663d04743005bcef5b2
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 25 17:06:13 2017 +0200

    dma-mapping: remove dma_alloc_noncoherent and dma_free_noncoherent
    
    No users left, everyone switched to the _attrs versions.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 66d8ea68f40b..4c98cc96971f 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -549,20 +549,6 @@ static inline void dma_free_coherent(struct device *dev, size_t size,
 	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
 }
 
-static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
-		dma_addr_t *dma_handle, gfp_t gfp)
-{
-	return dma_alloc_attrs(dev, size, dma_handle, gfp,
-			       DMA_ATTR_NON_CONSISTENT);
-}
-
-static inline void dma_free_noncoherent(struct device *dev, size_t size,
-		void *cpu_addr, dma_addr_t dma_handle)
-{
-	dma_free_attrs(dev, size, cpu_addr, dma_handle,
-		       DMA_ATTR_NON_CONSISTENT);
-}
-
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);

commit 413d63d71b222108d19703f3fd5cf9108652a730
Merge: d6c8103b0265 90a6cd503982
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Aug 26 09:19:13 2017 +0200

    Merge branch 'linus' into x86/mm to pick up fixes and to fix conflicts
    
    Conflicts:
            arch/x86/kernel/head64.c
            arch/x86/mm/mmap.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5237e95f63761477b7ea45499d08b89383a77eab
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jul 24 18:29:27 2017 +0100

    dma-mapping: reduce dma_mapping_error inline bloat
    
    Thanks to the nested inlining, all drivers correctly calling
    dma_mapping_error() after a mapping a page or single buffer generate two
    calls to get_arch_dma_ops() per callsite, which all adds up to a fair
    old chunk of useless code, e.g. ~3KB for an arm64 defconfig plus extras:
    
       text    data     bss     dec     hex filename
    13051391        1503898  327768 14883057         e318f1 vmlinux.o.old
    13050751        1503898  327768 14882417         e31671 vmlinux.o.new
    
    Give the compiler a hand by making it clear we want the same ops.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 03c0196a6f24..66d8ea68f40b 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -565,10 +565,11 @@ static inline void dma_free_noncoherent(struct device *dev, size_t size,
 
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
-	debug_dma_mapping_error(dev, dma_addr);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
-	if (get_dma_ops(dev)->mapping_error)
-		return get_dma_ops(dev)->mapping_error(dev, dma_addr);
+	debug_dma_mapping_error(dev, dma_addr);
+	if (ops->mapping_error)
+		return ops->mapping_error(dev, dma_addr);
 	return 0;
 }
 

commit 43fc509c3efb5c973991ee24c449ab2a0d71dd1e
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Thu Jul 20 11:19:58 2017 +0100

    dma-coherent: introduce interface for default DMA pool
    
    Christoph noticed [1] that default DMA pool in current form overload
    the DMA coherent infrastructure. In reply, Robin suggested [2] to
    split the per-device vs. global pool interfaces, so allocation/release
    from default DMA pool is driven by dma ops implementation.
    
    This patch implements Robin's idea and provide interface to
    allocate/release/mmap the default (aka global) DMA pool.
    
    To make it clear that existing *_from_coherent routines work on
    per-device pool rename them to *_from_dev_coherent.
    
    [1] https://lkml.org/lkml/2017/7/7/370
    [2] https://lkml.org/lkml/2017/7/7/431
    
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Suggested-by: Robin Murphy <robin.murphy@arm.com>
    Tested-by: Andras Szemzo <sza@esh.hu>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 843ab866e0f4..03c0196a6f24 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -157,16 +157,40 @@ static inline int is_device_dma_capable(struct device *dev)
  * These three functions are only for dma allocator.
  * Don't use them in device drivers.
  */
-int dma_alloc_from_coherent(struct device *dev, ssize_t size,
+int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,
 				       dma_addr_t *dma_handle, void **ret);
-int dma_release_from_coherent(struct device *dev, int order, void *vaddr);
+int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr);
 
-int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
+int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,
 			    void *cpu_addr, size_t size, int *ret);
+
+void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle);
+int dma_release_from_global_coherent(int order, void *vaddr);
+int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,
+				  size_t size, int *ret);
+
 #else
-#define dma_alloc_from_coherent(dev, size, handle, ret) (0)
-#define dma_release_from_coherent(dev, order, vaddr) (0)
-#define dma_mmap_from_coherent(dev, vma, vaddr, order, ret) (0)
+#define dma_alloc_from_dev_coherent(dev, size, handle, ret) (0)
+#define dma_release_from_dev_coherent(dev, order, vaddr) (0)
+#define dma_mmap_from_dev_coherent(dev, vma, vaddr, order, ret) (0)
+
+static inline void *dma_alloc_from_global_coherent(ssize_t size,
+						   dma_addr_t *dma_handle)
+{
+	return NULL;
+}
+
+static inline int dma_release_from_global_coherent(int order, void *vaddr)
+{
+	return 0;
+}
+
+static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,
+						void *cpu_addr, size_t size,
+						int *ret)
+{
+	return 0;
+}
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
 #ifdef CONFIG_HAS_DMA
@@ -481,7 +505,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 
 	BUG_ON(!ops);
 
-	if (dma_alloc_from_coherent(dev, size, dma_handle, &cpu_addr))
+	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
 		return cpu_addr;
 
 	if (!arch_dma_alloc_attrs(&dev, &flag))
@@ -503,7 +527,7 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 	BUG_ON(!ops);
 	WARN_ON(irqs_disabled());
 
-	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
+	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
 		return;
 
 	if (!ops->free || !cpu_addr)

commit 648babb7078c6310d2af5b8aa01f086030916968
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jul 17 16:10:22 2017 -0500

    swiotlb: Add warnings for use of bounce buffers with SME
    
    Add warnings to let the user know when bounce buffers are being used for
    DMA when SME is active.  Since the bounce buffers are not in encrypted
    memory, these notifications are to allow the user to determine some
    appropriate action - if necessary.  Actions can range from utilizing an
    IOMMU, replacing the device with another device that can support 64-bit
    DMA, ignoring the message if the device isn't used much, etc.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brijesh Singh <brijesh.singh@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Toshimitsu Kani <toshi.kani@hpe.com>
    Cc: kasan-dev@googlegroups.com
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-efi@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/d112564053c3f2e86ca634a8d4fa4abc0eb53a6a.1500319216.git.thomas.lendacky@amd.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 843ab866e0f4..fce2369ecf82 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -10,6 +10,7 @@
 #include <linux/scatterlist.h>
 #include <linux/kmemcheck.h>
 #include <linux/bug.h>
+#include <linux/mem_encrypt.h>
 
 /**
  * List of possible attributes associated with a DMA mapping. The semantics
@@ -548,6 +549,12 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
+static inline void dma_check_mask(struct device *dev, u64 mask)
+{
+	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
+		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
+}
+
 static inline int dma_supported(struct device *dev, u64 mask)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
@@ -564,6 +571,9 @@ static inline int dma_set_mask(struct device *dev, u64 mask)
 {
 	if (!dev->dma_mask || !dma_supported(dev, mask))
 		return -EIO;
+
+	dma_check_mask(dev, mask);
+
 	*dev->dma_mask = mask;
 	return 0;
 }
@@ -583,6 +593,9 @@ static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
 {
 	if (!dma_supported(dev, mask))
 		return -EIO;
+
+	dma_check_mask(dev, mask);
+
 	dev->coherent_dma_mask = mask;
 	return 0;
 }

commit 63d36c95500400642f656ba1970980746cf437f3
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 12 19:15:04 2017 +0200

    dma-mapping: replace dmam_alloc_noncoherent with dmam_alloc_attrs
    
    dmam_alloc_noncoherent is a trivial wrapper around dmam_alloc_attrs,
    that hardcodes one particular flag.  Make the devres code more
    flexible by allowing the callers to pass arbitrary flags.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4038dd34afa3..843ab866e0f4 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -734,8 +734,9 @@ extern void *dmam_alloc_coherent(struct device *dev, size_t size,
 				 dma_addr_t *dma_handle, gfp_t gfp);
 extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
 			       dma_addr_t dma_handle);
-extern void *dmam_alloc_noncoherent(struct device *dev, size_t size,
-				    dma_addr_t *dma_handle, gfp_t gfp);
+extern void *dmam_alloc_attrs(struct device *dev, size_t size,
+			      dma_addr_t *dma_handle, gfp_t gfp,
+			      unsigned long attrs);
 #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 extern int dmam_declare_coherent_memory(struct device *dev,
 					phys_addr_t phys_addr,

commit 03b643866d889d6edc87cdcee2b3880b7879a441
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 12 19:05:09 2017 +0200

    dma-mapping: remove dmam_free_noncoherent
    
    This function was never used since it was added.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 527f2ed8c645..4038dd34afa3 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -736,8 +736,6 @@ extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
 			       dma_addr_t dma_handle);
 extern void *dmam_alloc_noncoherent(struct device *dev, size_t size,
 				    dma_addr_t *dma_handle, gfp_t gfp);
-extern void dmam_free_noncoherent(struct device *dev, size_t size, void *vaddr,
-				  dma_addr_t dma_handle);
 #ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 extern int dmam_declare_coherent_memory(struct device *dev,
 					phys_addr_t phys_addr,

commit 8cc9c26029d8ac3c627ecf8545b617fb78def5d4
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 5 17:05:25 2017 +0200

    dma-mapping: remove the set_dma_mask method
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 3e5908656226..527f2ed8c645 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -127,7 +127,6 @@ struct dma_map_ops {
 				   enum dma_data_direction dir);
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
-	int (*set_dma_mask)(struct device *dev, u64 mask);
 #ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
 	u64 (*get_required_mask)(struct device *dev);
 #endif
@@ -563,11 +562,6 @@ static inline int dma_supported(struct device *dev, u64 mask)
 #ifndef HAVE_ARCH_DMA_SET_MASK
 static inline int dma_set_mask(struct device *dev, u64 mask)
 {
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	if (ops->set_dma_mask)
-		return ops->set_dma_mask(dev, mask);
-
 	if (!dev->dma_mask || !dma_supported(dev, mask))
 		return -EIO;
 	*dev->dma_mask = mask;

commit 447d899b18169b2ee5e42b2fa8b32dbb40a30a24
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 11:40:56 2017 +0200

    dma-mapping: remove HAVE_ARCH_DMA_SUPPORTED
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index a57875309bfd..3e5908656226 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -549,7 +549,6 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
-#ifndef HAVE_ARCH_DMA_SUPPORTED
 static inline int dma_supported(struct device *dev, u64 mask)
 {
 	const struct dma_map_ops *ops = get_dma_ops(dev);
@@ -560,7 +559,6 @@ static inline int dma_supported(struct device *dev, u64 mask)
 		return 1;
 	return ops->dma_supported(dev, mask);
 }
-#endif
 
 #ifndef HAVE_ARCH_DMA_SET_MASK
 static inline int dma_set_mask(struct device *dev, u64 mask)

commit f51f288e237cbcfd3dbd1d4fa2d3dec00d7253e2
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 10:58:49 2017 +0200

    dma-mapping: remove DMA_ERROR_CODE
    
    And update the documentation - dma_mapping_error has been supported
    everywhere for a long time.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4f3eecedca2d..a57875309bfd 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -546,12 +546,7 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 
 	if (get_dma_ops(dev)->mapping_error)
 		return get_dma_ops(dev)->mapping_error(dev, dma_addr);
-
-#ifdef DMA_ERROR_CODE
-	return dma_addr == DMA_ERROR_CODE;
-#else
 	return 0;
-#endif
 }
 
 #ifndef HAVE_ARCH_DMA_SUPPORTED

commit 09515ef5ddad71c7820e5e428da418b709feeb26
Author: Sricharan R <sricharan@codeaurora.org>
Date:   Mon Apr 10 16:51:01 2017 +0530

    of/acpi: Configure dma operations at probe time for platform/amba/pci bus devices
    
    Configuring DMA ops at probe time will allow deferring device probe when
    the IOMMU isn't available yet. The dma_configure for the device is
    now called from the generic device_attach callback just before the
    bus/driver probe is called. This way, configuring the DMA ops for the
    device would be called at the same place for all bus_types, hence the
    deferred probing mechanism should work for all buses as well.
    
    pci_bus_add_devices    (platform/amba)(_device_create/driver_register)
           |                         |
    pci_bus_add_device     (device_add/driver_register)
           |                         |
    device_attach           device_initial_probe
           |                         |
    __device_attach_driver    __device_attach_driver
           |
    driver_probe_device
           |
    really_probe
           |
    dma_configure
    
    Similarly on the device/driver_unregister path __device_release_driver is
    called which inturn calls dma_deconfigure.
    
    This patch changes the dma ops configuration to probe time for
    both OF and ACPI based platform/amba/pci bus devices.
    
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Rob Herring <robh@kernel.org>
    Acked-by: Bjorn Helgaas <bhelgaas@google.com> (drivers/pci part)
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sricharan R <sricharan@codeaurora.org>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0977317c6835..4f3eecedca2d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -728,6 +728,18 @@ dma_mark_declared_memory_occupied(struct device *dev,
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
+#ifdef CONFIG_HAS_DMA
+int dma_configure(struct device *dev);
+void dma_deconfigure(struct device *dev);
+#else
+static inline int dma_configure(struct device *dev)
+{
+	return 0;
+}
+
+static inline void dma_deconfigure(struct device *dev) {}
+#endif
+
 /*
  * Managed DMA API
  */

commit ac1820fb286b552b6885d40ab34f1e59b815f1f1
Merge: edccb5942965 0bbb3b7496ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 25 13:45:43 2017 -0800

    Merge tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma DMA mapping updates from Doug Ledford:
     "Drop IB DMA mapping code and use core DMA code instead.
    
      Bart Van Assche noted that the ib DMA mapping code was significantly
      similar enough to the core DMA mapping code that with a few changes it
      was possible to remove the IB DMA mapping code entirely and switch the
      RDMA stack to use the core DMA mapping code.
    
      This resulted in a nice set of cleanups, but touched the entire tree
      and has been kept separate for that reason."
    
    * tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (37 commits)
      IB/rxe, IB/rdmavt: Use dma_virt_ops instead of duplicating it
      IB/core: Remove ib_device.dma_device
      nvme-rdma: Switch from dma_device to dev.parent
      RDS: net: Switch from dma_device to dev.parent
      IB/srpt: Modify a debug statement
      IB/srp: Switch from dma_device to dev.parent
      IB/iser: Switch from dma_device to dev.parent
      IB/IPoIB: Switch from dma_device to dev.parent
      IB/rxe: Switch from dma_device to dev.parent
      IB/vmw_pvrdma: Switch from dma_device to dev.parent
      IB/usnic: Switch from dma_device to dev.parent
      IB/qib: Switch from dma_device to dev.parent
      IB/qedr: Switch from dma_device to dev.parent
      IB/ocrdma: Switch from dma_device to dev.parent
      IB/nes: Remove a superfluous assignment statement
      IB/mthca: Switch from dma_device to dev.parent
      IB/mlx5: Switch from dma_device to dev.parent
      IB/mlx4: Switch from dma_device to dev.parent
      IB/i40iw: Remove a superfluous assignment statement
      IB/hns: Switch from dma_device to dev.parent
      ...

commit 551199aca1c3102ebed390566d681cc1290284ca
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:07 2017 -0800

    lib/dma-virt: Add dma_virt_ops
    
    Several RDMA drivers (hfi1, qib and rxe) expect that ib_sge.addr
    is a virtual address. Provide DMA mapping operations that are
    suitable for these drivers.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ab8710888ddf..426c43d4fdbf 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -128,6 +128,7 @@ struct dma_map_ops {
 };
 
 extern const struct dma_map_ops dma_noop_ops;
+extern const struct dma_map_ops dma_virt_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 

commit 815dd18788fe0d41899f51b91d0560279cf16b0d
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:04 2017 -0800

    treewide: Consolidate get_dma_ops() implementations
    
    Introduce a new architecture-specific get_arch_dma_ops() function
    that takes a struct bus_type * argument. Add get_dma_ops() in
    <linux/dma-mapping.h>.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e97f23e8b2d9..ab8710888ddf 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -164,6 +164,13 @@ int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
 
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
+static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
+{
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
+	return get_arch_dma_ops(dev ? dev->bus : NULL);
+}
+
 static inline void set_dma_ops(struct device *dev,
 			       const struct dma_map_ops *dma_ops)
 {

commit ca6e8e1031419549f67291ca31b43126f07cecdb
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:03 2017 -0800

    treewide: Consolidate set_dma_ops() implementations
    
    Now that all set_dma_ops() implementations are identical (ignoring
    BUG_ON() statements), remove the architecture specific definitions
    and add a definition in <linux/dma-mapping.h>.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index f1da68b82c63..e97f23e8b2d9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -164,6 +164,11 @@ int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
 
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
+static inline void set_dma_ops(struct device *dev,
+			       const struct dma_map_ops *dma_ops)
+{
+	dev->dma_ops = dma_ops;
+}
 #else
 /*
  * Define the dma api to allow compilation but not linking of

commit 5299709d0a87342dadc1fc9850484fadeb488bf8
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:01 2017 -0800

    treewide: Constify most dma_map_ops structures
    
    Most dma_map_ops structures are never modified. Constify these
    structures such that these can be write-protected. This patch
    has been generated as follows:
    
    git grep -l 'struct dma_map_ops' |
      xargs -d\\n sed -i \
        -e 's/struct dma_map_ops/const struct dma_map_ops/g' \
        -e 's/const struct dma_map_ops {/struct dma_map_ops {/g' \
        -e 's/^const struct dma_map_ops;$/struct dma_map_ops;/' \
        -e 's/const const struct dma_map_ops /const struct dma_map_ops /g';
    sed -i -e 's/const \(struct dma_map_ops intel_dma_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops intel_dma_ops');
    sed -i -e 's/const \(struct dma_map_ops dma_iommu_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops' | grep ^arch/powerpc);
    sed -i -e '/^struct vmd_dev {$/,/^};$/ s/const \(struct dma_map_ops[[:blank:]]dma_ops;\)/\1/' \
           -e '/^static void vmd_setup_dma_ops/,/^}$/ s/const \(struct dma_map_ops \*dest\)/\1/' \
           -e 's/const \(struct dma_map_ops \*dest = \&vmd->dma_ops\)/\1/' \
        drivers/pci/host/*.c
    sed -i -e '/^void __init pci_iommu_alloc(void)$/,/^}$/ s/dma_ops->/intel_dma_ops./' arch/ia64/kernel/pci-dma.c
    sed -i -e 's/static const struct dma_map_ops sn_dma_ops/static struct dma_map_ops sn_dma_ops/' arch/ia64/sn/pci/pci_dma.c
    sed -i -e 's/(const struct dma_map_ops \*)//' drivers/misc/mic/bus/vop_bus.c
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 10c5a17b1f51..f1da68b82c63 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -127,7 +127,7 @@ struct dma_map_ops {
 	int is_phys;
 };
 
-extern struct dma_map_ops dma_noop_ops;
+extern const struct dma_map_ops dma_noop_ops;
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
@@ -170,8 +170,8 @@ int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
  * dma dependent code.  Code that depends on the dma-mapping
  * API needs to set 'depends on HAS_DMA' in its Kconfig
  */
-extern struct dma_map_ops bad_dma_ops;
-static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+extern const struct dma_map_ops bad_dma_ops;
+static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
 	return &bad_dma_ops;
 }
@@ -182,7 +182,7 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 					      enum dma_data_direction dir,
 					      unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
 	kmemcheck_mark_initialized(ptr, size);
@@ -201,7 +201,7 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 					  enum dma_data_direction dir,
 					  unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->unmap_page)
@@ -217,7 +217,7 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 				   int nents, enum dma_data_direction dir,
 				   unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	int i, ents;
 	struct scatterlist *s;
 
@@ -235,7 +235,7 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
 				      int nents, enum dma_data_direction dir,
 				      unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_unmap_sg(dev, sg, nents, dir);
@@ -249,7 +249,7 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 					    enum dma_data_direction dir,
 					    unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
 	kmemcheck_mark_initialized(page_address(page) + offset, size);
@@ -265,7 +265,7 @@ static inline void dma_unmap_page_attrs(struct device *dev,
 					enum dma_data_direction dir,
 					unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->unmap_page)
@@ -279,7 +279,7 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 					  enum dma_data_direction dir,
 					  unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
@@ -300,7 +300,7 @@ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
 				      size_t size, enum dma_data_direction dir,
 				      unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->unmap_resource)
@@ -312,7 +312,7 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 					   size_t size,
 					   enum dma_data_direction dir)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->sync_single_for_cpu)
@@ -324,7 +324,7 @@ static inline void dma_sync_single_for_device(struct device *dev,
 					      dma_addr_t addr, size_t size,
 					      enum dma_data_direction dir)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->sync_single_for_device)
@@ -364,7 +364,7 @@ static inline void
 dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 		    int nelems, enum dma_data_direction dir)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->sync_sg_for_cpu)
@@ -376,7 +376,7 @@ static inline void
 dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 		       int nelems, enum dma_data_direction dir)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->sync_sg_for_device)
@@ -421,7 +421,7 @@ static inline int
 dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	BUG_ON(!ops);
 	if (ops->mmap)
 		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
@@ -439,7 +439,7 @@ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 		      dma_addr_t dma_addr, size_t size,
 		      unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	BUG_ON(!ops);
 	if (ops->get_sgtable)
 		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
@@ -457,7 +457,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 				       dma_addr_t *dma_handle, gfp_t flag,
 				       unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 	void *cpu_addr;
 
 	BUG_ON(!ops);
@@ -479,7 +479,7 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 				     void *cpu_addr, dma_addr_t dma_handle,
 				     unsigned long attrs)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!ops);
 	WARN_ON(irqs_disabled());
@@ -537,7 +537,7 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 #ifndef HAVE_ARCH_DMA_SUPPORTED
 static inline int dma_supported(struct device *dev, u64 mask)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	if (!ops)
 		return 0;
@@ -550,7 +550,7 @@ static inline int dma_supported(struct device *dev, u64 mask)
 #ifndef HAVE_ARCH_DMA_SET_MASK
 static inline int dma_set_mask(struct device *dev, u64 mask)
 {
-	struct dma_map_ops *ops = get_dma_ops(dev);
+	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	if (ops->set_dma_mask)
 		return ops->set_dma_mask(dev, mask);

commit b2fb366425ceb85dca56afa538257ec5a2c4f6d1
Author: Mitchel Humpherys <mitchelh@codeaurora.org>
Date:   Fri Jan 6 18:58:11 2017 +0530

    common: DMA-mapping: add DMA_ATTR_PRIVILEGED attribute
    
    This patch adds the DMA_ATTR_PRIVILEGED attribute to the DMA-mapping
    subsystem.
    
    Some advanced peripherals such as remote processors and GPUs perform
    accesses to DMA buffers in both privileged "supervisor" and unprivileged
    "user" modes.  This attribute is used to indicate to the DMA-mapping
    subsystem that the buffer is fully accessible at the elevated privilege
    level (and ideally inaccessible or at least read-only at the
    lesser-privileged levels).
    
    Cc: linux-doc@vger.kernel.org
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mitchel Humpherys <mitchelh@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 10c5a17b1f51..c24721a33b4c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -62,6 +62,13 @@
  */
 #define DMA_ATTR_NO_WARN	(1UL << 8)
 
+/*
+ * DMA_ATTR_PRIVILEGED: used to indicate that the buffer is fully
+ * accessible at an elevated privilege level (and ideally inaccessible or
+ * at least read-only at lesser-privileged levels).
+ */
+#define DMA_ATTR_PRIVILEGED		(1UL << 9)
+
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.
  * It can be given to a device to use as a DMA source or target.  A CPU cannot

commit 0495c3d367944e4af053983ff3cdf256b567b053
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Wed Dec 14 15:05:23 2016 -0800

    dma: add calls for dma_map_page_attrs and dma_unmap_page_attrs
    
    Add support for mapping and unmapping a page with attributes.
    
    The primary use for this is currently to allow for us to pass the
    DMA_ATTR_SKIP_CPU_SYNC attribute when mapping and unmapping a page.  On
    some architectures such as ARM the synchronization has significant
    overhead and if we are already taking care of the sync_for_cpu and
    sync_for_device from the driver there isn't much need to handle this in
    the map/unmap calls as well.
    
    Link: http://lkml.kernel.org/r/20161110113601.76501.46095.stgit@ahduyck-blue-test.jf.intel.com
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 08528afdf58b..10c5a17b1f51 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -243,29 +243,33 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
 		ops->unmap_sg(dev, sg, nents, dir, attrs);
 }
 
-static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
-				      size_t offset, size_t size,
-				      enum dma_data_direction dir)
+static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+					    struct page *page,
+					    size_t offset, size_t size,
+					    enum dma_data_direction dir,
+					    unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
 	kmemcheck_mark_initialized(page_address(page) + offset, size);
 	BUG_ON(!valid_dma_direction(dir));
-	addr = ops->map_page(dev, page, offset, size, dir, 0);
+	addr = ops->map_page(dev, page, offset, size, dir, attrs);
 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 
 	return addr;
 }
 
-static inline void dma_unmap_page(struct device *dev, dma_addr_t addr,
-				  size_t size, enum dma_data_direction dir)
+static inline void dma_unmap_page_attrs(struct device *dev,
+					dma_addr_t addr, size_t size,
+					enum dma_data_direction dir,
+					unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->unmap_page)
-		ops->unmap_page(dev, addr, size, dir, 0);
+		ops->unmap_page(dev, addr, size, dir, attrs);
 	debug_dma_unmap_page(dev, addr, size, dir, false);
 }
 
@@ -385,6 +389,8 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
 #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
 #define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
+#define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
+#define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
 
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 			   void *cpu_addr, dma_addr_t dma_addr, size_t size);

commit a9a62c9384417545620aee1b5ad1d9357350c17a
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Tue Oct 11 13:54:14 2016 -0700

    dma-mapping: introduce the DMA_ATTR_NO_WARN attribute
    
    Introduce the DMA_ATTR_NO_WARN attribute, and document it.
    
    Link: http://lkml.kernel.org/r/1470092390-25451-2-git-send-email-mauricfo@linux.vnet.ibm.com
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0f90eb5e3c6b..08528afdf58b 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -56,6 +56,11 @@
  * that gives better TLB efficiency.
  */
 #define DMA_ATTR_ALLOC_SINGLE_PAGES	(1UL << 7)
+/*
+ * DMA_ATTR_NO_WARN: This tells the DMA-mapping subsystem to suppress
+ * allocation failure reports (similarly to __GFP_NOWARN).
+ */
+#define DMA_ATTR_NO_WARN	(1UL << 8)
 
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.

commit 553911c67eb701d46e2dcd073f603c6f6546c38b
Merge: 521f3970853a c84750906b48
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 17:13:54 2016 -0700

    Merge tag 'dmaengine-4.9-rc1' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull dmaengine updates from Vinod Koul:
     "This is bit large pile of code which bring in some nice additions:
    
       - Error reporting: we have added a new mechanism for users of
         dmaenegine to register a callback_result which tells them the
         result of the dma transaction. Right now only one user (ntb) is
         using it.
    
       - As we discussed on KS mailing list and pointed out NO_IRQ has no
         place in kernel, this also remove NO_IRQ from dmaengine subsystem
         (both arm and ppc users)
    
       - Support for IOMMU slave transfers and its implementation for arm.
    
       - To get better build coverage, enable COMPILE_TEST for bunch of
         driver, and fix the warning and sparse complaints on these.
    
       - Apart from above, usual updates spread across drivers"
    
    * tag 'dmaengine-4.9-rc1' of git://git.infradead.org/users/vkoul/slave-dma: (169 commits)
      async_pq_val: fix DMA memory leak
      dmaengine: virt-dma: move function declarations
      dmaengine: omap-dma: Enable burst and data pack for SG
      DT: dmaengine: rcar-dmac: document R8A7743/5 support
      dmaengine: fsldma: Unmap region obtained by of_iomap
      dmaengine: jz4780: fix resource leaks on error exit return
      dma-debug: fix ia64 build, use PHYS_PFN
      dmaengine: coh901318: fix integer overflow when shifting more than 32 places
      dmaengine: edma: avoid uninitialized variable use
      dma-mapping: fix m32r build warning
      dma-mapping: fix ia64 build, use PHYS_PFN
      dmaengine: ti-dma-crossbar: enable COMPILE_TEST
      dmaengine: omap-dma: enable COMPILE_TEST
      dmaengine: edma: enable COMPILE_TEST
      dmaengine: ti-dma-crossbar: Fix of_device_id data parameter usage
      dmaengine: ti-dma-crossbar: Correct type for of_find_property() third parameter
      dmaengine/ARM: omap-dma: Fix the DMAengine compile test on non OMAP configs
      dmaengine: edma: Rename set_bits and remove unused clear_bits helper
      dmaengine: edma: Use correct type for of_find_property() third parameter
      dmaengine: edma: Fix of_device_id data parameter usage (legacy vs TPCC)
      ...

commit 3757dc48a66f829cf6ba82a612ba4587ab4b5f1c
Author: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
Date:   Thu Sep 29 12:02:40 2016 +0200

    dma-mapping: fix m32r build warning
    
    kbuild test robot reports:
    
       In file included from include/linux/skbuff.h:34:0,
                        from include/linux/icmpv6.h:4,
                        from include/linux/ipv6.h:75,
                        from include/net/ipv6.h:16,
                        from include/linux/sunrpc/clnt.h:27,
                        from include/linux/nfs_fs.h:30,
                        from fs/lockd/clntlock.c:13:
       include/linux/dma-mapping.h: In function 'dma_map_resource':
    >> include/linux/dma-mapping.h:274:16: warning: unused variable 'pfn' [-Wunused-variable]
         unsigned long pfn = __phys_to_pfn(phys_addr);
                       ^~~
    
    The pfn value is only used once in the call to pfn_valid(), remove the
    variable and calculate the pfn when it's needed. Note that the kbuild
    report is old and PHYS_PFN() is now used instead of __phys_to_pfn() to
    calculate the pfn.
    
    Signed-off-by: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ff7c87fb0305..642cb4c7ad37 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -271,13 +271,12 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 					  unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
-	unsigned long pfn = PHYS_PFN(phys_addr);
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
 
 	/* Don't allow RAM to be mapped */
-	BUG_ON(pfn_valid(pfn));
+	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
 
 	addr = phys_addr;
 	if (ops->map_resource)

commit 2895e1f8048d1be7b1b5be6439c740621c0e5361
Author: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
Date:   Thu Sep 29 12:02:39 2016 +0200

    dma-mapping: fix ia64 build, use PHYS_PFN
    
    kbuild test robot reports:
    
       In file included from include/linux/skbuff.h:34:0,
                        from include/linux/tcp.h:21,
                        from drivers/net/ethernet/amd/xgbe/xgbe-drv.c:119:
       include/linux/dma-mapping.h: In function 'dma_map_resource':
    >> include/linux/dma-mapping.h:274:22: error: implicit declaration of function '__phys_to_pfn' [-Werror=implicit-function-declaration]
         unsigned long pfn = __phys_to_pfn(phys_addr);
                             ^~~~~~~~~~~~~
    
    ia64 does not provide __phys_to_pfn(), use the PHYS_PFN() alias.
    
    Signed-off-by: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 6e00c7fdbbd3..ff7c87fb0305 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -271,7 +271,7 @@ static inline dma_addr_t dma_map_resource(struct device *dev,
 					  unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
-	unsigned long pfn = __phys_to_pfn(phys_addr);
+	unsigned long pfn = PHYS_PFN(phys_addr);
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));

commit 2481366afd71a0c0b7cd725e6750c04cf589673b
Author: Andrey Smirnov <andrew.smirnov@gmail.com>
Date:   Wed Sep 28 15:22:33 2016 -0700

    dma-mapping.h: preserve unmap info for CONFIG_DMA_API_DEBUG
    
    When CONFIG_DMA_API_DEBUG is enabled we need to preserve unmapping address
    even if "unmap" is a no-op for our architecutre because we need
    debug_dma_unmap_page() to correctly cleanup all of the debug bookkeeping.
    Failing to do so results in a false positive warnings about previously
    mapped areas never being unmapped.
    
    Link: http://lkml.kernel.org/r/1474387125-3713-1-git-send-email-andrew.smirnov@gmail.com
    Signed-off-by: Andrey Smirnov <andrew.smirnov@gmail.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Zhen Lei <thunder.leizhen@huawei.com>
    Cc: "Luis R. Rodriguez" <mcgrof@suse.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Geliang Tang <geliangtang@163.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 66533e18276c..dc69df04abc1 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -718,7 +718,7 @@ static inline int dma_mmap_wc(struct device *dev,
 #define dma_mmap_writecombine dma_mmap_wc
 #endif
 
-#ifdef CONFIG_NEED_DMA_MAP_STATE
+#if defined(CONFIG_NEED_DMA_MAP_STATE) || defined(CONFIG_DMA_API_DEBUG)
 #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME
 #define DEFINE_DMA_UNMAP_LEN(LEN_NAME)          __u32 LEN_NAME
 #define dma_unmap_addr(PTR, ADDR_NAME)           ((PTR)->ADDR_NAME)

commit 6f3d87968f9c8b529bc81eff5a1f45e92553493d
Author: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
Date:   Wed Aug 10 13:22:16 2016 +0200

    dma-mapping: add dma_{map,unmap}_resource
    
    Map/Unmap a device MMIO resource from a physical address. If no dma_map_ops
    method is available the operation is a no-op.
    
    Signed-off-by: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 673b7be85f7a..6e00c7fdbbd3 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -264,6 +264,42 @@ static inline void dma_unmap_page(struct device *dev, dma_addr_t addr,
 	debug_dma_unmap_page(dev, addr, size, dir, false);
 }
 
+static inline dma_addr_t dma_map_resource(struct device *dev,
+					  phys_addr_t phys_addr,
+					  size_t size,
+					  enum dma_data_direction dir,
+					  unsigned long attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	unsigned long pfn = __phys_to_pfn(phys_addr);
+	dma_addr_t addr;
+
+	BUG_ON(!valid_dma_direction(dir));
+
+	/* Don't allow RAM to be mapped */
+	BUG_ON(pfn_valid(pfn));
+
+	addr = phys_addr;
+	if (ops->map_resource)
+		addr = ops->map_resource(dev, phys_addr, size, dir, attrs);
+
+	debug_dma_map_resource(dev, phys_addr, size, dir, addr);
+
+	return addr;
+}
+
+static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
+				      size_t size, enum dma_data_direction dir,
+				      unsigned long attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->unmap_resource)
+		ops->unmap_resource(dev, addr, size, dir, attrs);
+	debug_dma_unmap_resource(dev, addr, size, dir);
+}
+
 static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 					   size_t size,
 					   enum dma_data_direction dir)

commit ba409b31b3d37b52dda4eefcde04f5837c7ee4aa
Author: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
Date:   Wed Aug 10 13:22:14 2016 +0200

    dma-mapping: add {map,unmap}_resource to dma_map_ops
    
    Add methods to handle mapping of device resources from a physical
    address. This is needed for example to be able to map MMIO FIFO
    registers to a IOMMU.
    
    Signed-off-by: Niklas Sderlund <niklas.soderlund+renesas@ragnatech.se>
    Reviewed-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 66533e18276c..673b7be85f7a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -95,6 +95,12 @@ struct dma_map_ops {
 			 struct scatterlist *sg, int nents,
 			 enum dma_data_direction dir,
 			 unsigned long attrs);
+	dma_addr_t (*map_resource)(struct device *dev, phys_addr_t phys_addr,
+			       size_t size, enum dma_data_direction dir,
+			       unsigned long attrs);
+	void (*unmap_resource)(struct device *dev, dma_addr_t dma_handle,
+			   size_t size, enum dma_data_direction dir,
+			   unsigned long attrs);
 	void (*sync_single_for_cpu)(struct device *dev,
 				    dma_addr_t dma_handle, size_t size,
 				    enum dma_data_direction dir);

commit 00085f1efa387a8ce100e3734920f7639c80caa3
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Wed Aug 3 13:46:00 2016 -0700

    dma-mapping: use unsigned long for dma_attrs
    
    The dma-mapping core and the implementations do not change the DMA
    attributes passed by pointer.  Thus the pointer can point to const data.
    However the attributes do not have to be a bitfield.  Instead unsigned
    long will do fine:
    
    1. This is just simpler.  Both in terms of reading the code and setting
       attributes.  Instead of initializing local attributes on the stack
       and passing pointer to it to dma_set_attr(), just set the bits.
    
    2. It brings safeness and checking for const correctness because the
       attributes are passed by value.
    
    Semantic patches for this change (at least most of them):
    
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
    
        @@
        f(...,
        - struct dma_attrs *attrs
        + unsigned long attrs
        , ...)
        {
        ...
        }
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    and
    
        // Options: --all-includes
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
        type t;
    
        @@
        t f(..., struct dma_attrs *attrs);
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    Link: http://lkml.kernel.org/r/1468399300-5399-2-git-send-email-k.kozlowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no>
    Acked-by: Mark Salter <msalter@redhat.com> [c6x]
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com> [cris]
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch> [drm]
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Fabien Dessenne <fabien.dessenne@st.com> [bdisp]
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com> [vb2-core]
    Acked-by: David Vrabel <david.vrabel@citrix.com> [xen]
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen swiotlb]
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Richard Kuo <rkuo@codeaurora.org> [hexagon]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> [s390]
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no> [avr32]
    Acked-by: Vineet Gupta <vgupta@synopsys.com> [arc]
    Acked-by: Robin Murphy <robin.murphy@arm.com> [arm64 and dma-iommu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 71c1b215ef66..66533e18276c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -5,13 +5,58 @@
 #include <linux/string.h>
 #include <linux/device.h>
 #include <linux/err.h>
-#include <linux/dma-attrs.h>
 #include <linux/dma-debug.h>
 #include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
 #include <linux/kmemcheck.h>
 #include <linux/bug.h>
 
+/**
+ * List of possible attributes associated with a DMA mapping. The semantics
+ * of each attribute should be defined in Documentation/DMA-attributes.txt.
+ *
+ * DMA_ATTR_WRITE_BARRIER: DMA to a memory region with this attribute
+ * forces all pending DMA writes to complete.
+ */
+#define DMA_ATTR_WRITE_BARRIER		(1UL << 0)
+/*
+ * DMA_ATTR_WEAK_ORDERING: Specifies that reads and writes to the mapping
+ * may be weakly ordered, that is that reads and writes may pass each other.
+ */
+#define DMA_ATTR_WEAK_ORDERING		(1UL << 1)
+/*
+ * DMA_ATTR_WRITE_COMBINE: Specifies that writes to the mapping may be
+ * buffered to improve performance.
+ */
+#define DMA_ATTR_WRITE_COMBINE		(1UL << 2)
+/*
+ * DMA_ATTR_NON_CONSISTENT: Lets the platform to choose to return either
+ * consistent or non-consistent memory as it sees fit.
+ */
+#define DMA_ATTR_NON_CONSISTENT		(1UL << 3)
+/*
+ * DMA_ATTR_NO_KERNEL_MAPPING: Lets the platform to avoid creating a kernel
+ * virtual mapping for the allocated buffer.
+ */
+#define DMA_ATTR_NO_KERNEL_MAPPING	(1UL << 4)
+/*
+ * DMA_ATTR_SKIP_CPU_SYNC: Allows platform code to skip synchronization of
+ * the CPU cache for the given buffer assuming that it has been already
+ * transferred to 'device' domain.
+ */
+#define DMA_ATTR_SKIP_CPU_SYNC		(1UL << 5)
+/*
+ * DMA_ATTR_FORCE_CONTIGUOUS: Forces contiguous allocation of the buffer
+ * in physical memory.
+ */
+#define DMA_ATTR_FORCE_CONTIGUOUS	(1UL << 6)
+/*
+ * DMA_ATTR_ALLOC_SINGLE_PAGES: This is a hint to the DMA-mapping subsystem
+ * that it's probably not worth the time to try to allocate memory to in a way
+ * that gives better TLB efficiency.
+ */
+#define DMA_ATTR_ALLOC_SINGLE_PAGES	(1UL << 7)
+
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.
  * It can be given to a device to use as a DMA source or target.  A CPU cannot
@@ -21,34 +66,35 @@
 struct dma_map_ops {
 	void* (*alloc)(struct device *dev, size_t size,
 				dma_addr_t *dma_handle, gfp_t gfp,
-				struct dma_attrs *attrs);
+				unsigned long attrs);
 	void (*free)(struct device *dev, size_t size,
 			      void *vaddr, dma_addr_t dma_handle,
-			      struct dma_attrs *attrs);
+			      unsigned long attrs);
 	int (*mmap)(struct device *, struct vm_area_struct *,
-			  void *, dma_addr_t, size_t, struct dma_attrs *attrs);
+			  void *, dma_addr_t, size_t,
+			  unsigned long attrs);
 
 	int (*get_sgtable)(struct device *dev, struct sg_table *sgt, void *,
-			   dma_addr_t, size_t, struct dma_attrs *attrs);
+			   dma_addr_t, size_t, unsigned long attrs);
 
 	dma_addr_t (*map_page)(struct device *dev, struct page *page,
 			       unsigned long offset, size_t size,
 			       enum dma_data_direction dir,
-			       struct dma_attrs *attrs);
+			       unsigned long attrs);
 	void (*unmap_page)(struct device *dev, dma_addr_t dma_handle,
 			   size_t size, enum dma_data_direction dir,
-			   struct dma_attrs *attrs);
+			   unsigned long attrs);
 	/*
 	 * map_sg returns 0 on error and a value > 0 on success.
 	 * It should never return a value < 0.
 	 */
 	int (*map_sg)(struct device *dev, struct scatterlist *sg,
 		      int nents, enum dma_data_direction dir,
-		      struct dma_attrs *attrs);
+		      unsigned long attrs);
 	void (*unmap_sg)(struct device *dev,
 			 struct scatterlist *sg, int nents,
 			 enum dma_data_direction dir,
-			 struct dma_attrs *attrs);
+			 unsigned long attrs);
 	void (*sync_single_for_cpu)(struct device *dev,
 				    dma_addr_t dma_handle, size_t size,
 				    enum dma_data_direction dir);
@@ -123,7 +169,7 @@ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 					      size_t size,
 					      enum dma_data_direction dir,
-					      struct dma_attrs *attrs)
+					      unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
@@ -142,7 +188,7 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 					  size_t size,
 					  enum dma_data_direction dir,
-					  struct dma_attrs *attrs)
+					  unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 
@@ -158,7 +204,7 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
  */
 static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 				   int nents, enum dma_data_direction dir,
-				   struct dma_attrs *attrs)
+				   unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	int i, ents;
@@ -176,7 +222,7 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 
 static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
 				      int nents, enum dma_data_direction dir,
-				      struct dma_attrs *attrs)
+				      unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 
@@ -195,7 +241,7 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 
 	kmemcheck_mark_initialized(page_address(page) + offset, size);
 	BUG_ON(!valid_dma_direction(dir));
-	addr = ops->map_page(dev, page, offset, size, dir, NULL);
+	addr = ops->map_page(dev, page, offset, size, dir, 0);
 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 
 	return addr;
@@ -208,7 +254,7 @@ static inline void dma_unmap_page(struct device *dev, dma_addr_t addr,
 
 	BUG_ON(!valid_dma_direction(dir));
 	if (ops->unmap_page)
-		ops->unmap_page(dev, addr, size, dir, NULL);
+		ops->unmap_page(dev, addr, size, dir, 0);
 	debug_dma_unmap_page(dev, addr, size, dir, false);
 }
 
@@ -289,10 +335,10 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 
 }
 
-#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, NULL)
-#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, NULL)
-#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, NULL)
-#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, NULL)
+#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
+#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
+#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
+#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
 
 extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
@@ -321,7 +367,7 @@ void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
  */
 static inline int
 dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
-	       dma_addr_t dma_addr, size_t size, struct dma_attrs *attrs)
+	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	BUG_ON(!ops);
@@ -330,7 +376,7 @@ dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
 	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
 }
 
-#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, NULL)
+#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 
 int
 dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
@@ -338,7 +384,8 @@ dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 
 static inline int
 dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
-		      dma_addr_t dma_addr, size_t size, struct dma_attrs *attrs)
+		      dma_addr_t dma_addr, size_t size,
+		      unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	BUG_ON(!ops);
@@ -348,7 +395,7 @@ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size);
 }
 
-#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, NULL)
+#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 
 #ifndef arch_dma_alloc_attrs
 #define arch_dma_alloc_attrs(dev, flag)	(true)
@@ -356,7 +403,7 @@ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 
 static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 				       dma_addr_t *dma_handle, gfp_t flag,
-				       struct dma_attrs *attrs)
+				       unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 	void *cpu_addr;
@@ -378,7 +425,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 
 static inline void dma_free_attrs(struct device *dev, size_t size,
 				     void *cpu_addr, dma_addr_t dma_handle,
-				     struct dma_attrs *attrs)
+				     unsigned long attrs)
 {
 	struct dma_map_ops *ops = get_dma_ops(dev);
 
@@ -398,31 +445,27 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 static inline void *dma_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t flag)
 {
-	return dma_alloc_attrs(dev, size, dma_handle, flag, NULL);
+	return dma_alloc_attrs(dev, size, dma_handle, flag, 0);
 }
 
 static inline void dma_free_coherent(struct device *dev, size_t size,
 		void *cpu_addr, dma_addr_t dma_handle)
 {
-	return dma_free_attrs(dev, size, cpu_addr, dma_handle, NULL);
+	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
 }
 
 static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
 {
-	DEFINE_DMA_ATTRS(attrs);
-
-	dma_set_attr(DMA_ATTR_NON_CONSISTENT, &attrs);
-	return dma_alloc_attrs(dev, size, dma_handle, gfp, &attrs);
+	return dma_alloc_attrs(dev, size, dma_handle, gfp,
+			       DMA_ATTR_NON_CONSISTENT);
 }
 
 static inline void dma_free_noncoherent(struct device *dev, size_t size,
 		void *cpu_addr, dma_addr_t dma_handle)
 {
-	DEFINE_DMA_ATTRS(attrs);
-
-	dma_set_attr(DMA_ATTR_NON_CONSISTENT, &attrs);
-	dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
+	dma_free_attrs(dev, size, cpu_addr, dma_handle,
+		       DMA_ATTR_NON_CONSISTENT);
 }
 
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
@@ -646,9 +689,8 @@ static inline void dmam_release_declared_memory(struct device *dev)
 static inline void *dma_alloc_wc(struct device *dev, size_t size,
 				 dma_addr_t *dma_addr, gfp_t gfp)
 {
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_alloc_attrs(dev, size, dma_addr, gfp, &attrs);
+	return dma_alloc_attrs(dev, size, dma_addr, gfp,
+			       DMA_ATTR_WRITE_COMBINE);
 }
 #ifndef dma_alloc_writecombine
 #define dma_alloc_writecombine dma_alloc_wc
@@ -657,9 +699,8 @@ static inline void *dma_alloc_wc(struct device *dev, size_t size,
 static inline void dma_free_wc(struct device *dev, size_t size,
 			       void *cpu_addr, dma_addr_t dma_addr)
 {
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_free_attrs(dev, size, cpu_addr, dma_addr, &attrs);
+	return dma_free_attrs(dev, size, cpu_addr, dma_addr,
+			      DMA_ATTR_WRITE_COMBINE);
 }
 #ifndef dma_free_writecombine
 #define dma_free_writecombine dma_free_wc
@@ -670,9 +711,8 @@ static inline int dma_mmap_wc(struct device *dev,
 			      void *cpu_addr, dma_addr_t dma_addr,
 			      size_t size)
 {
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
+	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size,
+			      DMA_ATTR_WRITE_COMBINE);
 }
 #ifndef dma_mmap_writecombine
 #define dma_mmap_writecombine dma_mmap_wc

commit 53c92d793395fdab9edbd2f79b084bb6b2e6ae79
Author: Robin Murphy <Robin.Murphy@arm.com>
Date:   Thu Apr 7 18:42:05 2016 +0100

    iommu: of: enforce const-ness of struct iommu_ops
    
    As a set of driver-provided callbacks and static data, there is no
    compelling reason for struct iommu_ops to be mutable in core code, so
    enforce const-ness throughout.
    
    Acked-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 9ea9aba28049..71c1b215ef66 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -514,7 +514,7 @@ extern u64 dma_get_required_mask(struct device *dev);
 
 #ifndef arch_setup_dma_ops
 static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
-				      u64 size, struct iommu_ops *iommu,
+				      u64 size, const struct iommu_ops *iommu,
 				      bool coherent) { }
 #endif
 

commit f0691533b756931089902464ca15afc218a49d70
Merge: 2b2f72d8ce59 c67f5db82027
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 20 13:28:18 2016 -0700

    Merge tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost
    
    Pull virtio/vhost updates from Michael Tsirkin:
     "New features, performance improvements, cleanups:
    
       - basic polling support for vhost
       - rework virtio to optionally use DMA API, fixing it on Xen
       - balloon stats gained a new entry
       - using the new napi_alloc_skb speeds up virtio net
       - virtio blk stats can now be read while another VCPU is busy
         inflating or deflating the balloon
    
      plus misc cleanups in various places"
    
    * tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost:
      virtio_net: replace netdev_alloc_skb_ip_align() with napi_alloc_skb()
      vhost_net: basic polling support
      vhost: introduce vhost_vq_avail_empty()
      vhost: introduce vhost_has_work()
      virtio_balloon: Allow to resize and update the balloon stats in parallel
      virtio_balloon: Use a workqueue instead of "vballoon" kthread
      virtio/s390: size of SET_IND payload
      virtio/s390: use dev_to_virtio
      vhost: rename vhost_init_used()
      vhost: rename cross-endian helpers
      virtio_blk: VIRTIO_BLK_F_WCE->VIRTIO_BLK_F_FLUSH
      vring: Use the DMA API on Xen
      virtio_pci: Use the DMA API if enabled
      virtio_mmio: Use the DMA API if enabled
      virtio: Add improved queue allocation API
      virtio_ring: Support DMA APIs
      vring: Introduce vring_use_dma_api()
      s390/dma: Allow per device dma ops
      alpha/dma: use common noop dma ops
      dma: Provide simple noop dma ops

commit 5ec942463bfbc6c98ea4a96689b498e88a031c4a
Merge: fbed0bc0915e f6e45661f9be
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 16:31:41 2016 -0700

    Merge branch 'mm-pat-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull dma_*_writecombine rename from Ingo Molnar:
     "Rename dma_*_writecombine() to dma_*_wc()
    
      This is a tree-wide API rename, to move the dma_*() write-combining
      APIs closer in name to their usual API families.  (The old API names
      are kept as compatibility wrappers to not introduce extra breakage.)
    
      The patch was Coccinelle generated"
    
    * 'mm-pat-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      dma, mm/pat: Rename dma_*_writecombine() to dma_*_wc()

commit d6b7eaeb03421139e32800324ef04ab50bba886d
Author: Zhen Lei <thunder.leizhen@huawei.com>
Date:   Wed Mar 9 14:08:38 2016 -0800

    dma-mapping: avoid oops when parameter cpu_addr is null
    
    To keep consistent with kfree, which tolerate ptr is NULL.  We do this
    because sometimes we may use goto statement, so that success and failure
    case can share parts of the code.  But unfortunately, dma_free_coherent
    called with parameter cpu_addr is null will cause oops, such as showed
    below:
    
      Unable to handle kernel paging request at virtual address ffffffc020d3b2b8
      pgd = ffffffc083a61000
      [ffffffc020d3b2b8] *pgd=0000000000000000, *pud=0000000000000000
      CPU: 4 PID: 1489 Comm: malloc_dma_1 Tainted: G           O    4.1.12 #1
      Hardware name: ARM64 (DT)
      PC is at __dma_free_coherent.isra.10+0x74/0xc8
      LR is at __dma_free+0x9c/0xb0
      Process malloc_dma_1 (pid: 1489, stack limit = 0xffffffc0837fc020)
      [...]
      Call trace:
        __dma_free_coherent.isra.10+0x74/0xc8
        __dma_free+0x9c/0xb0
        malloc_dma+0x104/0x158 [dma_alloc_coherent_mtmalloc]
        kthread+0xec/0xfc
    
    Signed-off-by: Zhen Lei <thunder.leizhen@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 75857cda38e9..728ef074602a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -386,7 +386,7 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
 		return;
 
-	if (!ops->free)
+	if (!ops->free || !cpu_addr)
 		return;
 
 	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);

commit f6e45661f9be546811b62b2b01f32f4bf0c436c0
Author: Luis R. Rodriguez <mcgrof@suse.com>
Date:   Fri Jan 22 18:34:22 2016 -0800

    dma, mm/pat: Rename dma_*_writecombine() to dma_*_wc()
    
    Rename dma_*_writecombine() to dma_*_wc(), so that the naming
    is coherent across the various write-combining APIs. Keep the
    old names for compatibility for a while, these can be removed
    at a later time. A guard is left to enable backporting of the
    rename, and later remove of the old mapping defines seemlessly.
    
    Build tested successfully with allmodconfig.
    
    The following Coccinelle SmPL patch was used for this simple
    transformation:
    
    @ rename_dma_alloc_writecombine @
    expression dev, size, dma_addr, gfp;
    @@
    
    -dma_alloc_writecombine(dev, size, dma_addr, gfp)
    +dma_alloc_wc(dev, size, dma_addr, gfp)
    
    @ rename_dma_free_writecombine @
    expression dev, size, cpu_addr, dma_addr;
    @@
    
    -dma_free_writecombine(dev, size, cpu_addr, dma_addr)
    +dma_free_wc(dev, size, cpu_addr, dma_addr)
    
    @ rename_dma_mmap_writecombine @
    expression dev, vma, cpu_addr, dma_addr, size;
    @@
    
    -dma_mmap_writecombine(dev, vma, cpu_addr, dma_addr, size)
    +dma_mmap_wc(dev, vma, cpu_addr, dma_addr, size)
    
    We also keep the old names as compatibility helpers, and
    guard against their definition to make backporting easier.
    
    Generated-by: Coccinelle SmPL
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bhelgaas@google.com
    Cc: bp@suse.de
    Cc: dan.j.williams@intel.com
    Cc: daniel.vetter@ffwll.ch
    Cc: dhowells@redhat.com
    Cc: julia.lawall@lip6.fr
    Cc: konrad.wilk@oracle.com
    Cc: linux-fbdev@vger.kernel.org
    Cc: linux-pci@vger.kernel.org
    Cc: luto@amacapital.net
    Cc: mst@redhat.com
    Cc: tomi.valkeinen@ti.com
    Cc: toshi.kani@hp.com
    Cc: vinod.koul@intel.com
    Cc: xen-devel@lists.xensource.com
    Link: http://lkml.kernel.org/r/1453516462-4844-1-git-send-email-mcgrof@do-not-panic.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 75857cda38e9..471e064af29f 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -641,31 +641,40 @@ static inline void dmam_release_declared_memory(struct device *dev)
 }
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
-static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
-					   dma_addr_t *dma_addr, gfp_t gfp)
+static inline void *dma_alloc_wc(struct device *dev, size_t size,
+				 dma_addr_t *dma_addr, gfp_t gfp)
 {
 	DEFINE_DMA_ATTRS(attrs);
 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
 	return dma_alloc_attrs(dev, size, dma_addr, gfp, &attrs);
 }
+#ifndef dma_alloc_writecombine
+#define dma_alloc_writecombine dma_alloc_wc
+#endif
 
-static inline void dma_free_writecombine(struct device *dev, size_t size,
-					 void *cpu_addr, dma_addr_t dma_addr)
+static inline void dma_free_wc(struct device *dev, size_t size,
+			       void *cpu_addr, dma_addr_t dma_addr)
 {
 	DEFINE_DMA_ATTRS(attrs);
 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
 	return dma_free_attrs(dev, size, cpu_addr, dma_addr, &attrs);
 }
+#ifndef dma_free_writecombine
+#define dma_free_writecombine dma_free_wc
+#endif
 
-static inline int dma_mmap_writecombine(struct device *dev,
-					struct vm_area_struct *vma,
-					void *cpu_addr, dma_addr_t dma_addr,
-					size_t size)
+static inline int dma_mmap_wc(struct device *dev,
+			      struct vm_area_struct *vma,
+			      void *cpu_addr, dma_addr_t dma_addr,
+			      size_t size)
 {
 	DEFINE_DMA_ATTRS(attrs);
 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
 	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
 }
+#ifndef dma_mmap_writecombine
+#define dma_mmap_writecombine dma_mmap_wc
+#endif
 
 #ifdef CONFIG_NEED_DMA_MAP_STATE
 #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME

commit a8463d4b0e47d1f37af684d97884ffcf35de043b
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Feb 2 21:46:32 2016 -0800

    dma: Provide simple noop dma ops
    
    We are going to require dma_ops for several common drivers, even for
    systems that do have an identity mapping. Lets provide some minimal
    no-op dma_ops that can be used for that purpose.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 75857cda38e9..c0b27ff2c784 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -70,6 +70,8 @@ struct dma_map_ops {
 	int is_phys;
 };
 
+extern struct dma_map_ops dma_noop_ops;
+
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
 #define DMA_MASK_NONE	0x0ULL

commit 8e99469ab0f821bea77625cd4775ca529d4ca7d4
Author: Geliang Tang <geliangtang@163.com>
Date:   Wed Jan 20 15:02:12 2016 -0800

    dma-mapping: use offset_in_page macro
    
    Use offset_in_page macro instead of (addr & ~PAGE_MASK).
    
    Signed-off-by: Geliang Tang <geliangtang@163.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d6b575bb45a7..75857cda38e9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -129,10 +129,10 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 	kmemcheck_mark_initialized(ptr, size);
 	BUG_ON(!valid_dma_direction(dir));
 	addr = ops->map_page(dev, virt_to_page(ptr),
-			     (unsigned long)ptr & ~PAGE_MASK, size,
+			     offset_in_page(ptr), size,
 			     dir, attrs);
 	debug_dma_map_page(dev, virt_to_page(ptr),
-			   (unsigned long)ptr & ~PAGE_MASK, size,
+			   offset_in_page(ptr), size,
 			   dir, addr, true);
 	return addr;
 }

commit 20d666e41166f8023ff3d960e832d87ded18c5c4
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 20 15:02:09 2016 -0800

    dma-mapping: remove <asm-generic/dma-coherent.h>
    
    This wasn't an asm-generic header to start with, and can be merged into
    dma-mapping.h trivially.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index cc0517b71c5e..d6b575bb45a7 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -11,7 +11,6 @@
 #include <linux/scatterlist.h>
 #include <linux/kmemcheck.h>
 #include <linux/bug.h>
-#include <asm-generic/dma-coherent.h>
 
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.
@@ -87,6 +86,23 @@ static inline int is_device_dma_capable(struct device *dev)
 	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
 }
 
+#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
+/*
+ * These three functions are only for dma allocator.
+ * Don't use them in device drivers.
+ */
+int dma_alloc_from_coherent(struct device *dev, ssize_t size,
+				       dma_addr_t *dma_handle, void **ret);
+int dma_release_from_coherent(struct device *dev, int order, void *vaddr);
+
+int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
+			    void *cpu_addr, size_t size, int *ret);
+#else
+#define dma_alloc_from_coherent(dev, size, handle, ret) (0)
+#define dma_release_from_coherent(dev, order, vaddr) (0)
+#define dma_mmap_from_coherent(dev, vma, vaddr, order, ret) (0)
+#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
+
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 #else
@@ -568,7 +584,13 @@ static inline int dma_get_cache_alignment(void)
 #define DMA_MEMORY_INCLUDES_CHILDREN	0x04
 #define DMA_MEMORY_EXCLUSIVE		0x08
 
-#ifndef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
+#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
+int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
+				dma_addr_t device_addr, size_t size, int flags);
+void dma_release_declared_memory(struct device *dev);
+void *dma_mark_declared_memory_occupied(struct device *dev,
+					dma_addr_t device_addr, size_t size);
+#else
 static inline int
 dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 			    dma_addr_t device_addr, size_t size, int flags)
@@ -587,7 +609,7 @@ dma_mark_declared_memory_occupied(struct device *dev,
 {
 	return ERR_PTR(-EBUSY);
 }
-#endif
+#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
 /*
  * Managed DMA API
@@ -600,13 +622,13 @@ extern void *dmam_alloc_noncoherent(struct device *dev, size_t size,
 				    dma_addr_t *dma_handle, gfp_t gfp);
 extern void dmam_free_noncoherent(struct device *dev, size_t size, void *vaddr,
 				  dma_addr_t dma_handle);
-#ifdef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
+#ifdef CONFIG_HAVE_GENERIC_DMA_COHERENT
 extern int dmam_declare_coherent_memory(struct device *dev,
 					phys_addr_t phys_addr,
 					dma_addr_t device_addr, size_t size,
 					int flags);
 extern void dmam_release_declared_memory(struct device *dev);
-#else /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
+#else /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 static inline int dmam_declare_coherent_memory(struct device *dev,
 				phys_addr_t phys_addr, dma_addr_t device_addr,
 				size_t size, gfp_t gfp)
@@ -617,7 +639,7 @@ static inline int dmam_declare_coherent_memory(struct device *dev,
 static inline void dmam_release_declared_memory(struct device *dev)
 {
 }
-#endif /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
+#endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
 static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
 					   dma_addr_t *dma_addr, gfp_t gfp)

commit e1c7e324539ada3b2b13ca2898bcb4948a9ef9db
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 20 15:02:05 2016 -0800

    dma-mapping: always provide the dma_map_ops based implementation
    
    Move the generic implementation to <linux/dma-mapping.h> now that all
    architectures support it and remove the HAVE_DMA_ATTR Kconfig symbol now
    that everyone supports them.
    
    [valentinrothberg@gmail.com: remove leftovers in Kconfig]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Valentin Rothberg <valentinrothberg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2e551e2d2d03..cc0517b71c5e 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -6,8 +6,12 @@
 #include <linux/device.h>
 #include <linux/err.h>
 #include <linux/dma-attrs.h>
+#include <linux/dma-debug.h>
 #include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
+#include <linux/kmemcheck.h>
+#include <linux/bug.h>
+#include <asm-generic/dma-coherent.h>
 
 /*
  * A dma_addr_t can hold any valid DMA or bus address for the platform.
@@ -86,7 +90,363 @@ static inline int is_device_dma_capable(struct device *dev)
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 #else
-#include <asm-generic/dma-mapping-broken.h>
+/*
+ * Define the dma api to allow compilation but not linking of
+ * dma dependent code.  Code that depends on the dma-mapping
+ * API needs to set 'depends on HAS_DMA' in its Kconfig
+ */
+extern struct dma_map_ops bad_dma_ops;
+static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+{
+	return &bad_dma_ops;
+}
+#endif
+
+static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
+					      size_t size,
+					      enum dma_data_direction dir,
+					      struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	dma_addr_t addr;
+
+	kmemcheck_mark_initialized(ptr, size);
+	BUG_ON(!valid_dma_direction(dir));
+	addr = ops->map_page(dev, virt_to_page(ptr),
+			     (unsigned long)ptr & ~PAGE_MASK, size,
+			     dir, attrs);
+	debug_dma_map_page(dev, virt_to_page(ptr),
+			   (unsigned long)ptr & ~PAGE_MASK, size,
+			   dir, addr, true);
+	return addr;
+}
+
+static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
+					  size_t size,
+					  enum dma_data_direction dir,
+					  struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->unmap_page)
+		ops->unmap_page(dev, addr, size, dir, attrs);
+	debug_dma_unmap_page(dev, addr, size, dir, true);
+}
+
+/*
+ * dma_maps_sg_attrs returns 0 on error and > 0 on success.
+ * It should never return a value < 0.
+ */
+static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
+				   int nents, enum dma_data_direction dir,
+				   struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	int i, ents;
+	struct scatterlist *s;
+
+	for_each_sg(sg, s, nents, i)
+		kmemcheck_mark_initialized(sg_virt(s), s->length);
+	BUG_ON(!valid_dma_direction(dir));
+	ents = ops->map_sg(dev, sg, nents, dir, attrs);
+	BUG_ON(ents < 0);
+	debug_dma_map_sg(dev, sg, nents, ents, dir);
+
+	return ents;
+}
+
+static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
+				      int nents, enum dma_data_direction dir,
+				      struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	debug_dma_unmap_sg(dev, sg, nents, dir);
+	if (ops->unmap_sg)
+		ops->unmap_sg(dev, sg, nents, dir, attrs);
+}
+
+static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
+				      size_t offset, size_t size,
+				      enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	dma_addr_t addr;
+
+	kmemcheck_mark_initialized(page_address(page) + offset, size);
+	BUG_ON(!valid_dma_direction(dir));
+	addr = ops->map_page(dev, page, offset, size, dir, NULL);
+	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
+
+	return addr;
+}
+
+static inline void dma_unmap_page(struct device *dev, dma_addr_t addr,
+				  size_t size, enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->unmap_page)
+		ops->unmap_page(dev, addr, size, dir, NULL);
+	debug_dma_unmap_page(dev, addr, size, dir, false);
+}
+
+static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+					   size_t size,
+					   enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_single_for_cpu)
+		ops->sync_single_for_cpu(dev, addr, size, dir);
+	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
+}
+
+static inline void dma_sync_single_for_device(struct device *dev,
+					      dma_addr_t addr, size_t size,
+					      enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_single_for_device)
+		ops->sync_single_for_device(dev, addr, size, dir);
+	debug_dma_sync_single_for_device(dev, addr, size, dir);
+}
+
+static inline void dma_sync_single_range_for_cpu(struct device *dev,
+						 dma_addr_t addr,
+						 unsigned long offset,
+						 size_t size,
+						 enum dma_data_direction dir)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_single_for_cpu)
+		ops->sync_single_for_cpu(dev, addr + offset, size, dir);
+	debug_dma_sync_single_range_for_cpu(dev, addr, offset, size, dir);
+}
+
+static inline void dma_sync_single_range_for_device(struct device *dev,
+						    dma_addr_t addr,
+						    unsigned long offset,
+						    size_t size,
+						    enum dma_data_direction dir)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_single_for_device)
+		ops->sync_single_for_device(dev, addr + offset, size, dir);
+	debug_dma_sync_single_range_for_device(dev, addr, offset, size, dir);
+}
+
+static inline void
+dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
+		    int nelems, enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_sg_for_cpu)
+		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
+	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
+}
+
+static inline void
+dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
+		       int nelems, enum dma_data_direction dir)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!valid_dma_direction(dir));
+	if (ops->sync_sg_for_device)
+		ops->sync_sg_for_device(dev, sg, nelems, dir);
+	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
+
+}
+
+#define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, NULL)
+#define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, NULL)
+#define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, NULL)
+#define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, NULL)
+
+extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
+			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
+
+void *dma_common_contiguous_remap(struct page *page, size_t size,
+			unsigned long vm_flags,
+			pgprot_t prot, const void *caller);
+
+void *dma_common_pages_remap(struct page **pages, size_t size,
+			unsigned long vm_flags, pgprot_t prot,
+			const void *caller);
+void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
+
+/**
+ * dma_mmap_attrs - map a coherent DMA allocation into user space
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @vma: vm_area_struct describing requested user mapping
+ * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
+ * @handle: device-view address returned from dma_alloc_attrs
+ * @size: size of memory originally requested in dma_alloc_attrs
+ * @attrs: attributes of mapping properties requested in dma_alloc_attrs
+ *
+ * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
+ * into user space.  The coherent DMA buffer must not be freed by the
+ * driver until the user space mapping has been released.
+ */
+static inline int
+dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
+	       dma_addr_t dma_addr, size_t size, struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (ops->mmap)
+		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
+}
+
+#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, NULL)
+
+int
+dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
+		       void *cpu_addr, dma_addr_t dma_addr, size_t size);
+
+static inline int
+dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
+		      dma_addr_t dma_addr, size_t size, struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (ops->get_sgtable)
+		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+					attrs);
+	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size);
+}
+
+#define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, NULL)
+
+#ifndef arch_dma_alloc_attrs
+#define arch_dma_alloc_attrs(dev, flag)	(true)
+#endif
+
+static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+				       dma_addr_t *dma_handle, gfp_t flag,
+				       struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	void *cpu_addr;
+
+	BUG_ON(!ops);
+
+	if (dma_alloc_from_coherent(dev, size, dma_handle, &cpu_addr))
+		return cpu_addr;
+
+	if (!arch_dma_alloc_attrs(&dev, &flag))
+		return NULL;
+	if (!ops->alloc)
+		return NULL;
+
+	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+	return cpu_addr;
+}
+
+static inline void dma_free_attrs(struct device *dev, size_t size,
+				     void *cpu_addr, dma_addr_t dma_handle,
+				     struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!ops);
+	WARN_ON(irqs_disabled());
+
+	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
+		return;
+
+	if (!ops->free)
+		return;
+
+	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+	ops->free(dev, size, cpu_addr, dma_handle, attrs);
+}
+
+static inline void *dma_alloc_coherent(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t flag)
+{
+	return dma_alloc_attrs(dev, size, dma_handle, flag, NULL);
+}
+
+static inline void dma_free_coherent(struct device *dev, size_t size,
+		void *cpu_addr, dma_addr_t dma_handle)
+{
+	return dma_free_attrs(dev, size, cpu_addr, dma_handle, NULL);
+}
+
+static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, gfp_t gfp)
+{
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_NON_CONSISTENT, &attrs);
+	return dma_alloc_attrs(dev, size, dma_handle, gfp, &attrs);
+}
+
+static inline void dma_free_noncoherent(struct device *dev, size_t size,
+		void *cpu_addr, dma_addr_t dma_handle)
+{
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_NON_CONSISTENT, &attrs);
+	dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
+}
+
+static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	debug_dma_mapping_error(dev, dma_addr);
+
+	if (get_dma_ops(dev)->mapping_error)
+		return get_dma_ops(dev)->mapping_error(dev, dma_addr);
+
+#ifdef DMA_ERROR_CODE
+	return dma_addr == DMA_ERROR_CODE;
+#else
+	return 0;
+#endif
+}
+
+#ifndef HAVE_ARCH_DMA_SUPPORTED
+static inline int dma_supported(struct device *dev, u64 mask)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	if (!ops)
+		return 0;
+	if (!ops->dma_supported)
+		return 1;
+	return ops->dma_supported(dev, mask);
+}
+#endif
+
+#ifndef HAVE_ARCH_DMA_SET_MASK
+static inline int dma_set_mask(struct device *dev, u64 mask)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+
+	if (ops->set_dma_mask)
+		return ops->set_dma_mask(dev, mask);
+
+	if (!dev->dma_mask || !dma_supported(dev, mask))
+		return -EIO;
+	*dev->dma_mask = mask;
+	return 0;
+}
 #endif
 
 static inline u64 dma_get_mask(struct device *dev)
@@ -259,22 +619,6 @@ static inline void dmam_release_declared_memory(struct device *dev)
 }
 #endif /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
 
-#ifndef CONFIG_HAVE_DMA_ATTRS
-struct dma_attrs;
-
-#define dma_map_single_attrs(dev, cpu_addr, size, dir, attrs) \
-	dma_map_single(dev, cpu_addr, size, dir)
-
-#define dma_unmap_single_attrs(dev, dma_addr, size, dir, attrs) \
-	dma_unmap_single(dev, dma_addr, size, dir)
-
-#define dma_map_sg_attrs(dev, sgl, nents, dir, attrs) \
-	dma_map_sg(dev, sgl, nents, dir)
-
-#define dma_unmap_sg_attrs(dev, sgl, nents, dir, attrs) \
-	dma_unmap_sg(dev, sgl, nents, dir)
-
-#else
 static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
 					   dma_addr_t *dma_addr, gfp_t gfp)
 {
@@ -300,7 +644,6 @@ static inline int dma_mmap_writecombine(struct device *dev,
 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
 	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
 }
-#endif /* CONFIG_HAVE_DMA_ATTRS */
 
 #ifdef CONFIG_NEED_DMA_MAP_STATE
 #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME

commit 002edb6f6f2a79bea50de11260ddc9572e6db731
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Fri Nov 6 16:32:51 2015 -0800

    dma-mapping: tidy up dma_parms default handling
    
    Many DMA controllers and other devices set max_segment_size to
    indicate their scatter-gather capability, but have no interest in
    segment_boundary_mask. However, the existence of a dma_parms structure
    precludes the use of any default value, leaving them as zeros (assuming
    a properly kzalloc'ed structure). If a well-behaved IOMMU (or SWIOTLB)
    then tries to respect this by ensuring a mapped segment does not cross
    a zero-byte boundary, hilarity ensues.
    
    Since zero is a nonsensical value for either parameter, treat it as an
    indicator for "default", as might be expected. In the process, clean up
    a bit by replacing the bare constants with slightly more meaningful
    macros and removing the superfluous "else" statements.
    
    [akpm@linux-foundation.org: dma-mapping.h needs sizes.h for SZ_64K]
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Sumit Semwal <sumit.semwal@linaro.org>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Sakari Ailus <sakari.ailus@iki.fi>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ac07ff090919..2e551e2d2d03 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -1,6 +1,7 @@
 #ifndef _LINUX_DMA_MAPPING_H
 #define _LINUX_DMA_MAPPING_H
 
+#include <linux/sizes.h>
 #include <linux/string.h>
 #include <linux/device.h>
 #include <linux/err.h>
@@ -145,7 +146,9 @@ static inline void arch_teardown_dma_ops(struct device *dev) { }
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)
 {
-	return dev->dma_parms ? dev->dma_parms->max_segment_size : 65536;
+	if (dev->dma_parms && dev->dma_parms->max_segment_size)
+		return dev->dma_parms->max_segment_size;
+	return SZ_64K;
 }
 
 static inline unsigned int dma_set_max_seg_size(struct device *dev,
@@ -154,14 +157,15 @@ static inline unsigned int dma_set_max_seg_size(struct device *dev,
 	if (dev->dma_parms) {
 		dev->dma_parms->max_segment_size = size;
 		return 0;
-	} else
-		return -EIO;
+	}
+	return -EIO;
 }
 
 static inline unsigned long dma_get_seg_boundary(struct device *dev)
 {
-	return dev->dma_parms ?
-		dev->dma_parms->segment_boundary_mask : 0xffffffff;
+	if (dev->dma_parms && dev->dma_parms->segment_boundary_mask)
+		return dev->dma_parms->segment_boundary_mask;
+	return DMA_BIT_MASK(32);
 }
 
 static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
@@ -169,8 +173,8 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 	if (dev->dma_parms) {
 		dev->dma_parms->segment_boundary_mask = mask;
 		return 0;
-	} else
-		return -EIO;
+	}
+	return -EIO;
 }
 
 #ifndef dma_max_pfn

commit 04abab698285297115e5096b3100df1064045529
Author: Ricardo Ribalda Delgado <ricardo.ribalda@gmail.com>
Date:   Wed Feb 11 13:53:15 2015 +0100

    include/dma-mapping: Clarify output of dma_map_sg
    
    Although dma_map_sg returns 0 on error and it cannot return a
    value < 0, the function returns a signed integer.
    
    Most of the time, this function is used with a scatterlist structure.
    This structure uses an unsigned integer for the number of memory.
    
    A dma developer that has not read in detail DMA-API.txt, can wrongly
    return a value < 0 on error.
    
    Signed-off-by: Ricardo Ribalda Delgado <ricardo.ribalda@gmail.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index c3007cb4bfa6..ac07ff090919 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -34,6 +34,10 @@ struct dma_map_ops {
 	void (*unmap_page)(struct device *dev, dma_addr_t dma_handle,
 			   size_t size, enum dma_data_direction dir,
 			   struct dma_attrs *attrs);
+	/*
+	 * map_sg returns 0 on error and a value > 0 on success.
+	 * It should never return a value < 0.
+	 */
 	int (*map_sg)(struct device *dev, struct scatterlist *sg,
 		      int nents, enum dma_data_direction dir,
 		      struct dma_attrs *attrs);

commit 97890ba9289c66e23f2f2d431937693b6498d6f6
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 16:24:20 2014 +0100

    dma-mapping: detect and configure IOMMU in of_dma_configure
    
    This patch extends of_dma_configure so that it sets up the IOMMU for a
    device, as well as the coherent/non-coherent DMA mapping ops.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8a1560f95d4a..c3007cb4bfa6 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -130,7 +130,13 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 extern u64 dma_get_required_mask(struct device *dev);
 
 #ifndef arch_setup_dma_ops
-static inline void arch_setup_dma_ops(struct device *dev, bool coherent) { }
+static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
+				      u64 size, struct iommu_ops *iommu,
+				      bool coherent) { }
+#endif
+
+#ifndef arch_teardown_dma_ops
+static inline void arch_teardown_dma_ops(struct device *dev) { }
 #endif
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit a3a60f81ee6f8fa65a57fa186b395bcd1f1bb097
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 15:49:10 2014 +0100

    dma-mapping: replace set_arch_dma_coherent_ops with arch_setup_dma_ops
    
    set_arch_dma_coherent_ops is called from of_dma_configure in order to
    swizzle the architectural dma-mapping functions over to a cache-coherent
    implementation. This is currently implemented only for ARM.
    
    In anticipation of re-using this mechanism for IOMMU-backed dma-mapping
    ops too, this patch replaces the function with a broader
    arch_setup_dma_ops callback which will be extended in future.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d5d388160f42..8a1560f95d4a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -129,11 +129,8 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 
 extern u64 dma_get_required_mask(struct device *dev);
 
-#ifndef set_arch_dma_coherent_ops
-static inline int set_arch_dma_coherent_ops(struct device *dev)
-{
-	return 0;
-}
+#ifndef arch_setup_dma_ops
+static inline void arch_setup_dma_ops(struct device *dev, bool coherent) { }
 #endif
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit b4bbb107d73bbc0d92c9ae7fd8e69580aa9381e7
Author: Thierry Reding <treding@nvidia.com>
Date:   Fri Jun 27 11:56:58 2014 +0200

    dma-mapping: Provide write-combine allocations
    
    Provide an implementation for dma_{alloc,free,mmap}_writecombine() when
    the architecture supports DMA attributes.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 931b70986272..d5d388160f42 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -263,6 +263,32 @@ struct dma_attrs;
 #define dma_unmap_sg_attrs(dev, sgl, nents, dir, attrs) \
 	dma_unmap_sg(dev, sgl, nents, dir)
 
+#else
+static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
+					   dma_addr_t *dma_addr, gfp_t gfp)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_alloc_attrs(dev, size, dma_addr, gfp, &attrs);
+}
+
+static inline void dma_free_writecombine(struct device *dev, size_t size,
+					 void *cpu_addr, dma_addr_t dma_addr)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_free_attrs(dev, size, cpu_addr, dma_addr, &attrs);
+}
+
+static inline int dma_mmap_writecombine(struct device *dev,
+					struct vm_area_struct *vma,
+					void *cpu_addr, dma_addr_t dma_addr,
+					size_t size)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
+}
 #endif /* CONFIG_HAVE_DMA_ATTRS */
 
 #ifdef CONFIG_NEED_DMA_MAP_STATE

commit eb3d3ec567e868c8a3bfbfdfc9465ffd52983d11
Merge: c3c55a072039 bd63ce27d9d6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 5 15:57:04 2014 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm into next
    
    Pull ARM updates from Russell King:
    
     - Major clean-up of the L2 cache support code.  The existing mess was
       becoming rather unmaintainable through all the additions that others
       have done over time.  This turns it into a much nicer structure, and
       implements a few performance improvements as well.
    
     - Clean up some of the CP15 control register tweaks for alignment
       support, moving some code and data into alignment.c
    
     - DMA properties for ARM, from Santosh and reviewed by DT people.  This
       adds DT properties to specify bus translations we can't discover
       automatically, and to indicate whether devices are coherent.
    
     - Hibernation support for ARM
    
     - Make ftrace work with read-only text in modules
    
     - add suspend support for PJ4B CPUs
    
     - rework interrupt masking for undefined instruction handling, which
       allows us to enable interrupts earlier in the handling of these
       exceptions.
    
     - support for big endian page tables
    
     - fix stacktrace support to exclude stacktrace functions from the
       trace, and add save_stack_trace_regs() implementation so that kprobes
       can record stack traces.
    
     - Add support for the Cortex-A17 CPU.
    
     - Remove last vestiges of ARM710 support.
    
     - Removal of ARM "meminfo" structure, finally converting us solely to
       memblock to handle the early memory initialisation.
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (142 commits)
      ARM: ensure C page table setup code follows assembly code (part II)
      ARM: ensure C page table setup code follows assembly code
      ARM: consolidate last remaining open-coded alignment trap enable
      ARM: remove global cr_no_alignment
      ARM: remove CPU_CP15 conditional from alignment.c
      ARM: remove unused adjust_cr() function
      ARM: move "noalign" command line option to alignment.c
      ARM: provide common method to clear bits in CPU control register
      ARM: 8025/1: Get rid of meminfo
      ARM: 8060/1: mm: allow sub-architectures to override PCI I/O memory type
      ARM: 8066/1: correction for ARM patch 8031/2
      ARM: 8049/1: ftrace/add save_stack_trace_regs() implementation
      ARM: 8065/1: remove last use of CONFIG_CPU_ARM710
      ARM: 8062/1: Modify ldrt fixup handler to re-execute the userspace instruction
      ARM: 8047/1: rwsem: use asm-generic rwsem implementation
      ARM: l2c: trial at enabling some Cortex-A9 optimisations
      ARM: l2c: add warnings for stuff modifying aux_ctrl register values
      ARM: l2c: print a warning with L2C-310 caches if the cache size is modified
      ARM: l2c: remove old .set_debug method
      ARM: l2c: kill L2X0_AUX_CTRL_MASK before anyone else makes use of this
      ...

commit 88a984ba0795f14a3847edbd7fabe652289ea89b
Author: Bjorn Helgaas <bhelgaas@google.com>
Date:   Tue May 20 16:54:22 2014 -0600

    DMA-API: Change dma_declare_coherent_memory() CPU address to phys_addr_t
    
    dma_declare_coherent_memory() takes two addresses for a region of memory: a
    "bus_addr" and a "device_addr".  I think the intent is that "bus_addr" is
    the physical address a *CPU* would use to access the region, and
    "device_addr" is the bus address the *device* would use to address the
    region.
    
    Rename "bus_addr" to "phys_addr" and change its type to phys_addr_t.
    Most callers already supply a phys_addr_t for this argument.  The others
    supply a 32-bit integer (a constant, unsigned int, or __u32) and need no
    change.
    
    Use "unsigned long", not phys_addr_t, to hold PFNs.
    
    No functional change (this could theoretically fix a truncation in a config
    with 32-bit dma_addr_t and 64-bit phys_addr_t, but I don't think there are
    any such cases involving this code).
    
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: James Bottomley <jbottomley@Parallels.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index b9aa2b97aab5..0c3eab1e39ac 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -192,7 +192,7 @@ static inline int dma_get_cache_alignment(void)
 
 #ifndef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
 static inline int
-dma_declare_coherent_memory(struct device *dev, dma_addr_t bus_addr,
+dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
 			    dma_addr_t device_addr, size_t size, int flags)
 {
 	return 0;
@@ -223,13 +223,14 @@ extern void *dmam_alloc_noncoherent(struct device *dev, size_t size,
 extern void dmam_free_noncoherent(struct device *dev, size_t size, void *vaddr,
 				  dma_addr_t dma_handle);
 #ifdef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
-extern int dmam_declare_coherent_memory(struct device *dev, dma_addr_t bus_addr,
+extern int dmam_declare_coherent_memory(struct device *dev,
+					phys_addr_t phys_addr,
 					dma_addr_t device_addr, size_t size,
 					int flags);
 extern void dmam_release_declared_memory(struct device *dev);
 #else /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
 static inline int dmam_declare_coherent_memory(struct device *dev,
-				dma_addr_t bus_addr, dma_addr_t device_addr,
+				phys_addr_t phys_addr, dma_addr_t device_addr,
 				size_t size, gfp_t gfp)
 {
 	return 0;

commit 77f2ea2f8d0833f9e976368481fb9a0775acf9e7
Author: Bjorn Helgaas <bhelgaas@google.com>
Date:   Wed Apr 30 11:20:53 2014 -0600

    DMA-API: Clarify physical/bus address distinction
    
    The DMA-API documentation sometimes refers to "physical addresses" when it
    really means "bus addresses."  Sometimes these are identical, but they may
    be different if the bridge leading to the bus performs address translation.
    Update the documentation to use "bus address" when appropriate.
    
    Also, consistently capitalize "DMA", use parens with function names, use
    dev_printk() in examples, and reword a few sections for clarity.
    
    No functional change; documentation changes only.
    
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: James Bottomley <jbottomley@Parallels.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index fd4aee29ad10..b9aa2b97aab5 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -8,6 +8,12 @@
 #include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
 
+/*
+ * A dma_addr_t can hold any valid DMA or bus address for the platform.
+ * It can be given to a device to use as a DMA source or target.  A CPU cannot
+ * reference a dma_addr_t directly because there may be translation between
+ * its physical address space and the bus address space.
+ */
 struct dma_map_ops {
 	void* (*alloc)(struct device *dev, size_t size,
 				dma_addr_t *dma_handle, gfp_t gfp,

commit 591c1ee465ce5372385dbc41e7d3e36cbb477bd8
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Thu Apr 24 11:30:04 2014 -0400

    of: configure the platform device dma parameters
    
    Retrieve DMA configuration from DT and setup platform device's DMA
    parameters. The DMA configuration in DT has to be specified using
    "dma-ranges" and "dma-coherent" properties if supported.
    
    We setup dma_pfn_offset using "dma-ranges" and dma_coherent_ops
    using "dma-coherent" device tree properties.
    
    The set_arch_dma_coherent_ops macro has to be defined by arch if
    it supports coherent dma_ops. Otherwise, set_arch_dma_coherent_ops() is
    declared as nop.
    
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index fd4aee29ad10..c7d9b1b14ce7 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -123,6 +123,13 @@ static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
 
 extern u64 dma_get_required_mask(struct device *dev);
 
+#ifndef set_arch_dma_coherent_ops
+static inline int set_arch_dma_coherent_ops(struct device *dev)
+{
+	return 0;
+}
+#endif
+
 static inline unsigned int dma_get_max_seg_size(struct device *dev)
 {
 	return dev->dma_parms ? dev->dma_parms->max_segment_size : 65536;

commit 00c8f1623658947a97345ecb86b71232ff540d0d
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Jul 29 14:18:48 2013 +0100

    ARM: 7795/1: mm: dma-mapping: Add dma_max_pfn(dev) helper function
    
    Most of the kernel assumes that PFN0 is the start of the physical
    memory (RAM). This assumptions is not true on most of the ARM SOCs
    and hence and if one try to update the ARM port to follow the assumptions,
    we end of breaking the dma bounce limit for few block layer drivers.
    One such example is trying to unify the meaning of max*_pfn on ARM
    as the bootmem layer expects, breaks few block layer driver dma
    bounce limit.
    
    To fix this problem, we introduce dma_max_pfn(dev) generic helper with
    a possibility of override from the architecture code. The helper converts
    a DMA bitmask of bits to a block PFN number. In all the generic cases,
    it is just  "dev->dma_mask >> PAGE_SHIFT" and hence default behavior
    is maintained as is.
    
    Subsequent patches will make use of the helper. No functional change.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 27d1421ad42c..fd4aee29ad10 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -153,6 +153,13 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 		return -EIO;
 }
 
+#ifndef dma_max_pfn
+static inline unsigned long dma_max_pfn(struct device *dev)
+{
+	return *dev->dma_mask >> PAGE_SHIFT;
+}
+#endif
+
 static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
 					dma_addr_t *dma_handle, gfp_t flag)
 {

commit fa6a8d6d65b19ab44e5244ea499bcd553cc72343
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jun 27 12:21:45 2013 +0100

    DMA-API: provide a helper to setup DMA masks
    
    Many drivers contain code such as:
    
            dev->dma_mask = &dev->coherent_dma_mask;
            dev->coherent_dma_mask = MASK;
    
    Let's move this pattern out of drivers and have the DMA API provide a
    helper for it.  This helper uses dma_set_mask_and_coherent() to allow
    platform issues to be properly dealt with via dma_set_mask()/
    dma_is_supported().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ec951f98e3d9..27d1421ad42c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -111,6 +111,16 @@ static inline int dma_set_mask_and_coherent(struct device *dev, u64 mask)
 	return rc;
 }
 
+/*
+ * Similar to the above, except it deals with the case where the device
+ * does not have dev->dma_mask appropriately setup.
+ */
+static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
+{
+	dev->dma_mask = &dev->coherent_dma_mask;
+	return dma_set_mask_and_coherent(dev, mask);
+}
+
 extern u64 dma_get_required_mask(struct device *dev);
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit 4aa806b771d16b810771d86ce23c4c3160888db3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jun 26 13:49:44 2013 +0100

    DMA-API: provide a helper to set both DMA and coherent DMA masks
    
    Provide a helper to set both the DMA and coherent DMA masks to the
    same value - this avoids duplicated code in a number of drivers,
    sometimes with buggy error handling, and also allows us identify
    which drivers do things differently.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 3a8d0a2af607..ec951f98e3d9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -97,6 +97,20 @@ static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
 }
 #endif
 
+/*
+ * Set both the DMA mask and the coherent DMA mask to the same thing.
+ * Note that we don't check the return value from dma_set_coherent_mask()
+ * as the DMA API guarantees that the coherent DMA mask can be set to
+ * the same or smaller than the streaming DMA mask.
+ */
+static inline int dma_set_mask_and_coherent(struct device *dev, u64 mask)
+{
+	int rc = dma_set_mask(dev, mask);
+	if (rc == 0)
+		dma_set_coherent_mask(dev, mask);
+	return rc;
+}
+
 extern u64 dma_get_required_mask(struct device *dev);
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit ede23fa8161c1a04aa1b3bf5447812ca14b3fef1
Author: Joe Perches <joe@perches.com>
Date:   Mon Aug 26 22:45:23 2013 -0700

    drivers:net: Convert dma_alloc_coherent(...__GFP_ZERO) to dma_zalloc_coherent
    
    __GFP_ZERO is an uncommon flag and perhaps is better
    not used.  static inline dma_zalloc_coherent exists
    so convert the uses of dma_alloc_coherent with __GFP_ZERO
    to the more common kernel style with zalloc.
    
    Remove memset from the static inline dma_zalloc_coherent
    and add just one use of __GFP_ZERO instead.
    
    Trivially reduces the size of the existing uses of
    dma_zalloc_coherent.
    
    Realign arguments as appropriate.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Acked-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 94af41858513..3a8d0a2af607 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -132,9 +132,8 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
 					dma_addr_t *dma_handle, gfp_t flag)
 {
-	void *ret = dma_alloc_coherent(dev, size, dma_handle, flag);
-	if (ret)
-		memset(ret, 0, size);
+	void *ret = dma_alloc_coherent(dev, size, dma_handle,
+				       flag | __GFP_ZERO);
 	return ret;
 }
 

commit d2b7428eb0caa7c66e34b6ac869a43915b294123
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 13 10:05:52 2012 +0200

    common: dma-mapping: introduce dma_get_sgtable() function
    
    This patch adds dma_get_sgtable() function which is required to let
    drivers to share the buffers allocated by DMA-mapping subsystem. Right
    now the driver gets a dma address of the allocated buffer and the kernel
    virtual mapping for it. If it wants to share it with other device (= map
    into its dma address space) it usually hacks around kernel virtual
    addresses to get pointers to pages or assumes that both devices share
    the DMA address space. Both solutions are just hacks for the special
    cases, which should be avoided in the final version of buffer sharing.
    
    To solve this issue in a generic way, a new call to DMA mapping has been
    introduced - dma_get_sgtable(). It allocates a scatter-list which
    describes the allocated buffer and lets the driver(s) to use it with
    other device(s) by calling dma_map_sg() on it.
    
    This patch provides a generic implementation based on virt_to_page()
    call. Architectures which require more sophisticated translation might
    provide their own get_sgtable() methods.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index dfc099e56a66..94af41858513 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -18,6 +18,9 @@ struct dma_map_ops {
 	int (*mmap)(struct device *, struct vm_area_struct *,
 			  void *, dma_addr_t, size_t, struct dma_attrs *attrs);
 
+	int (*get_sgtable)(struct device *dev, struct sg_table *sgt, void *,
+			   dma_addr_t, size_t, struct dma_attrs *attrs);
+
 	dma_addr_t (*map_page)(struct device *dev, struct page *page,
 			       unsigned long offset, size_t size,
 			       enum dma_data_direction dir,

commit 58bca4a8fa90fcf9069379653b396b2cec642f7f
Merge: 64ebe9873118 64d70fe5d364
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 4 17:13:43 2012 -0700

    Merge branch 'for-linus' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping
    
    Pull DMA mapping branch from Marek Szyprowski:
     "Short summary for the whole series:
    
      A few limitations have been identified in the current dma-mapping
      design and its implementations for various architectures.  There exist
      more than one function for allocating and freeing the buffers:
      currently these 3 are used dma_{alloc, free}_coherent,
      dma_{alloc,free}_writecombine, dma_{alloc,free}_noncoherent.
    
      For most of the systems these calls are almost equivalent and can be
      interchanged.  For others, especially the truly non-coherent ones
      (like ARM), the difference can be easily noticed in overall driver
      performance.  Sadly not all architectures provide implementations for
      all of them, so the drivers might need to be adapted and cannot be
      easily shared between different architectures.  The provided patches
      unify all these functions and hide the differences under the already
      existing dma attributes concept.  The thread with more references is
      available here:
    
        http://www.spinics.net/lists/linux-sh/msg09777.html
    
      These patches are also a prerequisite for unifying DMA-mapping
      implementation on ARM architecture with the common one provided by
      dma_map_ops structure and extending it with IOMMU support.  More
      information is available in the following thread:
    
        http://thread.gmane.org/gmane.linux.kernel.cross-arch/12819
    
      More works on dma-mapping framework are planned, especially in the
      area of buffer sharing and managing the shared mappings (together with
      the recently introduced dma_buf interface: commit d15bd7ee445d
      "dma-buf: Introduce dma buffer sharing mechanism").
    
      The patches in the current set introduce a new alloc/free methods
      (with support for memory attributes) in dma_map_ops structure, which
      will later replace dma_alloc_coherent and dma_alloc_writecombine
      functions."
    
    People finally started piping up with support for merging this, so I'm
    merging it as the last of the pending stuff from the merge window.
    Looks like pohmelfs is going to wait for 3.5 and more external support
    for merging.
    
    * 'for-linus' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping:
      common: DMA-mapping: add NON-CONSISTENT attribute
      common: DMA-mapping: add WRITE_COMBINE attribute
      common: dma-mapping: introduce mmap method
      common: dma-mapping: remove old alloc_coherent and free_coherent methods
      Hexagon: adapt for dma_map_ops changes
      Unicore32: adapt for dma_map_ops changes
      Microblaze: adapt for dma_map_ops changes
      SH: adapt for dma_map_ops changes
      Alpha: adapt for dma_map_ops changes
      SPARC: adapt for dma_map_ops changes
      PowerPC: adapt for dma_map_ops changes
      MIPS: adapt for dma_map_ops changes
      X86 & IA64: adapt for dma_map_ops changes
      common: dma-mapping: introduce generic alloc() and free() methods

commit 9adc537452e1e341cabd39a02d4788d3c510b0e2
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Dec 21 16:55:33 2011 +0100

    common: dma-mapping: introduce mmap method
    
    Introduce new generic mmap method with attributes argument.
    
    This method lets drivers to create a userspace mapping for a DMA buffer
    in generic, architecture independent way.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2fc413a873ea..b903a20867f7 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -15,6 +15,9 @@ struct dma_map_ops {
 	void (*free)(struct device *dev, size_t size,
 			      void *vaddr, dma_addr_t dma_handle,
 			      struct dma_attrs *attrs);
+	int (*mmap)(struct device *, struct vm_area_struct *,
+			  void *, dma_addr_t, size_t, struct dma_attrs *attrs);
+
 	dma_addr_t (*map_page)(struct device *dev, struct page *page,
 			       unsigned long offset, size_t size,
 			       enum dma_data_direction dir,

commit 645ae3bce3e62541f8dc2701dde2a2791d842b6c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Dec 21 16:55:57 2011 +0100

    common: dma-mapping: remove old alloc_coherent and free_coherent methods
    
    Remove old, unused alloc_coherent and free_coherent methods from
    dma_map_ops structure.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8cc7f95289ac..2fc413a873ea 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -9,10 +9,6 @@
 #include <linux/scatterlist.h>
 
 struct dma_map_ops {
-	void* (*alloc_coherent)(struct device *dev, size_t size,
-				dma_addr_t *dma_handle, gfp_t gfp);
-	void (*free_coherent)(struct device *dev, size_t size,
-			      void *vaddr, dma_addr_t dma_handle);
 	void* (*alloc)(struct device *dev, size_t size,
 				dma_addr_t *dma_handle, gfp_t gfp,
 				struct dma_attrs *attrs);

commit 613c4578d4079a14dbee76ef7e0c80f635522fe3
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Mar 28 16:36:27 2012 +0200

    common: dma-mapping: introduce generic alloc() and free() methods
    
    Introduce new generic alloc and free methods with attributes argument.
    
    Existing alloc_coherent and free_coherent can be implemented on top of the
    new calls with NULL attributes argument. Later also dma_alloc_non_coherent
    can be implemented using DMA_ATTR_NONCOHERENT attribute as well as
    dma_alloc_writecombine with separate DMA_ATTR_WRITECOMBINE attribute.
    
    This way the drivers will get more generic, platform independent way of
    allocating dma buffers with specific parameters.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.ud.au>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e13117cbd2f7..8cc7f95289ac 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -13,6 +13,12 @@ struct dma_map_ops {
 				dma_addr_t *dma_handle, gfp_t gfp);
 	void (*free_coherent)(struct device *dev, size_t size,
 			      void *vaddr, dma_addr_t dma_handle);
+	void* (*alloc)(struct device *dev, size_t size,
+				dma_addr_t *dma_handle, gfp_t gfp,
+				struct dma_attrs *attrs);
+	void (*free)(struct device *dev, size_t size,
+			      void *vaddr, dma_addr_t dma_handle,
+			      struct dma_attrs *attrs);
 	dma_addr_t (*map_page)(struct device *dev, struct page *page,
 			       unsigned long offset, size_t size,
 			       enum dma_data_direction dir,

commit 58af4a244fa9f7ef86f45aa9f8fa835a89274bdd
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Tue Mar 20 14:33:01 2012 -0500

    ARM: dma-mapping: convert ARCH_HAS_DMA_SET_COHERENT_MASK to kconfig symbol
    
    The only users of ARCH_HAS_DMA_SET_COHERENT_MASK are 2 ARM platforms:
    ixp4xx and pxa cm_x2xx. We've been getting lucky that the define is
    implicitly included before dma-mapping.h, but the removal of io.h broke
    things (c334bc1 ARM: make mach/io.h include optional). Since memory.h
    is the correct place, but no longer exists, convert the define to a
    kconfig entry.
    
    Reported-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Imre Kaloz <kaloz@openwrt.org>
    Cc: Krzysztof Halasa <khc@pm.waw.pl>
    Cc: Eric Miao <eric.y.miao@gmail.com>
    Acked-by: Haojian Zhuang <haojian.zhuang@marvell.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e13117cbd2f7..5a736af3cc7a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -77,7 +77,7 @@ static inline u64 dma_get_mask(struct device *dev)
 	return DMA_BIT_MASK(32);
 }
 
-#ifdef ARCH_HAS_DMA_SET_COHERENT_MASK
+#ifdef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
 int dma_set_coherent_mask(struct device *dev, u64 mask);
 #else
 static inline int dma_set_coherent_mask(struct device *dev, u64 mask)

commit 1197ab2942f920f261952de0c392ac749a35796b
Merge: ec773e99ab4a 96cc017c5b7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 17:12:03 2011 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (106 commits)
      powerpc/p3060qds: Add support for P3060QDS board
      powerpc/83xx: Add shutdown request support to MCU handling on MPC8349 MITX
      powerpc/85xx: Make kexec to interate over online cpus
      powerpc/fsl_booke: Fix comment in head_fsl_booke.S
      powerpc/85xx: issue 15 EOI after core reset for FSL CoreNet devices
      powerpc/8xxx: Fix interrupt handling in MPC8xxx GPIO driver
      powerpc/85xx: Add 'fsl,pq3-gpio' compatiable for GPIO driver
      powerpc/86xx: Correct Gianfar support for GE boards
      powerpc/cpm: Clear muram before it is in use.
      drivers/virt: add ioctl for 32-bit compat on 64-bit to fsl-hv-manager
      powerpc/fsl_msi: add support for "msi-address-64" property
      powerpc/85xx: Setup secondary cores PIR with hard SMP id
      powerpc/fsl-booke: Fix settlbcam for 64-bit
      powerpc/85xx: Adding DCSR node to dtsi device trees
      powerpc/85xx: clean up FPGA device tree nodes for Freecsale QorIQ boards
      powerpc/85xx: fix PHYS_64BIT selection for P1022DS
      powerpc/fsl-booke: Fix setup_initial_memory_limit to not blindly map
      powerpc: respect mem= setting for early memory limit setup
      powerpc: Update corenet64_smp_defconfig
      powerpc: Update mpc85xx/corenet 32-bit defconfigs
      ...
    
    Fix up trivial conflicts in:
     - arch/powerpc/configs/40x/hcu4_defconfig
            removed stale file, edited elsewhere
     - arch/powerpc/include/asm/udbg.h, arch/powerpc/kernel/udbg.c:
            added opal and gelic drivers vs added ePAPR driver
     - drivers/tty/serial/8250.c
            moved UPIO_TSI to powerpc vs removed UPIO_DWAPB support

commit 842fa69f3e0c9a178b294e7af7c07f4c9d9e7af2
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Nov 2 13:39:33 2011 -0700

    include/linux/dma-mapping.h: add dma_zalloc_coherent()
    
    Lots of driver code does a dma_alloc_coherent() and then zeroes out the
    memory with a memset.  Make it easy for them.
    
    Cc: Alexandre Bounine <alexandre.bounine@idt.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 347fdc32177a..be86ae13893f 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -1,6 +1,7 @@
 #ifndef _LINUX_DMA_MAPPING_H
 #define _LINUX_DMA_MAPPING_H
 
+#include <linux/string.h>
 #include <linux/device.h>
 #include <linux/err.h>
 #include <linux/dma-attrs.h>
@@ -117,6 +118,15 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 		return -EIO;
 }
 
+static inline void *dma_zalloc_coherent(struct device *dev, size_t size,
+					dma_addr_t *dma_handle, gfp_t flag)
+{
+	void *ret = dma_alloc_coherent(dev, size, dma_handle, flag);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+
 #ifdef CONFIG_HAS_DMA
 static inline int dma_get_cache_alignment(void)
 {

commit 3a8f7558e475b68254d8bc3a2211f3f89bf67a71
Author: Milton Miller <miltonm@bga.com>
Date:   Fri Jun 24 09:05:23 2011 +0000

    dma-mapping: Add get_required_mask if arch overrides default
    
    If an architecture sets ARCH_HAS_DMA_GET_REQUIRED_MASK and has settable
    dma_map_ops, the required mask may change by the ops implementation.
    For example, a system that always has an mmu inline may only require 32
    bits while a swiotlb would desire bits to cover all of memory.
    
    Therefore add the field if the architecture does not use the generic
    definition of dma_get_required_mask. The first use will by by powerpc.
    Note that this does add some dependency on the order in which files are
    visible here.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 347fdc32177a..aa32fecd1d34 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -41,6 +41,9 @@ struct dma_map_ops {
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
 	int (*set_dma_mask)(struct device *dev, u64 mask);
+#ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
+	u64 (*get_required_mask)(struct device *dev);
+#endif
 	int is_phys;
 };
 

commit 91f6cdf86e8a2bd9b9c292a750179dc2c728a2d4
Author: WANG Cong <xiyou.wangcong@gmail.com>
Date:   Tue Jul 26 16:08:50 2011 -0700

    include/linux/dma-mapping.h: remove DMA_xxBIT_MASK macros
    
    git grep shows there are no users in tree, so we can remove them safely.
    
    Signed-off-by: WANG Cong <xiyou.wangcong@gmail.com>
    Acked-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Vinod Koul <vinod.koul@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 1a167c48d84d..347fdc32177a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -46,27 +46,6 @@ struct dma_map_ops {
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
-typedef u64 DMA_nnBIT_MASK __deprecated;
-
-/*
- * NOTE: do not use the below macros in new code and do not add new definitions
- * here.
- *
- * Instead, just open-code DMA_BIT_MASK(n) within your driver
- */
-#define DMA_64BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(64)
-#define DMA_48BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(48)
-#define DMA_47BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(47)
-#define DMA_40BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(40)
-#define DMA_39BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(39)
-#define DMA_35BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(35)
-#define DMA_32BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(32)
-#define DMA_31BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(31)
-#define DMA_30BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(30)
-#define DMA_29BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(29)
-#define DMA_28BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(28)
-#define DMA_24BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(24)
-
 #define DMA_MASK_NONE	0x0ULL
 
 static inline int valid_dma_direction(int dma_direction)

commit b7f080cfe223b3b7424872639d153695615a9255
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Jun 16 11:01:34 2011 +0000

    net: remove mm.h inclusion from netdevice.h
    
    Remove linux/mm.h inclusion from netdevice.h -- it's unused (I've checked manually).
    
    To prevent mm.h inclusion via other channels also extract "enum dma_data_direction"
    definition into separate header. This tiny piece is what gluing netdevice.h with mm.h
    via "netdevice.h => dmaengine.h => dma-mapping.h => scatterlist.h => mm.h".
    Removal of mm.h from scatterlist.h was tried and was found not feasible
    on most archs, so the link was cutoff earlier.
    
    Hope people are OK with tiny include file.
    
    Note, that mm_types.h is still dragged in, but it is a separate story.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ba8319ae5fcc..1a167c48d84d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -4,17 +4,9 @@
 #include <linux/device.h>
 #include <linux/err.h>
 #include <linux/dma-attrs.h>
+#include <linux/dma-direction.h>
 #include <linux/scatterlist.h>
 
-/* These definitions mirror those in pci.h, so they can be used
- * interchangeably with their PCI_ counterparts */
-enum dma_data_direction {
-	DMA_BIDIRECTIONAL = 0,
-	DMA_TO_DEVICE = 1,
-	DMA_FROM_DEVICE = 2,
-	DMA_NONE = 3,
-};
-
 struct dma_map_ops {
 	void* (*alloc_coherent)(struct device *dev, size_t size,
 				dma_addr_t *dma_handle, gfp_t gfp);

commit 710224fa2750cf449c02dd115548acebfdd2c86a
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Sep 22 13:04:55 2010 -0700

    arm: fix "arm: fix pci_set_consistent_dma_mask for dmabounce devices"
    
    This fixes the regression caused by the commit 6fee48cd330c68
    ("dma-mapping: arm: use generic pci_set_dma_mask and
    pci_set_consistent_dma_mask").
    
    ARM needs to clip the dma coherent mask for dmabounce devices. This
    restores the old trick.
    
    Note that strictly speaking, the DMA API doesn't allow architectures to do
    such but I'm not sure it's worth adding the new API to set the dma mask
    that allows architectures to clip it.
    
    Reported-by: Krzysztof Halasa <khc@pm.waw.pl>
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ce29b8151198..ba8319ae5fcc 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -102,6 +102,9 @@ static inline u64 dma_get_mask(struct device *dev)
 	return DMA_BIT_MASK(32);
 }
 
+#ifdef ARCH_HAS_DMA_SET_COHERENT_MASK
+int dma_set_coherent_mask(struct device *dev, u64 mask);
+#else
 static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
 {
 	if (!dma_supported(dev, mask))
@@ -109,6 +112,7 @@ static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
 	dev->coherent_dma_mask = mask;
 	return 0;
 }
+#endif
 
 extern u64 dma_get_required_mask(struct device *dev);
 

commit e259f191f2244df04a7746fac1df8aa68ebd0106
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Aug 13 09:39:18 2010 +0200

    dma-mapping: fix build errors on !HAS_DMA architectures
    
    commit 4565f0170dfc849b3629c27d769db800467baa62 "dma-mapping: unify
    dma_get_cache_alignment implementations" causes build errors on
    !HAS_DMA architectures/platforms like s390 and sun3:
    
    include/linux/dma-mapping.h:145: error: static declaration of 'dma_get_cache_alignment' follows non-static declaration
    include/asm-generic/dma-mapping-broken.h:73: error: previous declaration of 'dma_get_cache_alignment' was here
    
    Fix this by adding an explicit ifdef.
    
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index e0670a512056..ce29b8151198 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -142,6 +142,7 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 		return -EIO;
 }
 
+#ifdef CONFIG_HAS_DMA
 static inline int dma_get_cache_alignment(void)
 {
 #ifdef ARCH_DMA_MINALIGN
@@ -149,6 +150,7 @@ static inline int dma_get_cache_alignment(void)
 #endif
 	return 1;
 }
+#endif
 
 /* flags for the coherent memory api */
 #define	DMA_MEMORY_MAP			0x01

commit 4565f0170dfc849b3629c27d769db800467baa62
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:22 2010 -0700

    dma-mapping: unify dma_get_cache_alignment implementations
    
    dma_get_cache_alignment returns the minimum DMA alignment.  Architectures
    defines it as ARCH_DMA_MINALIGN (formally ARCH_KMALLOC_MINALIGN).  So we
    can unify dma_get_cache_alignment implementations.
    
    Note that some architectures implement dma_get_cache_alignment wrongly.
    dma_get_cache_alignment() should return the minimum DMA alignment.  So
    fully-coherent architectures should return 1.  This patch also fixes this
    issue.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 89b7e1a605b8..e0670a512056 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -142,6 +142,14 @@ static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
 		return -EIO;
 }
 
+static inline int dma_get_cache_alignment(void)
+{
+#ifdef ARCH_DMA_MINALIGN
+	return ARCH_DMA_MINALIGN;
+#endif
+	return 1;
+}
+
 /* flags for the coherent memory api */
 #define	DMA_MEMORY_MAP			0x01
 #define DMA_MEMORY_IO			0x02

commit 99d1bd2c131b2da10cf451c69e5b8c51761a901b
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed May 26 14:44:20 2010 -0700

    dma-mapping: remove deprecated dma_sync_single and dma_sync_sg API
    
    Since 2.6.5, it had been commented, 'for backwards compatibility,
    removed in 2.7.x'. Since 2.6.31, it have been marked as __deprecated.
    
    I think that we can remove the API safely now.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2ea149435c44..89b7e1a605b8 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -95,21 +95,6 @@ static inline int is_device_dma_capable(struct device *dev)
 #include <asm-generic/dma-mapping-broken.h>
 #endif
 
-/* for backwards compatibility, removed soon */
-static inline void __deprecated dma_sync_single(struct device *dev,
-						dma_addr_t addr, size_t size,
-						enum dma_data_direction dir)
-{
-	dma_sync_single_for_cpu(dev, addr, size, dir);
-}
-
-static inline void __deprecated dma_sync_sg(struct device *dev,
-					    struct scatterlist *sg, int nelems,
-					    enum dma_data_direction dir)
-{
-	dma_sync_sg_for_cpu(dev, sg, nelems, dir);
-}
-
 static inline u64 dma_get_mask(struct device *dev)
 {
 	if (dev && dev->dma_mask && *dev->dma_mask)

commit 5fd75a7850b5156aa93c9fb6404adb8f563b6e02
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed May 26 14:44:18 2010 -0700

    dma-mapping: remove unnecessary sync_single_range_* in dma_map_ops
    
    sync_single_range_for_cpu and sync_single_range_for_device hooks are
    unnecessary because sync_single_for_cpu and sync_single_for_device can
    be used instead.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ca32ed78b057..2ea149435c44 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -40,16 +40,6 @@ struct dma_map_ops {
 	void (*sync_single_for_device)(struct device *dev,
 				       dma_addr_t dma_handle, size_t size,
 				       enum dma_data_direction dir);
-	void (*sync_single_range_for_cpu)(struct device *dev,
-					  dma_addr_t dma_handle,
-					  unsigned long offset,
-					  size_t size,
-					  enum dma_data_direction dir);
-	void (*sync_single_range_for_device)(struct device *dev,
-					     dma_addr_t dma_handle,
-					     unsigned long offset,
-					     size_t size,
-					     enum dma_data_direction dir);
 	void (*sync_sg_for_cpu)(struct device *dev,
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction dir);

commit 6a1961f49ee8d7339ea2454443dfc0460e0b2748
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Mar 10 15:23:39 2010 -0800

    dma-mapping: dma-mapping.h: add dma_set_coherent_mask
    
    dma_set_coherent_mask corresponds to pci_set_consistent_dma_mask.  This is
    necessary to move to the generic device model DMA API from the PCI bus
    specific API in the long term.
    
    dma_set_coherent_mask works in the exact same way that
    pci_set_consistent_dma_mask does.  So this patch also changes
    pci_set_consistent_dma_mask to call dma_set_coherent_mask.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: James Bottomley <James.Bottomley@suse.de>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Greg KH <greg@kroah.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index c5ac9d49cc06..ca32ed78b057 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -127,6 +127,14 @@ static inline u64 dma_get_mask(struct device *dev)
 	return DMA_BIT_MASK(32);
 }
 
+static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
+{
+	if (!dma_supported(dev, mask))
+		return -EIO;
+	dev->coherent_dma_mask = mask;
+	return 0;
+}
+
 extern u64 dma_get_required_mask(struct device *dev);
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit 0acedc124aca35f5cce9d4ee288dc372bf517e09
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Mar 10 15:23:31 2010 -0800

    dma-mapping.h: add the dma_unmap state API
    
    Adds the following macros:
    
    DECLARE_DMA_UNMAP_ADDR(ADDR_NAME)
    DECLARE_DMA_UNMAP_LEN(LEN_NAME)
    dma_unmap_addr(PTR, ADDR_NAME)
    dma_unmap_addr_set(PTR, ADDR_NAME, VAL)
    dma_unmap_len(PTR, LEN_NAME)
    dma_unmap_len_set(PTR, LEN_NAME, VAL)
    
    The API corresponds to the pci_unmap state API.  We'll move to this new
    generic API from the PCI specific API in the long term.  As
    include/asm-generic/pci-dma-compat.h does, the pci_unmap API simply calls
    the new generic API for some time.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: James Bottomley <James.Bottomley@suse.de>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 91b761846061..c5ac9d49cc06 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -232,4 +232,20 @@ struct dma_attrs;
 
 #endif /* CONFIG_HAVE_DMA_ATTRS */
 
+#ifdef CONFIG_NEED_DMA_MAP_STATE
+#define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME
+#define DEFINE_DMA_UNMAP_LEN(LEN_NAME)          __u32 LEN_NAME
+#define dma_unmap_addr(PTR, ADDR_NAME)           ((PTR)->ADDR_NAME)
+#define dma_unmap_addr_set(PTR, ADDR_NAME, VAL)  (((PTR)->ADDR_NAME) = (VAL))
+#define dma_unmap_len(PTR, LEN_NAME)             ((PTR)->LEN_NAME)
+#define dma_unmap_len_set(PTR, LEN_NAME, VAL)    (((PTR)->LEN_NAME) = (VAL))
+#else
+#define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)
+#define DEFINE_DMA_UNMAP_LEN(LEN_NAME)
+#define dma_unmap_addr(PTR, ADDR_NAME)           (0)
+#define dma_unmap_addr_set(PTR, ADDR_NAME, VAL)  do { } while (0)
+#define dma_unmap_len(PTR, LEN_NAME)             (0)
+#define dma_unmap_len_set(PTR, LEN_NAME, VAL)    do { } while (0)
+#endif
+
 #endif

commit f726f30e32305a34a203ff975e60885aa7556c6a
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 4 19:08:24 2009 +0000

    dma: Add set_dma_mask hook to struct dma_map_ops
    
    POWERPC needs this hook. SPARC could use it too.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index c0f6c3cd788c..91b761846061 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -58,6 +58,7 @@ struct dma_map_ops {
 				   enum dma_data_direction dir);
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
+	int (*set_dma_mask)(struct device *dev, u64 mask);
 	int is_phys;
 };
 

commit 8f2502fd8157632909ff335a3c628e7caeec5e03
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Jul 10 10:05:00 2009 +0900

    remove is_buffer_dma_capable()
    
    is_buffer_dma_capable() was replaced with dma_capable().
    
    is_buffer_dma_capable() tells if a buffer is dma-capable or
    not. However, it doesn't take a pointer to struct device so it doesn't
    work for POWERPC.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Becky Bruce <beckyb@kernel.crashing.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 07dfd460d286..c0f6c3cd788c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -98,11 +98,6 @@ static inline int is_device_dma_capable(struct device *dev)
 	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
 }
 
-static inline int is_buffer_dma_capable(u64 mask, dma_addr_t addr, size_t size)
-{
-	return addr + size <= mask;
-}
-
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 #else

commit dbe6f1869188b6e04e38aa861dd198befb08bcd7
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Jun 18 16:49:19 2009 -0700

    dma-mapping: mark dma_sync_single and dma_sync_sg as deprecated
    
    dma_sync_single() and dma_sync_sg() have been described as "Backwards
    compat, remove in 2.7.x" for a long time (since 2.6.5).
    
    This marks dma_sync_single() and dma_sync_sg() as deprecated so the users
    get notified before removing them.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 38f5608d2460..07dfd460d286 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -109,9 +109,20 @@ static inline int is_buffer_dma_capable(u64 mask, dma_addr_t addr, size_t size)
 #include <asm-generic/dma-mapping-broken.h>
 #endif
 
-/* Backwards compat, remove in 2.7.x */
-#define dma_sync_single		dma_sync_single_for_cpu
-#define dma_sync_sg		dma_sync_sg_for_cpu
+/* for backwards compatibility, removed soon */
+static inline void __deprecated dma_sync_single(struct device *dev,
+						dma_addr_t addr, size_t size,
+						enum dma_data_direction dir)
+{
+	dma_sync_single_for_cpu(dev, addr, size, dir);
+}
+
+static inline void __deprecated dma_sync_sg(struct device *dev,
+					    struct scatterlist *sg, int nelems,
+					    enum dma_data_direction dir)
+{
+	dma_sync_sg_for_cpu(dev, sg, nelems, dir);
+}
 
 static inline u64 dma_get_mask(struct device *dev)
 {

commit d68412b6d1e8e5ed42d568330e598bd81914ccbd
Author: Jiri Slaby <jirislaby@gmail.com>
Date:   Thu Jun 18 16:49:18 2009 -0700

    dma-mapping: mark DMA_nBITS_MASK as deprecated
    
    Mark them deprecated so that out-of-tree developers get notified about
    this before their modules break when these macros are removed.
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Cc: Yang Hongyang <yanghy@cn.fujitsu.com>
    Acked-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8083b6a36a38..38f5608d2460 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -63,24 +63,26 @@ struct dma_map_ops {
 
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
+typedef u64 DMA_nnBIT_MASK __deprecated;
+
 /*
  * NOTE: do not use the below macros in new code and do not add new definitions
  * here.
  *
  * Instead, just open-code DMA_BIT_MASK(n) within your driver
  */
-#define DMA_64BIT_MASK	DMA_BIT_MASK(64)
-#define DMA_48BIT_MASK	DMA_BIT_MASK(48)
-#define DMA_47BIT_MASK	DMA_BIT_MASK(47)
-#define DMA_40BIT_MASK	DMA_BIT_MASK(40)
-#define DMA_39BIT_MASK	DMA_BIT_MASK(39)
-#define DMA_35BIT_MASK	DMA_BIT_MASK(35)
-#define DMA_32BIT_MASK	DMA_BIT_MASK(32)
-#define DMA_31BIT_MASK	DMA_BIT_MASK(31)
-#define DMA_30BIT_MASK	DMA_BIT_MASK(30)
-#define DMA_29BIT_MASK	DMA_BIT_MASK(29)
-#define DMA_28BIT_MASK	DMA_BIT_MASK(28)
-#define DMA_24BIT_MASK	DMA_BIT_MASK(24)
+#define DMA_64BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(64)
+#define DMA_48BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(48)
+#define DMA_47BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(47)
+#define DMA_40BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(40)
+#define DMA_39BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(39)
+#define DMA_35BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(35)
+#define DMA_32BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(32)
+#define DMA_31BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(31)
+#define DMA_30BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(30)
+#define DMA_29BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(29)
+#define DMA_28BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(28)
+#define DMA_24BIT_MASK	(DMA_nnBIT_MASK)DMA_BIT_MASK(24)
 
 #define DMA_MASK_NONE	0x0ULL
 

commit 284901a90a9e0b812ca3f5f852cbbfb60d10249d
Author: Yang Hongyang <yanghy@cn.fujitsu.com>
Date:   Mon Apr 6 19:01:15 2009 -0700

    dma-mapping: replace all DMA_32BIT_MASK macro with DMA_BIT_MASK(32)
    
    Replace all DMA_32BIT_MASK macro with DMA_BIT_MASK(32)
    
    Signed-off-by: Yang Hongyang<yanghy@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index d7d090d21031..8083b6a36a38 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -115,7 +115,7 @@ static inline u64 dma_get_mask(struct device *dev)
 {
 	if (dev && dev->dma_mask && *dev->dma_mask)
 		return *dev->dma_mask;
-	return DMA_32BIT_MASK;
+	return DMA_BIT_MASK(32);
 }
 
 extern u64 dma_get_required_mask(struct device *dev);

commit f0402a262e1a4c03fc66b83659823bdcaac3c41a
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Mon Jan 5 23:59:01 2009 +0900

    generic: add common struct for dma map operations
    
    This adds struct dma_map_ops include/linux/dma-mapping.h, which, is
    used to handle multiple sets of dma mapping API.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ba9114ec5d3a..d7d090d21031 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -3,6 +3,8 @@
 
 #include <linux/device.h>
 #include <linux/err.h>
+#include <linux/dma-attrs.h>
+#include <linux/scatterlist.h>
 
 /* These definitions mirror those in pci.h, so they can be used
  * interchangeably with their PCI_ counterparts */
@@ -13,6 +15,52 @@ enum dma_data_direction {
 	DMA_NONE = 3,
 };
 
+struct dma_map_ops {
+	void* (*alloc_coherent)(struct device *dev, size_t size,
+				dma_addr_t *dma_handle, gfp_t gfp);
+	void (*free_coherent)(struct device *dev, size_t size,
+			      void *vaddr, dma_addr_t dma_handle);
+	dma_addr_t (*map_page)(struct device *dev, struct page *page,
+			       unsigned long offset, size_t size,
+			       enum dma_data_direction dir,
+			       struct dma_attrs *attrs);
+	void (*unmap_page)(struct device *dev, dma_addr_t dma_handle,
+			   size_t size, enum dma_data_direction dir,
+			   struct dma_attrs *attrs);
+	int (*map_sg)(struct device *dev, struct scatterlist *sg,
+		      int nents, enum dma_data_direction dir,
+		      struct dma_attrs *attrs);
+	void (*unmap_sg)(struct device *dev,
+			 struct scatterlist *sg, int nents,
+			 enum dma_data_direction dir,
+			 struct dma_attrs *attrs);
+	void (*sync_single_for_cpu)(struct device *dev,
+				    dma_addr_t dma_handle, size_t size,
+				    enum dma_data_direction dir);
+	void (*sync_single_for_device)(struct device *dev,
+				       dma_addr_t dma_handle, size_t size,
+				       enum dma_data_direction dir);
+	void (*sync_single_range_for_cpu)(struct device *dev,
+					  dma_addr_t dma_handle,
+					  unsigned long offset,
+					  size_t size,
+					  enum dma_data_direction dir);
+	void (*sync_single_range_for_device)(struct device *dev,
+					     dma_addr_t dma_handle,
+					     unsigned long offset,
+					     size_t size,
+					     enum dma_data_direction dir);
+	void (*sync_sg_for_cpu)(struct device *dev,
+				struct scatterlist *sg, int nents,
+				enum dma_data_direction dir);
+	void (*sync_sg_for_device)(struct device *dev,
+				   struct scatterlist *sg, int nents,
+				   enum dma_data_direction dir);
+	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
+	int (*dma_supported)(struct device *dev, u64 mask);
+	int is_phys;
+};
+
 #define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
 /*

commit 07a2c01a0c2a0cb4581a67d50d4f17cb4d2457c4
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Sep 19 02:02:05 2008 +0900

    convert swiotlb to use dma_get_mask
    
    swiotlb can use dma_get_mask() instead of the homegrown function.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: tony.luck@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0dba7433af18..ba9114ec5d3a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -65,7 +65,7 @@ static inline int is_buffer_dma_capable(u64 mask, dma_addr_t addr, size_t size)
 
 static inline u64 dma_get_mask(struct device *dev)
 {
-	if (dev->dma_mask && *dev->dma_mask)
+	if (dev && dev->dma_mask && *dev->dma_mask)
 		return *dev->dma_mask;
 	return DMA_32BIT_MASK;
 }

commit 589fc9a6e2102b498978f6350581ec7fa5aeb032
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Sep 12 19:42:34 2008 +0900

    iommu: add dma_get_mask helper function
    
    Several IOMMUs do the same thing to get the dma_mask of a device. This
    adds a helper function to do the same thing to sweep them.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 6ed50c1642f1..0dba7433af18 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -63,6 +63,13 @@ static inline int is_buffer_dma_capable(u64 mask, dma_addr_t addr, size_t size)
 #define dma_sync_single		dma_sync_single_for_cpu
 #define dma_sync_sg		dma_sync_sg_for_cpu
 
+static inline u64 dma_get_mask(struct device *dev)
+{
+	if (dev->dma_mask && *dev->dma_mask)
+		return *dev->dma_mask;
+	return DMA_32BIT_MASK;
+}
+
 extern u64 dma_get_required_mask(struct device *dev);
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)

commit 636dc67cbf8c481a996faf6c23f0532d0f02ebad
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Sep 10 01:06:46 2008 +0900

    add is_buffer_dma_capable helper function
    
    is_buffer_dma_capable helper function is to see if a memory region is
    DMA-capable or not. The arugments are the dma_mask (or
    coherent_dma_mask) of a device and the address and size of a memory
    region.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Joerg Roedel <joerg.roedel@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 952e0f857ac9..6ed50c1642f1 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -48,6 +48,11 @@ static inline int is_device_dma_capable(struct device *dev)
 	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
 }
 
+static inline int is_buffer_dma_capable(u64 mask, dma_addr_t addr, size_t size)
+{
+	return addr + size <= mask;
+}
+
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 #else

commit 74bc7ceebfa1c84ddd3a843ebfb56df013bf7ef5
Author: Arthur Kepner <akepner@sgi.com>
Date:   Tue Apr 29 01:00:30 2008 -0700

    dma: add dma_*map*_attrs() interfaces
    
    Introduce new interfaces, dma_*map*_attrs(), for passing architecture-specific
    attributes when memory is mapped and unmapped for DMA.  Give the interfaces
    default implementations which ignore attributes.  Also introduce the
    dma_{set|get}_attr() interfaces for setting and retrieving individual
    attributes.  Define one attribute, DMA_ATTR_WRITE_BARRIER, in anticipation of
    its use by ia64/sn.  Select whether architectures implement arch-specific
    versions of the dma_*map*_attrs() interfaces via HAVE_DMA_ATTRS in Kconfig.
    
    [markn@au1.ibm.com: dma_{set,get}_attr() have to be static inline]
    Signed-off-by: Arthur Kepner <akepner@sgi.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Grundler <grundler@parisc-linux.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Mark Nelson <markn@au1.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 332030709623..952e0f857ac9 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -146,4 +146,21 @@ static inline void dmam_release_declared_memory(struct device *dev)
 }
 #endif /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
 
+#ifndef CONFIG_HAVE_DMA_ATTRS
+struct dma_attrs;
+
+#define dma_map_single_attrs(dev, cpu_addr, size, dir, attrs) \
+	dma_map_single(dev, cpu_addr, size, dir)
+
+#define dma_unmap_single_attrs(dev, dma_addr, size, dir, attrs) \
+	dma_unmap_single(dev, dma_addr, size, dir)
+
+#define dma_map_sg_attrs(dev, sgl, nents, dir, attrs) \
+	dma_map_sg(dev, sgl, nents, dir)
+
+#define dma_unmap_sg_attrs(dev, sgl, nents, dir, attrs) \
+	dma_unmap_sg(dev, sgl, nents, dir)
+
+#endif /* CONFIG_HAVE_DMA_ATTRS */
+
 #endif

commit d22a6966b8029913fac37d078ab2403898d94c63
Author: FUJITA Tomonori <tomof@acm.org>
Date:   Mon Feb 4 22:28:13 2008 -0800

    iommu sg merging: add accessors for segment_boundary_mask in device_dma_parameters()
    
    This adds new accessors for segment_boundary_mask in device_dma_parameters
    structure in the same way I did for max_segment_size.  So we can easily change
    where to place struct device_dma_parameters in the future.
    
    dma_get_segment boundary returns 0xffffffff if dma_parms in struct device
    isn't set up properly.  0xffffffff is the default value used in the block
    layer and the scsi mid layer.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Greg KH <greg@kroah.com>
    Cc: Jeff Garzik <jeff@garzik.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index df3a3610caf0..332030709623 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -75,6 +75,21 @@ static inline unsigned int dma_set_max_seg_size(struct device *dev,
 		return -EIO;
 }
 
+static inline unsigned long dma_get_seg_boundary(struct device *dev)
+{
+	return dev->dma_parms ?
+		dev->dma_parms->segment_boundary_mask : 0xffffffff;
+}
+
+static inline int dma_set_seg_boundary(struct device *dev, unsigned long mask)
+{
+	if (dev->dma_parms) {
+		dev->dma_parms->segment_boundary_mask = mask;
+		return 0;
+	} else
+		return -EIO;
+}
+
 /* flags for the coherent memory api */
 #define	DMA_MEMORY_MAP			0x01
 #define DMA_MEMORY_IO			0x02

commit 6b7b651055221127304a4e373ee9b762398d54d7
Author: FUJITA Tomonori <tomof@acm.org>
Date:   Mon Feb 4 22:27:55 2008 -0800

    iommu sg merging: add device_dma_parameters structure
    
    IOMMUs merges scatter/gather segments without considering a low level
    driver's restrictions. The problem is that IOMMUs can't access to the
    limitations because they are in request_queue.
    
    This patchset introduces a new structure, device_dma_parameters,
    including dma information. A pointer to device_dma_parameters is added
    to struct device. The bus specific structures (like pci_dev) includes
    device_dma_parameters. Low level drivers can use dma_set_max_seg_size
    to tell IOMMUs about the restrictions.
    
    We can move more dma stuff in struct device (like dma_mask) to struct
    device_dma_parameters later (needs some cleanups before that).
    
    This includes patches for all the IOMMUs that could merge sg (x86_64,
    ppc, IA64, alpha, sparc64, and parisc) though only the ppc patch was
    tested. The patches for other IOMMUs are only compile tested.
    
    This patch:
    
    Add a new structure, device_dma_parameters, including dma information.  A
    pointer to device_dma_parameters is added to struct device.
    
    - there are only max_segment_size and segment_boundary_mask there but we'll
      move more dma stuff in struct device (like dma_mask) to struct
      device_dma_parameters later.  segment_boundary_mask is not supported yet.
    
    - new accessors for the dma parameters are added.  So we can easily change
      where to place struct device_dma_parameters in the future.
    
    - dma_get_max_seg_size returns 64K if dma_parms in struct device isn't set
      up properly.  64K is the default max_segment_size in the block layer.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jeff Garzik <jeff@garzik.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 4470950892be..df3a3610caf0 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -60,6 +60,21 @@ static inline int is_device_dma_capable(struct device *dev)
 
 extern u64 dma_get_required_mask(struct device *dev);
 
+static inline unsigned int dma_get_max_seg_size(struct device *dev)
+{
+	return dev->dma_parms ? dev->dma_parms->max_segment_size : 65536;
+}
+
+static inline unsigned int dma_set_max_seg_size(struct device *dev,
+						unsigned int size)
+{
+	if (dev->dma_parms) {
+		dev->dma_parms->max_segment_size = size;
+		return 0;
+	} else
+		return -EIO;
+}
+
 /* flags for the coherent memory api */
 #define	DMA_MEMORY_MAP			0x01
 #define DMA_MEMORY_IO			0x02

commit 96532babc3e2ec4e0fce891d64319f183f043855
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Sun Feb 3 15:06:26 2008 +0200

    linux/dma-mapping.h: rename macro to prevent multiple inclusion
    
    Having the macro to prevent multiple inclusion of
    include/linux/dma-mapping.h contain the prefix "_ASM" is just begging
    for possible confusion some day.
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 101a2d4636be..4470950892be 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -1,5 +1,5 @@
-#ifndef _ASM_LINUX_DMA_MAPPING_H
-#define _ASM_LINUX_DMA_MAPPING_H
+#ifndef _LINUX_DMA_MAPPING_H
+#define _LINUX_DMA_MAPPING_H
 
 #include <linux/device.h>
 #include <linux/err.h>

commit 8f286c33f1e838d631f4a3260b33efce4bc5973c
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Oct 18 03:05:07 2007 -0700

    stop using DMA_xxBIT_MASK
    
    Now that we have DMA_BIT_MASK(), these macros are pointless.
    
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 29b028597dcd..101a2d4636be 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -13,9 +13,15 @@ enum dma_data_direction {
 	DMA_NONE = 3,
 };
 
-#define DMA_BIT_MASK(n)	((1ULL<<(n))-1)
+#define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
 
-#define DMA_64BIT_MASK	(~0ULL)
+/*
+ * NOTE: do not use the below macros in new code and do not add new definitions
+ * here.
+ *
+ * Instead, just open-code DMA_BIT_MASK(n) within your driver
+ */
+#define DMA_64BIT_MASK	DMA_BIT_MASK(64)
 #define DMA_48BIT_MASK	DMA_BIT_MASK(48)
 #define DMA_47BIT_MASK	DMA_BIT_MASK(47)
 #define DMA_40BIT_MASK	DMA_BIT_MASK(40)

commit 34c6538413e5648b63cb785add509f500b3a7b11
Author: Borislav Petkov <bbpetkov@yahoo.de>
Date:   Thu Oct 18 03:05:06 2007 -0700

    unify DMA_..BIT_MASK definitions: v3.1
    
    Remove redundant DMA_..BIT_MASK definitions across two drivers.  The
    computation of the majority of the bitmasks is done by the compiler.  The
    initial split of the patch touching each a different file got removed due
    to possible git bisect breakage.
    
    Signed-off-by: Borislav Petkov <bbpetkov@yahoo.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Muli Ben-Yehuda <muli@il.ibm.com>
    Cc: Jeff Garzik <jeff@garzik.org>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Reviewed-by: Satyam Sharma <satyam@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 0ebfafbd338c..29b028597dcd 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -13,16 +13,20 @@ enum dma_data_direction {
 	DMA_NONE = 3,
 };
 
-#define DMA_64BIT_MASK	0xffffffffffffffffULL
-#define DMA_48BIT_MASK	0x0000ffffffffffffULL
-#define DMA_40BIT_MASK	0x000000ffffffffffULL
-#define DMA_39BIT_MASK	0x0000007fffffffffULL
-#define DMA_32BIT_MASK	0x00000000ffffffffULL
-#define DMA_31BIT_MASK	0x000000007fffffffULL
-#define DMA_30BIT_MASK	0x000000003fffffffULL
-#define DMA_29BIT_MASK	0x000000001fffffffULL
-#define DMA_28BIT_MASK	0x000000000fffffffULL
-#define DMA_24BIT_MASK	0x0000000000ffffffULL
+#define DMA_BIT_MASK(n)	((1ULL<<(n))-1)
+
+#define DMA_64BIT_MASK	(~0ULL)
+#define DMA_48BIT_MASK	DMA_BIT_MASK(48)
+#define DMA_47BIT_MASK	DMA_BIT_MASK(47)
+#define DMA_40BIT_MASK	DMA_BIT_MASK(40)
+#define DMA_39BIT_MASK	DMA_BIT_MASK(39)
+#define DMA_35BIT_MASK	DMA_BIT_MASK(35)
+#define DMA_32BIT_MASK	DMA_BIT_MASK(32)
+#define DMA_31BIT_MASK	DMA_BIT_MASK(31)
+#define DMA_30BIT_MASK	DMA_BIT_MASK(30)
+#define DMA_29BIT_MASK	DMA_BIT_MASK(29)
+#define DMA_28BIT_MASK	DMA_BIT_MASK(28)
+#define DMA_24BIT_MASK	DMA_BIT_MASK(24)
 
 #define DMA_MASK_NONE	0x0ULL
 

commit 32e8f70230c0c417490787b3f48b6ed6c48e7ec9
Author: James Bottomley <James.Bottomley@SteelEye.com>
Date:   Tue Oct 16 01:23:55 2007 -0700

    introduce DMA_MASK_NONE as a signal for unable to do DMA
    
    Some devices are incapable of DMA and need to be recognised as such.
    Introduce a NONE dma mask to facilitate this plus an inline function:
    is_device_dma_capable() to check this.
    
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Natalie Protasevich <protasnb@gmail.com>
    Cc: Jeff Garzik <jgarzik@pobox.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2dc21cbeb304..0ebfafbd338c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -24,6 +24,8 @@ enum dma_data_direction {
 #define DMA_28BIT_MASK	0x000000000fffffffULL
 #define DMA_24BIT_MASK	0x0000000000ffffffULL
 
+#define DMA_MASK_NONE	0x0ULL
+
 static inline int valid_dma_direction(int dma_direction)
 {
 	return ((dma_direction == DMA_BIDIRECTIONAL) ||
@@ -31,6 +33,11 @@ static inline int valid_dma_direction(int dma_direction)
 		(dma_direction == DMA_FROM_DEVICE));
 }
 
+static inline int is_device_dma_capable(struct device *dev)
+{
+	return dev->dma_mask != NULL && *dev->dma_mask != DMA_MASK_NONE;
+}
+
 #ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
 #else

commit 1b0fac45878bb88759eec347c273285195649ff7
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Jul 15 23:40:26 2007 -0700

    dma-mapping: prevent dma dependent code from linking on !HAS_DMA archs
    
    Continuing the work started in 411f0f3edc141a582190d3605cadd1d993abb6df ...
    
    This enables code with a dma path, that compiles away, to build without
    requiring additional code factoring.  It also prevents code that calls
    dma_alloc_coherent and dma_free_coherent from linking whereas previously
    the code would hit a BUG() at run time.  Finally, it allows archs that set
    !HAS_DMA to delete their asm/dma-mapping.h file.
    
    Cc: Cornelia Huck <cornelia.huck@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: John W. Linville <linville@tuxdriver.com>
    Cc: Kyle McMartin <kyle@parisc-linux.org>
    Cc: James Bottomley <James.Bottomley@SteelEye.com>
    Cc: Tejun Heo <htejun@gmail.com>
    Cc: Jeff Garzik <jeff@garzik.org>
    Cc: <geert@linux-m68k.org>
    Cc: <zippel@linux-m68k.org>
    Cc: <spyro@f2s.com>
    Cc: <ysato@users.sourceforge.jp>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 9a663c6db16a..2dc21cbeb304 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -31,7 +31,11 @@ static inline int valid_dma_direction(int dma_direction)
 		(dma_direction == DMA_FROM_DEVICE));
 }
 
+#ifdef CONFIG_HAS_DMA
 #include <asm/dma-mapping.h>
+#else
+#include <asm-generic/dma-mapping-broken.h>
+#endif
 
 /* Backwards compat, remove in 2.7.x */
 #define dma_sync_single		dma_sync_single_for_cpu

commit 9ac7849e35f705830f7b016ff272b0ff1f7ff759
Author: Tejun Heo <htejun@gmail.com>
Date:   Sat Jan 20 16:00:26 2007 +0900

    devres: device resource management
    
    Implement device resource management, in short, devres.  A device
    driver can allocate arbirary size of devres data which is associated
    with a release function.  On driver detach, release function is
    invoked on the devres data, then, devres data is freed.
    
    devreses are typed by associated release functions.  Some devreses are
    better represented by single instance of the type while others need
    multiple instances sharing the same release function.  Both usages are
    supported.
    
    devreses can be grouped using devres group such that a device driver
    can easily release acquired resources halfway through initialization
    or selectively release resources (e.g. resources for port 1 out of 4
    ports).
    
    This patch adds devres core including documentation and the following
    managed interfaces.
    
    * alloc/free    : devm_kzalloc(), devm_kzfree()
    * IO region     : devm_request_region(), devm_release_region()
    * IRQ           : devm_request_irq(), devm_free_irq()
    * DMA           : dmam_alloc_coherent(), dmam_free_coherent(),
                      dmam_declare_coherent_memory(), dmam_pool_create(),
                      dmam_pool_destroy()
    * PCI           : pcim_enable_device(), pcim_pin_device(), pci_is_managed()
    * iomap         : devm_ioport_map(), devm_ioport_unmap(), devm_ioremap(),
                      devm_ioremap_nocache(), devm_iounmap(), pcim_iomap_table(),
                      pcim_iomap(), pcim_iounmap()
    
    Signed-off-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Jeff Garzik <jeff@garzik.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ff203c465fed..9a663c6db16a 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -66,6 +66,33 @@ dma_mark_declared_memory_occupied(struct device *dev,
 }
 #endif
 
-#endif
+/*
+ * Managed DMA API
+ */
+extern void *dmam_alloc_coherent(struct device *dev, size_t size,
+				 dma_addr_t *dma_handle, gfp_t gfp);
+extern void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,
+			       dma_addr_t dma_handle);
+extern void *dmam_alloc_noncoherent(struct device *dev, size_t size,
+				    dma_addr_t *dma_handle, gfp_t gfp);
+extern void dmam_free_noncoherent(struct device *dev, size_t size, void *vaddr,
+				  dma_addr_t dma_handle);
+#ifdef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
+extern int dmam_declare_coherent_memory(struct device *dev, dma_addr_t bus_addr,
+					dma_addr_t device_addr, size_t size,
+					int flags);
+extern void dmam_release_declared_memory(struct device *dev);
+#else /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
+static inline int dmam_declare_coherent_memory(struct device *dev,
+				dma_addr_t bus_addr, dma_addr_t device_addr,
+				size_t size, gfp_t gfp)
+{
+	return 0;
+}
 
+static inline void dmam_release_declared_memory(struct device *dev)
+{
+}
+#endif /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
 
+#endif

commit d6bd3a39f7c6ebad49c261c3d458df974c880758
Author: Rolf Eike Beer <eike-kernel@sf-tec.de>
Date:   Fri Sep 29 01:59:48 2006 -0700

    [PATCH] Move valid_dma_direction() from x86_64 to generic code
    
    As suggested by Muli Ben-Yehuda this function is moved to generic code as
    may be useful for all archs.
    
    [akpm@osdl.org: fix]
    Signed-off-by: Rolf Eike Beer <eike-kernel@sf-tec.de>
    Cc: Muli Ben-Yehuda <muli@il.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 635690cf3e3d..ff203c465fed 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -24,6 +24,13 @@ enum dma_data_direction {
 #define DMA_28BIT_MASK	0x000000000fffffffULL
 #define DMA_24BIT_MASK	0x0000000000ffffffULL
 
+static inline int valid_dma_direction(int dma_direction)
+{
+	return ((dma_direction == DMA_BIDIRECTIONAL) ||
+		(dma_direction == DMA_TO_DEVICE) ||
+		(dma_direction == DMA_FROM_DEVICE));
+}
+
 #include <asm/dma-mapping.h>
 
 /* Backwards compat, remove in 2.7.x */

commit 4c1b46226ce4424a93b8ac544e37afb26c8a72c6
Author: Francois Romieu <romieu@fr.zoreil.com>
Date:   Wed May 10 12:48:57 2006 -0700

    dl2k: use DMA_48BIT_MASK constant
    
    Typo will be harder with this one.
    
    Signed-off-by: Francois Romieu <romieu@fr.zoreil.com>
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index ff61817082fa..635690cf3e3d 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -14,6 +14,7 @@ enum dma_data_direction {
 };
 
 #define DMA_64BIT_MASK	0xffffffffffffffffULL
+#define DMA_48BIT_MASK	0x0000ffffffffffffULL
 #define DMA_40BIT_MASK	0x000000ffffffffffULL
 #define DMA_39BIT_MASK	0x0000007fffffffffULL
 #define DMA_32BIT_MASK	0x00000000ffffffffULL

commit 56b146d36db933844011d5026c6f55593037c7b8
Author: Tobias Klauser <tklauser@nuerscht.ch>
Date:   Mon Apr 10 22:54:21 2006 -0700

    [PATCH] Last DMA_xBIT_MASK cleanups
    
    These are the last conversions of pci_set_dma_mask(),
    pci_set_consistent_dma_mask() and pci_dma_supported() to use DMA_xBIT_MASK
    constants from linux/dma-mapping.h
    
    Signed-off-by: Tobias Klauser <tklauser@nuerscht.ch>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 9b4751aecc23..ff61817082fa 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -21,7 +21,7 @@ enum dma_data_direction {
 #define DMA_30BIT_MASK	0x000000003fffffffULL
 #define DMA_29BIT_MASK	0x000000001fffffffULL
 #define DMA_28BIT_MASK	0x000000000fffffffULL
-#define DMA_24BIT_MASK 0x0000000000ffffffULL
+#define DMA_24BIT_MASK	0x0000000000ffffffULL
 
 #include <asm/dma-mapping.h>
 

commit 910638ae7ed4be27d6af55f6c9b5bf54b838e78b
Author: Matthias Gehre <M.Gehre@gmx.de>
Date:   Tue Mar 28 01:56:48 2006 -0800

    [PATCH] Replace 0xff.. with correct DMA_xBIT_MASK
    
    Replace all occurences of 0xff..  in calls to function pci_set_dma_mask()
    and pci_set_consistant_dma_mask() with the corresponding DMA_xBIT_MASK from
    linux/dma-mapping.h.
    
    Signed-off-by: Matthias Gehre <M.Gehre@gmx.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index a8731062a74c..9b4751aecc23 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -21,6 +21,7 @@ enum dma_data_direction {
 #define DMA_30BIT_MASK	0x000000003fffffffULL
 #define DMA_29BIT_MASK	0x000000001fffffffULL
 #define DMA_28BIT_MASK	0x000000000fffffffULL
+#define DMA_24BIT_MASK 0x0000000000ffffffULL
 
 #include <asm/dma-mapping.h>
 

commit 9d2f928ddf64ca0361562e30faf584cd33055c60
Author: Tobias Klauser <tklauser@nuerscht.ch>
Date:   Wed Mar 22 10:53:19 2006 +0100

    [PATCH] Intruduce DMA_28BIT_MASK
    
    This patch introduces the DMA_28BIT_MASK constant in dma-mapping.h
    ALSA drivers using this mask are changed to use the new constant.
    
    Signed-off-by: Tobias Klauser <tklauser@nuerscht.ch>
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Jaroslav Kysela <perex@suse.cz>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 2d80cc761a15..a8731062a74c 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -20,6 +20,7 @@ enum dma_data_direction {
 #define DMA_31BIT_MASK	0x000000007fffffffULL
 #define DMA_30BIT_MASK	0x000000003fffffffULL
 #define DMA_29BIT_MASK	0x000000001fffffffULL
+#define DMA_28BIT_MASK	0x000000000fffffffULL
 
 #include <asm/dma-mapping.h>
 

commit b8112df71cae7d6a86158caeb19d215f56c4f9ab
Author: Lee Revell <rlrevell@joe-job.com>
Date:   Wed Jun 15 14:19:03 2005 -0400

    [SCSI] Add DMA mask constants other than 32 and 64 bit
    
    Signed-Off-By: Lee Revell <rlrevell@joe-job.com>
    Signed-off-by: James Bottomley <James.Bottomley@SteelEye.com>

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 806c305332c1..2d80cc761a15 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -14,7 +14,12 @@ enum dma_data_direction {
 };
 
 #define DMA_64BIT_MASK	0xffffffffffffffffULL
+#define DMA_40BIT_MASK	0x000000ffffffffffULL
+#define DMA_39BIT_MASK	0x0000007fffffffffULL
 #define DMA_32BIT_MASK	0x00000000ffffffffULL
+#define DMA_31BIT_MASK	0x000000007fffffffULL
+#define DMA_30BIT_MASK	0x000000003fffffffULL
+#define DMA_29BIT_MASK	0x000000001fffffffULL
 
 #include <asm/dma-mapping.h>
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
new file mode 100644
index 000000000000..806c305332c1
--- /dev/null
+++ b/include/linux/dma-mapping.h
@@ -0,0 +1,56 @@
+#ifndef _ASM_LINUX_DMA_MAPPING_H
+#define _ASM_LINUX_DMA_MAPPING_H
+
+#include <linux/device.h>
+#include <linux/err.h>
+
+/* These definitions mirror those in pci.h, so they can be used
+ * interchangeably with their PCI_ counterparts */
+enum dma_data_direction {
+	DMA_BIDIRECTIONAL = 0,
+	DMA_TO_DEVICE = 1,
+	DMA_FROM_DEVICE = 2,
+	DMA_NONE = 3,
+};
+
+#define DMA_64BIT_MASK	0xffffffffffffffffULL
+#define DMA_32BIT_MASK	0x00000000ffffffffULL
+
+#include <asm/dma-mapping.h>
+
+/* Backwards compat, remove in 2.7.x */
+#define dma_sync_single		dma_sync_single_for_cpu
+#define dma_sync_sg		dma_sync_sg_for_cpu
+
+extern u64 dma_get_required_mask(struct device *dev);
+
+/* flags for the coherent memory api */
+#define	DMA_MEMORY_MAP			0x01
+#define DMA_MEMORY_IO			0x02
+#define DMA_MEMORY_INCLUDES_CHILDREN	0x04
+#define DMA_MEMORY_EXCLUSIVE		0x08
+
+#ifndef ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY
+static inline int
+dma_declare_coherent_memory(struct device *dev, dma_addr_t bus_addr,
+			    dma_addr_t device_addr, size_t size, int flags)
+{
+	return 0;
+}
+
+static inline void
+dma_release_declared_memory(struct device *dev)
+{
+}
+
+static inline void *
+dma_mark_declared_memory_occupied(struct device *dev,
+				  dma_addr_t device_addr, size_t size)
+{
+	return ERR_PTR(-EBUSY);
+}
+#endif
+
+#endif
+
+
