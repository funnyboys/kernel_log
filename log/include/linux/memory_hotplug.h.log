commit 09102704c67457c6cdea6c0394c34843484a852c
Merge: 84fc461db99b 044e4b092230
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 13:42:09 2020 -0700

    Merge tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost
    
    Pull virtio updates from Michael Tsirkin:
    
     - virtio-mem: paravirtualized memory hotplug
    
     - support doorbell mapping for vdpa
    
     - config interrupt support in ifc
    
     - fixes all over the place
    
    * tag 'for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mst/vhost: (40 commits)
      vhost/test: fix up after API change
      virtio_mem: convert device block size into 64bit
      virtio-mem: drop unnecessary initialization
      ifcvf: implement config interrupt in IFCVF
      vhost: replace -1 with VHOST_FILE_UNBIND in ioctls
      vhost_vdpa: Support config interrupt in vdpa
      ifcvf: ignore continuous setting same status value
      virtio-mem: Don't rely on implicit compiler padding for requests
      virtio-mem: Try to unplug the complete online memory block first
      virtio-mem: Use -ETXTBSY as error code if the device is busy
      virtio-mem: Unplug subblocks right-to-left
      virtio-mem: Drop manual check for already present memory
      virtio-mem: Add parent resource for all added "System RAM"
      virtio-mem: Better retry handling
      virtio-mem: Offline and remove completely unplugged memory blocks
      mm/memory_hotplug: Introduce offline_and_remove_memory()
      virtio-mem: Allow to offline partially unplugged memory blocks
      mm: Allow to offline unmovable PageOffline() pages via MEM_GOING_OFFLINE
      virtio-mem: Paravirtualized memory hotunplug part 2
      virtio-mem: Paravirtualized memory hotunplug part 1
      ...

commit 7b7b27214bba1966772f9213cd2d8e5d67f8487f
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jun 4 16:48:41 2020 -0700

    mm/memory_hotplug: introduce add_memory_driver_managed()
    
    Patch series "mm/memory_hotplug: Interface to add driver-managed system
    ram", v4.
    
    kexec (via kexec_load()) can currently not properly handle memory added
    via dax/kmem, and will have similar issues with virtio-mem.  kexec-tools
    will currently add all memory to the fixed-up initial firmware memmap.  In
    case of dax/kmem, this means that - in contrast to a proper reboot - how
    that persistent memory will be used can no longer be configured by the
    kexec'd kernel.  In case of virtio-mem it will be harmful, because that
    memory might contain inaccessible pieces that require coordination with
    hypervisor first.
    
    In both cases, we want to let the driver in the kexec'd kernel handle
    detecting and adding the memory, like during an ordinary reboot.
    Introduce add_memory_driver_managed().  More on the samentics are in patch
    #1.
    
    In the future, we might want to make this behavior configurable for
    dax/kmem- either by configuring it in the kernel (which would then also
    allow to configure kexec_file_load()) or in kexec-tools by also adding
    "System RAM (kmem)" memory from /proc/iomem to the fixed-up initial
    firmware memmap.
    
    More on the motivation can be found in [1] and [2].
    
    [1] https://lkml.kernel.org/r/20200429160803.109056-1-david@redhat.com
    [2] https://lkml.kernel.org/r/20200430102908.10107-1-david@redhat.com
    
    This patch (of 3):
    
    Some device drivers rely on memory they managed to not get added to the
    initial (firmware) memmap as system RAM - so it's not used as initial
    system RAM by the kernel and the driver is under control.  While this is
    the case during cold boot and after a reboot, kexec is not aware of that
    and might add such memory to the initial (firmware) memmap of the kexec
    kernel.  We need ways to teach kernel and userspace that this system ram
    is different.
    
    For example, dax/kmem allows to decide at runtime if persistent memory is
    to be used as system ram.  Another future user is virtio-mem, which has to
    coordinate with its hypervisor to deal with inaccessible parts within
    memory resources.
    
    We want to let users in the kernel (esp. kexec) but also user space
    (esp. kexec-tools) know that this memory has different semantics and
    needs to be handled differently:
    1. Don't create entries in /sys/firmware/memmap/
    2. Name the memory resource "System RAM ($DRIVER)" (exposed via
       /proc/iomem) ($DRIVER might be "kmem", "virtio_mem").
    3. Flag the memory resource IORESOURCE_MEM_DRIVER_MANAGED
    
    /sys/firmware/memmap/ [1] represents the "raw firmware-provided memory
    map" because "on most architectures that firmware-provided memory map is
    modified afterwards by the kernel itself".  The primary user is kexec on
    x86-64.  Since commit d96ae5309165 ("memory-hotplug: create
    /sys/firmware/memmap entry for new memory"), we add all hotplugged memory
    to that firmware memmap - which makes perfect sense for traditional memory
    hotplug on x86-64, where real HW will also add hotplugged DIMMs to the
    firmware memmap.  We replicate what the "raw firmware-provided memory map"
    looks like after hot(un)plug.
    
    To keep things simple, let the user provide the full resource name instead
    of only the driver name - this way, we don't have to manually
    allocate/craft strings for memory resources.  Also use the resource name
    to make decisions, to avoid passing additional flags.  In case the name
    isn't "System RAM", it's special.
    
    We don't have to worry about firmware_map_remove() on the removal path.
    If there is no entry, it will simply return with -EINVAL.
    
    We'll adapt dax/kmem in a follow-up patch.
    
    [1] https://www.kernel.org/doc/Documentation/ABI/testing/sysfs-firmware-memmap
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Link: http://lkml.kernel.org/r/20200508084217.9160-1-david@redhat.com
    Link: http://lkml.kernel.org/r/20200508084217.9160-3-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 7dca9cd6076b..fee7fab5d706 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -342,6 +342,8 @@ extern void __ref free_area_init_core_hotplug(int nid);
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource);
+extern int add_memory_driver_managed(int nid, u64 start, u64 size,
+				     const char *resource_name);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern void remove_pfn_range_from_zone(struct zone *zone,

commit 04f3465c98665b7c5a3484d7194f1858954069f5
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jun 4 16:48:31 2020 -0700

    mm/memory_hotplug: remove is_mem_section_removable()
    
    Fortunately, all users of is_mem_section_removable() are gone.  Get rid of
    it, including some now unnecessary functions.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Oscar Salvador <osalvador@suse.de>
    Link: http://lkml.kernel.org/r/20200407135416.24093-3-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 93d9ada74ddd..7dca9cd6076b 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -314,19 +314,12 @@ static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
-extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int remove_memory(int nid, u64 start, u64 size);
 extern void __remove_memory(int nid, u64 start, u64 size);
 
 #else
-static inline bool is_mem_section_removable(unsigned long pfn,
-					unsigned long nr_pages)
-{
-	return false;
-}
-
 static inline void try_offline_node(int nid) {}
 
 static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)

commit 08b3acd7a68fc17902e1cb6b146389322840deab
Author: David Hildenbrand <david@redhat.com>
Date:   Thu May 7 16:01:32 2020 +0200

    mm/memory_hotplug: Introduce offline_and_remove_memory()
    
    virtio-mem wants to offline and remove a memory block once it unplugged
    all subblocks (e.g., using alloc_contig_range()). Let's provide
    an interface to do that from a driver. virtio-mem already supports to
    offline partially unplugged memory blocks. Offlining a fully unplugged
    memory block will not require to migrate any pages. All unplugged
    subblocks are PageOffline() and have a reference count of 0 - so
    offlining code will simply skip them.
    
    All we need is an interface to offline and remove the memory from kernel
    module context, where we don't have access to the memory block devices
    (esp. find_memory_block() and device_offline()) and the device hotplug
    lock.
    
    To keep things simple, allow to only work on a single memory block.
    
    Acked-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Link: https://lore.kernel.org/r/20200507140139.17083-9-david@redhat.com
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 93d9ada74ddd..cb7499843f5c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -319,6 +319,7 @@ extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int remove_memory(int nid, u64 start, u64 size);
 extern void __remove_memory(int nid, u64 start, u64 size);
+extern int offline_and_remove_memory(int nid, u64 start, u64 size);
 
 #else
 static inline bool is_mem_section_removable(unsigned long pfn,

commit bfeb022f8fe4c5afdcfd7a3d868fac9765f9bcad
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:36 2020 -0700

    mm/memory_hotplug: add pgprot_t to mhp_params
    
    devm_memremap_pages() is currently used by the PCI P2PDMA code to create
    struct page mappings for IO memory.  At present, these mappings are
    created with PAGE_KERNEL which implies setting the PAT bits to be WB.
    However, on x86, an mtrr register will typically override this and force
    the cache type to be UC-.  In the case firmware doesn't set this
    register it is effectively WB and will typically result in a machine
    check exception when it's accessed.
    
    Other arches are not currently likely to function correctly seeing they
    don't have any MTRR registers to fall back on.
    
    To solve this, provide a way to specify the pgprot value explicitly to
    arch_add_memory().
    
    Of the arches that support MEMORY_HOTPLUG: x86_64, and arm64 need a
    simple change to pass the pgprot_t down to their respective functions
    which set up the page tables.  For x86_32, set the page tables
    explicitly using _set_memory_prot() (seeing they are already mapped).
    
    For ia64, s390 and sh, reject anything but PAGE_KERNEL settings -- this
    should be fine, for now, seeing these architectures don't support
    ZONE_DEVICE.
    
    A check in __add_pages() is also added to ensure the pgprot parameter
    was set for all arches.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-7-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 75f0f6304735..93d9ada74ddd 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -60,9 +60,12 @@ enum {
 /*
  * Extended parameters for memory hotplug:
  * altmap: alternative allocator for memmap array (optional)
+ * pgprot: page protection flags to apply to newly created page tables
+ *	(required)
  */
 struct mhp_params {
 	struct vmem_altmap *altmap;
+	pgprot_t pgprot;
 };
 
 /*

commit f5637d3b42ab0465ef71d5fb8461bce97fba95e8
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:21 2020 -0700

    mm/memory_hotplug: rename mhp_restrictions to mhp_params
    
    The mhp_restrictions struct really doesn't specify anything resembling a
    restriction anymore so rename it to be mhp_params as it is a list of
    extended parameters.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-3-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 7c1bcff11672..75f0f6304735 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -58,10 +58,10 @@ enum {
 };
 
 /*
- * Restrictions for the memory hotplug:
- * altmap: alternative allocator for memmap array
+ * Extended parameters for memory hotplug:
+ * altmap: alternative allocator for memmap array (optional)
  */
-struct mhp_restrictions {
+struct mhp_params {
 	struct vmem_altmap *altmap;
 };
 
@@ -112,7 +112,7 @@ extern int restore_online_page_callback(online_page_callback_t callback);
 extern int try_online_node(int nid);
 
 extern int arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions);
+			   struct mhp_params *params);
 extern u64 max_mem_size;
 
 extern int memhp_online_type_from_str(const char *str);
@@ -133,17 +133,17 @@ extern void __remove_pages(unsigned long start_pfn, unsigned long nr_pages,
 
 /* reasonably generic interface to expand the physical pages */
 extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-		       struct mhp_restrictions *restrictions);
+		       struct mhp_params *params);
 
 #ifndef CONFIG_ARCH_HAS_ADD_PAGES
 static inline int add_pages(int nid, unsigned long start_pfn,
-		unsigned long nr_pages, struct mhp_restrictions *restrictions)
+		unsigned long nr_pages, struct mhp_params *params)
 {
-	return __add_pages(nid, start_pfn, nr_pages, restrictions);
+	return __add_pages(nid, start_pfn, nr_pages, params);
 }
 #else /* ARCH_HAS_ADD_PAGES */
 int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-	      struct mhp_restrictions *restrictions);
+	      struct mhp_params *params);
 #endif /* ARCH_HAS_ADD_PAGES */
 
 #ifdef CONFIG_NUMA

commit 96c6b598135e7cec66161e8943823470c7c8954e
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:17 2020 -0700

    mm/memory_hotplug: drop the flags field from struct mhp_restrictions
    
    Patch series "Allow setting caching mode in arch_add_memory() for
    P2PDMA", v4.
    
    Currently, the page tables created using memremap_pages() are always
    created with the PAGE_KERNEL cacheing mode.  However, the P2PDMA code is
    creating pages for PCI BAR memory which should never be accessed through
    the cache and instead use either WC or UC.  This still works in most
    cases, on x86, because the MTRR registers typically override the caching
    settings in the page tables for all of the IO memory to be UC-.
    However, this tends not to work so well on other arches or some rare x86
    machines that have firmware which does not setup the MTRR registers in
    this way.
    
    Instead of this, this series proposes a change to arch_add_memory() to
    take the pgprot required by the mapping which allows us to explicitly
    set pagetable entries for P2PDMA memory to UC.
    
    This changes is pretty routine for most of the arches: x86_64, arm64 and
    powerpc simply need to thread the pgprot through to where the page
    tables are setup.  x86_32 unfortunately sets up the page tables at boot
    so must use _set_memory_prot() to change their caching mode.  ia64, s390
    and sh don't appear to have an easy way to change the page tables so,
    for now at least, we just return -EINVAL on such mappings and thus they
    will not support P2PDMA memory until the work for this is done.  This
    should be fine as they don't yet support ZONE_DEVICE.
    
    This patch (of 7):
    
    This variable is not used anywhere and should therefore be removed from
    the structure.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-2-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ef55115320fb..7c1bcff11672 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -59,11 +59,9 @@ enum {
 
 /*
  * Restrictions for the memory hotplug:
- * flags:  MHP_ flags
  * altmap: alternative allocator for memmap array
  */
 struct mhp_restrictions {
-	unsigned long flags;
 	struct vmem_altmap *altmap;
 };
 

commit 5f47adf762b78cae97de58d9ff01d2d44db09467
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Apr 6 20:07:44 2020 -0700

    mm/memory_hotplug: allow to specify a default online_type
    
    For now, distributions implement advanced udev rules to essentially
    - Don't online any hotplugged memory (s390x)
    - Online all memory to ZONE_NORMAL (e.g., most virt environments like
      hyperv)
    - Online all memory to ZONE_MOVABLE in case the zone imbalance is taken
      care of (e.g., bare metal, special virt environments)
    
    In summary: All memory is usually onlined the same way, however, the
    kernel always has to ask user space to come up with the same answer.
    E.g., Hyper-V always waits for a memory block to get onlined before
    continuing, otherwise it might end up adding memory faster than
    onlining it, which can result in strange OOM situations.  This waiting
    slows down adding of a bigger amount of memory.
    
    Let's allow to specify a default online_type, not just "online" and
    "offline".  This allows distributions to configure the default online_type
    when booting up and be done with it.
    
    We can now specify "offline", "online", "online_movable" and
    "online_kernel" via
    - "memhp_default_state=" on the kernel cmdline
    - /sys/devices/system/memory/auto_online_blocks
    just like we are able to specify for a single memory block via
    /sys/devices/system/memory/memoryX/state
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Yumei Huang <yuhuang@redhat.com>
    Link: http://lkml.kernel.org/r/20200317104942.11178-9-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index c6e090b34c4b..ef55115320fb 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -117,6 +117,8 @@ extern int arch_add_memory(int nid, u64 start, u64 size,
 			struct mhp_restrictions *restrictions);
 extern u64 max_mem_size;
 
+extern int memhp_online_type_from_str(const char *str);
+
 /* Default online_type (MMOP_*) when new memory blocks are added. */
 extern int memhp_default_online_type;
 /* If movable_node boot option specified */

commit 862919e568356cc36288a11b42cd88ec3a7100e9
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Apr 6 20:07:40 2020 -0700

    mm/memory_hotplug: convert memhp_auto_online to store an online_type
    
    ...  and rename it to memhp_default_online_type.  This is a preparation
    for more detailed default online behavior.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Yumei Huang <yuhuang@redhat.com>
    Link: http://lkml.kernel.org/r/20200317104942.11178-8-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index c2e06ed5e0e9..c6e090b34c4b 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -117,7 +117,8 @@ extern int arch_add_memory(int nid, u64 start, u64 size,
 			struct mhp_restrictions *restrictions);
 extern u64 max_mem_size;
 
-extern bool memhp_auto_online;
+/* Default online_type (MMOP_*) when new memory blocks are added. */
+extern int memhp_default_online_type;
 /* If movable_node boot option specified */
 extern bool movable_node_enabled;
 static inline bool movable_node_is_enabled(void)

commit efc978ad0e05ed6401c7854811750bf55b67f4b9
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Apr 6 20:07:20 2020 -0700

    drivers/base/memory: map MMOP_OFFLINE to 0
    
    Historically, we used the value -1.  Just treat 0 as the special case now.
    Clarify a comment (which was wrong, when we come via device_online() the
    first time, the online_type would have been 0 / MEM_ONLINE).  The default
    is now always MMOP_OFFLINE.  This removes the last user of the manual
    "-1", which didn't use the enum value.
    
    This is a preparation to use the online_type as an array index.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Yumei Huang <yuhuang@redhat.com>
    Link: http://lkml.kernel.org/r/20200317104942.11178-3-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 261dbf010d5d..c2e06ed5e0e9 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -48,7 +48,7 @@ enum {
 /* Types for control the zone type of onlined and offlined memory */
 enum {
 	/* Offline the memory. */
-	MMOP_OFFLINE = -1,
+	MMOP_OFFLINE = 0,
 	/* Online the memory. Zone depends, see default_zone_for_pfn(). */
 	MMOP_ONLINE,
 	/* Online the memory to ZONE_NORMAL. */

commit 956f8b445061667c3545baa24778f890d1d522f4
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Apr 6 20:07:16 2020 -0700

    drivers/base/memory: rename MMOP_ONLINE_KEEP to MMOP_ONLINE
    
    Patch series "mm/memory_hotplug: allow to specify a default online_type", v3.
    
    Distributions nowadays use udev rules ([1] [2]) to specify if and how to
    online hotplugged memory.  The rules seem to get more complex with many
    special cases.  Due to the various special cases,
    CONFIG_MEMORY_HOTPLUG_DEFAULT_ONLINE cannot be used.  All memory hotplug
    is handled via udev rules.
    
    Every time we hotplug memory, the udev rule will come to the same
    conclusion.  Especially Hyper-V (but also soon virtio-mem) add a lot of
    memory in separate memory blocks and wait for memory to get onlined by
    user space before continuing to add more memory blocks (to not add memory
    faster than it is getting onlined).  This of course slows down the whole
    memory hotplug process.
    
    To make the job of distributions easier and to avoid udev rules that get
    more and more complicated, let's extend the mechanism provided by
    - /sys/devices/system/memory/auto_online_blocks
    - "memhp_default_state=" on the kernel cmdline
    to be able to specify also "online_movable" as well as "online_kernel"
    
    === Example /usr/libexec/config-memhotplug ===
    
    #!/bin/bash
    
    VIRT=`systemd-detect-virt --vm`
    ARCH=`uname -p`
    
    sense_virtio_mem() {
      if [ -d "/sys/bus/virtio/drivers/virtio_mem/" ]; then
        DEVICES=`find /sys/bus/virtio/drivers/virtio_mem/ -maxdepth 1 -type l | wc -l`
        if [ $DEVICES != "0" ]; then
            return 0
        fi
      fi
      return 1
    }
    
    if [ ! -e "/sys/devices/system/memory/auto_online_blocks" ]; then
      echo "Memory hotplug configuration support missing in the kernel"
      exit 1
    fi
    
    if grep "memhp_default_state=" /proc/cmdline > /dev/null; then
      echo "Memory hotplug configuration overridden in kernel cmdline (memhp_default_state=)"
      exit 1
    fi
    
    if [ $VIRT == "microsoft" ]; then
      echo "Detected Hyper-V on $ARCH"
      # Hyper-V wants all memory in ZONE_NORMAL
      ONLINE_TYPE="online_kernel"
    elif sense_virtio_mem; then
      echo "Detected virtio-mem on $ARCH"
      # virtio-mem wants all memory in ZONE_NORMAL
      ONLINE_TYPE="online_kernel"
    elif [ $ARCH == "s390x" ] || [ $ARCH == "s390" ]; then
      echo "Detected $ARCH"
      # standby memory should not be onlined automatically
      ONLINE_TYPE="offline"
    elif [ $ARCH == "ppc64" ] || [ $ARCH == "ppc64le" ]; then
      echo "Detected" $ARCH
      # PPC64 onlines all hotplugged memory right from the kernel
      ONLINE_TYPE="offline"
    elif [ $VIRT == "none" ]; then
      echo "Detected bare-metal on $ARCH"
      # Bare metal users expect hotplugged memory to be unpluggable. We assume
      # that ZONE imbalances on such enterpise servers cannot happen and is
      # properly documented
      ONLINE_TYPE="online_movable"
    else
      # TODO: Hypervisors that want to unplug DIMMs and can guarantee that ZONE
      # imbalances won't happen
      echo "Detected $VIRT on $ARCH"
      # Usually, ballooning is used in virtual environments, so memory should go to
      # ZONE_NORMAL. However, sometimes "movable_node" is relevant.
      ONLINE_TYPE="online"
    fi
    
    echo "Selected online_type:" $ONLINE_TYPE
    
    # Configure what to do with memory that will be hotplugged in the future
    echo $ONLINE_TYPE 2>/dev/null > /sys/devices/system/memory/auto_online_blocks
    if [ $? != "0" ]; then
      echo "Memory hotplug cannot be configured (e.g., old kernel or missing permissions)"
      # A backup udev rule should handle old kernels if necessary
      exit 1
    fi
    
    # Process all already pluggedd blocks (e.g., DIMMs, but also Hyper-V or virtio-mem)
    if [ $ONLINE_TYPE != "offline" ]; then
      for MEMORY in /sys/devices/system/memory/memory*; do
        STATE=`cat $MEMORY/state`
        if [ $STATE == "offline" ]; then
            echo $ONLINE_TYPE > $MEMORY/state
        fi
      done
    fi
    
    === Example /usr/lib/systemd/system/config-memhotplug.service ===
    
    [Unit]
    Description=Configure memory hotplug behavior
    DefaultDependencies=no
    Conflicts=shutdown.target
    Before=sysinit.target shutdown.target
    After=systemd-modules-load.service
    ConditionPathExists=|/sys/devices/system/memory/auto_online_blocks
    
    [Service]
    ExecStart=/usr/libexec/config-memhotplug
    Type=oneshot
    TimeoutSec=0
    RemainAfterExit=yes
    
    [Install]
    WantedBy=sysinit.target
    
    === Example modification to the 40-redhat.rules [2] ===
    
    : diff --git a/40-redhat.rules b/40-redhat.rules-new
    : index 2c690e5..168fd03 100644
    : --- a/40-redhat.rules
    : +++ b/40-redhat.rules-new
    : @@ -6,6 +6,9 @@ SUBSYSTEM=="cpu", ACTION=="add", TEST=="online", ATTR{online}=="0", ATTR{online}
    :  # Memory hotadd request
    :  SUBSYSTEM!="memory", GOTO="memory_hotplug_end"
    :  ACTION!="add", GOTO="memory_hotplug_end"
    : +# memory hotplug behavior configured
    : +PROGRAM=="grep online /sys/devices/system/memory/auto_online_blocks", GOTO="memory_hotplug_end"
    : +
    :  PROGRAM="/bin/uname -p", RESULT=="s390*", GOTO="memory_hotplug_end"
    :
    :  ENV{.state}="online"
    
    ===
    
    [1] https://github.com/lnykryn/systemd-rhel/pull/281
    [2] https://github.com/lnykryn/systemd-rhel/blob/staging/rules/40-redhat.rules
    
    This patch (of 8):
    
    The name is misleading and it's not really clear what is "kept".  Let's
    just name it like the online_type name we expose to user space ("online").
    
    Add some documentation to the types.
    
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Yumei Huang <yuhuang@redhat.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Wei Liu <wei.liu@kernel.org>
    Link: http://lkml.kernel.org/r/20200319131221.14044-1-david@redhat.com
    Link: http://lkml.kernel.org/r/20200317104942.11178-2-david@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index f4d59155f3d4..261dbf010d5d 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -47,9 +47,13 @@ enum {
 
 /* Types for control the zone type of onlined and offlined memory */
 enum {
+	/* Offline the memory. */
 	MMOP_OFFLINE = -1,
-	MMOP_ONLINE_KEEP,
+	/* Online the memory. Zone depends, see default_zone_for_pfn(). */
+	MMOP_ONLINE,
+	/* Online the memory to ZONE_NORMAL. */
 	MMOP_ONLINE_KERNEL,
+	/* Online the memory to ZONE_MOVABLE. */
 	MMOP_ONLINE_MOVABLE,
 };
 

commit 92917998849eea951707c8fea2dc3007bb2ad2cd
Author: David Hildenbrand <david@redhat.com>
Date:   Mon Feb 3 17:34:26 2020 -0800

    mm/memory_hotplug: drop valid_start/valid_end from test_pages_in_a_zone()
    
    The callers are only interested in the actual zone, they don't care about
    boundaries.  Return the zone instead to simplify.
    
    Link: http://lkml.kernel.org/r/20200110183308.11849-1-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ffa6ad12d84a..f4d59155f3d4 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -96,8 +96,8 @@ extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long pfn, unsigned long nr_pages,
 			int online_type, int nid);
-extern int test_pages_in_a_zone(unsigned long start_pfn, unsigned long end_pfn,
-	unsigned long *valid_start, unsigned long *valid_end);
+extern struct zone *test_pages_in_a_zone(unsigned long start_pfn,
+					 unsigned long end_pfn);
 extern unsigned long __offline_isolated_pages(unsigned long start_pfn,
 						unsigned long end_pfn);
 

commit bd5c2344f9eb1ebf7ff2501ddb13d83151939780
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jan 30 22:14:54 2020 -0800

    mm/memory_hotplug: pass in nid to online_pages()
    
    Patch series "mm/memory_hotplug: pass in nid to online_pages()".
    
    Simplify onlining code and get rid of find_memory_block().  Pass in the
    nid from the memory block we are trying to online directly, instead of
    manually looking it up.
    
    This patch (of 2):
    
    No need to lookup the memory block, we can directly pass in the nid.
    
    Link: http://lkml.kernel.org/r/20200113113354.6341-2-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ba0dca6aac6e..ffa6ad12d84a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -94,7 +94,8 @@ extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
 extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* VM interface that may be used by firmware interface */
-extern int online_pages(unsigned long, unsigned long, int);
+extern int online_pages(unsigned long pfn, unsigned long nr_pages,
+			int online_type, int nid);
 extern int test_pages_in_a_zone(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long *valid_start, unsigned long *valid_end);
 extern unsigned long __offline_isolated_pages(unsigned long start_pfn,

commit feee6b2989165631b17ac6d4ccdbf6759254e85a
Author: David Hildenbrand <david@redhat.com>
Date:   Sat Jan 4 12:59:33 2020 -0800

    mm/memory_hotplug: shrink zones when offlining memory
    
    We currently try to shrink a single zone when removing memory.  We use
    the zone of the first page of the memory we are removing.  If that
    memmap was never initialized (e.g., memory was never onlined), we will
    read garbage and can trigger kernel BUGs (due to a stale pointer):
    
        BUG: unable to handle page fault for address: 000000000000353d
        #PF: supervisor write access in kernel mode
        #PF: error_code(0x0002) - not-present page
        PGD 0 P4D 0
        Oops: 0002 [#1] SMP PTI
        CPU: 1 PID: 7 Comm: kworker/u8:0 Not tainted 5.3.0-rc5-next-20190820+ #317
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.1-0-ga5cab58e9a3f-prebuilt.qemu.4
        Workqueue: kacpi_hotplug acpi_hotplug_work_fn
        RIP: 0010:clear_zone_contiguous+0x5/0x10
        Code: 48 89 c6 48 89 c3 e8 2a fe ff ff 48 85 c0 75 cf 5b 5d c3 c6 85 fd 05 00 00 01 5b 5d c3 0f 1f 840
        RSP: 0018:ffffad2400043c98 EFLAGS: 00010246
        RAX: 0000000000000000 RBX: 0000000200000000 RCX: 0000000000000000
        RDX: 0000000000200000 RSI: 0000000000140000 RDI: 0000000000002f40
        RBP: 0000000140000000 R08: 0000000000000000 R09: 0000000000000001
        R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000140000
        R13: 0000000000140000 R14: 0000000000002f40 R15: ffff9e3e7aff3680
        FS:  0000000000000000(0000) GS:ffff9e3e7bb00000(0000) knlGS:0000000000000000
        CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
        CR2: 000000000000353d CR3: 0000000058610000 CR4: 00000000000006e0
        DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
        DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
        Call Trace:
         __remove_pages+0x4b/0x640
         arch_remove_memory+0x63/0x8d
         try_remove_memory+0xdb/0x130
         __remove_memory+0xa/0x11
         acpi_memory_device_remove+0x70/0x100
         acpi_bus_trim+0x55/0x90
         acpi_device_hotplug+0x227/0x3a0
         acpi_hotplug_work_fn+0x1a/0x30
         process_one_work+0x221/0x550
         worker_thread+0x50/0x3b0
         kthread+0x105/0x140
         ret_from_fork+0x3a/0x50
        Modules linked in:
        CR2: 000000000000353d
    
    Instead, shrink the zones when offlining memory or when onlining failed.
    Introduce and use remove_pfn_range_from_zone(() for that.  We now
    properly shrink the zones, even if we have DIMMs whereby
    
     - Some memory blocks fall into no zone (never onlined)
    
     - Some memory blocks fall into multiple zones (offlined+re-onlined)
    
     - Multiple memory blocks that fall into different zones
    
    Drop the zone parameter (with a potential dubious value) from
    __remove_pages() and __remove_section().
    
    Link: http://lkml.kernel.org/r/20191006085646.5768-6-david@redhat.com
    Fixes: f1dd2cd13c4b ("mm, memory_hotplug: do not associate hotadded memory to zones until online")      [visible after d0dc12e86b319]
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: <stable@vger.kernel.org>    [5.0+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3a08ecdfca11..ba0dca6aac6e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -122,8 +122,8 @@ static inline bool movable_node_is_enabled(void)
 
 extern void arch_remove_memory(int nid, u64 start, u64 size,
 			       struct vmem_altmap *altmap);
-extern void __remove_pages(struct zone *zone, unsigned long start_pfn,
-			   unsigned long nr_pages, struct vmem_altmap *altmap);
+extern void __remove_pages(unsigned long start_pfn, unsigned long nr_pages,
+			   struct vmem_altmap *altmap);
 
 /* reasonably generic interface to expand the physical pages */
 extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
@@ -342,6 +342,9 @@ extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
+extern void remove_pfn_range_from_zone(struct zone *zone,
+				       unsigned long start_pfn,
+				       unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern int sparse_add_section(int nid, unsigned long pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);

commit 12cc1c7345b6bf34c45ccaa75393e2d6eb707d7b
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Sat Nov 30 17:58:20 2019 -0800

    mm/memory_hotplug.c: remove __online_page_set_limits()
    
    __online_page_set_limits() is a dummy function - remove it and all
    callers.
    
    Link: http://lkml.kernel.org/r/8e1bc9d3b492f6bde16e95ebc1dee11d6aefabd7.1567889743.git.jrdr.linux@gmail.com
    Link: http://lkml.kernel.org/r/854db2cf8145d9635249c95584d9a91fd774a229.1567889743.git.jrdr.linux@gmail.com
    Link: http://lkml.kernel.org/r/9afe6c5a18158f3884a6b302ac2c772f3da49ccc.1567889743.git.jrdr.linux@gmail.com
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 101d97e7e2ac..3a08ecdfca11 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -106,8 +106,6 @@ extern void generic_online_page(struct page *page, unsigned int order);
 extern int set_online_page_callback(online_page_callback_t callback);
 extern int restore_online_page_callback(online_page_callback_t callback);
 
-extern void __online_page_set_limits(struct page *page);
-
 extern int try_online_node(int nid);
 
 extern int arch_add_memory(int nid, u64 start, u64 size,

commit aba9817da150e9dcf4c599c0508c38d1971d66e1
Author: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
Date:   Sat Nov 30 17:54:10 2019 -0800

    include/linux/memory_hotplug.h: move definitions of {set,clear}_zone_contiguous
    
    The {set,clear}_zone_contiguous are built whatever the configuratoon so
    move the definitions outside the current ifdef to avoid the following
    compiler warnings:
    
      mm/page_alloc.c:1550:6: warning: no previous prototype for 'set_zone_contiguous' [-Wmissing-prototypes]
      mm/page_alloc.c:1571:6: warning: no previous prototype for 'clear_zone_contiguous' [-Wmissing-prototypes]
    
    Link: http://lkml.kernel.org/r/20191106123911.7435-1-ben.dooks@codethink.co.uk
    Signed-off-by: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index fb638cadf8c0..101d97e7e2ac 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -228,9 +228,6 @@ void put_online_mems(void);
 void mem_hotplug_begin(void);
 void mem_hotplug_done(void);
 
-extern void set_zone_contiguous(struct zone *zone);
-extern void clear_zone_contiguous(struct zone *zone);
-
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 #define pfn_to_online_page(pfn)			\
 ({						\
@@ -338,6 +335,9 @@ static inline int remove_memory(int nid, u64 start, u64 size)
 static inline void __remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+extern void set_zone_contiguous(struct zone *zone);
+extern void clear_zone_contiguous(struct zone *zone);
+
 extern void __ref free_area_init_core_hotplug(int nid);
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);

commit 0ec47097434847c0c3a3bb7287feb46386a62720
Author: David Hildenbrand <david@redhat.com>
Date:   Sat Nov 30 17:54:00 2019 -0800

    mm/memory_hotplug: remove __online_page_free() and __online_page_increment_counters()
    
    Let's drop the now unused functions.
    
    Link: http://lkml.kernel.org/r/20190909114830.662-4-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3b3b1c7641fe..fb638cadf8c0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -107,8 +107,6 @@ extern int set_online_page_callback(online_page_callback_t callback);
 extern int restore_online_page_callback(online_page_callback_t callback);
 
 extern void __online_page_set_limits(struct page *page);
-extern void __online_page_increment_counters(struct page *page);
-extern void __online_page_free(struct page *page);
 
 extern int try_online_node(int nid);
 

commit 18db149120c106cf2b1a2595f82f3229f9d223b8
Author: David Hildenbrand <david@redhat.com>
Date:   Sat Nov 30 17:53:51 2019 -0800

    mm/memory_hotplug: export generic_online_page()
    
    Patch series "mm/memory_hotplug: Export generic_online_page()".
    
    Let's replace the __online_page...() functions by generic_online_page().
    Hyper-V only wants to delay the actual onlining of un-backed pages, so
    we can simpy re-use the generic function.
    
    This patch (of 3):
    
    Let's expose generic_online_page() so online_page_callback users can
    simply fall back to the generic implementation when actually deciding to
    online the pages.
    
    Link: http://lkml.kernel.org/r/20190909114830.662-2-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index f46ea71b4ffd..3b3b1c7641fe 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -102,6 +102,7 @@ extern unsigned long __offline_isolated_pages(unsigned long start_pfn,
 
 typedef void (*online_page_callback_t)(struct page *page, unsigned int order);
 
+extern void generic_online_page(struct page *page, unsigned int order);
 extern int set_online_page_callback(online_page_callback_t callback);
 extern int restore_online_page_callback(online_page_callback_t callback);
 

commit ba72b4c8cf60e452cf6f0258ed9ee697957b7dfd
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:26 2019 -0700

    mm/sparsemem: support sub-section hotplug
    
    The libnvdimm sub-system has suffered a series of hacks and broken
    workarounds for the memory-hotplug implementation's awkward
    section-aligned (128MB) granularity.
    
    For example the following backtrace is emitted when attempting
    arch_add_memory() with physical address ranges that intersect 'System
    RAM' (RAM) with 'Persistent Memory' (PMEM) within a given section:
    
        # cat /proc/iomem | grep -A1 -B1 Persistent\ Memory
        100000000-1ffffffff : System RAM
        200000000-303ffffff : Persistent Memory (legacy)
        304000000-43fffffff : System RAM
        440000000-23ffffffff : Persistent Memory
        2400000000-43bfffffff : Persistent Memory
          2400000000-43bfffffff : namespace2.0
    
        WARNING: CPU: 38 PID: 928 at arch/x86/mm/init_64.c:850 add_pages+0x5c/0x60
        [..]
        RIP: 0010:add_pages+0x5c/0x60
        [..]
        Call Trace:
         devm_memremap_pages+0x460/0x6e0
         pmem_attach_disk+0x29e/0x680 [nd_pmem]
         ? nd_dax_probe+0xfc/0x120 [libnvdimm]
         nvdimm_bus_probe+0x66/0x160 [libnvdimm]
    
    It was discovered that the problem goes beyond RAM vs PMEM collisions as
    some platform produce PMEM vs PMEM collisions within a given section.
    The libnvdimm workaround for that case revealed that the libnvdimm
    section-alignment-padding implementation has been broken for a long
    while.
    
    A fix for that long-standing breakage introduces as many problems as it
    solves as it would require a backward-incompatible change to the
    namespace metadata interpretation.  Instead of that dubious route [1],
    address the root problem in the memory-hotplug implementation.
    
    Note that EEXIST is no longer treated as success as that is how
    sparse_add_section() reports subsection collisions, it was also obviated
    by recent changes to perform the request_region() for 'System RAM'
    before arch_add_memory() in the add_memory() sequence.
    
    [1] https://lore.kernel.org/r/155000671719.348031.2347363160141119237.stgit@dwillia2-desk3.amr.corp.intel.com
    
    [osalvador@suse.de: fix deactivate_section for early sections]
      Link: http://lkml.kernel.org/r/20190715081549.32577-2-osalvador@suse.de
    Link: http://lkml.kernel.org/r/156092354368.979959.6232443923440952359.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 2d636a7491a4..f46ea71b4ffd 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -348,7 +348,7 @@ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern int sparse_add_section(int nid, unsigned long pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
-extern void sparse_remove_one_section(struct mem_section *ms,
+extern void sparse_remove_section(struct mem_section *ms,
 		unsigned long pfn, unsigned long nr_pages,
 		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,

commit 7ea6216049ff9cf250a6722cd766d99c8d1424e5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Jul 18 15:58:22 2019 -0700

    mm/sparsemem: prepare for sub-section ranges
    
    Prepare the memory hot-{add,remove} paths for handling sub-section
    ranges by plumbing the starting page frame and number of pages being
    handled through arch_{add,remove}_memory() to
    sparse_{add,remove}_one_section().
    
    This is simply plumbing, small cleanups, and some identifier renames.
    No intended functional changes.
    
    Link: http://lkml.kernel.org/r/156092353780.979959.9713046515562743194.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>        [ppc64]
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jane Chu <jane.chu@oracle.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 475aff8efbf8..2d636a7491a4 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -346,9 +346,10 @@ extern int add_memory_resource(int nid, struct resource *resource);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern bool is_memblock_offlined(struct memory_block *mem);
-extern int sparse_add_one_section(int nid, unsigned long start_pfn,
-				  struct vmem_altmap *altmap);
+extern int sparse_add_section(int nid, unsigned long pfn,
+		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct mem_section *ms,
+		unsigned long pfn, unsigned long nr_pages,
 		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);

commit ea8846411ad686ff626e00bb2c3821b3db2ab56a
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:57:50 2019 -0700

    mm/memory_hotplug: move and simplify walk_memory_blocks()
    
    Let's move walk_memory_blocks() to the place where memory block logic
    resides and simplify it.  While at it, add a type for the callback
    function.
    
    Link: http://lkml.kernel.org/r/20190614100114.311-6-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index d9fffc34949f..475aff8efbf8 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -340,8 +340,6 @@ static inline void __remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 extern void __ref free_area_init_core_hotplug(int nid);
-extern int walk_memory_blocks(unsigned long start, unsigned long size,
-		void *arg, int (*func)(struct memory_block *, void *));
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource);

commit fbcf73ce65827c3d8935f38b832a43153a0c78d1
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:57:46 2019 -0700

    mm/memory_hotplug: rename walk_memory_range() and pass start+size instead of pfns
    
    walk_memory_range() was once used to iterate over sections.  Now, it
    iterates over memory blocks.  Rename the function, fixup the
    documentation.
    
    Also, pass start+size instead of PFNs, which is what most callers
    already have at hand.  (we'll rework link_mem_sections() most probably
    soon)
    
    Follow-up patches will rework, simplify, and move walk_memory_blocks()
    to drivers/base/memory.c.
    
    Note: walk_memory_blocks() only works correctly right now if the
    start_pfn is aligned to a section start.  This is the case right now,
    but we'll generalize the function in a follow up patch so the semantics
    match the documentation.
    
    [akpm@linux-foundation.org: remove unused variable]
    Link: http://lkml.kernel.org/r/20190614100114.311-5-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Rashmica Gupta <rashmica.g@gmail.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 79e0add6a597..d9fffc34949f 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -340,7 +340,7 @@ static inline void __remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 extern void __ref free_area_init_core_hotplug(int nid);
-extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
+extern int walk_memory_blocks(unsigned long start, unsigned long size,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);

commit b9bf8d342d9b443c0d19aa57883d8ddb38d965de
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:57:17 2019 -0700

    mm/memory_hotplug: remove "zone" parameter from sparse_remove_one_section
    
    The parameter is unused, so let's drop it.  Memory removal paths should
    never care about zones.  This is the job of memory offlining and will
    require more refactorings.
    
    Link: http://lkml.kernel.org/r/20190527111152.16324-12-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Wei Yang <richardw.yang@linux.intel.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Jun Yao <yaojun8558363@gmail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: "mike.travis@hpe.com" <mike.travis@hpe.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 36c514b80cf1..79e0add6a597 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -350,7 +350,7 @@ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern int sparse_add_one_section(int nid, unsigned long start_pfn,
 				  struct vmem_altmap *altmap);
-extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
+extern void sparse_remove_one_section(struct mem_section *ms,
 		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);

commit 05f800a0bd08e14606ac63e0a5c63ed6880acaab
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:57:01 2019 -0700

    mm/memory_hotplug: drop MHP_MEMBLOCK_API
    
    No longer needed, the callers of arch_add_memory() can handle this
    manually.
    
    Link: http://lkml.kernel.org/r/20190527111152.16324-9-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Wei Yang <richardw.yang@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: Jun Yao <yaojun8558363@gmail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: "mike.travis@hpe.com" <mike.travis@hpe.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 87bf9c4a889e..36c514b80cf1 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -128,14 +128,6 @@ extern void arch_remove_memory(int nid, u64 start, u64 size,
 extern void __remove_pages(struct zone *zone, unsigned long start_pfn,
 			   unsigned long nr_pages, struct vmem_altmap *altmap);
 
-/*
- * Do we want sysfs memblock files created. This will allow userspace to online
- * and offline memory explicitly. Lack of this bit means that the caller has to
- * call move_pfn_range_to_zone to finish the initialization.
- */
-
-#define MHP_MEMBLOCK_API               (1<<0)
-
 /* reasonably generic interface to expand the physical pages */
 extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
 		       struct mhp_restrictions *restrictions);

commit 80ec922dbd87fd38d15719c86a94457204648aeb
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:56:51 2019 -0700

    mm/memory_hotplug: allow arch_remove_memory() without CONFIG_MEMORY_HOTREMOVE
    
    We want to improve error handling while adding memory by allowing to use
    arch_remove_memory() and __remove_pages() even if
    CONFIG_MEMORY_HOTREMOVE is not set to e.g., implement something like:
    
            arch_add_memory()
            rc = do_something();
            if (rc) {
                    arch_remove_memory();
            }
    
    We won't get rid of CONFIG_MEMORY_HOTREMOVE for now, as it will require
    quite some dependencies for memory offlining.
    
    Link: http://lkml.kernel.org/r/20190527111152.16324-7-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: "mike.travis@hpe.com" <mike.travis@hpe.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Jun Yao <yaojun8558363@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 988fde33cd7f..87bf9c4a889e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -123,12 +123,10 @@ static inline bool movable_node_is_enabled(void)
 	return movable_node_enabled;
 }
 
-#ifdef CONFIG_MEMORY_HOTREMOVE
 extern void arch_remove_memory(int nid, u64 start, u64 size,
 			       struct vmem_altmap *altmap);
 extern void __remove_pages(struct zone *zone, unsigned long start_pfn,
 			   unsigned long nr_pages, struct vmem_altmap *altmap);
-#endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /*
  * Do we want sysfs memblock files created. This will allow userspace to online

commit eca499ab3749a4537dee77ffead47a1a2c0dee19
Author: Pavel Tatashin <pasha.tatashin@soleen.com>
Date:   Tue Jul 16 16:30:31 2019 -0700

    mm/hotplug: make remove_memory() interface usable
    
    Presently the remove_memory() interface is inherently broken.  It tries
    to remove memory but panics if some memory is not offline.  The problem
    is that it is impossible to ensure that all memory blocks are offline as
    this function also takes lock_device_hotplug that is required to change
    memory state via sysfs.
    
    So, between calling this function and offlining all memory blocks there
    is always a window when lock_device_hotplug is released, and therefore,
    there is always a chance for a panic during this window.
    
    Make this interface to return an error if memory removal fails.  This
    way it is safe to call this function without panicking machine, and also
    makes it symmetric to add_memory() which already returns an error.
    
    Link: http://lkml.kernel.org/r/20190517215438.6487-3-pasha.tatashin@soleen.com
    Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Jrme Glisse <jglisse@redhat.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Ross Zwisler <zwisler@kernel.org>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Takashi Iwai <tiwai@suse.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Vishal Verma <vishal.l.verma@intel.com>
    Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ae892eef8b82..988fde33cd7f 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -324,7 +324,7 @@ static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
 extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
-extern void remove_memory(int nid, u64 start, u64 size);
+extern int remove_memory(int nid, u64 start, u64 size);
 extern void __remove_memory(int nid, u64 start, u64 size);
 
 #else
@@ -341,7 +341,11 @@ static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
 	return -EINVAL;
 }
 
-static inline void remove_memory(int nid, u64 start, u64 size) {}
+static inline int remove_memory(int nid, u64 start, u64 size)
+{
+	return -EBUSY;
+}
+
 static inline void __remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 

commit ac5c94264580f498e484c854031d0226b3c1038f
Author: David Hildenbrand <david@redhat.com>
Date:   Mon May 13 17:21:46 2019 -0700

    mm/memory_hotplug: make __remove_pages() and arch_remove_memory() never fail
    
    All callers of arch_remove_memory() ignore errors.  And we should really
    try to remove any errors from the memory removal path.  No more errors are
    reported from __remove_pages().  BUG() in s390x code in case
    arch_remove_memory() is triggered.  We may implement that properly later.
    WARN in case powerpc code failed to remove the section mapping, which is
    better than ignoring the error completely right now.
    
    Link: http://lkml.kernel.org/r/20190409100148.24703-5-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Stefan Agner <stefan@agner.ch>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index b24aca54353e..ae892eef8b82 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -124,10 +124,10 @@ static inline bool movable_node_is_enabled(void)
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-extern int arch_remove_memory(int nid, u64 start, u64 size,
-				struct vmem_altmap *altmap);
-extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages, struct vmem_altmap *altmap);
+extern void arch_remove_memory(int nid, u64 start, u64 size,
+			       struct vmem_altmap *altmap);
+extern void __remove_pages(struct zone *zone, unsigned long start_pfn,
+			   unsigned long nr_pages, struct vmem_altmap *altmap);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /*

commit 940519f0c8b757fdcbc5d14c93cdaada20ded14c
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 13 17:21:26 2019 -0700

    mm, memory_hotplug: provide a more generic restrictions for memory hotplug
    
    arch_add_memory, __add_pages take a want_memblock which controls whether
    the newly added memory should get the sysfs memblock user API (e.g.
    ZONE_DEVICE users do not want/need this interface).  Some callers even
    want to control where do we allocate the memmap from by configuring
    altmap.
    
    Add a more generic hotplug context for arch_add_memory and __add_pages.
    struct mhp_restrictions contains flags which contains additional features
    to be enabled by the memory hotplug (MHP_MEMBLOCK_API currently) and
    altmap for alternative memmap allocator.
    
    This patch shouldn't introduce any functional change.
    
    [akpm@linux-foundation.org: build fix]
    Link: http://lkml.kernel.org/r/20190408082633.2864-3-osalvador@suse.de
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3c8cf347804c..b24aca54353e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -53,6 +53,16 @@ enum {
 	MMOP_ONLINE_MOVABLE,
 };
 
+/*
+ * Restrictions for the memory hotplug:
+ * flags:  MHP_ flags
+ * altmap: alternative allocator for memmap array
+ */
+struct mhp_restrictions {
+	unsigned long flags;
+	struct vmem_altmap *altmap;
+};
+
 /*
  * Zone resizing functions
  *
@@ -101,6 +111,8 @@ extern void __online_page_free(struct page *page);
 
 extern int try_online_node(int nid);
 
+extern int arch_add_memory(int nid, u64 start, u64 size,
+			struct mhp_restrictions *restrictions);
 extern u64 max_mem_size;
 
 extern bool memhp_auto_online;
@@ -118,20 +130,27 @@ extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages, struct vmem_altmap *altmap);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+/*
+ * Do we want sysfs memblock files created. This will allow userspace to online
+ * and offline memory explicitly. Lack of this bit means that the caller has to
+ * call move_pfn_range_to_zone to finish the initialization.
+ */
+
+#define MHP_MEMBLOCK_API               (1<<0)
+
 /* reasonably generic interface to expand the physical pages */
 extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-		struct vmem_altmap *altmap, bool want_memblock);
+		       struct mhp_restrictions *restrictions);
 
 #ifndef CONFIG_ARCH_HAS_ADD_PAGES
 static inline int add_pages(int nid, unsigned long start_pfn,
-		unsigned long nr_pages, struct vmem_altmap *altmap,
-		bool want_memblock)
+		unsigned long nr_pages, struct mhp_restrictions *restrictions)
 {
-	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
+	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
 #else /* ARCH_HAS_ADD_PAGES */
 int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-		struct vmem_altmap *altmap, bool want_memblock);
+	      struct mhp_restrictions *restrictions);
 #endif /* ARCH_HAS_ADD_PAGES */
 
 #ifdef CONFIG_NUMA
@@ -332,8 +351,6 @@ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource);
-extern int arch_add_memory(int nid, u64 start, u64 size,
-		struct vmem_altmap *altmap, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern bool is_memblock_offlined(struct memory_block *mem);

commit 5557c766abad25acc8091ccb9641b96e3b3da06f
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 13 17:21:24 2019 -0700

    mm, memory_hotplug: cleanup memory offline path
    
    check_pages_isolated_cb currently accounts the whole pfn range as being
    offlined if test_pages_isolated suceeds on the range.  This is based on
    the assumption that all pages in the range are freed which is currently
    the case in most cases but it won't be with later changes, as pages marked
    as vmemmap won't be isolated.
    
    Move the offlined pages counting to offline_isolated_pages_cb and rely on
    __offline_isolated_pages to return the correct value.
    check_pages_isolated_cb will still do it's primary job and check the pfn
    range.
    
    While we are at it remove check_pages_isolated and offline_isolated_pages
    and use directly walk_system_ram_range as do in online_pages.
    
    Link: http://lkml.kernel.org/r/20190408082633.2864-2-osalvador@suse.de
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8ade08c50d26..3c8cf347804c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -87,7 +87,8 @@ extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 extern int online_pages(unsigned long, unsigned long, int);
 extern int test_pages_in_a_zone(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long *valid_start, unsigned long *valid_end);
-extern void __offline_isolated_pages(unsigned long, unsigned long);
+extern unsigned long __offline_isolated_pages(unsigned long start_pfn,
+						unsigned long end_pfn);
 
 typedef void (*online_page_callback_t)(struct page *page, unsigned int order);
 

commit d14d7f14f177834788a276fc7b1317b539cedca2
Merge: 6cdfa54cd229 01bd2ac2f55a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 11 17:08:14 2019 -0700

    Merge tag 'for-linus-5.1a-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
     "xen fixes and features:
    
       - remove fallback code for very old Xen hypervisors
    
       - three patches for fixing Xen dom0 boot regressions
    
       - an old patch for Xen PCI passthrough which was never applied for
         unknown reasons
    
       - some more minor fixes and cleanup patches"
    
    * tag 'for-linus-5.1a-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen: fix dom0 boot on huge systems
      xen, cpu_hotplug: Prevent an out of bounds access
      xen: remove pre-xen3 fallback handlers
      xen/ACPI: Switch to bitmap_zalloc()
      x86/xen: dont add memory above max allowed allocation
      x86: respect memory size limiting via mem= parameter
      xen/gntdev: Check and release imported dma-bufs on close
      xen/gntdev: Do not destroy context while dma-bufs are in use
      xen/pciback: Don't disable PCI_COMMAND on PCI device reset.
      xen-scsiback: mark expected switch fall-through
      xen: mark expected switch fall-through

commit a9cd410a3d296846a8125aa43d97a573a354c472
Author: Arun KS <arunks@codeaurora.org>
Date:   Tue Mar 5 15:42:14 2019 -0800

    mm/page_alloc.c: memory hotplug: free pages as higher order
    
    When freeing pages are done with higher order, time spent on coalescing
    pages by buddy allocator can be reduced.  With section size of 256MB,
    hot add latency of a single section shows improvement from 50-60 ms to
    less than 1 ms, hence improving the hot add latency by 60 times.  Modify
    external providers of online callback to align with the change.
    
    [arunks@codeaurora.org: v11]
      Link: http://lkml.kernel.org/r/1547792588-18032-1-git-send-email-arunks@codeaurora.org
    [akpm@linux-foundation.org: remove unused local, per Arun]
    [akpm@linux-foundation.org: avoid return of void-returning __free_pages_core(), per Oscar]
    [akpm@linux-foundation.org: fix it for mm-convert-totalram_pages-and-totalhigh_pages-variables-to-atomic.patch]
    [arunks@codeaurora.org: v8]
      Link: http://lkml.kernel.org/r/1547032395-24582-1-git-send-email-arunks@codeaurora.org
    [arunks@codeaurora.org: v9]
      Link: http://lkml.kernel.org/r/1547098543-26452-1-git-send-email-arunks@codeaurora.org
    Link: http://lkml.kernel.org/r/1538727006-5727-1-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Souptick Joarder <jrdr.linux@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Srivatsa Vaddagiri <vatsa@codeaurora.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 368267c1b71b..52869d6d38b3 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -89,7 +89,7 @@ extern int test_pages_in_a_zone(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long *valid_start, unsigned long *valid_end);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
-typedef void (*online_page_callback_t)(struct page *page);
+typedef void (*online_page_callback_t)(struct page *page, unsigned int order);
 
 extern int set_online_page_callback(online_page_callback_t callback);
 extern int restore_online_page_callback(online_page_callback_t callback);

commit 357b4da50a62e2fd70eacee21cdbd22d4c7a7b60
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Feb 14 11:42:39 2019 +0100

    x86: respect memory size limiting via mem= parameter
    
    When limiting memory size via kernel parameter "mem=" this should be
    respected even in case of memory made accessible via a PCI card.
    
    Today this kind of memory won't be made usable in initial memory
    setup as the memory won't be visible in E820 map, but it might be
    added when adding PCI devices due to corresponding ACPI table entries.
    
    Not respecting "mem=" can be corrected by adding a global max_mem_size
    variable set by parse_memopt() which will result in rejecting adding
    memory areas resulting in a memory size above the allowed limit.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 368267c1b71b..cfd12078172a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -100,6 +100,8 @@ extern void __online_page_free(struct page *page);
 
 extern int try_online_node(int nid);
 
+extern u64 max_mem_size;
+
 extern bool memhp_auto_online;
 /* If movable_node boot option specified */
 extern bool movable_node_enabled;

commit b13bc35193d9e7a8c050a24928ca5c9e7c9a009b
Author: Qian Cai <cai@lca.pw>
Date:   Fri Feb 1 14:20:51 2019 -0800

    mm/hotplug: invalid PFNs from pfn_to_online_page()
    
    On an arm64 ThunderX2 server, the first kmemleak scan would crash [1]
    with CONFIG_DEBUG_VM_PGFLAGS=y due to page_to_nid() found a pfn that is
    not directly mapped (MEMBLOCK_NOMAP).  Hence, the page->flags is
    uninitialized.
    
    This is due to the commit 9f1eb38e0e11 ("mm, kmemleak: little
    optimization while scanning") starts to use pfn_to_online_page() instead
    of pfn_valid().  However, in the CONFIG_MEMORY_HOTPLUG=y case,
    pfn_to_online_page() does not call memblock_is_map_memory() while
    pfn_valid() does.
    
    Historically, the commit 68709f45385a ("arm64: only consider memblocks
    with NOMAP cleared for linear mapping") causes pages marked as nomap
    being no long reassigned to the new zone in memmap_init_zone() by
    calling __init_single_page().
    
    Since the commit 2d070eab2e82 ("mm: consider zone which is not fully
    populated to have holes") introduced pfn_to_online_page() and was
    designed to return a valid pfn only, but it is clearly broken on arm64.
    
    Therefore, let pfn_to_online_page() call pfn_valid_within(), so it can
    handle nomap thanks to the commit f52bb98f5ade ("arm64: mm: always
    enable CONFIG_HOLES_IN_ZONE"), while it will be optimized away on
    architectures where have no HOLES_IN_ZONE.
    
    [1]
      Unable to handle kernel NULL pointer dereference at virtual address 0000000000000006
      Mem abort info:
        ESR = 0x96000005
        Exception class = DABT (current EL), IL = 32 bits
        SET = 0, FnV = 0
        EA = 0, S1PTW = 0
      Data abort info:
        ISV = 0, ISS = 0x00000005
        CM = 0, WnR = 0
      Internal error: Oops: 96000005 [#1] SMP
      CPU: 60 PID: 1408 Comm: kmemleak Not tainted 5.0.0-rc2+ #8
      pstate: 60400009 (nZCv daif +PAN -UAO)
      pc : page_mapping+0x24/0x144
      lr : __dump_page+0x34/0x3dc
      sp : ffff00003a5cfd10
      x29: ffff00003a5cfd10 x28: 000000000000802f
      x27: 0000000000000000 x26: 0000000000277d00
      x25: ffff000010791f56 x24: ffff7fe000000000
      x23: ffff000010772f8b x22: ffff00001125f670
      x21: ffff000011311000 x20: ffff000010772f8b
      x19: fffffffffffffffe x18: 0000000000000000
      x17: 0000000000000000 x16: 0000000000000000
      x15: 0000000000000000 x14: ffff802698b19600
      x13: ffff802698b1a200 x12: ffff802698b16f00
      x11: ffff802698b1a400 x10: 0000000000001400
      x9 : 0000000000000001 x8 : ffff00001121a000
      x7 : 0000000000000000 x6 : ffff0000102c53b8
      x5 : 0000000000000000 x4 : 0000000000000003
      x3 : 0000000000000100 x2 : 0000000000000000
      x1 : ffff000010772f8b x0 : ffffffffffffffff
      Process kmemleak (pid: 1408, stack limit = 0x(____ptrval____))
      Call trace:
       page_mapping+0x24/0x144
       __dump_page+0x34/0x3dc
       dump_page+0x28/0x4c
       kmemleak_scan+0x4ac/0x680
       kmemleak_scan_thread+0xb4/0xdc
       kthread+0x12c/0x13c
       ret_from_fork+0x10/0x18
      Code: d503201f f9400660 36000040 d1000413 (f9400661)
      ---[ end trace 4d4bd7f573490c8e ]---
      Kernel panic - not syncing: Fatal exception
      SMP: stopping secondary CPUs
      Kernel Offset: disabled
      CPU features: 0x002,20000c38
      Memory Limit: none
      ---[ end Kernel panic - not syncing: Fatal exception ]---
    
    Link: http://lkml.kernel.org/r/20190122132916.28360-1-cai@lca.pw
    Fixes: 9f1eb38e0e11 ("mm, kmemleak: little optimization while scanning")
    Signed-off-by: Qian Cai <cai@lca.pw>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 07da5c6c5ba0..368267c1b71b 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -21,14 +21,16 @@ struct vmem_altmap;
  * walkers which rely on the fully initialized page->flags and others
  * should use this rather than pfn_valid && pfn_to_page
  */
-#define pfn_to_online_page(pfn)				\
-({							\
-	struct page *___page = NULL;			\
-	unsigned long ___nr = pfn_to_section_nr(pfn);	\
-							\
-	if (___nr < NR_MEM_SECTIONS && online_section_nr(___nr))\
-		___page = pfn_to_page(pfn);		\
-	___page;					\
+#define pfn_to_online_page(pfn)					   \
+({								   \
+	struct page *___page = NULL;				   \
+	unsigned long ___pfn = pfn;				   \
+	unsigned long ___nr = pfn_to_section_nr(___pfn);	   \
+								   \
+	if (___nr < NR_MEM_SECTIONS && online_section_nr(___nr) && \
+	    pfn_valid_within(___pfn))				   \
+		___page = pfn_to_page(___pfn);			   \
+	___page;						   \
 })
 
 /*

commit 0614ce9776b037b6a08a9adcbfcc382c0053b178
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:38:13 2018 -0800

    include/linux/memory_hotplug.h: remove duplicate declaration of offline_pages()
    
    offline_pages() is already declared in this file.
    
    Just remove the duplicated one.
    
    Link: http://lkml.kernel.org/r/20181205031357.24769-1-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8ed6e09a5c0c..07da5c6c5ba0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -331,7 +331,6 @@ extern int arch_add_memory(int nid, u64 start, u64 size,
 		struct vmem_altmap *altmap, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
-extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern int sparse_add_one_section(int nid, unsigned long start_pfn,
 				  struct vmem_altmap *altmap);

commit 4e0d2e7ef14d9e1c900dac909db45263822b824f
Author: Wei Yang <richard.weiyang@gmail.com>
Date:   Fri Dec 28 00:37:06 2018 -0800

    mm, sparse: pass nid instead of pgdat to sparse_add_one_section()
    
    Since the information needed in sparse_add_one_section() is node id to
    allocate proper memory, it is not necessary to pass its pgdat.
    
    This patch changes the prototype of sparse_add_one_section() to pass node
    id directly.  This is intended to reduce misleading that
    sparse_add_one_section() would touch pgdat.
    
    Link: http://lkml.kernel.org/r/20181204085657.20472-2-richard.weiyang@gmail.com
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 9e4d9b9b93ea..8ed6e09a5c0c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -333,8 +333,8 @@ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
-extern int sparse_add_one_section(struct pglist_data *pgdat,
-		unsigned long start_pfn, struct vmem_altmap *altmap);
+extern int sparse_add_one_section(int nid, unsigned long start_pfn,
+				  struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,

commit 2c2a5af6fed20cf74401c9d64319c76c5ff81309
Author: Oscar Salvador <osalvador@suse.com>
Date:   Fri Dec 28 00:36:22 2018 -0800

    mm, memory_hotplug: add nid parameter to arch_remove_memory
    
    Patch series "Do not touch pages in hot-remove path", v2.
    
    This patchset aims for two things:
    
     1) A better definition about offline and hot-remove stage
     2) Solving bugs where we can access non-initialized pages
        during hot-remove operations [2] [3].
    
    This is achieved by moving all page/zone handling to the offline
    stage, so we do not need to access pages when hot-removing memory.
    
    [1] https://patchwork.kernel.org/cover/10691415/
    [2] https://patchwork.kernel.org/patch/10547445/
    [3] https://www.spinics.net/lists/linux-mm/msg161316.html
    
    This patch (of 5):
    
    This is a preparation for the following-up patches.  The idea of passing
    the nid is that it will allow us to get rid of the zone parameter
    afterwards.
    
    Link: http://lkml.kernel.org/r/20181127162005.15833-2-osalvador@suse.de
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 7383a7a76d69..9e4d9b9b93ea 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -107,8 +107,8 @@ static inline bool movable_node_is_enabled(void)
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-extern int arch_remove_memory(u64 start, u64 size,
-		struct vmem_altmap *altmap);
+extern int arch_remove_memory(int nid, u64 start, u64 size,
+				struct vmem_altmap *altmap);
 extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages, struct vmem_altmap *altmap);
 #endif /* CONFIG_MEMORY_HOTREMOVE */

commit f29d8e9c0191a2a02500945db505e5c89159c3f4
Author: David Hildenbrand <david@redhat.com>
Date:   Fri Dec 28 00:35:36 2018 -0800

    mm/memory_hotplug: drop "online" parameter from add_memory_resource()
    
    Userspace should always be in charge of how to online memory and if memory
    should be onlined automatically in the kernel.  Let's drop the parameter
    to overwrite this - XEN passes memhp_auto_online, just like add_memory(),
    so we can directly use that instead internally.
    
    Link: http://lkml.kernel.org/r/20181123123740.27652-1-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ffd9cd10fcf3..7383a7a76d69 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -326,7 +326,7 @@ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);
-extern int add_memory_resource(int nid, struct resource *resource, bool online);
+extern int add_memory_resource(int nid, struct resource *resource);
 extern int arch_add_memory(int nid, u64 start, u64 size,
 		struct vmem_altmap *altmap, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,

commit 8df1d0e4a265f25dc1e7e7624ccdbcb4a6630c89
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Oct 30 15:10:24 2018 -0700

    mm/memory_hotplug: make add_memory() take the device_hotplug_lock
    
    add_memory() currently does not take the device_hotplug_lock, however
    is aleady called under the lock from
            arch/powerpc/platforms/pseries/hotplug-memory.c
            drivers/acpi/acpi_memhotplug.c
    to synchronize against CPU hot-remove and similar.
    
    In general, we should hold the device_hotplug_lock when adding memory to
    synchronize against online/offline request (e.g.  from user space) - which
    already resulted in lock inversions due to device_lock() and
    mem_hotplug_lock - see 30467e0b3be ("mm, hotplug: fix concurrent memory
    hot-add deadlock").  add_memory()/add_memory_resource() will create memory
    block devices, so this really feels like the right thing to do.
    
    Holding the device_hotplug_lock makes sure that a memory block device
    can really only be accessed (e.g. via .online/.state) from user space,
    once the memory has been fully added to the system.
    
    The lock is not held yet in
            drivers/xen/balloon.c
            arch/powerpc/platforms/powernv/memtrace.c
            drivers/s390/char/sclp_cmd.c
            drivers/hv/hv_balloon.c
    So, let's either use the locked variants or take the lock.
    
    Don't export add_memory_resource(), as it once was exported to be used by
    XEN, which is never built as a module.  If somebody requires it, we also
    have to export a locked variant (as device_hotplug_lock is never
    exported).
    
    Link: http://lkml.kernel.org/r/20180925091457.28651-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Rashmica Gupta <rashmica.g@gmail.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Cc: John Allen <jallen@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Cc: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 1f096852f479..ffd9cd10fcf3 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -324,6 +324,7 @@ static inline void __remove_memory(int nid, u64 start, u64 size) {}
 extern void __ref free_area_init_core_hotplug(int nid);
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
+extern int __add_memory(int nid, u64 start, u64 size);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource, bool online);
 extern int arch_add_memory(int nid, u64 start, u64 size,

commit d15e59260f62bd5e0f625cf5f5240f6ffac78ab6
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Oct 30 15:10:18 2018 -0700

    mm/memory_hotplug: make remove_memory() take the device_hotplug_lock
    
    Patch series "mm: online/offline_pages called w.o. mem_hotplug_lock", v3.
    
    Reading through the code and studying how mem_hotplug_lock is to be used,
    I noticed that there are two places where we can end up calling
    device_online()/device_offline() - online_pages()/offline_pages() without
    the mem_hotplug_lock.  And there are other places where we call
    device_online()/device_offline() without the device_hotplug_lock.
    
    While e.g.
            echo "online" > /sys/devices/system/memory/memory9/state
    is fine, e.g.
            echo 1 > /sys/devices/system/memory/memory9/online
    Will not take the mem_hotplug_lock. However the device_lock() and
    device_hotplug_lock.
    
    E.g.  via memory_probe_store(), we can end up calling
    add_memory()->online_pages() without the device_hotplug_lock.  So we can
    have concurrent callers in online_pages().  We e.g.  touch in
    online_pages() basically unprotected zone->present_pages then.
    
    Looks like there is a longer history to that (see Patch #2 for details),
    and fixing it to work the way it was intended is not really possible.  We
    would e.g.  have to take the mem_hotplug_lock in device/base/core.c, which
    sounds wrong.
    
    Summary: We had a lock inversion on mem_hotplug_lock and device_lock().
    More details can be found in patch 3 and patch 6.
    
    I propose the general rules (documentation added in patch 6):
    
    1. add_memory/add_memory_resource() must only be called with
       device_hotplug_lock.
    2. remove_memory() must only be called with device_hotplug_lock. This is
       already documented and holds for all callers.
    3. device_online()/device_offline() must only be called with
       device_hotplug_lock. This is already documented and true for now in core
       code. Other callers (related to memory hotplug) have to be fixed up.
    4. mem_hotplug_lock is taken inside of add_memory/remove_memory/
       online_pages/offline_pages.
    
    To me, this looks way cleaner than what we have right now (and easier to
    verify).  And looking at the documentation of remove_memory, using
    lock_device_hotplug also for add_memory() feels natural.
    
    This patch (of 6):
    
    remove_memory() is exported right now but requires the
    device_hotplug_lock, which is not exported.  So let's provide a variant
    that takes the lock and only export that one.
    
    The lock is already held in
            arch/powerpc/platforms/pseries/hotplug-memory.c
            drivers/acpi/acpi_memhotplug.c
            arch/powerpc/platforms/powernv/memtrace.c
    
    Apart from that, there are not other users in the tree.
    
    Link: http://lkml.kernel.org/r/20180925091457.28651-2-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Rashmica Gupta <rashmica.g@gmail.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Rashmica Gupta <rashmica.g@gmail.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Cc: John Allen <jallen@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 34a28227068d..1f096852f479 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -301,6 +301,7 @@ extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern void remove_memory(int nid, u64 start, u64 size);
+extern void __remove_memory(int nid, u64 start, u64 size);
 
 #else
 static inline bool is_mem_section_removable(unsigned long pfn,
@@ -317,6 +318,7 @@ static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
 }
 
 static inline void remove_memory(int nid, u64 start, u64 size) {}
+static inline void __remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 extern void __ref free_area_init_core_hotplug(int nid);
@@ -330,7 +332,6 @@ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
-extern void remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct pglist_data *pgdat,
 		unsigned long start_pfn, struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,

commit 03e85f9d5f1f8c74f127c5f7a87575d74a78d248
Author: Oscar Salvador <osalvador@suse.de>
Date:   Tue Aug 21 21:53:43 2018 -0700

    mm/page_alloc: Introduce free_area_init_core_hotplug
    
    Currently, whenever a new node is created/re-used from the memhotplug
    path, we call free_area_init_node()->free_area_init_core().  But there is
    some code that we do not really need to run when we are coming from such
    path.
    
    free_area_init_core() performs the following actions:
    
    1) Initializes pgdat internals, such as spinlock, waitqueues and more.
    2) Account # nr_all_pages and # nr_kernel_pages. These values are used later on
       when creating hash tables.
    3) Account number of managed_pages per zone, substracting dma_reserved and
       memmap pages.
    4) Initializes some fields of the zone structure data
    5) Calls init_currently_empty_zone to initialize all the freelists
    6) Calls memmap_init to initialize all pages belonging to certain zone
    
    When called from memhotplug path, free_area_init_core() only performs
    actions #1 and #4.
    
    Action #2 is pointless as the zones do not have any pages since either the
    node was freed, or we are re-using it, eitherway all zones belonging to
    this node should have 0 pages.  For the same reason, action #3 results
    always in manages_pages being 0.
    
    Action #5 and #6 are performed later on when onlining the pages:
     online_pages()->move_pfn_range_to_zone()->init_currently_empty_zone()
     online_pages()->move_pfn_range_to_zone()->memmap_init_zone()
    
    This patch does two things:
    
    First, moves the node/zone initializtion to their own function, so it
    allows us to create a small version of free_area_init_core, where we only
    perform:
    
    1) Initialization of pgdat internals, such as spinlock, waitqueues and more
    4) Initialization of some fields of the zone structure data
    
    These two functions are: pgdat_init_internals() and zone_init_internals().
    
    The second thing this patch does, is to introduce
    free_area_init_core_hotplug(), the memhotplug version of
    free_area_init_core():
    
    Currently, we call free_area_init_node() from the memhotplug path.  In
    there, we set some pgdat's fields, and call calculate_node_totalpages().
    calculate_node_totalpages() calculates the # of pages the node has.
    
    Since the node is either new, or we are re-using it, the zones belonging
    to this node should not have any pages, so there is no point to calculate
    this now.
    
    Actually, we re-set these values to 0 later on with the calls to:
    
    reset_node_managed_pages()
    reset_node_present_pages()
    
    The # of pages per node and the # of pages per zone will be calculated when
    onlining the pages:
    
    online_pages()->move_pfn_range()->move_pfn_range_to_zone()->resize_zone_range()
    online_pages()->move_pfn_range()->move_pfn_range_to_zone()->resize_pgdat_range()
    
    Also, since free_area_init_core/free_area_init_node will now only get called during early init, let us replace
    __paginginit with __init, so their code gets freed up.
    
    [osalvador@techadventures.net: fix section usage]
      Link: http://lkml.kernel.org/r/20180731101752.GA473@techadventures.net
    [osalvador@suse.de: v6]
      Link: http://lkml.kernel.org/r/20180801122348.21588-6-osalvador@techadventures.net
    Link: http://lkml.kernel.org/r/20180730101757.28058-5-osalvador@techadventures.net
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Pasha Tatashin <Pavel.Tatashin@microsoft.com>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4e9828cda7a2..34a28227068d 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -319,6 +319,7 @@ static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
 static inline void remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+extern void __ref free_area_init_core_hotplug(int nid);
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);

commit fb52bbaee598f58352d8732637ebe7013b2df79f
Author: Mathieu Malaterre <malat@debian.org>
Date:   Thu Jun 7 17:07:43 2018 -0700

    mm: move is_pageblock_removable_nolock() to mm/memory_hotplug.c
    
    is_pageblock_removable_nolock() is not used outside of
    mm/memory_hotplug.c.  Move it next to unique caller
    is_mem_section_removable() and make it static.
    
    Remove prototype in <linux/memory_hotplug.h> to silence gcc warning (W=1):
    
      mm/page_alloc.c:7704:6: warning: no previous prototype for `is_pageblock_removable_nolock' [-Wmissing-prototypes]
    
    Link: http://lkml.kernel.org/r/20180509190001.24789-1-malat@debian.org
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Suggested-by: Michal Hocko <mhocko@kernel.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 2b0265265c28..4e9828cda7a2 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -107,7 +107,6 @@ static inline bool movable_node_is_enabled(void)
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-extern bool is_pageblock_removable_nolock(struct page *page);
 extern int arch_remove_memory(u64 start, u64 size,
 		struct vmem_altmap *altmap);
 extern int __remove_pages(struct zone *zone, unsigned long start_pfn,

commit d883c6cf3b39f1f42506e82ad2779fb88004acf3
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Wed May 23 10:18:21 2018 +0900

    Revert "mm/cma: manage the memory of the CMA area by using the ZONE_MOVABLE"
    
    This reverts the following commits that change CMA design in MM.
    
     3d2054ad8c2d ("ARM: CMA: avoid double mapping to the CMA area if CONFIG_HIGHMEM=y")
    
     1d47a3ec09b5 ("mm/cma: remove ALLOC_CMA")
    
     bad8c6c0b114 ("mm/cma: manage the memory of the CMA area by using the ZONE_MOVABLE")
    
    Ville reported a following error on i386.
    
      Inode-cache hash table entries: 65536 (order: 6, 262144 bytes)
      microcode: microcode updated early to revision 0x4, date = 2013-06-28
      Initializing CPU#0
      Initializing HighMem for node 0 (000377fe:00118000)
      Initializing Movable for node 0 (00000001:00118000)
      BUG: Bad page state in process swapper  pfn:377fe
      page:f53effc0 count:0 mapcount:-127 mapping:00000000 index:0x0
      flags: 0x80000000()
      raw: 80000000 00000000 00000000 ffffff80 00000000 00000100 00000200 00000001
      page dumped because: nonzero mapcount
      Modules linked in:
      CPU: 0 PID: 0 Comm: swapper Not tainted 4.17.0-rc5-elk+ #145
      Hardware name: Dell Inc. Latitude E5410/03VXMC, BIOS A15 07/11/2013
      Call Trace:
       dump_stack+0x60/0x96
       bad_page+0x9a/0x100
       free_pages_check_bad+0x3f/0x60
       free_pcppages_bulk+0x29d/0x5b0
       free_unref_page_commit+0x84/0xb0
       free_unref_page+0x3e/0x70
       __free_pages+0x1d/0x20
       free_highmem_page+0x19/0x40
       add_highpages_with_active_regions+0xab/0xeb
       set_highmem_pages_init+0x66/0x73
       mem_init+0x1b/0x1d7
       start_kernel+0x17a/0x363
       i386_start_kernel+0x95/0x99
       startup_32_smp+0x164/0x168
    
    The reason for this error is that the span of MOVABLE_ZONE is extended
    to whole node span for future CMA initialization, and, normal memory is
    wrongly freed here.  I submitted the fix and it seems to work, but,
    another problem happened.
    
    It's so late time to fix the later problem so I decide to reverting the
    series.
    
    Reported-by: Ville Syrjl <ville.syrjala@linux.intel.com>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index e0e49b5b1ee1..2b0265265c28 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -216,6 +216,9 @@ void put_online_mems(void);
 void mem_hotplug_begin(void);
 void mem_hotplug_done(void);
 
+extern void set_zone_contiguous(struct zone *zone);
+extern void clear_zone_contiguous(struct zone *zone);
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 #define pfn_to_online_page(pfn)			\
 ({						\

commit bad8c6c0b1144694ecb0bc5629ede9b8b578b86e
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Apr 10 16:30:15 2018 -0700

    mm/cma: manage the memory of the CMA area by using the ZONE_MOVABLE
    
    Patch series "mm/cma: manage the memory of the CMA area by using the
    ZONE_MOVABLE", v2.
    
    0. History
    
    This patchset is the follow-up of the discussion about the "Introduce
    ZONE_CMA (v7)" [1].  Please reference it if more information is needed.
    
    1. What does this patch do?
    
    This patch changes the management way for the memory of the CMA area in
    the MM subsystem.  Currently the memory of the CMA area is managed by
    the zone where their pfn is belong to.  However, this approach has some
    problems since MM subsystem doesn't have enough logic to handle the
    situation that different characteristic memories are in a single zone.
    To solve this issue, this patch try to manage all the memory of the CMA
    area by using the MOVABLE zone.  In MM subsystem's point of view,
    characteristic of the memory on the MOVABLE zone and the memory of the
    CMA area are the same.  So, managing the memory of the CMA area by using
    the MOVABLE zone will not have any problem.
    
    2. Motivation
    
    There are some problems with current approach.  See following.  Although
    these problem would not be inherent and it could be fixed without this
    conception change, it requires many hooks addition in various code path
    and it would be intrusive to core MM and would be really error-prone.
    Therefore, I try to solve them with this new approach.  Anyway,
    following is the problems of the current implementation.
    
    o CMA memory utilization
    
    First, following is the freepage calculation logic in MM.
    
     - For movable allocation: freepage = total freepage
     - For unmovable allocation: freepage = total freepage - CMA freepage
    
    Freepages on the CMA area is used after the normal freepages in the zone
    where the memory of the CMA area is belong to are exhausted.  At that
    moment that the number of the normal freepages is zero, so
    
     - For movable allocation: freepage = total freepage = CMA freepage
     - For unmovable allocation: freepage = 0
    
    If unmovable allocation comes at this moment, allocation request would
    fail to pass the watermark check and reclaim is started.  After reclaim,
    there would exist the normal freepages so freepages on the CMA areas
    would not be used.
    
    FYI, there is another attempt [2] trying to solve this problem in lkml.
    And, as far as I know, Qualcomm also has out-of-tree solution for this
    problem.
    
    Useless reclaim:
    
    There is no logic to distinguish CMA pages in the reclaim path.  Hence,
    CMA page is reclaimed even if the system just needs the page that can be
    usable for the kernel allocation.
    
    Atomic allocation failure:
    
    This is also related to the fallback allocation policy for the memory of
    the CMA area.  Consider the situation that the number of the normal
    freepages is *zero* since the bunch of the movable allocation requests
    come.  Kswapd would not be woken up due to following freepage
    calculation logic.
    
    - For movable allocation: freepage = total freepage = CMA freepage
    
    If atomic unmovable allocation request comes at this moment, it would
    fails due to following logic.
    
    - For unmovable allocation: freepage = total freepage - CMA freepage = 0
    
    It was reported by Aneesh [3].
    
    Useless compaction:
    
    Usual high-order allocation request is unmovable allocation request and
    it cannot be served from the memory of the CMA area.  In compaction,
    migration scanner try to migrate the page in the CMA area and make
    high-order page there.  As mentioned above, it cannot be usable for the
    unmovable allocation request so it's just waste.
    
    3. Current approach and new approach
    
    Current approach is that the memory of the CMA area is managed by the
    zone where their pfn is belong to.  However, these memory should be
    distinguishable since they have a strong limitation.  So, they are
    marked as MIGRATE_CMA in pageblock flag and handled specially.  However,
    as mentioned in section 2, the MM subsystem doesn't have enough logic to
    deal with this special pageblock so many problems raised.
    
    New approach is that the memory of the CMA area is managed by the
    MOVABLE zone.  MM already have enough logic to deal with special zone
    like as HIGHMEM and MOVABLE zone.  So, managing the memory of the CMA
    area by the MOVABLE zone just naturally work well because constraints
    for the memory of the CMA area that the memory should always be
    migratable is the same with the constraint for the MOVABLE zone.
    
    There is one side-effect for the usability of the memory of the CMA
    area.  The use of MOVABLE zone is only allowed for a request with
    GFP_HIGHMEM && GFP_MOVABLE so now the memory of the CMA area is also
    only allowed for this gfp flag.  Before this patchset, a request with
    GFP_MOVABLE can use them.  IMO, It would not be a big issue since most
    of GFP_MOVABLE request also has GFP_HIGHMEM flag.  For example, file
    cache page and anonymous page.  However, file cache page for blockdev
    file is an exception.  Request for it has no GFP_HIGHMEM flag.  There is
    pros and cons on this exception.  In my experience, blockdev file cache
    pages are one of the top reason that causes cma_alloc() to fail
    temporarily.  So, we can get more guarantee of cma_alloc() success by
    discarding this case.
    
    Note that there is no change in admin POV since this patchset is just
    for internal implementation change in MM subsystem.  Just one minor
    difference for admin is that the memory stat for CMA area will be
    printed in the MOVABLE zone.  That's all.
    
    4. Result
    
    Following is the experimental result related to utilization problem.
    
    8 CPUs, 1024 MB, VIRTUAL MACHINE
    make -j16
    
    <Before>
      CMA area:               0 MB            512 MB
      Elapsed-time:           92.4          186.5
      pswpin:                 82            18647
      pswpout:                160           69839
    
    <After>
      CMA        :            0 MB            512 MB
      Elapsed-time:           93.1          93.4
      pswpin:                 84            46
      pswpout:                183           92
    
    akpm: "kernel test robot" reported a 26% improvement in
    vm-scalability.throughput:
    http://lkml.kernel.org/r/20180330012721.GA3845@yexl-desktop
    
    [1]: lkml.kernel.org/r/1491880640-9944-1-git-send-email-iamjoonsoo.kim@lge.com
    [2]: https://lkml.org/lkml/2014/10/15/623
    [3]: http://www.spinics.net/lists/linux-mm/msg100562.html
    
    Link: http://lkml.kernel.org/r/1512114786-5085-2-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Tested-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 2b0265265c28..e0e49b5b1ee1 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -216,9 +216,6 @@ void put_online_mems(void);
 void mem_hotplug_begin(void);
 void mem_hotplug_done(void);
 
-extern void set_zone_contiguous(struct zone *zone);
-extern void clear_zone_contiguous(struct zone *zone);
-
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 #define pfn_to_online_page(pfn)			\
 ({						\

commit 3a2d7fa8a3d5ae740bd0c21d933acc6220857ed0
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Apr 5 16:22:27 2018 -0700

    mm: disable interrupts while initializing deferred pages
    
    Vlastimil Babka reported about a window issue during which when deferred
    pages are initialized, and the current version of on-demand
    initialization is finished, allocations may fail.  While this is highly
    unlikely scenario, since this kind of allocation request must be large,
    and must come from interrupt handler, we still want to cover it.
    
    We solve this by initializing deferred pages with interrupts disabled,
    and holding node_size_lock spin lock while pages in the node are being
    initialized.  The on-demand deferred page initialization that comes
    later will use the same lock, and thus synchronize with
    deferred_init_memmap().
    
    It is unlikely for threads that initialize deferred pages to be
    interrupted.  They run soon after smp_init(), but before modules are
    initialized, and long before user space programs.  This is why there is
    no adverse effect of having these threads running with interrupts
    disabled.
    
    [pasha.tatashin@oracle.com: v6]
      Link: http://lkml.kernel.org/r/20180313182355.17669-2-pasha.tatashin@oracle.com
    Link: http://lkml.kernel.org/r/20180309220807.24961-2-pasha.tatashin@oracle.com
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven Sistare <steven.sistare@oracle.com>
    Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Gioh Kim <gi-oh.kim@profitbricks.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Miles Chen <miles.chen@mediatek.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index aba5f86eb038..2b0265265c28 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -51,24 +51,6 @@ enum {
 	MMOP_ONLINE_MOVABLE,
 };
 
-/*
- * pgdat resizing functions
- */
-static inline
-void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
-{
-	spin_lock_irqsave(&pgdat->node_size_lock, *flags);
-}
-static inline
-void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
-{
-	spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
-}
-static inline
-void pgdat_resize_init(struct pglist_data *pgdat)
-{
-	spin_lock_init(&pgdat->node_size_lock);
-}
 /*
  * Zone resizing functions
  *
@@ -246,13 +228,6 @@ extern void clear_zone_contiguous(struct zone *zone);
 	___page;				\
  })
 
-/*
- * Stub functions for when hotplug is off
- */
-static inline void pgdat_resize_lock(struct pglist_data *p, unsigned long *f) {}
-static inline void pgdat_resize_unlock(struct pglist_data *p, unsigned long *f) {}
-static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
-
 static inline unsigned zone_span_seqbegin(struct zone *zone)
 {
 	return 0;
@@ -293,6 +268,34 @@ static inline bool movable_node_is_enabled(void)
 }
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
+#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)
+/*
+ * pgdat resizing functions
+ */
+static inline
+void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
+{
+	spin_lock_irqsave(&pgdat->node_size_lock, *flags);
+}
+static inline
+void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
+{
+	spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
+}
+static inline
+void pgdat_resize_init(struct pglist_data *pgdat)
+{
+	spin_lock_init(&pgdat->node_size_lock);
+}
+#else /* !(CONFIG_MEMORY_HOTPLUG || CONFIG_DEFERRED_STRUCT_PAGE_INIT) */
+/*
+ * Stub functions for when hotplug is off
+ */
+static inline void pgdat_resize_lock(struct pglist_data *p, unsigned long *f) {}
+static inline void pgdat_resize_unlock(struct pglist_data *p, unsigned long *f) {}
+static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
+#endif /* !(CONFIG_MEMORY_HOTPLUG || CONFIG_DEFERRED_STRUCT_PAGE_INIT) */
+
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
 extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);

commit a99583e780c751003ac9c0105eec9a3b23ec3bc4
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:57 2017 +0100

    mm: pass the vmem_altmap to memmap_init_zone
    
    Pass the vmem_altmap two levels down instead of needing a lookup.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 20dd98ad44a0..aba5f86eb038 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -324,7 +324,7 @@ extern int add_memory_resource(int nid, struct resource *resource, bool online);
 extern int arch_add_memory(int nid, u64 start, u64 size,
 		struct vmem_altmap *altmap, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
-		unsigned long nr_pages);
+		unsigned long nr_pages, struct vmem_altmap *altmap);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);

commit 24b6d4164348370c6b6a58b4248babd85ff9e982
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:56 2017 +0100

    mm: pass the vmem_altmap to vmemmap_free
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking a few levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index e71927d0d46b..20dd98ad44a0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -331,7 +331,7 @@ extern void remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct pglist_data *pgdat,
 		unsigned long start_pfn, struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
-		unsigned long map_offset);
+		unsigned long map_offset, struct vmem_altmap *altmap);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 extern bool allow_online_pfn_range(int nid, unsigned long pfn, unsigned long nr_pages,

commit da024512a1fa5c979257e442130ee1d468285057
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:55 2017 +0100

    mm: pass the vmem_altmap to arch_remove_memory and __remove_pages
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking 2 levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index cbdd6d52e877..e71927d0d46b 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -126,9 +126,10 @@ static inline bool movable_node_is_enabled(void)
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
-extern int arch_remove_memory(u64 start, u64 size);
+extern int arch_remove_memory(u64 start, u64 size,
+		struct vmem_altmap *altmap);
 extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages);
+	unsigned long nr_pages, struct vmem_altmap *altmap);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /* reasonably generic interface to expand the physical pages */

commit 7b73d978a5d0d2a3637bdd57191cb6ffbad3feca
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:54 2017 +0100

    mm: pass the vmem_altmap to vmemmap_populate
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking a few levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index db276afbefcc..cbdd6d52e877 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -327,7 +327,8 @@ extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
-extern int sparse_add_one_section(struct pglist_data *pgdat, unsigned long start_pfn);
+extern int sparse_add_one_section(struct pglist_data *pgdat,
+		unsigned long start_pfn, struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 		unsigned long map_offset);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,

commit 24e6d5a59ac7d31adc0322de2d0117dfa370936f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:53 2017 +0100

    mm: pass the vmem_altmap to arch_add_memory and __add_pages
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking 2 levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 58e110aee7ab..db276afbefcc 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -13,6 +13,7 @@ struct pglist_data;
 struct mem_section;
 struct memory_block;
 struct resource;
+struct vmem_altmap;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 /*
@@ -131,18 +132,19 @@ extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /* reasonably generic interface to expand the physical pages */
-extern int __add_pages(int nid, unsigned long start_pfn,
-	unsigned long nr_pages, bool want_memblock);
+extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
+		struct vmem_altmap *altmap, bool want_memblock);
 
 #ifndef CONFIG_ARCH_HAS_ADD_PAGES
 static inline int add_pages(int nid, unsigned long start_pfn,
-			    unsigned long nr_pages, bool want_memblock)
+		unsigned long nr_pages, struct vmem_altmap *altmap,
+		bool want_memblock)
 {
-	return __add_pages(nid, start_pfn, nr_pages, want_memblock);
+	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
 }
 #else /* ARCH_HAS_ADD_PAGES */
-int add_pages(int nid, unsigned long start_pfn,
-	      unsigned long nr_pages, bool want_memblock);
+int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
+		struct vmem_altmap *altmap, bool want_memblock);
 #endif /* ARCH_HAS_ADD_PAGES */
 
 #ifdef CONFIG_NUMA
@@ -318,7 +320,8 @@ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource, bool online);
-extern int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock);
+extern int arch_add_memory(int nid, u64 start, u64 size,
+		struct vmem_altmap *altmap, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 0995e1a2b458..58e110aee7ab 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __LINUX_MEMORY_HOTPLUG_H
 #define __LINUX_MEMORY_HOTPLUG_H
 

commit 3072e413e305e353cd4654f8a57d953b66e85bf3
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Sep 8 16:11:39 2017 -0700

    mm/memory_hotplug: introduce add_pages
    
    There are new users of memory hotplug emerging.  Some of them require
    different subset of arch_add_memory.  There are some which only require
    allocation of struct pages without mapping those pages to the kernel
    address space.  We currently have __add_pages for that purpose.  But this
    is rather lowlevel and not very suitable for the code outside of the
    memory hotplug.  E.g.  x86_64 wants to update max_pfn which should be done
    by the caller.  Introduce add_pages() which should care about those
    details if they are needed.  Each architecture should define its
    implementation and select CONFIG_ARCH_HAS_ADD_PAGES.  All others use the
    currently existing __add_pages.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-7-jglisse@redhat.com
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Jrme Glisse <jglisse@redhat.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Evgeny Baskakov <ebaskakov@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mark Hairgrove <mhairgrove@nvidia.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Sherry Cheung <SCheung@nvidia.com>
    Cc: Subhash Gutti <sgutti@nvidia.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 5e6e4cc36ff4..0995e1a2b458 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -133,6 +133,17 @@ extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 extern int __add_pages(int nid, unsigned long start_pfn,
 	unsigned long nr_pages, bool want_memblock);
 
+#ifndef CONFIG_ARCH_HAS_ADD_PAGES
+static inline int add_pages(int nid, unsigned long start_pfn,
+			    unsigned long nr_pages, bool want_memblock)
+{
+	return __add_pages(nid, start_pfn, nr_pages, want_memblock);
+}
+#else /* ARCH_HAS_ADD_PAGES */
+int add_pages(int nid, unsigned long start_pfn,
+	      unsigned long nr_pages, bool want_memblock);
+#endif /* ARCH_HAS_ADD_PAGES */
+
 #ifdef CONFIG_NUMA
 extern int memory_add_physaddr_to_nid(u64 start);
 #else

commit e5e68930263377c6d4f6da0ff06f36b55d83a83f
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 6 16:19:37 2017 -0700

    mm, memory_hotplug: display allowed zones in the preferred ordering
    
    Prior to commit f1dd2cd13c4b ("mm, memory_hotplug: do not associate
    hotadded memory to zones until online") we used to allow to change the
    valid zone types of a memory block if it is adjacent to a different zone
    type.
    
    This fact was reflected in memoryNN/valid_zones by the ordering of
    printed zones.  The first one was default (echo online > memoryNN/state)
    and the other one could be onlined explicitly by online_{movable,kernel}.
    
    This behavior was removed by the said patch and as such the ordering was
    not all that important.  In most cases a kernel zone would be default
    anyway.  The only exception is movable_node handled by "mm,
    memory_hotplug: support movable_node for hotpluggable nodes".
    
    Let's reintroduce this behavior again because later patch will remove
    the zone overlap restriction and so user will be allowed to online
    kernel resp.  movable block regardless of its placement.  Original
    behavior will then become significant again because it would be
    non-trivial for users to see what is the default zone to online into.
    
    Implementation is really simple.  Pull out zone selection out of
    move_pfn_range into zone_for_pfn_range helper and use it in
    show_valid_zones to display the zone for default onlining and then both
    kernel and movable if they are allowed.  Default online zone is not
    duplicated.
    
    Link: http://lkml.kernel.org/r/20170714121233.16861-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Yasuaki Ishimatsu <yasu.isimatu@gmail.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Kani Toshimitsu <toshi.kani@hpe.com>
    Cc: <slaoub@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index c8a5056a5ae0..5e6e4cc36ff4 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -319,6 +319,6 @@ extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 extern bool allow_online_pfn_range(int nid, unsigned long pfn, unsigned long nr_pages,
 		int online_type);
-extern struct zone *default_zone_for_pfn(int nid, unsigned long pfn,
+extern struct zone *zone_for_pfn_range(int online_type, int nid, unsigned start_pfn,
 		unsigned long nr_pages);
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 4932381ee2a77a21641009149722e1bb92bd99e2
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:41:05 2017 -0700

    mm, memory_hotplug: move movable_node to the hotplug proper
    
    movable_node_is_enabled is defined in memblock proper while it is
    initialized from the memory hotplug proper.  This is quite messy and it
    makes a dependency between the two so move movable_node along with the
    helper functions to memory_hotplug.
    
    To make it more entertaining the kernel parameter is ignored unless
    CONFIG_HAVE_MEMBLOCK_NODE_MAP=y because we do not have the node
    information for each memblock otherwise.  So let's warn when the option
    is disabled.
    
    Link: http://lkml.kernel.org/r/20170529114141.536-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Yasuaki Ishimatsu <yasu.isimatu@gmail.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Kani Toshimitsu <toshi.kani@hpe.com>
    Cc: Chen Yucong <slaoub@gmail.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ed167541e4fc..c8a5056a5ae0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -115,6 +115,12 @@ extern void __online_page_free(struct page *page);
 extern int try_online_node(int nid);
 
 extern bool memhp_auto_online;
+/* If movable_node boot option specified */
+extern bool movable_node_enabled;
+static inline bool movable_node_is_enabled(void)
+{
+	return movable_node_enabled;
+}
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
@@ -266,6 +272,10 @@ static inline void put_online_mems(void) {}
 static inline void mem_hotplug_begin(void) {}
 static inline void mem_hotplug_done(void) {}
 
+static inline bool movable_node_is_enabled(void)
+{
+	return false;
+}
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit 559bfc7d1beff814a8e9999d102bf1157ef1f010
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:28 2017 -0700

    mm, memory_hotplug: remove unused cruft after memory hotplug rework
    
    zone_for_memory doesn't have any user anymore as well as the whole zone
    shifting infrastructure so drop them all.
    
    This shouldn't introduce any functional changes.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-15-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 780c806e17d3..ed167541e4fc 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -296,8 +296,6 @@ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource, bool online);
-extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
-		bool for_device);
 extern int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages);

commit 3d79a728f9b2e6ddcce4e02c91c4de1076548a4c
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:21 2017 -0700

    mm, memory_hotplug: replace for_device by want_memblock in arch_add_memory
    
    arch_add_memory gets for_device argument which then controls whether we
    want to create memblocks for created memory sections.  Simplify the
    logic by telling whether we want memblocks directly rather than going
    through pointless negation.  This also makes the api easier to
    understand because it is clear what we want rather than nothing telling
    for_device which can mean anything.
    
    This shouldn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-13-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4d65a2fcac15..780c806e17d3 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -298,7 +298,7 @@ extern int add_memory(int nid, u64 start, u64 size);
 extern int add_memory_resource(int nid, struct resource *resource, bool online);
 extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 		bool for_device);
-extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
+extern int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock);
 extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 		unsigned long nr_pages);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);

commit c246a213f5bad687c6c2cea27d7265eaf8f6f5d7
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:18 2017 -0700

    mm, memory_hotplug: do not assume ZONE_NORMAL is default kernel zone
    
    Heiko Carstens has noticed that he can generate overlapping zones for
    ZONE_DMA and ZONE_NORMAL:
    
      DMA      [mem 0x0000000000000000-0x000000007fffffff]
      Normal   [mem 0x0000000080000000-0x000000017fffffff]
    
      $ cat /sys/devices/system/memory/block_size_bytes
      10000000
      $ cat /sys/devices/system/memory/memory5/valid_zones
      DMA
      $ echo 0 > /sys/devices/system/memory/memory5/online
      $ cat /sys/devices/system/memory/memory5/valid_zones
      Normal
      $ echo 1 > /sys/devices/system/memory/memory5/online
      Normal
    
      $ cat /proc/zoneinfo
      Node 0, zone      DMA
      spanned  524288        <-----
      present  458752
      managed  455078
      start_pfn:           0 <-----
    
      Node 0, zone   Normal
      spanned  720896
      present  589824
      managed  571648
      start_pfn:           327680 <-----
    
    The reason is that we assume that the default zone for kernel onlining
    is ZONE_NORMAL.  This was a simplification introduced by the memory
    hotplug rework and it is easily fixable by checking the range overlap in
    the zone order and considering the first matching zone as the default
    one.  If there is no such zone then assume ZONE_NORMAL as we have been
    doing so far.
    
    Fixes: "mm, memory_hotplug: do not associate hotadded memory to zones until online"
    Link: http://lkml.kernel.org/r/20170601083746.4924-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8a07a49fd8dc..4d65a2fcac15 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -311,4 +311,6 @@ extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 extern bool allow_online_pfn_range(int nid, unsigned long pfn, unsigned long nr_pages,
 		int online_type);
+extern struct zone *default_zone_for_pfn(int nid, unsigned long pfn,
+		unsigned long nr_pages);
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit f1dd2cd13c4bbbc9a7c4617b3b034fa643de98fe
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:11 2017 -0700

    mm, memory_hotplug: do not associate hotadded memory to zones until online
    
    The current memory hotplug implementation relies on having all the
    struct pages associate with a zone/node during the physical hotplug
    phase (arch_add_memory->__add_pages->__add_section->__add_zone).  In the
    vast majority of cases this means that they are added to ZONE_NORMAL.
    This has been so since 9d99aaa31f59 ("[PATCH] x86_64: Support memory
    hotadd without sparsemem") and it wasn't a big deal back then because
    movable onlining didn't exist yet.
    
    Much later memory hotplug wanted to (ab)use ZONE_MOVABLE for movable
    onlining 511c2aba8f07 ("mm, memory-hotplug: dynamic configure movable
    memory and portion memory") and then things got more complicated.
    Rather than reconsidering the zone association which was no longer
    needed (because the memory hotplug already depended on SPARSEMEM) a
    convoluted semantic of zone shifting has been developed.  Only the
    currently last memblock or the one adjacent to the zone_movable can be
    onlined movable.  This essentially means that the online type changes as
    the new memblocks are added.
    
    Let's simulate memory hot online manually
      $ echo 0x100000000 > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory32/valid_zones
      Normal Movable
    
      $ echo $((0x100000000+(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      $ echo $((0x100000000+2*(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      $ echo online_movable > /sys/devices/system/memory/memory34/state
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable Normal
    
    This is an awkward semantic because an udev event is sent as soon as the
    block is onlined and an udev handler might want to online it based on
    some policy (e.g.  association with a node) but it will inherently race
    with new blocks showing up.
    
    This patch changes the physical online phase to not associate pages with
    any zone at all.  All the pages are just marked reserved and wait for
    the onlining phase to be associated with the zone as per the online
    request.  There are only two requirements
    
            - existing ZONE_NORMAL and ZONE_MOVABLE cannot overlap
    
            - ZONE_NORMAL precedes ZONE_MOVABLE in physical addresses
    
    the latter one is not an inherent requirement and can be changed in the
    future.  It preserves the current behavior and made the code slightly
    simpler.  This is subject to change in future.
    
    This means that the same physical online steps as above will lead to the
    following state: Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
    
    Implementation:
    The current move_pfn_range is reimplemented to check the above
    requirements (allow_online_pfn_range) and then updates the respective
    zone (move_pfn_range_to_zone), the pgdat and links all the pages in the
    pfn range with the zone/node.  __add_pages is updated to not require the
    zone and only initializes sections in the range.  This allowed to
    simplify the arch_add_memory code (s390 could get rid of quite some of
    code).
    
    devm_memremap_pages is the only user of arch_add_memory which relies on
    the zone association because it only hooks into the memory hotplug only
    half way.  It uses it to associate the new memory with ZONE_DEVICE but
    doesn't allow it to be {on,off}lined via sysfs.  This means that this
    particular code path has to call move_pfn_range_to_zone explicitly.
    
    The original zone shifting code is kept in place and will be removed in
    the follow up patch for an easier review.
    
    Please note that this patch also changes the original behavior when
    offlining a memory block adjacent to another zone (Normal vs.  Movable)
    used to allow to change its movable type.  This will be handled later.
    
    [richard.weiyang@gmail.com: simplify zone_intersects()]
      Link: http://lkml.kernel.org/r/20170616092335.5177-1-richard.weiyang@gmail.com
    [richard.weiyang@gmail.com: remove duplicate call for set_page_links]
      Link: http://lkml.kernel.org/r/20170616092335.5177-2-richard.weiyang@gmail.com
    [akpm@linux-foundation.org: remove unused local `i']
    Link: http://lkml.kernel.org/r/20170515085827.16474-12-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # For s390 bits
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index a61aede1b391..8a07a49fd8dc 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -123,8 +123,8 @@ extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
-/* reasonably generic interface to expand the physical pages in a zone  */
-extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
+/* reasonably generic interface to expand the physical pages */
+extern int __add_pages(int nid, unsigned long start_pfn,
 	unsigned long nr_pages, bool want_memblock);
 
 #ifdef CONFIG_NUMA
@@ -299,15 +299,16 @@ extern int add_memory_resource(int nid, struct resource *resource, bool online);
 extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 		bool for_device);
 extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
+extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
+		unsigned long nr_pages);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
-extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn);
+extern int sparse_add_one_section(struct pglist_data *pgdat, unsigned long start_pfn);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 		unsigned long map_offset);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
-extern bool zone_can_shift(unsigned long pfn, unsigned long nr_pages,
-			  enum zone_type target, int *zone_shift);
-
+extern bool allow_online_pfn_range(int nid, unsigned long pfn, unsigned long nr_pages,
+		int online_type);
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 2d070eab2e8270c8a84d480bb91e4f739315f03d
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:37:56 2017 -0700

    mm: consider zone which is not fully populated to have holes
    
    __pageblock_pfn_to_page has two users currently, set_zone_contiguous
    which checks whether the given zone contains holes and
    pageblock_pfn_to_page which then carefully returns a first valid page
    from the given pfn range for the given zone.  This doesn't handle zones
    which are not fully populated though.  Memory pageblocks can be offlined
    or might not have been onlined yet.  In such a case the zone should be
    considered to have holes otherwise pfn walkers can touch and play with
    offline pages.
    
    Current callers of pageblock_pfn_to_page in compaction seem to work
    properly right now because they only isolate PageBuddy
    (isolate_freepages_block) or PageLRU resp.  __PageMovable
    (isolate_migratepages_block) which will be always false for these pages.
    It would be safer to skip these pages altogether, though.
    
    In order to do this patch adds a new memory section state
    (SECTION_IS_ONLINE) which is set in memory_present (during boot time) or
    in online_pages_range during the memory hotplug.  Similarly
    offline_mem_sections clears the bit and it is called when the memory
    range is offlined.
    
    pfn_to_online_page helper is then added which check the mem section and
    only returns a page if it is onlined already.
    
    Use the new helper in __pageblock_pfn_to_page and skip the whole page
    block in such a case.
    
    [mhocko@suse.com: check valid section number in pfn_to_online_page (Vlastimil),
     mark sections online after all struct pages are initialized in
     online_pages_range (Vlastimil)]
      Link: http://lkml.kernel.org/r/20170518164210.GD18333@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/20170515085827.16474-8-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3c8cf86201c3..a61aede1b391 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -14,6 +14,20 @@ struct memory_block;
 struct resource;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
+/*
+ * Return page for the valid pfn only if the page is online. All pfn
+ * walkers which rely on the fully initialized page->flags and others
+ * should use this rather than pfn_valid && pfn_to_page
+ */
+#define pfn_to_online_page(pfn)				\
+({							\
+	struct page *___page = NULL;			\
+	unsigned long ___nr = pfn_to_section_nr(pfn);	\
+							\
+	if (___nr < NR_MEM_SECTIONS && online_section_nr(___nr))\
+		___page = pfn_to_page(pfn);		\
+	___page;					\
+})
 
 /*
  * Types for free bootmem stored in page->lru.next. These have to be in
@@ -203,6 +217,14 @@ extern void set_zone_contiguous(struct zone *zone);
 extern void clear_zone_contiguous(struct zone *zone);
 
 #else /* ! CONFIG_MEMORY_HOTPLUG */
+#define pfn_to_online_page(pfn)			\
+({						\
+	struct page *___page = NULL;		\
+	if (pfn_valid(pfn))			\
+		___page = pfn_to_page(pfn);	\
+	___page;				\
+ })
+
 /*
  * Stub functions for when hotplug is off
  */

commit 1b862aecfbd419cdc4553645bf86d07554279bed
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:37:45 2017 -0700

    mm, memory_hotplug: get rid of is_zone_device_section
    
    Device memory hotplug hooks into regular memory hotplug only half way.
    It needs memory sections to track struct pages but there is no
    need/desire to associate those sections with memory blocks and export
    them to the userspace via sysfs because they cannot be onlined anyway.
    
    This is currently expressed by for_device argument to arch_add_memory
    which then makes sure to associate the given memory range with
    ZONE_DEVICE.  register_new_memory then relies on is_zone_device_section
    to distinguish special memory hotplug from the regular one.  While this
    works now, later patches in this series want to move __add_zone outside
    of arch_add_memory path so we have to come up with something else.
    
    Add want_memblock down the __add_pages path and use it to control
    whether the section->memblock association should be done.
    arch_add_memory then just trivially want memblock for everything but
    for_device hotplug.
    
    remove_memory_section doesn't need is_zone_device_section either.  We
    can simply skip all the memblock specific cleanup if there is no
    memblock for the given section.
    
    This shouldn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 134a2f69c21a..3c8cf86201c3 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -111,7 +111,7 @@ extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages);
+	unsigned long nr_pages, bool want_memblock);
 
 #ifdef CONFIG_NUMA
 extern int memory_add_physaddr_to_nid(u64 start);

commit a96dfddbcc04336bbed50dc2b24823e45e09e80c
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Fri Feb 3 13:13:23 2017 -0800

    base/memory, hotplug: fix a kernel oops in show_valid_zones()
    
    Reading a sysfs "memoryN/valid_zones" file leads to the following oops
    when the first page of a range is not backed by struct page.
    show_valid_zones() assumes that 'start_pfn' is always valid for
    page_zone().
    
     BUG: unable to handle kernel paging request at ffffea017a000000
     IP: show_valid_zones+0x6f/0x160
    
    This issue may happen on x86-64 systems with 64GiB or more memory since
    their memory block size is bumped up to 2GiB.  [1] An example of such
    systems is desribed below.  0x3240000000 is only aligned by 1GiB and
    this memory block starts from 0x3200000000, which is not backed by
    struct page.
    
     BIOS-e820: [mem 0x0000003240000000-0x000000603fffffff] usable
    
    Since test_pages_in_a_zone() already checks holes, fix this issue by
    extending this function to return 'valid_start' and 'valid_end' for a
    given range.  show_valid_zones() then proceeds with the valid range.
    
    [1] 'Commit bdee237c0343 ("x86: mm: Use 2GB memory block size on
        large-memory x86-64 systems")'
    
    Link: http://lkml.kernel.org/r/20170127222149.30893-3-toshi.kani@hpe.com
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: <stable@vger.kernel.org>    [4.4+]
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index c1784c0b4f35..134a2f69c21a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -85,7 +85,8 @@ extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long, int);
-extern int test_pages_in_a_zone(unsigned long, unsigned long);
+extern int test_pages_in_a_zone(unsigned long start_pfn, unsigned long end_pfn,
+	unsigned long *valid_start, unsigned long *valid_end);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
 typedef void (*online_page_callback_t)(struct page *page);

commit 8a1f780e7f28c7c1d640118242cf68d528c456cd
Author: Yasuaki Ishimatsu <yasu.isimatu@gmail.com>
Date:   Tue Jan 24 15:17:45 2017 -0800

    memory_hotplug: make zone_can_shift() return a boolean value
    
    online_{kernel|movable} is used to change the memory zone to
    ZONE_{NORMAL|MOVABLE} and online the memory.
    
    To check that memory zone can be changed, zone_can_shift() is used.
    Currently the function returns minus integer value, plus integer
    value and 0. When the function returns minus or plus integer value,
    it means that the memory zone can be changed to ZONE_{NORNAL|MOVABLE}.
    
    But when the function returns 0, there are two meanings.
    
    One of the meanings is that the memory zone does not need to be changed.
    For example, when memory is in ZONE_NORMAL and onlined by online_kernel
    the memory zone does not need to be changed.
    
    Another meaning is that the memory zone cannot be changed. When memory
    is in ZONE_NORMAL and onlined by online_movable, the memory zone may
    not be changed to ZONE_MOVALBE due to memory online limitation(see
    Documentation/memory-hotplug.txt). In this case, memory must not be
    onlined.
    
    The patch changes the return type of zone_can_shift() so that memory
    online operation fails when memory zone cannot be changed as follows:
    
    Before applying patch:
       # grep -A 35 "Node 2" /proc/zoneinfo
       Node 2, zone   Normal
       <snip>
          node_scanned  0
               spanned  8388608
               present  7864320
               managed  7864320
       # echo online_movable > memory4097/state
       # grep -A 35 "Node 2" /proc/zoneinfo
       Node 2, zone   Normal
       <snip>
          node_scanned  0
               spanned  8388608
               present  8388608
               managed  8388608
    
       online_movable operation succeeded. But memory is onlined as
       ZONE_NORMAL, not ZONE_MOVABLE.
    
    After applying patch:
       # grep -A 35 "Node 2" /proc/zoneinfo
       Node 2, zone   Normal
       <snip>
          node_scanned  0
               spanned  8388608
               present  7864320
               managed  7864320
       # echo online_movable > memory4097/state
       bash: echo: write error: Invalid argument
       # grep -A 35 "Node 2" /proc/zoneinfo
       Node 2, zone   Normal
       <snip>
          node_scanned  0
               spanned  8388608
               present  7864320
               managed  7864320
    
       online_movable operation failed because of failure of changing
       the memory zone from ZONE_NORMAL to ZONE_MOVABLE
    
    Fixes: df429ac03936 ("memory-hotplug: more general validation of zone during online")
    Link: http://lkml.kernel.org/r/2f9c3837-33d7-b6e5-59c0-6ca4372b2d84@gmail.com
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Reviewed-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 01033fadea47..c1784c0b4f35 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -284,7 +284,7 @@ extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 		unsigned long map_offset);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
-extern int zone_can_shift(unsigned long pfn, unsigned long nr_pages,
-			  enum zone_type target);
+extern bool zone_can_shift(unsigned long pfn, unsigned long nr_pages,
+			  enum zone_type target, int *zone_shift);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit df429ac039360005299d56247647ca77098d660e
Author: Reza Arbab <arbab@linux.vnet.ibm.com>
Date:   Tue Jul 26 15:22:23 2016 -0700

    memory-hotplug: more general validation of zone during online
    
    When memory is onlined, we are only able to rezone from ZONE_MOVABLE to
    ZONE_KERNEL, or from (ZONE_MOVABLE - 1) to ZONE_MOVABLE.
    
    To be more flexible, use the following criteria instead; to online
    memory from zone X into zone Y,
    
    * Any zones between X and Y must be unused.
    * If X is lower than Y, the onlined memory must lie at the end of X.
    * If X is higher than Y, the onlined memory must lie at the start of X.
    
    Add zone_can_shift() to make this determination.
    
    Link: http://lkml.kernel.org/r/1462816419-4479-3-git-send-email-arbab@linux.vnet.ibm.com
    Signed-off-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Reviewd-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Andrew Banman <abanman@sgi.com>
    Cc: Chen Yucong <slaoub@gmail.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 5145620ba48a..01033fadea47 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -284,5 +284,7 @@ extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 		unsigned long map_offset);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
+extern int zone_can_shift(unsigned long pfn, unsigned long nr_pages,
+			  enum zone_type target);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 7ded384a12688c2a86b618da16bc87713404dfcc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 27 15:23:32 2016 -0700

    mm: fix section mismatch warning
    
    The register_page_bootmem_info_node() function needs to be marked __init
    in order to avoid a new warning introduced by commit f65e91df25aa ("mm:
    use early_pfn_to_nid in register_page_bootmem_info_node").
    
    Otherwise you'll get a warning about how a non-init function calls
    early_pfn_to_nid (which is __meminit)
    
    Cc: Yang Shi <yang.shi@linaro.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 20d8a5d4d133..5145620ba48a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -182,7 +182,7 @@ static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
 #endif /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 
 #ifdef CONFIG_HAVE_BOOTMEM_INFO_NODE
-extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
+extern void __init register_page_bootmem_info_node(struct pglist_data *pgdat);
 #else
 static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 {

commit c98940f6fa3d06fa8fec75aa2362b25227573d06
Author: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
Date:   Thu May 19 17:11:26 2016 -0700

    mm/memory_hotplug: is_mem_section_removable() can return bool
    
    Make is_mem_section_removable() return bool to improve readability due
    to this particular function only using either one or zero as its return
    value.
    
    Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index adbef586e696..20d8a5d4d133 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -247,16 +247,16 @@ static inline void mem_hotplug_done(void) {}
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
-extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
+extern bool is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern void remove_memory(int nid, u64 start, u64 size);
 
 #else
-static inline int is_mem_section_removable(unsigned long pfn,
+static inline bool is_mem_section_removable(unsigned long pfn,
 					unsigned long nr_pages)
 {
-	return 0;
+	return false;
 }
 
 static inline void try_offline_node(int nid) {}

commit 7cf91a98e607c2f935dbcc177d70011e95b8faff
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Mar 15 14:57:51 2016 -0700

    mm/compaction: speed up pageblock_pfn_to_page() when zone is contiguous
    
    There is a performance drop report due to hugepage allocation and in
    there half of cpu time are spent on pageblock_pfn_to_page() in
    compaction [1].
    
    In that workload, compaction is triggered to make hugepage but most of
    pageblocks are un-available for compaction due to pageblock type and
    skip bit so compaction usually fails.  Most costly operations in this
    case is to find valid pageblock while scanning whole zone range.  To
    check if pageblock is valid to compact, valid pfn within pageblock is
    required and we can obtain it by calling pageblock_pfn_to_page().  This
    function checks whether pageblock is in a single zone and return valid
    pfn if possible.  Problem is that we need to check it every time before
    scanning pageblock even if we re-visit it and this turns out to be very
    expensive in this workload.
    
    Although we have no way to skip this pageblock check in the system where
    hole exists at arbitrary position, we can use cached value for zone
    continuity and just do pfn_to_page() in the system where hole doesn't
    exist.  This optimization considerably speeds up in above workload.
    
    Before vs After
      Max: 1096 MB/s vs 1325 MB/s
      Min: 635 MB/s 1015 MB/s
      Avg: 899 MB/s 1194 MB/s
    
    Avg is improved by roughly 30% [2].
    
    [1]: http://www.spinics.net/lists/linux-mm/msg97378.html
    [2]: https://lkml.org/lkml/2015/12/9/23
    
    [akpm@linux-foundation.org: don't forget to restore zone->contiguous on error path, per Vlastimil]
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Reported-by: Aaron Lu <aaron.lu@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Tested-by: Aaron Lu <aaron.lu@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 769d76870550..adbef586e696 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -198,6 +198,9 @@ void put_online_mems(void);
 void mem_hotplug_begin(void);
 void mem_hotplug_done(void);
 
+extern void set_zone_contiguous(struct zone *zone);
+extern void clear_zone_contiguous(struct zone *zone);
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off

commit 31bc3858ea3ebcc3157b3f5f0e624c5962f5a7a6
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Tue Mar 15 14:56:48 2016 -0700

    memory-hotplug: add automatic onlining policy for the newly added memory
    
    Currently, all newly added memory blocks remain in 'offline' state
    unless someone onlines them, some linux distributions carry special udev
    rules like:
    
      SUBSYSTEM=="memory", ACTION=="add", ATTR{state}=="offline", ATTR{state}="online"
    
    to make this happen automatically.  This is not a great solution for
    virtual machines where memory hotplug is being used to address high
    memory pressure situations as such onlining is slow and a userspace
    process doing this (udev) has a chance of being killed by the OOM killer
    as it will probably require to allocate some memory.
    
    Introduce default policy for the newly added memory blocks in
    /sys/devices/system/memory/auto_online_blocks file with two possible
    values: "offline" which preserves the current behavior and "online"
    which causes all newly added memory blocks to go online as soon as
    they're added.  The default is "offline".
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 43405992d027..769d76870550 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -99,6 +99,8 @@ extern void __online_page_free(struct page *page);
 
 extern int try_online_node(int nid);
 
+extern bool memhp_auto_online;
+
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
 extern int arch_remove_memory(u64 start, u64 size);
@@ -267,7 +269,7 @@ static inline void remove_memory(int nid, u64 start, u64 size) {}
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
-extern int add_memory_resource(int nid, struct resource *resource);
+extern int add_memory_resource(int nid, struct resource *resource, bool online);
 extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 		bool for_device);
 extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);

commit 4b94ffdc4163bae1ec73b6e977ffb7a7da3d06d3
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jan 15 16:56:22 2016 -0800

    x86, mm: introduce vmem_altmap to augment vmemmap_populate()
    
    In support of providing struct page for large persistent memory
    capacities, use struct vmem_altmap to change the default policy for
    allocating memory for the memmap array.  The default vmemmap_populate()
    allocates page table storage area from the page allocator.  Given
    persistent memory capacities relative to DRAM it may not be feasible to
    store the memmap in 'System Memory'.  Instead vmem_altmap represents
    pre-allocated "device pages" to satisfy vmemmap_alloc_block_buf()
    requests.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reported-by: kbuild test robot <lkp@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 2ea574ff9714..43405992d027 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -275,7 +275,8 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn);
-extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);
+extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
+		unsigned long map_offset);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 

commit 62cedb9f135794ec26a93ae29e5f0231ab263c84
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Jun 25 16:35:49 2015 +0100

    mm: memory hotplug with an existing resource
    
    Add add_memory_resource() to add memory using an existing "System RAM"
    resource.  This is useful if the memory region is being located by
    finding a free resource slot with allocate_resource().
    
    Xen guests will make use of this in their balloon driver to hotplug
    arbitrary amounts of memory in response to toolstack requests.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Daniel Kiper <daniel.kiper@oracle.com>
    Reviewed-by: Tang Chen <tangchen@cn.fujitsu.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8f60e899b33c..2ea574ff9714 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -11,6 +11,7 @@ struct zone;
 struct pglist_data;
 struct mem_section;
 struct memory_block;
+struct resource;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 
@@ -266,6 +267,7 @@ static inline void remove_memory(int nid, u64 start, u64 size) {}
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
+extern int add_memory_resource(int nid, struct resource *resource);
 extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
 		bool for_device);
 extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);

commit 033fbae988fcb67e5077203512181890848b8e90
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Aug 9 15:29:06 2015 -0400

    mm: ZONE_DEVICE for "device memory"
    
    While pmem is usable as a block device or via DAX mappings to userspace
    there are several usage scenarios that can not target pmem due to its
    lack of struct page coverage. In preparation for "hot plugging" pmem
    into the vmemmap add ZONE_DEVICE as a new zone to tag these pages
    separately from the ones that are subject to standard page allocations.
    Importantly "device memory" can be removed at will by userspace
    unbinding the driver of the device.
    
    Having a separate zone prevents allocation and otherwise marks these
    pages that are distinct from typical uniform memory.  Device memory has
    different lifetime and performance characteristics than RAM.  However,
    since we have run out of ZONES_SHIFT bits this functionality currently
    depends on sacrificing ZONE_DMA.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Jerome Glisse <j.glisse@gmail.com>
    [hch: various simplifications in the arch interface]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 6ffa0ac7f7d6..8f60e899b33c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -266,8 +266,9 @@ static inline void remove_memory(int nid, u64 start, u64 size) {}
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
-extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default);
-extern int arch_add_memory(int nid, u64 start, u64 size);
+extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
+		bool for_device);
+extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);

commit 30467e0b3be83c286d60039f8267dd421128ca74
Author: David Rientjes <rientjes@google.com>
Date:   Tue Apr 14 15:45:11 2015 -0700

    mm, hotplug: fix concurrent memory hot-add deadlock
    
    There's a deadlock when concurrently hot-adding memory through the probe
    interface and switching a memory block from offline to online.
    
    When hot-adding memory via the probe interface, add_memory() first takes
    mem_hotplug_begin() and then device_lock() is later taken when registering
    the newly initialized memory block.  This creates a lock dependency of (1)
    mem_hotplug.lock (2) dev->mutex.
    
    When switching a memory block from offline to online, dev->mutex is first
    grabbed in device_online() when the write(2) transitions an existing
    memory block from offline to online, and then online_pages() will take
    mem_hotplug_begin().
    
    This creates a lock inversion between mem_hotplug.lock and dev->mutex.
    Vitaly reports that this deadlock can happen when kworker handling a probe
    event races with systemd-udevd switching a memory block's state.
    
    This patch requires the state transition to take mem_hotplug_begin()
    before dev->mutex.  Hot-adding memory via the probe interface creates a
    memory block while holding mem_hotplug_begin(), there is no way to take
    dev->mutex first in this case.
    
    online_pages() and offline_pages() are only called when transitioning
    memory block state.  We now require that mem_hotplug_begin() is taken
    before calling them -- this requires exporting the mem_hotplug_begin() and
    mem_hotplug_done() to generic code.  In all hot-add and hot-remove cases,
    mem_hotplug_begin() is done prior to device_online().  This is all that is
    needed to avoid the deadlock.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Tested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8f1a41951df9..6ffa0ac7f7d6 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -192,6 +192,9 @@ extern void get_page_bootmem(unsigned long ingo, struct page *page,
 void get_online_mems(void);
 void put_online_mems(void);
 
+void mem_hotplug_begin(void);
+void mem_hotplug_done(void);
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -231,6 +234,9 @@ static inline int try_online_node(int nid)
 static inline void get_online_mems(void) {}
 static inline void put_online_mems(void) {}
 
+static inline void mem_hotplug_begin(void) {}
+static inline void mem_hotplug_done(void) {}
+
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit ed2f240094f900833ac06f533ab8bbcf0a1e8199
Author: Zhang Zhen <zhenzhang.zhang@huawei.com>
Date:   Thu Oct 9 15:26:31 2014 -0700

    memory-hotplug: add sysfs valid_zones attribute
    
    Currently memory-hotplug has two limits:
    
    1. If the memory block is in ZONE_NORMAL, you can change it to
       ZONE_MOVABLE, but this memory block must be adjacent to ZONE_MOVABLE.
    
    2. If the memory block is in ZONE_MOVABLE, you can change it to
       ZONE_NORMAL, but this memory block must be adjacent to ZONE_NORMAL.
    
    With this patch, we can easy to know a memory block can be onlined to
    which zone, and don't need to know the above two limits.
    
    Updated the related Documentation.
    
    [akpm@linux-foundation.org: use conventional comment layout]
    [akpm@linux-foundation.org: fix build with CONFIG_MEMORY_HOTREMOVE=n]
    [akpm@linux-foundation.org: remove unused local zone_prev]
    Signed-off-by: Zhang Zhen <zhenzhang.zhang@huawei.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index d9524c49d767..8f1a41951df9 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -84,6 +84,7 @@ extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long, int);
+extern int test_pages_in_a_zone(unsigned long, unsigned long);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
 typedef void (*online_page_callback_t)(struct page *page);

commit 6326440077a48d2c3b2993f3b3f2d969f09b6917
Author: Wang Nan <wangnan0@huawei.com>
Date:   Wed Aug 6 16:07:36 2014 -0700

    memory-hotplug: add zone_for_memory() for selecting zone for new memory
    
    This series of patches fixes a problem when adding memory in bad manner.
    For example: for a x86_64 machine booted with "mem=400M" and with 2GiB
    memory installed, following commands cause problem:
    
      # echo 0x40000000 > /sys/devices/system/memory/probe
     [   28.613895] init_memory_mapping: [mem 0x40000000-0x47ffffff]
      # echo 0x48000000 > /sys/devices/system/memory/probe
     [   28.693675] init_memory_mapping: [mem 0x48000000-0x4fffffff]
      # echo online_movable > /sys/devices/system/memory/memory9/state
      # echo 0x50000000 > /sys/devices/system/memory/probe
     [   29.084090] init_memory_mapping: [mem 0x50000000-0x57ffffff]
      # echo 0x58000000 > /sys/devices/system/memory/probe
     [   29.151880] init_memory_mapping: [mem 0x58000000-0x5fffffff]
      # echo online_movable > /sys/devices/system/memory/memory11/state
      # echo online> /sys/devices/system/memory/memory8/state
      # echo online> /sys/devices/system/memory/memory10/state
      # echo offline> /sys/devices/system/memory/memory9/state
     [   30.558819] Offlined Pages 32768
      # free
                  total       used       free     shared    buffers     cached
     Mem:        780588 18014398509432020     830552          0          0      51180
     -/+ buffers/cache: 18014398509380840     881732
     Swap:            0          0          0
    
    This is because the above commands probe higher memory after online a
    section with online_movable, which causes ZONE_HIGHMEM (or ZONE_NORMAL
    for systems without ZONE_HIGHMEM) overlaps ZONE_MOVABLE.
    
    After the second online_movable, the problem can be observed from
    zoneinfo:
    
      # cat /proc/zoneinfo
      ...
      Node 0, zone  Movable
        pages free     65491
              min      250
              low      312
              high     375
              scanned  0
              spanned  18446744073709518848
              present  65536
              managed  65536
      ...
    
    This series of patches solve the problem by checking ZONE_MOVABLE when
    choosing zone for new memory.  If new memory is inside or higher than
    ZONE_MOVABLE, makes it go there instead.
    
    After applying this series of patches, following are free and zoneinfo
    result (after offlining memory9):
    
      bash-4.2# free
                    total       used       free     shared    buffers     cached
       Mem:        780956      80112     700844          0          0      51180
       -/+ buffers/cache:      28932     752024
       Swap:            0          0          0
    
      bash-4.2# cat /proc/zoneinfo
    
      Node 0, zone      DMA
        pages free     3389
              min      14
              low      17
              high     21
              scanned  0
              spanned  4095
              present  3998
              managed  3977
          nr_free_pages 3389
      ...
        start_pfn:         1
        inactive_ratio:    1
      Node 0, zone    DMA32
        pages free     73724
              min      341
              low      426
              high     511
              scanned  0
              spanned  98304
              present  98304
              managed  92958
          nr_free_pages 73724
        ...
        start_pfn:         4096
        inactive_ratio:    1
      Node 0, zone   Normal
        pages free     32630
              min      120
              low      150
              high     180
              scanned  0
              spanned  32768
              present  32768
              managed  32768
          nr_free_pages 32630
      ...
        start_pfn:         262144
        inactive_ratio:    1
      Node 0, zone  Movable
        pages free     65476
              min      241
              low      301
              high     361
              scanned  0
              spanned  98304
              present  65536
              managed  65536
          nr_free_pages 65476
      ...
        start_pfn:         294912
        inactive_ratio:    1
    
    This patch (of 7):
    
    Introduce zone_for_memory() in arch independent code for
    arch_add_memory() use.
    
    Many arch_add_memory() function simply selects ZONE_HIGHMEM or
    ZONE_NORMAL and add new memory into it.  However, with the existance of
    ZONE_MOVABLE, the selection method should be carefully considered: if
    new, higher memory is added after ZONE_MOVABLE is setup, the default
    zone and ZONE_MOVABLE may overlap each other.
    
    should_add_memory_movable() checks the status of ZONE_MOVABLE.  If it
    has already contain memory, compare the address of new memory and
    movable memory.  If new memory is higher than movable, it should be
    added into ZONE_MOVABLE instead of default zone.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: "Mel Gorman" <mgorman@suse.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 79dd9eca054f..d9524c49d767 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -259,6 +259,7 @@ static inline void remove_memory(int nid, u64 start, u64 size) {}
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int add_memory(int nid, u64 start, u64 size);
+extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);

commit 4f7c6b49c45a398d72763d1f0e64ddff8b3653c7
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Aug 6 16:05:13 2014 -0700

    mem-hotplug: introduce MMOP_OFFLINE to replace the hard coding -1
    
    In store_mem_state(), we have:
    
      ...
      334         else if (!strncmp(buf, "offline", min_t(int, count, 7)))
      335                 online_type = -1;
      ...
      355         case -1:
      356                 ret = device_offline(&mem->dev);
      357                 break;
      ...
    
    Here, "offline" is hard coded as -1.
    
    This patch does the following renaming:
    
     ONLINE_KEEP     ->  MMOP_ONLINE_KEEP
     ONLINE_KERNEL   ->  MMOP_ONLINE_KERNEL
     ONLINE_MOVABLE  ->  MMOP_ONLINE_MOVABLE
    
    and introduces MMOP_OFFLINE = -1 to avoid hard coding.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Hu Tao <hutao@cn.fujitsu.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 010d125bffbf..79dd9eca054f 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -26,11 +26,12 @@ enum {
 	MEMORY_HOTPLUG_MAX_BOOTMEM_TYPE = NODE_INFO,
 };
 
-/* Types for control the zone type of onlined memory */
+/* Types for control the zone type of onlined and offlined memory */
 enum {
-	ONLINE_KEEP,
-	ONLINE_KERNEL,
-	ONLINE_MOVABLE,
+	MMOP_OFFLINE = -1,
+	MMOP_ONLINE_KEEP,
+	MMOP_ONLINE_KERNEL,
+	MMOP_ONLINE_MOVABLE,
 };
 
 /*

commit bfc8c90139ebd049b9801a951db3b9a4a00bed9c
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:07:18 2014 -0700

    mem-hotplug: implement get/put_online_mems
    
    kmem_cache_{create,destroy,shrink} need to get a stable value of
    cpu/node online mask, because they init/destroy/access per-cpu/node
    kmem_cache parts, which can be allocated or destroyed on cpu/mem
    hotplug.  To protect against cpu hotplug, these functions use
    {get,put}_online_cpus.  However, they do nothing to synchronize with
    memory hotplug - taking the slab_mutex does not eliminate the
    possibility of race as described in patch 2.
    
    What we need there is something like get_online_cpus, but for memory.
    We already have lock_memory_hotplug, which serves for the purpose, but
    it's a bit of a hammer right now, because it's backed by a mutex.  As a
    result, it imposes some limitations to locking order, which are not
    desirable, and can't be used just like get_online_cpus.  That's why in
    patch 1 I substitute it with get/put_online_mems, which work exactly
    like get/put_online_cpus except they block not cpu, but memory hotplug.
    
    [ v1 can be found at https://lkml.org/lkml/2014/4/6/68.  I NAK'ed it by
      myself, because it used an rw semaphore for get/put_online_mems,
      making them dead lock prune.  ]
    
    This patch (of 2):
    
    {un}lock_memory_hotplug, which is used to synchronize against memory
    hotplug, is currently backed by a mutex, which makes it a bit of a
    hammer - threads that only want to get a stable value of online nodes
    mask won't be able to proceed concurrently.  Also, it imposes some
    strong locking ordering rules on it, which narrows down the set of its
    usage scenarios.
    
    This patch introduces get/put_online_mems, which are the same as
    get/put_online_cpus, but for memory hotplug, i.e.  executing a code
    inside a get/put_online_mems section will guarantee a stable value of
    online nodes, present pages, etc.
    
    lock_memory_hotplug()/unlock_memory_hotplug() are removed altogether.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4ca3d951fe91..010d125bffbf 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -187,14 +187,8 @@ extern void put_page_bootmem(struct page *page);
 extern void get_page_bootmem(unsigned long ingo, struct page *page,
 			     unsigned long type);
 
-/*
- * Lock for memory hotplug guarantees 1) all callbacks for memory hotplug
- * notifier will be called under this. 2) offline/online/add/remove memory
- * will not run simultaneously.
- */
-
-void lock_memory_hotplug(void);
-void unlock_memory_hotplug(void);
+void get_online_mems(void);
+void put_online_mems(void);
 
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
@@ -232,8 +226,8 @@ static inline int try_online_node(int nid)
 	return 0;
 }
 
-static inline void lock_memory_hotplug(void) {}
-static inline void unlock_memory_hotplug(void) {}
+static inline void get_online_mems(void) {}
+static inline void put_online_mems(void) {}
 
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 

commit 85b35feaecd4d2284505b22708795bc1f03fc897
Author: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
Date:   Tue Nov 12 15:07:42 2013 -0800

    mm/sparsemem: use PAGES_PER_SECTION to remove redundant nr_pages parameter
    
    For below functions,
    
    - sparse_add_one_section()
    - kmalloc_section_memmap()
    - __kmalloc_section_memmap()
    - __kfree_section_memmap()
    
    they are always invoked to operate on one memory section, so it is
    redundant to always pass a nr_pages parameter, which is the page numbers
    in one section.  So we can directly use predefined macro PAGES_PER_SECTION
    instead of passing the parameter.
    
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 22203c293f07..4ca3d951fe91 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -268,8 +268,7 @@ extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
-extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
-								int nr_pages);
+extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);

commit 01b0f19707c51ef247404e6af1d4a97a11ba34f7
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Nov 12 15:07:25 2013 -0800

    cpu/mem hotplug: add try_online_node() for cpu_up()
    
    cpu_up() has #ifdef CONFIG_MEMORY_HOTPLUG code blocks, which call
    mem_online_node() to put its node online if offlined and then call
    build_all_zonelists() to initialize the zone list.
    
    These steps are specific to memory hotplug, and should be managed in
    mm/memory_hotplug.c.  lock_memory_hotplug() should also be held for the
    whole steps.
    
    For this reason, this patch replaces mem_online_node() with
    try_online_node(), which performs the whole steps with
    lock_memory_hotplug() held.  try_online_node() is named after
    try_offline_node() as they have similar purpose.
    
    There is no functional change in this patch.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index dd38e62b84d2..22203c293f07 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -94,6 +94,8 @@ extern void __online_page_set_limits(struct page *page);
 extern void __online_page_increment_counters(struct page *page);
 extern void __online_page_free(struct page *page);
 
+extern int try_online_node(int nid);
+
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
 extern int arch_remove_memory(u64 start, u64 size);
@@ -225,6 +227,11 @@ static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 {
 }
 
+static inline int try_online_node(int nid)
+{
+	return 0;
+}
+
 static inline void lock_memory_hotplug(void) {}
 static inline void unlock_memory_hotplug(void) {}
 
@@ -256,7 +263,6 @@ static inline void remove_memory(int nid, u64 start, u64 size) {}
 
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
-extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);

commit aba6efc47133af4941cda16e690f71b7ad894da2
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jun 1 22:24:07 2013 +0200

    Memory hotplug: Move alternative function definitions to header
    
    Move the definitions of offline_pages() and remove_memory()
    for CONFIG_MEMORY_HOTREMOVE to memory_hotplug.h, where they belong,
    and make them static inline.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 00569fb4ed6a..dd38e62b84d2 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -234,6 +234,8 @@ static inline void unlock_memory_hotplug(void) {}
 
 extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
 extern void try_offline_node(int nid);
+extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
+extern void remove_memory(int nid, u64 start, u64 size);
 
 #else
 static inline int is_mem_section_removable(unsigned long pfn,
@@ -243,6 +245,13 @@ static inline int is_mem_section_removable(unsigned long pfn,
 }
 
 static inline void try_offline_node(int nid) {}
+
+static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
+{
+	return -EINVAL;
+}
+
+static inline void remove_memory(int nid, u64 start, u64 size) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,

commit 242831eb15a06fa4414eaa705fdc6dd432ab98d1
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 27 12:58:46 2013 +0200

    Memory hotplug / ACPI: Simplify memory removal
    
    Now that the memory offlining should be taken care of by the
    companion device offlining code in acpi_scan_hot_remove(), the
    ACPI memory hotplug driver doesn't need to offline it in
    remove_memory() any more.  Moreover, since the return value of
    remove_memory() is not used, it's better to make it be a void
    function and trigger a BUG() if the memory scheduled for removal is
    not offline.
    
    Change the code in accordance with the above observations.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ae5480a00963..00569fb4ed6a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -252,7 +252,7 @@ extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
-extern int remove_memory(int nid, u64 start, u64 size);
+extern void remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);

commit ea50be59345a2b714fd3ed43e1bba89906c177c3
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu May 23 10:41:50 2013 +0200

    Driver core / MM: Drop offline_memory_block()
    
    Since offline_memory_block(mem) is functionally equivalent to
    device_offline(&mem->dev), make the only caller of the former use
    the latter instead and drop offline_memory_block() entirely.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Toshi Kani <toshi.kani@hp.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 2975b7b2a9d8..ae5480a00963 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -251,7 +251,6 @@ extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
-extern int offline_memory_block(struct memory_block *mem);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern int remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,

commit e2ff39400d81233374e780b133496a2296643d7d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed May 8 00:29:49 2013 +0200

    ACPI / memhotplug: Bind removable memory blocks to ACPI device nodes
    
    During ACPI memory hotplug configuration bind memory blocks residing
    in modules removable through the standard ACPI mechanism to struct
    acpi_device objects associated with ACPI namespace objects
    representing those modules.  Accordingly, unbind those memory blocks
    from the struct acpi_device objects when the memory modules in
    question are being removed.
    
    When "offline" operation for devices representing memory blocks is
    introduced, this will allow the ACPI core's device hot-remove code to
    use it to carry out remove_memory() for those memory blocks and check
    the results of that before it actually removes the modules holding
    them from the system.
    
    Since walk_memory_range() is used for accessing all memory blocks
    corresponding to a given ACPI namespace object, it is exported from
    memory_hotplug.c so that the code in acpi_memhotplug.c can use it.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Tested-by: Vasilis Liaskovitis <vasilis.liaskovitis@profitbricks.com>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3e622c610925..2975b7b2a9d8 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -245,6 +245,8 @@ static inline int is_mem_section_removable(unsigned long pfn,
 static inline void try_offline_node(int nid) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
+		void *arg, int (*func)(struct memory_block *, void *));
 extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);

commit 4edd7ceff0662afde195da6f6c43e7cbe1ed2dc4
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 29 15:08:22 2013 -0700

    mm, hotplug: avoid compiling memory hotremove functions when disabled
    
    __remove_pages() is only necessary for CONFIG_MEMORY_HOTREMOVE.  PowerPC
    pseries will return -EOPNOTSUPP if unsupported.
    
    Adding an #ifdef causes several other functions it depends on to also
    become unnecessary, which saves in .text when disabled (it's disabled in
    most defconfigs besides powerpc, including x86).  remove_memory_block()
    becomes static since it is not referenced outside of
    drivers/base/memory.c.
    
    Build tested on x86 and powerpc with CONFIG_MEMORY_HOTREMOVE both enabled
    and disabled.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index b6a3be7d47bf..3e622c610925 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -97,13 +97,13 @@ extern void __online_page_free(struct page *page);
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
 extern int arch_remove_memory(u64 start, u64 size);
+extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
+	unsigned long nr_pages);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
-extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages);
 
 #ifdef CONFIG_NUMA
 extern int memory_add_physaddr_to_nid(u64 start);

commit 90b30cdc1d87450e2ae89c8f8a29102dc2c1992e
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 16:33:27 2013 -0800

    memory-hotplug: export the function try_offline_node()
    
    try_offline_node() will be needed in the tristate
    drivers/acpi/processor_driver.c.
    
    The node will be offlined when all memory/cpu on the node have been
    hotremoved.  So we need the function try_offline_node() in cpu-hotplug
    path.
    
    If the memory-hotplug is disabled, and cpu-hotplug is enabled
    
    1. no memory no the node
       we don't online the node, and cpu's node is the nearest node.
    
    2. the node contains some memory
       the node has been onlined, and cpu's node is still needed
       to migrate the sleep task on the cpu to the same node.
    
    So we do nothing in try_offline_node() in this case.
    
    [rientjes@google.com: export the function try_offline_node() fix]
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Len Brown <lenb@kernel.org>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 69903ccf549e..b6a3be7d47bf 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -233,6 +233,7 @@ static inline void unlock_memory_hotplug(void) {}
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
 extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
+extern void try_offline_node(int nid);
 
 #else
 static inline int is_mem_section_removable(unsigned long pfn,
@@ -240,6 +241,8 @@ static inline int is_mem_section_removable(unsigned long pfn,
 {
 	return 0;
 }
+
+static inline void try_offline_node(int nid) {}
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 extern int mem_online_node(int nid);

commit 60a5a19e7419ba0bc22ed01b3285e8940b42944c
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Fri Feb 22 16:33:14 2013 -0800

    memory-hotplug: remove sysfs file of node
    
    Introduce a new function try_offline_node() to remove sysfs file of node
    when all memory sections of this node are removed.  If some memory
    sections of this node are not removed, this function does nothing.
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4d523fe75ba1..69903ccf549e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -248,7 +248,7 @@ extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int offline_memory_block(struct memory_block *mem);
 extern bool is_memblock_offlined(struct memory_block *mem);
-extern int remove_memory(u64 start, u64 size);
+extern int remove_memory(int nid, u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);

commit 46723bfa540f0a1e494476a1734d03626a0bd1e0
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Fri Feb 22 16:33:00 2013 -0800

    memory-hotplug: implement register_page_bootmem_info_section of sparse-vmemmap
    
    For removing memmap region of sparse-vmemmap which is allocated bootmem,
    memmap region of sparse-vmemmap needs to be registered by
    get_page_bootmem().  So the patch searches pages of virtual mapping and
    registers the pages by get_page_bootmem().
    
    NOTE: register_page_bootmem_memmap() is not implemented for ia64,
          ppc, s390, and sparc.  So introduce CONFIG_HAVE_BOOTMEM_INFO_NODE
          and revert register_page_bootmem_info_node() when platform doesn't
          support it.
    
          It's implemented by adding a new Kconfig option named
          CONFIG_HAVE_BOOTMEM_INFO_NODE, which will be automatically selected
          by memory-hotplug feature fully supported archs(currently only on
          x86_64).
    
          Since we have 2 config options called MEMORY_HOTPLUG and
          MEMORY_HOTREMOVE used for memory hot-add and hot-remove separately,
          and codes in function register_page_bootmem_info_node() are only
          used for collecting infomation for hot-remove, so reside it under
          MEMORY_HOTREMOVE.
    
          Besides page_isolation.c selected by MEMORY_ISOLATION under
          MEMORY_HOTPLUG is also such case, move it too.
    
    [mhocko@suse.cz: put register_page_bootmem_memmap inside CONFIG_MEMORY_HOTPLUG_SPARSE]
    [linfeng@cn.fujitsu.com: introduce CONFIG_HAVE_BOOTMEM_INFO_NODE and revert register_page_bootmem_info_node()]
    [mhocko@suse.cz: remove the arch specific functions without any implementation]
    [linfeng@cn.fujitsu.com: mm/Kconfig: move auto selects from MEMORY_HOTPLUG to MEMORY_HOTREMOVE as needed]
    [rientjes@google.com: fix defined but not used warning]
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Wu Jianguo <wujianguo@huawei.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Lin Feng <linfeng@cn.fujitsu.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 31a563bbd936..4d523fe75ba1 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -174,17 +174,16 @@ static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
 #endif /* CONFIG_NUMA */
 #endif /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 
-#ifdef CONFIG_SPARSEMEM_VMEMMAP
+#ifdef CONFIG_HAVE_BOOTMEM_INFO_NODE
+extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
+#else
 static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 {
 }
-static inline void put_page_bootmem(struct page *page)
-{
-}
-#else
-extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
-extern void put_page_bootmem(struct page *page);
 #endif
+extern void put_page_bootmem(struct page *page);
+extern void get_page_bootmem(unsigned long ingo, struct page *page,
+			     unsigned long type);
 
 /*
  * Lock for memory hotplug guarantees 1) all callbacks for memory hotplug

commit 24d335ca3606b610ec69c66a1e42760c96d89470
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 16:32:58 2013 -0800

    memory-hotplug: introduce new arch_remove_memory() for removing page table
    
    For removing memory, we need to remove page tables.  But it depends on
    architecture.  So the patch introduce arch_remove_memory() for removing
    page table.  Now it only calls __remove_pages().
    
    Note: __remove_pages() for some archtecuture is not implemented
          (I don't know how to implement it for s390).
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8dd0950a6a7a..31a563bbd936 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -96,6 +96,7 @@ extern void __online_page_free(struct page *page);
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
+extern int arch_remove_memory(u64 start, u64 size);
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
 /* reasonably generic interface to expand the physical pages in a zone  */

commit 6677e3eaf4d78abd7b09133414c05dc3ec353e7f
Author: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
Date:   Fri Feb 22 16:32:52 2013 -0800

    memory-hotplug: check whether all memory blocks are offlined or not when removing memory
    
    We remove the memory like this:
    
     1. lock memory hotplug
     2. offline a memory block
     3. unlock memory hotplug
     4. repeat 1-3 to offline all memory blocks
     5. lock memory hotplug
     6. remove memory(TODO)
     7. unlock memory hotplug
    
    All memory blocks must be offlined before removing memory.  But we don't
    hold the lock in the whole operation.  So we should check whether all
    memory blocks are offlined before step6.  Otherwise, kernel maybe
    panicked.
    
    Offlining a memory block and removing a memory device can be two
    different operations.  Users can just offline some memory blocks without
    removing the memory device.  For this purpose, the kernel has held
    lock_memory_hotplug() in __offline_pages().  To reuse the code for
    memory hot-remove, we repeat step 1-3 to offline all the memory blocks,
    repeatedly lock and unlock memory hotplug, but not hold the memory
    hotplug lock in the whole operation.
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4a45c4e50025..8dd0950a6a7a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -247,6 +247,7 @@ extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int offline_memory_block(struct memory_block *mem);
+extern bool is_memblock_offlined(struct memory_block *mem);
 extern int remove_memory(u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);

commit 511c2aba8f07fc45bdcba548cb63f7b8a450c6dc
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 11 16:03:16 2012 -0800

    mm, memory-hotplug: dynamic configure movable memory and portion memory
    
    Add online_movable and online_kernel for logic memory hotplug.  This is
    the dynamic version of "movablecore" & "kernelcore".
    
    We have the same reason to introduce it as to introduce "movablecore" &
    "kernelcore".  It has the same motive as "movablecore" & "kernelcore", but
    it is dynamic/running-time:
    
    o We can configure memory as kernelcore or movablecore after boot.
    
      Userspace workload is increased, we need more hugepage, we can't use
      "online_movable" to add memory and allow the system use more
      THP(transparent-huge-page), vice-verse when kernel workload is increase.
    
      Also help for virtualization to dynamic configure host/guest's memory,
      to save/(reduce waste) memory.
    
      Memory capacity on Demand
    
    o When a new node is physically online after boot, we need to use
      "online_movable" or "online_kernel" to configure/portion it as we
      expected when we logic-online it.
    
      This configuration also helps for physically-memory-migrate.
    
    o all benefit as the same as existed "movablecore" & "kernelcore".
    
    o Preparing for movable-node, which is very important for power-saving,
      hardware partitioning and high-available-system(hardware fault
      management).
    
    (Note, we don't introduce movable-node here.)
    
    Action behavior:
    When a memoryblock/memorysection is onlined by "online_movable", the kernel
    will not have directly reference to the page of the memoryblock,
    thus we can remove that memory any time when needed.
    
    When it is online by "online_kernel", the kernel can use it.
    When it is online by "online", the zone type doesn't changed.
    
    Current constraints:
    Only the memoryblock which is adjacent to the ZONE_MOVABLE
    can be online from ZONE_NORMAL to ZONE_MOVABLE.
    
    [akpm@linux-foundation.org: use min_t, cleanups]
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 95573ec4ee6c..4a45c4e50025 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -26,6 +26,13 @@ enum {
 	MEMORY_HOTPLUG_MAX_BOOTMEM_TYPE = NODE_INFO,
 };
 
+/* Types for control the zone type of onlined memory */
+enum {
+	ONLINE_KEEP,
+	ONLINE_KERNEL,
+	ONLINE_MOVABLE,
+};
+
 /*
  * pgdat resizing functions
  */
@@ -46,6 +53,10 @@ void pgdat_resize_init(struct pglist_data *pgdat)
 }
 /*
  * Zone resizing functions
+ *
+ * Note: any attempt to resize a zone should has pgdat_resize_lock()
+ * zone_span_writelock() both held. This ensure the size of a zone
+ * can't be changed while pgdat_resize_lock() held.
  */
 static inline unsigned zone_span_seqbegin(struct zone *zone)
 {
@@ -71,7 +82,7 @@ extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
 extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* VM interface that may be used by firmware interface */
-extern int online_pages(unsigned long, unsigned long);
+extern int online_pages(unsigned long, unsigned long, int);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
 typedef void (*online_page_callback_t)(struct page *page);

commit e90bdb7f52f94204c78fb40b0804645defdebd71
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Mon Oct 8 16:34:01 2012 -0700

    memory-hotplug: update memory block's state and notify userspace
    
    remove_memory() will be called when hot removing a memory device.  But
    even if offlining memory, we cannot notice it.  So the patch updates the
    memory block's state and sends notification to userspace.
    
    Additionally, the memory device may contain more than one memory block.
    If the memory block has been offlined, __offline_pages() will fail.  So we
    should try to offline one memory block at a time.
    
    Thus remove_memory() also check each memory block's state.  So there is no
    need to check the memory block's state before calling remove_memory().
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index e64fe80eba96..95573ec4ee6c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -10,6 +10,7 @@ struct page;
 struct zone;
 struct pglist_data;
 struct mem_section;
+struct memory_block;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 
@@ -234,6 +235,7 @@ extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
+extern int offline_memory_block(struct memory_block *mem);
 extern int remove_memory(u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);

commit a16cee10c7ab994546ed98d9abfd4de74050124a
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Mon Oct 8 16:33:58 2012 -0700

    memory-hotplug: preparation to notify memory block's state at memory hot remove
    
    remove_memory() is called in two cases:
    1. echo offline >/sys/devices/system/memory/memoryXX/state
    2. hot remove a memory device
    
    In the 1st case, the memory block's state is changed and the notification
    that memory block's state changed is sent to userland after calling
    remove_memory().  So user can notice memory block is changed.
    
    But in the 2nd case, the memory block's state is not changed and the
    notification is not also sent to userspcae even if calling
    remove_memory().  So user cannot notice memory block is changed.
    
    For adding the notification at memory hot remove, the patch just prepare
    as follows:
    1st case uses offline_pages() for offlining memory.
    2nd case uses remove_memory() for offlining memory and changing memory block's
        state and notifing the information.
    
    The patch does not implement notification to remove_memory().
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 910550f3b70e..e64fe80eba96 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -233,6 +233,7 @@ static inline int is_mem_section_removable(unsigned long pfn,
 extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
+extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern int remove_memory(u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 0b8e2a742600..910550f3b70e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -4,6 +4,7 @@
 #include <linux/mmzone.h>
 #include <linux/spinlock.h>
 #include <linux/notifier.h>
+#include <linux/bug.h>
 
 struct page;
 struct zone;

commit 9d0ad8ca43ce8023bb834a409c2258bd7197fb05
Author: Daniel Kiper <dkiper@net-space.pl>
Date:   Mon Jul 25 17:12:05 2011 -0700

    mm: extend memory hotplug API to allow memory hotplug in virtual machines
    
    This patch contains online_page_callback and apropriate functions for
    registering/unregistering online page callbacks.  It allows to do some
    machine specific tasks during online page stage which is required to
    implement memory hotplug in virtual machines.  Currently this patch is
    required by latest memory hotplug support for Xen balloon driver patch
    which will be posted soon.
    
    Additionally, originial online_page() function was splited into
    following functions doing "atomic" operations:
    
      - __online_page_set_limits() - set new limits for memory management code,
      - __online_page_increment_counters() - increment totalram_pages and totalhigh_pages,
      - __online_page_free() - free page to allocator.
    
    It was done to:
      - not duplicate existing code,
      - ease hotplug code devolpment by usage of well defined interface,
      - avoid stupid bugs which are unavoidable when the same code
        (by design) is developed in many places.
    
    [akpm@linux-foundation.org: use explicit indirect-call syntax]
    Signed-off-by: Daniel Kiper <dkiper@net-space.pl>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8122018d3000..0b8e2a742600 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -68,12 +68,19 @@ static inline void zone_seqlock_init(struct zone *zone)
 extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
 extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
-/* need some defines for these for archs that don't support it */
-extern void online_page(struct page *page);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
+typedef void (*online_page_callback_t)(struct page *page);
+
+extern int set_online_page_callback(online_page_callback_t callback);
+extern int restore_online_page_callback(online_page_callback_t callback);
+
+extern void __online_page_set_limits(struct page *page);
+extern void __online_page_increment_counters(struct page *page);
+extern void __online_page_free(struct page *page);
+
 #ifdef CONFIG_MEMORY_HOTREMOVE
 extern bool is_pageblock_removable_nolock(struct page *page);
 #endif /* CONFIG_MEMORY_HOTREMOVE */

commit 597fb188cbee2d371246e1669bbc6051bb666aa9
Merge: 38567333a6da 04d94879c8a4
Author: Pekka Enberg <penberg@kernel.org>
Date:   Sat Jan 15 13:28:17 2011 +0200

    Merge branch 'slub/hotplug' into slab/urgent

commit 5f24ce5fd34c3ca1b3d10d30da754732da64d5c0
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:47:00 2011 -0800

    thp: remove PG_buddy
    
    PG_buddy can be converted to _mapcount == -2.  So the PG_compound_lock can
    be added to page->flags without overflowing (because of the sparse section
    bits increasing) with CONFIG_X86_PAE=y and CONFIG_X86_PAT=y.  This also
    has to move the memory hotplug code from _mapcount to lru.next to avoid
    any risk of clashes.  We can't use lru.next for PG_buddy removal, but
    memory hotplug can use lru.next even more easily than the mapcount
    instead.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 31c237a00c48..24376fe7ee68 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -13,12 +13,16 @@ struct mem_section;
 #ifdef CONFIG_MEMORY_HOTPLUG
 
 /*
- * Types for free bootmem.
- * The normal smallest mapcount is -1. Here is smaller value than it.
+ * Types for free bootmem stored in page->lru.next. These have to be in
+ * some random range in unsigned long space for debugging purposes.
  */
-#define SECTION_INFO		(-1 - 1)
-#define MIX_SECTION_INFO	(-1 - 2)
-#define NODE_INFO		(-1 - 3)
+enum {
+	MEMORY_HOTPLUG_MIN_BOOTMEM_TYPE = 12,
+	SECTION_INFO = MEMORY_HOTPLUG_MIN_BOOTMEM_TYPE,
+	MIX_SECTION_INFO,
+	NODE_INFO,
+	MEMORY_HOTPLUG_MAX_BOOTMEM_TYPE = NODE_INFO,
+};
 
 /*
  * pgdat resizing functions

commit 925268a06dc2b1ff7bfcc37419a6827a0e739639
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Jan 11 16:44:01 2011 +0900

    memory hotplug: one more lock on memory hotplug
    
    Now, memory_hotplug_(un)lock() is used for add/remove/offline pages
    for avoiding races with hibernation. But this should be held in
    online_pages(), too. It seems asymmetric.
    
    There are cases where one has to avoid a race with memory hotplug
    notifier and his own local code, and hotplug v.s. hotplug.
    This will add a generic solution for avoiding races. In other view,
    having lock here has no big impacts. online pages is tend to be
    done by udev script at el against each memory section one by one.
    
    Then, it's better to have lock here, too.
    
    Cc: <stable@kernel.org> # 2.6.37
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Pekka Enberg <penberg@kernel.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 31c237a00c48..12b9eb5a36c3 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -161,6 +161,12 @@ extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
 extern void put_page_bootmem(struct page *page);
 #endif
 
+/*
+ * Lock for memory hotplug guarantees 1) all callbacks for memory hotplug
+ * notifier will be called under this. 2) offline/online/add/remove memory
+ * will not run simultaneously.
+ */
+
 void lock_memory_hotplug(void);
 void unlock_memory_hotplug(void);
 

commit 20d6c96b5f1cad5c5da4641945ec17a1d9a1afc8
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu Dec 2 14:31:19 2010 -0800

    mem-hotplug: introduce {un}lock_memory_hotplug()
    
    Presently hwpoison is using lock_system_sleep() to prevent a race with
    memory hotplug.  However lock_system_sleep() is a no-op if
    CONFIG_HIBERNATION=n.  Therefore we need a new lock.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Suggested-by: Hugh Dickins <hughd@google.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4307231bd22f..31c237a00c48 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -161,6 +161,9 @@ extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
 extern void put_page_bootmem(struct page *page);
 #endif
 
+void lock_memory_hotplug(void);
+void unlock_memory_hotplug(void);
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -192,6 +195,9 @@ static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 {
 }
 
+static inline void lock_memory_hotplug(void) {}
+static inline void unlock_memory_hotplug(void) {}
+
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit 49ac825587f33afec8841b7fab2eb4db775014e6
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 26 14:21:30 2010 -0700

    memory hotplug: unify is_removable and offline detection code
    
    Now, sysfs interface of memory hotplug shows whether the section is
    removable or not.  But it checks only migrateype of pages and doesn't
    check details of cluster of pages.
    
    Next, memory hotplug's set_migratetype_isolate() has the same kind of
    check, too.
    
    This patch adds the function __count_unmovable_pages() and makes above 2
    checks to use the same logic.  Then, is_removable and hotremove code uses
    the same logic.  No changes in the hotremove logic itself.
    
    TODO: need to find a way to check RECLAMABLE. But, considering bit,
          calling shrink_slab() against a range before starting memory hotremove
          sounds better. If so, this patch's logic doesn't need to be changed.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reported-by: Michal Hocko <mhocko@suse.cz>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 864035fb8f8a..4307231bd22f 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -70,6 +70,10 @@ extern void online_page(struct page *page);
 extern int online_pages(unsigned long, unsigned long);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
 
+#ifdef CONFIG_MEMORY_HOTREMOVE
+extern bool is_pageblock_removable_nolock(struct page *page);
+#endif /* CONFIG_MEMORY_HOTREMOVE */
+
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);

commit cf23422b9d76215316855253da491d4c9f294372
Author: minskey guo <chaohong_guo@linux.intel.com>
Date:   Mon May 24 14:32:41 2010 -0700

    cpu/mem hotplug: enable CPUs online before local memory online
    
    Enable users to online CPUs even if the CPUs belongs to a numa node which
    doesn't have onlined local memory.
    
    The zonlists(pg_data_t.node_zonelists[]) of a numa node are created either
    in system boot/init period, or at the time of local memory online.  For a
    numa node without onlined local memory, its zonelists are not initialized
    at present.  As a result, any memory allocation operations executed by
    CPUs within this node will fail.  In fact, an out-of-memory error is
    triggered when attempt to online CPUs before memory comes to online.
    
    This patch tries to create zonelists for such numa nodes, so that the
    memory allocation for this node can be fallback'ed to other nodes.
    
    [akpm@linux-foundation.org: remove unneeded export]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: minskey guo<chaohong.guo@intel.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 35b07b773e6c..864035fb8f8a 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -202,6 +202,7 @@ static inline int is_mem_section_removable(unsigned long pfn,
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);

commit b4e655a4aaa327810110457cef92681447dd13e4
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Dec 14 17:59:35 2009 -0800

    mm: memory_hotplug: make offline_pages() static
    
    It has no references outside memory_hotplug.c.
    
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index fed969281a41..35b07b773e6c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -69,7 +69,6 @@ extern void online_page(struct page *page);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
-extern int offline_pages(unsigned long, unsigned long, unsigned long);
 
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,

commit 908eedc6168bd92e89f90d89fa389065a36358fa
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Sep 22 16:45:46 2009 -0700

    walk system ram range
    
    Originally, walk_memory_resource() was introduced to traverse all memory
    of "System RAM" for detecting memory hotplug/unplug range.  For doing so,
    flags of IORESOUCE_MEM|IORESOURCE_BUSY was used and this was enough for
    memory hotplug.
    
    But for using other purpose, /proc/kcore, this may includes some firmware
    area marked as IORESOURCE_BUSY | IORESOUCE_MEM.  This patch makes the
    check strict to find out busy "System RAM".
    
    Note: PPC64 keeps their own walk_memory_resouce(), which walk through
    ppc64's lmb informaton.  Because old kclist_add() is called per lmb, this
    patch makes no difference in behavior, finally.
    
    And this patch removes CONFIG_MEMORY_HOTPLUG check from this function.
    Because pfn_valid() just show "there is memmap or not* and cannot be used
    for "there is physical memory or not", this function is useful in generic
    to scan physical memory range.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Amrico Wang <xiyou.wangcong@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Roland Dreier <rolandd@cisco.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index d95f72e79b82..fed969281a41 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -191,14 +191,6 @@ static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
-/*
- * Walk through all memory which is registered as resource.
- * arg is (start_pfn, nr_pages, private_arg_pointer)
- */
-extern int walk_memory_resource(unsigned long start_pfn,
-			unsigned long nr_pages, void *arg,
-			int (*func)(unsigned long, unsigned long, void *));
-
 #ifdef CONFIG_MEMORY_HOTREMOVE
 
 extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);

commit c04fc586c1a480ba198f03ae7b6cbd7b57380b91
Author: Gary Hade <garyhade@us.ibm.com>
Date:   Tue Jan 6 14:39:14 2009 -0800

    mm: show node to memory section relationship with symlinks in sysfs
    
    Show node to memory section relationship with symlinks in sysfs
    
    Add /sys/devices/system/node/nodeX/memoryY symlinks for all
    the memory sections located on nodeX.  For example:
    /sys/devices/system/node/node1/memory135 -> ../../memory/memory135
    indicates that memory section 135 resides on node1.
    
    Also revises documentation to cover this change as well as updating
    Documentation/ABI/testing/sysfs-devices-memory to include descriptions
    of memory hotremove files 'phys_device', 'phys_index', and 'state'
    that were previously not described there.
    
    In addition to it always being a good policy to provide users with
    the maximum possible amount of physical location information for
    resources that can be hot-added and/or hot-removed, the following
    are some (but likely not all) of the user benefits provided by
    this change.
    Immediate:
      - Provides information needed to determine the specific node
        on which a defective DIMM is located.  This will reduce system
        downtime when the node or defective DIMM is swapped out.
      - Prevents unintended onlining of a memory section that was
        previously offlined due to a defective DIMM.  This could happen
        during node hot-add when the user or node hot-add assist script
        onlines _all_ offlined sections due to user or script inability
        to identify the specific memory sections located on the hot-added
        node.  The consequences of reintroducing the defective memory
        could be ugly.
      - Provides information needed to vary the amount and distribution
        of memory on specific nodes for testing or debugging purposes.
    Future:
      - Will provide information needed to identify the memory
        sections that need to be offlined prior to physical removal
        of a specific node.
    
    Symlink creation during boot was tested on 2-node x86_64, 2-node
    ppc64, and 2-node ia64 systems.  Symlink creation during physical
    memory hot-add tested on a 2-node x86_64 system.
    
    Signed-off-by: Gary Hade <garyhade@us.ibm.com>
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 763ba81fc0f0..d95f72e79b82 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -72,7 +72,7 @@ extern void __offline_isolated_pages(unsigned long, unsigned long);
 extern int offline_pages(unsigned long, unsigned long, unsigned long);
 
 /* reasonably generic interface to expand the physical pages in a zone  */
-extern int __add_pages(struct zone *zone, unsigned long start_pfn,
+extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
 extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);

commit 5c755e9fd813810680abd56ec09a5f90143e815b
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Wed Jul 23 21:28:19 2008 -0700

    memory-hotplug: add sysfs removable attribute for hotplug memory remove
    
    Memory may be hot-removed on a per-memory-block basis, particularly on
    POWER where the SPARSEMEM section size often matches the memory-block
    size.  A user-level agent must be able to identify which sections of
    memory are likely to be removable before attempting the potentially
    expensive operation.  This patch adds a file called "removable" to the
    memory directory in sysfs to help such an agent.  In this patch, a memory
    block is considered removable if;
    
    o It contains only MOVABLE pageblocks
    o It contains only pageblocks with free pages regardless of pageblock type
    
    On the other hand, a memory block starting with a PageReserved() page will
    never be considered removable.  Without this patch, the user-agent is
    forced to choose a memory block to remove randomly.
    
    Sample output of the sysfs files:
    
    ./memory/memory0/removable: 0
    ./memory/memory1/removable: 0
    ./memory/memory2/removable: 0
    ./memory/memory3/removable: 0
    ./memory/memory4/removable: 0
    ./memory/memory5/removable: 0
    ./memory/memory6/removable: 0
    ./memory/memory7/removable: 1
    ./memory/memory8/removable: 0
    ./memory/memory9/removable: 0
    ./memory/memory10/removable: 0
    ./memory/memory11/removable: 0
    ./memory/memory12/removable: 0
    ./memory/memory13/removable: 0
    ./memory/memory14/removable: 0
    ./memory/memory15/removable: 0
    ./memory/memory16/removable: 0
    ./memory/memory17/removable: 1
    ./memory/memory18/removable: 1
    ./memory/memory19/removable: 1
    ./memory/memory20/removable: 1
    ./memory/memory21/removable: 1
    ./memory/memory22/removable: 1
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 3628e5088f64..763ba81fc0f0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -199,6 +199,18 @@ extern int walk_memory_resource(unsigned long start_pfn,
 			unsigned long nr_pages, void *arg,
 			int (*func)(unsigned long, unsigned long, void *));
 
+#ifdef CONFIG_MEMORY_HOTREMOVE
+
+extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
+
+#else
+static inline int is_mem_section_removable(unsigned long pfn,
+					unsigned long nr_pages)
+{
+	return 0;
+}
+#endif /* CONFIG_MEMORY_HOTREMOVE */
+
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);

commit af370fb8cb3031f20438f246798d5f0d98089f29
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Wed Jul 23 21:28:17 2008 -0700

    memory hotplug: small fixes to bootmem freeing for memory hotremove
    
    - Change some naming
      * Magic -> types
      * MIX_INFO -> MIX_SECTION_INFO
      * Change definition of bootmem type from direct hex value
    
    - __free_pages_bootmem() becomes __meminit.
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Johannes Weiner <hannes@saeurebad.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ea9f5ad9ec8e..3628e5088f64 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -13,12 +13,12 @@ struct mem_section;
 #ifdef CONFIG_MEMORY_HOTPLUG
 
 /*
- * Magic number for free bootmem.
+ * Types for free bootmem.
  * The normal smallest mapcount is -1. Here is smaller value than it.
  */
-#define SECTION_INFO		0xfffffffe
-#define MIX_INFO		0xfffffffd
-#define NODE_INFO		0xfffffffc
+#define SECTION_INFO		(-1 - 1)
+#define MIX_SECTION_INFO	(-1 - 2)
+#define NODE_INFO		(-1 - 3)
 
 /*
  * pgdat resizing functions

commit 0d5799449f0f373ca12681d86c941ae464146a37
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed Jun 4 08:30:54 2008 +1000

    [POWERPC] Make walk_memory_resource available with MEMORY_HOTPLUG=n
    
    The ehea driver was recently changed[1] to use walk_memory_resource() to
    detect the system's memory layout.  However, walk_memory_resource() is
    available only when memory hotplug is enabled.  So CONFIG_EHEA was
    made to depend on MEMORY_HOTPLUG [2], but it is inappropriate for a
    network driver to have such a dependency.
    
    Make the declaration of walk_memory_resource() and its powerpc
    implementation (ehea is powerpc-specific) unconditionally available.
    
    [1] 48cfb14f8b89d4d5b3df6c16f08b258686fb12ad
        "ehea: Add DLPAR memory remove support"
    
    [2] fb7b6ca2b6b7c23b52be143bdd5f55a23b9780c8
        "ehea: Add dependency to Kconfig"
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Acked-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 73e358612eaf..ea9f5ad9ec8e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -77,14 +77,6 @@ extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
 
-/*
- * Walk through all memory which is registered as resource.
- * arg is (start_pfn, nr_pages, private_arg_pointer)
- */
-extern int walk_memory_resource(unsigned long start_pfn,
-			unsigned long nr_pages, void *arg,
-			int (*func)(unsigned long, unsigned long, void *));
-
 #ifdef CONFIG_NUMA
 extern int memory_add_physaddr_to_nid(u64 start);
 #else
@@ -199,6 +191,14 @@ static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
 
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
+/*
+ * Walk through all memory which is registered as resource.
+ * arg is (start_pfn, nr_pages, private_arg_pointer)
+ */
+extern int walk_memory_resource(unsigned long start_pfn,
+			unsigned long nr_pages, void *arg,
+			int (*func)(unsigned long, unsigned long, void *));
+
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);

commit 04753278769f3b6c3b79a080edb52f21d83bf6e2
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Mon Apr 28 02:13:31 2008 -0700

    memory hotplug: register section/node id to free
    
    This patch set is to free pages which is allocated by bootmem for
    memory-hotremove.  Some structures of memory management are allocated by
    bootmem.  ex) memmap, etc.
    
    To remove memory physically, some of them must be freed according to
    circumstance.  This patch set makes basis to free those pages, and free
    memmaps.
    
    Basic my idea is using remain members of struct page to remember information
    of users of bootmem (section number or node id).  When the section is
    removing, kernel can confirm it.  By this information, some issues can be
    solved.
    
      1) When the memmap of removing section is allocated on other
         section by bootmem, it should/can be free.
      2) When the memmap of removing section is allocated on the
         same section, it shouldn't be freed. Because the section has to be
         logical memory offlined already and all pages must be isolated against
         page allocater. If it is freed, page allocator may use it which will
         be removed physically soon.
      3) When removing section has other section's memmap,
         kernel will be able to show easily which section should be removed
         before it for user. (Not implemented yet)
      4) When the above case 2), the page isolation will be able to check and skip
         memmap's page when logical memory offline (offline_pages()).
         Current page isolation code fails in this case because this page is
         just reserved page and it can't distinguish this pages can be
         removed or not. But, it will be able to do by this patch.
         (Not implemented yet.)
      5) The node information like pgdat has similar issues. But, this
         will be able to be solved too by this.
         (Not implemented yet, but, remembering node id in the pages.)
    
    Fortunately, current bootmem allocator just keeps PageReserved flags,
    and doesn't use any other members of page struct. The users of
    bootmem doesn't use them too.
    
    This patch:
    
    This is to register information which is node or section's id.  Kernel can
    distinguish which node/section uses the pages allcated by bootmem.  This is
    basis for hot-remove sections or nodes.
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yinghai Lu <yhlu.kernel@gmail.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index aca9c65f8d08..73e358612eaf 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -11,6 +11,15 @@ struct pglist_data;
 struct mem_section;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
+
+/*
+ * Magic number for free bootmem.
+ * The normal smallest mapcount is -1. Here is smaller value than it.
+ */
+#define SECTION_INFO		0xfffffffe
+#define MIX_INFO		0xfffffffd
+#define NODE_INFO		0xfffffffc
+
 /*
  * pgdat resizing functions
  */
@@ -145,6 +154,18 @@ static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
 #endif /* CONFIG_NUMA */
 #endif /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
+static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
+{
+}
+static inline void put_page_bootmem(struct page *page)
+{
+}
+#else
+extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
+extern void put_page_bootmem(struct page *page);
+#endif
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -172,6 +193,10 @@ static inline int mhp_notimplemented(const char *func)
 	return -ENOSYS;
 }
 
+static inline void register_page_bootmem_info_node(struct pglist_data *pgdat)
+{
+}
+
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 
 extern int add_memory(int nid, u64 start, u64 size);
@@ -180,5 +205,7 @@ extern int remove_memory(u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);
+extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
+					  unsigned long pnum);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit ea01ea937dcae2caa146dea1918cccf2f16ed3c4
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Mon Apr 28 02:12:01 2008 -0700

    hotplug memory remove: generic __remove_pages() support
    
    Generic helper function to remove section mappings and sysfs entries for the
    section of the memory we are removing.  offline_pages() correctly adjusted
    zone and marked the pages reserved.
    
    TODO: Yasunori Goto is working on patches to free up allocations from bootmem.
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 8fee7a45736b..aca9c65f8d08 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -8,6 +8,7 @@
 struct page;
 struct zone;
 struct pglist_data;
+struct mem_section;
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 /*
@@ -64,9 +65,11 @@ extern int offline_pages(unsigned long, unsigned long, unsigned long);
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
+extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
+	unsigned long nr_pages);
 
 /*
- * Walk thorugh all memory which is registered as resource.
+ * Walk through all memory which is registered as resource.
  * arg is (start_pfn, nr_pages, private_arg_pointer)
  */
 extern int walk_memory_resource(unsigned long start_pfn,
@@ -176,5 +179,6 @@ extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);
 extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
 								int nr_pages);
+extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 48e94196a533dbee17c252bf80d0310fb8c8c2eb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:14 2007 -0700

    fix memory hot remove not configured case.
    
    Now, arch dependent code around CONFIG_MEMORY_HOTREMOVE is a mess.
    This patch cleans up them. This is against 2.6.23-rc6-mm1.
    
     - fix compile failure on ia64/ CONFIG_MEMORY_HOTPLUG && !CONFIG_MEMORY_HOTREMOVE case.
     - For !CONFIG_MEMORY_HOTREMOVE, add generic no-op remove_memory(),
       which returns -EINVAL.
     - removed remove_pages() only used in powerpc.
     - removed no-op remove_memory() in i386, sh, sparc64, x86_64.
    
     - only powerpc returns -ENOSYS at memory hot remove(no-op). changes it
       to return -EINVAL.
    
    Note:
    Currently, only ia64 supports CONFIG_MEMORY_HOTREMOVE. I welcome other
    archs if there are requirements and testers.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 665951ef0390..8fee7a45736b 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -58,10 +58,9 @@ extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 extern void online_page(struct page *page);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long);
-#ifdef CONFIG_MEMORY_HOTREMOVE
-extern int offline_pages(unsigned long, unsigned long, unsigned long);
 extern void __offline_isolated_pages(unsigned long, unsigned long);
-#endif
+extern int offline_pages(unsigned long, unsigned long, unsigned long);
+
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
@@ -171,13 +170,6 @@ static inline int mhp_notimplemented(const char *func)
 }
 
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
-static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages)
-{
-	printk(KERN_WARNING "%s() called, not yet supported\n", __FUNCTION__);
-	dump_stack();
-	return -ENOSYS;
-}
 
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);

commit 0c0e6195896535481173df98935ad8db174f4d45
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:12 2007 -0700

    memory unplug: page offline
    
    Logic.
     - set all pages in  [start,end)  as isolated migration-type.
       by this, all free pages in the range will be not-for-use.
     - Migrate all LRU pages in the range.
     - Test all pages in the range's refcnt is zero or not.
    
    Todo:
     - allocate migration destination page from better area.
     - confirm page_count(page)== 0 && PageReserved(page) page is safe to be freed..
     (I don't like this kind of page but..
     - Find out pages which cannot be migrated.
     - more running tests.
     - Use reclaim for unplugging other memory type area.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 0a14dad95453..665951ef0390 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -58,7 +58,10 @@ extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 extern void online_page(struct page *page);
 /* VM interface that may be used by firmware interface */
 extern int online_pages(unsigned long, unsigned long);
-
+#ifdef CONFIG_MEMORY_HOTREMOVE
+extern int offline_pages(unsigned long, unsigned long, unsigned long);
+extern void __offline_isolated_pages(unsigned long, unsigned long);
+#endif
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);

commit 75884fb1c6388f3713ddcca662f3647b3129aaeb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:10 2007 -0700

    memory unplug: memory hotplug cleanup
    
    A clean up patch for "scanning memory resource [start, end)" operation.
    
    Now, find_next_system_ram() function is used in memory hotplug, but this
    interface is not easy to use and codes are complicated.
    
    This patch adds walk_memory_resouce(start,len,arg,func) function.
    The function 'func' is called per valid memory resouce range in [start,pfn).
    
    [pbadari@us.ibm.com: Error handling in walk_memory_resource()]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index b573d1ec871f..0a14dad95453 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -63,6 +63,14 @@ extern int online_pages(unsigned long, unsigned long);
 extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
 
+/*
+ * Walk thorugh all memory which is registered as resource.
+ * arg is (start_pfn, nr_pages, private_arg_pointer)
+ */
+extern int walk_memory_resource(unsigned long start_pfn,
+			unsigned long nr_pages, void *arg,
+			int (*func)(unsigned long, unsigned long, void *));
+
 #ifdef CONFIG_NUMA
 extern int memory_add_physaddr_to_nid(u64 start);
 #else

commit 39e91e433169bdfd5a312654e5988986662afd7f
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Tue Oct 16 01:24:30 2007 -0700

    Clean up duplicate includes in include/linux/memory_hotplug.h
    
    This patch cleans up duplicate includes in
            include/linux/memory_hotplug.h
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 7b54666cea8e..b573d1ec871f 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -3,7 +3,6 @@
 
 #include <linux/mmzone.h>
 #include <linux/spinlock.h>
-#include <linux/mmzone.h>
 #include <linux/notifier.h>
 
 struct page;

commit f28c5edc06ecd8068b38b7662ad19f4d20d741af
Author: Keith Mannthey <kmannth@us.ibm.com>
Date:   Sat Sep 30 23:27:04 2006 -0700

    [PATCH] hot-add-mem x86_64: fixup externs
    
    Fix up externs in memory_hotplug.c.  Cleanup.
    
    Signed-off-by: Keith Mannthey <kmannth@us.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 218501cfaeb9..7b54666cea8e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -172,5 +172,7 @@ static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
 extern int add_memory(int nid, u64 start, u64 size);
 extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);
+extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
+								int nr_pages);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit dd0932d9d4301bd58a4d5a634a3a8298c4fc5e24
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:40 2006 -0700

    [PATCH] pgdat allocation and update for ia64 of memory hotplug: allocate pgdat and per node data
    
    This is a patch to allocate pgdat and per node data area for ia64.  The size
    for them can be calculated by compute_pernodesize().
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 9b6260007e5e..218501cfaeb9 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -84,13 +84,8 @@ static inline int memory_add_physaddr_to_nid(u64 start)
  * Now, arch_free_nodedata() is just defined for error path of node_hot_add.
  *
  */
-static inline pg_data_t *arch_alloc_nodedata(int nid)
-{
-	return NULL;
-}
-static inline void arch_free_nodedata(pg_data_t *pgdat)
-{
-}
+extern pg_data_t *arch_alloc_nodedata(int nid);
+extern void arch_free_nodedata(pg_data_t *pgdat);
 extern void arch_refresh_nodedata(int nid, pg_data_t *pgdat);
 
 #else /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */

commit 7049027c6f0098eb6b23b8f6ca65a905541faf81
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:39 2006 -0700

    [PATCH] pgdat allocation and update for ia64 of memory hotplug: update pgdat address array
    
    This is to refresh node_data[] array for ia64.  As I mentioned previous
    patches, ia64 has copies of information of pgdat address array on each node as
    per node data.
    
    At v2 of node_add, this function used stop_machine_run() to update them.  (I
    wished that they were copied safety as much as possible.) But, in this patch,
    this arrays are just copied simply, and set node_online_map bit after
    completion of pgdat initialization.
    
    So, kernel must touch NODE_DATA() macro after checking node_online_map().
    (Current code has already done it.) This is more simple way for just
    hot-add.....
    
    Note : It will be problem when hot-remove will occur,
           because, even if online_map bit is set, kernel may
           touch NODE_DATA() due to race condition. :-(
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 569b1f6c27d1..9b6260007e5e 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -91,9 +91,7 @@ static inline pg_data_t *arch_alloc_nodedata(int nid)
 static inline void arch_free_nodedata(pg_data_t *pgdat)
 {
 }
-static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
-{
-}
+extern void arch_refresh_nodedata(int nid, pg_data_t *pgdat);
 
 #else /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 

commit 10ad400b49aca15ecf83b0fde7e35e4064b15c85
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:33 2006 -0700

    [PATCH] pgdat allocation for new node add (refresh node_data[])
    
    Refresh NODE_DATA() for generic archs.  In this case, NODE_DATA(nid) ==
    node_data[nid].  node_data[] is array of address of pgdat.  So, refresh is
    quite simple.
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index c6fd2c0323fc..569b1f6c27d1 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -91,6 +91,9 @@ static inline pg_data_t *arch_alloc_nodedata(int nid)
 static inline void arch_free_nodedata(pg_data_t *pgdat)
 {
 }
+static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
+{
+}
 
 #else /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 
@@ -114,6 +117,12 @@ static inline void arch_free_nodedata(pg_data_t *pgdat)
  */
 #define generic_free_nodedata(pgdat)	kfree(pgdat)
 
+extern pg_data_t *node_data[];
+static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
+{
+	node_data[nid] = pgdat;
+}
+
 #else /* !CONFIG_NUMA */
 
 /* never called */
@@ -125,6 +134,9 @@ static inline pg_data_t *generic_alloc_nodedata(int nid)
 static inline void generic_free_nodedata(pg_data_t *pgdat)
 {
 }
+static inline void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
+{
+}
 #endif /* CONFIG_NUMA */
 #endif /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
 

commit 306d6cbe86e2e6603ac3162e1294d5c75cfdeca6
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:32 2006 -0700

    [PATCH] pgdat allocation for new node add (generic alloc node_data)
    
    For node hotplug, basically we have to allocate new pgdat.  But, there are
    several types of implementations of pgdat.
    
    1. Allocate only pgdat.
       This style allocate only pgdat area.
       And its address is recorded in node_data[].
       It is most popular style.
    
    2. Static array of pgdat
       In this case, all of pgdats are static array.
       Some archs use this style.
    
    3. Allocate not only pgdat, but also per node data.
       To increase performance, each node has copy of some data as
       a per node data. So, this area must be allocated too.
    
       Ia64 is this style. Ia64 has the copies of node_data[] array
       on each per node data to increase performance.
    
    In this series of patches, treat (1) as generic arch.
    
    generic archs can use generic function. (2) and (3) should have
    its own if necessary.
    
    This patch defines pgdat allocator.
    Updating NODE_DATA() macro function is in other patch.
    
    Signed-off-by: Yasonori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 29c1472efad0..c6fd2c0323fc 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -73,6 +73,61 @@ static inline int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
+#ifdef CONFIG_HAVE_ARCH_NODEDATA_EXTENSION
+/*
+ * For supporting node-hotadd, we have to allocate a new pgdat.
+ *
+ * If an arch has generic style NODE_DATA(),
+ * node_data[nid] = kzalloc() works well. But it depends on the architecture.
+ *
+ * In general, generic_alloc_nodedata() is used.
+ * Now, arch_free_nodedata() is just defined for error path of node_hot_add.
+ *
+ */
+static inline pg_data_t *arch_alloc_nodedata(int nid)
+{
+	return NULL;
+}
+static inline void arch_free_nodedata(pg_data_t *pgdat)
+{
+}
+
+#else /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
+
+#define arch_alloc_nodedata(nid)	generic_alloc_nodedata(nid)
+#define arch_free_nodedata(pgdat)	generic_free_nodedata(pgdat)
+
+#ifdef CONFIG_NUMA
+/*
+ * If ARCH_HAS_NODEDATA_EXTENSION=n, this func is used to allocate pgdat.
+ * XXX: kmalloc_node() can't work well to get new node's memory at this time.
+ *	Because, pgdat for the new node is not allocated/initialized yet itself.
+ *	To use new node's memory, more consideration will be necessary.
+ */
+#define generic_alloc_nodedata(nid)				\
+({								\
+	kzalloc(sizeof(pg_data_t), GFP_KERNEL);			\
+})
+/*
+ * This definition is just for error path in node hotadd.
+ * For node hotremove, we have to replace this.
+ */
+#define generic_free_nodedata(pgdat)	kfree(pgdat)
+
+#else /* !CONFIG_NUMA */
+
+/* never called */
+static inline pg_data_t *generic_alloc_nodedata(int nid)
+{
+	BUG();
+	return NULL;
+}
+static inline void generic_free_nodedata(pg_data_t *pgdat)
+{
+}
+#endif /* CONFIG_NUMA */
+#endif /* CONFIG_HAVE_ARCH_NODEDATA_EXTENSION */
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off

commit bc02af93dd2bbddce1b55e0a493f833a1b7cf140
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:30 2006 -0700

    [PATCH] pgdat allocation for new node add (specify node id)
    
    Change the name of old add_memory() to arch_add_memory.  And use node id to
    get pgdat for the node at NODE_DATA().
    
    Note: Powerpc's old add_memory() is defined as __devinit. However,
          add_memory() is usually called only after bootup.
          I suppose it may be redundant. But, I'm not well known about powerpc.
          So, I keep it. (But, __meminit is better at least.)
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 911206386171..29c1472efad0 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -63,6 +63,16 @@ extern int online_pages(unsigned long, unsigned long);
 /* reasonably generic interface to expand the physical pages in a zone  */
 extern int __add_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages);
+
+#ifdef CONFIG_NUMA
+extern int memory_add_physaddr_to_nid(u64 start);
+#else
+static inline int memory_add_physaddr_to_nid(u64 start)
+{
+	return 0;
+}
+#endif
+
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -99,7 +109,8 @@ static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	return -ENOSYS;
 }
 
-extern int add_memory(u64 start, u64 size);
+extern int add_memory(int nid, u64 start, u64 size);
+extern int arch_add_memory(int nid, u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 72b38d436e4cd18185de11f4b48a6e62eb104644
Author: Adrian Bunk <bunk@stusta.de>
Date:   Thu Apr 20 02:43:23 2006 -0700

    [PATCH] memory_hotplug.h cleanup
    
    We don't have to #if guard prototypes.
    
    This also fixes a bug observed by Randy Dunlap due to a misspelled
    option in the #if.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4ca3e6ad03ec..911206386171 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -99,10 +99,7 @@ static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	return -ENOSYS;
 }
 
-#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_ACPI_HOTPLUG_MEMORY) \
-	|| defined(CONFIG_ACPI_HOTPLUG_MEMORY_MODULE)
 extern int add_memory(u64 start, u64 size);
 extern int remove_memory(u64 start, u64 size);
-#endif
 
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 9d99aaa31f5994d1923c3713ce9144c4c42332e1
Author: Andi Kleen <ak@suse.de>
Date:   Fri Apr 7 19:49:15 2006 +0200

    [PATCH] x86_64: Support memory hotadd without sparsemem
    
    Memory hotadd doesn't need SPARSEMEM, but can be handled by just preallocating
    mem_maps. This only needs some untangling of ifdefs to enable the necessary
    code even without SPARSEMEM.
    
    Originally from Keith Mannthey, hacked by AK.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 968b1aa3732c..4ca3e6ad03ec 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -58,8 +58,6 @@ extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
 /* need some defines for these for archs that don't support it */
 extern void online_page(struct page *page);
 /* VM interface that may be used by firmware interface */
-extern int add_memory(u64 start, u64 size);
-extern int remove_memory(u64 start, u64 size);
 extern int online_pages(unsigned long, unsigned long);
 
 /* reasonably generic interface to expand the physical pages in a zone  */
@@ -92,11 +90,6 @@ static inline int mhp_notimplemented(const char *func)
 	return -ENOSYS;
 }
 
-static inline int __add_pages(struct zone *zone, unsigned long start_pfn,
-	unsigned long nr_pages)
-{
-	return mhp_notimplemented(__FUNCTION__);
-}
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
 static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	unsigned long nr_pages)
@@ -105,4 +98,11 @@ static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
 	dump_stack();
 	return -ENOSYS;
 }
+
+#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_ACPI_HOTPLUG_MEMORY) \
+	|| defined(CONFIG_ACPI_HOTPLUG_MEMORY_MODULE)
+extern int add_memory(u64 start, u64 size);
+extern int remove_memory(u64 start, u64 size);
+#endif
+
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 78679302fe428f4f3dc853a51ee24f306010d874
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 6 15:42:49 2006 -0800

    [PATCH] memory-hotplug compile fix
    
    include/linux/memory_hotplug.h:53: warning: 'struct page' declared inside parameter list
    
    (akpm: I tossed in a couple more possibly-needed-sometime struct decls too)
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 01f03bc06eff..968b1aa3732c 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -6,6 +6,10 @@
 #include <linux/mmzone.h>
 #include <linux/notifier.h>
 
+struct page;
+struct zone;
+struct pglist_data;
+
 #ifdef CONFIG_MEMORY_HOTPLUG
 /*
  * pgdat resizing functions

commit 3947be1969a9ce455ec30f60ef51efb10e4323d1
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:54 2005 -0700

    [PATCH] memory hotplug: sysfs and add/remove functions
    
    This adds generic memory add/remove and supporting functions for memory
    hotplug into a new file as well as a memory hotplug kernel config option.
    
    Individual architecture patches will follow.
    
    For now, disable memory hotplug when swsusp is enabled.  There's a lot of
    churn there right now.  We'll fix it up properly once it calms down.
    
    Signed-off-by: Matt Tolentino <matthew.e.tolentino@intel.com>
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 4b08bc947578..01f03bc06eff 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -3,6 +3,8 @@
 
 #include <linux/mmzone.h>
 #include <linux/spinlock.h>
+#include <linux/mmzone.h>
+#include <linux/notifier.h>
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 /*
@@ -46,6 +48,19 @@ static inline void zone_seqlock_init(struct zone *zone)
 {
 	seqlock_init(&zone->span_seqlock);
 }
+extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
+extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
+extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
+/* need some defines for these for archs that don't support it */
+extern void online_page(struct page *page);
+/* VM interface that may be used by firmware interface */
+extern int add_memory(u64 start, u64 size);
+extern int remove_memory(u64 start, u64 size);
+extern int online_pages(unsigned long, unsigned long);
+
+/* reasonably generic interface to expand the physical pages in a zone  */
+extern int __add_pages(struct zone *zone, unsigned long start_pfn,
+	unsigned long nr_pages);
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -65,5 +80,25 @@ static inline int zone_span_seqretry(struct zone *zone, unsigned iv)
 static inline void zone_span_writelock(struct zone *zone) {}
 static inline void zone_span_writeunlock(struct zone *zone) {}
 static inline void zone_seqlock_init(struct zone *zone) {}
+
+static inline int mhp_notimplemented(const char *func)
+{
+	printk(KERN_WARNING "%s() called, with CONFIG_MEMORY_HOTPLUG disabled\n", func);
+	dump_stack();
+	return -ENOSYS;
+}
+
+static inline int __add_pages(struct zone *zone, unsigned long start_pfn,
+	unsigned long nr_pages)
+{
+	return mhp_notimplemented(__FUNCTION__);
+}
 #endif /* ! CONFIG_MEMORY_HOTPLUG */
+static inline int __remove_pages(struct zone *zone, unsigned long start_pfn,
+	unsigned long nr_pages)
+{
+	printk(KERN_WARNING "%s() called, not yet supported\n", __FUNCTION__);
+	dump_stack();
+	return -ENOSYS;
+}
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit bdc8cb984576ab5b550c8b24c6fa111a873503e3
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:53 2005 -0700

    [PATCH] memory hotplug locking: zone span seqlock
    
    See the "fixup bad_range()" patch for more information, but this actually
    creates a the lock to protect things making assumptions about a zone's size
    staying constant at runtime.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index e8103be9d528..4b08bc947578 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -16,13 +16,36 @@ void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
 static inline
 void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
 {
-	spin_lock_irqrestore(&pgdat->node_size_lock, *flags);
+	spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
 }
 static inline
 void pgdat_resize_init(struct pglist_data *pgdat)
 {
 	spin_lock_init(&pgdat->node_size_lock);
 }
+/*
+ * Zone resizing functions
+ */
+static inline unsigned zone_span_seqbegin(struct zone *zone)
+{
+	return read_seqbegin(&zone->span_seqlock);
+}
+static inline int zone_span_seqretry(struct zone *zone, unsigned iv)
+{
+	return read_seqretry(&zone->span_seqlock, iv);
+}
+static inline void zone_span_writelock(struct zone *zone)
+{
+	write_seqlock(&zone->span_seqlock);
+}
+static inline void zone_span_writeunlock(struct zone *zone)
+{
+	write_sequnlock(&zone->span_seqlock);
+}
+static inline void zone_seqlock_init(struct zone *zone)
+{
+	seqlock_init(&zone->span_seqlock);
+}
 #else /* ! CONFIG_MEMORY_HOTPLUG */
 /*
  * Stub functions for when hotplug is off
@@ -30,5 +53,17 @@ void pgdat_resize_init(struct pglist_data *pgdat)
 static inline void pgdat_resize_lock(struct pglist_data *p, unsigned long *f) {}
 static inline void pgdat_resize_unlock(struct pglist_data *p, unsigned long *f) {}
 static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
-#endif
+
+static inline unsigned zone_span_seqbegin(struct zone *zone)
+{
+	return 0;
+}
+static inline int zone_span_seqretry(struct zone *zone, unsigned iv)
+{
+	return 0;
+}
+static inline void zone_span_writelock(struct zone *zone) {}
+static inline void zone_span_writeunlock(struct zone *zone) {}
+static inline void zone_seqlock_init(struct zone *zone) {}
+#endif /* ! CONFIG_MEMORY_HOTPLUG */
 #endif /* __LINUX_MEMORY_HOTPLUG_H */

commit 208d54e5513c0c02d85af0990901354c74364d5c
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Sat Oct 29 18:16:52 2005 -0700

    [PATCH] memory hotplug locking: node_size_lock
    
    pgdat->node_size_lock is basically only neeeded in one place in the normal
    code: show_mem(), which is the arch-specific sysrq-m printing function.
    
    Strictly speaking, the architectures not doing memory hotplug do no need this
    locking in show_mem().  However, they are all included for completeness.  This
    should also make any future consolidation of all of the implementations a
    little more straightforward.
    
    This lock is also held in the sparsemem code during a memory removal, as
    sections are invalidated.  This is the place there pfn_valid() is made false
    for a memory area that's being removed.  The lock is only required when doing
    pfn_valid() operations on memory which the user does not already have a
    reference on the page, such as in show_mem().
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
new file mode 100644
index 000000000000..e8103be9d528
--- /dev/null
+++ b/include/linux/memory_hotplug.h
@@ -0,0 +1,34 @@
+#ifndef __LINUX_MEMORY_HOTPLUG_H
+#define __LINUX_MEMORY_HOTPLUG_H
+
+#include <linux/mmzone.h>
+#include <linux/spinlock.h>
+
+#ifdef CONFIG_MEMORY_HOTPLUG
+/*
+ * pgdat resizing functions
+ */
+static inline
+void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
+{
+	spin_lock_irqsave(&pgdat->node_size_lock, *flags);
+}
+static inline
+void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
+{
+	spin_lock_irqrestore(&pgdat->node_size_lock, *flags);
+}
+static inline
+void pgdat_resize_init(struct pglist_data *pgdat)
+{
+	spin_lock_init(&pgdat->node_size_lock);
+}
+#else /* ! CONFIG_MEMORY_HOTPLUG */
+/*
+ * Stub functions for when hotplug is off
+ */
+static inline void pgdat_resize_lock(struct pglist_data *p, unsigned long *f) {}
+static inline void pgdat_resize_unlock(struct pglist_data *p, unsigned long *f) {}
+static inline void pgdat_resize_init(struct pglist_data *pgdat) {}
+#endif
+#endif /* __LINUX_MEMORY_HOTPLUG_H */
