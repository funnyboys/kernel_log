commit 07325d4a90d2d84de45cc07b134fd0f023dbb971
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:16 2020 +0200

    rcu: Provide rcu_irq_exit_check_preempt()
    
    Provide a debug check which can be invoked from exception return to kernel
    mode before an attempt is made to schedule. Warn if RCU is not ready for
    this.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.089709607@linutronix.de

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index c869fb20cc51..8512caeb7682 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -72,6 +72,7 @@ static inline void rcu_irq_exit_irqson(void) { }
 static inline void rcu_irq_enter_irqson(void) { }
 static inline void rcu_irq_exit(void) { }
 static inline void rcu_irq_exit_preempt(void) { }
+static inline void rcu_irq_exit_check_preempt(void) { }
 static inline void exit_rcu(void) { }
 static inline bool rcu_preempt_need_deferred_qs(struct task_struct *t)
 {

commit b1fcf9b83c4149c63d1e0c699e85f93cbe28e211
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 12 09:44:43 2020 +0200

    rcu: Provide __rcu_is_watching()
    
    Same as rcu_is_watching() but without the preempt_disable/enable() pair
    inside the function. It is merked noinstr so it ends up in the
    non-instrumentable text section.
    
    This is useful for non-preemptible code especially in the low level entry
    section. Using rcu_is_watching() there results in a call to the
    preempt_schedule_notrace() thunk which triggers noinstr section warnings in
    objtool.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200512213810.518709291@linutronix.de

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 980eb78751d9..c869fb20cc51 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -86,6 +86,7 @@ static inline void rcu_scheduler_starting(void) { }
 static inline void rcu_end_inkernel_boot(void) { }
 static inline bool rcu_inkernel_boot_has_ended(void) { return true; }
 static inline bool rcu_is_watching(void) { return true; }
+static inline bool __rcu_is_watching(void) { return true; }
 static inline void rcu_momentary_dyntick_idle(void) { }
 static inline void kfree_rcu_scheduler_running(void) { }
 static inline bool rcu_gp_might_be_stalled(void) { return false; }

commit 8ae0ae6737ad449c8ae21e2bb01d9736f360a933
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 3 15:08:52 2020 +0200

    rcu: Provide rcu_irq_exit_preempt()
    
    Interrupts and exceptions invoke rcu_irq_enter() on entry and need to
    invoke rcu_irq_exit() before they either return to the interrupted code or
    invoke the scheduler due to preemption.
    
    The general assumption is that RCU idle code has to have preemption
    disabled so that a return from interrupt cannot schedule. So the return
    from interrupt code invokes rcu_irq_exit() and preempt_schedule_irq().
    
    If there is any imbalance in the rcu_irq/nmi* invocations or RCU idle code
    had preemption enabled then this goes unnoticed until the CPU goes idle or
    some other RCU check is executed.
    
    Provide rcu_irq_exit_preempt() which can be invoked from the
    interrupt/exception return code in case that preemption is enabled. It
    invokes rcu_irq_exit() and contains a few sanity checks in case that
    CONFIG_PROVE_RCU is enabled to catch such issues directly.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Paul E. McKenney <paulmck@kernel.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134904.364456424@linutronix.de

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 3465ba704a11..980eb78751d9 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -71,6 +71,7 @@ static inline void rcu_irq_enter(void) { }
 static inline void rcu_irq_exit_irqson(void) { }
 static inline void rcu_irq_enter_irqson(void) { }
 static inline void rcu_irq_exit(void) { }
+static inline void rcu_irq_exit_preempt(void) { }
 static inline void exit_rcu(void) { }
 static inline bool rcu_preempt_need_deferred_qs(struct task_struct *t)
 {

commit f736e0f1a55a88cb258b73da77463573739e9ac9
Merge: e2f3ccfa6200 6be7436d2245 e5a971d76d70 33b2b93bd831 3c80b4024579
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu May 7 10:18:32 2020 -0700

    Merge branches 'fixes.2020.04.27a', 'kfree_rcu.2020.04.27a', 'rcu-tasks.2020.04.27a', 'stall.2020.04.27a' and 'torture.2020.05.07a' into HEAD
    
    fixes.2020.04.27a:  Miscellaneous fixes.
    kfree_rcu.2020.04.27a:  Changes related to kfree_rcu().
    rcu-tasks.2020.04.27a:  Addition of new RCU-tasks flavors.
    stall.2020.04.27a:  RCU CPU stall-warning updates.
    torture.2020.05.07a:  Torture-test updates.

commit 43766c3eadcf6033c92eb953f88801aebac0f785
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Mar 16 20:38:29 2020 -0700

    rcu-tasks: Make RCU Tasks Trace make use of RCU scheduler hooks
    
    This commit makes the calls to rcu_tasks_qs() detect and report
    quiescent states for RCU tasks trace.  If the task is in a quiescent
    state and if ->trc_reader_checked is not yet set, the task sets its own
    ->trc_reader_checked.  This will cause the grace-period kthread to
    remove it from the holdout list if it still remains there.
    
    [ paulmck: Fix conditional compilation per kbuild test robot feedback. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 045c28b71f4f..d77e11186afd 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -49,7 +49,7 @@ static inline void rcu_softirq_qs(void)
 #define rcu_note_context_switch(preempt) \
 	do { \
 		rcu_qs(); \
-		rcu_tasks_qs(current); \
+		rcu_tasks_qs(current, (preempt)); \
 	} while (0)
 
 static inline int rcu_needs_cpu(u64 basemono, u64 *nextevt)

commit 6be7436d2245d3dd8b9a8f949367c13841c23308
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Apr 10 13:47:41 2020 -0700

    rcu: Add rcu_gp_might_be_stalled()
    
    This commit adds rcu_gp_might_be_stalled(), which returns true if there
    is some reason to believe that the RCU grace period is stalled.  The use
    case is where an RCU free-memory path needs to allocate memory in order
    to free it, a situation that should be avoided where possible.
    
    But where it is necessary, there is always the alternative of using
    synchronize_rcu() to wait for a grace period in order to avoid the
    allocation.  And if the grace period is stalled, allocating memory to
    asynchronously wait for it is a bad idea of epic proportions: Far better
    to let others use the memory, because these others might actually be
    able to free that memory before the grace period ends.
    
    Thus, rcu_gp_might_be_stalled() can be used to help decide whether
    allocating memory on an RCU free path is a semi-reasonable course
    of action.
    
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Uladzislau Rezki <urezki@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 045c28b71f4f..dbf5ac439594 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -87,6 +87,7 @@ static inline bool rcu_inkernel_boot_has_ended(void) { return true; }
 static inline bool rcu_is_watching(void) { return true; }
 static inline void rcu_momentary_dyntick_idle(void) { }
 static inline void kfree_rcu_scheduler_running(void) { }
+static inline bool rcu_gp_might_be_stalled(void) { return false; }
 
 /* Avoid RCU read-side critical sections leaking across. */
 static inline void rcu_all_qs(void) { barrier(); }

commit 59ee0326ccf712f9a637d5df2465a16a784cbfb0
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Nov 28 18:54:06 2019 -0800

    rcutorture: Suppress forward-progress complaints during early boot
    
    Some larger systems can take in excess of 50 seconds to complete their
    early boot initcalls prior to spawing init.  This does not in any way
    help the forward-progress judgments of built-in rcutorture (when
    rcutorture is built as a module, the insmod or modprobe command normally
    cannot happen until some time after boot completes).  This commit
    therefore suppresses such complaints until about the time that init
    is spawned.
    
    This also includes a fix to a stupid error located by kbuild test robot.
    
    [ paulmck: Apply kbuild test robot feedback. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    [ paulmck: Fix to nohz_full slow-expediting recovery logic, per bpetkov. ]
    [ paulmck: Restrict splat to CONFIG_PREEMPT_RT=y kernels and simplify. ]
    Tested-by: Borislav Petkov <bp@alien8.de>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index b2b2dc990da9..045c28b71f4f 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -83,6 +83,7 @@ void rcu_scheduler_starting(void);
 static inline void rcu_scheduler_starting(void) { }
 #endif /* #else #ifndef CONFIG_SRCU */
 static inline void rcu_end_inkernel_boot(void) { }
+static inline bool rcu_inkernel_boot_has_ended(void) { return true; }
 static inline bool rcu_is_watching(void) { return true; }
 static inline void rcu_momentary_dyntick_idle(void) { }
 static inline void kfree_rcu_scheduler_running(void) { }

commit 189a6883dcf7fa70e17403ae4225c60ffc9e404b
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Fri Aug 30 12:36:33 2019 -0400

    rcu: Remove kfree_call_rcu_nobatch()
    
    Now that the kfree_rcu() special-casing has been removed from tree RCU,
    this commit removes kfree_call_rcu_nobatch() since it is no longer needed.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 1bd166aab6f3..b2b2dc990da9 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -39,11 +39,6 @@ static inline void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
 	call_rcu(head, func);
 }
 
-static inline void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
-{
-	call_rcu(head, func);
-}
-
 void rcu_qs(void);
 
 static inline void rcu_softirq_qs(void)

commit a35d16905efc6ad5523d864a5c6efcb1e657e386
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 5 18:22:27 2019 -0400

    rcu: Add basic support for kfree_rcu() batching
    
    Recently a discussion about stability and performance of a system
    involving a high rate of kfree_rcu() calls surfaced on the list [1]
    which led to another discussion how to prepare for this situation.
    
    This patch adds basic batching support for kfree_rcu(). It is "basic"
    because we do none of the slab management, dynamic allocation, code
    moving or any of the other things, some of which previous attempts did
    [2]. These fancier improvements can be follow-up patches and there are
    different ideas being discussed in those regards. This is an effort to
    start simple, and build up from there. In the future, an extension to
    use kfree_bulk and possibly per-slab batching could be done to further
    improve performance due to cache-locality and slab-specific bulk free
    optimizations. By using an array of pointers, the worker thread
    processing the work would need to read lesser data since it does not
    need to deal with large rcu_head(s) any longer.
    
    Torture tests follow in the next patch and show improvements of around
    5x reduction in number of  grace periods on a 16 CPU system. More
    details and test data are in that patch.
    
    There is an implication with rcu_barrier() with this patch. Since the
    kfree_rcu() calls can be batched, and may not be handed yet to the RCU
    machinery in fact, the monitor may not have even run yet to do the
    queue_rcu_work(), there seems no easy way of implementing rcu_barrier()
    to wait for those kfree_rcu()s that are already made. So this means a
    kfree_rcu() followed by an rcu_barrier() does not imply that memory will
    be freed once rcu_barrier() returns.
    
    Another implication is higher active memory usage (although not
    run-away..) until the kfree_rcu() flooding ends, in comparison to
    without batching. More details about this are in the second patch which
    adds an rcuperf test.
    
    Finally, in the near future we will get rid of kfree_rcu() special casing
    within RCU such as in rcu_do_batch and switch everything to just
    batching. Currently we don't do that since timer subsystem is not yet up
    and we cannot schedule the kfree_rcu() monitor as the timer subsystem's
    lock are not initialized. That would also mean getting rid of
    kfree_call_rcu_nobatch() entirely.
    
    [1] http://lore.kernel.org/lkml/20190723035725-mutt-send-email-mst@kernel.org
    [2] https://lkml.org/lkml/2017/12/19/824
    
    Cc: kernel-team@android.com
    Cc: kernel-team@lge.com
    Co-developed-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    [ paulmck: Applied 0day and Paul Walmsley feedback on ->monitor_todo. ]
    [ paulmck: Make it work during early boot. ]
    [ paulmck: Add a crude early boot self-test. ]
    [ paulmck: Style adjustments and experimental docbook structure header. ]
    Link: https://lore.kernel.org/lkml/alpine.DEB.2.21.9999.1908161931110.32497@viisi.sifive.com/T/#me9956f66cb611b95d26ae92700e1d901f46e8c59
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 37b6f0c2b79d..1bd166aab6f3 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -39,6 +39,11 @@ static inline void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
 	call_rcu(head, func);
 }
 
+static inline void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
+{
+	call_rcu(head, func);
+}
+
 void rcu_qs(void);
 
 static inline void rcu_softirq_qs(void)
@@ -85,6 +90,7 @@ static inline void rcu_scheduler_starting(void) { }
 static inline void rcu_end_inkernel_boot(void) { }
 static inline bool rcu_is_watching(void) { return true; }
 static inline void rcu_momentary_dyntick_idle(void) { }
+static inline void kfree_rcu_scheduler_running(void) { }
 
 /* Avoid RCU read-side critical sections leaking across. */
 static inline void rcu_all_qs(void) { barrier(); }

commit 79ba7ff5a9925f5c170f51ed7a96d1475eb6c27f
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Sun Aug 4 13:17:35 2019 -0700

    rcutorture: Emulate dyntick aspect of userspace nohz_full sojourn
    
    During an actual call_rcu() flood, there would be frequent trips to
    userspace (in-kernel call_rcu() floods must be otherwise housebroken).
    Userspace execution on nohz_full CPUs implies an RCU dyntick idle/not-idle
    transition pair, so this commit adds emulation of that pair.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 9bf1dfe7781f..37b6f0c2b79d 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -84,6 +84,7 @@ static inline void rcu_scheduler_starting(void) { }
 #endif /* #else #ifndef CONFIG_SRCU */
 static inline void rcu_end_inkernel_boot(void) { }
 static inline bool rcu_is_watching(void) { return true; }
+static inline void rcu_momentary_dyntick_idle(void) { }
 
 /* Avoid RCU read-side critical sections leaking across. */
 static inline void rcu_all_qs(void) { barrier(); }

commit 24691069a348f82a95e0fa9697bb5656c6d8c48c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Aug 22 10:53:43 2019 +0900

    rcu: Don't include <linux/ktime.h> in rcutiny.h
    
    The kbuild reported a built failure due to a header loop when RCUTINY is
    enabled with my pending riscv-nommu port.  Switch rcutiny.h to only
    include the minimal required header to get HZ instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 8e727f57d814..9bf1dfe7781f 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -12,7 +12,7 @@
 #ifndef __LINUX_TINY_H
 #define __LINUX_TINY_H
 
-#include <linux/ktime.h>
+#include <asm/param.h> /* for HZ */
 
 /* Never flag non-existent other CPUs! */
 static inline bool rcu_eqs_special_set(int cpu) { return false; }

commit 6c4421273694bd2351e230f491c1033b118734fd
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Jan 17 10:34:35 2019 -0800

    linux/rcutiny: Convert to SPDX license identifier
    
    Replace the license boiler plate with a SPDX license identifier.
    While in the area, update an email address.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Update .h SPDX format per Joe Perches. ]
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index af65d1f36ddb..8e727f57d814 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -1,23 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
 /*
  * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
  * Copyright IBM Corporation, 2008
  *
- * Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ * Author: Paul E. McKenney <paulmck@linux.ibm.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -
  *		Documentation/RCU

commit b56ada120921fbb0a4fb2a5bee163717182e7e9e
Merge: 5c3f78ec285b 894d45bbf7e7 4e6ea4ef56f9 7c590fcca66b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 30 16:12:53 2018 -0700

    Merge branches 'doc.2018.08.30a', 'dynticks.2018.08.30b', 'srcu.2018.08.30b' and 'torture.2018.08.29a' into HEAD
    
    doc.2018.08.30a: Documentation updates
    dynticks.2018.08.30b: RCU flavor consolidation updates and cleanups
    srcu.2018.08.30b: SRCU updates
    torture.2018.08.29a: Torture-test updates

commit 31ab604bf3232374e6471a2df9a83c4e75538390
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 16:15:54 2018 -0700

    rcu: Remove unused rcu_dynticks_snap() from Tiny RCU
    
    The rcu_dynticks_snap() function is defined in include/linux/rcutiny.h,
    but is no longer used by Tiny RCU.  This commit therefore removes it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 7fa4fb9e899e..f183683bdf79 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,12 +27,6 @@
 
 #include <linux/ktime.h>
 
-struct rcu_dynticks;
-static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
-{
-	return 0;
-}
-
 /* Never flag non-existent other CPUs! */
 static inline bool rcu_eqs_special_set(int cpu) { return false; }
 

commit a8bb74acd8efe2eb934d524ae20859980975b602
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 6 11:46:47 2018 -0700

    rcu: Consolidate RCU-sched update-side function definitions
    
    This commit saves a few lines by consolidating the RCU-sched function
    definitions at the end of include/linux/rcupdate.h.  This consolidation
    also makes it easier to remove them all when the time comes.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index df82bada9b19..7fa4fb9e899e 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -36,11 +36,6 @@ static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 /* Never flag non-existent other CPUs! */
 static inline bool rcu_eqs_special_set(int cpu) { return false; }
 
-static inline void synchronize_sched(void)
-{
-	synchronize_rcu();
-}
-
 static inline unsigned long get_state_synchronize_rcu(void)
 {
 	return 0;
@@ -51,36 +46,11 @@ static inline void cond_synchronize_rcu(unsigned long oldstate)
 	might_sleep();
 }
 
-static inline unsigned long get_state_synchronize_sched(void)
-{
-	return 0;
-}
-
-static inline void cond_synchronize_sched(unsigned long oldstate)
-{
-	might_sleep();
-}
-
 extern void rcu_barrier(void);
 
-static inline void rcu_barrier_sched(void)
-{
-	rcu_barrier();  /* Only one CPU, so only one list of callbacks! */
-}
-
 static inline void synchronize_rcu_expedited(void)
 {
-	synchronize_sched();
-}
-
-static inline void synchronize_sched_expedited(void)
-{
-	synchronize_sched();
-}
-
-static inline void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
-{
-	call_rcu(head, func);
+	synchronize_rcu();
 }
 
 static inline void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)

commit 4c7e9c1434c6fc960774a5475f2fbccbf557fdeb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 6 09:54:25 2018 -0700

    rcu: Consolidate RCU-bh update-side function definitions
    
    This commit saves a few lines by consolidating the RCU-bh function
    definitions at the end of include/linux/rcupdate.h.  This consolidation
    also makes it easier to remove them all when the time comes.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e66fb8bc2127..df82bada9b19 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -68,21 +68,6 @@ static inline void rcu_barrier_sched(void)
 	rcu_barrier();  /* Only one CPU, so only one list of callbacks! */
 }
 
-static inline void rcu_barrier_bh(void)
-{
-	rcu_barrier();
-}
-
-static inline void synchronize_rcu_bh(void)
-{
-	synchronize_sched();
-}
-
-static inline void synchronize_rcu_bh_expedited(void)
-{
-	synchronize_sched();
-}
-
 static inline void synchronize_rcu_expedited(void)
 {
 	synchronize_sched();

commit 709fdce7545c978e69f52eb19082ea3af44332f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 10:44:44 2018 -0700

    rcu: Express Tiny RCU updates in terms of RCU rather than RCU-sched
    
    This commit renames Tiny RCU functions so that the lowest level of
    functionality is RCU (e.g., synchronize_rcu()) rather than RCU-sched
    (e.g., synchronize_sched()).  This provides greater naming compatibility
    with Tree RCU, which will in turn permit more LoC removal once
    the RCU-sched and RCU-bh update-side API is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix Tiny call_rcu()'s EXPORT_SYMBOL() in response to a bug
      report from kbuild test robot. ]

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index df2c0895c5e7..e66fb8bc2127 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -36,9 +36,9 @@ static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 /* Never flag non-existent other CPUs! */
 static inline bool rcu_eqs_special_set(int cpu) { return false; }
 
-static inline void synchronize_rcu(void)
+static inline void synchronize_sched(void)
 {
-	synchronize_sched();
+	synchronize_rcu();
 }
 
 static inline unsigned long get_state_synchronize_rcu(void)
@@ -61,16 +61,11 @@ static inline void cond_synchronize_sched(unsigned long oldstate)
 	might_sleep();
 }
 
-static inline void synchronize_rcu_expedited(void)
-{
-	synchronize_sched();	/* Only one CPU, so pretty fast anyway!!! */
-}
+extern void rcu_barrier(void);
 
-extern void rcu_barrier_sched(void);
-
-static inline void rcu_barrier(void)
+static inline void rcu_barrier_sched(void)
 {
-	rcu_barrier_sched();  /* Only one CPU, so only one list of callbacks! */
+	rcu_barrier();  /* Only one CPU, so only one list of callbacks! */
 }
 
 static inline void rcu_barrier_bh(void)
@@ -88,27 +83,36 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched();
 }
 
+static inline void synchronize_rcu_expedited(void)
+{
+	synchronize_sched();
+}
+
 static inline void synchronize_sched_expedited(void)
 {
 	synchronize_sched();
 }
 
-static inline void kfree_call_rcu(struct rcu_head *head,
-				  rcu_callback_t func)
+static inline void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
+{
+	call_rcu(head, func);
+}
+
+static inline void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
 {
 	call_rcu(head, func);
 }
 
-void rcu_sched_qs(void);
+void rcu_qs(void);
 
 static inline void rcu_softirq_qs(void)
 {
-	rcu_sched_qs();
+	rcu_qs();
 }
 
 #define rcu_note_context_switch(preempt) \
 	do { \
-		rcu_sched_qs(); \
+		rcu_qs(); \
 		rcu_tasks_qs(current); \
 	} while (0)
 

commit 45975c7d21a1c0aba97e3d8007e2a7c123145748
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 14:30:37 2018 -0700

    rcu: Define RCU-sched API in terms of RCU for Tree RCU PREEMPT builds
    
    Now that RCU-preempt knows about preemption disabling, its implementation
    of synchronize_rcu() works for synchronize_sched(), and likewise for the
    other RCU-sched update-side API members.  This commit therefore confines
    the RCU-sched update-side code to CONFIG_PREEMPT=n builds, and defines
    RCU-sched's update-side API members in terms of those of RCU-preempt.
    
    This means that any given build of the Linux kernel has only one
    update-side flavor of RCU, namely RCU-preempt for CONFIG_PREEMPT=y builds
    and RCU-sched for CONFIG_PREEMPT=n builds.  This in turn means that kernels
    built with CONFIG_RCU_NOCB_CPU=y have only one rcuo kthread per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ac26c27ccde8..df2c0895c5e7 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -36,6 +36,11 @@ static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 /* Never flag non-existent other CPUs! */
 static inline bool rcu_eqs_special_set(int cpu) { return false; }
 
+static inline void synchronize_rcu(void)
+{
+	synchronize_sched();
+}
+
 static inline unsigned long get_state_synchronize_rcu(void)
 {
 	return 0;
@@ -94,6 +99,8 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
+void rcu_sched_qs(void);
+
 static inline void rcu_softirq_qs(void)
 {
 	rcu_sched_qs();

commit 65cfe3583b612a22e12fba9a7bbd2d37ca5ad941
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 1 07:40:52 2018 -0700

    rcu: Define RCU-bh update API in terms of RCU
    
    Now that the main RCU API knows about softirq disabling and softirq's
    quiescent states, the RCU-bh update code can be dispensed with.
    This commit therefore removes the RCU-bh update-side implementation and
    defines RCU-bh's update-side API in terms of that of either RCU-preempt or
    RCU-sched, depending on the setting of the CONFIG_PREEMPT Kconfig option.
    
    In kernels built with CONFIG_RCU_NOCB_CPU=y this has the knock-on effect
    of reducing by one the number of rcuo kthreads per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index bcfbc40a7239..ac26c27ccde8 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -56,19 +56,23 @@ static inline void cond_synchronize_sched(unsigned long oldstate)
 	might_sleep();
 }
 
-extern void rcu_barrier_bh(void);
-extern void rcu_barrier_sched(void);
-
 static inline void synchronize_rcu_expedited(void)
 {
 	synchronize_sched();	/* Only one CPU, so pretty fast anyway!!! */
 }
 
+extern void rcu_barrier_sched(void);
+
 static inline void rcu_barrier(void)
 {
 	rcu_barrier_sched();  /* Only one CPU, so only one list of callbacks! */
 }
 
+static inline void rcu_barrier_bh(void)
+{
+	rcu_barrier();
+}
+
 static inline void synchronize_rcu_bh(void)
 {
 	synchronize_sched();

commit d28139c4e96713d52a300fb9036c5be2f45e0741
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 14:45:25 2018 -0700

    rcu: Apply RCU-bh QSes to RCU-sched and RCU-preempt when safe
    
    One necessary step towards consolidating the three flavors of RCU is to
    make sure that the resulting consolidated "one flavor to rule them all"
    correctly handles networking denial-of-service attacks.  One thing that
    allows RCU-bh to do so is that __do_softirq() invokes rcu_bh_qs() every
    so often, and so something similar has to happen for consolidated RCU.
    
    This must be done carefully.  For example, if a preemption-disabled
    region of code takes an interrupt which does softirq processing before
    returning, consolidated RCU must ignore the resulting rcu_bh_qs()
    invocations -- preemption is still disabled, and that means an RCU
    reader for the consolidated flavor.
    
    This commit therefore creates a new rcu_softirq_qs() that is called only
    from the ksoftirqd task, thus avoiding the interrupted-a-preempted-region
    problem.  This new rcu_softirq_qs() function invokes rcu_sched_qs(),
    rcu_preempt_qs(), and rcu_preempt_deferred_qs().  The latter call handles
    any deferred quiescent states.
    
    Note that __do_softirq() still invokes rcu_bh_qs().  It will continue to
    do so until a later stage of cleanup when the RCU-bh flavor is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix !SMP issue located by kbuild test robot. ]

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index f617ab19bb51..bcfbc40a7239 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -90,6 +90,11 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
+static inline void rcu_softirq_qs(void)
+{
+	rcu_sched_qs();
+}
+
 #define rcu_note_context_switch(preempt) \
 	do { \
 		rcu_sched_qs(); \

commit 3e31009898699dfca823893054748d85048dc7b3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 21 12:50:01 2018 -0700

    rcu: Defer reporting RCU-preempt quiescent states when disabled
    
    This commit defers reporting of RCU-preempt quiescent states at
    rcu_read_unlock_special() time when any of interrupts, softirq, or
    preemption are disabled.  These deferred quiescent states are reported
    at a later RCU_SOFTIRQ, context switch, idle entry, or CPU-hotplug
    offline operation.  Of course, if another RCU read-side critical
    section has started in the meantime, the reporting of the quiescent
    state will be further deferred.
    
    This also means that disabling preemption, interrupts, and/or
    softirqs will act as an RCU-preempt read-side critical section.
    This is enforced by checking preempt_count() as needed.
    
    Some special cases must be handled on an ad-hoc basis, for example,
    context switch is a quiescent state even though both the scheduler and
    do_exit() disable preemption.  In these cases, additional calls to
    rcu_preempt_deferred_qs() override the preemption disabling.  Similar
    logic overrides disabled interrupts in rcu_preempt_check_callbacks()
    because in this case the quiescent state happened just before the
    corresponding scheduling-clock interrupt.
    
    In theory, this change lifts a long-standing restriction that required
    that if interrupts were disabled across a call to rcu_read_unlock()
    that the matching rcu_read_lock() also be contained within that
    interrupts-disabled region of code.  Because the reporting of the
    corresponding RCU-preempt quiescent state is now deferred until
    after interrupts have been enabled, it is no longer possible for this
    situation to result in deadlocks involving the scheduler's runqueue and
    priority-inheritance locks.  This may allow some code simplification that
    might reduce interrupt latency a bit.  Unfortunately, in practice this
    would also defer deboosting a low-priority task that had been subjected
    to RCU priority boosting, so real-time-response considerations might
    well force this restriction to remain in place.
    
    Because RCU-preempt grace periods are now blocked not only by RCU
    read-side critical sections, but also by disabling of interrupts,
    preemption, and softirqs, it will be possible to eliminate RCU-bh and
    RCU-sched in favor of RCU-preempt in CONFIG_PREEMPT=y kernels.  This may
    require some additional plumbing to provide the network denial-of-service
    guarantees that have been traditionally provided by RCU-bh.  Once these
    are in place, CONFIG_PREEMPT=n kernels will be able to fold RCU-bh
    into RCU-sched.  This would mean that all kernels would have but
    one flavor of RCU, which would open the door to significant code
    cleanup.
    
    Moving to a single flavor of RCU would also have the beneficial effect
    of reducing the NOCB kthreads by at least a factor of two.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Apply rcu_read_unlock_special() preempt_count() feedback
      from Joel Fernandes. ]
    [ paulmck: Adjust rcu_eqs_enter() call to rcu_preempt_deferred_qs() in
      response to bug reports from kbuild test robot. ]
    [ paulmck: Fix bug located by kbuild test robot involving recursion
      via rcu_preempt_deferred_qs(). ]

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 8d9a0ea8f0b5..f617ab19bb51 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -115,6 +115,11 @@ static inline void rcu_irq_exit_irqson(void) { }
 static inline void rcu_irq_enter_irqson(void) { }
 static inline void rcu_irq_exit(void) { }
 static inline void exit_rcu(void) { }
+static inline bool rcu_preempt_need_deferred_qs(struct task_struct *t)
+{
+	return false;
+}
+static inline void rcu_preempt_deferred_qs(struct task_struct *t) { }
 #ifdef CONFIG_SRCU
 void rcu_scheduler_starting(void);
 #else /* #ifndef CONFIG_SRCU */

commit 1b27291b1ea4f1f2090fb07c3425db474cdb99ba
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 18 14:32:31 2018 -0700

    rcutorture: Add forward-progress tests for RCU grace periods
    
    This commit adds a kthread that loops going into and out of RCU
    read-side critical sections, but also including a cond_resched(),
    optionally guarded by a check of need_resched(), in that same loop.
    This commit relies solely on rcu_torture_writer() progress to judge
    the forward progress of grace periods.
    
    Note that Tasks RCU and SRCU are exempted from forward-progress testing
    due their (intentionally) less-robust forward-progress guarantees.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 8d9a0ea8f0b5..a6353f3d6094 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -108,6 +108,7 @@ static inline int rcu_needs_cpu(u64 basemono, u64 *nextevt)
  */
 static inline void rcu_virt_note_context_switch(int cpu) { }
 static inline void rcu_cpu_stall_reset(void) { }
+static inline int rcu_jiffies_till_stall_check(void) { return 21 * HZ; }
 static inline void rcu_idle_enter(void) { }
 static inline void rcu_idle_exit(void) { }
 static inline void rcu_irq_enter(void) { }

commit 6f56f714db067056c80f5d71510118f82872e34c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 14 13:52:27 2018 -0700

    rcu: Improve RCU-tasks naming and comments
    
    The naming and comments associated with some RCU-tasks code make
    the faulty assumption that context switches due to cond_resched()
    are voluntary.  As several people pointed out, this is not the case.
    This commit therefore updates function names and comments to better
    reflect current reality.
    
    Reported-by: Byungchul Park <byungchul.park@lge.com>
    Reported-by: Joel Fernandes <joel@joelfernandes.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 7b3c82e8a625..8d9a0ea8f0b5 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -93,7 +93,7 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 #define rcu_note_context_switch(preempt) \
 	do { \
 		rcu_sched_qs(); \
-		rcu_note_voluntary_context_switch_lite(current); \
+		rcu_tasks_qs(current); \
 	} while (0)
 
 static inline int rcu_needs_cpu(u64 basemono, u64 *nextevt)

commit f64c6013a2029316ea552f99823d116ee5f5f955
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 22 09:50:53 2018 -0700

    rcu/x86: Provide early rcu_cpu_starting() callback
    
    The x86/mtrr code does horrific things because hardware. It uses
    stop_machine_from_inactive_cpu(), which does a wakeup (of the stopper
    thread on another CPU), which uses RCU, all before the CPU is onlined.
    
    RCU complains about this, because wakeups use RCU and RCU does
    (rightfully) not consider offline CPUs for grace-periods.
    
    Fix this by initializing RCU way early in the MTRR case.
    
    Tested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Add !SMP support, per 0day Test Robot report. ]

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ce9beec35e34..7b3c82e8a625 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -132,5 +132,6 @@ static inline void rcu_all_qs(void) { barrier(); }
 #define rcutree_offline_cpu      NULL
 #define rcutree_dead_cpu         NULL
 #define rcutree_dying_cpu        NULL
+static inline void rcu_cpu_starting(unsigned int cpu) { }
 
 #endif /* __LINUX_RCUTINY_H */

commit 844ccdd7dce2c1a6ea9b437fcf8c3265b136e4a5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 3 16:51:47 2017 -0700

    rcu: Eliminate rcu_irq_enter_disabled()
    
    Now that the irq path uses the rcu_nmi_{enter,exit}() algorithm,
    rcu_irq_enter() and rcu_irq_exit() may be used from any context.  There is
    thus no need for rcu_irq_enter_disabled() and for the checks using it.
    This commit therefore eliminates rcu_irq_enter_disabled().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index b3dbf9502fd0..ce9beec35e34 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -111,7 +111,6 @@ static inline void rcu_cpu_stall_reset(void) { }
 static inline void rcu_idle_enter(void) { }
 static inline void rcu_idle_exit(void) { }
 static inline void rcu_irq_enter(void) { }
-static inline bool rcu_irq_enter_disabled(void) { return false; }
 static inline void rcu_irq_exit_irqson(void) { }
 static inline void rcu_irq_enter_irqson(void) { }
 static inline void rcu_irq_exit(void) { }

commit 825c5bd2fd47d30148db15fc121216c483682b01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri May 26 16:16:40 2017 -0700

    srcu: Move rcu_scheduler_starting() from Tiny RCU to Tiny SRCU
    
    Other than lockdep support, Tiny RCU has no need for the
    scheduler status.  However, Tiny SRCU will need this to control
    boot-time behavior independent of lockdep.  Therefore, this commit
    moves rcu_scheduler_starting() from kernel/rcu/tiny_plugin.h to
    kernel/rcu/srcutiny.c.  This in turn allows the complete removal of
    kernel/rcu/tiny_plugin.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 5becbbccb998..b3dbf9502fd0 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -116,13 +116,11 @@ static inline void rcu_irq_exit_irqson(void) { }
 static inline void rcu_irq_enter_irqson(void) { }
 static inline void rcu_irq_exit(void) { }
 static inline void exit_rcu(void) { }
-
-#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU)
-extern int rcu_scheduler_active __read_mostly;
+#ifdef CONFIG_SRCU
 void rcu_scheduler_starting(void);
-#else /* #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
+#else /* #ifndef CONFIG_SRCU */
 static inline void rcu_scheduler_starting(void) { }
-#endif /* #else #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
+#endif /* #else #ifndef CONFIG_SRCU */
 static inline void rcu_end_inkernel_boot(void) { }
 static inline bool rcu_is_watching(void) { return true; }
 

commit d2b1654f91f9e928011fbea7138854ee2044f470
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 11 12:01:50 2017 -0700

    rcu: Remove #ifdef moving rcu_end_inkernel_boot from rcupdate.h
    
    This commit removes a #ifdef and saves a few lines of code by moving
    the rcu_end_inkernel_boot() function from include/linux/rcupdate.h to
    include/linux/rcutiny.h (for TINY_RCU) and to include/linux/rcutree.h
    (for TREE_RCU).
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index c869785f16bd..5becbbccb998 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -123,7 +123,7 @@ void rcu_scheduler_starting(void);
 #else /* #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
 static inline void rcu_scheduler_starting(void) { }
 #endif /* #else #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
-
+static inline void rcu_end_inkernel_boot(void) { }
 static inline bool rcu_is_watching(void) { return true; }
 
 /* Avoid RCU read-side critical sections leaking across. */

commit 5f192ab027a5d865be24c817005d42eb96314dc2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 15:24:25 2017 -0700

    rcu: Refactor #includes from include/linux/rcupdate.h
    
    The list of #includes from include/linux/rcupdate.h has grown quite
    a bit, so it is time to trim it.  This commit moves the #include
    of include/linux/ktime.h to include/linux/rcutiny.h, along with the
    Tiny-RCU-only function that was the only thing needing ktimem.h.  It then
    reconstructs the files included into include/linux/ktime.h based on what
    is actually needed, with significant help from the 0day Test Robot.
    
    This single change reduces the .i file footprint from rcupdate.h from
    9018 lines to 7101 lines.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 2bfe48bc0e3b..c869785f16bd 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -25,7 +25,7 @@
 #ifndef __LINUX_TINY_H
 #define __LINUX_TINY_H
 
-#include <linux/cache.h>
+#include <linux/ktime.h>
 
 struct rcu_dynticks;
 static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
@@ -96,6 +96,12 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 		rcu_note_voluntary_context_switch_lite(current); \
 	} while (0)
 
+static inline int rcu_needs_cpu(u64 basemono, u64 *nextevt)
+{
+	*nextevt = KTIME_MAX;
+	return 0;
+}
+
 /*
  * Take advantage of the fact that there is only one CPU, which
  * allows us to ignore virtualization-based context switches.

commit 71c40fd0b5ceb300c6cb8753835d9d94a8bfc56f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 13:51:42 2017 -0700

    rcu: Move rcutiny.h to new empty/true/false-function style
    
    This commit saves a few lines in include/linux/rcutiny.h by moving
    to single-line definitions for empty functions, instead of the old
    style where the two curly braces each get their own line.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index f5067941bc27..2bfe48bc0e3b 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -33,10 +33,8 @@ static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 	return 0;
 }
 
-static inline bool rcu_eqs_special_set(int cpu)
-{
-	return false;  /* Never flag non-existent other CPUs! */
-}
+/* Never flag non-existent other CPUs! */
+static inline bool rcu_eqs_special_set(int cpu) { return false; }
 
 static inline unsigned long get_state_synchronize_rcu(void)
 {
@@ -102,65 +100,28 @@ static inline void kfree_call_rcu(struct rcu_head *head,
  * Take advantage of the fact that there is only one CPU, which
  * allows us to ignore virtualization-based context switches.
  */
-static inline void rcu_virt_note_context_switch(int cpu)
-{
-}
-
-static inline void rcu_cpu_stall_reset(void)
-{
-}
-
-static inline void rcu_idle_enter(void)
-{
-}
-
-static inline void rcu_idle_exit(void)
-{
-}
-
-static inline void rcu_irq_enter(void)
-{
-}
-
-static inline bool rcu_irq_enter_disabled(void)
-{
-	return false;
-}
-
-static inline void rcu_irq_exit_irqson(void)
-{
-}
-
-static inline void rcu_irq_enter_irqson(void)
-{
-}
-
-static inline void rcu_irq_exit(void)
-{
-}
-
-static inline void exit_rcu(void)
-{
-}
+static inline void rcu_virt_note_context_switch(int cpu) { }
+static inline void rcu_cpu_stall_reset(void) { }
+static inline void rcu_idle_enter(void) { }
+static inline void rcu_idle_exit(void) { }
+static inline void rcu_irq_enter(void) { }
+static inline bool rcu_irq_enter_disabled(void) { return false; }
+static inline void rcu_irq_exit_irqson(void) { }
+static inline void rcu_irq_enter_irqson(void) { }
+static inline void rcu_irq_exit(void) { }
+static inline void exit_rcu(void) { }
 
 #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU)
 extern int rcu_scheduler_active __read_mostly;
 void rcu_scheduler_starting(void);
 #else /* #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
-static inline void rcu_scheduler_starting(void)
-{
-}
+static inline void rcu_scheduler_starting(void) { }
 #endif /* #else #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
 
-static inline bool rcu_is_watching(void)
-{
-	return true;
-}
+static inline bool rcu_is_watching(void) { return true; }
 
-static inline void rcu_all_qs(void)
-{
-	barrier(); /* Avoid RCU read-side critical sections leaking across. */
-}
+/* Avoid RCU read-side critical sections leaking across. */
+static inline void rcu_all_qs(void) { barrier(); }
 
 /* RCUtree hotplug events */
 #define rcutree_prepare_cpu      NULL

commit fe21a27e8ca0937a5ac298de1f4b46382e9c5c88
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 13:45:51 2017 -0700

    rcu: Move rcu_request_urgent_qs_task() out of rcutiny.h and rcutree.h
    
    The rcu_request_urgent_qs_task() function is used only within RCU,
    so there is no point in exporting it to the rest of the kernel from
    nclude/linux/rcutiny.h and include/linux/rcutree.h.  This commit therefore
    moves this function to kernel/rcu/rcu.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 0d9270913686..f5067941bc27 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -157,10 +157,6 @@ static inline bool rcu_is_watching(void)
 	return true;
 }
 
-static inline void rcu_request_urgent_qs_task(struct task_struct *t)
-{
-}
-
 static inline void rcu_all_qs(void)
 {
 	barrier(); /* Avoid RCU read-side critical sections leaking across. */

commit e3c8d51e1a58c73a557eb38a9a6afb4f704a3379
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 13:37:16 2017 -0700

    rcu: Move torture-related functions out of rcutiny.h and rcutree.h
    
    The various functions similar to rcu_batches_started(), the
    function show_rcu_gp_kthreads(), the various functions similar to
    rcu_force_quiescent_state(), and the variables rcutorture_testseq and
    rcutorture_vernum are used only within RCU.  There is therefore no point
    in exporting them to the kernel at large from include/linux/rcutiny.h
    and include/linux/rcutree.h.  This commit therefore moves all of these
    to kernel/rcu/rcu.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 5ed6934152a6..0d9270913686 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -106,86 +106,6 @@ static inline void rcu_virt_note_context_switch(int cpu)
 {
 }
 
-/*
- * Return the number of grace periods started.
- */
-static inline unsigned long rcu_batches_started(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of bottom-half grace periods started.
- */
-static inline unsigned long rcu_batches_started_bh(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of sched grace periods started.
- */
-static inline unsigned long rcu_batches_started_sched(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of grace periods completed.
- */
-static inline unsigned long rcu_batches_completed(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of bottom-half grace periods completed.
- */
-static inline unsigned long rcu_batches_completed_bh(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of sched grace periods completed.
- */
-static inline unsigned long rcu_batches_completed_sched(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of expedited grace periods completed.
- */
-static inline unsigned long rcu_exp_batches_completed(void)
-{
-	return 0;
-}
-
-/*
- * Return the number of expedited sched grace periods completed.
- */
-static inline unsigned long rcu_exp_batches_completed_sched(void)
-{
-	return 0;
-}
-
-static inline void rcu_force_quiescent_state(void)
-{
-}
-
-static inline void rcu_bh_force_quiescent_state(void)
-{
-}
-
-static inline void rcu_sched_force_quiescent_state(void)
-{
-}
-
-static inline void show_rcu_gp_kthreads(void)
-{
-}
-
 static inline void rcu_cpu_stall_reset(void)
 {
 }

commit 791875d16e2f6e2e5b90328ccac643f512ac76c4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 11:06:05 2017 -0700

    rcu: Eliminate the unused __rcu_is_watching() function
    
    The __rcu_is_watching() function is currently not used, aside from
    to implement the rcu_is_watching() function.  This commit therefore
    eliminates __rcu_is_watching(), which has the beneficial side-effect
    of shrinking include/linux/rcupdate.h a bit.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ade360e0d58c..5ed6934152a6 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -232,22 +232,11 @@ static inline void rcu_scheduler_starting(void)
 }
 #endif /* #else #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
 
-#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
-
-static inline bool rcu_is_watching(void)
-{
-	return __rcu_is_watching();
-}
-
-#else /* defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
-
 static inline bool rcu_is_watching(void)
 {
 	return true;
 }
 
-#endif /* #else defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
-
 static inline void rcu_request_urgent_qs_task(struct task_struct *t)
 {
 }

commit cad7b3897279c869de61dc88133037b941f84233
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 10:22:57 2017 -0700

    rcu: Move torture-related definitions from rcupdate.h to rcu.h
    
    The include/linux/rcupdate.h file contains a number of definitions that
    are used only to communicate between rcutorture, rcuperf, and the RCU code
    itself.  There is no point in having these definitions exposed globally
    throughout the kernel, so this commit moves them to kernel/rcu/rcu.h.
    This change has the added benefit of shrinking rcupdate.h.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 74d9c3a1feee..ade360e0d58c 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -202,6 +202,11 @@ static inline void rcu_irq_enter(void)
 {
 }
 
+static inline bool rcu_irq_enter_disabled(void)
+{
+	return false;
+}
+
 static inline void rcu_irq_exit_irqson(void)
 {
 }

commit bcbfdd01dce5556a952fae84ef16fd0f12525e7b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 11 15:50:41 2017 -0700

    rcu: Make non-preemptive schedule be Tasks RCU quiescent state
    
    Currently, a call to schedule() acts as a Tasks RCU quiescent state
    only if a context switch actually takes place.  However, just the
    call to schedule() guarantees that the calling task has moved off of
    whatever tracing trampoline that it might have been one previously.
    This commit therefore plumbs schedule()'s "preempt" parameter into
    rcu_note_context_switch(), which then records the Tasks RCU quiescent
    state, but only if this call to schedule() was -not- due to a preemption.
    
    To avoid adding overhead to the common-case context-switch path,
    this commit hides the rcu_note_context_switch() check under an existing
    non-common-case check.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 5219be250f00..74d9c3a1feee 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -92,10 +92,11 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
-static inline void rcu_note_context_switch(void)
-{
-	rcu_sched_qs();
-}
+#define rcu_note_context_switch(preempt) \
+	do { \
+		rcu_sched_qs(); \
+		rcu_note_voluntary_context_switch_lite(current); \
+	} while (0)
 
 /*
  * Take advantage of the fact that there is only one CPU, which
@@ -242,6 +243,10 @@ static inline bool rcu_is_watching(void)
 
 #endif /* #else defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
 
+static inline void rcu_request_urgent_qs_task(struct task_struct *t)
+{
+}
+
 static inline void rcu_all_qs(void)
 {
 	barrier(); /* Avoid RCU read-side critical sections leaking across. */

commit 900b1028ec388e50c98200641ae4274794c807cf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Feb 10 14:32:54 2017 -0800

    srcu: Allow SRCU to access rcu_scheduler_active
    
    This is primarily a code-movement commit in preparation for allowing
    SRCU to handle early-boot SRCU grace periods.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 6c9d941e3962..5219be250f00 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -217,14 +217,14 @@ static inline void exit_rcu(void)
 {
 }
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU)
 extern int rcu_scheduler_active __read_mostly;
 void rcu_scheduler_starting(void);
-#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+#else /* #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
 static inline void rcu_scheduler_starting(void)
 {
 }
-#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+#endif /* #else #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_SRCU) */
 
 #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
 

commit b8c17e6664c461e4aed545a943304c3b32dd309c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Nov 8 14:25:21 2016 -0800

    rcu: Maintain special bits at bottom of ->dynticks counter
    
    Currently, IPIs are used to force other CPUs to invalidate their TLBs
    in response to a kernel virtual-memory mapping change.  This works, but
    degrades both battery lifetime (for idle CPUs) and real-time response
    (for nohz_full CPUs), and in addition results in unnecessary IPIs due to
    the fact that CPUs executing in usermode are unaffected by stale kernel
    mappings.  It would be better to cause a CPU executing in usermode to
    wait until it is entering kernel mode to do the flush, first to avoid
    interrupting usemode tasks and second to handle multiple flush requests
    with a single flush in the case of a long-running user task.
    
    This commit therefore reserves a bit at the bottom of the ->dynticks
    counter, which is checked upon exit from extended quiescent states.
    If it is set, it is cleared and then a new rcu_eqs_special_exit() macro is
    invoked, which, if not supplied, is an empty single-pass do-while loop.
    If this bottom bit is set on -entry- to an extended quiescent state,
    then a WARN_ON_ONCE() triggers.
    
    This bottom bit may be set using a new rcu_eqs_special_set() function,
    which returns true if the bit was set, or false if the CPU turned
    out to not be in an extended quiescent state.  Please note that this
    function refuses to set the bit for a non-nohz_full CPU when that CPU
    is executing in usermode because usermode execution is tracked by RCU
    as a dyntick-idle extended quiescent state only for nohz_full CPUs.
    
    Reported-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index b452953e21c8..6c9d941e3962 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -33,6 +33,11 @@ static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 	return 0;
 }
 
+static inline bool rcu_eqs_special_set(int cpu)
+{
+	return false;  /* Never flag non-existent other CPUs! */
+}
+
 static inline unsigned long get_state_synchronize_rcu(void)
 {
 	return 0;

commit f9411ebe3d85cbbea06298241e6053d031d281fc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 09:50:49 2017 +0100

    rcu: Separate the RCU synchronization types and APIs into <linux/rcupdate_wait.h>
    
    So rcupdate.h is a pretty complex header, in particular it includes
    <linux/completion.h> which includes <linux/wait.h> - creating a
    dependency that includes <linux/wait.h> in <linux/sched.h>,
    which prevents the isolation of <linux/sched.h> from the derived
    <linux/wait.h> header.
    
    Solve part of the problem by decoupling rcupdate.h from completions:
    this can be done by separating out the rcu_synchronize types and APIs,
    and updating their usage sites.
    
    Since this is a mostly RCU-internal types this will not just simplify
    <linux/sched.h>'s dependencies, but will make all the hundreds of
    .c files that include rcupdate.h but not completions or wait.h build
    faster.
    
    ( For rcutiny this means that two dependent APIs have to be uninlined,
      but that shouldn't be much of a problem as they are rare variants. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 4f9b2fa2173d..b452953e21c8 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -53,15 +53,8 @@ static inline void cond_synchronize_sched(unsigned long oldstate)
 	might_sleep();
 }
 
-static inline void rcu_barrier_bh(void)
-{
-	wait_rcu_gp(call_rcu_bh);
-}
-
-static inline void rcu_barrier_sched(void)
-{
-	wait_rcu_gp(call_rcu_sched);
-}
+extern void rcu_barrier_bh(void);
+extern void rcu_barrier_sched(void);
 
 static inline void synchronize_rcu_expedited(void)
 {

commit 02a5c550b2738f2bfea8e1e00aa75944d71c9e18
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Nov 2 17:25:06 2016 -0700

    rcu: Abstract extended quiescent state determination
    
    This commit is the fourth step towards full abstraction of all accesses
    to the ->dynticks counter, implementing previously open-coded checks and
    comparisons in new rcu_dynticks_in_eqs() and rcu_dynticks_in_eqs_since()
    functions.  This abstraction will ease changes to the ->dynticks counter
    operation.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ac81e4063b40..4f9b2fa2173d 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,6 +27,12 @@
 
 #include <linux/cache.h>
 
+struct rcu_dynticks;
+static inline int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
+{
+	return 0;
+}
+
 static inline unsigned long get_state_synchronize_rcu(void)
 {
 	return 0;

commit 4df8374254ea9294dfe4b8c447a1b7eddc543dbf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:17:03 2016 +0000

    rcu: Convert rcutree to hotplug state machine
    
    Straight forward conversion to the state machine. Though the question arises
    whether this needs really all these state transitions to work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153337.982013161@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 93aea75029fb..ac81e4063b40 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -243,4 +243,11 @@ static inline void rcu_all_qs(void)
 	barrier(); /* Avoid RCU read-side critical sections leaking across. */
 }
 
+/* RCUtree hotplug events */
+#define rcutree_prepare_cpu      NULL
+#define rcutree_online_cpu       NULL
+#define rcutree_offline_cpu      NULL
+#define rcutree_dead_cpu         NULL
+#define rcutree_dying_cpu        NULL
+
 #endif /* __LINUX_RCUTINY_H */

commit 291783b8ad77a83a6fdf91d55eee7f1ad72ed4d1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jan 12 13:43:30 2016 -0800

    rcutorture: Expedited-GP batch progress access to torturing
    
    This commit provides rcu_exp_batches_completed() and
    rcu_exp_batches_completed_sched() functions to allow torture-test modules
    to check how many expedited grace period batches have completed.
    These are analogous to the existing rcu_batches_completed(),
    rcu_batches_completed_bh(), and rcu_batches_completed_sched() functions.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 64809aea661c..93aea75029fb 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -149,6 +149,22 @@ static inline unsigned long rcu_batches_completed_sched(void)
 	return 0;
 }
 
+/*
+ * Return the number of expedited grace periods completed.
+ */
+static inline unsigned long rcu_exp_batches_completed(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of expedited sched grace periods completed.
+ */
+static inline unsigned long rcu_exp_batches_completed_sched(void)
+{
+	return 0;
+}
+
 static inline void rcu_force_quiescent_state(void)
 {
 }

commit 7c9906ca5e582a773fff696975e312cef58a7386
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Oct 31 00:59:01 2015 -0700

    rcu: Don't redundantly disable irqs in rcu_irq_{enter,exit}()
    
    This commit replaces a local_irq_save()/local_irq_restore() pair with
    a lockdep assertion that interrupts are already disabled.  This should
    remove the corresponding overhead from the interrupt entry/exit fastpaths.
    
    This change was inspired by the fact that Iftekhar Ahmed's mutation
    testing showed that removing rcu_irq_enter()'s call to local_ird_restore()
    had no effect, which might indicate that interrupts were always enabled
    anyway.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 4c1aaf9cce7b..64809aea661c 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -181,6 +181,14 @@ static inline void rcu_irq_enter(void)
 {
 }
 
+static inline void rcu_irq_exit_irqson(void)
+{
+}
+
+static inline void rcu_irq_enter_irqson(void)
+{
+}
+
 static inline void rcu_irq_exit(void)
 {
 }

commit bb73c52bad3666997ed2ec83c0c80c3f8ef55008
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Jul 30 16:55:38 2015 -0700

    rcu: Don't disable preemption for Tiny and Tree RCU readers
    
    Because preempt_disable() maps to barrier() for non-debug builds,
    it forces the compiler to spill and reload registers.  Because Tree
    RCU and Tiny RCU now only appear in CONFIG_PREEMPT=n builds, these
    barrier() instances generate needless extra code for each instance of
    rcu_read_lock() and rcu_read_unlock().  This extra code slows down Tree
    RCU and bloats Tiny RCU.
    
    This commit therefore removes the preempt_disable() and preempt_enable()
    from the non-preemptible implementations of __rcu_read_lock() and
    __rcu_read_unlock(), respectively.  However, for debug purposes,
    preempt_disable() and preempt_enable() are still invoked if
    CONFIG_PREEMPT_COUNT=y, because this allows detection of sleeping inside
    atomic sections in non-preemptible kernels.
    
    However, Tiny and Tree RCU operates by coalescing all RCU read-side
    critical sections on a given CPU that lie between successive quiescent
    states.  It is therefore necessary to compensate for removing barriers
    from __rcu_read_lock() and __rcu_read_unlock() by adding them to a
    couple of the RCU functions invoked during quiescent states, namely to
    rcu_all_qs() and rcu_note_context_switch().  However, note that the latter
    is more paranoia than necessity, at least until link-time optimizations
    become more aggressive.
    
    This is based on an earlier patch by Paul E. McKenney, fixing
    a bug encountered in kernels built with CONFIG_PREEMPT=n and
    CONFIG_PREEMPT_COUNT=y.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index c8a0722f77ea..4c1aaf9cce7b 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -216,6 +216,7 @@ static inline bool rcu_is_watching(void)
 
 static inline void rcu_all_qs(void)
 {
+	barrier(); /* Avoid RCU read-side critical sections leaking across. */
 }
 
 #endif /* __LINUX_RCUTINY_H */

commit b6a4ae766e3133a4db73fabc81e522d1601cb623
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Wed Jul 29 13:29:38 2015 +0800

    rcu: Use rcu_callback_t in call_rcu*() and friends
    
    As we now have rcu_callback_t typedefs as the type of rcu callbacks, we
    should use it in call_rcu*() and friends as the type of parameters. This
    could save us a few lines of code and make it clear which function
    requires an rcu callbacks rather than other callbacks as its argument.
    
    Besides, this can also help cscope to generate a better database for
    code reading.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ff968b7af3a4..c8a0722f77ea 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -83,7 +83,7 @@ static inline void synchronize_sched_expedited(void)
 }
 
 static inline void kfree_call_rcu(struct rcu_head *head,
-				  void (*func)(struct rcu_head *rcu))
+				  rcu_callback_t func)
 {
 	call_rcu(head, func);
 }

commit 24560056de61d86153cecb84d04e4237437f5888
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat May 30 10:11:24 2015 -0700

    rcu: Add RCU-sched flavors of get-state and cond-sync
    
    The get_state_synchronize_rcu() and cond_synchronize_rcu() functions
    allow polling for grace-period completion, with an actual wait for a
    grace period occurring only when cond_synchronize_rcu() is called too
    soon after the corresponding get_state_synchronize_rcu().  However,
    these functions work only for vanilla RCU.  This commit adds the
    get_state_synchronize_sched() and cond_synchronize_sched(), which provide
    the same capability for RCU-sched.
    
    Reported-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 3df6c1ec4e25..ff968b7af3a4 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -37,6 +37,16 @@ static inline void cond_synchronize_rcu(unsigned long oldstate)
 	might_sleep();
 }
 
+static inline unsigned long get_state_synchronize_sched(void)
+{
+	return 0;
+}
+
+static inline void cond_synchronize_sched(unsigned long oldstate)
+{
+	might_sleep();
+}
+
 static inline void rcu_barrier_bh(void)
 {
 	wait_rcu_gp(call_rcu_bh);

commit 51952bc633064311410b041fad38da1614f4539e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 21 11:15:30 2015 -0700

    rcu: Further shrink Tiny RCU by making empty functions static inlines
    
    The Tiny RCU counterparts to rcu_idle_enter(), rcu_idle_exit(),
    rcu_irq_enter(), and rcu_irq_exit() are empty functions, but each has
    EXPORT_SYMBOL_GPL(), which needlessly consumes extra memory, especially
    in kernels built with module support.  This commit therefore moves these
    functions to static inlines in rcutiny.h, removing the need for exports.
    
    This won't affect the size of the tiniest kernels, which are likely
    built without module support, but might help semi-tiny kernels that
    might include module support.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 937edaeb150d..3df6c1ec4e25 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -159,6 +159,22 @@ static inline void rcu_cpu_stall_reset(void)
 {
 }
 
+static inline void rcu_idle_enter(void)
+{
+}
+
+static inline void rcu_idle_exit(void)
+{
+}
+
+static inline void rcu_irq_enter(void)
+{
+}
+
+static inline void rcu_irq_exit(void)
+{
+}
+
 static inline void exit_rcu(void)
 {
 }

commit 78e691f4ae2d5edea0199ca802bb505b9cdced88
Merge: d87510c5a6e3 60479676eb6e ab954c167ed9 83fe27ea5311 630181c4a915 7602de4af192
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 15 23:34:34 2015 -0800

    Merge branches 'doc.2015.01.07a', 'fixes.2015.01.15a', 'preempt.2015.01.06a', 'srcu.2015.01.06a', 'stall.2015.01.16a' and 'torture.2015.01.11a' into HEAD
    
    doc.2015.01.07a: Documentation updates.
    fixes.2015.01.15a: Miscellaneous fixes.
    preempt.2015.01.06a: Changes to handling of lists of preempted tasks.
    srcu.2015.01.06a: SRCU updates.
    stall.2015.01.16a: RCU CPU stall-warning updates and fixes.
    torture.2015.01.11a: RCU torture-test updates and fixes.

commit 5cd37193ce8539be1e6ef76be226f4bcc984e0f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Dec 13 20:32:04 2014 -0800

    rcu: Make cond_resched_rcu_qs() apply to normal RCU flavors
    
    Although cond_resched_rcu_qs() only applies to TASKS_RCU, it is used
    in places where it would be useful for it to apply to the normal RCU
    flavors, rcu_preempt, rcu_sched, and rcu_bh.  This is especially the
    case for workloads that aggressively overload the system, particularly
    those that generate large numbers of RCU updates on systems running
    NO_HZ_FULL CPUs.  This commit therefore communicates quiescent states
    from cond_resched_rcu_qs() to the normal RCU flavors.
    
    Note that it is unfortunately necessary to leave the old ->passed_quiesce
    mechanism in place to allow quiescent states that apply to only one
    flavor to be recorded.  (Yes, we could decrement ->rcu_qs_ctr_snap in
    that case, but that is not so good for debugging of RCU internals.)
    In addition, if one of the RCU flavor's grace period has stalled, this
    will invoke rcu_momentary_dyntick_idle(), resulting in a heavy-weight
    quiescent state visible from other CPUs.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Merge commit from Sasha Levin fixing a bug where __this_cpu()
      was used in preemptible code. ]

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 0e5366200154..fabd3fad8516 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -154,7 +154,10 @@ static inline bool rcu_is_watching(void)
 	return true;
 }
 
-
 #endif /* #else defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
 
+static inline void rcu_all_qs(void)
+{
+}
+
 #endif /* __LINUX_RCUTINY_H */

commit 917963d0b30f9c4153c372c165178501d97b6b55
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 17:10:16 2014 -0800

    rcutorture: Check from beginning to end of grace period
    
    Currently, rcutorture's Reader Batch checks measure from the end of
    the previous grace period to the end of the current one.  This commit
    tightens up these checks by measuring from the start and end of the same
    grace period.  This involves adding rcu_batches_started() and friends
    corresponding to the existing rcu_batches_completed() and friends.
    
    We leave SRCU alone for the moment, as it does not yet have a way of
    tracking both ends of its grace periods.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 1ce2d6b8f0c3..984192160e9b 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -92,7 +92,31 @@ static inline void rcu_virt_note_context_switch(int cpu)
 }
 
 /*
- * Return the number of grace periods.
+ * Return the number of grace periods started.
+ */
+static inline unsigned long rcu_batches_started(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of bottom-half grace periods started.
+ */
+static inline unsigned long rcu_batches_started_bh(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of sched grace periods started.
+ */
+static inline unsigned long rcu_batches_started_sched(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of grace periods completed.
  */
 static inline unsigned long rcu_batches_completed(void)
 {
@@ -100,7 +124,7 @@ static inline unsigned long rcu_batches_completed(void)
 }
 
 /*
- * Return the number of bottom-half grace periods.
+ * Return the number of bottom-half grace periods completed.
  */
 static inline unsigned long rcu_batches_completed_bh(void)
 {
@@ -108,7 +132,7 @@ static inline unsigned long rcu_batches_completed_bh(void)
 }
 
 /*
- * Return the number of sched grace periods.
+ * Return the number of sched grace periods completed.
  */
 static inline unsigned long rcu_batches_completed_sched(void)
 {

commit c1fe9cde4ae904fffb5b4d975d0a37e99136ff50
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 15:45:27 2014 -0800

    rcu: Provide rcu_batches_completed_sched() for TINY_RCU
    
    A bug in rcutorture has caused it to ignore completed batches.
    In preparation for fixing that bug, this commit provides TINY_RCU with
    the required rcu_batches_completed_sched().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 91f7e4c37800..1ce2d6b8f0c3 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -107,6 +107,14 @@ static inline unsigned long rcu_batches_completed_bh(void)
 	return 0;
 }
 
+/*
+ * Return the number of sched grace periods.
+ */
+static inline unsigned long rcu_batches_completed_sched(void)
+{
+	return 0;
+}
+
 static inline void rcu_force_quiescent_state(void)
 {
 }

commit 9733e4f0a973a354034f5dd603b4142a3095c85f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 12:49:13 2014 -0800

    rcu: Make _batches_completed() functions return unsigned long
    
    Long ago, the various ->completed fields were of type long, but now are
    unsigned long due to signed-integer-overflow concerns.  However, the
    various _batches_completed() functions remained of type long, even though
    their only purpose in life is to return the corresponding ->completed
    field.  This patch cleans this up by changing these functions' return
    types to unsigned long.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 0e5366200154..91f7e4c37800 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -94,7 +94,7 @@ static inline void rcu_virt_note_context_switch(int cpu)
 /*
  * Return the number of grace periods.
  */
-static inline long rcu_batches_completed(void)
+static inline unsigned long rcu_batches_completed(void)
 {
 	return 0;
 }
@@ -102,7 +102,7 @@ static inline long rcu_batches_completed(void)
 /*
  * Return the number of bottom-half grace periods.
  */
-static inline long rcu_batches_completed_bh(void)
+static inline unsigned long rcu_batches_completed_bh(void)
 {
 	return 0;
 }

commit 38200cf24702e5d79ce6c8f4c62036c41845c62d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 12:50:04 2014 -0700

    rcu: Remove "cpu" argument to rcu_note_context_switch()
    
    The "cpu" argument to rcu_note_context_switch() is always the current
    CPU, so drop it.  This in turn allows the "cpu" argument to
    rcu_preempt_note_context_switch() to be removed, which allows the sole
    use of "cpu" in both functions to be replaced with a this_cpu_ptr().
    Again, the anticipated cross-CPU uses of these functions has been
    replaced by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 38cc5b1e252d..0e5366200154 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -78,7 +78,7 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
-static inline void rcu_note_context_switch(int cpu)
+static inline void rcu_note_context_switch(void)
 {
 	rcu_sched_qs();
 }

commit 284a8c93af47306beed967a303d84730b32bab39
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 14 16:38:46 2014 -0700

    rcu: Per-CPU operation cleanups to rcu_*_qs() functions
    
    The rcu_bh_qs(), rcu_preempt_qs(), and rcu_sched_qs() functions use
    old-style per-CPU variable access and write to ->passed_quiesce even
    if it is already set.  This commit therefore updates to use the new-style
    per-CPU variable access functions and avoids the spurious writes.
    This commit also eliminates the "cpu" argument to these functions because
    they are always invoked on the indicated CPU.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index d40a6a451330..38cc5b1e252d 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -80,7 +80,7 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 
 static inline void rcu_note_context_switch(int cpu)
 {
-	rcu_sched_qs(cpu);
+	rcu_sched_qs();
 }
 
 /*

commit afea227fd4acf4f097a9e77bbc2f07d4856ebd01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 12 07:10:41 2014 -0700

    rcutorture: Export RCU grace-period kthread wait state to rcutorture
    
    This commit allows rcutorture to print additional state for the
    RCU grace-period kthreads in cases where RCU seems reluctant to
    start a new grace period.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 425c659d54e5..d40a6a451330 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -119,6 +119,10 @@ static inline void rcu_sched_force_quiescent_state(void)
 {
 }
 
+static inline void show_rcu_gp_kthreads(void)
+{
+}
+
 static inline void rcu_cpu_stall_reset(void)
 {
 }

commit 765a3f4fed708ae429ee095914a7897acb3a65bd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Mar 14 16:37:08 2014 -0700

    rcu: Provide grace-period piggybacking API
    
    The following pattern is currently not well supported by RCU:
    
    1.      Make data element inaccessible to RCU readers.
    
    2.      Do work that probably lasts for more than one grace period.
    
    3.      Do something to make sure RCU readers in flight before #1 above
            have completed.
    
    Here are some things that could currently be done:
    
    a.      Do a synchronize_rcu() unconditionally at either #1 or #3 above.
            This works, but imposes needless work and latency.
    
    b.      Post an RCU callback at #1 above that does a wakeup, then
            wait for the wakeup at #3.  This works well, but likely results
            in an extra unneeded grace period.  Open-coding this is also
            a bit more semi-tricky code than would be good.
    
    This commit therefore adds get_state_synchronize_rcu() and
    cond_synchronize_rcu() APIs.  Call get_state_synchronize_rcu() at #1
    above and pass its return value to cond_synchronize_rcu() at #3 above.
    This results in a call to synchronize_rcu() if no grace period has
    elapsed between #1 and #3, but requires only a load, comparison, and
    memory barrier if a full grace period did elapse.
    
    Requested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e8cb6e3b52a7..425c659d54e5 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,6 +27,16 @@
 
 #include <linux/cache.h>
 
+static inline unsigned long get_state_synchronize_rcu(void)
+{
+	return 0;
+}
+
+static inline void cond_synchronize_rcu(unsigned long oldstate)
+{
+	might_sleep();
+}
+
 static inline void rcu_barrier_bh(void)
 {
 	wait_rcu_gp(call_rcu_bh);

commit 322efba5b6442f331ac8aa24e92a817d804cc938
Merge: 8dd853d7b6ef 5cb5c6e18f82 f1f399d1281e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 26 06:36:09 2014 -0800

    Merge branches 'doc.2014.02.24a', 'fixes.2014.02.26a' and 'rt.2014.02.17b' into HEAD
    
    doc.2014.02.24a: Documentation changes
    fixes.2014.02.26a: Miscellaneous fixes
    rt.2014.02.17b: Response-time-related changes

commit ffa83fb565fbc397cbafb4b71fd1cce276d4c3b6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 17 19:27:16 2013 -0800

    rcu: Optimize rcu_needs_cpu() for RCU_NOCB_CPU_ALL
    
    If CONFIG_RCU_NOCB_CPU_ALL=y, then rcu_needs_cpu() will always
    return false, however, the current version nevertheless checks
    for RCU callbacks.  This commit therefore creates a static inline
    implementation of rcu_needs_cpu() that unconditionally returns false
    when CONFIG_RCU_NOCB_CPU_ALL=y.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 6f01771b571c..9524903487d0 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -68,12 +68,6 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
-static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
-{
-	*delta_jiffies = ULONG_MAX;
-	return 0;
-}
-
 static inline void rcu_note_context_switch(int cpu)
 {
 	rcu_sched_qs(cpu);

commit 87de1cfdc55b16b794e245b07322340725149d62
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 3 10:02:52 2013 -0800

    rcu: Stop tracking FSF's postal address
    
    All of the RCU source files have the usual GPL header, which contains a
    long-obsolete postal address for FSF.  To avoid the need to track the
    FSF office's movements, this commit substitutes the URL where GPL may
    be found.
    
    Reported-by: Greg KH <gregkh@linuxfoundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 6f01771b571c..c364e9148de2 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -12,8 +12,8 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
  *
  * Copyright IBM Corporation, 2008
  *

commit 584dc4ce55267765b415a8517613d1207f1741e5
Author: Teodora Baluta <teobaluta@gmail.com>
Date:   Mon Nov 11 17:11:23 2013 +0200

    rcu: Remove "extern" from function declarations in include/linux/*rcu*.h
    
    Function prototypes don't need to have the "extern" keyword since this
    is the default behavior. Its explicit use is redundant.  This commit
    therefore removes them.
    
    Signed-off-by: Teodora Baluta <teobaluta@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 09ebcbe9fd78..6f01771b571c 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -125,7 +125,7 @@ static inline void exit_rcu(void)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 extern int rcu_scheduler_active __read_mostly;
-extern void rcu_scheduler_starting(void);
+void rcu_scheduler_starting(void);
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 static inline void rcu_scheduler_starting(void)
 {

commit 5c173eb8bcb9c1aa888bd6d14a4cb746f3dd2420
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 13 17:20:11 2013 -0700

    rcu: Consistent rcu_is_watching() naming
    
    The old rcu_is_cpu_idle() function is just __rcu_is_watching() with
    preemption disabled.  This commit therefore renames rcu_is_cpu_idle()
    to rcu_is_watching.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index bee665964878..09ebcbe9fd78 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -132,13 +132,21 @@ static inline void rcu_scheduler_starting(void)
 }
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
-#ifdef CONFIG_RCU_TRACE
+#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
 
-static inline bool __rcu_is_watching(void)
+static inline bool rcu_is_watching(void)
 {
-	return !rcu_is_cpu_idle();
+	return __rcu_is_watching();
 }
 
-#endif /* #ifdef CONFIG_RCU_TRACE */
+#else /* defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
+
+static inline bool rcu_is_watching(void)
+{
+	return true;
+}
+
+
+#endif /* #else defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
 
 #endif /* __LINUX_RCUTINY_H */

commit cc6783f788d8fe8b23ec6fc2762f5e8c9a418eee
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 6 17:39:49 2013 -0700

    rcu: Is it safe to enter an RCU read-side critical section?
    
    There is currently no way for kernel code to determine whether it
    is safe to enter an RCU read-side critical section, in other words,
    whether or not RCU is paying attention to the currently running CPU.
    Given the large and increasing quantity of code shared by the idle loop
    and non-idle code, the this shortcoming is becoming increasingly painful.
    
    This commit therefore adds __rcu_is_watching(), which returns true if
    it is safe to enter an RCU read-side critical section on the currently
    running CPU.  This function is quite fast, using only a __this_cpu_read().
    However, the caller must disable preemption.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e31005ee339e..bee665964878 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -132,4 +132,13 @@ static inline void rcu_scheduler_starting(void)
 }
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
+#ifdef CONFIG_RCU_TRACE
+
+static inline bool __rcu_is_watching(void)
+{
+	return !rcu_is_cpu_idle();
+}
+
+#endif /* #ifdef CONFIG_RCU_TRACE */
+
 #endif /* __LINUX_RCUTINY_H */

commit 2439b696cb5303f1eeb6aeebcee19e0056c3dd6e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 11 10:15:52 2013 -0700

    rcu: Shrink TINY_RCU by moving exit_rcu()
    
    Now that TINY_PREEMPT_RCU is no more, exit_rcu() is always an empty
    function.  But if TINY_RCU is going to have an empty function, it should
    be in include/linux/rcutiny.h, where it does not bloat the kernel.
    This commit therefore moves exit_rcu() out of kernel/rcupdate.c to
    kernel/rcutree_plugin.h, and places a static inline empty function in
    include/linux/rcutiny.h in order to shrink TINY_RCU a bit.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 51230b63be93..e31005ee339e 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -119,6 +119,10 @@ static inline void rcu_cpu_stall_reset(void)
 {
 }
 
+static inline void exit_rcu(void)
+{
+}
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 extern int rcu_scheduler_active __read_mostly;
 extern void rcu_scheduler_starting(void);

commit fa2b3b0a50949537ac95a521b3546e9a87dc0565
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 27 10:32:00 2013 -0700

    rcu: Remove rcu_preempt_note_context_switch()
    
    With the removal of CONFIG_TINY_PREEMPT_RCU, rcu_preempt_note_context_switch()
    is now an empty function.  This commit therefore eliminates it by inlining it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 07b5affb92fa..51230b63be93 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -68,10 +68,6 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
-static inline void rcu_preempt_note_context_switch(void)
-{
-}
-
 static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 {
 	*delta_jiffies = ULONG_MAX;
@@ -81,7 +77,6 @@ static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 static inline void rcu_note_context_switch(int cpu)
 {
 	rcu_sched_qs(cpu);
-	rcu_preempt_note_context_switch();
 }
 
 /*

commit 57f1801a11d5a5313990ac56f5072e6be36994c3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 15 08:25:27 2013 -0700

    rcu: Remove the CONFIG_TINY_RCU ifdefs in rcutiny.h
    
    Now that CONFIG_TINY_PREEMPT_RCU is no more, this commit removes
    the CONFIG_TINY_RCU ifdefs from include/linux/rcutiny.h in favor of
    unconditionally compiling the CONFIG_TINY_RCU legs of those ifdefs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Moved removal of #else to "Remove TINY_PREEMPT_RCU" as
      suggested by Josh Triplett. ]
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e756251b39bc..07b5affb92fa 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -37,8 +37,6 @@ static inline void rcu_barrier_sched(void)
 	wait_rcu_gp(call_rcu_sched);
 }
 
-#ifdef CONFIG_TINY_RCU
-
 static inline void synchronize_rcu_expedited(void)
 {
 	synchronize_sched();	/* Only one CPU, so pretty fast anyway!!! */
@@ -49,8 +47,6 @@ static inline void rcu_barrier(void)
 	rcu_barrier_sched();  /* Only one CPU, so only one list of callbacks! */
 }
 
-#endif /* #ifdef CONFIG_TINY_RCU */
-
 static inline void synchronize_rcu_bh(void)
 {
 	synchronize_sched();
@@ -72,8 +68,6 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 	call_rcu(head, func);
 }
 
-#ifdef CONFIG_TINY_RCU
-
 static inline void rcu_preempt_note_context_switch(void)
 {
 }
@@ -84,8 +78,6 @@ static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 	return 0;
 }
 
-#endif /* #ifdef CONFIG_TINY_RCU */
-
 static inline void rcu_note_context_switch(int cpu)
 {
 	rcu_sched_qs(cpu);

commit 9dc5ad32488a75504349372330cc228d4dd678db
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 27 10:11:15 2013 -0700

    rcu: Simplify RCU_TINY RCU callback invocation
    
    TINY_PREEMPT_RCU could use a kthread to handle RCU callback invocation,
    which required an API to abstract kthread vs. softirq invocation.
    Now that TINY_PREEMPT_RCU is no longer with us, this commit retires
    this API in favor of direct use of the relevant softirq primitives.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index d3c094ffa960..e756251b39bc 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,10 +27,6 @@
 
 #include <linux/cache.h>
 
-static inline void rcu_init(void)
-{
-}
-
 static inline void rcu_barrier_bh(void)
 {
 	wait_rcu_gp(call_rcu_bh);

commit 127781d1ba1ee5bbe1780afa35dd0e71583b143d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 27 08:44:00 2013 -0700

    rcu: Remove TINY_PREEMPT_RCU
    
    TINY_PREEMPT_RCU adds significant code and complexity, but does not
    offer commensurate benefits.  People currently using TINY_PREEMPT_RCU
    can get much better memory footprint with TINY_RCU, or, if they really
    need preemptible RCU, they can use TREE_PREEMPT_RCU with a relatively
    minor degradation in memory footprint.  Please note that this move
    has been widely publicized on LKML (https://lkml.org/lkml/2012/11/12/545)
    and on LWN (http://lwn.net/Articles/541037/).
    
    This commit therefore removes TINY_PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Updated to eliminate #else in rcutiny.h as suggested by Josh ]
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 4e56a9c69a35..d3c094ffa960 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -53,16 +53,7 @@ static inline void rcu_barrier(void)
 	rcu_barrier_sched();  /* Only one CPU, so only one list of callbacks! */
 }
 
-#else /* #ifdef CONFIG_TINY_RCU */
-
-void synchronize_rcu_expedited(void);
-
-static inline void rcu_barrier(void)
-{
-	wait_rcu_gp(call_rcu);
-}
-
-#endif /* #else #ifdef CONFIG_TINY_RCU */
+#endif /* #ifdef CONFIG_TINY_RCU */
 
 static inline void synchronize_rcu_bh(void)
 {
@@ -97,18 +88,7 @@ static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 	return 0;
 }
 
-#else /* #ifdef CONFIG_TINY_RCU */
-
-void rcu_preempt_note_context_switch(void);
-int rcu_preempt_needs_cpu(void);
-
-static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
-{
-	*delta_jiffies = ULONG_MAX;
-	return rcu_preempt_needs_cpu();
-}
-
-#endif /* #else #ifdef CONFIG_TINY_RCU */
+#endif /* #ifdef CONFIG_TINY_RCU */
 
 static inline void rcu_note_context_switch(int cpu)
 {

commit cba6d0d64ee53772b285d0c0c288deefbeaf7775
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 07:08:42 2012 -0700

    Revert "rcu: Move PREEMPT_RCU preemption to switch_to() invocation"
    
    This reverts commit 616c310e83b872024271c915c1b9ab505b9efad9.
    (Move PREEMPT_RCU preemption to switch_to() invocation).
    Testing by Sasha Levin <levinsasha928@gmail.com> showed that this
    can result in deadlock due to invoking the scheduler when one of
    the runqueue locks is held.  Because this commit was simply a
    performance optimization, revert it.
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 854dc4c5c271..4e56a9c69a35 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -87,6 +87,10 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 
 #ifdef CONFIG_TINY_RCU
 
+static inline void rcu_preempt_note_context_switch(void)
+{
+}
+
 static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 {
 	*delta_jiffies = ULONG_MAX;
@@ -95,6 +99,7 @@ static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 
 #else /* #ifdef CONFIG_TINY_RCU */
 
+void rcu_preempt_note_context_switch(void);
 int rcu_preempt_needs_cpu(void);
 
 static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
@@ -108,6 +113,7 @@ static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 static inline void rcu_note_context_switch(int cpu)
 {
 	rcu_sched_qs(cpu);
+	rcu_preempt_note_context_switch();
 }
 
 /*

commit aa9b16306e3243229580ff889cc59fd66bf77973
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu May 10 16:41:44 2012 -0700

    rcu: Precompute RCU_FAST_NO_HZ timer offsets
    
    When a CPU is entering dyntick-idle mode, tick_nohz_stop_sched_tick()
    calls rcu_needs_cpu() see if RCU needs that CPU, and, if not, computes the
    next wakeup time based on the timer wheels.  Only later, when actually
    entering the idle loop, rcu_prepare_for_idle() will be invoked.  In some
    cases, rcu_prepare_for_idle() will post timers to wake the CPU back up.
    But all for naught: The next wakeup time for the CPU has already been
    computed, and posting a timer afterwards does not force that wakeup
    time to be recomputed.  This means that rcu_prepare_for_idle()'s have
    no effect.
    
    This is not a problem on a busy system because something else will wake
    up the CPU soon enough.  However, on lightly loaded systems, the CPU
    might stay asleep for a considerable length of time.  If that CPU has
    a callback that the rest of the system is waiting on, the system might
    run very slowly or (in theory) even hang.
    
    This commit avoids this problem by having rcu_needs_cpu() give
    tick_nohz_stop_sched_tick() an estimate of when RCU will need the CPU
    to wake back up, which tick_nohz_stop_sched_tick() takes into account
    when programming the CPU's wakeup time.  An alternative approach is
    for rcu_prepare_for_idle() to use hrtimers instead of normal timers,
    but timers are much more efficient than are hrtimers for frequently
    and repeatedly posting and cancelling a given timer, which is exactly
    what RCU_FAST_NO_HZ does.
    
    Reported-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index adb5e5a38cae..854dc4c5c271 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -87,8 +87,9 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 
 #ifdef CONFIG_TINY_RCU
 
-static inline int rcu_needs_cpu(int cpu)
+static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 {
+	*delta_jiffies = ULONG_MAX;
 	return 0;
 }
 
@@ -96,8 +97,9 @@ static inline int rcu_needs_cpu(int cpu)
 
 int rcu_preempt_needs_cpu(void);
 
-static inline int rcu_needs_cpu(int cpu)
+static inline int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 {
+	*delta_jiffies = ULONG_MAX;
 	return rcu_preempt_needs_cpu();
 }
 

commit 9dd8fb16c36178df2066387d2abd44d8b4dca8c8
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Apr 13 12:54:22 2012 -0700

    rcu: Make exit_rcu() more precise and consolidate
    
    When running preemptible RCU, if a task exits in an RCU read-side
    critical section having blocked within that same RCU read-side critical
    section, the task must be removed from the list of tasks blocking a
    grace period (perhaps the current grace period, perhaps the next grace
    period, depending on timing).  The exit() path invokes exit_rcu() to
    do this cleanup.
    
    However, the current implementation of exit_rcu() needlessly does the
    cleanup even if the task did not block within the current RCU read-side
    critical section, which wastes time and needlessly increases the size
    of the state space.  Fix this by only doing the cleanup if the current
    task is actually on the list of tasks blocking some grace period.
    
    While we are at it, consolidate the two identical exit_rcu() functions
    into a single function.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Conflicts:
    
            kernel/rcupdate.c

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 080b5bdda28e..adb5e5a38cae 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -87,10 +87,6 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 
 #ifdef CONFIG_TINY_RCU
 
-static inline void exit_rcu(void)
-{
-}
-
 static inline int rcu_needs_cpu(int cpu)
 {
 	return 0;
@@ -98,7 +94,6 @@ static inline int rcu_needs_cpu(int cpu)
 
 #else /* #ifdef CONFIG_TINY_RCU */
 
-extern void exit_rcu(void);
 int rcu_preempt_needs_cpu(void);
 
 static inline int rcu_needs_cpu(int cpu)

commit 616c310e83b872024271c915c1b9ab505b9efad9
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Mar 27 16:02:08 2012 -0700

    rcu: Move PREEMPT_RCU preemption to switch_to() invocation
    
    Currently, PREEMPT_RCU readers are enqueued upon entry to the scheduler.
    This is inefficient because enqueuing is required only if there is a
    context switch, and entry to the scheduler does not guarantee a context
    switch.
    
    The commit therefore moves the enqueuing to immediately precede the
    call to switch_to() from the scheduler.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e93df77176d1..080b5bdda28e 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -87,10 +87,6 @@ static inline void kfree_call_rcu(struct rcu_head *head,
 
 #ifdef CONFIG_TINY_RCU
 
-static inline void rcu_preempt_note_context_switch(void)
-{
-}
-
 static inline void exit_rcu(void)
 {
 }
@@ -102,7 +98,6 @@ static inline int rcu_needs_cpu(int cpu)
 
 #else /* #ifdef CONFIG_TINY_RCU */
 
-void rcu_preempt_note_context_switch(void);
 extern void exit_rcu(void);
 int rcu_preempt_needs_cpu(void);
 
@@ -116,7 +111,6 @@ static inline int rcu_needs_cpu(int cpu)
 static inline void rcu_note_context_switch(int cpu)
 {
 	rcu_sched_qs(cpu);
-	rcu_preempt_note_context_switch();
 }
 
 /*

commit 768dfffdffbfcc07d6927bdd642c714c0dd64c99
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Jan 11 16:33:17 2012 -0800

    rcu: Prevent RCU callbacks from executing before scheduler initialized
    
    This is a port of commit #b0d3041 from TREE_RCU to TREE_PREEMPT_RCU.
    
    Under some rare but real combinations of configuration parameters, RCU
    callbacks are posted during early boot that use kernel facilities that are
    not yet initialized.  Therefore, when these callbacks are invoked, hard
    hangs and crashes ensue.  This commit therefore prevents RCU callbacks
    from being invoked until after the scheduler is fully up and running,
    as in after multiple tasks have been spawned.
    
    It might well turn out that a better approach is to identify the specific
    RCU callbacks that are causing this problem, but that discussion will
    wait until such time as someone really needs an RCU callback to be invoked
    (as opposed to merely registered) during early boot.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 51bf29c81485..e93df77176d1 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,13 +27,9 @@
 
 #include <linux/cache.h>
 
-#ifdef CONFIG_RCU_BOOST
 static inline void rcu_init(void)
 {
 }
-#else /* #ifdef CONFIG_RCU_BOOST */
-void rcu_init(void);
-#endif /* #else #ifdef CONFIG_RCU_BOOST */
 
 static inline void rcu_barrier_bh(void)
 {

commit 486e259340fc4c60474f2c14703e3b3634bb58ca
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri Jan 6 14:11:30 2012 -0800

    rcu: Avoid waking up CPUs having only kfree_rcu() callbacks
    
    When CONFIG_RCU_FAST_NO_HZ is enabled, RCU will allow a given CPU to
    enter dyntick-idle mode even if it still has RCU callbacks queued.
    RCU avoids system hangs in this case by scheduling a timer for several
    jiffies in the future.  However, if all of the callbacks on that CPU
    are from kfree_rcu(), there is no reason to wake the CPU up, as it is
    not a problem to defer freeing of memory.
    
    This commit therefore tracks the number of callbacks on a given CPU
    that are from kfree_rcu(), and avoids scheduling the timer if all of
    a given CPU's callbacks are from kfree_rcu().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 00b7a5e493d2..51bf29c81485 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -83,6 +83,12 @@ static inline void synchronize_sched_expedited(void)
 	synchronize_sched();
 }
 
+static inline void kfree_call_rcu(struct rcu_head *head,
+				  void (*func)(struct rcu_head *rcu))
+{
+	call_rcu(head, func);
+}
+
 #ifdef CONFIG_TINY_RCU
 
 static inline void rcu_preempt_note_context_switch(void)

commit 965a002b4f1a458c5dcb334ec29f48a0046faa25
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sat Jun 18 09:55:39 2011 -0700

    rcu: Make TINY_RCU also use softirq for RCU_BOOST=n
    
    This patch #ifdefs TINY_RCU kthreads out of the kernel unless RCU_BOOST=y,
    thus eliminating context-switch overhead if RCU priority boosting has
    not been configured.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 4eab233a00cd..00b7a5e493d2 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,9 +27,13 @@
 
 #include <linux/cache.h>
 
+#ifdef CONFIG_RCU_BOOST
 static inline void rcu_init(void)
 {
 }
+#else /* #ifdef CONFIG_RCU_BOOST */
+void rcu_init(void);
+#endif /* #else #ifdef CONFIG_RCU_BOOST */
 
 static inline void rcu_barrier_bh(void)
 {

commit 2c42818e962e2858334bf45bfc56662b3752df34
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 26 22:14:36 2011 -0700

    rcu: Abstract common code for RCU grace-period-wait primitives
    
    Pull the code that waits for an RCU grace period into a single function,
    which is then called by synchronize_rcu() and friends in the case of
    TREE_RCU and TREE_PREEMPT_RCU, and from rcu_barrier() and friends in
    the case of TINY_RCU and TINY_PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 52b3e0281fd0..4eab233a00cd 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -31,6 +31,16 @@ static inline void rcu_init(void)
 {
 }
 
+static inline void rcu_barrier_bh(void)
+{
+	wait_rcu_gp(call_rcu_bh);
+}
+
+static inline void rcu_barrier_sched(void)
+{
+	wait_rcu_gp(call_rcu_sched);
+}
+
 #ifdef CONFIG_TINY_RCU
 
 static inline void synchronize_rcu_expedited(void)
@@ -45,9 +55,13 @@ static inline void rcu_barrier(void)
 
 #else /* #ifdef CONFIG_TINY_RCU */
 
-void rcu_barrier(void);
 void synchronize_rcu_expedited(void);
 
+static inline void rcu_barrier(void)
+{
+	wait_rcu_gp(call_rcu);
+}
+
 #endif /* #else #ifdef CONFIG_TINY_RCU */
 
 static inline void synchronize_rcu_bh(void)

commit 29ce831000081dd757d3116bf774aafffc4b6b20
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed May 4 16:31:03 2011 +0300

    rcu: provide rcu_virt_note_context_switch() function.
    
    Provide rcu_virt_note_context_switch() for vitalization use to note
    quiescent state during guest entry.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 30ebd7c8d874..52b3e0281fd0 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -99,6 +99,14 @@ static inline void rcu_note_context_switch(int cpu)
 	rcu_preempt_note_context_switch();
 }
 
+/*
+ * Take advantage of the fact that there is only one CPU, which
+ * allows us to ignore virtualization-based context switches.
+ */
+static inline void rcu_virt_note_context_switch(int cpu)
+{
+}
+
 /*
  * Return the number of grace periods.
  */

commit 7b27d5475f86186914e54e4a6bb994e9a985337b
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Oct 21 11:29:05 2010 +0800

    rcu,cleanup: move synchronize_sched_expedited() out of sched.c
    
    The first version of synchronize_sched_expedited() used the migration
    code in the scheduler, and was therefore implemented in kernel/sched.c.
    However, the more recent version of this code no longer uses the
    migration code, so this commit moves it to the main RCU source files.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ea025a611fcc..30ebd7c8d874 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -60,6 +60,11 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched();
 }
 
+static inline void synchronize_sched_expedited(void)
+{
+	synchronize_sched();
+}
+
 #ifdef CONFIG_TINY_RCU
 
 static inline void rcu_preempt_note_context_switch(void)

commit b2c0710c464ede15e1fc52fb1e7ee9ba54cea186
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Sep 9 13:40:39 2010 -0700

    rcu: move TINY_RCU from softirq to kthread
    
    If RCU priority boosting is to be meaningful, callback invocation must
    be boosted in addition to preempted RCU readers.  Otherwise, in presence
    of CPU real-time threads, the grace period ends, but the callbacks don't
    get invoked.  If the callbacks don't get invoked, the associated memory
    doesn't get freed, so the system is still subject to OOM.
    
    But it is not reasonable to priority-boost RCU_SOFTIRQ, so this commit
    moves the callback invocations to a kthread, which can be boosted easily.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 13877cb93a60..ea025a611fcc 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,7 +27,9 @@
 
 #include <linux/cache.h>
 
-#define rcu_init_sched()	do { } while (0)
+static inline void rcu_init(void)
+{
+}
 
 #ifdef CONFIG_TINY_RCU
 
@@ -125,16 +127,12 @@ static inline void rcu_cpu_stall_reset(void)
 }
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
-
 extern int rcu_scheduler_active __read_mostly;
 extern void rcu_scheduler_starting(void);
-
 #else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
-
 static inline void rcu_scheduler_starting(void)
 {
 }
-
 #endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
 #endif /* __LINUX_RCUTINY_H */

commit 7b0b759b65247cbc66384a912be9acf8d4800636
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 17 14:18:46 2010 -0700

    rcu: combine duplicate code, courtesy of CONFIG_PREEMPT_RCU
    
    The CONFIG_PREEMPT_RCU kernel configuration parameter was recently
    re-introduced, but as an indication of the type of RCU (preemptible
    vs. non-preemptible) instead of as selecting a given implementation.
    This commit uses CONFIG_PREEMPT_RCU to combine duplicate code
    from include/linux/rcutiny.h and include/linux/rcutree.h into
    include/linux/rcupdate.h.  This commit also combines a few other pieces
    of duplicate code that have accumulated.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index c6b11dc5ba0a..13877cb93a60 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -27,34 +27,10 @@
 
 #include <linux/cache.h>
 
-void rcu_sched_qs(int cpu);
-void rcu_bh_qs(int cpu);
-
-#ifdef CONFIG_TINY_RCU
-#define __rcu_read_lock()	preempt_disable()
-#define __rcu_read_unlock()	preempt_enable()
-#else /* #ifdef CONFIG_TINY_RCU */
-void __rcu_read_lock(void);
-void __rcu_read_unlock(void);
-#endif /* #else #ifdef CONFIG_TINY_RCU */
-#define __rcu_read_lock_bh()	local_bh_disable()
-#define __rcu_read_unlock_bh()	local_bh_enable()
-extern void call_rcu_sched(struct rcu_head *head,
-			   void (*func)(struct rcu_head *rcu));
-
 #define rcu_init_sched()	do { } while (0)
 
-extern void synchronize_sched(void);
-
 #ifdef CONFIG_TINY_RCU
 
-#define call_rcu		call_rcu_sched
-
-static inline void synchronize_rcu(void)
-{
-	synchronize_sched();
-}
-
 static inline void synchronize_rcu_expedited(void)
 {
 	synchronize_sched();	/* Only one CPU, so pretty fast anyway!!! */
@@ -67,7 +43,6 @@ static inline void rcu_barrier(void)
 
 #else /* #ifdef CONFIG_TINY_RCU */
 
-void synchronize_rcu(void);
 void rcu_barrier(void);
 void synchronize_rcu_expedited(void);
 
@@ -83,25 +58,6 @@ static inline void synchronize_rcu_bh_expedited(void)
 	synchronize_sched();
 }
 
-struct notifier_block;
-
-#ifdef CONFIG_NO_HZ
-
-extern void rcu_enter_nohz(void);
-extern void rcu_exit_nohz(void);
-
-#else /* #ifdef CONFIG_NO_HZ */
-
-static inline void rcu_enter_nohz(void)
-{
-}
-
-static inline void rcu_exit_nohz(void)
-{
-}
-
-#endif /* #else #ifdef CONFIG_NO_HZ */
-
 #ifdef CONFIG_TINY_RCU
 
 static inline void rcu_preempt_note_context_switch(void)
@@ -117,11 +73,6 @@ static inline int rcu_needs_cpu(int cpu)
 	return 0;
 }
 
-static inline int rcu_preempt_depth(void)
-{
-	return 0;
-}
-
 #else /* #ifdef CONFIG_TINY_RCU */
 
 void rcu_preempt_note_context_switch(void);
@@ -141,8 +92,6 @@ static inline void rcu_note_context_switch(int cpu)
 	rcu_preempt_note_context_switch();
 }
 
-extern void rcu_check_callbacks(int cpu, int user);
-
 /*
  * Return the number of grace periods.
  */

commit a3dc3fb161f9b4066c0fce22db72638af8baf83b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 13 16:16:25 2010 -0700

    rcu: repair code-duplication FIXMEs
    
    Combine the duplicate definitions of ULONG_CMP_GE(), ULONG_CMP_LT(),
    and rcu_preempt_depth() into include/linux/rcupdate.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 3fa179784e18..c6b11dc5ba0a 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -133,13 +133,6 @@ static inline int rcu_needs_cpu(int cpu)
 	return rcu_preempt_needs_cpu();
 }
 
-/*
- * Defined as macro as it is a very low level header
- * included from areas that don't even know about current
- * FIXME: combine with include/linux/rcutree.h into rcupdate.h.
- */
-#define rcu_preempt_depth() (current->rcu_read_lock_nesting)
-
 #endif /* #else #ifdef CONFIG_TINY_RCU */
 
 static inline void rcu_note_context_switch(int cpu)

commit 53d84e004d5e8c018be395c4330dc72fd60bd13e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 10 14:28:53 2010 -0700

    rcu: permit suppressing current grace period's CPU stall warnings
    
    When using a kernel debugger, a long sojourn in the debugger can get
    you lots of RCU CPU stall warnings once you resume.  This might not be
    helpful, especially if you are using the system console.  This patch
    therefore allows RCU CPU stall warnings to be suppressed, but only for
    the duration of the current set of grace periods.
    
    This differs from Jason's original patch in that it adds support for
    tiny RCU and preemptible RCU, and uses a slightly different method for
    suppressing the RCU CPU stall warning messages.
    
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Jason Wessel <jason.wessel@windriver.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 4cc5eba41616..3fa179784e18 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -178,6 +178,10 @@ static inline void rcu_sched_force_quiescent_state(void)
 {
 }
 
+static inline void rcu_cpu_stall_reset(void)
+{
+}
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 extern int rcu_scheduler_active __read_mostly;

commit a57eb940d130477a799dfb24a570ee04979c0f7f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 29 16:49:16 2010 -0700

    rcu: Add a TINY_PREEMPT_RCU
    
    Implement a small-memory-footprint uniprocessor-only implementation of
    preemptible RCU.  This implementation uses but a single blocked-tasks
    list rather than the combinatorial number used per leaf rcu_node by
    TREE_PREEMPT_RCU, which reduces memory consumption and greatly simplifies
    processing.  This version also takes advantage of uniprocessor execution
    to accelerate grace periods in the case where there are no readers.
    
    The general design is otherwise broadly similar to that of TREE_PREEMPT_RCU.
    
    This implementation is a step towards having RCU implementation driven
    off of the SMP and PREEMPT kernel configuration variables, which can
    happen once this implementation has accumulated sufficient experience.
    
    Removed ACCESS_ONCE() from __rcu_read_unlock() and added barrier() as
    suggested by Steve Rostedt in order to avoid the compiler-reordering
    issue noted by Mathieu Desnoyers (http://lkml.org/lkml/2010/8/16/183).
    
    As can be seen below, CONFIG_TINY_PREEMPT_RCU represents almost 5Kbyte
    savings compared to CONFIG_TREE_PREEMPT_RCU.  Of course, for non-real-time
    workloads, CONFIG_TINY_RCU is even better.
    
            CONFIG_TREE_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               6170     825      28    7023    kernel/rcutree.o
                                       ----
                                       7026    Total
    
            CONFIG_TINY_PREEMPT_RCU
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
               2081      81       8    2170    kernel/rcutiny.o
                                       ----
                                       2183    Total
    
            CONFIG_TINY_RCU (non-preemptible)
    
               text    data     bss     dec    filename
                 13       0       0      13    kernel/rcupdate.o
                719      25       0     744    kernel/rcutiny.o
                                        ---
                                        757    Total
    
    Requested-by: Loc Minier <loic.minier@canonical.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index e2e893144a84..4cc5eba41616 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -29,66 +29,51 @@
 
 void rcu_sched_qs(int cpu);
 void rcu_bh_qs(int cpu);
-static inline void rcu_note_context_switch(int cpu)
-{
-	rcu_sched_qs(cpu);
-}
 
+#ifdef CONFIG_TINY_RCU
 #define __rcu_read_lock()	preempt_disable()
 #define __rcu_read_unlock()	preempt_enable()
+#else /* #ifdef CONFIG_TINY_RCU */
+void __rcu_read_lock(void);
+void __rcu_read_unlock(void);
+#endif /* #else #ifdef CONFIG_TINY_RCU */
 #define __rcu_read_lock_bh()	local_bh_disable()
 #define __rcu_read_unlock_bh()	local_bh_enable()
-#define call_rcu_sched		call_rcu
+extern void call_rcu_sched(struct rcu_head *head,
+			   void (*func)(struct rcu_head *rcu));
 
 #define rcu_init_sched()	do { } while (0)
-extern void rcu_check_callbacks(int cpu, int user);
 
-static inline int rcu_needs_cpu(int cpu)
-{
-	return 0;
-}
+extern void synchronize_sched(void);
 
-/*
- * Return the number of grace periods.
- */
-static inline long rcu_batches_completed(void)
-{
-	return 0;
-}
+#ifdef CONFIG_TINY_RCU
 
-/*
- * Return the number of bottom-half grace periods.
- */
-static inline long rcu_batches_completed_bh(void)
-{
-	return 0;
-}
+#define call_rcu		call_rcu_sched
 
-static inline void rcu_force_quiescent_state(void)
+static inline void synchronize_rcu(void)
 {
+	synchronize_sched();
 }
 
-static inline void rcu_bh_force_quiescent_state(void)
+static inline void synchronize_rcu_expedited(void)
 {
+	synchronize_sched();	/* Only one CPU, so pretty fast anyway!!! */
 }
 
-static inline void rcu_sched_force_quiescent_state(void)
+static inline void rcu_barrier(void)
 {
+	rcu_barrier_sched();  /* Only one CPU, so only one list of callbacks! */
 }
 
-extern void synchronize_sched(void);
+#else /* #ifdef CONFIG_TINY_RCU */
 
-static inline void synchronize_rcu(void)
-{
-	synchronize_sched();
-}
+void synchronize_rcu(void);
+void rcu_barrier(void);
+void synchronize_rcu_expedited(void);
 
-static inline void synchronize_rcu_bh(void)
-{
-	synchronize_sched();
-}
+#endif /* #else #ifdef CONFIG_TINY_RCU */
 
-static inline void synchronize_rcu_expedited(void)
+static inline void synchronize_rcu_bh(void)
 {
 	synchronize_sched();
 }
@@ -117,15 +102,82 @@ static inline void rcu_exit_nohz(void)
 
 #endif /* #else #ifdef CONFIG_NO_HZ */
 
+#ifdef CONFIG_TINY_RCU
+
+static inline void rcu_preempt_note_context_switch(void)
+{
+}
+
 static inline void exit_rcu(void)
 {
 }
 
+static inline int rcu_needs_cpu(int cpu)
+{
+	return 0;
+}
+
 static inline int rcu_preempt_depth(void)
 {
 	return 0;
 }
 
+#else /* #ifdef CONFIG_TINY_RCU */
+
+void rcu_preempt_note_context_switch(void);
+extern void exit_rcu(void);
+int rcu_preempt_needs_cpu(void);
+
+static inline int rcu_needs_cpu(int cpu)
+{
+	return rcu_preempt_needs_cpu();
+}
+
+/*
+ * Defined as macro as it is a very low level header
+ * included from areas that don't even know about current
+ * FIXME: combine with include/linux/rcutree.h into rcupdate.h.
+ */
+#define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+
+#endif /* #else #ifdef CONFIG_TINY_RCU */
+
+static inline void rcu_note_context_switch(int cpu)
+{
+	rcu_sched_qs(cpu);
+	rcu_preempt_note_context_switch();
+}
+
+extern void rcu_check_callbacks(int cpu, int user);
+
+/*
+ * Return the number of grace periods.
+ */
+static inline long rcu_batches_completed(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of bottom-half grace periods.
+ */
+static inline long rcu_batches_completed_bh(void)
+{
+	return 0;
+}
+
+static inline void rcu_force_quiescent_state(void)
+{
+}
+
+static inline void rcu_bh_force_quiescent_state(void)
+{
+}
+
+static inline void rcu_sched_force_quiescent_state(void)
+{
+}
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 extern int rcu_scheduler_active __read_mostly;

commit b8ae30ee26d379db436b0b8c8c3ff1b52f69e5d1
Merge: 4d7b4ac22fbe 9c6f7e43b4e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:27:54 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (49 commits)
      stop_machine: Move local variable closer to the usage site in cpu_stop_cpu_callback()
      sched, wait: Use wrapper functions
      sched: Remove a stale comment
      ondemand: Make the iowait-is-busy time a sysfs tunable
      ondemand: Solve a big performance issue by counting IOWAIT time as busy
      sched: Intoduce get_cpu_iowait_time_us()
      sched: Eliminate the ts->idle_lastupdate field
      sched: Fold updating of the last_update_time_info into update_ts_time_stats()
      sched: Update the idle statistics in get_cpu_idle_time_us()
      sched: Introduce a function to update the idle statistics
      sched: Add a comment to get_cpu_idle_time_us()
      cpu_stop: add dummy implementation for UP
      sched: Remove rq argument to the tracepoints
      rcu: need barrier() in UP synchronize_sched_expedited()
      sched: correctly place paranioa memory barriers in synchronize_sched_expedited()
      sched: kill paranoia check in synchronize_sched_expedited()
      sched: replace migration_thread with cpu_stop
      stop_machine: reimplement using cpu_stop
      cpu_stop: implement stop_cpu[s]()
      sched: Fix select_idle_sibling() logic in select_task_rq_fair()
      ...

commit bbad937983147c017c25406860287cb94da9af7c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Apr 2 16:17:17 2010 -0700

    rcu: slim down rcutiny by removing rcu_scheduler_active and friends
    
    TINY_RCU does not need rcu_scheduler_active unless CONFIG_DEBUG_LOCK_ALLOC.
    So conditionally compile rcu_scheduler_active in order to slim down
    rcutiny a bit more.  Also gets rid of an EXPORT_SYMBOL_GPL, which is
    responsible for most of the slimming.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index ff22b97fb979..14e5a76b2c06 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -128,4 +128,17 @@ static inline int rcu_preempt_depth(void)
 	return 0;
 }
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+
+extern int rcu_scheduler_active __read_mostly;
+extern void rcu_scheduler_starting(void);
+
+#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
+static inline void rcu_scheduler_starting(void)
+{
+}
+
+#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+
 #endif /* __LINUX_RCUTINY_H */

commit 25502a6c13745f4650cc59322bd198194f55e796
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 1 17:37:01 2010 -0700

    rcu: refactor RCU's context-switch handling
    
    The addition of preemptible RCU to treercu resulted in a bit of
    confusion and inefficiency surrounding the handling of context switches
    for RCU-sched and for RCU-preempt.  For RCU-sched, a context switch
    is a quiescent state, pure and simple, just like it always has been.
    For RCU-preempt, a context switch is in no way a quiescent state, but
    special handling is required when a task blocks in an RCU read-side
    critical section.
    
    However, the callout from the scheduler and the outer loop in ksoftirqd
    still calls something named rcu_sched_qs(), whose name is no longer
    accurate.  Furthermore, when rcu_check_callbacks() notes an RCU-sched
    quiescent state, it ends up unnecessarily (though harmlessly, aside
    from the performance hit) enqueuing the current task if it happens to
    be running in an RCU-preempt read-side critical section.  This not only
    increases the maximum latency of scheduler_tick(), it also needlessly
    increases the overhead of the next outermost rcu_read_unlock() invocation.
    
    This patch addresses this situation by separating the notion of RCU's
    context-switch handling from that of RCU-sched's quiescent states.
    The context-switch handling is covered by rcu_note_context_switch() in
    general and by rcu_preempt_note_context_switch() for preemptible RCU.
    This permits rcu_sched_qs() to handle quiescent states and only quiescent
    states.  It also reduces the maximum latency of scheduler_tick(), though
    probably by much less than a microsecond.  Finally, it means that tasks
    within preemptible-RCU read-side critical sections avoid incurring the
    overhead of queuing unless there really is a context switch.
    
    Suggested-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index bbeb55b7709b..ff22b97fb979 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -29,6 +29,10 @@
 
 void rcu_sched_qs(int cpu);
 void rcu_bh_qs(int cpu);
+static inline void rcu_note_context_switch(int cpu)
+{
+	rcu_sched_qs(cpu);
+}
 
 #define __rcu_read_lock()	preempt_disable()
 #define __rcu_read_unlock()	preempt_enable()

commit da848c47bc6e873a54a445ea1960423a495b6b32
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 30 15:46:01 2010 -0700

    rcu: shrink rcutiny by making synchronize_rcu_bh() be inline
    
    Because synchronize_rcu_bh() is identical to synchronize_sched(),
    make the former a static inline invoking the latter, saving the
    overhead of an EXPORT_SYMBOL_GPL() and the duplicate code.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index a5195875480a..bbeb55b7709b 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -74,7 +74,17 @@ static inline void rcu_sched_force_quiescent_state(void)
 {
 }
 
-#define synchronize_rcu synchronize_sched
+extern void synchronize_sched(void);
+
+static inline void synchronize_rcu(void)
+{
+	synchronize_sched();
+}
+
+static inline void synchronize_rcu_bh(void)
+{
+	synchronize_sched();
+}
 
 static inline void synchronize_rcu_expedited(void)
 {

commit 969c79215a35b06e5e3efe69b9412f858df7856c
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 6 18:49:21 2010 +0200

    sched: replace migration_thread with cpu_stop
    
    Currently migration_thread is serving three purposes - migration
    pusher, context to execute active_load_balance() and forced context
    switcher for expedited RCU synchronize_sched.  All three roles are
    hardcoded into migration_thread() and determining which job is
    scheduled is slightly messy.
    
    This patch kills migration_thread and replaces all three uses with
    cpu_stop.  The three different roles of migration_thread() are
    splitted into three separate cpu_stop callbacks -
    migration_cpu_stop(), active_load_balance_cpu_stop() and
    synchronize_sched_expedited_cpu_stop() - and each use case now simply
    asks cpu_stop to execute the callback as necessary.
    
    synchronize_sched_expedited() was implemented with private
    preallocated resources and custom multi-cpu queueing and waiting
    logic, both of which are provided by cpu_stop.
    synchronize_sched_expedited_count is made atomic and all other shared
    resources along with the mutex are dropped.
    
    synchronize_sched_expedited() also implemented a check to detect cases
    where not all the callback got executed on their assigned cpus and
    fall back to synchronize_sched().  If called with cpu hotplug blocked,
    cpu_stop already guarantees that and the condition cannot happen;
    otherwise, stop_machine() would break.  However, this patch preserves
    the paranoid check using a cpumask to record on which cpus the stopper
    ran so that it can serve as a bisection point if something actually
    goes wrong theree.
    
    Because the internal execution state is no longer visible,
    rcu_expedited_torture_stats() is removed.
    
    This patch also renames cpu_stop threads to from "stopper/%d" to
    "migration/%d".  The names of these threads ultimately don't matter
    and there's no reason to make unnecessary userland visible changes.
    
    With this patch applied, stop_machine() and sched now share the same
    resources.  stop_machine() is faster without wasting any resources and
    sched migration users are much cleaner.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Cc: Josh Triplett <josh@freedesktop.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Dimitri Sivanich <sivanich@sgi.com>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index a5195875480a..0006b2df00e1 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -60,8 +60,6 @@ static inline long rcu_batches_completed_bh(void)
 	return 0;
 }
 
-extern int rcu_expedited_torture_stats(char *page);
-
 static inline void rcu_force_quiescent_state(void)
 {
 }

commit d9f1bb6ad7fc53c406706f47858dd5ff030b14a3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Feb 25 14:06:47 2010 -0800

    rcu: Make rcu_read_lock_sched_held() take boot time into account
    
    Before the scheduler starts, all tasks are non-preemptible by
    definition. So, during that time, rcu_read_lock_sched_held()
    needs to always return "true".  This patch makes that be so.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1267135607-7056-2-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 2b70d4e37383..a5195875480a 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -105,10 +105,6 @@ static inline void rcu_exit_nohz(void)
 
 #endif /* #else #ifdef CONFIG_NO_HZ */
 
-static inline void rcu_scheduler_starting(void)
-{
-}
-
 static inline void exit_rcu(void)
 {
 }

commit bf66f18e79e34c421bbd8f6511e2c556b779df2f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jan 4 15:09:10 2010 -0800

    rcu: Add force_quiescent_state() testing to rcutorture
    
    Add force_quiescent_state() testing to rcutorture, with a
    separate thread that repeatedly invokes force_quiescent_state()
    in bursts. This can greatly increase the probability of
    encountering certain types of race conditions.
    
    Suggested-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1262646551116-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 96cc307ed9f4..2b70d4e37383 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -62,6 +62,18 @@ static inline long rcu_batches_completed_bh(void)
 
 extern int rcu_expedited_torture_stats(char *page);
 
+static inline void rcu_force_quiescent_state(void)
+{
+}
+
+static inline void rcu_bh_force_quiescent_state(void)
+{
+}
+
+static inline void rcu_sched_force_quiescent_state(void)
+{
+}
+
 #define synchronize_rcu synchronize_sched
 
 static inline void synchronize_rcu_expedited(void)

commit 234da7bcdc7aaa935846534c3b726dbc79a9cdd5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 16 20:21:05 2009 +0100

    sched: Teach might_sleep() about preemptible RCU
    
    In practice, it is harmless to voluntarily sleep in a
    rcu_read_lock() section if we are running under preempt rcu, but
    it is illegal if we build a kernel running non-preemptable rcu.
    
    Currently, might_sleep() doesn't notice sleepable operations
    under rcu_read_lock() sections if we are running under
    preemptable rcu because preempt_count() is left untouched after
    rcu_read_lock() in this case. But we want developers who test
    their changes under such config to notice the "sleeping while
    atomic" issues.
    
    So we add rcu_read_lock_nesting to prempt_count() in
    might_sleep() checks.
    
    [ v2: Handle rcu-tiny ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1260991265-8451-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index c4ba9a78721e..96cc307ed9f4 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -101,4 +101,9 @@ static inline void exit_rcu(void)
 {
 }
 
+static inline int rcu_preempt_depth(void)
+{
+	return 0;
+}
+
 #endif /* __LINUX_RCUTINY_H */

commit 6ebb237bece23275d1da149b61a342f0d4d06a08
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 22 08:53:50 2009 -0800

    rcu: Re-arrange code to reduce #ifdef pain
    
    Remove #ifdefs from kernel/rcupdate.c and
    include/linux/rcupdate.h by moving code to
    include/linux/rcutiny.h, include/linux/rcutree.h, and
    kernel/rcutree.c.
    
    Also remove some definitions that are no longer used.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1258908830885-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index a3b6272af2dd..c4ba9a78721e 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -39,6 +39,11 @@ void rcu_bh_qs(int cpu);
 #define rcu_init_sched()	do { } while (0)
 extern void rcu_check_callbacks(int cpu, int user);
 
+static inline int rcu_needs_cpu(int cpu)
+{
+	return 0;
+}
+
 /*
  * Return the number of grace periods.
  */
@@ -57,6 +62,8 @@ static inline long rcu_batches_completed_bh(void)
 
 extern int rcu_expedited_torture_stats(char *page);
 
+#define synchronize_rcu synchronize_sched
+
 static inline void synchronize_rcu_expedited(void)
 {
 	synchronize_sched();
@@ -86,6 +93,10 @@ static inline void rcu_exit_nohz(void)
 
 #endif /* #else #ifdef CONFIG_NO_HZ */
 
+static inline void rcu_scheduler_starting(void)
+{
+}
+
 static inline void exit_rcu(void)
 {
 }

commit 9f680ab41485edfdc96331b70afa7513aa0a7720
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 22 08:53:49 2009 -0800

    rcu: Eliminate unneeded function wrapping
    
    The functions rcu_init() is a wrapper for __rcu_init(), and also
    sets up the CPU-hotplug notifier for rcu_barrier_cpu_hotplug().
    But TINY_RCU doesn't need CPU-hotplug notification, and the
    rcu_barrier_cpu_hotplug() is a simple wrapper for
    rcu_cpu_notify().
    
    So push rcu_init() out to kernel/rcutree.c and kernel/rcutiny.c
    and get rid of the wrapper function rcu_barrier_cpu_hotplug().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12589088302320-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 2c1fe8373e71..a3b6272af2dd 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -38,7 +38,6 @@ void rcu_bh_qs(int cpu);
 
 #define rcu_init_sched()	do { } while (0)
 extern void rcu_check_callbacks(int cpu, int user);
-extern void __rcu_init(void);
 
 /*
  * Return the number of grace periods.
@@ -69,7 +68,6 @@ static inline void synchronize_rcu_bh_expedited(void)
 }
 
 struct notifier_block;
-extern int rcu_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu);
 
 #ifdef CONFIG_NO_HZ
 

commit 4ce5b90340879ce93d169b7b523c2cbbe7c45843
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 26 07:55:55 2009 +0100

    rcu: Do tiny cleanups in rcutiny
    
    No change in functionality - just straighten out a few small
    stylistic details.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: avi@redhat.com
    Cc: mtosatti@redhat.com
    LKML-Reference: <12565226351355-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
index 891073c264dc..2c1fe8373e71 100644
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@ -20,9 +20,8 @@
  * Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -
- * 		Documentation/RCU
+ *		Documentation/RCU
  */
-
 #ifndef __LINUX_TINY_H
 #define __LINUX_TINY_H
 
@@ -70,8 +69,7 @@ static inline void synchronize_rcu_bh_expedited(void)
 }
 
 struct notifier_block;
-extern int rcu_cpu_notify(struct notifier_block *self,
-			  unsigned long action, void *hcpu);
+extern int rcu_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu);
 
 #ifdef CONFIG_NO_HZ
 

commit 9b1d82fa1611706fa7ee1505f290160a18caf95d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Oct 25 19:03:50 2009 -0700

    rcu: "Tiny RCU", The Bloatwatch Edition
    
    This patch is a version of RCU designed for !SMP provided for a
    small-footprint RCU implementation.  In particular, the
    implementation of synchronize_rcu() is extremely lightweight and
    high performance. It passes rcutorture testing in each of the
    four relevant configurations (combinations of NO_HZ and PREEMPT)
    on x86.  This saves about 1K bytes compared to old Classic RCU
    (which is no longer in mainline), and more than three kilobytes
    compared to Hierarchical RCU (updated to 2.6.30):
    
            CONFIG_TREE_RCU:
    
               text    data     bss     dec     filename
                183       4       0     187     kernel/rcupdate.o
               2783     520      36    3339     kernel/rcutree.o
                                       3526 Total (vs 4565 for v7)
    
            CONFIG_TREE_PREEMPT_RCU:
    
               text    data     bss     dec     filename
                263       4       0     267     kernel/rcupdate.o
               4594     776      52    5422     kernel/rcutree.o
                                       5689 Total (6155 for v7)
    
            CONFIG_TINY_RCU:
    
               text    data     bss     dec     filename
                 96       4       0     100     kernel/rcupdate.o
                734      24       0     758     kernel/rcutiny.o
                                        858 Total (vs 848 for v7)
    
    The above is for x86.  Your mileage may vary on other platforms.
    Further compression is possible, but is being procrastinated.
    
    Changes from v7 (http://lkml.org/lkml/2009/10/9/388)
    
    o       Apply Lai Jiangshan's review comments (aside from
    might_sleep()   in synchronize_sched(), which is covered by SMP builds).
    
    o       Fix up expedited primitives.
    
    Changes from v6 (http://lkml.org/lkml/2009/9/23/293).
    
    o       Forward ported to put it into the 2.6.33 stream.
    
    o       Added lockdep support.
    
    o       Make lightweight rcu_barrier.
    
    Changes from v5 (http://lkml.org/lkml/2009/6/23/12).
    
    o       Ported to latest pre-2.6.32 merge window kernel.
    
            - Renamed rcu_qsctr_inc() to rcu_sched_qs().
            - Renamed rcu_bh_qsctr_inc() to rcu_bh_qs().
            - Provided trivial rcu_cpu_notify().
            - Provided trivial exit_rcu().
            - Provided trivial rcu_needs_cpu().
            - Fixed up the rcu_*_enter/exit() functions in linux/hardirq.h.
    
    o       Removed the dependence on EMBEDDED, with a view to making
            TINY_RCU default for !SMP at some time in the future.
    
    o       Added (trivial) support for expedited grace periods.
    
    Changes from v4 (http://lkml.org/lkml/2009/5/2/91) include:
    
    o       Squeeze the size down a bit further by removing the
            ->completed field from struct rcu_ctrlblk.
    
    o       This permits synchronize_rcu() to become the empty function.
            Previous concerns about rcutorture were unfounded, as
            rcutorture correctly handles a constant value from
            rcu_batches_completed() and rcu_batches_completed_bh().
    
    Changes from v3 (http://lkml.org/lkml/2009/3/29/221) include:
    
    o       Changed rcu_batches_completed(), rcu_batches_completed_bh()
            rcu_enter_nohz(), rcu_exit_nohz(), rcu_nmi_enter(), and
            rcu_nmi_exit(), to be static inlines, as suggested by David
            Howells.  Doing this saves about 100 bytes from rcutiny.o.
            (The numbers between v3 and this v4 of the patch are not directly
            comparable, since they are against different versions of Linux.)
    
    Changes from v2 (http://lkml.org/lkml/2009/2/3/333) include:
    
    o       Fix whitespace issues.
    
    o       Change short-circuit "||" operator to instead be "+" in order
    to      fix performance bug noted by "kraai" on LWN.
    
                    (http://lwn.net/Articles/324348/)
    
    Changes from v1 (http://lkml.org/lkml/2009/1/13/440) include:
    
    o       This version depends on EMBEDDED as well as !SMP, as suggested
            by Ingo.
    
    o       Updated rcu_needs_cpu() to unconditionally return zero,
            permitting the CPU to enter dynticks-idle mode at any time.
            This works because callbacks can be invoked upon entry to
            dynticks-idle mode.
    
    o       Paul is now OK with this being included, based on a poll at
    the     Kernel Miniconf at linux.conf.au, where about ten people said
            that they cared about saving 900 bytes on single-CPU systems.
    
    o       Applies to both mainline and tip/core/rcu.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Acked-by: Josh Triplett <josh@joshtriplett.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: avi@redhat.com
    Cc: mtosatti@redhat.com
    LKML-Reference: <12565226351355-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/linux/rcutiny.h b/include/linux/rcutiny.h
new file mode 100644
index 000000000000..891073c264dc
--- /dev/null
+++ b/include/linux/rcutiny.h
@@ -0,0 +1,97 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright IBM Corporation, 2008
+ *
+ * Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ * 		Documentation/RCU
+ */
+
+#ifndef __LINUX_TINY_H
+#define __LINUX_TINY_H
+
+#include <linux/cache.h>
+
+void rcu_sched_qs(int cpu);
+void rcu_bh_qs(int cpu);
+
+#define __rcu_read_lock()	preempt_disable()
+#define __rcu_read_unlock()	preempt_enable()
+#define __rcu_read_lock_bh()	local_bh_disable()
+#define __rcu_read_unlock_bh()	local_bh_enable()
+#define call_rcu_sched		call_rcu
+
+#define rcu_init_sched()	do { } while (0)
+extern void rcu_check_callbacks(int cpu, int user);
+extern void __rcu_init(void);
+
+/*
+ * Return the number of grace periods.
+ */
+static inline long rcu_batches_completed(void)
+{
+	return 0;
+}
+
+/*
+ * Return the number of bottom-half grace periods.
+ */
+static inline long rcu_batches_completed_bh(void)
+{
+	return 0;
+}
+
+extern int rcu_expedited_torture_stats(char *page);
+
+static inline void synchronize_rcu_expedited(void)
+{
+	synchronize_sched();
+}
+
+static inline void synchronize_rcu_bh_expedited(void)
+{
+	synchronize_sched();
+}
+
+struct notifier_block;
+extern int rcu_cpu_notify(struct notifier_block *self,
+			  unsigned long action, void *hcpu);
+
+#ifdef CONFIG_NO_HZ
+
+extern void rcu_enter_nohz(void);
+extern void rcu_exit_nohz(void);
+
+#else /* #ifdef CONFIG_NO_HZ */
+
+static inline void rcu_enter_nohz(void)
+{
+}
+
+static inline void rcu_exit_nohz(void)
+{
+}
+
+#endif /* #else #ifdef CONFIG_NO_HZ */
+
+static inline void exit_rcu(void)
+{
+}
+
+#endif /* __LINUX_RCUTINY_H */
