commit 7a0e27b2a0ce2735e27e21ebc8b777550fe0ed81
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 25 20:30:47 2020 -0700

    mm: remove vmalloc_exec
    
    Merge vmalloc_exec into its only caller.  Note that for !CONFIG_MMU
    __vmalloc_node_range maps to __vmalloc, which directly clears the
    __GFP_HIGHMEM added by the vmalloc_exec stub anyway.
    
    Link: http://lkml.kernel.org/r/20200618064307.32739-4-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dexuan Cui <decui@microsoft.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 48bb681e6c2a..0221f852a7e1 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -106,7 +106,6 @@ extern void *vzalloc(unsigned long size);
 extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);
 extern void *vzalloc_node(unsigned long size, int node);
-extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask);

commit 73f693c3a705756032c2863bfb37570276902d7d
Author: Joerg Roedel <jroedel@suse.de>
Date:   Mon Jun 1 21:52:36 2020 -0700

    mm: remove vmalloc_sync_(un)mappings()
    
    These functions are not needed anymore because the vmalloc and ioremap
    mappings are now synchronized when they are created or torn down.
    
    Remove all callers and function definitions.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/20200515140023.25469-7-joro@8bytes.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 0efc35dc5b25..48bb681e6c2a 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -130,8 +130,6 @@ extern int remap_vmalloc_range_partial(struct vm_area_struct *vma,
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
-void vmalloc_sync_mappings(void);
-void vmalloc_sync_unmappings(void);
 
 /*
  * Architectures can set this mask to a combination of PGTBL_P?D_MODIFIED values

commit 2ba3e6947aed9bb9575eb1603c0ac6e39185d32a
Author: Joerg Roedel <jroedel@suse.de>
Date:   Mon Jun 1 21:52:22 2020 -0700

    mm/vmalloc: track which page-table levels were modified
    
    Track at which levels in the page-table entries were modified by
    vmap/vunmap.
    
    After the page-table has been modified, use that information do decide
    whether the new arch_sync_kernel_mappings() needs to be called.
    
    [akpm@linux-foundation.org: map_kernel_range_noflush() needs the arch_sync_kernel_mappings() call]
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/20200515140023.25469-3-joro@8bytes.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 3332926295d4..0efc35dc5b25 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -133,6 +133,22 @@ extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 void vmalloc_sync_mappings(void);
 void vmalloc_sync_unmappings(void);
 
+/*
+ * Architectures can set this mask to a combination of PGTBL_P?D_MODIFIED values
+ * and let generic vmalloc and ioremap code know when arch_sync_kernel_mappings()
+ * needs to be called.
+ */
+#ifndef ARCH_PAGE_TABLE_SYNC_MASK
+#define ARCH_PAGE_TABLE_SYNC_MASK 0
+#endif
+
+/*
+ * There is no default implementation for arch_sync_kernel_mappings(). It is
+ * relied upon the compiler to optimize calls out if ARCH_PAGE_TABLE_SYNC_MASK
+ * is 0.
+ */
+void arch_sync_kernel_mappings(unsigned long start, unsigned long end);
+
 /*
  *	Lowlevel-APIs (not for driver use!)
  */

commit 041de93ff86fc500aa73e5360039c95f4d31b95f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:52:02 2020 -0700

    mm: remove vmalloc_user_node_flags
    
    Open code it in __bpf_map_area_alloc, which is the only caller.  Also
    clean up __bpf_map_area_alloc to have a single vmalloc call with slightly
    different flags instead of the current two different calls.
    
    For this to compile for the nommu case add a __vmalloc_node_range stub to
    nommu.c.
    
    [akpm@linux-foundation.org: fix nommu.c build]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200414131348.444715-27-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 964d58a060ca..3332926295d4 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -106,7 +106,6 @@ extern void *vzalloc(unsigned long size);
 extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);
 extern void *vzalloc_node(unsigned long size, int node);
-extern void *vmalloc_user_node_flags(unsigned long size, int node, gfp_t flags);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);

commit 2b9059489c839e67ca9254913325e18cea11a980
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:53 2020 -0700

    mm: remove __vmalloc_node_flags_caller
    
    Just use __vmalloc_node instead which gets and extra argument.  To be able
    to to use __vmalloc_node in all caller make it available outside of
    vmalloc and implement it in nommu.c.
    
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200414131348.444715-25-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 91d143783458..964d58a060ca 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -115,8 +115,8 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
 			pgprot_t prot, unsigned long vm_flags, int node,
 			const void *caller);
-extern void *__vmalloc_node_flags_caller(unsigned long size,
-					 int node, gfp_t flags, void *caller);
+void *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,
+		int node, const void *caller);
 
 extern void vfree(const void *addr);
 extern void vfree_atomic(const void *addr);

commit 4d39d7285f45cc6c72b850f040d3addd626658e4
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:49 2020 -0700

    mm: remove both instances of __vmalloc_node_flags
    
    The real version just had a few callers that can open code it and remove
    one layer of indirection.  The nommu stub was public but only had a single
    caller, so remove it and avoid a CONFIG_MMU ifdef in vmalloc.h.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-24-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 1c278e030599..91d143783458 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -115,17 +115,8 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
 			pgprot_t prot, unsigned long vm_flags, int node,
 			const void *caller);
-#ifndef CONFIG_MMU
-extern void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags);
-static inline void *__vmalloc_node_flags_caller(unsigned long size, int node,
-						gfp_t flags, void *caller)
-{
-	return __vmalloc_node_flags(size, node, flags);
-}
-#else
 extern void *__vmalloc_node_flags_caller(unsigned long size,
 					 int node, gfp_t flags, void *caller);
-#endif
 
 extern void vfree(const void *addr);
 extern void vfree_atomic(const void *addr);

commit 88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:40 2020 -0700

    mm: remove the pgprot argument to __vmalloc
    
    The pgprot argument to __vmalloc is always PAGE_KERNEL now, so remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com> [hyperv]
    Acked-by: Gao Xiang <xiang@kernel.org> [erofs]
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Wei Liu <wei.liu@kernel.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-22-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 5488cea5ef11..1c278e030599 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -110,7 +110,7 @@ extern void *vmalloc_user_node_flags(unsigned long size, int node, gfp_t flags);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);
-extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
+extern void *__vmalloc(unsigned long size, gfp_t gfp_mask);
 extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
 			pgprot_t prot, unsigned long vm_flags, int node,

commit d4efd79a81abc7096a418ee3103f261cfb6ab634
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:27 2020 -0700

    mm: remove the prot argument from vm_map_ram
    
    This is always PAGE_KERNEL - for long term mappings with other properties
    vmap should be used.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-19-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index a1e9bdc3ad9e..5488cea5ef11 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -88,8 +88,7 @@ struct vmap_area {
  *	Highlevel APIs for driver use
  */
 extern void vm_unmap_ram(const void *mem, unsigned int count);
-extern void *vm_map_ram(struct page **pages, unsigned int count,
-				int node, pgprot_t prot);
+extern void *vm_map_ram(struct page **pages, unsigned int count, int node);
 extern void vm_unmap_aliases(void);
 
 #ifdef CONFIG_MMU

commit ed1f324c5fed06c91f30a36aedb66f34244ab86e
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:19 2020 -0700

    mm: remove map_vm_range
    
    Switch all callers to map_kernel_range, which symmetric to the unmap side
    (as well as the _noflush versions).
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-17-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index ff69d1e037ca..a1e9bdc3ad9e 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -168,11 +168,11 @@ extern struct vm_struct *__get_vm_area_caller(unsigned long size,
 extern struct vm_struct *remove_vm_area(const void *addr);
 extern struct vm_struct *find_vm_area(const void *addr);
 
-extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
-			struct page **pages);
 #ifdef CONFIG_MMU
 extern int map_kernel_range_noflush(unsigned long start, unsigned long size,
 				    pgprot_t prot, struct page **pages);
+int map_kernel_range(unsigned long start, unsigned long size, pgprot_t prot,
+		struct page **pages);
 extern void unmap_kernel_range_noflush(unsigned long addr, unsigned long size);
 extern void unmap_kernel_range(unsigned long addr, unsigned long size);
 static inline void set_vm_flush_reset_perms(void *addr)
@@ -189,14 +189,12 @@ map_kernel_range_noflush(unsigned long start, unsigned long size,
 {
 	return size >> PAGE_SHIFT;
 }
+#define map_kernel_range map_kernel_range_noflush
 static inline void
 unmap_kernel_range_noflush(unsigned long addr, unsigned long size)
 {
 }
-static inline void
-unmap_kernel_range(unsigned long addr, unsigned long size)
-{
-}
+#define unmap_kernel_range unmap_kernel_range_noflush
 static inline void set_vm_flush_reset_perms(void *addr)
 {
 }

commit 4926627793c0a7e7db2bc674e1d06777e86d8dab
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:50:45 2020 -0700

    mm: remove __get_vm_area
    
    Switch the two remaining callers to use __get_vm_area_caller instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-9-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index a95d3cc74d79..ff69d1e037ca 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -161,8 +161,6 @@ static inline size_t get_vm_area_size(const struct vm_struct *area)
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *get_vm_area_caller(unsigned long size,
 					unsigned long flags, const void *caller);
-extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
-					unsigned long start, unsigned long end);
 extern struct vm_struct *__get_vm_area_caller(unsigned long size,
 					unsigned long flags,
 					unsigned long start, unsigned long end,

commit bdebd6a2831b6fab69eb85cee74a8ba77f1a1cc2
Author: Jann Horn <jannh@google.com>
Date:   Mon Apr 20 18:14:11 2020 -0700

    vmalloc: fix remap_vmalloc_range() bounds checks
    
    remap_vmalloc_range() has had various issues with the bounds checks it
    promises to perform ("This function checks that addr is a valid
    vmalloc'ed area, and that it is big enough to cover the vma") over time,
    e.g.:
    
     - not detecting pgoff<<PAGE_SHIFT overflow
    
     - not detecting (pgoff<<PAGE_SHIFT)+usize overflow
    
     - not checking whether addr and addr+(pgoff<<PAGE_SHIFT) are the same
       vmalloc allocation
    
     - comparing a potentially wildly out-of-bounds pointer with the end of
       the vmalloc region
    
    In particular, since commit fc9702273e2e ("bpf: Add mmap() support for
    BPF_MAP_TYPE_ARRAY"), unprivileged users can cause kernel null pointer
    dereferences by calling mmap() on a BPF map with a size that is bigger
    than the distance from the start of the BPF map to the end of the
    address space.
    
    This could theoretically be used as a kernel ASLR bypass, by using
    whether mmap() with a given offset oopses or returns an error code to
    perform a binary search over the possible address range.
    
    To allow remap_vmalloc_range_partial() to verify that addr and
    addr+(pgoff<<PAGE_SHIFT) are in the same vmalloc region, pass the offset
    to remap_vmalloc_range_partial() instead of adding it to the pointer in
    remap_vmalloc_range().
    
    In remap_vmalloc_range_partial(), fix the check against
    get_vm_area_size() by using size comparisons instead of pointer
    comparisons, and add checks for pgoff.
    
    Fixes: 833423143c3a ("[PATCH] mm: introduce remap_vmalloc_range()")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: stable@vger.kernel.org
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Yonghong Song <yhs@fb.com>
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: John Fastabend <john.fastabend@gmail.com>
    Cc: KP Singh <kpsingh@chromium.org>
    Link: http://lkml.kernel.org/r/20200415222312.236431-1-jannh@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 0507a162ccd0..a95d3cc74d79 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -137,7 +137,7 @@ extern void vunmap(const void *addr);
 
 extern int remap_vmalloc_range_partial(struct vm_area_struct *vma,
 				       unsigned long uaddr, void *kaddr,
-				       unsigned long size);
+				       unsigned long pgoff, unsigned long size);
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);

commit 763802b53a427ed3cbd419dbba255c414fdd9e7c
Author: Joerg Roedel <jroedel@suse.de>
Date:   Sat Mar 21 18:22:41 2020 -0700

    x86/mm: split vmalloc_sync_all()
    
    Commit 3f8fd02b1bf1 ("mm/vmalloc: Sync unmappings in
    __purge_vmap_area_lazy()") introduced a call to vmalloc_sync_all() in
    the vunmap() code-path.  While this change was necessary to maintain
    correctness on x86-32-pae kernels, it also adds additional cycles for
    architectures that don't need it.
    
    Specifically on x86-64 with CONFIG_VMAP_STACK=y some people reported
    severe performance regressions in micro-benchmarks because it now also
    calls the x86-64 implementation of vmalloc_sync_all() on vunmap().  But
    the vmalloc_sync_all() implementation on x86-64 is only needed for newly
    created mappings.
    
    To avoid the unnecessary work on x86-64 and to gain the performance
    back, split up vmalloc_sync_all() into two functions:
    
            * vmalloc_sync_mappings(), and
            * vmalloc_sync_unmappings()
    
    Most call-sites to vmalloc_sync_all() only care about new mappings being
    synchronized.  The only exception is the new call-site added in the
    above mentioned commit.
    
    Shile Zhang directed us to a report of an 80% regression in reaim
    throughput.
    
    Fixes: 3f8fd02b1bf1 ("mm/vmalloc: Sync unmappings in __purge_vmap_area_lazy()")
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Reported-by: Shile Zhang <shile.zhang@linux.alibaba.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Borislav Petkov <bp@suse.de>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>        [GHES]
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: <stable@vger.kernel.org>
    Link: http://lkml.kernel.org/r/20191009124418.8286-1-joro@8bytes.org
    Link: https://lists.01.org/hyperkitty/list/lkp@lists.01.org/thread/4D3JPPHBNOSPFK2KEPC6KGKS6J25AIDB/
    Link: http://lkml.kernel.org/r/20191113095530.228959-1-shile.zhang@linux.alibaba.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index ec3813236699..0507a162ccd0 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -141,8 +141,9 @@ extern int remap_vmalloc_range_partial(struct vm_area_struct *vma,
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
-void vmalloc_sync_all(void);
- 
+void vmalloc_sync_mappings(void);
+void vmalloc_sync_unmappings(void);
+
 /*
  *	Lowlevel-APIs (not for driver use!)
  */

commit 1f059dfdf5d170dccbac92193be2fee3c1763384
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Nov 28 08:19:36 2019 +0100

    mm/vmalloc: Add empty <asm/vmalloc.h> headers and use them from <linux/vmalloc.h>
    
    In the x86 MM code we'd like to untangle various types of historic
    header dependency spaghetti, but for this we'd need to pass to
    the generic vmalloc code various vmalloc related defines that
    customarily come via the <asm/page.h> low level arch header.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index a4b241102771..ec3813236699 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -10,6 +10,8 @@
 #include <linux/rbtree.h>
 #include <linux/overflow.h>
 
+#include <asm/vmalloc.h>
+
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 struct notifier_block;		/* in notifier.h */
 

commit 3c5c3cfb9ef4da957e3357a2bd36f76ee34c0862
Author: Daniel Axtens <dja@axtens.net>
Date:   Sat Nov 30 17:54:50 2019 -0800

    kasan: support backing vmalloc space with real shadow memory
    
    Patch series "kasan: support backing vmalloc space with real shadow
    memory", v11.
    
    Currently, vmalloc space is backed by the early shadow page.  This means
    that kasan is incompatible with VMAP_STACK.
    
    This series provides a mechanism to back vmalloc space with real,
    dynamically allocated memory.  I have only wired up x86, because that's
    the only currently supported arch I can work with easily, but it's very
    easy to wire up other architectures, and it appears that there is some
    work-in-progress code to do this on arm64 and s390.
    
    This has been discussed before in the context of VMAP_STACK:
     - https://bugzilla.kernel.org/show_bug.cgi?id=202009
     - https://lkml.org/lkml/2018/7/22/198
     - https://lkml.org/lkml/2019/7/19/822
    
    In terms of implementation details:
    
    Most mappings in vmalloc space are small, requiring less than a full
    page of shadow space.  Allocating a full shadow page per mapping would
    therefore be wasteful.  Furthermore, to ensure that different mappings
    use different shadow pages, mappings would have to be aligned to
    KASAN_SHADOW_SCALE_SIZE * PAGE_SIZE.
    
    Instead, share backing space across multiple mappings.  Allocate a
    backing page when a mapping in vmalloc space uses a particular page of
    the shadow region.  This page can be shared by other vmalloc mappings
    later on.
    
    We hook in to the vmap infrastructure to lazily clean up unused shadow
    memory.
    
    Testing with test_vmalloc.sh on an x86 VM with 2 vCPUs shows that:
    
     - Turning on KASAN, inline instrumentation, without vmalloc, introuduces
       a 4.1x-4.2x slowdown in vmalloc operations.
    
     - Turning this on introduces the following slowdowns over KASAN:
         * ~1.76x slower single-threaded (test_vmalloc.sh performance)
         * ~2.18x slower when both cpus are performing operations
           simultaneously (test_vmalloc.sh sequential_test_order=1)
    
    This is unfortunate but given that this is a debug feature only, not the
    end of the world.  The benchmarks are also a stress-test for the vmalloc
    subsystem: they're not indicative of an overall 2x slowdown!
    
    This patch (of 4):
    
    Hook into vmalloc and vmap, and dynamically allocate real shadow memory
    to back the mappings.
    
    Most mappings in vmalloc space are small, requiring less than a full
    page of shadow space.  Allocating a full shadow page per mapping would
    therefore be wasteful.  Furthermore, to ensure that different mappings
    use different shadow pages, mappings would have to be aligned to
    KASAN_SHADOW_SCALE_SIZE * PAGE_SIZE.
    
    Instead, share backing space across multiple mappings.  Allocate a
    backing page when a mapping in vmalloc space uses a particular page of
    the shadow region.  This page can be shared by other vmalloc mappings
    later on.
    
    We hook in to the vmap infrastructure to lazily clean up unused shadow
    memory.
    
    To avoid the difficulties around swapping mappings around, this code
    expects that the part of the shadow region that covers the vmalloc space
    will not be covered by the early shadow page, but will be left unmapped.
    This will require changes in arch-specific code.
    
    This allows KASAN with VMAP_STACK, and may be helpful for architectures
    that do not have a separate module space (e.g.  powerpc64, which I am
    currently working on).  It also allows relaxing the module alignment
    back to PAGE_SIZE.
    
    Testing with test_vmalloc.sh on an x86 VM with 2 vCPUs shows that:
    
     - Turning on KASAN, inline instrumentation, without vmalloc, introuduces
       a 4.1x-4.2x slowdown in vmalloc operations.
    
     - Turning this on introduces the following slowdowns over KASAN:
         * ~1.76x slower single-threaded (test_vmalloc.sh performance)
         * ~2.18x slower when both cpus are performing operations
           simultaneously (test_vmalloc.sh sequential_test_order=3D1)
    
    This is unfortunate but given that this is a debug feature only, not the
    end of the world.
    
    The full benchmark results are:
    
    Performance
    
                                  No KASAN      KASAN original x baseline  KASAN vmalloc x baseline    x KASAN
    
    fix_size_alloc_test             662004            11404956      17.23       19144610      28.92       1.68
    full_fit_alloc_test             710950            12029752      16.92       13184651      18.55       1.10
    long_busy_list_alloc_test      9431875            43990172       4.66       82970178       8.80       1.89
    random_size_alloc_test         5033626            23061762       4.58       47158834       9.37       2.04
    fix_align_alloc_test           1252514            15276910      12.20       31266116      24.96       2.05
    random_size_align_alloc_te     1648501            14578321       8.84       25560052      15.51       1.75
    align_shift_alloc_test             147                 830       5.65           5692      38.72       6.86
    pcpu_alloc_test                  80732              125520       1.55         140864       1.74       1.12
    Total Cycles              119240774314        763211341128       6.40  1390338696894      11.66       1.82
    
    Sequential, 2 cpus
    
                                  No KASAN      KASAN original x baseline  KASAN vmalloc x baseline    x KASAN
    
    fix_size_alloc_test            1423150            14276550      10.03       27733022      19.49       1.94
    full_fit_alloc_test            1754219            14722640       8.39       15030786       8.57       1.02
    long_busy_list_alloc_test     11451858            52154973       4.55      107016027       9.34       2.05
    random_size_alloc_test         5989020            26735276       4.46       68885923      11.50       2.58
    fix_align_alloc_test           2050976            20166900       9.83       50491675      24.62       2.50
    random_size_align_alloc_te     2858229            17971700       6.29       38730225      13.55       2.16
    align_shift_alloc_test             405                6428      15.87          26253      64.82       4.08
    pcpu_alloc_test                 127183              151464       1.19         216263       1.70       1.43
    Total Cycles               54181269392        308723699764       5.70   650772566394      12.01       2.11
    fix_size_alloc_test            1420404            14289308      10.06       27790035      19.56       1.94
    full_fit_alloc_test            1736145            14806234       8.53       15274301       8.80       1.03
    long_busy_list_alloc_test     11404638            52270785       4.58      107550254       9.43       2.06
    random_size_alloc_test         6017006            26650625       4.43       68696127      11.42       2.58
    fix_align_alloc_test           2045504            20280985       9.91       50414862      24.65       2.49
    random_size_align_alloc_te     2845338            17931018       6.30       38510276      13.53       2.15
    align_shift_alloc_test             472                3760       7.97           9656      20.46       2.57
    pcpu_alloc_test                 118643              132732       1.12         146504       1.23       1.10
    Total Cycles               54040011688        309102805492       5.72   651325675652      12.05       2.11
    
    [dja@axtens.net: fixups]
      Link: http://lkml.kernel.org/r/20191120052719.7201-1-dja@axtens.net
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=3D202009
    Link: http://lkml.kernel.org/r/20191031093909.9228-2-dja@axtens.net
    Signed-off-by: Mark Rutland <mark.rutland@arm.com> [shadow rework]
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Co-developed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Vasily Gorbik <gor@linux.ibm.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index b4c58a191eb1..a4b241102771 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -22,6 +22,18 @@ struct notifier_block;		/* in notifier.h */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
 #define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */
+
+/*
+ * VM_KASAN is used slighly differently depending on CONFIG_KASAN_VMALLOC.
+ *
+ * If IS_ENABLED(CONFIG_KASAN_VMALLOC), VM_KASAN is set on a vm_struct after
+ * shadow memory has been mapped. It's used to handle allocation errors so that
+ * we don't try to poision shadow on free if it was never allocated.
+ *
+ * Otherwise, VM_KASAN is set for kasan_module_alloc() allocations and used to
+ * determine which allocations need the module shadow freed.
+ */
+
 /*
  * Memory with VM_FLUSH_RESET_PERMS cannot be freed in an interrupt or with
  * vfree_atomic().

commit fc9702273e2edb90400a34b3be76f7b08fa3344b
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Nov 17 09:28:04 2019 -0800

    bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY
    
    Add ability to memory-map contents of BPF array map. This is extremely useful
    for working with BPF global data from userspace programs. It allows to avoid
    typical bpf_map_{lookup,update}_elem operations, improving both performance
    and usability.
    
    There had to be special considerations for map freezing, to avoid having
    writable memory view into a frozen map. To solve this issue, map freezing and
    mmap-ing is happening under mutex now:
      - if map is already frozen, no writable mapping is allowed;
      - if map has writable memory mappings active (accounted in map->writecnt),
        map freezing will keep failing with -EBUSY;
      - once number of writable memory mappings drops to zero, map freezing can be
        performed again.
    
    Only non-per-CPU plain arrays are supported right now. Maps with spinlocks
    can't be memory mapped either.
    
    For BPF_F_MMAPABLE array, memory allocation has to be done through vmalloc()
    to be mmap()'able. We also need to make sure that array data memory is
    page-sized and page-aligned, so we over-allocate memory in such a way that
    struct bpf_array is at the end of a single page of memory with array->value
    being aligned with the start of the second page. On deallocation we need to
    accomodate this memory arrangement to free vmalloc()'ed memory correctly.
    
    One important consideration regarding how memory-mapping subsystem functions.
    Memory-mapping subsystem provides few optional callbacks, among them open()
    and close().  close() is called for each memory region that is unmapped, so
    that users can decrease their reference counters and free up resources, if
    necessary. open() is *almost* symmetrical: it's called for each memory region
    that is being mapped, **except** the very first one. So bpf_map_mmap does
    initial refcnt bump, while open() will do any extra ones after that. Thus
    number of close() calls is equal to number of open() calls plus one more.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Link: https://lore.kernel.org/bpf/20191117172806.2195367-4-andriin@fb.com

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4e7809408073..b4c58a191eb1 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -93,6 +93,7 @@ extern void *vzalloc(unsigned long size);
 extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);
 extern void *vzalloc_node(unsigned long size, int node);
+extern void *vmalloc_user_node_flags(unsigned long size, int node, gfp_t flags);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);

commit 688fcbfc06e4fdfbb7e1d5a942a1460fe6379d2d
Author: Pengfei Li <lpf.vector@gmail.com>
Date:   Mon Sep 23 15:36:39 2019 -0700

    mm/vmalloc: modify struct vmap_area to reduce its size
    
    Objective
    ---------
    
    The current implementation of struct vmap_area wasted space.
    
    After applying this commit, sizeof(struct vmap_area) has been
    reduced from 11 words to 8 words.
    
    Description
    -----------
    
    1) Pack "subtree_max_size", "vm" and "purge_list".  This is no problem
       because
    
    A) "subtree_max_size" is only used when vmap_area is in "free" tree
    
    B) "vm" is only used when vmap_area is in "busy" tree
    
    C) "purge_list" is only used when vmap_area is in vmap_purge_list
    
    2) Eliminate "flags".
    
    ;Since only one flag VM_VM_AREA is being used, and the same thing can be
    done by judging whether "vm" is NULL, then the "flags" can be eliminated.
    
    Link: http://lkml.kernel.org/r/20190716152656.12255-3-lpf.vector@gmail.com
    Signed-off-by: Pengfei Li <lpf.vector@gmail.com>
    Suggested-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Reviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oleksiy Avramchenko <oleksiy.avramchenko@sonymobile.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index dfa718ffdd4f..4e7809408073 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -53,15 +53,21 @@ struct vmap_area {
 	unsigned long va_start;
 	unsigned long va_end;
 
-	/*
-	 * Largest available free size in subtree.
-	 */
-	unsigned long subtree_max_size;
-	unsigned long flags;
 	struct rb_node rb_node;         /* address sorted rbtree */
 	struct list_head list;          /* address sorted list */
-	struct llist_node purge_list;    /* "lazy purge" list */
-	struct vm_struct *vm;
+
+	/*
+	 * The following three variables can be packed, because
+	 * a vmap_area object is always one of the three states:
+	 *    1) in "free" tree (root is vmap_area_root)
+	 *    2) in "busy" tree (root is free_vmap_area_root)
+	 *    3) in purge list  (head is vmap_purge_list)
+	 */
+	union {
+		unsigned long subtree_max_size; /* in "free" tree */
+		struct vm_struct *vm;           /* in "busy" tree */
+		struct llist_node purge_list;   /* in purge list */
+	};
 };
 
 /*

commit fe9041c245196c6c61091ccc2c74b73ab9a5fc50
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 3 08:55:13 2019 +0200

    vmalloc: lift the arm flag for coherent mappings to common code
    
    The arm architecture had a VM_ARM_DMA_CONSISTENT flag to mark DMA
    coherent remapping for a while.  Lift this flag to common code so
    that we can use it generically.  We also check it in the only place
    VM_USERMAP is directly check so that we can entirely replace that
    flag as well (although I'm not even sure why we'd want to allow
    remapping DMA appings, but I'd rather not change behavior).
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 9b21d0047710..dfa718ffdd4f 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -18,6 +18,7 @@ struct notifier_block;		/* in notifier.h */
 #define VM_ALLOC		0x00000002	/* vmalloc() */
 #define VM_MAP			0x00000004	/* vmap()ed pages */
 #define VM_USERMAP		0x00000008	/* suitable for remap_vmalloc_range */
+#define VM_DMA_COHERENT		0x00000010	/* dma_alloc_coherent */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
 #define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */
@@ -26,6 +27,7 @@ struct notifier_block;		/* in notifier.h */
  * vfree_atomic().
  */
 #define VM_FLUSH_RESET_PERMS	0x00000100      /* Reset direct map and flush TLB on unmap */
+
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*

commit 97105f0ab7b877a8ece2005e214894e93793950c
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Jul 11 21:00:13 2019 -0700

    mm: vmalloc: show number of vmalloc pages in /proc/meminfo
    
    Vmalloc() is getting more and more used these days (kernel stacks, bpf and
    percpu allocator are new top users), and the total % of memory consumed by
    vmalloc() can be pretty significant and changes dynamically.
    
    /proc/meminfo is the best place to display this information: its top goal
    is to show top consumers of the memory.
    
    Since the VmallocUsed field in /proc/meminfo is not in use for quite a
    long time (it has been defined to 0 by a5ad88ce8c7f ("mm: get rid of
    'vmalloc_info' from /proc/meminfo")), let's reuse it for showing the
    actual physical memory consumption of vmalloc().
    
    Link: http://lkml.kernel.org/r/20190417194002.12369-3-guro@fb.com
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 51e131245379..9b21d0047710 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -72,10 +72,12 @@ extern void vm_unmap_aliases(void);
 
 #ifdef CONFIG_MMU
 extern void __init vmalloc_init(void);
+extern unsigned long vmalloc_nr_pages(void);
 #else
 static inline void vmalloc_init(void)
 {
 }
+static inline unsigned long vmalloc_nr_pages(void) { return 0; }
 #endif
 
 extern void *vmalloc(unsigned long size);

commit 68ad4a3304335358f95a417f2a2b0c909e5119c4
Author: Uladzislau Rezki (Sony) <urezki@gmail.com>
Date:   Fri May 17 14:31:31 2019 -0700

    mm/vmalloc.c: keep track of free blocks for vmap allocation
    
    Patch series "improve vmap allocation", v3.
    
    Objective
    ---------
    
    Please have a look for the description at:
    
      https://lkml.org/lkml/2018/10/19/786
    
    but let me also summarize it a bit here as well.
    
    The current implementation has O(N) complexity. Requests with different
    permissive parameters can lead to long allocation time. When i say
    "long" i mean milliseconds.
    
    Description
    -----------
    
    This approach organizes the KVA memory layout into free areas of the
    1-ULONG_MAX range, i.e.  an allocation is done over free areas lookups,
    instead of finding a hole between two busy blocks.  It allows to have
    lower number of objects which represent the free space, therefore to have
    less fragmented memory allocator.  Because free blocks are always as large
    as possible.
    
    It uses the augment tree where all free areas are sorted in ascending
    order of va->va_start address in pair with linked list that provides
    O(1) access to prev/next elements.
    
    Since the tree is augment, we also maintain the "subtree_max_size" of VA
    that reflects a maximum available free block in its left or right
    sub-tree.  Knowing that, we can easily traversal toward the lowest (left
    most path) free area.
    
    Allocation: ~O(log(N)) complexity.  It is sequential allocation method
    therefore tends to maximize locality.  The search is done until a first
    suitable block is large enough to encompass the requested parameters.
    Bigger areas are split.
    
    I copy paste here the description of how the area is split, since i
    described it in https://lkml.org/lkml/2018/10/19/786
    
    <snip>
    
    A free block can be split by three different ways.  Their names are
    FL_FIT_TYPE, LE_FIT_TYPE/RE_FIT_TYPE and NE_FIT_TYPE, i.e.  they
    correspond to how requested size and alignment fit to a free block.
    
    FL_FIT_TYPE - in this case a free block is just removed from the free
    list/tree because it fully fits.  Comparing with current design there is
    an extra work with rb-tree updating.
    
    LE_FIT_TYPE/RE_FIT_TYPE - left/right edges fit.  In this case what we do
    is just cutting a free block.  It is as fast as a current design.  Most of
    the vmalloc allocations just end up with this case, because the edge is
    always aligned to 1.
    
    NE_FIT_TYPE - Is much less common case.  Basically it happens when
    requested size and alignment does not fit left nor right edges, i.e.  it
    is between them.  In this case during splitting we have to build a
    remaining left free area and place it back to the free list/tree.
    
    Comparing with current design there are two extra steps.  First one is we
    have to allocate a new vmap_area structure.  Second one we have to insert
    that remaining free block to the address sorted list/tree.
    
    In order to optimize a first case there is a cache with free_vmap objects.
    Instead of allocating from slab we just take an object from the cache and
    reuse it.
    
    Second one is pretty optimized.  Since we know a start point in the tree
    we do not do a search from the top.  Instead a traversal begins from a
    rb-tree node we split.
    <snip>
    
    De-allocation.  ~O(log(N)) complexity.  An area is not inserted straight
    away to the tree/list, instead we identify the spot first, checking if it
    can be merged around neighbors.  The list provides O(1) access to
    prev/next, so it is pretty fast to check it.  Summarizing.  If merged then
    large coalesced areas are created, if not the area is just linked making
    more fragments.
    
    There is one more thing that i should mention here.  After modification of
    VA node, its subtree_max_size is updated if it was/is the biggest area in
    its left or right sub-tree.  Apart of that it can also be populated back
    to upper levels to fix the tree.  For more details please have a look at
    the __augment_tree_propagate_from() function and the description.
    
    Tests and stressing
    -------------------
    
    I use the "test_vmalloc.sh" test driver available under
    "tools/testing/selftests/vm/" since 5.1-rc1 kernel.  Just trigger "sudo
    ./test_vmalloc.sh" to find out how to deal with it.
    
    Tested on different platforms including x86_64/i686/ARM64/x86_64_NUMA.
    Regarding last one, i do not have any physical access to NUMA system,
    therefore i emulated it.  The time of stressing is days.
    
    If you run the test driver in "stress mode", you also need the patch that
    is in Andrew's tree but not in Linux 5.1-rc1.  So, please apply it:
    
    http://git.cmpxchg.org/cgit.cgi/linux-mmotm.git/commit/?id=e0cf7749bade6da318e98e934a24d8b62fab512c
    
    After massive testing, i have not identified any problems like memory
    leaks, crashes or kernel panics.  I find it stable, but more testing would
    be good.
    
    Performance analysis
    --------------------
    
    I have used two systems to test.  One is i5-3320M CPU @ 2.60GHz and
    another is HiKey960(arm64) board.  i5-3320M runs on 4.20 kernel, whereas
    Hikey960 uses 4.15 kernel.  I have both system which could run on 5.1-rc1
    as well, but the results have not been ready by time i an writing this.
    
    Currently it consist of 8 tests.  There are three of them which correspond
    to different types of splitting(to compare with default).  We have 3
    ones(see above).  Another 5 do allocations in different conditions.
    
    a) sudo ./test_vmalloc.sh performance
    
    When the test driver is run in "performance" mode, it runs all available
    tests pinned to first online CPU with sequential execution test order.  We
    do it in order to get stable and repeatable results.  Take a look at time
    difference in "long_busy_list_alloc_test".  It is not surprising because
    the worst case is O(N).
    
    # i5-3320M
    How many cycles all tests took:
    CPU0=646919905370(default) cycles vs CPU0=193290498550(patched) cycles
    
    # See detailed table with results here:
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/i5-3320M_performance_default.txt
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/i5-3320M_performance_patched.txt
    
    # Hikey960 8x CPUs
    How many cycles all tests took:
    CPU0=3478683207 cycles vs CPU0=463767978 cycles
    
    # See detailed table with results here:
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/HiKey960_performance_default.txt
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/HiKey960_performance_patched.txt
    
    b) time sudo ./test_vmalloc.sh test_repeat_count=1
    
    With this configuration, all tests are run on all available online CPUs.
    Before running each CPU shuffles its tests execution order.  It gives
    random allocation behaviour.  So it is rough comparison, but it puts in
    the picture for sure.
    
    # i5-3320M
    <default>            vs            <patched>
    real    101m22.813s                real    0m56.805s
    user    0m0.011s                   user    0m0.015s
    sys     0m5.076s                   sys     0m0.023s
    
    # See detailed table with results here:
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/i5-3320M_test_repeat_count_1_default.txt
    ftp://vps418301.ovh.net/incoming/vmap_test_results_v2/i5-3320M_test_repeat_count_1_patched.txt
    
    # Hikey960 8x CPUs
    <default>            vs            <patched>
    real    unknown                    real    4m25.214s
    user    unknown                    user    0m0.011s
    sys     unknown                    sys     0m0.670s
    
    I did not manage to complete this test on "default Hikey960" kernel
    version.  After 24 hours it was still running, therefore i had to cancel
    it.  That is why real/user/sys are "unknown".
    
    This patch (of 3):
    
    Currently an allocation of the new vmap area is done over busy list
    iteration(complexity O(n)) until a suitable hole is found between two busy
    areas.  Therefore each new allocation causes the list being grown.  Due to
    over fragmented list and different permissive parameters an allocation can
    take a long time.  For example on embedded devices it is milliseconds.
    
    This patch organizes the KVA memory layout into free areas of the
    1-ULONG_MAX range.  It uses an augment red-black tree that keeps blocks
    sorted by their offsets in pair with linked list keeping the free space in
    order of increasing addresses.
    
    Nodes are augmented with the size of the maximum available free block in
    its left or right sub-tree.  Thus, that allows to take a decision and
    traversal toward the block that will fit and will have the lowest start
    address, i.e.  it is sequential allocation.
    
    Allocation: to allocate a new block a search is done over the tree until a
    suitable lowest(left most) block is large enough to encompass: the
    requested size, alignment and vstart point.  If the block is bigger than
    requested size - it is split.
    
    De-allocation: when a busy vmap area is freed it can either be merged or
    inserted to the tree.  Red-black tree allows efficiently find a spot
    whereas a linked list provides a constant-time access to previous and next
    blocks to check if merging can be done.  In case of merging of
    de-allocated memory chunk a large coalesced area is created.
    
    Complexity: ~O(log(N))
    
    [urezki@gmail.com: v3]
      Link: http://lkml.kernel.org/r/20190402162531.10888-2-urezki@gmail.com
    [urezki@gmail.com: v4]
      Link: http://lkml.kernel.org/r/20190406183508.25273-2-urezki@gmail.com
    Link: http://lkml.kernel.org/r/20190321190327.11813-2-urezki@gmail.com
    Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Reviewed-by: Roman Gushchin <guro@fb.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Thomas Garnier <thgarnie@google.com>
    Cc: Oleksiy Avramchenko <oleksiy.avramchenko@sonymobile.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index c6eebb839552..51e131245379 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -50,12 +50,16 @@ struct vm_struct {
 struct vmap_area {
 	unsigned long va_start;
 	unsigned long va_end;
+
+	/*
+	 * Largest available free size in subtree.
+	 */
+	unsigned long subtree_max_size;
 	unsigned long flags;
 	struct rb_node rb_node;         /* address sorted rbtree */
 	struct list_head list;          /* address sorted list */
 	struct llist_node purge_list;    /* "lazy purge" list */
 	struct vm_struct *vm;
-	struct rcu_head rcu_head;
 };
 
 /*

commit 868b104d7379e28013e9d48bdd2db25e0bdcf751
Author: Rick Edgecombe <rick.p.edgecombe@intel.com>
Date:   Thu Apr 25 17:11:36 2019 -0700

    mm/vmalloc: Add flag for freeing of special permsissions
    
    Add a new flag VM_FLUSH_RESET_PERMS, for enabling vfree operations to
    immediately clear executable TLB entries before freeing pages, and handle
    resetting permissions on the directmap. This flag is useful for any kind
    of memory with elevated permissions, or where there can be related
    permissions changes on the directmap. Today this is RO+X and RO memory.
    
    Although this enables directly vfreeing non-writeable memory now,
    non-writable memory cannot be freed in an interrupt because the allocation
    itself is used as a node on deferred free list. So when RO memory needs to
    be freed in an interrupt the code doing the vfree needs to have its own
    work queue, as was the case before the deferred vfree list was added to
    vmalloc.
    
    For architectures with set_direct_map_ implementations this whole operation
    can be done with one TLB flush when centralized like this. For others with
    directmap permissions, currently only arm64, a backup method using
    set_memory functions is used to reset the directmap. When arm64 adds
    set_direct_map_ functions, this backup can be removed.
    
    When the TLB is flushed to both remove TLB entries for the vmalloc range
    mapping and the direct map permissions, the lazy purge operation could be
    done to try to save a TLB flush later. However today vm_unmap_aliases
    could flush a TLB range that does not include the directmap. So a helper
    is added with extra parameters that can allow both the vmalloc address and
    the direct mapping to be flushed during this operation. The behavior of the
    normal vm_unmap_aliases function is unchanged.
    
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Suggested-by: Andy Lutomirski <luto@kernel.org>
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <akpm@linux-foundation.org>
    Cc: <ard.biesheuvel@linaro.org>
    Cc: <deneen.t.dock@intel.com>
    Cc: <kernel-hardening@lists.openwall.com>
    Cc: <kristen@linux.intel.com>
    Cc: <linux_dti@icloud.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190426001143.4983-17-namit@vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 398e9c95cd61..c6eebb839552 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -21,6 +21,11 @@ struct notifier_block;		/* in notifier.h */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
 #define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */
+/*
+ * Memory with VM_FLUSH_RESET_PERMS cannot be freed in an interrupt or with
+ * vfree_atomic().
+ */
+#define VM_FLUSH_RESET_PERMS	0x00000100      /* Reset direct map and flush TLB on unmap */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*
@@ -142,6 +147,13 @@ extern int map_kernel_range_noflush(unsigned long start, unsigned long size,
 				    pgprot_t prot, struct page **pages);
 extern void unmap_kernel_range_noflush(unsigned long addr, unsigned long size);
 extern void unmap_kernel_range(unsigned long addr, unsigned long size);
+static inline void set_vm_flush_reset_perms(void *addr)
+{
+	struct vm_struct *vm = find_vm_area(addr);
+
+	if (vm)
+		vm->flags |= VM_FLUSH_RESET_PERMS;
+}
 #else
 static inline int
 map_kernel_range_noflush(unsigned long start, unsigned long size,
@@ -157,6 +169,9 @@ static inline void
 unmap_kernel_range(unsigned long addr, unsigned long size)
 {
 }
+static inline void set_vm_flush_reset_perms(void *addr)
+{
+}
 #endif
 
 /* Allocate/destroy a 'vmalloc' VM area. */

commit 3b3b1a29eb89ba93f91213cbebb332a2ac31fa8b
Author: Kees Cook <keescook@chromium.org>
Date:   Tue May 8 12:55:26 2018 -0700

    mm: Use overflow helpers in kvmalloc()
    
    Instead of open-coded multiplication and bounds checking, use the new
    overflow helper. Additionally prepare for vmalloc() users to add
    array_size()-family helpers in the future.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 1e5d8c392f15..398e9c95cd61 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -8,6 +8,7 @@
 #include <linux/llist.h>
 #include <asm/page.h>		/* pgprot_t */
 #include <linux/rbtree.h>
+#include <linux/overflow.h>
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 struct notifier_block;		/* in notifier.h */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 2d92dd002abd..1e5d8c392f15 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_VMALLOC_H
 #define _LINUX_VMALLOC_H
 

commit 8594a21cf7a8318baedbedc3fcd2967a17ddeec0
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri May 12 15:46:41 2017 -0700

    mm, vmalloc: fix vmalloc users tracking properly
    
    Commit 1f5307b1e094 ("mm, vmalloc: properly track vmalloc users") has
    pulled asm/pgtable.h include dependency to linux/vmalloc.h and that
    turned out to be a bad idea for some architectures.  E.g.  m68k fails
    with
    
       In file included from arch/m68k/include/asm/pgtable_mm.h:145:0,
                        from arch/m68k/include/asm/pgtable.h:4,
                        from include/linux/vmalloc.h:9,
                        from arch/m68k/kernel/module.c:9:
       arch/m68k/include/asm/mcf_pgtable.h: In function 'nocache_page':
    >> arch/m68k/include/asm/mcf_pgtable.h:339:43: error: 'init_mm' undeclared (first use in this function)
        #define pgd_offset_k(address) pgd_offset(&init_mm, address)
    
    as spotted by kernel build bot. nios2 fails for other reason
    
      In file included from include/asm-generic/io.h:767:0,
                       from arch/nios2/include/asm/io.h:61,
                       from include/linux/io.h:25,
                       from arch/nios2/include/asm/pgtable.h:18,
                       from include/linux/mm.h:70,
                       from include/linux/pid_namespace.h:6,
                       from include/linux/ptrace.h:9,
                       from arch/nios2/include/uapi/asm/elf.h:23,
                       from arch/nios2/include/asm/elf.h:22,
                       from include/linux/elf.h:4,
                       from include/linux/module.h:15,
                       from init/main.c:16:
      include/linux/vmalloc.h: In function '__vmalloc_node_flags':
      include/linux/vmalloc.h:99:40: error: 'PAGE_KERNEL' undeclared (first use in this function); did you mean 'GFP_KERNEL'?
    
    which is due to the newly added #include <asm/pgtable.h>, which on nios2
    includes <linux/io.h> and thus <asm/io.h> and <asm-generic/io.h> which
    again includes <linux/vmalloc.h>.
    
    Tweaking that around just turns out a bigger headache than necessary.
    This patch reverts 1f5307b1e094 and reimplements the original fix in a
    different way.  __vmalloc_node_flags can stay static inline which will
    cover vmalloc* functions.  We only have one external user
    (kvmalloc_node) and we can export __vmalloc_node_flags_caller and
    provide the caller directly.  This is much simpler and it doesn't really
    need any games with header files.
    
    [akpm@linux-foundation.org: coding-style fixes]
    [mhocko@kernel.org: revert old comment]
      Link: http://lkml.kernel.org/r/20170509211054.GB16325@dhcp22.suse.cz
    Fixes: 1f5307b1e094 ("mm, vmalloc: properly track vmalloc users")
    Link: http://lkml.kernel.org/r/20170509153702.GR6481@dhcp22.suse.cz
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tobias Klauser <tklauser@distanz.ch>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 0328ce003992..2d92dd002abd 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -6,7 +6,6 @@
 #include <linux/list.h>
 #include <linux/llist.h>
 #include <asm/page.h>		/* pgprot_t */
-#include <asm/pgtable.h>	/* PAGE_KERNEL */
 #include <linux/rbtree.h>
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
@@ -83,22 +82,14 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			const void *caller);
 #ifndef CONFIG_MMU
 extern void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags);
-#else
-extern void *__vmalloc_node(unsigned long size, unsigned long align,
-			    gfp_t gfp_mask, pgprot_t prot,
-			    int node, const void *caller);
-
-/*
- * We really want to have this inlined due to caller tracking. This
- * function is used by the highlevel vmalloc apis and so we want to track
- * their callers and inlining will achieve that.
- */
-static inline void *__vmalloc_node_flags(unsigned long size,
-					int node, gfp_t flags)
+static inline void *__vmalloc_node_flags_caller(unsigned long size, int node,
+						gfp_t flags, void *caller)
 {
-	return __vmalloc_node(size, 1, flags, PAGE_KERNEL,
-					node, __builtin_return_address(0));
+	return __vmalloc_node_flags(size, node, flags);
 }
+#else
+extern void *__vmalloc_node_flags_caller(unsigned long size,
+					 int node, gfp_t flags, void *caller);
 #endif
 
 extern void vfree(const void *addr);

commit 1f5307b1e094bfffa83c65c40ac6e3415c108780
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:12 2017 -0700

    mm, vmalloc: properly track vmalloc users
    
    __vmalloc_node_flags used to be static inline but this has changed by
    "mm: introduce kv[mz]alloc helpers" because kvmalloc_node needs to use
    it as well and the code is outside of the vmalloc proper.  I haven't
    realized that changing this will lead to a subtle bug though.  The
    function is responsible to track the caller as well.  This caller is
    then printed by /proc/vmallocinfo.  If __vmalloc_node_flags is not
    inline then we would get only direct users of __vmalloc_node_flags as
    callers (e.g.  v[mz]alloc) which reduces usefulness of this debugging
    feature considerably.  It simply doesn't help to see that the given
    range belongs to vmalloc as a caller:
    
      0xffffc90002c79000-0xffffc90002c7d000   16384 vmalloc+0x16/0x18 pages=3 vmalloc N0=3
      0xffffc90002c81000-0xffffc90002c85000   16384 vmalloc+0x16/0x18 pages=3 vmalloc N1=3
      0xffffc90002c8d000-0xffffc90002c91000   16384 vmalloc+0x16/0x18 pages=3 vmalloc N1=3
      0xffffc90002c95000-0xffffc90002c99000   16384 vmalloc+0x16/0x18 pages=3 vmalloc N1=3
    
    We really want to catch the _caller_ of the vmalloc function.  Fix this
    issue by making __vmalloc_node_flags static inline again.
    
    Link: http://lkml.kernel.org/r/20170502134657.12381-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 46991ad3ddd5..0328ce003992 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -6,6 +6,7 @@
 #include <linux/list.h>
 #include <linux/llist.h>
 #include <asm/page.h>		/* pgprot_t */
+#include <asm/pgtable.h>	/* PAGE_KERNEL */
 #include <linux/rbtree.h>
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
@@ -80,7 +81,25 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
 			pgprot_t prot, unsigned long vm_flags, int node,
 			const void *caller);
+#ifndef CONFIG_MMU
 extern void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags);
+#else
+extern void *__vmalloc_node(unsigned long size, unsigned long align,
+			    gfp_t gfp_mask, pgprot_t prot,
+			    int node, const void *caller);
+
+/*
+ * We really want to have this inlined due to caller tracking. This
+ * function is used by the highlevel vmalloc apis and so we want to track
+ * their callers and inlining will achieve that.
+ */
+static inline void *__vmalloc_node_flags(unsigned long size,
+					int node, gfp_t flags)
+{
+	return __vmalloc_node(size, 1, flags, PAGE_KERNEL,
+					node, __builtin_return_address(0));
+}
+#endif
 
 extern void vfree(const void *addr);
 extern void vfree_atomic(const void *addr);

commit a7c3e901a46ff54c016d040847eda598a9e3e653
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:09 2017 -0700

    mm: introduce kv[mz]alloc helpers
    
    Patch series "kvmalloc", v5.
    
    There are many open coded kmalloc with vmalloc fallback instances in the
    tree.  Most of them are not careful enough or simply do not care about
    the underlying semantic of the kmalloc/page allocator which means that
    a) some vmalloc fallbacks are basically unreachable because the kmalloc
    part will keep retrying until it succeeds b) the page allocator can
    invoke a really disruptive steps like the OOM killer to move forward
    which doesn't sound appropriate when we consider that the vmalloc
    fallback is available.
    
    As it can be seen implementing kvmalloc requires quite an intimate
    knowledge if the page allocator and the memory reclaim internals which
    strongly suggests that a helper should be implemented in the memory
    subsystem proper.
    
    Most callers, I could find, have been converted to use the helper
    instead.  This is patch 6.  There are some more relying on __GFP_REPEAT
    in the networking stack which I have converted as well and Eric Dumazet
    was not opposed [2] to convert them as well.
    
    [1] http://lkml.kernel.org/r/20170130094940.13546-1-mhocko@kernel.org
    [2] http://lkml.kernel.org/r/1485273626.16328.301.camel@edumazet-glaptop3.roam.corp.google.com
    
    This patch (of 9):
    
    Using kmalloc with the vmalloc fallback for larger allocations is a
    common pattern in the kernel code.  Yet we do not have any common helper
    for that and so users have invented their own helpers.  Some of them are
    really creative when doing so.  Let's just add kv[mz]alloc and make sure
    it is implemented properly.  This implementation makes sure to not make
    a large memory pressure for > PAGE_SZE requests (__GFP_NORETRY) and also
    to not warn about allocation failures.  This also rules out the OOM
    killer as the vmalloc is a more approapriate fallback than a disruptive
    user visible action.
    
    This patch also changes some existing users and removes helpers which
    are specific for them.  In some cases this is not possible (e.g.
    ext4_kvmalloc, libcfs_kvzalloc) because those seems to be broken and
    require GFP_NO{FS,IO} context which is not vmalloc compatible in general
    (note that the page table allocation is GFP_KERNEL).  Those need to be
    fixed separately.
    
    While we are at it, document that __vmalloc{_node} about unsupported gfp
    mask because there seems to be a lot of confusion out there.
    kvmalloc_node will warn about GFP_KERNEL incompatible (which are not
    superset) flags to catch new abusers.  Existing ones would have to die
    slowly.
    
    [sfr@canb.auug.org.au: f2fs fixup]
      Link: http://lkml.kernel.org/r/20170320163735.332e64b7@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170306103032.2540-2-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Reviewed-by: Andreas Dilger <adilger@dilger.ca> [ext4 part]
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index d68edffbf142..46991ad3ddd5 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -80,6 +80,7 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
 			pgprot_t prot, unsigned long vm_flags, int node,
 			const void *caller);
+extern void *__vmalloc_node_flags(unsigned long size, int node, gfp_t flags);
 
 extern void vfree(const void *addr);
 extern void vfree_atomic(const void *addr);

commit bf22e37a641327e34681b7b6959d9646e3886770
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Mon Dec 12 16:44:10 2016 -0800

    mm: add vfree_atomic()
    
    We are going to use sleeping lock for freeing vmap.  However some
    vfree() users want to free memory from atomic (but not from interrupt)
    context.  For this we add vfree_atomic() - deferred variation of vfree()
    which can be used in any atomic context (except NMIs).
    
    [akpm@linux-foundation.org: tweak comment grammar]
    [aryabinin@virtuozzo.com: use raw_cpu_ptr() instead of this_cpu_ptr()]
      Link: http://lkml.kernel.org/r/1481553981-3856-1-git-send-email-aryabinin@virtuozzo.com
    Link: http://lkml.kernel.org/r/1479474236-4139-5-git-send-email-hch@lst.de
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Jisheng Zhang <jszhang@marvell.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: John Dias <joaodias@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 3d9d786a943c..d68edffbf142 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -82,6 +82,7 @@ extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			const void *caller);
 
 extern void vfree(const void *addr);
+extern void vfree_atomic(const void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,
 			unsigned long flags, pgprot_t prot);

commit 1d6da87a3241deb13d073c4125d19ed0e5a0c62c
Merge: 1f40c49570eb a39ed680bddb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 11:48:48 2016 -0700

    Merge branch 'drm-next' of git://people.freedesktop.org/~airlied/linux
    
    Pull drm updates from Dave Airlie:
     "Here's the main drm pull request for 4.7, it's been a busy one, and
      I've been a bit more distracted in real life this merge window.  Lots
      more ARM drivers, not sure if it'll ever end.  I think I've at least
      one more coming the next merge window.
    
      But changes are all over the place, support for AMD Polaris GPUs is in
      here, some missing GM108 support for nouveau (found in some Lenovos),
      a bunch of MST and skylake fixes.
    
      I've also noticed a few fixes from Arnd in my inbox, that I'll try and
      get in asap, but I didn't think they should hold this up.
    
      New drivers:
       - Hisilicon kirin display driver
       - Mediatek MT8173 display driver
       - ARC PGU - bitstreamer on Synopsys ARC SDP boards
       - Allwinner A13 initial RGB output driver
       - Analogix driver for DisplayPort IP found in exynos and rockchip
    
      DRM Core:
       - UAPI headers fixes and C++ safety
       - DRM connector reference counting
       - DisplayID mode parsing for Dell 5K monitors
       - Removal of struct_mutex from drivers
       - Connector registration cleanups
       - MST robustness fixes
       - MAINTAINERS updates
       - Lockless GEM object freeing
       - Generic fbdev deferred IO support
    
      panel:
       - Support for a bunch of new panels
    
      i915:
       - VBT refactoring
       - PLL computation cleanups
       - DSI support for BXT
       - Color manager support
       - More atomic patches
       - GEM improvements
       - GuC fw loading fixes
       - DP detection fixes
       - SKL GPU hang fixes
       - Lots of BXT fixes
    
      radeon/amdgpu:
       - Initial Polaris support
       - GPUVM/Scheduler/Clock/Power improvements
       - ASYNC pageflip support
       - New mesa feature support
    
      nouveau:
       - GM108 support
       - Power sensor support improvements
       - GR init + ucode fixes.
       - Use GPU provided topology information
    
      vmwgfx:
       - Add host messaging support
    
      gma500:
       - Some cleanups and fixes
    
      atmel:
       - Bridge support
       - Async atomic commit support
    
      fsl-dcu:
       - Timing controller for LCD support
       - Pixel clock polarity support
    
      rcar-du:
       - Misc fixes
    
      exynos:
       - Pipeline clock support
       - Exynoss4533 SoC support
       - HW trigger mode support
       - export HDMI_PHY clock
       - DECON5433 fixes
       - Use generic prime functions
       - use DMA mapping APIs
    
      rockchip:
       - Lots of little fixes
    
      vc4:
       - Render node support
       - Gamma ramp support
       - DPI output support
    
      msm:
       - Mostly cleanups and fixes
       - Conversion to generic struct fence
    
      etnaviv:
       - Fix for prime buffer handling
       - Allow hangcheck to be coalesced with other wakeups
    
      tegra:
       - Gamme table size fix"
    
    * 'drm-next' of git://people.freedesktop.org/~airlied/linux: (1050 commits)
      drm/edid: add displayid detailed 1 timings to the modelist. (v1.1)
      drm/edid: move displayid validation to it's own function.
      drm/displayid: Iterate over all DisplayID blocks
      drm/edid: move displayid tiled block parsing into separate function.
      drm: Nuke ->vblank_disable_allowed
      drm/vmwgfx: Report vmwgfx version to vmware.log
      drm/vmwgfx: Add VMWare host messaging capability
      drm/vmwgfx: Kill some lockdep warnings
      drm/nouveau/gr/gf100-: fix race condition in fecs/gpccs ucode
      drm/nouveau/core: recognise GM108 chipsets
      drm/nouveau/gr/gm107-: fix touching non-existent ppcs in attrib cb setup
      drm/nouveau/gr/gk104-: share implementation of ppc exception init
      drm/nouveau/gr/gk104-: move rop_active_fbps init to nonctx
      drm/nouveau/bios/pll: check BIT table version before trying to parse it
      drm/nouveau/bios/pll: prevent oops when limits table can't be parsed
      drm/nouveau/volt/gk104: round up in gk104_volt_set
      drm/nouveau/fb/gm200: setup mmu debug buffer registers at init()
      drm/nouveau/fb/gk20a,gm20b: setup mmu debug buffer registers at init()
      drm/nouveau/fb/gf100-: allocate mmu debug buffers
      drm/nouveau/fb: allow chipset-specific actions for oneinit()
      ...

commit 80c4bd7a5e4368b680e0aeb57050a1b06eb573d8
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri May 20 16:57:38 2016 -0700

    mm/vmalloc: keep a separate lazy-free list
    
    When mixing lots of vmallocs and set_memory_*() (which calls
    vm_unmap_aliases()) I encountered situations where the performance
    degraded severely due to the walking of the entire vmap_area list each
    invocation.
    
    One simple improvement is to add the lazily freed vmap_area to a
    separate lockless free list, such that we then avoid having to walk the
    full list on each purge.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Roman Pen <r.peniaev@gmail.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Roman Pen <r.peniaev@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Shawn Lin <shawn.lin@rock-chips.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index d1f1d338af20..957adb741b6f 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -4,6 +4,7 @@
 #include <linux/spinlock.h>
 #include <linux/init.h>
 #include <linux/list.h>
+#include <linux/llist.h>
 #include <asm/page.h>		/* pgprot_t */
 #include <linux/rbtree.h>
 
@@ -44,7 +45,7 @@ struct vmap_area {
 	unsigned long flags;
 	struct rb_node rb_node;         /* address sorted rbtree */
 	struct list_head list;          /* address sorted list */
-	struct list_head purge_list;    /* "lazy purge" list */
+	struct llist_node purge_list;    /* "lazy purge" list */
 	struct vm_struct *vm;
 	struct rcu_head rcu_head;
 };

commit 4da56b99d99e5a7df2b7f11e87bfea935f909732
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 4 14:46:42 2016 +0100

    mm/vmap: Add a notifier for when we run out of vmap address space
    
    vmaps are temporary kernel mappings that may be of long duration.
    Reusing a vmap on an object is preferrable for a driver as the cost of
    setting up the vmap can otherwise dominate the operation on the object.
    However, the vmap address space is rather limited on 32bit systems and
    so we add a notification for vmap pressure in order for the driver to
    release any cached vmappings.
    
    The interface is styled after the oom-notifier where the callees are
    passed a pointer to an unsigned long counter for them to indicate if they
    have freed any space.
    
    v2: Guard the blocking notifier call with gfpflags_allow_blocking()
    v3: Correct typo in forward declaration and move to head of file
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Roman Peniaev <r.peniaev@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: linux-mm@kvack.org
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Andrew Morton <akpm@linux-foundation.org> # for inclusion via DRM
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/1459777603-23618-3-git-send-email-chris@chris-wilson.co.uk
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index d1f1d338af20..8b51df3ab334 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -8,6 +8,7 @@
 #include <linux/rbtree.h>
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
+struct notifier_block;		/* in notifier.h */
 
 /* bits in flags of vmalloc's vm_struct below */
 #define VM_IOREMAP		0x00000001	/* ioremap() and friends */
@@ -187,4 +188,7 @@ pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 #define VMALLOC_TOTAL 0UL
 #endif
 
+int register_vmap_purge_notifier(struct notifier_block *nb);
+int unregister_vmap_purge_notifier(struct notifier_block *nb);
+
 #endif /* _LINUX_VMALLOC_H */

commit 244d63ee345bd9d45c87f665ef5e3f7bcd5db45b
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jan 14 15:19:35 2016 -0800

    mm, vmalloc: remove VM_VPAGES
    
    VM_VPAGES is unnecessary, it's easier to check is_vmalloc_addr() when
    reading /proc/vmallocinfo.
    
    [akpm@linux-foundation.org: remove VM_VPAGES reference via kvfree()]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 3bff87a25a42..d1f1d338af20 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -14,7 +14,6 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 #define VM_ALLOC		0x00000002	/* vmalloc() */
 #define VM_MAP			0x00000004	/* vmap()ed pages */
 #define VM_USERMAP		0x00000008	/* suitable for remap_vmalloc_range */
-#define VM_VPAGES		0x00000010	/* buffer for pages was vmalloc'ed */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
 #define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */

commit a5ad88ce8c7fae7ddc72ee49a11a75aa837788e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 1 17:09:15 2015 -0800

    mm: get rid of 'vmalloc_info' from /proc/meminfo
    
    It turns out that at least some versions of glibc end up reading
    /proc/meminfo at every single startup, because glibc wants to know the
    amount of memory the machine has.  And while that's arguably insane,
    it's just how things are.
    
    And it turns out that it's not all that expensive most of the time, but
    the vmalloc information statistics (amount of virtual memory used in the
    vmalloc space, and the biggest remaining chunk) can be rather expensive
    to compute.
    
    The 'get_vmalloc_info()' function actually showed up on my profiles as
    4% of the CPU usage of "make test" in the git source repository, because
    the git tests are lots of very short-lived shell-scripts etc.
    
    It turns out that apparently this same silly vmalloc info gathering
    shows up on the facebook servers too, according to Dave Jones.  So it's
    not just "make test" for git.
    
    We had two patches to just cache the information (one by me, one by
    Ingo) to mitigate this issue, but the whole vmalloc information of of
    rather dubious value to begin with, and people who *actually* want to
    know what the situation is wrt the vmalloc area should just look at the
    much more complete /proc/vmallocinfo instead.
    
    In fact, according to my testing - and perhaps more importantly,
    according to that big search engine in the sky: Google - there is
    nothing out there that actually cares about those two expensive fields:
    VmallocUsed and VmallocChunk.
    
    So let's try to just remove them entirely.  Actually, this just removes
    the computation and reports the numbers as zero for now, just to try to
    be minimally intrusive.
    
    If this breaks anything, we'll obviously have to re-introduce the code
    to compute this all and add the caching patches on top.  But if given
    the option, I'd really prefer to just remove this bad idea entirely
    rather than add even more code to work around our historical mistake
    that likely nobody really cares about.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 0ec598381f97..3bff87a25a42 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -182,22 +182,10 @@ pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 # endif
 #endif
 
-struct vmalloc_info {
-	unsigned long   used;
-	unsigned long   largest_chunk;
-};
-
 #ifdef CONFIG_MMU
 #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)
-extern void get_vmalloc_info(struct vmalloc_info *vmi);
 #else
-
 #define VMALLOC_TOTAL 0UL
-#define get_vmalloc_info(vmi)			\
-do {						\
-	(vmi)->used = 0;			\
-	(vmi)->largest_chunk = 0;		\
-} while (0)
 #endif
 
 #endif /* _LINUX_VMALLOC_H */

commit a5af5aa8b67dfdba36c853b70564fd2dfe73d478
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Thu Mar 12 16:26:11 2015 -0700

    kasan, module, vmalloc: rework shadow allocation for modules
    
    Current approach in handling shadow memory for modules is broken.
    
    Shadow memory could be freed only after memory shadow corresponds it is no
    longer used.  vfree() called from interrupt context could use memory its
    freeing to store 'struct llist_node' in it:
    
        void vfree(const void *addr)
        {
        ...
            if (unlikely(in_interrupt())) {
                struct vfree_deferred *p = this_cpu_ptr(&vfree_deferred);
                if (llist_add((struct llist_node *)addr, &p->list))
                        schedule_work(&p->wq);
    
    Later this list node used in free_work() which actually frees memory.
    Currently module_memfree() called in interrupt context will free shadow
    before freeing module's memory which could provoke kernel crash.
    
    So shadow memory should be freed after module's memory.  However, such
    deallocation order could race with kasan_module_alloc() in module_alloc().
    
    Free shadow right before releasing vm area.  At this point vfree()'d
    memory is not used anymore and yet not available for other allocations.
    New VM_KASAN flag used to indicate that vm area has dynamically allocated
    shadow memory so kasan frees shadow only if it was previously allocated.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 7d7acb35603d..0ec598381f97 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -17,6 +17,7 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 #define VM_VPAGES		0x00000010	/* buffer for pages was vmalloc'ed */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
+#define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*

commit cb9e3c292d0115499c660028ad35ac5501d722b5
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:40:07 2015 -0800

    mm: vmalloc: pass additional vm_flags to __vmalloc_node_range()
    
    For instrumenting global variables KASan will shadow memory backing memory
    for modules.  So on module loading we will need to allocate memory for
    shadow and map it at address in shadow that corresponds to the address
    allocated in module_alloc().
    
    __vmalloc_node_range() could be used for this purpose, except it puts a
    guard hole after allocated area.  Guard hole in shadow memory should be a
    problem because at some future point we might need to have a shadow memory
    at address occupied by guard hole.  So we could fail to allocate shadow
    for module_alloc().
    
    Now we have VM_NO_GUARD flag disabling guard page, so we need to pass into
    __vmalloc_node_range().  Add new parameter 'vm_flags' to
    __vmalloc_node_range() function.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 1526fe712ca0..7d7acb35603d 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -76,7 +76,9 @@ extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
 extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
-			pgprot_t prot, int node, const void *caller);
+			pgprot_t prot, unsigned long vm_flags, int node,
+			const void *caller);
+
 extern void vfree(const void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,

commit 71394fe50146202f2c8d92cf50f5ebc761acf254
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:40:03 2015 -0800

    mm: vmalloc: add flag preventing guard hole allocation
    
    For instrumenting global variables KASan will shadow memory backing memory
    for modules.  So on module loading we will need to allocate memory for
    shadow and map it at address in shadow that corresponds to the address
    allocated in module_alloc().
    
    __vmalloc_node_range() could be used for this purpose, except it puts a
    guard hole after allocated area.  Guard hole in shadow memory should be a
    problem because at some future point we might need to have a shadow memory
    at address occupied by guard hole.  So we could fail to allocate shadow
    for module_alloc().
    
    Add a new vm_struct flag 'VM_NO_GUARD' indicating that vm area doesn't
    have a guard hole.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index b87696fdf06a..1526fe712ca0 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -16,6 +16,7 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 #define VM_USERMAP		0x00000008	/* suitable for remap_vmalloc_range */
 #define VM_VPAGES		0x00000010	/* buffer for pages was vmalloc'ed */
 #define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
+#define VM_NO_GUARD		0x00000040      /* don't add guard page */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*
@@ -96,8 +97,12 @@ void vmalloc_sync_all(void);
 
 static inline size_t get_vm_area_size(const struct vm_struct *area)
 {
-	/* return actual size without guard page */
-	return area->size - PAGE_SIZE;
+	if (!(area->flags & VM_NO_GUARD))
+		/* return actual size without guard page */
+		return area->size - PAGE_SIZE;
+	else
+		return area->size;
+
 }
 
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);

commit f6f8ed47353597dcb895eb4a15a28af657392e72
Author: WANG Chao <chaowang@redhat.com>
Date:   Wed Aug 6 16:06:58 2014 -0700

    mm/vmalloc.c: clean up map_vm_area third argument
    
    Currently map_vm_area() takes (struct page *** pages) as third argument,
    and after mapping, it moves (*pages) to point to (*pages +
    nr_mappped_pages).
    
    It looks like this kind of increment is useless to its caller these
    days.  The callers don't care about the increments and actually they're
    trying to avoid this by passing another copy to map_vm_area().
    
    The caller can always guarantee all the pages can be mapped into vm_area
    as specified in first argument and the caller only cares about whether
    map_vm_area() fails or not.
    
    This patch cleans up the pointer movement in map_vm_area() and updates
    its callers accordingly.
    
    Signed-off-by: WANG Chao <chaowang@redhat.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4b8a89189a29..b87696fdf06a 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -113,7 +113,7 @@ extern struct vm_struct *remove_vm_area(const void *addr);
 extern struct vm_struct *find_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
-			struct page ***pages);
+			struct page **pages);
 #ifdef CONFIG_MMU
 extern int map_kernel_range_noflush(unsigned long start, unsigned long size,
 				    pgprot_t prot, struct page **pages);

commit 20fc02b477c526c6a85f84e3770373778ff2f97e
Author: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
Date:   Mon Jul 8 15:59:58 2013 -0700

    mm/vmalloc.c: rename VM_UNLIST to VM_UNINITIALIZED
    
    VM_UNLIST was used to indicate that the vm_struct is not listed in
    vmlist.
    
    But after commit 4341fa454796 ("mm, vmalloc: remove list management of
    vmlist after initializing vmalloc"), the meaning of this flag changed.
    It now means the vm_struct is not fully initialized.  So renaming it to
    VM_UNINITIALIZED seems more reasonable.
    
    Also change clear_vm_unlist to clear_vm_uninitialized_flag.
    
    Signed-off-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index dd0a2c810529..4b8a89189a29 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -10,12 +10,12 @@
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 
 /* bits in flags of vmalloc's vm_struct below */
-#define VM_IOREMAP	0x00000001	/* ioremap() and friends */
-#define VM_ALLOC	0x00000002	/* vmalloc() */
-#define VM_MAP		0x00000004	/* vmap()ed pages */
-#define VM_USERMAP	0x00000008	/* suitable for remap_vmalloc_range */
-#define VM_VPAGES	0x00000010	/* buffer for pages was vmalloc'ed */
-#define VM_UNLIST	0x00000020	/* vm_struct is not listed in vmlist */
+#define VM_IOREMAP		0x00000001	/* ioremap() and friends */
+#define VM_ALLOC		0x00000002	/* vmalloc() */
+#define VM_MAP			0x00000004	/* vmap()ed pages */
+#define VM_USERMAP		0x00000008	/* suitable for remap_vmalloc_range */
+#define VM_VPAGES		0x00000010	/* buffer for pages was vmalloc'ed */
+#define VM_UNINITIALIZED	0x00000020	/* vm_struct is not fully initialized */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*

commit e69e9d4aee712a22665f008ae0550bb3d7c7f7c1
Author: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
Date:   Wed Jul 3 15:02:18 2013 -0700

    vmalloc: introduce remap_vmalloc_range_partial
    
    We want to allocate ELF note segment buffer on the 2nd kernel in vmalloc
    space and remap it to user-space in order to reduce the risk that memory
    allocation fails on system with huge number of CPUs and so with huge ELF
    note segment that exceeds 11-order block size.
    
    Although there's already remap_vmalloc_range for the purpose of
    remapping vmalloc memory to user-space, we need to specify user-space
    range via vma.
     Mmap on /proc/vmcore needs to remap range across multiple objects, so
    the interface that requires vma to cover full range is problematic.
    
    This patch introduces remap_vmalloc_range_partial that receives user-space
    range as a pair of base address and size and can be used for mmap on
    /proc/vmcore case.
    
    remap_vmalloc_range is rewritten using remap_vmalloc_range_partial.
    
    [akpm@linux-foundation.org: use PAGE_ALIGNED()]
    Signed-off-by: HATAYAMA Daisuke <d.hatayama@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Cc: Lisa Mitchell <lisa.mitchell@hp.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 7d5773a99f20..dd0a2c810529 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -82,6 +82,10 @@ extern void *vmap(struct page **pages, unsigned int count,
 			unsigned long flags, pgprot_t prot);
 extern void vunmap(const void *addr);
 
+extern int remap_vmalloc_range_partial(struct vm_area_struct *vma,
+				       unsigned long uaddr, void *kaddr,
+				       unsigned long size);
+
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
 void vmalloc_sync_all(void);

commit 13ba3fcbbe31068b1ee7c39a0b58ecbed03c4d72
Author: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
Date:   Mon Apr 29 15:07:40 2013 -0700

    kexec, vmalloc: export additional vmalloc layer information
    
    Now, vmap_area_list is exported as VMCOREINFO for makedumpfile to get
    the start address of vmalloc region (vmalloc_start).  The address which
    contains vmalloc_start value is represented as below:
    
      vmap_area_list.next - OFFSET(vmap_area.list) + OFFSET(vmap_area.va_start)
    
    However, both OFFSET(vmap_area.va_start) and OFFSET(vmap_area.list)
    aren't exported as VMCOREINFO.
    
    So this patch exports them externally with small cleanup.
    
    [akpm@linux-foundation.org: vmalloc.h should include list.h for list_head]
    Signed-off-by: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Dave Anderson <anderson@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 8a25f9081ed0..7d5773a99f20 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -3,7 +3,9 @@
 
 #include <linux/spinlock.h>
 #include <linux/init.h>
+#include <linux/list.h>
 #include <asm/page.h>		/* pgprot_t */
+#include <linux/rbtree.h>
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 
@@ -35,6 +37,17 @@ struct vm_struct {
 	const void		*caller;
 };
 
+struct vmap_area {
+	unsigned long va_start;
+	unsigned long va_end;
+	unsigned long flags;
+	struct rb_node rb_node;         /* address sorted rbtree */
+	struct list_head list;          /* address sorted list */
+	struct list_head purge_list;    /* "lazy purge" list */
+	struct vm_struct *vm;
+	struct rcu_head rcu_head;
+};
+
 /*
  *	Highlevel APIs for driver use
  */

commit f1c4069e1dc128dc8a851174cba2e273652e9216
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Apr 29 15:07:37 2013 -0700

    mm, vmalloc: export vmap_area_list, instead of vmlist
    
    Although our intention is to unexport internal structure entirely, but
    there is one exception for kexec.  kexec dumps address of vmlist and
    makedumpfile uses this information.
    
    We are about to remove vmlist, then another way to retrieve information
    of vmalloc layer is needed for makedumpfile.  For this purpose, we
    export vmap_area_list, instead of vmlist.
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Dave Anderson <anderson@redhat.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Cc: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 698b1e50d3a4..8a25f9081ed0 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -130,8 +130,7 @@ extern long vwrite(char *buf, char *addr, unsigned long count);
 /*
  *	Internals.  Dont't use..
  */
-extern rwlock_t vmlist_lock;
-extern struct vm_struct *vmlist;
+extern struct list_head vmap_area_list;
 extern __init void vm_area_add_early(struct vm_struct *vm);
 extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 

commit db3808c1bac64740b9d830fda92801ae65f1c851
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Mon Apr 29 15:07:28 2013 -0700

    mm, vmalloc: move get_vmalloc_info() to vmalloc.c
    
    Now get_vmalloc_info() is in fs/proc/mmu.c.  There is no reason that this
    code must be here and it's implementation needs vmlist_lock and it iterate
    a vmlist which may be internal data structure for vmalloc.
    
    It is preferable that vmlist_lock and vmlist is only used in vmalloc.c
    for maintainability. So move the code to vmalloc.c
    
    Signed-off-by: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Atsushi Kumagai <kumagai-atsushi@mxc.nes.nec.co.jp>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Dave Anderson <anderson@redhat.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 6071e911c7f4..698b1e50d3a4 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -158,4 +158,22 @@ pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 # endif
 #endif
 
+struct vmalloc_info {
+	unsigned long   used;
+	unsigned long   largest_chunk;
+};
+
+#ifdef CONFIG_MMU
+#define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)
+extern void get_vmalloc_info(struct vmalloc_info *vmi);
+#else
+
+#define VMALLOC_TOTAL 0UL
+#define get_vmalloc_info(vmi)			\
+do {						\
+	(vmi)->used = 0;			\
+	(vmi)->largest_chunk = 0;		\
+} while (0)
+#endif
+
 #endif /* _LINUX_VMALLOC_H */

commit e9da6e9905e639b0f842a244bc770b48ad0523e9
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 30 09:11:33 2012 +0200

    ARM: dma-mapping: remove custom consistent dma region
    
    This patch changes dma-mapping subsystem to use generic vmalloc areas
    for all consistent dma allocations. This increases the total size limit
    of the consistent allocations and removes platform hacks and a lot of
    duplicated code.
    
    Atomic allocations are served from special pool preallocated on boot,
    because vmalloc areas cannot be reliably created in atomic context.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 2e28f4d40d7f..6071e911c7f4 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -93,6 +93,7 @@ extern struct vm_struct *__get_vm_area_caller(unsigned long size,
 					unsigned long start, unsigned long end,
 					const void *caller);
 extern struct vm_struct *remove_vm_area(const void *addr);
+extern struct vm_struct *find_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);

commit 5e6cafc83e30f0f70c79a2b7aef237dc57e29f02
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Apr 13 12:32:09 2012 +0200

    mm: vmalloc: use const void * for caller argument
    
    'const void *' is a safer type for caller function type. This patch
    updates all references to caller function type.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index dcdfc2bda922..2e28f4d40d7f 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -32,7 +32,7 @@ struct vm_struct {
 	struct page		**pages;
 	unsigned int		nr_pages;
 	phys_addr_t		phys_addr;
-	void			*caller;
+	const void		*caller;
 };
 
 /*
@@ -62,7 +62,7 @@ extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
 extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
 			unsigned long start, unsigned long end, gfp_t gfp_mask,
-			pgprot_t prot, int node, void *caller);
+			pgprot_t prot, int node, const void *caller);
 extern void vfree(const void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,
@@ -85,13 +85,13 @@ static inline size_t get_vm_area_size(const struct vm_struct *area)
 
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *get_vm_area_caller(unsigned long size,
-					unsigned long flags, void *caller);
+					unsigned long flags, const void *caller);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
 extern struct vm_struct *__get_vm_area_caller(unsigned long size,
 					unsigned long flags,
 					unsigned long start, unsigned long end,
-					void *caller);
+					const void *caller);
 extern struct vm_struct *remove_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,

commit 73829af71fdb8655e7ba4b5a2a6612ad34a75a11
Merge: 742eaa6a6e35 0af362f8440a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Dec 5 23:27:54 2011 +0000

    Merge branch 'vmalloc' of git://git.linaro.org/people/nico/linux into devel-stable

commit be9b7335e70696bee731c152429b1737e42fe163
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Aug 25 00:24:21 2011 -0400

    mm: add vm_area_add_early()
    
    The existing vm_area_register_early() allows for early vmalloc space
    allocation.  However upcoming cleanups in the ARM architecture require
    that some fixed locations in the vmalloc area be reserved also very early.
    
    The name "vm_area_register_early" would have been a good name for the
    reservation part without the allocation.  Since it is already in use with
    different semantics, let's create vm_area_add_early() instead.
    
    Both vm_area_register_early() and vm_area_add_early() can be used together
    meaning that the former is now implemented using the later where it is
    ensured that no conflicting areas are added, but no attempt is made to
    make the allocation scheme in vm_area_register_early() more sophisticated.
    After all, you must know what you're doing when using those functions.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 687fb11e2010..4115d6aa80be 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -131,6 +131,7 @@ extern long vwrite(char *buf, char *addr, unsigned long count);
  */
 extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
+extern __init void vm_area_add_early(struct vm_struct *vm);
 extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 
 #ifdef CONFIG_SMP

commit cd12909cb576d37311fe35868780e82d5007d0c8
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Sep 29 16:53:32 2011 +0100

    xen: map foreign pages for shared rings by updating the PTEs directly
    
    When mapping a foreign page with xenbus_map_ring_valloc() with the
    GNTTABOP_map_grant_ref hypercall, set the GNTMAP_contains_pte flag and
    pass a pointer to the PTE (in init_mm).
    
    After the page is mapped, the usual fault mechanism can be used to
    update additional MMs.  This allows the vmalloc_sync_all() to be
    removed from alloc_vm_area().
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    [v1: Squashed fix by Michal for no-mmu case]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Michal Simek <monstr@monstr.eu>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 687fb11e2010..4bde182fcf93 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -119,7 +119,7 @@ unmap_kernel_range(unsigned long addr, unsigned long size)
 #endif
 
 /* Allocate/destroy a 'vmalloc' VM area. */
-extern struct vm_struct *alloc_vm_area(size_t size);
+extern struct vm_struct *alloc_vm_area(size_t size, pte_t **ptes);
 extern void free_vm_area(struct vm_struct *area);
 
 /* for /dev/kmem */

commit f5252e009d5b87071a919221e4f6624184005368
Author: Mitsuo Hayasaka <mitsuo.hayasaka.hu@hitachi.com>
Date:   Mon Oct 31 17:08:13 2011 -0700

    mm: avoid null pointer access in vm_struct via /proc/vmallocinfo
    
    The /proc/vmallocinfo shows information about vmalloc allocations in
    vmlist that is a linklist of vm_struct.  It, however, may access pages
    field of vm_struct where a page was not allocated.  This results in a null
    pointer access and leads to a kernel panic.
    
    Why this happens: In __vmalloc_node_range() called from vmalloc(), newly
    allocated vm_struct is added to vmlist at __get_vm_area_node() and then,
    some fields of vm_struct such as nr_pages and pages are set at
    __vmalloc_area_node().  In other words, it is added to vmlist before it is
    fully initialized.  At the same time, when the /proc/vmallocinfo is read,
    it accesses the pages field of vm_struct according to the nr_pages field
    at show_numa_info().  Thus, a null pointer access happens.
    
    The patch adds the newly allocated vm_struct to the vmlist *after* it is
    fully initialized.  So, it can avoid accessing the pages field with
    unallocated page when show_numa_info() is called.
    
    Signed-off-by: Mitsuo Hayasaka <mitsuo.hayasaka.hu@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: <stable@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 9332e52ea8c2..687fb11e2010 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -13,6 +13,7 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 #define VM_MAP		0x00000004	/* vmap()ed pages */
 #define VM_USERMAP	0x00000008	/* suitable for remap_vmalloc_range */
 #define VM_VPAGES	0x00000010	/* buffer for pages was vmalloc'ed */
+#define VM_UNLIST	0x00000020	/* vm_struct is not listed in vmlist */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*

commit b554cb426a955a267dba524f98f99e29bc947643
Author: Graf Yang <graf.yang@analog.com>
Date:   Mon Mar 28 12:53:29 2011 +0100

    NOMMU: support SMP dynamic percpu_alloc
    
    The percpu code requires more functions to be implemented in the mm core
    which nommu currently does not provide.  So add inline implementations
    since these are largely meaningless on nommu systems.
    
    Signed-off-by: Graf Yang <graf.yang@analog.com>
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Greg Ungerer <gerg@uclinux.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4ed6fcd6b726..9332e52ea8c2 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -95,10 +95,27 @@ extern struct vm_struct *remove_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
+#ifdef CONFIG_MMU
 extern int map_kernel_range_noflush(unsigned long start, unsigned long size,
 				    pgprot_t prot, struct page **pages);
 extern void unmap_kernel_range_noflush(unsigned long addr, unsigned long size);
 extern void unmap_kernel_range(unsigned long addr, unsigned long size);
+#else
+static inline int
+map_kernel_range_noflush(unsigned long start, unsigned long size,
+			pgprot_t prot, struct page **pages)
+{
+	return size >> PAGE_SHIFT;
+}
+static inline void
+unmap_kernel_range_noflush(unsigned long addr, unsigned long size)
+{
+}
+static inline void
+unmap_kernel_range(unsigned long addr, unsigned long size)
+{
+}
+#endif
 
 /* Allocate/destroy a 'vmalloc' VM area. */
 extern struct vm_struct *alloc_vm_area(size_t size);
@@ -116,11 +133,26 @@ extern struct vm_struct *vmlist;
 extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 
 #ifdef CONFIG_SMP
+# ifdef CONFIG_MMU
 struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 				     const size_t *sizes, int nr_vms,
 				     size_t align);
 
 void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);
+# else
+static inline struct vm_struct **
+pcpu_get_vm_areas(const unsigned long *offsets,
+		const size_t *sizes, int nr_vms,
+		size_t align)
+{
+	return NULL;
+}
+
+static inline void
+pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
+{
+}
+# endif
 #endif
 
 #endif /* _LINUX_VMALLOC_H */

commit d0a21265dfb5fa8ae54e90d0fb6d1c215b10a28a
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jan 13 15:46:02 2011 -0800

    mm: unify module_alloc code for vmalloc
    
    Four architectures (arm, mips, sparc, x86) use __vmalloc_area() for
    module_init().  Much of the code is duplicated and can be generalized in a
    globally accessible function, __vmalloc_node_range().
    
    __vmalloc_node() now calls into __vmalloc_node_range() with a range of
    [VMALLOC_START, VMALLOC_END) for functionally equivalent behavior.
    
    Each architecture may then use __vmalloc_node_range() directly to remove
    the duplication of code.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index c7348b8d0a81..4ed6fcd6b726 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -59,8 +59,9 @@ extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
-extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask,
-				pgprot_t prot);
+extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
+			unsigned long start, unsigned long end, gfp_t gfp_mask,
+			pgprot_t prot, int node, void *caller);
 extern void vfree(const void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,

commit ec3f64fc9c196a304c4b7db3e1ff56d640628509
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jan 13 15:46:01 2011 -0800

    mm: remove gfp mask from pcpu_get_vm_areas
    
    pcpu_get_vm_areas() only uses GFP_KERNEL allocations, so remove the gfp_t
    formal and use the mask internally.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index cb73c755fac8..c7348b8d0a81 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -117,7 +117,7 @@ extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 #ifdef CONFIG_SMP
 struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 				     const size_t *sizes, int nr_vms,
-				     size_t align, gfp_t gfp_mask);
+				     size_t align);
 
 void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);
 #endif

commit e5a5623b28198aa91ea71ee5d3846757fc76bc87
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jan 13 15:46:00 2011 -0800

    mm: remove unused get_vm_area_node
    
    get_vm_area_node() is unused in the kernel and can thus be removed.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 44b54f619ac6..cb73c755fac8 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -90,9 +90,6 @@ extern struct vm_struct *__get_vm_area_caller(unsigned long size,
 					unsigned long flags,
 					unsigned long start, unsigned long end,
 					void *caller);
-extern struct vm_struct *get_vm_area_node(unsigned long size,
-					  unsigned long flags, int node,
-					  gfp_t gfp_mask);
 extern struct vm_struct *remove_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,

commit 64141da587241301ce8638cc945f8b67853156ec
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Dec 2 14:31:18 2010 -0800

    vmalloc: eagerly clear ptes on vunmap
    
    On stock 2.6.37-rc4, running:
    
      # mount lilith:/export /mnt/lilith
      # find  /mnt/lilith/ -type f -print0 | xargs -0 file
    
    crashes the machine fairly quickly under Xen.  Often it results in oops
    messages, but the couple of times I tried just now, it just hung quietly
    and made Xen print some rude messages:
    
        (XEN) mm.c:2389:d80 Bad type (saw 7400000000000001 != exp
        3000000000000000) for mfn 1d7058 (pfn 18fa7)
        (XEN) mm.c:964:d80 Attempt to create linear p.t. with write perms
        (XEN) mm.c:2389:d80 Bad type (saw 7400000000000010 != exp
        1000000000000000) for mfn 1d2e04 (pfn 1d1fb)
        (XEN) mm.c:2965:d80 Error while pinning mfn 1d2e04
    
    Which means the domain tried to map a pagetable page RW, which would
    allow it to map arbitrary memory, so Xen stopped it.  This is because
    vm_unmap_ram() left some pages mapped in the vmalloc area after NFS had
    finished with them, and those pages got recycled as pagetable pages
    while still having these RW aliases.
    
    Removing those mappings immediately removes the Xen-visible aliases, and
    so it has no problem with those pages being reused as pagetable pages.
    Deferring the TLB flush doesn't upset Xen because it can flush the TLB
    itself as needed to maintain its invariants.
    
    When unmapping a region in the vmalloc space, clear the ptes
    immediately.  There's no point in deferring this because there's no
    amortization benefit.
    
    The TLBs are left dirty, and they are flushed lazily to amortize the
    cost of the IPIs.
    
    This specific motivation for this patch is an oops-causing regression
    since 2.6.36 when using NFS under Xen, triggered by the NFS client's use
    of vm_map_ram() introduced in 56e4ebf877b60 ("NFS: readdir with vmapped
    pages") .  XFS also uses vm_map_ram() and could cause similar problems.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Bryan Schumaker <bjschuma@netapp.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Alex Elder <aelder@sgi.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index a03dcf62ca9d..44b54f619ac6 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -7,8 +7,6 @@
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 
-extern bool vmap_lazy_unmap;
-
 /* bits in flags of vmalloc's vm_struct below */
 #define VM_IOREMAP	0x00000001	/* ioremap() and friends */
 #define VM_ALLOC	0x00000002	/* vmalloc() */

commit e1ca7788dec6773b1a2bce51b7141948f2b8bccf
Author: Dave Young <hidave.darkstar@gmail.com>
Date:   Tue Oct 26 14:22:06 2010 -0700

    mm: add vzalloc() and vzalloc_node() helpers
    
    Add vzalloc() and vzalloc_node() to encapsulate the
    vmalloc-then-memset-zero operation.
    
    Use __GFP_ZERO to zero fill the allocated memory.
    
    Signed-off-by: Dave Young <hidave.darkstar@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Greg Ungerer <gerg@snapgear.com>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 63a4fe6d51bd..a03dcf62ca9d 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -53,8 +53,10 @@ static inline void vmalloc_init(void)
 #endif
 
 extern void *vmalloc(unsigned long size);
+extern void *vzalloc(unsigned long size);
 extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);
+extern void *vzalloc_node(unsigned long size, int node);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *vmalloc_32_user(unsigned long size);

commit 4f8b02b4e5c6896e073bed736136d420bd44b627
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 18:22:47 2010 +0200

    vmalloc: pcpu_get/free_vm_areas() aren't needed on UP
    
    These functions are used only by percpu memory allocator on SMP.
    Don't build them on UP.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Reviewed-by: Chrsitoph Lameter <cl@linux.com>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 01c2145118dc..63a4fe6d51bd 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -117,10 +117,12 @@ extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
 extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 
+#ifdef CONFIG_SMP
 struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 				     const size_t *sizes, int nr_vms,
 				     size_t align, gfp_t gfp_mask);
 
 void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);
+#endif
 
 #endif /* _LINUX_VMALLOC_H */

commit 26f0cf91813bdc8e61595f8ad6660251e2ee9cf6
Merge: d862b13bc8cb fe96eb404e33
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 12 09:09:41 2010 -0700

    Merge branch 'stable/xen-swiotlb-0.8.6' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/xen-swiotlb-0.8.6' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      x86: Detect whether we should use Xen SWIOTLB.
      pci-swiotlb-xen: Add glue code to setup dma_ops utilizing xen_swiotlb_* functions.
      swiotlb-xen: SWIOTLB library for Xen PV guest with PCI passthrough.
      xen/mmu: inhibit vmap aliases rather than trying to clear them out
      vmap: add flag to allow lazy unmap to be disabled at runtime
      xen: Add xen_create_contiguous_region
      xen: Rename the balloon lock
      xen: Allow unprivileged Xen domains to create iomap pages
      xen: use _PAGE_IOMAP in ioremap to do machine mappings
    
    Fix up trivial conflicts (adding both xen swiotlb and xen pci platform
    driver setup close to each other) in drivers/xen/{Kconfig,Makefile} and
    include/xen/xen-ops.h

commit a0d40c80256e31b23849f2ba781b74bf0218a1fa
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Fri Mar 26 15:28:51 2010 -0700

    vmap: add flag to allow lazy unmap to be disabled at runtime
    
    Add a flag to force lazy_max_pages() to zero to prevent any outstanding
    mapped pages.  We'll need this for Xen.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Nick Piggin <npiggin@suse.de>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 227c2a585e4f..b840fdaf438c 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -7,6 +7,8 @@
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 
+extern bool vmap_lazy_unmap;
+
 /* bits in flags of vmalloc's vm_struct below */
 #define VM_IOREMAP	0x00000001	/* ioremap() and friends */
 #define VM_ALLOC	0x00000002	/* vmalloc() */

commit ffa71f33a820d1ab3f2fc5723819ac60fb76080b
Author: Kenji Kaneshige <kaneshige.kenji@jp.fujitsu.com>
Date:   Fri Jun 18 12:22:40 2010 +0900

    x86, ioremap: Fix incorrect physical address handling in PAE mode
    
    Current x86 ioremap() doesn't handle physical address higher than
    32-bit properly in X86_32 PAE mode. When physical address higher than
    32-bit is passed to ioremap(), higher 32-bits in physical address is
    cleared wrongly. Due to this bug, ioremap() can map wrong address to
    linear address space.
    
    In my case, 64-bit MMIO region was assigned to a PCI device (ioat
    device) on my system. Because of the ioremap()'s bug, wrong physical
    address (instead of MMIO region) was mapped to linear address space.
    Because of this, loading ioatdma driver caused unexpected behavior
    (kernel panic, kernel hangup, ...).
    
    Signed-off-by: Kenji Kaneshige <kaneshige.kenji@jp.fujitsu.com>
    LKML-Reference: <4C1AE680.7090408@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 227c2a585e4f..de05e96e0a70 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -30,7 +30,7 @@ struct vm_struct {
 	unsigned long		flags;
 	struct page		**pages;
 	unsigned int		nr_pages;
-	unsigned long		phys_addr;
+	phys_addr_t		phys_addr;
 	void			*caller;
 };
 

commit ca23e405e06d5fffb005df004c72781f76062f51
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 14 15:00:52 2009 +0900

    vmalloc: implement pcpu_get_vm_areas()
    
    To directly use spread NUMA memories for percpu units, percpu
    allocator will be updated to allow sparsely mapping units in a chunk.
    As the distances between units can be very large, this makes
    allocating single vmap area for each chunk undesirable.  This patch
    implements pcpu_get_vm_areas() and pcpu_free_vm_areas() which
    allocates and frees sparse congruent vmap areas.
    
    pcpu_get_vm_areas() take @offsets and @sizes array which define
    distances and sizes of vmap areas.  It scans down from the top of
    vmalloc area looking for the top-most address which can accomodate all
    the areas.  The top-down scan is to avoid interacting with regular
    vmallocs which can push up these congruent areas up little by little
    ending up wasting address space and page table.
    
    To speed up top-down scan, the highest possible address hint is
    maintained.  Although the scan is linear from the hint, given the
    usual large holes between memory addresses between NUMA nodes, the
    scanning is highly likely to finish after finding the first hole for
    the last unit which is scanned first.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Nick Piggin <npiggin@suse.de>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index a43ebec3a7b9..227c2a585e4f 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -115,4 +115,10 @@ extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
 extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 
+struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
+				     const size_t *sizes, int nr_vms,
+				     size_t align, gfp_t gfp_mask);
+
+void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);
+
 #endif /* _LINUX_VMALLOC_H */

commit 0edcf8d6926f4038443dbc24e319530177ca0353
Merge: 87b203079ed9 40150d37be7f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Feb 24 21:52:45 2009 +0100

    Merge branch 'tj-percpu' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into core/percpu
    
    Conflicts:
            arch/x86/include/asm/pgtable.h

commit c0c0a29379b5848aec2e8f1c58d853d3cb7118b8
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 24 11:57:21 2009 +0900

    vmalloc: add @align to vm_area_register_early()
    
    Impact: allow larger alignment for early vmalloc area allocation
    
    Some early vmalloc users might want larger alignment, for example, for
    custom large page mapping.  Add @align to vm_area_register_early().
    While at it, drop docbook comment on non-existent @size.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 599ba7984310..2f6994fdf0e0 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -109,6 +109,6 @@ extern long vwrite(char *buf, char *addr, unsigned long count);
  */
 extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
-extern __init void vm_area_register_early(struct vm_struct *vm);
+extern __init void vm_area_register_early(struct vm_struct *vm, size_t align);
 
 #endif /* _LINUX_VMALLOC_H */

commit 8fc48985006da4ceba24508db64ec77fc0dfe3bb
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Feb 20 16:29:08 2009 +0900

    vmalloc: add un/map_kernel_range_noflush()
    
    Impact: two more public map/unmap functions
    
    Implement map_kernel_range_noflush() and unmap_kernel_range_noflush().
    These functions respectively map and unmap address range in kernel VM
    area but doesn't do any vcache or tlb flushing.  These will be used by
    new percpu allocator.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index bbc051392298..599ba7984310 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -91,6 +91,9 @@ extern struct vm_struct *remove_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
+extern int map_kernel_range_noflush(unsigned long start, unsigned long size,
+				    pgprot_t prot, struct page **pages);
+extern void unmap_kernel_range_noflush(unsigned long addr, unsigned long size);
 extern void unmap_kernel_range(unsigned long addr, unsigned long size);
 
 /* Allocate/destroy a 'vmalloc' VM area. */

commit f0aa6617903648077dffe5cfcf7c4458f4610fa7
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Feb 20 16:29:08 2009 +0900

    vmalloc: implement vm_area_register_early()
    
    Impact: allow multiple early vm areas
    
    There are places where kernel VM area needs to be allocated before
    vmalloc is initialized.  This is done by allocating static vm_struct,
    initializing several fields and linking it to vmlist and later vmalloc
    initialization picking up these from vmlist.  This is currently done
    manually and if there's more than one such areas, there's no defined
    way to arbitrate who gets which address.
    
    This patch implements vm_area_register_early(), which takes vm_area
    struct with flags and size initialized, assigns address to it and puts
    it on the vmlist.  This way, multiple early vm areas can determine
    which addresses they should use.  The only current user - alpha mm
    init - is converted to use it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 506e7620a986..bbc051392298 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -106,5 +106,6 @@ extern long vwrite(char *buf, char *addr, unsigned long count);
  */
 extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
+extern __init void vm_area_register_early(struct vm_struct *vm);
 
 #endif /* _LINUX_VMALLOC_H */

commit c296861291669f305deef19b78042330d7135017
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Feb 18 14:48:12 2009 -0800

    vmalloc: add __get_vm_area_caller()
    
    We have get_vm_area_caller() and __get_vm_area() but not
    __get_vm_area_caller()
    
    On powerpc, I use __get_vm_area() to separate the ranges of addresses
    given to vmalloc vs.  ioremap (various good reasons for that) so in order
    to be able to implement the new caller tracking in /proc/vmallocinfo, I
    need a "_caller" variant of it.
    
    (akpm: needed for ongoing powerpc development, so merge it early)
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 506e7620a986..9c0890c7a06a 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -84,6 +84,10 @@ extern struct vm_struct *get_vm_area_caller(unsigned long size,
 					unsigned long flags, void *caller);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
+extern struct vm_struct *__get_vm_area_caller(unsigned long size,
+					unsigned long flags,
+					unsigned long start, unsigned long end,
+					void *caller);
 extern struct vm_struct *get_vm_area_node(unsigned long size,
 					  unsigned long flags, int node,
 					  gfp_t gfp_mask);

commit 69beeb1d3428424fbc7546f85e5cd7ac4119c09d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jan 6 14:39:46 2009 -0800

    mm: make vread() and vwrite() declaration
    
    Sparse output following warnings.
    
    mm/vmalloc.c:1436:6: warning: symbol 'vread' was not declared. Should it be static?
    mm/vmalloc.c:1474:6: warning: symbol 'vwrite' was not declared. Should it be static?
    
    However, it is used by /dev/kmem. fixed here.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 307b88577eaa..506e7620a986 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -97,6 +97,10 @@ extern void unmap_kernel_range(unsigned long addr, unsigned long size);
 extern struct vm_struct *alloc_vm_area(size_t size);
 extern void free_vm_area(struct vm_struct *area);
 
+/* for /dev/kmem */
+extern long vread(char *buf, char *addr, unsigned long count);
+extern long vwrite(char *buf, char *addr, unsigned long count);
+
 /*
  *	Internals.  Dont't use..
  */

commit 5f6a6a9c4e4d790aae55cb412a7643329057c5e0
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Oct 6 03:50:47 2008 +0400

    proc: move /proc/vmallocinfo to mm/vmalloc.c
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4c28c4d564e2..307b88577eaa 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -103,6 +103,4 @@ extern void free_vm_area(struct vm_struct *area);
 extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
 
-extern const struct seq_operations vmalloc_op;
-
 #endif /* _LINUX_VMALLOC_H */

commit db64fe02258f1507e13fe5212a989922323685ce
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:27:03 2008 -0700

    mm: rewrite vmap layer
    
    Rewrite the vmap allocator to use rbtrees and lazy tlb flushing, and
    provide a fast, scalable percpu frontend for small vmaps (requires a
    slightly different API, though).
    
    The biggest problem with vmap is actually vunmap.  Presently this requires
    a global kernel TLB flush, which on most architectures is a broadcast IPI
    to all CPUs to flush the cache.  This is all done under a global lock.  As
    the number of CPUs increases, so will the number of vunmaps a scaled
    workload will want to perform, and so will the cost of a global TLB flush.
     This gives terrible quadratic scalability characteristics.
    
    Another problem is that the entire vmap subsystem works under a single
    lock.  It is a rwlock, but it is actually taken for write in all the fast
    paths, and the read locking would likely never be run concurrently anyway,
    so it's just pointless.
    
    This is a rewrite of vmap subsystem to solve those problems.  The existing
    vmalloc API is implemented on top of the rewritten subsystem.
    
    The TLB flushing problem is solved by using lazy TLB unmapping.  vmap
    addresses do not have to be flushed immediately when they are vunmapped,
    because the kernel will not reuse them again (would be a use-after-free)
    until they are reallocated.  So the addresses aren't allocated again until
    a subsequent TLB flush.  A single TLB flush then can flush multiple
    vunmaps from each CPU.
    
    XEN and PAT and such do not like deferred TLB flushing because they can't
    always handle multiple aliasing virtual addresses to a physical address.
    They now call vm_unmap_aliases() in order to flush any deferred mappings.
    That call is very expensive (well, actually not a lot more expensive than
    a single vunmap under the old scheme), however it should be OK if not
    called too often.
    
    The virtual memory extent information is stored in an rbtree rather than a
    linked list to improve the algorithmic scalability.
    
    There is a per-CPU allocator for small vmaps, which amortizes or avoids
    global locking.
    
    To use the per-CPU interface, the vm_map_ram / vm_unmap_ram interfaces
    must be used in place of vmap and vunmap.  Vmalloc does not use these
    interfaces at the moment, so it will not be quite so scalable (although it
    will use lazy TLB flushing).
    
    As a quick test of performance, I ran a test that loops in the kernel,
    linearly mapping then touching then unmapping 4 pages.  Different numbers
    of tests were run in parallel on an 4 core, 2 socket opteron.  Results are
    in nanoseconds per map+touch+unmap.
    
    threads           vanilla         vmap rewrite
    1                 14700           2900
    2                 33600           3000
    4                 49500           2800
    8                 70631           2900
    
    So with a 8 cores, the rewritten version is already 25x faster.
    
    In a slightly more realistic test (although with an older and less
    scalable version of the patch), I ripped the not-very-good vunmap batching
    code out of XFS, and implemented the large buffer mapping with vm_map_ram
    and vm_unmap_ram...  along with a couple of other tricks, I was able to
    speed up a large directory workload by 20x on a 64 CPU system.  I believe
    vmap/vunmap is actually sped up a lot more than 20x on such a system, but
    I'm running into other locks now.  vmap is pretty well blown off the
    profiles.
    
    Before:
    1352059 total                                      0.1401
    798784 _write_lock                              8320.6667 <- vmlist_lock
    529313 default_idle                             1181.5022
     15242 smp_call_function                         15.8771  <- vmap tlb flushing
      2472 __get_vm_area_node                         1.9312  <- vmap
      1762 remove_vm_area                             4.5885  <- vunmap
       316 map_vm_area                                0.2297  <- vmap
       312 kfree                                      0.1950
       300 _spin_lock                                 3.1250
       252 sn_send_IPI_phys                           0.4375  <- tlb flushing
       238 vmap                                       0.8264  <- vmap
       216 find_lock_page                             0.5192
       196 find_next_bit                              0.3603
       136 sn2_send_IPI                               0.2024
       130 pio_phys_write_mmr                         2.0312
       118 unmap_kernel_range                         0.1229
    
    After:
     78406 total                                      0.0081
     40053 default_idle                              89.4040
     33576 ia64_spinlock_contention                 349.7500
      1650 _spin_lock                                17.1875
       319 __reg_op                                   0.5538
       281 _atomic_dec_and_lock                       1.0977
       153 mutex_unlock                               1.5938
       123 iget_locked                                0.1671
       117 xfs_dir_lookup                             0.1662
       117 dput                                       0.1406
       114 xfs_iget_core                              0.0268
        92 xfs_da_hashname                            0.1917
        75 d_alloc                                    0.0670
        68 vmap_page_range                            0.0462 <- vmap
        58 kmem_cache_alloc                           0.0604
        57 memset                                     0.0540
        52 rb_next                                    0.1625
        50 __copy_user                                0.0208
        49 bitmap_find_free_region                    0.2188 <- vmap
        46 ia64_sn_udelay                             0.1106
        45 find_inode_fast                            0.1406
        42 memcmp                                     0.2188
        42 finish_task_switch                         0.1094
        42 __d_lookup                                 0.0410
        40 radix_tree_lookup_slot                     0.1250
        37 _spin_unlock_irqrestore                    0.3854
        36 xfs_bmapi                                  0.0050
        36 kmem_cache_free                            0.0256
        35 xfs_vn_getattr                             0.0322
        34 radix_tree_lookup                          0.1062
        33 __link_path_walk                           0.0035
        31 xfs_da_do_buf                              0.0091
        30 _xfs_buf_find                              0.0204
        28 find_get_page                              0.0875
        27 xfs_iread                                  0.0241
        27 __strncpy_from_user                        0.2812
        26 _xfs_buf_initialize                        0.0406
        24 _xfs_buf_lookup_pages                      0.0179
        24 vunmap_page_range                          0.0250 <- vunmap
        23 find_lock_page                             0.0799
        22 vm_map_ram                                 0.0087 <- vmap
        20 kfree                                      0.0125
        19 put_page                                   0.0330
        18 __kmalloc                                  0.0176
        17 xfs_da_node_lookup_int                     0.0086
        17 _read_lock                                 0.0885
        17 page_waitqueue                             0.0664
    
    vmap has gone from being the top 5 on the profiles and flushing the crap
    out of all TLBs, to using less than 1% of kernel time.
    
    [akpm@linux-foundation.org: cleanups, section fix]
    [akpm@linux-foundation.org: fix build on alpha]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Krzysztof Helt <krzysztof.h1@poczta.fm>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 328eb4022727..4c28c4d564e2 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -2,6 +2,7 @@
 #define _LINUX_VMALLOC_H
 
 #include <linux/spinlock.h>
+#include <linux/init.h>
 #include <asm/page.h>		/* pgprot_t */
 
 struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
@@ -23,7 +24,6 @@ struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 #endif
 
 struct vm_struct {
-	/* keep next,addr,size together to speedup lookups */
 	struct vm_struct	*next;
 	void			*addr;
 	unsigned long		size;
@@ -37,6 +37,19 @@ struct vm_struct {
 /*
  *	Highlevel APIs for driver use
  */
+extern void vm_unmap_ram(const void *mem, unsigned int count);
+extern void *vm_map_ram(struct page **pages, unsigned int count,
+				int node, pgprot_t prot);
+extern void vm_unmap_aliases(void);
+
+#ifdef CONFIG_MMU
+extern void __init vmalloc_init(void);
+#else
+static inline void vmalloc_init(void)
+{
+}
+#endif
+
 extern void *vmalloc(unsigned long size);
 extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);

commit 605d9288b3e8a3d15e6f36185c2fc737b6979572
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Aug 16 11:07:21 2008 +0100

    mm: VM_flags comment fixes
    
    Try to comment away a little of the confusion between mm's vm_area_struct
    vm_flags and vmalloc's vm_struct flags: based on an idea by Ulrich Drepper.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 364789aae9f3..328eb4022727 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -4,9 +4,9 @@
 #include <linux/spinlock.h>
 #include <asm/page.h>		/* pgprot_t */
 
-struct vm_area_struct;
+struct vm_area_struct;		/* vma defining user mapping in mm_types.h */
 
-/* bits in vm_struct->flags */
+/* bits in flags of vmalloc's vm_struct below */
 #define VM_IOREMAP	0x00000001	/* ioremap() and friends */
 #define VM_ALLOC	0x00000002	/* vmalloc() */
 #define VM_MAP		0x00000004	/* vmap()ed pages */

commit 2301696932b55e2ea2085cefc84f7b94fa2dd54b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 28 02:12:42 2008 -0700

    vmallocinfo: add caller information
    
    Add caller information so that /proc/vmallocinfo shows where the allocation
    request for a slice of vmalloc memory originated.
    
    Results in output like this:
    
    0xffffc20000000000-0xffffc20000801000 8392704 alloc_large_system_hash+0x127/0x246 pages=2048 vmalloc vpages
    0xffffc20000801000-0xffffc20000806000   20480 alloc_large_system_hash+0x127/0x246 pages=4 vmalloc
    0xffffc20000806000-0xffffc20000c07000 4198400 alloc_large_system_hash+0x127/0x246 pages=1024 vmalloc vpages
    0xffffc20000c07000-0xffffc20000c0a000   12288 alloc_large_system_hash+0x127/0x246 pages=2 vmalloc
    0xffffc20000c0a000-0xffffc20000c0c000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c0c000-0xffffc20000c0f000   12288 acpi_os_map_memory+0x13/0x1c phys=cff64000 ioremap
    0xffffc20000c10000-0xffffc20000c15000   20480 acpi_os_map_memory+0x13/0x1c phys=cff65000 ioremap
    0xffffc20000c16000-0xffffc20000c18000    8192 acpi_os_map_memory+0x13/0x1c phys=cff69000 ioremap
    0xffffc20000c18000-0xffffc20000c1a000    8192 acpi_os_map_memory+0x13/0x1c phys=fed1f000 ioremap
    0xffffc20000c1a000-0xffffc20000c1c000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c1c000-0xffffc20000c1e000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c1e000-0xffffc20000c20000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c20000-0xffffc20000c22000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c22000-0xffffc20000c24000    8192 acpi_os_map_memory+0x13/0x1c phys=cff68000 ioremap
    0xffffc20000c24000-0xffffc20000c26000    8192 acpi_os_map_memory+0x13/0x1c phys=e0081000 ioremap
    0xffffc20000c26000-0xffffc20000c28000    8192 acpi_os_map_memory+0x13/0x1c phys=e0080000 ioremap
    0xffffc20000c28000-0xffffc20000c2d000   20480 alloc_large_system_hash+0x127/0x246 pages=4 vmalloc
    0xffffc20000c2d000-0xffffc20000c31000   16384 tcp_init+0xd5/0x31c pages=3 vmalloc
    0xffffc20000c31000-0xffffc20000c34000   12288 alloc_large_system_hash+0x127/0x246 pages=2 vmalloc
    0xffffc20000c34000-0xffffc20000c36000    8192 init_vdso_vars+0xde/0x1f1
    0xffffc20000c36000-0xffffc20000c38000    8192 pci_iomap+0x8a/0xb4 phys=d8e00000 ioremap
    0xffffc20000c38000-0xffffc20000c3a000    8192 usb_hcd_pci_probe+0x139/0x295 [usbcore] phys=d8e00000 ioremap
    0xffffc20000c3a000-0xffffc20000c3e000   16384 sys_swapon+0x509/0xa15 pages=3 vmalloc
    0xffffc20000c40000-0xffffc20000c61000  135168 e1000_probe+0x1c4/0xa32 phys=d8a20000 ioremap
    0xffffc20000c61000-0xffffc20000c6a000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20000c6a000-0xffffc20000c73000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20000c73000-0xffffc20000c7c000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20000c7c000-0xffffc20000c7f000   12288 e1000e_setup_tx_resources+0x29/0xbe pages=2 vmalloc
    0xffffc20000c80000-0xffffc20001481000 8392704 pci_mmcfg_arch_init+0x90/0x118 phys=e0000000 ioremap
    0xffffc20001481000-0xffffc20001682000 2101248 alloc_large_system_hash+0x127/0x246 pages=512 vmalloc
    0xffffc20001682000-0xffffc20001e83000 8392704 alloc_large_system_hash+0x127/0x246 pages=2048 vmalloc vpages
    0xffffc20001e83000-0xffffc20002204000 3674112 alloc_large_system_hash+0x127/0x246 pages=896 vmalloc vpages
    0xffffc20002204000-0xffffc2000220d000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc2000220d000-0xffffc20002216000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20002216000-0xffffc2000221f000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc2000221f000-0xffffc20002228000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20002228000-0xffffc20002231000   36864 _xfs_buf_map_pages+0x8e/0xc0 vmap
    0xffffc20002231000-0xffffc20002234000   12288 e1000e_setup_rx_resources+0x35/0x122 pages=2 vmalloc
    0xffffc20002240000-0xffffc20002261000  135168 e1000_probe+0x1c4/0xa32 phys=d8a60000 ioremap
    0xffffc20002261000-0xffffc2000270c000 4894720 sys_swapon+0x509/0xa15 pages=1194 vmalloc vpages
    0xffffffffa0000000-0xffffffffa0022000  139264 module_alloc+0x4f/0x55 pages=33 vmalloc
    0xffffffffa0022000-0xffffffffa0029000   28672 module_alloc+0x4f/0x55 pages=6 vmalloc
    0xffffffffa002b000-0xffffffffa0034000   36864 module_alloc+0x4f/0x55 pages=8 vmalloc
    0xffffffffa0034000-0xffffffffa003d000   36864 module_alloc+0x4f/0x55 pages=8 vmalloc
    0xffffffffa003d000-0xffffffffa0049000   49152 module_alloc+0x4f/0x55 pages=11 vmalloc
    0xffffffffa0049000-0xffffffffa0050000   28672 module_alloc+0x4f/0x55 pages=6 vmalloc
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 7f3adfda337a..364789aae9f3 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -31,6 +31,7 @@ struct vm_struct {
 	struct page		**pages;
 	unsigned int		nr_pages;
 	unsigned long		phys_addr;
+	void			*caller;
 };
 
 /*
@@ -66,6 +67,8 @@ static inline size_t get_vm_area_size(const struct vm_struct *area)
 }
 
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
+extern struct vm_struct *get_vm_area_caller(unsigned long size,
+					unsigned long flags, void *caller);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
 extern struct vm_struct *get_vm_area_node(unsigned long size,

commit a10aa579878fc6f9cd17455067380bbdf1d53c91
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Apr 28 02:12:40 2008 -0700

    vmalloc: show vmalloced areas via /proc/vmallocinfo
    
    Implement a new proc file that allows the display of the currently allocated
    vmalloc memory.
    
    It allows to see the users of vmalloc.  That is important if vmalloc space is
    scarce (i386 for example).
    
    And it's going to be important for the compound page fallback to vmalloc.
    Many of the current users can be switched to use compound pages with fallback.
     This means that the number of users of vmalloc is reduced and page tables no
    longer necessary to access the memory.  /proc/vmallocinfo allows to review how
    that reduction occurs.
    
    If memory becomes fragmented and larger order allocations are no longer
    possible then /proc/vmallocinfo allows to see which compound page allocations
    fell back to virtual compound pages.  That is important for new users of
    virtual compound pages.  Such as order 1 stack allocation etc that may
    fallback to virtual compound pages in the future.
    
    /proc/vmallocinfo permissions are made readable-only-by-root to avoid possible
    information leakage.
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: CONFIG_MMU=n build fix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index ce8e7da05807..7f3adfda337a 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -87,4 +87,6 @@ extern void free_vm_area(struct vm_struct *area);
 extern rwlock_t vmlist_lock;
 extern struct vm_struct *vmlist;
 
+extern const struct seq_operations vmalloc_op;
+
 #endif /* _LINUX_VMALLOC_H */

commit b3bdda02aa547a0753b4fdbc105e86ef9046b30b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Feb 4 22:28:32 2008 -0800

    vmalloc: add const to void* parameters
    
    Make vmalloc functions work the same way as kfree() and friends that
    take a const void * argument.
    
    [akpm@linux-foundation.org: fix consts, coding-style]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 89338b468d0d..ce8e7da05807 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -45,11 +45,11 @@ extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
 extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask,
 				pgprot_t prot);
-extern void vfree(void *addr);
+extern void vfree(const void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,
 			unsigned long flags, pgprot_t prot);
-extern void vunmap(void *addr);
+extern void vunmap(const void *addr);
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
@@ -71,7 +71,7 @@ extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 extern struct vm_struct *get_vm_area_node(unsigned long size,
 					  unsigned long flags, int node,
 					  gfp_t gfp_mask);
-extern struct vm_struct *remove_vm_area(void *addr);
+extern struct vm_struct *remove_vm_area(const void *addr);
 
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);

commit 9585116ba09f1d8c52d0a1346e20bb9d443e9c02
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Sat Jul 21 17:11:35 2007 +0200

    i386: fix iounmap's use of vm_struct's size field
    
    get_vm_area always returns an area with an adjacent guard page.  That guard
    page is included in vm_struct.size.  iounmap uses vm_struct.size to
    determine how much address space needs to have change_page_attr applied to
    it, which will BUG if applied to the guard page.
    
    This patch adds a helper function - get_vm_area_size() in linux/vmalloc.h -
    to return the actual size of a vm area, and uses it to make iounmap do the
    right thing.  There are probably other places which should be using
    get_vm_area_size().
    
    Thanks to Dave Young <hidave.darkstar@gmail.com> for debugging the
    problem.
    
    [ Andi, it wasn't clear to me whether x86_64 needs the same fix. ]
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Dave Young <hidave.darkstar@gmail.com>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index c2b10cae5da5..89338b468d0d 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -58,6 +58,13 @@ void vmalloc_sync_all(void);
 /*
  *	Lowlevel-APIs (not for driver use!)
  */
+
+static inline size_t get_vm_area_size(const struct vm_struct *area)
+{
+	/* return actual size without guard page */
+	return area->size - PAGE_SIZE;
+}
+
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);

commit 5f4352fbffd6c45123dbce9e195efd54df4e177e
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Tue Jul 17 18:37:04 2007 -0700

    Allocate and free vmalloc areas
    
    Allocate/release a chunk of vmalloc address space:
     alloc_vm_area reserves a chunk of address space, and makes sure all
     the pagetables are constructed for that address range - but no pages.
    
     free_vm_area releases the address space range.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Ian Pratt <ian.pratt@xensource.com>
    Signed-off-by: Christian Limpach <Christian.Limpach@cl.cam.ac.uk>
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>
    Cc: "Jan Beulich" <JBeulich@novell.com>
    Cc: "Andi Kleen" <ak@muc.de>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 132b260aef1e..c2b10cae5da5 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -70,6 +70,10 @@ extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
 extern void unmap_kernel_range(unsigned long addr, unsigned long size);
 
+/* Allocate/destroy a 'vmalloc' VM area. */
+extern struct vm_struct *alloc_vm_area(size_t size);
+extern void free_vm_area(struct vm_struct *area);
+
 /*
  *	Internals.  Dont't use..
  */

commit c19c03fc749147f565e807fa65f1729066800571
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jun 4 15:15:35 2007 +1000

    [POWERPC] unmap_vm_area becomes unmap_kernel_range for the public
    
    This makes unmap_vm_area static and a wrapper around a new
    exported unmap_kernel_range that takes an explicit range instead
    of a vm_area struct.
    
    This makes it more versatile for code that wants to play with kernel
    page tables outside of the standard vmalloc area.
    
    (One example is some rework of the PowerPC PCI IO space mapping
    code that depends on that patch and removes some code duplication
    and horrible abuse of forged struct vm_struct).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4b7ee83787c1..132b260aef1e 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -65,9 +65,10 @@ extern struct vm_struct *get_vm_area_node(unsigned long size,
 					  unsigned long flags, int node,
 					  gfp_t gfp_mask);
 extern struct vm_struct *remove_vm_area(void *addr);
+
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
-extern void unmap_vm_area(struct vm_struct *area);
+extern void unmap_kernel_range(unsigned long addr, unsigned long size);
 
 /*
  *	Internals.  Dont't use..

commit 1eeb66a1bb973534dc3d064920a5ca683823372e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 8 00:27:03 2007 -0700

    move die notifier handling to common code
    
    This patch moves the die notifier handling to common code.  Previous
    various architectures had exactly the same code for it.  Note that the new
    code is compiled unconditionally, this should be understood as an appel to
    the other architecture maintainer to implement support for it aswell (aka
    sprinkling a notify_die or two in the proper place)
    
    arm had a notifiy_die that did something totally different, I renamed it to
    arm_notify_die as part of the patch and made it static to the file it's
    declared and used at.  avr32 used to pass slightly less information through
    this interface and I brought it into line with the other architectures.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: fix vmalloc_sync_all bustage]
    [bryan.wu@analog.com: fix vmalloc_sync_all in nommu]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Bryan Wu <bryan.wu@analog.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 924e502905d4..4b7ee83787c1 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -53,6 +53,7 @@ extern void vunmap(void *addr);
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
+void vmalloc_sync_all(void);
  
 /*
  *	Lowlevel-APIs (not for driver use!)

commit 2b4ac44e7c7e16cf9411b81693ff3e604f332bf1
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Fri Nov 10 12:27:48 2006 -0800

    [PATCH] vmalloc: optimization, cleanup, bugfixes
    
    - reorder 'struct vm_struct' to speedup lookups on CPUS with small cache
      lines.  The fields 'next,addr,size' should be now in the same cache line,
      to speedup lookups.
    
    - One minor cleanup in __get_vm_area_node()
    
    - Bugfixes in vmalloc_user() and vmalloc_32_user() NULL returns from
      __vmalloc() and __find_vm_area() were not tested.
    
    [akpm@osdl.org: remove redundant BUG_ONs]
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index dc9a29d84abc..924e502905d4 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -23,13 +23,14 @@ struct vm_area_struct;
 #endif
 
 struct vm_struct {
+	/* keep next,addr,size together to speedup lookups */
+	struct vm_struct	*next;
 	void			*addr;
 	unsigned long		size;
 	unsigned long		flags;
 	struct page		**pages;
 	unsigned int		nr_pages;
 	unsigned long		phys_addr;
-	struct vm_struct	*next;
 };
 
 /*

commit 52fd24ca1db3a741f144bbc229beefe044202cac
Author: Giridhar Pemmasani <pgiri@yahoo.com>
Date:   Sat Oct 28 10:38:34 2006 -0700

    [PATCH] __vmalloc with GFP_ATOMIC causes 'sleeping from invalid context'
    
    If __vmalloc is called to allocate memory with GFP_ATOMIC in atomic
    context, the chain of calls results in __get_vm_area_node allocating memory
    for vm_struct with GFP_KERNEL, causing the 'sleeping from invalid context'
    warning.  This patch fixes it by passing the gfp flags along so
    __get_vm_area_node allocates memory for vm_struct with the same flags.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index ce5f1482e6be..dc9a29d84abc 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -60,7 +60,8 @@ extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
 extern struct vm_struct *get_vm_area_node(unsigned long size,
-					unsigned long flags, int node);
+					  unsigned long flags, int node,
+					  gfp_t gfp_mask);
 extern struct vm_struct *remove_vm_area(void *addr);
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);

commit d24afc57d51b1be41f95521e81399061fa5875a6
Author: Rolf Eike Beer <eike-kernel@sf-tec.de>
Date:   Wed Sep 27 01:50:13 2006 -0700

    [PATCH] Mark __remove_vm_area() static
    
    The function is exported but not used from anywhere else.  It's also marked as
    "not for driver use" so noone out there should really care.
    
    Signed-off-by: Rolf Eike Beer <eike-kernel@sf-tec.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index dee88c6b6fa7..ce5f1482e6be 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -62,7 +62,6 @@ extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 extern struct vm_struct *get_vm_area_node(unsigned long size,
 					unsigned long flags, int node);
 extern struct vm_struct *remove_vm_area(void *addr);
-extern struct vm_struct *__remove_vm_area(void *addr);
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
 extern void unmap_vm_area(struct vm_struct *area);

commit b221385bc41d6789edde3d2fa0cb20d5045730eb
Author: Adrian Bunk <bunk@stusta.de>
Date:   Mon Sep 25 23:31:02 2006 -0700

    [PATCH] mm/: make functions static
    
    This patch makes the following needlessly global functions static:
     - slab.c: kmem_find_general_cachep()
     - swap.c: __page_cache_release()
     - vmalloc.c: __vmalloc_node()
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 71b6363caaaf..dee88c6b6fa7 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -44,8 +44,6 @@ extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
 extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask,
 				pgprot_t prot);
-extern void *__vmalloc_node(unsigned long size, gfp_t gfp_mask,
-				pgprot_t prot, int node);
 extern void vfree(void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,

commit 8757d5fa6b75e8ea906baf0309d49b980e7f9bc9
Author: Jan Kiszka <jan.kiszka@web.de>
Date:   Fri Jul 14 00:23:56 2006 -0700

    [PATCH] mm: fix oom roll-back of __vmalloc_area_node
    
    __vunmap must not rely on area->nr_pages when picking the release methode
    for area->pages.  It may be too small when __vmalloc_area_node failed early
    due to lacking memory.  Instead, use a flag in vmstruct to differentiate.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@web.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index f6024ab4eff0..71b6363caaaf 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -11,6 +11,7 @@ struct vm_area_struct;
 #define VM_ALLOC	0x00000002	/* vmalloc() */
 #define VM_MAP		0x00000004	/* vmap()ed pages */
 #define VM_USERMAP	0x00000008	/* suitable for remap_vmalloc_range */
+#define VM_VPAGES	0x00000010	/* buffer for pages was vmalloc'ed */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*

commit 833423143c3a7c6545e409d65febd0d92deb351b
Author: Nick Piggin <npiggin@suse.de>
Date:   Fri Jun 23 02:03:20 2006 -0700

    [PATCH] mm: introduce remap_vmalloc_range()
    
    Add remap_vmalloc_range, vmalloc_user, and vmalloc_32_user so that drivers
    can have a nice interface for remapping vmalloc memory.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 1d5577b2b752..f6024ab4eff0 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -4,10 +4,13 @@
 #include <linux/spinlock.h>
 #include <asm/page.h>		/* pgprot_t */
 
+struct vm_area_struct;
+
 /* bits in vm_struct->flags */
 #define VM_IOREMAP	0x00000001	/* ioremap() and friends */
 #define VM_ALLOC	0x00000002	/* vmalloc() */
 #define VM_MAP		0x00000004	/* vmap()ed pages */
+#define VM_USERMAP	0x00000008	/* suitable for remap_vmalloc_range */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*
@@ -32,9 +35,11 @@ struct vm_struct {
  *	Highlevel APIs for driver use
  */
 extern void *vmalloc(unsigned long size);
+extern void *vmalloc_user(unsigned long size);
 extern void *vmalloc_node(unsigned long size, int node);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
+extern void *vmalloc_32_user(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
 extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask,
 				pgprot_t prot);
@@ -45,6 +50,9 @@ extern void vfree(void *addr);
 extern void *vmap(struct page **pages, unsigned int count,
 			unsigned long flags, pgprot_t prot);
 extern void vunmap(void *addr);
+
+extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
+							unsigned long pgoff);
  
 /*
  *	Lowlevel-APIs (not for driver use!)

commit 930fc45a49ddebe7555cc5c837d82b9c27e65ff4
Author: Christoph Lameter <clameter@engr.sgi.com>
Date:   Sat Oct 29 18:15:41 2005 -0700

    [PATCH] vmalloc_node
    
    This patch adds
    
    vmalloc_node(size, node)        -> Allocate necessary memory on the specified node
    
    and
    
    get_vm_area_node(size, flags, node)
    
    and the other functions that it depends on.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 3701a0673d2c..1d5577b2b752 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -32,10 +32,14 @@ struct vm_struct {
  *	Highlevel APIs for driver use
  */
 extern void *vmalloc(unsigned long size);
+extern void *vmalloc_node(unsigned long size, int node);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
 extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
-extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask, pgprot_t prot);
+extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask,
+				pgprot_t prot);
+extern void *__vmalloc_node(unsigned long size, gfp_t gfp_mask,
+				pgprot_t prot, int node);
 extern void vfree(void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,
@@ -48,6 +52,8 @@ extern void vunmap(void *addr);
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
+extern struct vm_struct *get_vm_area_node(unsigned long size,
+					unsigned long flags, int node);
 extern struct vm_struct *remove_vm_area(void *addr);
 extern struct vm_struct *__remove_vm_area(void *addr);
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,

commit dd0fc66fb33cd610bc1a5db8a5e232d34879b4d7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Oct 7 07:46:04 2005 +0100

    [PATCH] gfp flags annotations - part 1
    
     - added typedef unsigned int __nocast gfp_t;
    
     - replaced __nocast uses for gfp flags with gfp_t - it gives exactly
       the same warnings as far as sparse is concerned, doesn't change
       generated code (from gcc point of view we replaced unsigned int with
       typedef) and documents what's going on far better.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index b244f69ef682..3701a0673d2c 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -34,8 +34,8 @@ struct vm_struct {
 extern void *vmalloc(unsigned long size);
 extern void *vmalloc_exec(unsigned long size);
 extern void *vmalloc_32(unsigned long size);
-extern void *__vmalloc(unsigned long size, unsigned int __nocast gfp_mask, pgprot_t prot);
-extern void *__vmalloc_area(struct vm_struct *area, unsigned int __nocast gfp_mask, pgprot_t prot);
+extern void *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot);
+extern void *__vmalloc_area(struct vm_struct *area, gfp_t gfp_mask, pgprot_t prot);
 extern void vfree(void *addr);
 
 extern void *vmap(struct page **pages, unsigned int count,

commit fd195c49fb17a21e232f50bddb2267150053cf34
Author: Deepak Saxena <dsaxena@plexity.net>
Date:   Sat Sep 3 15:54:58 2005 -0700

    [PATCH] arm: allow for arch-specific IOREMAP_MAX_ORDER
    
    Version 6 of the ARM architecture introduces the concept of 16MB pages
    (supersections) and 36-bit (40-bit actually, but nobody uses this) physical
    addresses.  36-bit addressed memory and I/O and ARMv6 can only be mapped
    using supersections and the requirement on these is that both virtual and
    physical addresses be 16MB aligned.  In trying to add support for ioremap()
    of 36-bit I/O, we run into the issue that get_vm_area() allows for a
    maximum of 512K alignment via the IOREMAP_MAX_ORDER constant.  To work
    around this, we can:
    
    - Allocate a larger VM area than needed (size + (1ul << IOREMAP_MAX_ORDER))
      and then align the pointer ourselves, but this ends up with 512K of
      wasted VM per ioremap().
    
    - Provide a new __get_vm_area_aligned() API and make __get_vm_area() sit
      on top of this. I did this and it works but I don't like the idea
      adding another VM API just for this one case.
    
    - My preferred solution which is to allow the architecture to override
      the IOREMAP_MAX_ORDER constant with it's own version.
    
    Signed-off-by: Deepak Saxena <dsaxena@plexity.net>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 6409d9cf5965..b244f69ef682 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -10,6 +10,14 @@
 #define VM_MAP		0x00000004	/* vmap()ed pages */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
+/*
+ * Maximum alignment for ioremap() regions.
+ * Can be overriden by arch-specific value.
+ */
+#ifndef IOREMAP_MAX_ORDER
+#define IOREMAP_MAX_ORDER	(7 + PAGE_SHIFT)	/* 128 pages */
+#endif
+
 struct vm_struct {
 	void			*addr;
 	unsigned long		size;

commit 7856dfeb23c16ef3d8dac8871b4d5b93c70b59b9
Author: Andi Kleen <ak@suse.de>
Date:   Fri May 20 14:27:57 2005 -0700

    [PATCH] x86_64: Fixed guard page handling again in iounmap
    
    Caused oopses again.  Also fix potential mismatch in checking if
    change_page_attr was needed.
    
    To do it without races I needed to change mm/vmalloc.c to export a
    __remove_vm_area that does not take vmlist lock.
    
    Noticed by Terence Ripperda and based on a patch of his.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 3a358c895188..6409d9cf5965 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -41,6 +41,7 @@ extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
 extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
 					unsigned long start, unsigned long end);
 extern struct vm_struct *remove_vm_area(void *addr);
+extern struct vm_struct *__remove_vm_area(void *addr);
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
 extern void unmap_vm_area(struct vm_struct *area);

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
new file mode 100644
index 000000000000..3a358c895188
--- /dev/null
+++ b/include/linux/vmalloc.h
@@ -0,0 +1,54 @@
+#ifndef _LINUX_VMALLOC_H
+#define _LINUX_VMALLOC_H
+
+#include <linux/spinlock.h>
+#include <asm/page.h>		/* pgprot_t */
+
+/* bits in vm_struct->flags */
+#define VM_IOREMAP	0x00000001	/* ioremap() and friends */
+#define VM_ALLOC	0x00000002	/* vmalloc() */
+#define VM_MAP		0x00000004	/* vmap()ed pages */
+/* bits [20..32] reserved for arch specific ioremap internals */
+
+struct vm_struct {
+	void			*addr;
+	unsigned long		size;
+	unsigned long		flags;
+	struct page		**pages;
+	unsigned int		nr_pages;
+	unsigned long		phys_addr;
+	struct vm_struct	*next;
+};
+
+/*
+ *	Highlevel APIs for driver use
+ */
+extern void *vmalloc(unsigned long size);
+extern void *vmalloc_exec(unsigned long size);
+extern void *vmalloc_32(unsigned long size);
+extern void *__vmalloc(unsigned long size, unsigned int __nocast gfp_mask, pgprot_t prot);
+extern void *__vmalloc_area(struct vm_struct *area, unsigned int __nocast gfp_mask, pgprot_t prot);
+extern void vfree(void *addr);
+
+extern void *vmap(struct page **pages, unsigned int count,
+			unsigned long flags, pgprot_t prot);
+extern void vunmap(void *addr);
+ 
+/*
+ *	Lowlevel-APIs (not for driver use!)
+ */
+extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
+extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
+					unsigned long start, unsigned long end);
+extern struct vm_struct *remove_vm_area(void *addr);
+extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
+			struct page ***pages);
+extern void unmap_vm_area(struct vm_struct *area);
+
+/*
+ *	Internals.  Dont't use..
+ */
+extern rwlock_t vmlist_lock;
+extern struct vm_struct *vmlist;
+
+#endif /* _LINUX_VMALLOC_H */
