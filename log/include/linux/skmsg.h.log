commit bb0de3131f4c60a9bf976681e0fe4d1e55c7a821
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Jun 29 10:56:28 2020 +0100

    bpf: sockmap: Require attach_bpf_fd when detaching a program
    
    The sockmap code currently ignores the value of attach_bpf_fd when
    detaching a program. This is contrary to the usual behaviour of
    checking that attach_bpf_fd represents the currently attached
    program.
    
    Ensure that attach_bpf_fd is indeed the currently attached
    program. It turns out that all sockmap selftests already do this,
    which indicates that this is unlikely to cause breakage.
    
    Fixes: 604326b41a6f ("bpf, sockmap: convert to generic sk_msg interface")
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200629095630.7933-5-lmb@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 08674cd14d5a..1e9ed840b9fc 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -430,6 +430,19 @@ static inline void psock_set_prog(struct bpf_prog **pprog,
 		bpf_prog_put(prog);
 }
 
+static inline int psock_replace_prog(struct bpf_prog **pprog,
+				     struct bpf_prog *prog,
+				     struct bpf_prog *old)
+{
+	if (cmpxchg(pprog, old, prog) != old)
+		return -ENOENT;
+
+	if (old)
+		bpf_prog_put(old);
+
+	return 0;
+}
+
 static inline void psock_progs_drop(struct sk_psock_progs *progs)
 {
 	psock_set_prog(&progs->msg_parser, NULL);

commit e91de6afa81c10e9f855c5695eb9a53168d96b73
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri May 29 16:06:59 2020 -0700

    bpf: Fix running sk_skb program types with ktls
    
    KTLS uses a stream parser to collect TLS messages and send them to
    the upper layer tls receive handler. This ensures the tls receiver
    has a full TLS header to parse when it is run. However, when a
    socket has BPF_SK_SKB_STREAM_VERDICT program attached before KTLS
    is enabled we end up with two stream parsers running on the same
    socket.
    
    The result is both try to run on the same socket. First the KTLS
    stream parser runs and calls read_sock() which will tcp_read_sock
    which in turn calls tcp_rcv_skb(). This dequeues the skb from the
    sk_receive_queue. When this is done KTLS code then data_ready()
    callback which because we stacked KTLS on top of the bpf stream
    verdict program has been replaced with sk_psock_start_strp(). This
    will in turn kick the stream parser again and eventually do the
    same thing KTLS did above calling into tcp_rcv_skb() and dequeuing
    a skb from the sk_receive_queue.
    
    At this point the data stream is broke. Part of the stream was
    handled by the KTLS side some other bytes may have been handled
    by the BPF side. Generally this results in either missing data
    or more likely a "Bad Message" complaint from the kTLS receive
    handler as the BPF program steals some bytes meant to be in a
    TLS header and/or the TLS header length is no longer correct.
    
    We've already broke the idealized model where we can stack ULPs
    in any order with generic callbacks on the TX side to handle this.
    So in this patch we do the same thing but for RX side. We add
    a sk_psock_strp_enabled() helper so TLS can learn a BPF verdict
    program is running and add a tls_sw_has_ctx_rx() helper so BPF
    side can learn there is a TLS ULP on the socket.
    
    Then on BPF side we omit calling our stream parser to avoid
    breaking the data stream for the KTLS receiver. Then on the
    KTLS side we call BPF_SK_SKB_STREAM_VERDICT once the KTLS
    receiver is done with the packet but before it posts the
    msg to userspace. This gives us symmetry between the TX and
    RX halfs and IMO makes it usable again. On the TX side we
    process packets in this order BPF -> TLS -> TCP and on
    the receive side in the reverse order TCP -> TLS -> BPF.
    
    Discovered while testing OpenSSL 3.0 Alpha2.0 release.
    
    Fixes: d829e9c4112b5 ("tls: convert to generic sk_msg interface")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/159079361946.5745.605854335665044485.stgit@john-Precision-5820-Tower
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index ad31c9fb7158..08674cd14d5a 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -437,4 +437,12 @@ static inline void psock_progs_drop(struct sk_psock_progs *progs)
 	psock_set_prog(&progs->skb_verdict, NULL);
 }
 
+int sk_psock_tls_strp_read(struct sk_psock *psock, struct sk_buff *skb);
+
+static inline bool sk_psock_strp_enabled(struct sk_psock *psock)
+{
+	if (!psock)
+		return false;
+	return psock->parser.enabled;
+}
 #endif /* _LINUX_SKMSG_H */

commit 81aabbb9fb7b4b1efd073b62f0505d3adad442f3
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon May 4 10:21:44 2020 -0700

    bpf, sockmap: bpf_tcp_ingress needs to subtract bytes from sg.size
    
    In bpf_tcp_ingress we used apply_bytes to subtract bytes from sg.size
    which is used to track total bytes in a message. But this is not
    correct because apply_bytes is itself modified in the main loop doing
    the mem_charge.
    
    Then at the end of this we have sg.size incorrectly set and out of
    sync with actual sk values. Then we can get a splat if we try to
    cork the data later and again try to redirect the msg to ingress. To
    fix instead of trying to track msg.size do the easy thing and include
    it as part of the sk_msg_xfer logic so that when the msg is moved the
    sg.size is always correct.
    
    To reproduce the below users will need ingress + cork and hit an
    error path that will then try to 'free' the skmsg.
    
    [  173.699981] BUG: KASAN: null-ptr-deref in sk_msg_free_elem+0xdd/0x120
    [  173.699987] Read of size 8 at addr 0000000000000008 by task test_sockmap/5317
    
    [  173.700000] CPU: 2 PID: 5317 Comm: test_sockmap Tainted: G          I       5.7.0-rc1+ #43
    [  173.700005] Hardware name: Dell Inc. Precision 5820 Tower/002KVM, BIOS 1.9.2 01/24/2019
    [  173.700009] Call Trace:
    [  173.700021]  dump_stack+0x8e/0xcb
    [  173.700029]  ? sk_msg_free_elem+0xdd/0x120
    [  173.700034]  ? sk_msg_free_elem+0xdd/0x120
    [  173.700042]  __kasan_report+0x102/0x15f
    [  173.700052]  ? sk_msg_free_elem+0xdd/0x120
    [  173.700060]  kasan_report+0x32/0x50
    [  173.700070]  sk_msg_free_elem+0xdd/0x120
    [  173.700080]  __sk_msg_free+0x87/0x150
    [  173.700094]  tcp_bpf_send_verdict+0x179/0x4f0
    [  173.700109]  tcp_bpf_sendpage+0x3ce/0x5d0
    
    Fixes: 604326b41a6fb ("bpf, sockmap: convert to generic sk_msg interface")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/158861290407.14306.5327773422227552482.stgit@john-Precision-5820-Tower

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 8a709f63c5e5..ad31c9fb7158 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -187,6 +187,7 @@ static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
 	dst->sg.data[which] = src->sg.data[which];
 	dst->sg.data[which].length  = size;
 	dst->sg.size		   += size;
+	src->sg.size		   -= size;
 	src->sg.data[which].length -= size;
 	src->sg.data[which].offset += size;
 }

commit f747632b608f90217a4e9ebb1deba8a37612aa32
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:36 2020 +0000

    bpf: sockmap: Move generic sockmap hooks from BPF TCP
    
    The init, close and unhash handlers from TCP sockmap are generic,
    and can be reused by UDP sockmap. Move the helpers into the sockmap code
    base and expose them. This requires tcp_bpf_get_proto and tcp_bpf_clone to
    be conditional on BPF_STREAM_PARSER.
    
    The moved functions are unmodified, except that sk_psock_unlink is
    renamed to sock_map_unlink to better match its behaviour.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-6-lmb@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 2be51b7a5800..8a709f63c5e5 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -323,14 +323,6 @@ static inline void sk_psock_free_link(struct sk_psock_link *link)
 }
 
 struct sk_psock_link *sk_psock_link_pop(struct sk_psock *psock);
-#if defined(CONFIG_BPF_STREAM_PARSER)
-void sk_psock_unlink(struct sock *sk, struct sk_psock_link *link);
-#else
-static inline void sk_psock_unlink(struct sock *sk,
-				   struct sk_psock_link *link)
-{
-}
-#endif
 
 void __sk_psock_purge_ingress_msg(struct sk_psock *psock);
 
@@ -399,26 +391,6 @@ static inline bool sk_psock_test_state(const struct sk_psock *psock,
 	return test_bit(bit, &psock->state);
 }
 
-static inline struct sk_psock *sk_psock_get_checked(struct sock *sk)
-{
-	struct sk_psock *psock;
-
-	rcu_read_lock();
-	psock = sk_psock(sk);
-	if (psock) {
-		if (sk->sk_prot->recvmsg != tcp_bpf_recvmsg) {
-			psock = ERR_PTR(-EBUSY);
-			goto out;
-		}
-
-		if (!refcount_inc_not_zero(&psock->refcnt))
-			psock = ERR_PTR(-EBUSY);
-	}
-out:
-	rcu_read_unlock();
-	return psock;
-}
-
 static inline struct sk_psock *sk_psock_get(struct sock *sk)
 {
 	struct sk_psock *psock;

commit 1a2e20132db7bb76dd4f97b8364bd167227dd15f
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:33 2020 +0000

    skmsg: Update saved hooks only once
    
    Only update psock->saved_* if psock->sk_proto has not been initialized
    yet. This allows us to get rid of tcp_bpf_reinit_sk_prot.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-3-lmb@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 4d3d75d63066..2be51b7a5800 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -347,11 +347,23 @@ static inline void sk_psock_update_proto(struct sock *sk,
 					 struct sk_psock *psock,
 					 struct proto *ops)
 {
-	psock->saved_unhash = sk->sk_prot->unhash;
-	psock->saved_close = sk->sk_prot->close;
-	psock->saved_write_space = sk->sk_write_space;
+	/* Initialize saved callbacks and original proto only once, since this
+	 * function may be called multiple times for a psock, e.g. when
+	 * psock->progs.msg_parser is updated.
+	 *
+	 * Since we've not installed the new proto, psock is not yet in use and
+	 * we can initialize it without synchronization.
+	 */
+	if (!psock->sk_proto) {
+		struct proto *orig = READ_ONCE(sk->sk_prot);
+
+		psock->saved_unhash = orig->unhash;
+		psock->saved_close = orig->close;
+		psock->saved_write_space = sk->sk_write_space;
+
+		psock->sk_proto = orig;
+	}
 
-	psock->sk_proto = sk->sk_prot;
 	/* Pairs with lockless read in sk_clone_lock() */
 	WRITE_ONCE(sk->sk_prot, ops);
 }

commit 7b70973d7edb2f005511102d5a2e0116464a46a1
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:32 2020 +0000

    bpf: sockmap: Only check ULP for TCP sockets
    
    The sock map code checks that a socket does not have an active upper
    layer protocol before inserting it into the map. This requires casting
    via inet_csk, which isn't valid for UDP sockets.
    
    Guard checks for ULP by checking inet_sk(sk)->is_icsk first.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-2-lmb@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 112765bd146d..4d3d75d63066 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -360,7 +360,13 @@ static inline void sk_psock_restore_proto(struct sock *sk,
 					  struct sk_psock *psock)
 {
 	sk->sk_prot->unhash = psock->saved_unhash;
-	tcp_update_ulp(sk, psock->sk_proto, psock->saved_write_space);
+	if (inet_csk_has_ulp(sk)) {
+		tcp_update_ulp(sk, psock->sk_proto, psock->saved_write_space);
+	} else {
+		sk->sk_write_space = psock->saved_write_space;
+		/* Pairs with lockless read in sk_clone_lock() */
+		WRITE_ONCE(sk->sk_prot, psock->sk_proto);
+	}
 }
 
 static inline void sk_psock_set_state(struct sk_psock *psock,

commit b8e202d1d1d0f182f01062804efb523ea9a9008c
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Tue Feb 18 17:10:13 2020 +0000

    net, sk_msg: Annotate lockless access to sk_prot on clone
    
    sk_msg and ULP frameworks override protocol callbacks pointer in
    sk->sk_prot, while tcp accesses it locklessly when cloning the listening
    socket, that is with neither sk_lock nor sk_callback_lock held.
    
    Once we enable use of listening sockets with sockmap (and hence sk_msg),
    there will be shared access to sk->sk_prot if socket is getting cloned
    while being inserted/deleted to/from the sockmap from another CPU:
    
    Read side:
    
    tcp_v4_rcv
      sk = __inet_lookup_skb(...)
      tcp_check_req(sk)
        inet_csk(sk)->icsk_af_ops->syn_recv_sock
          tcp_v4_syn_recv_sock
            tcp_create_openreq_child
              inet_csk_clone_lock
                sk_clone_lock
                  READ_ONCE(sk->sk_prot)
    
    Write side:
    
    sock_map_ops->map_update_elem
      sock_map_update_elem
        sock_map_update_common
          sock_map_link_no_progs
            tcp_bpf_init
              tcp_bpf_update_sk_prot
                sk_psock_update_proto
                  WRITE_ONCE(sk->sk_prot, ops)
    
    sock_map_ops->map_delete_elem
      sock_map_delete_elem
        __sock_map_delete
         sock_map_unref
           sk_psock_put
             sk_psock_drop
               sk_psock_restore_proto
                 tcp_update_ulp
                   WRITE_ONCE(sk->sk_prot, proto)
    
    Mark the shared access with READ_ONCE/WRITE_ONCE annotations.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200218171023.844439-2-jakub@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index d90ef61712a1..112765bd146d 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -352,7 +352,8 @@ static inline void sk_psock_update_proto(struct sock *sk,
 	psock->saved_write_space = sk->sk_write_space;
 
 	psock->sk_proto = sk->sk_prot;
-	sk->sk_prot = ops;
+	/* Pairs with lockless read in sk_clone_lock() */
+	WRITE_ONCE(sk->sk_prot, ops);
 }
 
 static inline void sk_psock_restore_proto(struct sock *sk,

commit a178b4585865a4c756c41bc5376f63416b7d9271
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Mon Feb 17 12:15:29 2020 +0000

    bpf, sk_msg: Don't clear saved sock proto on restore
    
    There is no need to clear psock->sk_proto when restoring socket protocol
    callbacks in sk->sk_prot. The psock is about to get detached from the sock
    and eventually destroyed. At worst we will restore the protocol callbacks
    and the write callback twice.
    
    This makes reasoning about psock state easier. Once psock is initialized,
    we can count on psock->sk_proto always being set.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200217121530.754315-3-jakub@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 8605947d6c08..d90ef61712a1 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -359,13 +359,7 @@ static inline void sk_psock_restore_proto(struct sock *sk,
 					  struct sk_psock *psock)
 {
 	sk->sk_prot->unhash = psock->saved_unhash;
-
-	if (psock->sk_proto) {
-		tcp_update_ulp(sk, psock->sk_proto, psock->saved_write_space);
-		psock->sk_proto = NULL;
-	} else {
-		sk->sk_write_space = psock->saved_write_space;
-	}
+	tcp_update_ulp(sk, psock->sk_proto, psock->saved_write_space);
 }
 
 static inline void sk_psock_set_state(struct sk_psock *psock,

commit a4393861a351f66fef1102e775743c86a276afce
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Mon Feb 17 12:15:28 2020 +0000

    bpf, sk_msg: Let ULP restore sk_proto and write_space callback
    
    We don't need a fallback for when the socket is not using ULP.
    tcp_update_ulp handles this case exactly the same as we do in
    sk_psock_restore_proto. Get rid of the duplicated code.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200217121530.754315-2-jakub@cloudflare.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 14d61bba0b79..8605947d6c08 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -361,16 +361,7 @@ static inline void sk_psock_restore_proto(struct sock *sk,
 	sk->sk_prot->unhash = psock->saved_unhash;
 
 	if (psock->sk_proto) {
-		struct inet_connection_sock *icsk = inet_csk(sk);
-		bool has_ulp = !!icsk->icsk_ulp_data;
-
-		if (has_ulp) {
-			tcp_update_ulp(sk, psock->sk_proto,
-				       psock->saved_write_space);
-		} else {
-			sk->sk_prot = psock->sk_proto;
-			sk->sk_write_space = psock->saved_write_space;
-		}
+		tcp_update_ulp(sk, psock->sk_proto, psock->saved_write_space);
 		psock->sk_proto = NULL;
 	} else {
 		sk->sk_write_space = psock->saved_write_space;

commit 33bfe20dd7117dd81fd896a53f743a233e1ad64f
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Jan 11 06:12:01 2020 +0000

    bpf: Sockmap/tls, push write_space updates through ulp updates
    
    When sockmap sock with TLS enabled is removed we cleanup bpf/psock state
    and call tcp_update_ulp() to push updates to TLS ULP on top. However, we
    don't push the write_space callback up and instead simply overwrite the
    op with the psock stored previous op. This may or may not be correct so
    to ensure we don't overwrite the TLS write space hook pass this field to
    the ULP and have it fixup the ctx.
    
    This completes a previous fix that pushed the ops through to the ULP
    but at the time missed doing this for write_space, presumably because
    write_space TLS hook was added around the same time.
    
    Fixes: 95fa145479fbc ("bpf: sockmap/tls, close can race with map free")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/bpf/20200111061206.8028-4-john.fastabend@gmail.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index b6afe01f8592..14d61bba0b79 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -359,17 +359,21 @@ static inline void sk_psock_restore_proto(struct sock *sk,
 					  struct sk_psock *psock)
 {
 	sk->sk_prot->unhash = psock->saved_unhash;
-	sk->sk_write_space = psock->saved_write_space;
 
 	if (psock->sk_proto) {
 		struct inet_connection_sock *icsk = inet_csk(sk);
 		bool has_ulp = !!icsk->icsk_ulp_data;
 
-		if (has_ulp)
-			tcp_update_ulp(sk, psock->sk_proto);
-		else
+		if (has_ulp) {
+			tcp_update_ulp(sk, psock->sk_proto,
+				       psock->saved_write_space);
+		} else {
 			sk->sk_prot = psock->sk_proto;
+			sk->sk_write_space = psock->saved_write_space;
+		}
 		psock->sk_proto = NULL;
+	} else {
+		sk->sk_write_space = psock->saved_write_space;
 	}
 }
 

commit 4da6a196f93b1af7612340e8c1ad8ce71e18f955
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Jan 11 06:11:59 2020 +0000

    bpf: Sockmap/tls, during free we may call tcp_bpf_unhash() in loop
    
    When a sockmap is free'd and a socket in the map is enabled with tls
    we tear down the bpf context on the socket, the psock struct and state,
    and then call tcp_update_ulp(). The tcp_update_ulp() call is to inform
    the tls stack it needs to update its saved sock ops so that when the tls
    socket is later destroyed it doesn't try to call the now destroyed psock
    hooks.
    
    This is about keeping stacked ULPs in good shape so they always have
    the right set of stacked ops.
    
    However, recently unhash() hook was removed from TLS side. But, the
    sockmap/bpf side is not doing any extra work to update the unhash op
    when is torn down instead expecting TLS side to manage it. So both
    TLS and sockmap believe the other side is managing the op and instead
    no one updates the hook so it continues to point at tcp_bpf_unhash().
    When unhash hook is called we call tcp_bpf_unhash() which detects the
    psock has already been destroyed and calls sk->sk_prot_unhash() which
    calls tcp_bpf_unhash() yet again and so on looping and hanging the core.
    
    To fix have sockmap tear down logic fixup the stale pointer.
    
    Fixes: 5d92e631b8be ("net/tls: partially revert fix transition through disconnect with close")
    Reported-by: syzbot+83979935eb6304f8cd46@syzkaller.appspotmail.com
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/bpf/20200111061206.8028-2-john.fastabend@gmail.com

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index ef7031f8a304..b6afe01f8592 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -358,6 +358,7 @@ static inline void sk_psock_update_proto(struct sock *sk,
 static inline void sk_psock_restore_proto(struct sock *sk,
 					  struct sk_psock *psock)
 {
+	sk->sk_prot->unhash = psock->saved_unhash;
 	sk->sk_write_space = psock->saved_write_space;
 
 	if (psock->sk_proto) {

commit 031097d9e079e40dce401031d1012e83d80eaf01
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Nov 27 12:16:41 2019 -0800

    net: skmsg: fix TLS 1.3 crash with full sk_msg
    
    TLS 1.3 started using the entry at the end of the SG array
    for chaining-in the single byte content type entry. This mostly
    works:
    
    [ E E E E E E . . ]
      ^           ^
       start       end
    
                     E < content type
                   /
    [ E E E E E E C . ]
      ^           ^
       start       end
    
    (Where E denotes a populated SG entry; C denotes a chaining entry.)
    
    If the array is full, however, the end will point to the start:
    
    [ E E E E E E E E ]
      ^
       start
       end
    
    And we end up overwriting the start:
    
        E < content type
       /
    [ C E E E E E E E ]
      ^
       start
       end
    
    The sg array is supposed to be a circular buffer with start and
    end markers pointing anywhere. In case where start > end
    (i.e. the circular buffer has "wrapped") there is an extra entry
    reserved at the end to chain the two halves together.
    
    [ E E E E E E . . l ]
    
    (Where l is the reserved entry for "looping" back to front.
    
    As suggested by John, let's reserve another entry for chaining
    SG entries after the main circular buffer. Note that this entry
    has to be pointed to by the end entry so its position is not fixed.
    
    Examples of full messages:
    
    [ E E E E E E E E . l ]
      ^               ^
       start           end
    
       <---------------.
    [ E E . E E E E E E l ]
          ^ ^
       end   start
    
    Now the end will always point to an unused entry, so TLS 1.3
    can always use it.
    
    Fixes: 130b392c6cd6 ("net: tls: Add tls 1.3 support")
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 6cb077b646a5..ef7031f8a304 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -14,6 +14,7 @@
 #include <net/strparser.h>
 
 #define MAX_MSG_FRAGS			MAX_SKB_FRAGS
+#define NR_MSG_FRAG_IDS			(MAX_MSG_FRAGS + 1)
 
 enum __sk_action {
 	__SK_DROP = 0,
@@ -29,13 +30,15 @@ struct sk_msg_sg {
 	u32				size;
 	u32				copybreak;
 	unsigned long			copy;
-	/* The extra element is used for chaining the front and sections when
-	 * the list becomes partitioned (e.g. end < start). The crypto APIs
-	 * require the chaining.
+	/* The extra two elements:
+	 * 1) used for chaining the front and sections when the list becomes
+	 *    partitioned (e.g. end < start). The crypto APIs require the
+	 *    chaining;
+	 * 2) to chain tailer SG entries after the message.
 	 */
-	struct scatterlist		data[MAX_MSG_FRAGS + 1];
+	struct scatterlist		data[MAX_MSG_FRAGS + 2];
 };
-static_assert(BITS_PER_LONG >= MAX_MSG_FRAGS);
+static_assert(BITS_PER_LONG >= NR_MSG_FRAG_IDS);
 
 /* UAPI in filter.c depends on struct sk_msg_sg being first element. */
 struct sk_msg {
@@ -142,13 +145,13 @@ static inline void sk_msg_apply_bytes(struct sk_psock *psock, u32 bytes)
 
 static inline u32 sk_msg_iter_dist(u32 start, u32 end)
 {
-	return end >= start ? end - start : end + (MAX_MSG_FRAGS - start);
+	return end >= start ? end - start : end + (NR_MSG_FRAG_IDS - start);
 }
 
 #define sk_msg_iter_var_prev(var)			\
 	do {						\
 		if (var == 0)				\
-			var = MAX_MSG_FRAGS - 1;	\
+			var = NR_MSG_FRAG_IDS - 1;	\
 		else					\
 			var--;				\
 	} while (0)
@@ -156,7 +159,7 @@ static inline u32 sk_msg_iter_dist(u32 start, u32 end)
 #define sk_msg_iter_var_next(var)			\
 	do {						\
 		var++;					\
-		if (var == MAX_MSG_FRAGS)		\
+		if (var == NR_MSG_FRAG_IDS)		\
 			var = 0;			\
 	} while (0)
 
@@ -173,9 +176,9 @@ static inline void sk_msg_clear_meta(struct sk_msg *msg)
 
 static inline void sk_msg_init(struct sk_msg *msg)
 {
-	BUILD_BUG_ON(ARRAY_SIZE(msg->sg.data) - 1 != MAX_MSG_FRAGS);
+	BUILD_BUG_ON(ARRAY_SIZE(msg->sg.data) - 1 != NR_MSG_FRAG_IDS);
 	memset(msg, 0, sizeof(*msg));
-	sg_init_marker(msg->sg.data, MAX_MSG_FRAGS);
+	sg_init_marker(msg->sg.data, NR_MSG_FRAG_IDS);
 }
 
 static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
@@ -196,14 +199,11 @@ static inline void sk_msg_xfer_full(struct sk_msg *dst, struct sk_msg *src)
 
 static inline bool sk_msg_full(const struct sk_msg *msg)
 {
-	return (msg->sg.end == msg->sg.start) && msg->sg.size;
+	return sk_msg_iter_dist(msg->sg.start, msg->sg.end) == MAX_MSG_FRAGS;
 }
 
 static inline u32 sk_msg_elem_used(const struct sk_msg *msg)
 {
-	if (sk_msg_full(msg))
-		return MAX_MSG_FRAGS;
-
 	return sk_msg_iter_dist(msg->sg.start, msg->sg.end);
 }
 

commit 14684b93019a2d2ece0df5acaf921924541b928d
Merge: 92da362c07d4 0058b0a506e4
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 9 11:04:37 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    One conflict in the BPF samples Makefile, some fixes in 'net' whilst
    we were converting over to Makefile.target rules in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 683916f6a84023407761d843048f1aea486b2612
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 4 15:36:57 2019 -0800

    net/tls: fix sk_msg trim on fallback to copy mode
    
    sk_msg_trim() tries to only update curr pointer if it falls into
    the trimmed region. The logic, however, does not take into the
    account pointer wrapping that sk_msg_iter_var_prev() does nor
    (as John points out) the fact that msg->sg is a ring buffer.
    
    This means that when the message was trimmed completely, the new
    curr pointer would have the value of MAX_MSG_FRAGS - 1, which is
    neither smaller than any other value, nor would it actually be
    correct.
    
    Special case the trimming to 0 length a little bit and rework
    the comparison between curr and end to take into account wrapping.
    
    This bug caused the TLS code to not copy all of the message, if
    zero copy filled in fewer sg entries than memcopy would need.
    
    Big thanks to Alexander Potapenko for the non-KMSAN reproducer.
    
    v2:
     - take into account that msg->sg is a ring buffer (John).
    
    Link: https://lore.kernel.org/netdev/20191030160542.30295-1-jakub.kicinski@netronome.com/ (v1)
    
    Fixes: d829e9c4112b ("tls: convert to generic sk_msg interface")
    Reported-by: syzbot+f8495bff23a879a6d0bd@syzkaller.appspotmail.com
    Reported-by: syzbot+6f50c99e8f6194bf363f@syzkaller.appspotmail.com
    Co-developed-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index e4b3fb4bb77c..ce7055259877 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -139,6 +139,11 @@ static inline void sk_msg_apply_bytes(struct sk_psock *psock, u32 bytes)
 	}
 }
 
+static inline u32 sk_msg_iter_dist(u32 start, u32 end)
+{
+	return end >= start ? end - start : end + (MAX_MSG_FRAGS - start);
+}
+
 #define sk_msg_iter_var_prev(var)			\
 	do {						\
 		if (var == 0)				\
@@ -198,9 +203,7 @@ static inline u32 sk_msg_elem_used(const struct sk_msg *msg)
 	if (sk_msg_full(msg))
 		return MAX_MSG_FRAGS;
 
-	return msg->sg.end >= msg->sg.start ?
-		msg->sg.end - msg->sg.start :
-		msg->sg.end + (MAX_MSG_FRAGS - msg->sg.start);
+	return sk_msg_iter_dist(msg->sg.start, msg->sg.end);
 }
 
 static inline struct scatterlist *sk_msg_elem(struct sk_msg *msg, int which)

commit 163ab96b52ae2bb2d8f188cd29f0b570610f9007
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Sun Oct 6 21:09:27 2019 -0700

    net: sockmap: use bitmap for copy info
    
    Don't use bool array in struct sk_msg_sg, save 12 bytes.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index e4b3fb4bb77c..fe80d537945d 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -28,13 +28,14 @@ struct sk_msg_sg {
 	u32				end;
 	u32				size;
 	u32				copybreak;
-	bool				copy[MAX_MSG_FRAGS];
+	unsigned long			copy;
 	/* The extra element is used for chaining the front and sections when
 	 * the list becomes partitioned (e.g. end < start). The crypto APIs
 	 * require the chaining.
 	 */
 	struct scatterlist		data[MAX_MSG_FRAGS + 1];
 };
+static_assert(BITS_PER_LONG >= MAX_MSG_FRAGS);
 
 /* UAPI in filter.c depends on struct sk_msg_sg being first element. */
 struct sk_msg {
@@ -227,7 +228,7 @@ static inline void sk_msg_compute_data_pointers(struct sk_msg *msg)
 {
 	struct scatterlist *sge = sk_msg_elem(msg, msg->sg.start);
 
-	if (msg->sg.copy[msg->sg.start]) {
+	if (test_bit(msg->sg.start, &msg->sg.copy)) {
 		msg->data = NULL;
 		msg->data_end = NULL;
 	} else {
@@ -246,7 +247,7 @@ static inline void sk_msg_page_add(struct sk_msg *msg, struct page *page,
 	sg_set_page(sge, page, len, offset);
 	sg_unmark_end(sge);
 
-	msg->sg.copy[msg->sg.end] = true;
+	__set_bit(msg->sg.end, &msg->sg.copy);
 	msg->sg.size += len;
 	sk_msg_iter_next(msg, end);
 }
@@ -254,7 +255,10 @@ static inline void sk_msg_page_add(struct sk_msg *msg, struct page *page,
 static inline void sk_msg_sg_copy(struct sk_msg *msg, u32 i, bool copy_state)
 {
 	do {
-		msg->sg.copy[i] = copy_state;
+		if (copy_state)
+			__set_bit(i, &msg->sg.copy);
+		else
+			__clear_bit(i, &msg->sg.copy);
 		sk_msg_iter_var_next(i);
 		if (i == msg->sg.end)
 			break;

commit 95fa145479fbc0a0c1fd3274ceb42ec03c042a4a
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Jul 19 10:29:22 2019 -0700

    bpf: sockmap/tls, close can race with map free
    
    When a map free is called and in parallel a socket is closed we
    have two paths that can potentially reset the socket prot ops, the
    bpf close() path and the map free path. This creates a problem
    with which prot ops should be used from the socket closed side.
    
    If the map_free side completes first then we want to call the
    original lowest level ops. However, if the tls path runs first
    we want to call the sockmap ops. Additionally there was no locking
    around prot updates in TLS code paths so the prot ops could
    be changed multiple times once from TLS path and again from sockmap
    side potentially leaving ops pointed at either TLS or sockmap
    when psock and/or tls context have already been destroyed.
    
    To fix this race first only update ops inside callback lock
    so that TLS, sockmap and lowest level all agree on prot state.
    Second and a ULP callback update() so that lower layers can
    inform the upper layer when they are being removed allowing the
    upper layer to reset prot ops.
    
    This gets us close to allowing sockmap and tls to be stacked
    in arbitrary order but will save that patch for *next trees.
    
    v4:
     - make sure we don't free things for device;
     - remove the checks which swap the callbacks back
       only if TLS is at the top.
    
    Reported-by: syzbot+06537213db7ba2745c4a@syzkaller.appspotmail.com
    Fixes: 02c558b2d5d6 ("bpf: sockmap, support for msg_peek in sk_msg with redirect ingress")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 50ced8aba9db..e4b3fb4bb77c 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -354,7 +354,13 @@ static inline void sk_psock_restore_proto(struct sock *sk,
 	sk->sk_write_space = psock->saved_write_space;
 
 	if (psock->sk_proto) {
-		sk->sk_prot = psock->sk_proto;
+		struct inet_connection_sock *icsk = inet_csk(sk);
+		bool has_ulp = !!icsk->icsk_ulp_data;
+
+		if (has_ulp)
+			tcp_update_ulp(sk, psock->sk_proto);
+		else
+			sk->sk_prot = psock->sk_proto;
 		psock->sk_proto = NULL;
 	}
 }

commit 186bcc3dcd106dc52d706117f912054b616666e1
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Wed May 22 12:01:42 2019 +0200

    bpf: sockmap, restore sk_write_space when psock gets dropped
    
    Once psock gets unlinked from its sock (sk_psock_drop), user-space can
    still trigger a call to sk->sk_write_space by setting TCP_NOTSENT_LOWAT
    socket option. This causes a null-ptr-deref because we try to read
    psock->saved_write_space from sk_psock_write_space:
    
    ==================================================================
    BUG: KASAN: null-ptr-deref in sk_psock_write_space+0x69/0x80
    Read of size 8 at addr 00000000000001a0 by task sockmap-echo/131
    
    CPU: 0 PID: 131 Comm: sockmap-echo Not tainted 5.2.0-rc1-00094-gf49aa1de9836 #81
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS
    ?-20180724_192412-buildhw-07.phx2.fedoraproject.org-1.fc29 04/01/2014
    Call Trace:
     ? sk_psock_write_space+0x69/0x80
     __kasan_report.cold.2+0x5/0x3f
     ? sk_psock_write_space+0x69/0x80
     kasan_report+0xe/0x20
     sk_psock_write_space+0x69/0x80
     tcp_setsockopt+0x69a/0xfc0
     ? tcp_shutdown+0x70/0x70
     ? fsnotify+0x5b0/0x5f0
     ? remove_wait_queue+0x90/0x90
     ? __fget_light+0xa5/0xf0
     __sys_setsockopt+0xe6/0x180
     ? sockfd_lookup_light+0xb0/0xb0
     ? vfs_write+0x195/0x210
     ? ksys_write+0xc9/0x150
     ? __x64_sys_read+0x50/0x50
     ? __bpf_trace_x86_fpu+0x10/0x10
     __x64_sys_setsockopt+0x61/0x70
     do_syscall_64+0xc5/0x520
     ? vmacache_find+0xc0/0x110
     ? syscall_return_slowpath+0x110/0x110
     ? handle_mm_fault+0xb4/0x110
     ? entry_SYSCALL_64_after_hwframe+0x3e/0xbe
     ? trace_hardirqs_off_caller+0x4b/0x120
     ? trace_hardirqs_off_thunk+0x1a/0x3a
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x7f2e5e7cdcce
    Code: d8 64 89 02 48 c7 c0 ff ff ff ff eb b1 66 2e 0f 1f 84 00 00 00 00 00
    0f 1f 44 00 00 f3 0f 1e fa 49 89 ca b8 36 00 00 00 0f 05 <48> 3d 01 f0 ff
    ff 73 01 c3 48 8b 0d 8a 11 0c 00 f7 d8 64 89 01 48
    RSP: 002b:00007ffed011b778 EFLAGS: 00000206 ORIG_RAX: 0000000000000036
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 00007f2e5e7cdcce
    RDX: 0000000000000019 RSI: 0000000000000006 RDI: 0000000000000007
    RBP: 00007ffed011b790 R08: 0000000000000004 R09: 00007f2e5e84ee80
    R10: 00007ffed011b788 R11: 0000000000000206 R12: 00007ffed011b78c
    R13: 00007ffed011b788 R14: 0000000000000007 R15: 0000000000000068
    ==================================================================
    
    Restore the saved sk_write_space callback when psock is being dropped to
    fix the crash.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 178a3933a71b..50ced8aba9db 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -351,6 +351,8 @@ static inline void sk_psock_update_proto(struct sock *sk,
 static inline void sk_psock_restore_proto(struct sock *sk,
 					  struct sk_psock *psock)
 {
+	sk->sk_write_space = psock->saved_write_space;
+
 	if (psock->sk_proto) {
 		sk->sk_prot = psock->sk_proto;
 		psock->sk_proto = NULL;

commit 552de91068828daef50a227a665068cf8dde835e
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Dec 20 11:35:33 2018 -0800

    bpf: sk_msg, fix socket data_ready events
    
    When a skb verdict program is in-use and either another BPF program
    redirects to that socket or the new SK_PASS support is used the
    data_ready callback does not wake up application. Instead because
    the stream parser/verdict is using the sk data_ready callback we wake
    up the stream parser/verdict block.
    
    Fix this by adding a helper to check if the stream parser block is
    enabled on the sk and if so call the saved pointer which is the
    upper layers wake up function.
    
    This fixes application stalls observed when an application is waiting
    for data in a blocking read().
    
    Fixes: d829e9c4112b ("tls: convert to generic sk_msg interface")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index dd57e6f408b1..178a3933a71b 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -417,6 +417,14 @@ static inline void sk_psock_put(struct sock *sk, struct sk_psock *psock)
 		sk_psock_drop(sk, psock);
 }
 
+static inline void sk_psock_data_ready(struct sock *sk, struct sk_psock *psock)
+{
+	if (psock->parser.enabled)
+		psock->parser.saved_data_ready(sk);
+	else
+		sk->sk_data_ready(sk);
+}
+
 static inline void psock_set_prog(struct bpf_prog **pprog,
 				  struct bpf_prog *prog)
 {

commit 7a69c0f250568e6ab72f401b2c69aa0e666c94f2
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Dec 20 11:35:31 2018 -0800

    bpf: skmsg, replace comments with BUILD bug
    
    Enforce comment on structure layout dependency with a BUILD_BUG_ON
    to ensure the condition is maintained.
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index eb8f6cb84c10..dd57e6f408b1 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -36,9 +36,7 @@ struct sk_msg_sg {
 	struct scatterlist		data[MAX_MSG_FRAGS + 1];
 };
 
-/* UAPI in filter.c depends on struct sk_msg_sg being first element. If
- * this is moved filter.c also must be updated.
- */
+/* UAPI in filter.c depends on struct sk_msg_sg being first element. */
 struct sk_msg {
 	struct sk_msg_sg		sg;
 	void				*data;

commit 3bdbd0228e7555ec745e08469b98e5a0966409d6
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Dec 16 15:47:04 2018 -0800

    bpf: sockmap, metadata support for reporting size of msg
    
    This adds metadata to sk_msg_md for BPF programs to read the sk_msg
    size.
    
    When the SK_MSG program is running under an application that is using
    sendfile the data is not copied into sk_msg buffers by default. Rather
    the BPF program uses sk_msg_pull_data to read the bytes in. This
    avoids doing the costly memcopy instructions when they are not in
    fact needed. However, if we don't know the size of the sk_msg we
    have to guess if needed bytes are available by doing a pull request
    which may fail. By including the size of the sk_msg BPF programs can
    check the size before issuing sk_msg_pull_data requests.
    
    Additionally, the same applies for sendmsg calls when the application
    provides multiple iovs. Here the BPF program needs to pull in data
    to update data pointers but its not clear where the data ends without
    a size parameter. In many cases "guessing" is not easy to do
    and results in multiple calls to pull and without bounded loops
    everything gets fairly tricky.
    
    Clean this up by including a u32 size field. Note, all writes into
    sk_msg_md are rejected already from sk_msg_is_valid_access so nothing
    additional is needed there.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 2a11e9d91dfa..eb8f6cb84c10 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -36,6 +36,9 @@ struct sk_msg_sg {
 	struct scatterlist		data[MAX_MSG_FRAGS + 1];
 };
 
+/* UAPI in filter.c depends on struct sk_msg_sg being first element. If
+ * this is moved filter.c also must be updated.
+ */
 struct sk_msg {
 	struct sk_msg_sg		sg;
 	void				*data;

commit 6fff607e2f14bd7c63c06c464a6f93b8efbabe28
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Oct 19 19:56:49 2018 -0700

    bpf: sk_msg program helper bpf_msg_push_data
    
    This allows user to push data into a msg using sk_msg program types.
    The format is as follows,
    
            bpf_msg_push_data(msg, offset, len, flags)
    
    this will insert 'len' bytes at offset 'offset'. For example to
    prepend 10 bytes at the front of the message the user can,
    
            bpf_msg_push_data(msg, 0, 10, 0);
    
    This will invalidate data bounds so BPF user will have to then recheck
    data bounds after calling this. After this the msg size will have been
    updated and the user is free to write into the added bytes. We allow
    any offset/len as long as it is within the (data, data_end) range.
    However, a copy will be required if the ring is full and its possible
    for the helper to fail with ENOMEM or EINVAL errors which need to be
    handled by the BPF program.
    
    This can be used similar to XDP metadata to pass data between sk_msg
    layer and lower layers.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 84e18863f6a4..2a11e9d91dfa 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -207,6 +207,11 @@ static inline struct scatterlist *sk_msg_elem(struct sk_msg *msg, int which)
 	return &msg->sg.data[which];
 }
 
+static inline struct scatterlist sk_msg_elem_cpy(struct sk_msg *msg, int which)
+{
+	return msg->sg.data[which];
+}
+
 static inline struct page *sk_msg_page(struct sk_msg *msg, int which)
 {
 	return sg_page(sk_msg_elem(msg, which));

commit 5032d079909d1ac5c2535acc32d5f01cd245d8ea
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Oct 18 13:58:35 2018 -0700

    bpf: skmsg, fix psock create on existing kcm/tls port
    
    Before using the psock returned by sk_psock_get() when adding it to a
    sockmap we need to ensure it is actually a sockmap based psock.
    Previously we were only checking this after incrementing the reference
    counter which was an error. This resulted in a slab-out-of-bounds
    error when the psock was not actually a sockmap type.
    
    This moves the check up so the reference counter is only used
    if it is a sockmap psock.
    
    Eric reported the following KASAN BUG,
    
    BUG: KASAN: slab-out-of-bounds in atomic_read include/asm-generic/atomic-instrumented.h:21 [inline]
    BUG: KASAN: slab-out-of-bounds in refcount_inc_not_zero_checked+0x97/0x2f0 lib/refcount.c:120
    Read of size 4 at addr ffff88019548be58 by task syz-executor4/22387
    
    CPU: 1 PID: 22387 Comm: syz-executor4 Not tainted 4.19.0-rc7+ #264
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x1c4/0x2b4 lib/dump_stack.c:113
     print_address_description.cold.8+0x9/0x1ff mm/kasan/report.c:256
     kasan_report_error mm/kasan/report.c:354 [inline]
     kasan_report.cold.9+0x242/0x309 mm/kasan/report.c:412
     check_memory_region_inline mm/kasan/kasan.c:260 [inline]
     check_memory_region+0x13e/0x1b0 mm/kasan/kasan.c:267
     kasan_check_read+0x11/0x20 mm/kasan/kasan.c:272
     atomic_read include/asm-generic/atomic-instrumented.h:21 [inline]
     refcount_inc_not_zero_checked+0x97/0x2f0 lib/refcount.c:120
     sk_psock_get include/linux/skmsg.h:379 [inline]
     sock_map_link.isra.6+0x41f/0xe30 net/core/sock_map.c:178
     sock_hash_update_common+0x19b/0x11e0 net/core/sock_map.c:669
     sock_hash_update_elem+0x306/0x470 net/core/sock_map.c:738
     map_update_elem+0x819/0xdf0 kernel/bpf/syscall.c:818
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Reported-by: Eric Dumazet <eric.dumazet@gmail.com>
    Fixes: 604326b41a6f ("bpf, sockmap: convert to generic sk_msg interface")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 22347b08e1f8..84e18863f6a4 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -270,11 +270,6 @@ static inline struct sk_psock *sk_psock(const struct sock *sk)
 	return rcu_dereference_sk_user_data(sk);
 }
 
-static inline bool sk_has_psock(struct sock *sk)
-{
-	return sk_psock(sk) != NULL && sk->sk_prot->recvmsg == tcp_bpf_recvmsg;
-}
-
 static inline void sk_psock_queue_msg(struct sk_psock *psock,
 				      struct sk_msg *msg)
 {
@@ -374,6 +369,26 @@ static inline bool sk_psock_test_state(const struct sk_psock *psock,
 	return test_bit(bit, &psock->state);
 }
 
+static inline struct sk_psock *sk_psock_get_checked(struct sock *sk)
+{
+	struct sk_psock *psock;
+
+	rcu_read_lock();
+	psock = sk_psock(sk);
+	if (psock) {
+		if (sk->sk_prot->recvmsg != tcp_bpf_recvmsg) {
+			psock = ERR_PTR(-EBUSY);
+			goto out;
+		}
+
+		if (!refcount_inc_not_zero(&psock->refcnt))
+			psock = ERR_PTR(-EBUSY);
+	}
+out:
+	rcu_read_unlock();
+	return psock;
+}
+
 static inline struct sk_psock *sk_psock_get(struct sock *sk)
 {
 	struct sk_psock *psock;

commit 8734a162c13b1a893e7dff8de0df81fed04c51a6
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Oct 16 11:07:59 2018 -0700

    bpf: skmsg, improve sk_msg_used_element to work in cork context
    
    Currently sk_msg_used_element is only called in zerocopy context where
    cork is not possible and if this case happens we fallback to copy
    mode. However the helper is more useful if it works in all contexts.
    
    This patch resolved the case where if end == head indicating a full
    or empty ring the helper always reports an empty ring. To fix this
    add a test for the full ring case to avoid reporting a full ring
    has 0 elements. This additional functionality will be used in the
    next patches from recvmsg context where end = head with a full ring
    is a valid case.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 31df0d9fa536..22347b08e1f8 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -187,18 +187,21 @@ static inline void sk_msg_xfer_full(struct sk_msg *dst, struct sk_msg *src)
 	sk_msg_init(src);
 }
 
+static inline bool sk_msg_full(const struct sk_msg *msg)
+{
+	return (msg->sg.end == msg->sg.start) && msg->sg.size;
+}
+
 static inline u32 sk_msg_elem_used(const struct sk_msg *msg)
 {
+	if (sk_msg_full(msg))
+		return MAX_MSG_FRAGS;
+
 	return msg->sg.end >= msg->sg.start ?
 		msg->sg.end - msg->sg.start :
 		msg->sg.end + (MAX_MSG_FRAGS - msg->sg.start);
 }
 
-static inline bool sk_msg_full(const struct sk_msg *msg)
-{
-	return (msg->sg.end == msg->sg.start) && msg->sg.size;
-}
-
 static inline struct scatterlist *sk_msg_elem(struct sk_msg *msg, int which)
 {
 	return &msg->sg.data[which];

commit 3f4c3127d332000530349db4843deece27fe5e0c
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Oct 16 10:36:01 2018 -0700

    bpf: sockmap, fix skmsg recvmsg handler to track size correctly
    
    When converting sockmap to new skmsg generic data structures we missed
    that the recvmsg handler did not correctly use sg.size and instead was
    using individual elements length. The result is if a sock is closed
    with outstanding data we omit the call to sk_mem_uncharge() and can
    get the warning below.
    
    [   66.728282] WARNING: CPU: 6 PID: 5783 at net/core/stream.c:206 sk_stream_kill_queues+0x1fa/0x210
    
    To fix this correct the redirect handler to xfer the size along with
    the scatterlist and also decrement the size from the recvmsg handler.
    Now when a sock is closed the remaining 'size' will be decremented
    with sk_mem_uncharge().
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 0b919f0bc6d6..31df0d9fa536 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -176,6 +176,7 @@ static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
 {
 	dst->sg.data[which] = src->sg.data[which];
 	dst->sg.data[which].length  = size;
+	dst->sg.size		   += size;
 	src->sg.data[which].length -= size;
 	src->sg.data[which].offset += size;
 }

commit d3b18ad31f93d0b6bae105c679018a1ba7daa9ca
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Oct 13 02:46:01 2018 +0200

    tls: add bpf support to sk_msg handling
    
    This work adds BPF sk_msg verdict program support to kTLS
    allowing BPF and kTLS to be combined together. Previously kTLS
    and sk_msg verdict programs were mutually exclusive in the
    ULP layer which created challenges for the orchestrator when
    trying to apply TCP based policy, for example. To resolve this,
    leveraging the work from previous patches that consolidates
    the use of sk_msg, we can finally enable BPF sk_msg verdict
    programs so they continue to run after the kTLS socket is
    created. No change in behavior when kTLS is not used in
    combination with BPF, the kselftest suite for kTLS also runs
    successfully.
    
    Joint work with Daniel.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 4e84b3c2eff8..0b919f0bc6d6 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -29,7 +29,11 @@ struct sk_msg_sg {
 	u32				size;
 	u32				copybreak;
 	bool				copy[MAX_MSG_FRAGS];
-	struct scatterlist		data[MAX_MSG_FRAGS];
+	/* The extra element is used for chaining the front and sections when
+	 * the list becomes partitioned (e.g. end < start). The crypto APIs
+	 * require the chaining.
+	 */
+	struct scatterlist		data[MAX_MSG_FRAGS + 1];
 };
 
 struct sk_msg {
@@ -112,6 +116,7 @@ void sk_msg_free_partial_nocharge(struct sock *sk, struct sk_msg *msg,
 				  u32 bytes);
 
 void sk_msg_return(struct sock *sk, struct sk_msg *msg, int bytes);
+void sk_msg_return_zero(struct sock *sk, struct sk_msg *msg, int bytes);
 
 int sk_msg_zerocopy_from_iter(struct sock *sk, struct iov_iter *from,
 			      struct sk_msg *msg, u32 bytes);
@@ -161,8 +166,9 @@ static inline void sk_msg_clear_meta(struct sk_msg *msg)
 
 static inline void sk_msg_init(struct sk_msg *msg)
 {
+	BUILD_BUG_ON(ARRAY_SIZE(msg->sg.data) - 1 != MAX_MSG_FRAGS);
 	memset(msg, 0, sizeof(*msg));
-	sg_init_marker(msg->sg.data, ARRAY_SIZE(msg->sg.data));
+	sg_init_marker(msg->sg.data, MAX_MSG_FRAGS);
 }
 
 static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
@@ -174,6 +180,12 @@ static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
 	src->sg.data[which].offset += size;
 }
 
+static inline void sk_msg_xfer_full(struct sk_msg *dst, struct sk_msg *src)
+{
+	memcpy(dst, src, sizeof(*src));
+	sk_msg_init(src);
+}
+
 static inline u32 sk_msg_elem_used(const struct sk_msg *msg)
 {
 	return msg->sg.end >= msg->sg.start ?
@@ -229,6 +241,26 @@ static inline void sk_msg_page_add(struct sk_msg *msg, struct page *page,
 	sk_msg_iter_next(msg, end);
 }
 
+static inline void sk_msg_sg_copy(struct sk_msg *msg, u32 i, bool copy_state)
+{
+	do {
+		msg->sg.copy[i] = copy_state;
+		sk_msg_iter_var_next(i);
+		if (i == msg->sg.end)
+			break;
+	} while (1);
+}
+
+static inline void sk_msg_sg_copy_set(struct sk_msg *msg, u32 start)
+{
+	sk_msg_sg_copy(msg, start, true);
+}
+
+static inline void sk_msg_sg_copy_clear(struct sk_msg *msg, u32 start)
+{
+	sk_msg_sg_copy(msg, start, false);
+}
+
 static inline struct sk_psock *sk_psock(const struct sock *sk)
 {
 	return rcu_dereference_sk_user_data(sk);
@@ -245,6 +277,11 @@ static inline void sk_psock_queue_msg(struct sk_psock *psock,
 	list_add_tail(&msg->list, &psock->ingress_msg);
 }
 
+static inline bool sk_psock_queue_empty(const struct sk_psock *psock)
+{
+	return psock ? list_empty(&psock->ingress_msg) : true;
+}
+
 static inline void sk_psock_report_error(struct sk_psock *psock, int err)
 {
 	struct sock *sk = psock->sk;

commit d829e9c4112b52f4f00195900fd4c685f61365ab
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:59 2018 +0200

    tls: convert to generic sk_msg interface
    
    Convert kTLS over to make use of sk_msg interface for plaintext and
    encrypted scattergather data, so it reuses all the sk_msg helpers
    and data structure which later on in a second step enables to glue
    this to BPF.
    
    This also allows to remove quite a bit of open coded helpers which
    are covered by the sk_msg API. Recent changes in kTLs 80ece6a03aaf
    ("tls: Remove redundant vars from tls record structure") and
    4e6d47206c32 ("tls: Add support for inplace records encryption")
    changed the data path handling a bit; while we've kept the latter
    optimization intact, we had to undo the former change to better
    fit the sk_msg model, hence the sg_aead_in and sg_aead_out have
    been brought back and are linked into the sk_msg sgs. Now the kTLS
    record contains a msg_plaintext and msg_encrypted sk_msg each.
    
    In the original code, the zerocopy_from_iter() has been used out
    of TX but also RX path. For the strparser skb-based RX path,
    we've left the zerocopy_from_iter() in decrypt_internal() mostly
    untouched, meaning it has been moved into tls_setup_from_iter()
    with charging logic removed (as not used from RX). Given RX path
    is not based on sk_msg objects, we haven't pursued setting up a
    dummy sk_msg to call into sk_msg_zerocopy_from_iter(), but it
    could be an option to prusue in a later step.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
index 95678103c4a0..4e84b3c2eff8 100644
--- a/include/linux/skmsg.h
+++ b/include/linux/skmsg.h
@@ -102,6 +102,8 @@ struct sk_psock {
 
 int sk_msg_alloc(struct sock *sk, struct sk_msg *msg, int len,
 		 int elem_first_coalesce);
+int sk_msg_clone(struct sock *sk, struct sk_msg *dst, struct sk_msg *src,
+		 u32 off, u32 len);
 void sk_msg_trim(struct sock *sk, struct sk_msg *msg, int len);
 int sk_msg_free(struct sock *sk, struct sk_msg *msg);
 int sk_msg_free_nocharge(struct sock *sk, struct sk_msg *msg);

commit 604326b41a6fb9b4a78b6179335decee0365cd8c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:58 2018 +0200

    bpf, sockmap: convert to generic sk_msg interface
    
    Add a generic sk_msg layer, and convert current sockmap and later
    kTLS over to make use of it. While sk_buff handles network packet
    representation from netdevice up to socket, sk_msg handles data
    representation from application to socket layer.
    
    This means that sk_msg framework spans across ULP users in the
    kernel, and enables features such as introspection or filtering
    of data with the help of BPF programs that operate on this data
    structure.
    
    Latter becomes in particular useful for kTLS where data encryption
    is deferred into the kernel, and as such enabling the kernel to
    perform L7 introspection and policy based on BPF for TLS connections
    where the record is being encrypted after BPF has run and came to
    a verdict. In order to get there, first step is to transform open
    coding of scatter-gather list handling into a common core framework
    that subsystems can use.
    
    The code itself has been split and refactored into three bigger
    pieces: i) the generic sk_msg API which deals with managing the
    scatter gather ring, providing helpers for walking and mangling,
    transferring application data from user space into it, and preparing
    it for BPF pre/post-processing, ii) the plain sock map itself
    where sockets can be attached to or detached from; these bits
    are independent of i) which can now be used also without sock
    map, and iii) the integration with plain TCP as one protocol
    to be used for processing L7 application data (later this could
    e.g. also be extended to other protocols like UDP). The semantics
    are the same with the old sock map code and therefore no change
    of user facing behavior or APIs. While pursuing this work it
    also helped finding a number of bugs in the old sockmap code
    that we've fixed already in earlier commits. The test_sockmap
    kselftest suite passes through fine as well.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/skmsg.h b/include/linux/skmsg.h
new file mode 100644
index 000000000000..95678103c4a0
--- /dev/null
+++ b/include/linux/skmsg.h
@@ -0,0 +1,371 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2017 - 2018 Covalent IO, Inc. http://covalent.io */
+
+#ifndef _LINUX_SKMSG_H
+#define _LINUX_SKMSG_H
+
+#include <linux/bpf.h>
+#include <linux/filter.h>
+#include <linux/scatterlist.h>
+#include <linux/skbuff.h>
+
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/strparser.h>
+
+#define MAX_MSG_FRAGS			MAX_SKB_FRAGS
+
+enum __sk_action {
+	__SK_DROP = 0,
+	__SK_PASS,
+	__SK_REDIRECT,
+	__SK_NONE,
+};
+
+struct sk_msg_sg {
+	u32				start;
+	u32				curr;
+	u32				end;
+	u32				size;
+	u32				copybreak;
+	bool				copy[MAX_MSG_FRAGS];
+	struct scatterlist		data[MAX_MSG_FRAGS];
+};
+
+struct sk_msg {
+	struct sk_msg_sg		sg;
+	void				*data;
+	void				*data_end;
+	u32				apply_bytes;
+	u32				cork_bytes;
+	u32				flags;
+	struct sk_buff			*skb;
+	struct sock			*sk_redir;
+	struct sock			*sk;
+	struct list_head		list;
+};
+
+struct sk_psock_progs {
+	struct bpf_prog			*msg_parser;
+	struct bpf_prog			*skb_parser;
+	struct bpf_prog			*skb_verdict;
+};
+
+enum sk_psock_state_bits {
+	SK_PSOCK_TX_ENABLED,
+};
+
+struct sk_psock_link {
+	struct list_head		list;
+	struct bpf_map			*map;
+	void				*link_raw;
+};
+
+struct sk_psock_parser {
+	struct strparser		strp;
+	bool				enabled;
+	void (*saved_data_ready)(struct sock *sk);
+};
+
+struct sk_psock_work_state {
+	struct sk_buff			*skb;
+	u32				len;
+	u32				off;
+};
+
+struct sk_psock {
+	struct sock			*sk;
+	struct sock			*sk_redir;
+	u32				apply_bytes;
+	u32				cork_bytes;
+	u32				eval;
+	struct sk_msg			*cork;
+	struct sk_psock_progs		progs;
+	struct sk_psock_parser		parser;
+	struct sk_buff_head		ingress_skb;
+	struct list_head		ingress_msg;
+	unsigned long			state;
+	struct list_head		link;
+	spinlock_t			link_lock;
+	refcount_t			refcnt;
+	void (*saved_unhash)(struct sock *sk);
+	void (*saved_close)(struct sock *sk, long timeout);
+	void (*saved_write_space)(struct sock *sk);
+	struct proto			*sk_proto;
+	struct sk_psock_work_state	work_state;
+	struct work_struct		work;
+	union {
+		struct rcu_head		rcu;
+		struct work_struct	gc;
+	};
+};
+
+int sk_msg_alloc(struct sock *sk, struct sk_msg *msg, int len,
+		 int elem_first_coalesce);
+void sk_msg_trim(struct sock *sk, struct sk_msg *msg, int len);
+int sk_msg_free(struct sock *sk, struct sk_msg *msg);
+int sk_msg_free_nocharge(struct sock *sk, struct sk_msg *msg);
+void sk_msg_free_partial(struct sock *sk, struct sk_msg *msg, u32 bytes);
+void sk_msg_free_partial_nocharge(struct sock *sk, struct sk_msg *msg,
+				  u32 bytes);
+
+void sk_msg_return(struct sock *sk, struct sk_msg *msg, int bytes);
+
+int sk_msg_zerocopy_from_iter(struct sock *sk, struct iov_iter *from,
+			      struct sk_msg *msg, u32 bytes);
+int sk_msg_memcopy_from_iter(struct sock *sk, struct iov_iter *from,
+			     struct sk_msg *msg, u32 bytes);
+
+static inline void sk_msg_check_to_free(struct sk_msg *msg, u32 i, u32 bytes)
+{
+	WARN_ON(i == msg->sg.end && bytes);
+}
+
+static inline void sk_msg_apply_bytes(struct sk_psock *psock, u32 bytes)
+{
+	if (psock->apply_bytes) {
+		if (psock->apply_bytes < bytes)
+			psock->apply_bytes = 0;
+		else
+			psock->apply_bytes -= bytes;
+	}
+}
+
+#define sk_msg_iter_var_prev(var)			\
+	do {						\
+		if (var == 0)				\
+			var = MAX_MSG_FRAGS - 1;	\
+		else					\
+			var--;				\
+	} while (0)
+
+#define sk_msg_iter_var_next(var)			\
+	do {						\
+		var++;					\
+		if (var == MAX_MSG_FRAGS)		\
+			var = 0;			\
+	} while (0)
+
+#define sk_msg_iter_prev(msg, which)			\
+	sk_msg_iter_var_prev(msg->sg.which)
+
+#define sk_msg_iter_next(msg, which)			\
+	sk_msg_iter_var_next(msg->sg.which)
+
+static inline void sk_msg_clear_meta(struct sk_msg *msg)
+{
+	memset(&msg->sg, 0, offsetofend(struct sk_msg_sg, copy));
+}
+
+static inline void sk_msg_init(struct sk_msg *msg)
+{
+	memset(msg, 0, sizeof(*msg));
+	sg_init_marker(msg->sg.data, ARRAY_SIZE(msg->sg.data));
+}
+
+static inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,
+			       int which, u32 size)
+{
+	dst->sg.data[which] = src->sg.data[which];
+	dst->sg.data[which].length  = size;
+	src->sg.data[which].length -= size;
+	src->sg.data[which].offset += size;
+}
+
+static inline u32 sk_msg_elem_used(const struct sk_msg *msg)
+{
+	return msg->sg.end >= msg->sg.start ?
+		msg->sg.end - msg->sg.start :
+		msg->sg.end + (MAX_MSG_FRAGS - msg->sg.start);
+}
+
+static inline bool sk_msg_full(const struct sk_msg *msg)
+{
+	return (msg->sg.end == msg->sg.start) && msg->sg.size;
+}
+
+static inline struct scatterlist *sk_msg_elem(struct sk_msg *msg, int which)
+{
+	return &msg->sg.data[which];
+}
+
+static inline struct page *sk_msg_page(struct sk_msg *msg, int which)
+{
+	return sg_page(sk_msg_elem(msg, which));
+}
+
+static inline bool sk_msg_to_ingress(const struct sk_msg *msg)
+{
+	return msg->flags & BPF_F_INGRESS;
+}
+
+static inline void sk_msg_compute_data_pointers(struct sk_msg *msg)
+{
+	struct scatterlist *sge = sk_msg_elem(msg, msg->sg.start);
+
+	if (msg->sg.copy[msg->sg.start]) {
+		msg->data = NULL;
+		msg->data_end = NULL;
+	} else {
+		msg->data = sg_virt(sge);
+		msg->data_end = msg->data + sge->length;
+	}
+}
+
+static inline void sk_msg_page_add(struct sk_msg *msg, struct page *page,
+				   u32 len, u32 offset)
+{
+	struct scatterlist *sge;
+
+	get_page(page);
+	sge = sk_msg_elem(msg, msg->sg.end);
+	sg_set_page(sge, page, len, offset);
+	sg_unmark_end(sge);
+
+	msg->sg.copy[msg->sg.end] = true;
+	msg->sg.size += len;
+	sk_msg_iter_next(msg, end);
+}
+
+static inline struct sk_psock *sk_psock(const struct sock *sk)
+{
+	return rcu_dereference_sk_user_data(sk);
+}
+
+static inline bool sk_has_psock(struct sock *sk)
+{
+	return sk_psock(sk) != NULL && sk->sk_prot->recvmsg == tcp_bpf_recvmsg;
+}
+
+static inline void sk_psock_queue_msg(struct sk_psock *psock,
+				      struct sk_msg *msg)
+{
+	list_add_tail(&msg->list, &psock->ingress_msg);
+}
+
+static inline void sk_psock_report_error(struct sk_psock *psock, int err)
+{
+	struct sock *sk = psock->sk;
+
+	sk->sk_err = err;
+	sk->sk_error_report(sk);
+}
+
+struct sk_psock *sk_psock_init(struct sock *sk, int node);
+
+int sk_psock_init_strp(struct sock *sk, struct sk_psock *psock);
+void sk_psock_start_strp(struct sock *sk, struct sk_psock *psock);
+void sk_psock_stop_strp(struct sock *sk, struct sk_psock *psock);
+
+int sk_psock_msg_verdict(struct sock *sk, struct sk_psock *psock,
+			 struct sk_msg *msg);
+
+static inline struct sk_psock_link *sk_psock_init_link(void)
+{
+	return kzalloc(sizeof(struct sk_psock_link),
+		       GFP_ATOMIC | __GFP_NOWARN);
+}
+
+static inline void sk_psock_free_link(struct sk_psock_link *link)
+{
+	kfree(link);
+}
+
+struct sk_psock_link *sk_psock_link_pop(struct sk_psock *psock);
+#if defined(CONFIG_BPF_STREAM_PARSER)
+void sk_psock_unlink(struct sock *sk, struct sk_psock_link *link);
+#else
+static inline void sk_psock_unlink(struct sock *sk,
+				   struct sk_psock_link *link)
+{
+}
+#endif
+
+void __sk_psock_purge_ingress_msg(struct sk_psock *psock);
+
+static inline void sk_psock_cork_free(struct sk_psock *psock)
+{
+	if (psock->cork) {
+		sk_msg_free(psock->sk, psock->cork);
+		kfree(psock->cork);
+		psock->cork = NULL;
+	}
+}
+
+static inline void sk_psock_update_proto(struct sock *sk,
+					 struct sk_psock *psock,
+					 struct proto *ops)
+{
+	psock->saved_unhash = sk->sk_prot->unhash;
+	psock->saved_close = sk->sk_prot->close;
+	psock->saved_write_space = sk->sk_write_space;
+
+	psock->sk_proto = sk->sk_prot;
+	sk->sk_prot = ops;
+}
+
+static inline void sk_psock_restore_proto(struct sock *sk,
+					  struct sk_psock *psock)
+{
+	if (psock->sk_proto) {
+		sk->sk_prot = psock->sk_proto;
+		psock->sk_proto = NULL;
+	}
+}
+
+static inline void sk_psock_set_state(struct sk_psock *psock,
+				      enum sk_psock_state_bits bit)
+{
+	set_bit(bit, &psock->state);
+}
+
+static inline void sk_psock_clear_state(struct sk_psock *psock,
+					enum sk_psock_state_bits bit)
+{
+	clear_bit(bit, &psock->state);
+}
+
+static inline bool sk_psock_test_state(const struct sk_psock *psock,
+				       enum sk_psock_state_bits bit)
+{
+	return test_bit(bit, &psock->state);
+}
+
+static inline struct sk_psock *sk_psock_get(struct sock *sk)
+{
+	struct sk_psock *psock;
+
+	rcu_read_lock();
+	psock = sk_psock(sk);
+	if (psock && !refcount_inc_not_zero(&psock->refcnt))
+		psock = NULL;
+	rcu_read_unlock();
+	return psock;
+}
+
+void sk_psock_stop(struct sock *sk, struct sk_psock *psock);
+void sk_psock_destroy(struct rcu_head *rcu);
+void sk_psock_drop(struct sock *sk, struct sk_psock *psock);
+
+static inline void sk_psock_put(struct sock *sk, struct sk_psock *psock)
+{
+	if (refcount_dec_and_test(&psock->refcnt))
+		sk_psock_drop(sk, psock);
+}
+
+static inline void psock_set_prog(struct bpf_prog **pprog,
+				  struct bpf_prog *prog)
+{
+	prog = xchg(pprog, prog);
+	if (prog)
+		bpf_prog_put(prog);
+}
+
+static inline void psock_progs_drop(struct sk_psock_progs *progs)
+{
+	psock_set_prog(&progs->msg_parser, NULL);
+	psock_set_prog(&progs->skb_parser, NULL);
+	psock_set_prog(&progs->skb_verdict, NULL);
+}
+
+#endif /* _LINUX_SKMSG_H */
