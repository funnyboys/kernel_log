commit 63960260457a02af2a6cb35d75e6bdb17299c882
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Jul 2 15:45:23 2020 -0700

    bpf: Check correct cred for CAP_SYSLOG in bpf_dump_raw_ok()
    
    When evaluating access control over kallsyms visibility, credentials at
    open() time need to be used, not the "current" creds (though in BPF's
    case, this has likely always been the same). Plumb access to associated
    file->f_cred down through bpf_dump_raw_ok() and its callers now that
    kallsysm_show_value() has been refactored to take struct cred.
    
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: bpf@vger.kernel.org
    Cc: stable@vger.kernel.org
    Fixes: 7105e828c087 ("bpf: allow for correlation of maps and helpers in dump")
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 55104f6c78e8..0b0144752d78 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -884,12 +884,12 @@ void bpf_jit_compile(struct bpf_prog *prog);
 bool bpf_jit_needs_zext(void);
 bool bpf_helper_changes_pkt_data(void *func);
 
-static inline bool bpf_dump_raw_ok(void)
+static inline bool bpf_dump_raw_ok(const struct cred *cred)
 {
 	/* Reconstruction of call-sites is dependent on kallsyms,
 	 * thus make dump the same restriction.
 	 */
-	return kallsyms_show_value(current_cred());
+	return kallsyms_show_value(cred);
 }
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,

commit 160251842cd35a75edfb0a1d76afa3eb674ff40a
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Jul 2 11:49:23 2020 -0700

    kallsyms: Refactor kallsyms_show_value() to take cred
    
    In order to perform future tests against the cred saved during open(),
    switch kallsyms_show_value() to operate on a cred, and have all current
    callers pass current_cred(). This makes it very obvious where callers
    are checking the wrong credential in their "read" contexts. These will
    be fixed in the coming patches.
    
    Additionally switch return value to bool, since it is always used as a
    direct permission check, not a 0-on-success, negative-on-error style
    function return.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 259377723603..55104f6c78e8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -889,7 +889,7 @@ static inline bool bpf_dump_raw_ok(void)
 	/* Reconstruction of call-sites is dependent on kallsyms,
 	 * thus make dump the same restriction.
 	 */
-	return kallsyms_show_value() == 1;
+	return kallsyms_show_value(current_cred());
 }
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,

commit cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2
Merge: 2e63f6ce7ed2 065fcfd49763
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 16:27:18 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
    
     1) Allow setting bluetooth L2CAP modes via socket option, from Luiz
        Augusto von Dentz.
    
     2) Add GSO partial support to igc, from Sasha Neftin.
    
     3) Several cleanups and improvements to r8169 from Heiner Kallweit.
    
     4) Add IF_OPER_TESTING link state and use it when ethtool triggers a
        device self-test. From Andrew Lunn.
    
     5) Start moving away from custom driver versions, use the globally
        defined kernel version instead, from Leon Romanovsky.
    
     6) Support GRO vis gro_cells in DSA layer, from Alexander Lobakin.
    
     7) Allow hard IRQ deferral during NAPI, from Eric Dumazet.
    
     8) Add sriov and vf support to hinic, from Luo bin.
    
     9) Support Media Redundancy Protocol (MRP) in the bridging code, from
        Horatiu Vultur.
    
    10) Support netmap in the nft_nat code, from Pablo Neira Ayuso.
    
    11) Allow UDPv6 encapsulation of ESP in the ipsec code, from Sabrina
        Dubroca. Also add ipv6 support for espintcp.
    
    12) Lots of ReST conversions of the networking documentation, from Mauro
        Carvalho Chehab.
    
    13) Support configuration of ethtool rxnfc flows in bcmgenet driver,
        from Doug Berger.
    
    14) Allow to dump cgroup id and filter by it in inet_diag code, from
        Dmitry Yakunin.
    
    15) Add infrastructure to export netlink attribute policies to
        userspace, from Johannes Berg.
    
    16) Several optimizations to sch_fq scheduler, from Eric Dumazet.
    
    17) Fallback to the default qdisc if qdisc init fails because otherwise
        a packet scheduler init failure will make a device inoperative. From
        Jesper Dangaard Brouer.
    
    18) Several RISCV bpf jit optimizations, from Luke Nelson.
    
    19) Correct the return type of the ->ndo_start_xmit() method in several
        drivers, it's netdev_tx_t but many drivers were using
        'int'. From Yunjian Wang.
    
    20) Add an ethtool interface for PHY master/slave config, from Oleksij
        Rempel.
    
    21) Add BPF iterators, from Yonghang Song.
    
    22) Add cable test infrastructure, including ethool interfaces, from
        Andrew Lunn. Marvell PHY driver is the first to support this
        facility.
    
    23) Remove zero-length arrays all over, from Gustavo A. R. Silva.
    
    24) Calculate and maintain an explicit frame size in XDP, from Jesper
        Dangaard Brouer.
    
    25) Add CAP_BPF, from Alexei Starovoitov.
    
    26) Support terse dumps in the packet scheduler, from Vlad Buslov.
    
    27) Support XDP_TX bulking in dpaa2 driver, from Ioana Ciornei.
    
    28) Add devm_register_netdev(), from Bartosz Golaszewski.
    
    29) Minimize qdisc resets, from Cong Wang.
    
    30) Get rid of kernel_getsockopt and kernel_setsockopt in order to
        eliminate set_fs/get_fs calls. From Christoph Hellwig.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2517 commits)
      selftests: net: ip_defrag: ignore EPERM
      net_failover: fixed rollback in net_failover_open()
      Revert "tipc: Fix potential tipc_aead refcnt leak in tipc_crypto_rcv"
      Revert "tipc: Fix potential tipc_node refcnt leak in tipc_rcv"
      vmxnet3: allow rx flow hash ops only when rss is enabled
      hinic: add set_channels ethtool_ops support
      selftests/bpf: Add a default $(CXX) value
      tools/bpf: Don't use $(COMPILE.c)
      bpf, selftests: Use bpf_probe_read_kernel
      s390/bpf: Use bcr 0,%0 as tail call nop filler
      s390/bpf: Maintain 8-byte stack alignment
      selftests/bpf: Fix verifier test
      selftests/bpf: Fix sample_cnt shared between two threads
      bpf, selftests: Adapt cls_redirect to call csum_level helper
      bpf: Add csum_level helper for fixing up csum levels
      bpf: Fix up bpf_skb_adjust_room helper's skb csum setting
      sfc: add missing annotation for efx_ef10_try_update_nic_stats_vf()
      crypto/chtls: IPv6 support for inline TLS
      Crypto/chcr: Fixes a coccinile check error
      Crypto/chcr: Fixes compilations warnings
      ...

commit 228c4f265c6eb60eaa4ed0edb3bf7c113173576c
Author: Eric Biggers <ebiggers@google.com>
Date:   Sat May 2 11:24:27 2020 -0700

    crypto: lib/sha1 - fold linux/cryptohash.h into crypto/sha.h
    
    <linux/cryptohash.h> sounds very generic and important, like it's the
    header to include if you're doing cryptographic hashing in the kernel.
    But actually it only includes the library implementation of the SHA-1
    compression function (not even the full SHA-1).  This should basically
    never be used anymore; SHA-1 is no longer considered secure, and there
    are much better ways to do cryptographic hashing in the kernel.
    
    Remove this header and fold it into <crypto/sha.h> which already
    contains constants and functions for SHA-1 (along with SHA-2).
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f42662adffe4..ec45fd7992c9 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -16,11 +16,11 @@
 #include <linux/workqueue.h>
 #include <linux/sched.h>
 #include <linux/capability.h>
-#include <linux/cryptohash.h>
 #include <linux/set_memory.h>
 #include <linux/kallsyms.h>
 #include <linux/if_vlan.h>
 #include <linux/vmalloc.h>
+#include <crypto/sha.h>
 
 #include <net/sch_generic.h>
 

commit 6b0b0fa2bce61db790efc8070ae6e5696435b0a8
Author: Eric Biggers <ebiggers@google.com>
Date:   Sat May 2 11:24:25 2020 -0700

    crypto: lib/sha1 - rename "sha" to "sha1"
    
    The library implementation of the SHA-1 compression function is
    confusingly called just "sha_transform()".  Alongside it are some "SHA_"
    constants and "sha_init()".  Presumably these are left over from a time
    when SHA just meant SHA-1.  But now there are also SHA-2 and SHA-3, and
    moreover SHA-1 is now considered insecure and thus shouldn't be used.
    
    Therefore, rename these functions and constants to make it very clear
    that they are for SHA-1.  Also add a comment to make it clear that these
    shouldn't be used.
    
    For the extra-misleadingly named "SHA_MESSAGE_BYTES", rename it to
    SHA1_BLOCK_SIZE and define it to just '64' rather than '(512/8)' so that
    it matches the same definition in <crypto/sha.h>.  This prepares for
    merging <linux/cryptohash.h> into <crypto/sha.h>.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9b5aa5c483cc..f42662adffe4 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -746,7 +746,7 @@ static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
 static inline u32 bpf_prog_tag_scratch_size(const struct bpf_prog *prog)
 {
 	return round_up(bpf_prog_insn_size(prog) +
-			sizeof(__be64) + 1, SHA_MESSAGE_BYTES);
+			sizeof(__be64) + 1, SHA1_BLOCK_SIZE);
 }
 
 static inline unsigned int bpf_prog_size(unsigned int proglen)

commit d26c0cc53950464a24adfa76867f1d71f0cbbea6
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Apr 30 23:30:47 2020 +0200

    bpf: Avoid gcc-10 stringop-overflow warning in struct bpf_prog
    
    gcc-10 warns about accesses to zero-length arrays:
    
    kernel/bpf/core.c: In function 'bpf_patch_insn_single':
    cc1: warning: writing 8 bytes into a region of size 0 [-Wstringop-overflow=]
    In file included from kernel/bpf/core.c:21:
    include/linux/filter.h:550:20: note: at offset 0 to object 'insnsi' with size 0 declared here
      550 |   struct bpf_insn  insnsi[0];
          |                    ^~~~~~
    
    In this case, we really want to have two flexible-array members,
    but that is not possible. Removing the union to make insnsi a
    flexible-array member while leaving insns as a zero-length array
    fixes the warning, as nothing writes to the other one in that way.
    
    This trick only works on linux-3.18 or higher, as older versions
    had additional members in the union.
    
    Fixes: 60a3b2253c41 ("net: bpf: make eBPF interpreter images read-only")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200430213101.135134-6-arnd@arndb.de

diff --git a/include/linux/filter.h b/include/linux/filter.h
index af37318bb1c5..73d06a39e2d6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -545,10 +545,8 @@ struct bpf_prog {
 	unsigned int		(*bpf_func)(const void *ctx,
 					    const struct bpf_insn *insn);
 	/* Instructions for interpreter */
-	union {
-		struct sock_filter	insns[0];
-		struct bpf_insn		insnsi[0];
-	};
+	struct sock_filter	insns[0];
+	struct bpf_insn		insnsi[];
 };
 
 struct sk_filter {

commit 6890896bd765b0504761c61901c9804fca23bfb2
Author: Stanislav Fomichev <sdf@google.com>
Date:   Fri Apr 24 16:59:41 2020 -0700

    bpf: Fix missing bpf_base_func_proto in cgroup_base_func_proto for CGROUP_NET=n
    
    linux-next build bot reported compile issue [1] with one of its
    configs. It looks like when we have CONFIG_NET=n and
    CONFIG_BPF{,_SYSCALL}=y, we are missing the bpf_base_func_proto
    definition (from net/core/filter.c) in cgroup_base_func_proto.
    
    I'm reshuffling the code a bit to make it work. The common helpers
    are moved into kernel/bpf/helpers.c and the bpf_base_func_proto is
    exported from there.
    Also, bpf_get_raw_cpu_id goes into kernel/bpf/core.c akin to existing
    bpf_user_rnd_u32.
    
    [1] https://lore.kernel.org/linux-next/CAKH8qBsBvKHswiX1nx40LgO+BGeTmb1NX8tiTttt_0uu6T3dCA@mail.gmail.com/T/#mff8b0c083314c68c2e2ef0211cb11bc20dc13c72
    
    Fixes: 0456ea170cd6 ("bpf: Enable more helpers for BPF_PROG_TYPE_CGROUP_{DEVICE,SYSCTL,SOCKOPT}")
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200424235941.58382-1-sdf@google.com

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9b5aa5c483cc..af37318bb1c5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -863,8 +863,6 @@ int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
 			      bpf_aux_classic_check_t trans, bool save_orig);
 void bpf_prog_destroy(struct bpf_prog *fp);
-const struct bpf_func_proto *
-bpf_base_func_proto(enum bpf_func_id func_id);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);

commit bfea9a8574f34597581f74f792d044d38497b775
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Thu Mar 12 20:55:59 2020 +0100

    bpf: Add name to struct bpf_ksym
    
    Adding name to 'struct bpf_ksym' object to carry the name
    of the symbol for bpf_prog, bpf_trampoline, bpf_dispatcher
    objects.
    
    The current benefit is that name is now generated only when
    the symbol is added to the list, so we don't need to generate
    it every time it's accessed.
    
    The future benefit is that we will have all the bpf objects
    symbols represented by struct bpf_ksym.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200312195610.346362-5-jolsa@kernel.org
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6249679275b3..9b5aa5c483cc 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1083,7 +1083,6 @@ bpf_address_lookup(unsigned long addr, unsigned long *size,
 
 void bpf_prog_kallsyms_add(struct bpf_prog *fp);
 void bpf_prog_kallsyms_del(struct bpf_prog *fp);
-void bpf_get_prog_name(const struct bpf_prog *prog, char *sym);
 
 #else /* CONFIG_BPF_JIT */
 
@@ -1152,11 +1151,6 @@ static inline void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 {
 }
 
-static inline void bpf_get_prog_name(const struct bpf_prog *prog, char *sym)
-{
-	sym[0] = '\0';
-}
-
 #endif /* CONFIG_BPF_JIT */
 
 void bpf_prog_kallsyms_del_all(struct bpf_prog *fp);

commit 6a64037d4bf252bb8cf13917320c8e001da8997a
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Thu Mar 12 20:55:57 2020 +0100

    bpf: Add bpf_trampoline_ name prefix for DECLARE_BPF_DISPATCHER
    
    Adding bpf_trampoline_ name prefix for DECLARE_BPF_DISPATCHER,
    so all the dispatchers have the common name prefix.
    
    And also a small '_' cleanup for bpf_dispatcher_nopfunc function
    name.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200312195610.346362-3-jolsa@kernel.org
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 43b5e455d2f5..6249679275b3 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -577,7 +577,7 @@ DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
 	ret; })
 
 #define BPF_PROG_RUN(prog, ctx)						\
-	__BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc)
+	__BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nop_func)
 
 /*
  * Use in preemptible and therefore migratable context to make sure that
@@ -596,7 +596,7 @@ static inline u32 bpf_prog_run_pin_on_cpu(const struct bpf_prog *prog,
 	u32 ret;
 
 	migrate_disable();
-	ret = __BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc);
+	ret = __BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nop_func);
 	migrate_enable();
 	return ret;
 }
@@ -722,7 +722,7 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return res;
 }
 
-DECLARE_BPF_DISPATCHER(bpf_dispatcher_xdp)
+DECLARE_BPF_DISPATCHER(xdp)
 
 static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 					    struct xdp_buff *xdp)
@@ -733,8 +733,7 @@ static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 	 * already takes rcu_read_lock() when fetching the program, so
 	 * it's not necessary here anymore.
 	 */
-	return __BPF_PROG_RUN(prog, xdp,
-			      BPF_DISPATCHER_FUNC(bpf_dispatcher_xdp));
+	return __BPF_PROG_RUN(prog, xdp, BPF_DISPATCHER_FUNC(xdp));
 }
 
 void bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog);

commit 2a916f2f546ca1c1e3323e2a4269307f6d9890eb
Author: David Miller <davem@davemloft.net>
Date:   Mon Feb 24 15:01:46 2020 +0100

    bpf: Use migrate_disable/enable in array macros and cgroup/lirc code.
    
    Replace the preemption disable/enable with migrate_disable/enable() to
    reflect the actual requirement and to allow PREEMPT_RT to substitute it
    with an actual migration disable mechanism which does not disable
    preemption.
    
    Including the code paths that go via __bpf_prog_run_save_cb().
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145643.998293311@linutronix.de

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9270de2a0df8..43b5e455d2f5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -677,6 +677,7 @@ static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 	return qdisc_skb_cb(skb)->data;
 }
 
+/* Must be invoked with migration disabled */
 static inline u32 __bpf_prog_run_save_cb(const struct bpf_prog *prog,
 					 struct sk_buff *skb)
 {
@@ -702,9 +703,9 @@ static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
 {
 	u32 res;
 
-	preempt_disable();
+	migrate_disable();
 	res = __bpf_prog_run_save_cb(prog, skb);
-	preempt_enable();
+	migrate_enable();
 	return res;
 }
 

commit 3d9f773cf2876c01a505b9fe27270901d464e90a
Author: David Miller <davem@davemloft.net>
Date:   Mon Feb 24 15:01:43 2020 +0100

    bpf: Use bpf_prog_run_pin_on_cpu() at simple call sites.
    
    All of these cases are strictly of the form:
    
            preempt_disable();
            BPF_PROG_RUN(...);
            preempt_enable();
    
    Replace this with bpf_prog_run_pin_on_cpu() which wraps BPF_PROG_RUN()
    with:
    
            migrate_disable();
            BPF_PROG_RUN(...);
            migrate_enable();
    
    On non RT enabled kernels this maps to preempt_disable/enable() and on RT
    enabled kernels this solely prevents migration, which is sufficient as
    there is no requirement to prevent reentrancy to any BPF program from a
    preempting task. The only requirement is that the program stays on the same
    CPU.
    
    Therefore, this is a trivially correct transformation.
    
    The seccomp loop does not need protection over the loop. It only needs
    protection per BPF filter program
    
    [ tglx: Converted to bpf_prog_run_pin_on_cpu() ]
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145643.691493094@linutronix.de

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1982a52eb4c9..9270de2a0df8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -717,9 +717,7 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	if (unlikely(prog->cb_access))
 		memset(cb_data, 0, BPF_SKB_CB_LEN);
 
-	preempt_disable();
-	res = BPF_PROG_RUN(prog, skb);
-	preempt_enable();
+	res = bpf_prog_run_pin_on_cpu(prog, skb);
 	return res;
 }
 

commit 37e1d9202225635772b32e340294208367279c2b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 24 15:01:42 2020 +0100

    bpf: Replace cant_sleep() with cant_migrate()
    
    As already discussed in the previous change which introduced
    BPF_RUN_PROG_PIN_ON_CPU() BPF only requires to disable migration to
    guarantee per CPUness.
    
    If RT substitutes the preempt disable based migration protection then the
    cant_sleep() check will obviously trigger as preemption is not disabled.
    
    Replace it by cant_migrate() which maps to cant_sleep() on a non RT kernel
    and will verify that migration is disabled on a full RT kernel.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145643.583038889@linutronix.de

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 38f60188bb26..1982a52eb4c9 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -561,7 +561,7 @@ DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
 
 #define __BPF_PROG_RUN(prog, ctx, dfunc)	({			\
 	u32 ret;							\
-	cant_sleep();							\
+	cant_migrate();							\
 	if (static_branch_unlikely(&bpf_stats_enabled_key)) {		\
 		struct bpf_prog_stats *stats;				\
 		u64 start = sched_clock();				\

commit 3c58482a382bae89410439247152eb342e9872f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 24 15:01:41 2020 +0100

    bpf: Provide bpf_prog_run_pin_on_cpu() helper
    
    BPF programs require to run on one CPU to completion as they use per CPU
    storage, but according to Alexei they don't need reentrancy protection as
    obviously BPF programs running in thread context can always be 'preempted'
    by hard and soft interrupts and instrumentation and the same program can
    run concurrently on a different CPU.
    
    The currently used mechanism to ensure CPUness is to wrap the invocation
    into a preempt_disable/enable() pair. Disabling preemption is also
    disabling migration for a task.
    
    preempt_disable/enable() is used because there is no explicit way to
    reliably disable only migration.
    
    Provide a separate macro to invoke a BPF program which can be used in
    migrateable task context.
    
    It wraps BPF_PROG_RUN() in a migrate_disable/enable() pair which maps on
    non RT enabled kernels to preempt_disable/enable(). On RT enabled kernels
    this merely disables migration. Both methods ensure that the invoked BPF
    program runs on one CPU to completion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145643.474592620@linutronix.de

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f349e2c0884c..38f60188bb26 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -576,8 +576,30 @@ DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
 	}								\
 	ret; })
 
-#define BPF_PROG_RUN(prog, ctx) __BPF_PROG_RUN(prog, ctx,		\
-					       bpf_dispatcher_nopfunc)
+#define BPF_PROG_RUN(prog, ctx)						\
+	__BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc)
+
+/*
+ * Use in preemptible and therefore migratable context to make sure that
+ * the execution of the BPF program runs on one CPU.
+ *
+ * This uses migrate_disable/enable() explicitly to document that the
+ * invocation of a BPF program does not require reentrancy protection
+ * against a BPF program which is invoked from a preempting task.
+ *
+ * For non RT enabled kernels migrate_disable/enable() maps to
+ * preempt_disable/enable(), i.e. it disables also preemption.
+ */
+static inline u32 bpf_prog_run_pin_on_cpu(const struct bpf_prog *prog,
+					  const void *ctx)
+{
+	u32 ret;
+
+	migrate_disable();
+	ret = __BPF_PROG_RUN(prog, ctx, bpf_dispatcher_nopfunc);
+	migrate_enable();
+	return ret;
+}
 
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 

commit 1d233886dd904edbf239eeffe435c3308ae97625
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Thu Jan 16 16:14:45 2020 +0100

    xdp: Use bulking for non-map XDP_REDIRECT and consolidate code paths
    
    Since the bulk queue used by XDP_REDIRECT now lives in struct net_device,
    we can re-use the bulking for the non-map version of the bpf_redirect()
    helper. This is a simple matter of having xdp_do_redirect_slow() queue the
    frame on the bulk queue instead of sending it out with __bpf_tx_xdp().
    
    Unfortunately we can't make the bpf_redirect() helper return an error if
    the ifindex doesn't exit (as bpf_redirect_map() does), because we don't
    have a reference to the network namespace of the ingress device at the time
    the helper is called. So we have to leave it as-is and keep the device
    lookup in xdp_do_redirect_slow().
    
    Since this leaves less reason to have the non-map redirect code in a
    separate function, so we get rid of the xdp_do_redirect_slow() function
    entirely. This does lose us the tracepoint disambiguation, but fortunately
    the xdp_redirect and xdp_redirect_map tracepoints use the same tracepoint
    entry structures. This means both can contain a map index, so we can just
    amend the tracepoint definitions so we always emit the xdp_redirect(_err)
    tracepoints, but with the map ID only populated if a map is present. This
    means we retire the xdp_redirect_map(_err) tracepoints entirely, but keep
    the definitions around in case someone is still listening for them.
    
    With this change, the performance of the xdp_redirect sample program goes
    from 5Mpps to 8.4Mpps (a 68% increase).
    
    Since the flush functions are no longer map-specific, rename the flush()
    functions to drop _map from their names. One of the renamed functions is
    the xdp_do_flush_map() callback used in all the xdp-enabled drivers. To
    keep from having to update all drivers, use a #define to keep the old name
    working, and only update the virtual drivers in this patch.
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/157918768505.1458396.17518057312953572912.stgit@toke.dk

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a366a0b64a57..f349e2c0884c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -918,7 +918,7 @@ static inline int xdp_ok_fwd_dev(const struct net_device *fwd,
 	return 0;
 }
 
-/* The pair of xdp_do_redirect and xdp_do_flush_map MUST be called in the
+/* The pair of xdp_do_redirect and xdp_do_flush MUST be called in the
  * same cpu context. Further for best results no more than a single map
  * for the do_redirect/do_flush pair should be used. This limitation is
  * because we only track one map and force a flush when the map changes.
@@ -929,7 +929,13 @@ int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,
 int xdp_do_redirect(struct net_device *dev,
 		    struct xdp_buff *xdp,
 		    struct bpf_prog *prog);
-void xdp_do_flush_map(void);
+void xdp_do_flush(void);
+
+/* The xdp_do_flush_map() helper has been renamed to drop the _map suffix, as
+ * it is no longer only flushing maps. Keep this define for compatibility
+ * until all drivers are updated - do not use xdp_do_flush_map() in new code!
+ */
+#define xdp_do_flush_map xdp_do_flush
 
 void bpf_warn_invalid_xdp_action(u32 act);
 

commit 0baf26b0fcd74bbfcef53c5d5e8bad2b99c8d0d2
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:08 2020 -0800

    bpf: tcp: Support tcp_congestion_ops in bpf
    
    This patch makes "struct tcp_congestion_ops" to be the first user
    of BPF STRUCT_OPS.  It allows implementing a tcp_congestion_ops
    in bpf.
    
    The BPF implemented tcp_congestion_ops can be used like
    regular kernel tcp-cc through sysctl and setsockopt.  e.g.
    [root@arch-fb-vm1 bpf]# sysctl -a | egrep congestion
    net.ipv4.tcp_allowed_congestion_control = reno cubic bpf_cubic
    net.ipv4.tcp_available_congestion_control = reno bic cubic bpf_cubic
    net.ipv4.tcp_congestion_control = bpf_cubic
    
    There has been attempt to move the TCP CC to the user space
    (e.g. CCP in TCP).   The common arguments are faster turn around,
    get away from long-tail kernel versions in production...etc,
    which are legit points.
    
    BPF has been the continuous effort to join both kernel and
    userspace upsides together (e.g. XDP to gain the performance
    advantage without bypassing the kernel).  The recent BPF
    advancements (in particular BTF-aware verifier, BPF trampoline,
    BPF CO-RE...) made implementing kernel struct ops (e.g. tcp cc)
    possible in BPF.  It allows a faster turnaround for testing algorithm
    in the production while leveraging the existing (and continue growing)
    BPF feature/framework instead of building one specifically for
    userspace TCP CC.
    
    This patch allows write access to a few fields in tcp-sock
    (in bpf_tcp_ca_btf_struct_access()).
    
    The optional "get_info" is unsupported now.  It can be added
    later.  One possible way is to output the info with a btf-id
    to describe the content.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003508.3856115-1-kafai@fb.com

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 70e6dd960bca..a366a0b64a57 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -843,6 +843,8 @@ int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
 			      bpf_aux_classic_check_t trans, bool save_orig);
 void bpf_prog_destroy(struct bpf_prog *fp);
+const struct bpf_func_proto *
+bpf_base_func_proto(enum bpf_func_id func_id);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);

commit 2bbc078f812d45b8decb55935dab21199bd21489
Merge: 9e41fbf3dd38 7c8dce4b1661
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 27 14:20:10 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2019-12-27
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 127 non-merge commits during the last 17 day(s) which contain
    a total of 110 files changed, 6901 insertions(+), 2721 deletions(-).
    
    There are three merge conflicts. Conflicts and resolution looks as follows:
    
    1) Merge conflict in net/bpf/test_run.c:
    
    There was a tree-wide cleanup c593642c8be0 ("treewide: Use sizeof_field() macro")
    which gets in the way with b590cb5f802d ("bpf: Switch to offsetofend in
    BPF_PROG_TEST_RUN"):
    
      <<<<<<< HEAD
              if (!range_is_zero(__skb, offsetof(struct __sk_buff, priority) +
                                 sizeof_field(struct __sk_buff, priority),
      =======
              if (!range_is_zero(__skb, offsetofend(struct __sk_buff, priority),
      >>>>>>> 7c8dce4b166113743adad131b5a24c4acc12f92c
    
    There are a few occasions that look similar to this. Always take the chunk with
    offsetofend(). Note that there is one where the fields differ in here:
    
      <<<<<<< HEAD
              if (!range_is_zero(__skb, offsetof(struct __sk_buff, tstamp) +
                                 sizeof_field(struct __sk_buff, tstamp),
      =======
              if (!range_is_zero(__skb, offsetofend(struct __sk_buff, gso_segs),
      >>>>>>> 7c8dce4b166113743adad131b5a24c4acc12f92c
    
    Just take the one with offsetofend() /and/ gso_segs. Latter is correct due to
    850a88cc4096 ("bpf: Expose __sk_buff wire_len/gso_segs to BPF_PROG_TEST_RUN").
    
    2) Merge conflict in arch/riscv/net/bpf_jit_comp.c:
    
    (I'm keeping Bjorn in Cc here for a double-check in case I got it wrong.)
    
      <<<<<<< HEAD
              if (is_13b_check(off, insn))
                      return -1;
              emit(rv_blt(tcc, RV_REG_ZERO, off >> 1), ctx);
      =======
              emit_branch(BPF_JSLT, RV_REG_T1, RV_REG_ZERO, off, ctx);
      >>>>>>> 7c8dce4b166113743adad131b5a24c4acc12f92c
    
    Result should look like:
    
              emit_branch(BPF_JSLT, tcc, RV_REG_ZERO, off, ctx);
    
    3) Merge conflict in arch/riscv/include/asm/pgtable.h:
    
      <<<<<<< HEAD
      =======
      #define VMALLOC_SIZE     (KERN_VIRT_SIZE >> 1)
      #define VMALLOC_END      (PAGE_OFFSET - 1)
      #define VMALLOC_START    (PAGE_OFFSET - VMALLOC_SIZE)
    
      #define BPF_JIT_REGION_SIZE     (SZ_128M)
      #define BPF_JIT_REGION_START    (PAGE_OFFSET - BPF_JIT_REGION_SIZE)
      #define BPF_JIT_REGION_END      (VMALLOC_END)
    
      /*
       * Roughly size the vmemmap space to be large enough to fit enough
       * struct pages to map half the virtual address space. Then
       * position vmemmap directly below the VMALLOC region.
       */
      #define VMEMMAP_SHIFT \
              (CONFIG_VA_BITS - PAGE_SHIFT - 1 + STRUCT_PAGE_MAX_SHIFT)
      #define VMEMMAP_SIZE    BIT(VMEMMAP_SHIFT)
      #define VMEMMAP_END     (VMALLOC_START - 1)
      #define VMEMMAP_START   (VMALLOC_START - VMEMMAP_SIZE)
    
      #define vmemmap         ((struct page *)VMEMMAP_START)
    
      >>>>>>> 7c8dce4b166113743adad131b5a24c4acc12f92c
    
    Only take the BPF_* defines from there and move them higher up in the
    same file. Remove the rest from the chunk. The VMALLOC_* etc defines
    got moved via 01f52e16b868 ("riscv: define vmemmap before pfn_to_page
    calls"). Result:
    
      [...]
      #define __S101  PAGE_READ_EXEC
      #define __S110  PAGE_SHARED_EXEC
      #define __S111  PAGE_SHARED_EXEC
    
      #define VMALLOC_SIZE     (KERN_VIRT_SIZE >> 1)
      #define VMALLOC_END      (PAGE_OFFSET - 1)
      #define VMALLOC_START    (PAGE_OFFSET - VMALLOC_SIZE)
    
      #define BPF_JIT_REGION_SIZE     (SZ_128M)
      #define BPF_JIT_REGION_START    (PAGE_OFFSET - BPF_JIT_REGION_SIZE)
      #define BPF_JIT_REGION_END      (VMALLOC_END)
    
      /*
       * Roughly size the vmemmap space to be large enough to fit enough
       * struct pages to map half the virtual address space. Then
       * position vmemmap directly below the VMALLOC region.
       */
      #define VMEMMAP_SHIFT \
              (CONFIG_VA_BITS - PAGE_SHIFT - 1 + STRUCT_PAGE_MAX_SHIFT)
      #define VMEMMAP_SIZE    BIT(VMEMMAP_SHIFT)
      #define VMEMMAP_END     (VMALLOC_START - 1)
      #define VMEMMAP_START   (VMALLOC_START - VMEMMAP_SIZE)
    
      [...]
    
    Let me know if there are any other issues.
    
    Anyway, the main changes are:
    
    1) Extend bpftool to produce a struct (aka "skeleton") tailored and specific
       to a provided BPF object file. This provides an alternative, simplified API
       compared to standard libbpf interaction. Also, add libbpf extern variable
       resolution for .kconfig section to import Kconfig data, from Andrii Nakryiko.
    
    2) Add BPF dispatcher for XDP which is a mechanism to avoid indirect calls by
       generating a branch funnel as discussed back in bpfconf'19 at LSF/MM. Also,
       add various BPF riscv JIT improvements, from Björn Töpel.
    
    3) Extend bpftool to allow matching BPF programs and maps by name,
       from Paul Chaignon.
    
    4) Support for replacing cgroup BPF programs attached with BPF_F_ALLOW_MULTI
       flag for allowing updates without service interruption, from Andrey Ignatov.
    
    5) Cleanup and simplification of ring access functions for AF_XDP with a
       bonus of 0-5% performance improvement, from Magnus Karlsson.
    
    6) Enable BPF JITs for x86-64 and arm64 by default. Also, final version of
       audit support for BPF, from Daniel Borkmann and latter with Jiri Olsa.
    
    7) Move and extend test_select_reuseport into BPF program tests under
       BPF selftests, from Jakub Sitnicki.
    
    8) Various BPF sample improvements for xdpsock for customizing parameters
       to set up and benchmark AF_XDP, from Jay Jayatheerthan.
    
    9) Improve libbpf to provide a ulimit hint on permission denied errors.
       Also change XDP sample programs to attach in driver mode by default,
       from Toke Høiland-Jørgensen.
    
    10) Extend BPF test infrastructure to allow changing skb mark from tc BPF
        programs, from Nikita V. Shirokov.
    
    11) Optimize prologue code sequence in BPF arm32 JIT, from Russell King.
    
    12) Fix xdp_redirect_cpu BPF sample to manually attach to tracepoints after
        libbpf conversion, from Jesper Dangaard Brouer.
    
    13) Minor misc improvements from various others.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 332f22a60e4c3492d4953cd6f7aaa4e8bd0bba97
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Thu Dec 19 07:10:05 2019 +0100

    xdp: Remove map_to_flush and map swap detection
    
    Now that all XDP maps that can be used with bpf_redirect_map() tracks
    entries to be flushed in a global fashion, there is not need to track
    that the map has changed and flush from xdp_do_generic_map()
    anymore. All entries will be flushed in xdp_do_flush_map().
    
    This means that the map_to_flush can be removed, and the corresponding
    checks. Moving the flush logic to one place, xdp_do_flush_map(), give
    a bulking behavior and performance boost.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Link: https://lore.kernel.org/bpf/20191219061006.21980-8-bjorn.topel@gmail.com

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 37ac7025031d..69d6706fc889 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -592,7 +592,6 @@ struct bpf_redirect_info {
 	u32 tgt_index;
 	void *tgt_value;
 	struct bpf_map *map;
-	struct bpf_map *map_to_flush;
 	u32 kern_flags;
 };
 

commit 7e6897f95935973c3253fd756135b5ea58043dc8
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Fri Dec 13 18:51:09 2019 +0100

    bpf, xdp: Start using the BPF dispatcher for XDP
    
    This commit adds a BPF dispatcher for XDP. The dispatcher is updated
    from the XDP control-path, dev_xdp_install(), and used when an XDP
    program is run via bpf_prog_run_xdp().
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191213175112.30208-4-bjorn.topel@gmail.com

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a141cb07e76a..37ac7025031d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -559,23 +559,26 @@ struct sk_filter {
 
 DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
 
-#define BPF_PROG_RUN(prog, ctx)	({				\
-	u32 ret;						\
-	cant_sleep();						\
-	if (static_branch_unlikely(&bpf_stats_enabled_key)) {	\
-		struct bpf_prog_stats *stats;			\
-		u64 start = sched_clock();			\
-		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
-		stats = this_cpu_ptr(prog->aux->stats);		\
-		u64_stats_update_begin(&stats->syncp);		\
-		stats->cnt++;					\
-		stats->nsecs += sched_clock() - start;		\
-		u64_stats_update_end(&stats->syncp);		\
-	} else {						\
-		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
-	}							\
+#define __BPF_PROG_RUN(prog, ctx, dfunc)	({			\
+	u32 ret;							\
+	cant_sleep();							\
+	if (static_branch_unlikely(&bpf_stats_enabled_key)) {		\
+		struct bpf_prog_stats *stats;				\
+		u64 start = sched_clock();				\
+		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+		stats = this_cpu_ptr(prog->aux->stats);			\
+		u64_stats_update_begin(&stats->syncp);			\
+		stats->cnt++;						\
+		stats->nsecs += sched_clock() - start;			\
+		u64_stats_update_end(&stats->syncp);			\
+	} else {							\
+		ret = dfunc(ctx, (prog)->insnsi, (prog)->bpf_func);	\
+	}								\
 	ret; })
 
+#define BPF_PROG_RUN(prog, ctx) __BPF_PROG_RUN(prog, ctx,		\
+					       bpf_dispatcher_nopfunc)
+
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 
 struct bpf_skb_data_end {
@@ -699,6 +702,8 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return res;
 }
 
+DECLARE_BPF_DISPATCHER(bpf_dispatcher_xdp)
+
 static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 					    struct xdp_buff *xdp)
 {
@@ -708,9 +713,12 @@ static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 	 * already takes rcu_read_lock() when fetching the program, so
 	 * it's not necessary here anymore.
 	 */
-	return BPF_PROG_RUN(prog, xdp);
+	return __BPF_PROG_RUN(prog, xdp,
+			      BPF_DISPATCHER_FUNC(bpf_dispatcher_xdp));
 }
 
+void bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog);
+
 static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
 {
 	return prog->len * sizeof(struct bpf_insn);

commit c593642c8be046915ca3a4a300243a68077cd207
Author: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
Date:   Mon Dec 9 10:31:43 2019 -0800

    treewide: Use sizeof_field() macro
    
    Replace all the occurrences of FIELD_SIZEOF() with sizeof_field() except
    at places where these are defined. Later patches will remove the unused
    definition of FIELD_SIZEOF().
    
    This patch is generated using following script:
    
    EXCLUDE_FILES="include/linux/stddef.h|include/linux/kernel.h"
    
    git grep -l -e "\bFIELD_SIZEOF\b" | while read file;
    do
    
            if [[ "$file" =~ $EXCLUDE_FILES ]]; then
                    continue
            fi
            sed -i  -e 's/\bFIELD_SIZEOF\b/sizeof_field/g' $file;
    done
    
    Signed-off-by: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
    Link: https://lore.kernel.org/r/20190924105839.110713-3-pankaj.laxminarayan.bharadiya@intel.com
    Co-developed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: David Miller <davem@davemloft.net> # for net

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a141cb07e76a..345f3748e0fb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -420,7 +420,7 @@ static inline bool insn_is_zext(const struct bpf_insn *insn)
 
 #define BPF_FIELD_SIZEOF(type, field)				\
 	({							\
-		const int __size = bytes_to_bpf_size(FIELD_SIZEOF(type, field)); \
+		const int __size = bytes_to_bpf_size(sizeof_field(type, field)); \
 		BUILD_BUG_ON(__size < 0);			\
 		__size;						\
 	})
@@ -497,7 +497,7 @@ static inline bool insn_is_zext(const struct bpf_insn *insn)
 
 #define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)				\
 	({									\
-		BUILD_BUG_ON(FIELD_SIZEOF(TYPE, MEMBER) != (SIZE));		\
+		BUILD_BUG_ON(sizeof_field(TYPE, MEMBER) != (SIZE));		\
 		*(PTR_SIZE) = (SIZE);						\
 		offsetof(TYPE, MEMBER);						\
 	})
@@ -608,7 +608,7 @@ static inline void bpf_compute_data_pointers(struct sk_buff *skb)
 {
 	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
 
-	BUILD_BUG_ON(sizeof(*cb) > FIELD_SIZEOF(struct sk_buff, cb));
+	BUILD_BUG_ON(sizeof(*cb) > sizeof_field(struct sk_buff, cb));
 	cb->data_meta = skb->data - skb_metadata_len(skb);
 	cb->data_end  = skb->data + skb_headlen(skb);
 }
@@ -646,9 +646,9 @@ static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 	 * attached to sockets, we need to clear the bpf_skb_cb() area
 	 * to not leak previous contents to user space.
 	 */
-	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) != BPF_SKB_CB_LEN);
-	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) !=
-		     FIELD_SIZEOF(struct qdisc_skb_cb, data));
+	BUILD_BUG_ON(sizeof_field(struct __sk_buff, cb) != BPF_SKB_CB_LEN);
+	BUILD_BUG_ON(sizeof_field(struct __sk_buff, cb) !=
+		     sizeof_field(struct qdisc_skb_cb, data));
 
 	return qdisc_skb_cb(skb)->data;
 }

commit e1608f3fa857b600045b6df7f7dadc70eeaa4496
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 29 23:29:11 2019 +0100

    bpf: Avoid setting bpf insns pages read-only when prog is jited
    
    For the case where the interpreter is compiled out or when the prog is jited
    it is completely unnecessary to set the BPF insn pages as read-only. In fact,
    on frequent churn of BPF programs, it could lead to performance degradation of
    the system over time since it would break the direct map down to 4k pages when
    calling set_memory_ro() for the insn buffer on x86-64 / arm64 and there is no
    reverse operation. Thus, avoid breaking up large pages for data maps, and only
    limit this to the module range used by the JIT where it is necessary to set
    the image read-only and executable.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191129222911.3710-1-daniel@iogearbox.net

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1b1e8b8f88da..a141cb07e76a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -776,8 +776,12 @@ bpf_ctx_narrow_access_offset(u32 off, u32 size, u32 size_default)
 
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
-	set_vm_flush_reset_perms(fp);
-	set_memory_ro((unsigned long)fp, fp->pages);
+#ifndef CONFIG_BPF_JIT_ALWAYS_ON
+	if (!fp->jited) {
+		set_vm_flush_reset_perms(fp);
+		set_memory_ro((unsigned long)fp, fp->pages);
+	}
+#endif
 }
 
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)

commit b8cd76ca4ae34731d47cd6a876d912a08efcc240
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Nov 23 21:37:31 2019 +0100

    bpf: Add bpf_jit_blinding_enabled for !CONFIG_BPF_JIT
    
    Add a definition of bpf_jit_blinding_enabled() when CONFIG_BPF_JIT is not set
    in order to fix a recent build regression:
    
      [...]
      CC      kernel/bpf/verifier.o
      CC      kernel/bpf/inode.o
    kernel/bpf/verifier.c: In function ‘fixup_bpf_calls’:
    kernel/bpf/verifier.c:9132:25: error: implicit declaration of function ‘bpf_jit_blinding_enabled’; did you mean ‘bpf_jit_kallsyms_enabled’? [-Werror=implicit-function-declaration]
     9132 |  bool expect_blinding = bpf_jit_blinding_enabled(prog);
          |                         ^~~~~~~~~~~~~~~~~~~~~~~~
          |                         bpf_jit_kallsyms_enabled
      CC      kernel/bpf/helpers.o
      CC      kernel/bpf/hashtab.o
      [...]
    
    Fixes: d2e4c1e6c294 ("bpf: Constant map key tracking for prog array pokes")
    Reported-by: Jakub Sitnicki <jakub@cloudflare.com>
    Reported-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/40baf8f3507cac4851a310578edfb98ce73b5605.1574541375.git.daniel@iogearbox.net

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 796b60d8cc6c..1b1e8b8f88da 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1053,6 +1053,11 @@ static inline bool ebpf_jit_enabled(void)
 	return false;
 }
 
+static inline bool bpf_jit_blinding_enabled(struct bpf_prog *prog)
+{
+	return false;
+}
+
 static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
 {
 	return false;

commit a66886fe6c24ebeeb6dc10fbd9b75158029eacf7
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 22 21:07:57 2019 +0100

    bpf: Add initial poke descriptor table for jit images
    
    Add initial poke table data structures and management to the BPF
    prog that can later be used by JITs. Also add an instance of poke
    specific data for tail call maps; plan for later work is to extend
    this also for BPF static keys.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/1db285ec2ea4207ee0455b3f8e191a4fc58b9ade.1574452833.git.daniel@iogearbox.net

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ad80e9c6111c..796b60d8cc6c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -952,6 +952,9 @@ void *bpf_jit_alloc_exec(unsigned long size);
 void bpf_jit_free_exec(void *addr);
 void bpf_jit_free(struct bpf_prog *fp);
 
+int bpf_jit_add_poke_descriptor(struct bpf_prog *prog,
+				struct bpf_jit_poke_descriptor *poke);
+
 int bpf_jit_get_func_addr(const struct bpf_prog *prog,
 			  const struct bpf_insn *insn, bool extra_pass,
 			  u64 *func_addr, bool *func_addr_fixed);
@@ -1055,6 +1058,13 @@ static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
 	return false;
 }
 
+static inline int
+bpf_jit_add_poke_descriptor(struct bpf_prog *prog,
+			    struct bpf_jit_poke_descriptor *poke)
+{
+	return -ENOTSUPP;
+}
+
 static inline void bpf_jit_free(struct bpf_prog *fp)
 {
 	bpf_prog_unlock_free(fp);

commit b7b3fc8dd95bc02bd30680da258e09dda55270db
Author: Ilya Leoshkevich <iii@linux.ibm.com>
Date:   Fri Nov 15 13:37:22 2019 +0100

    bpf: Support doubleword alignment in bpf_jit_binary_alloc
    
    Currently passing alignment greater than 4 to bpf_jit_binary_alloc does
    not work: in such cases it silently aligns only to 4 bytes.
    
    On s390, in order to load a constant from memory in a large (>512k) BPF
    program, one must use lgrl instruction, whose memory operand must be
    aligned on an 8-byte boundary.
    
    This patch makes it possible to request 8-byte alignment from
    bpf_jit_binary_alloc, and also makes it issue a warning when an
    unsupported alignment is requested.
    
    Signed-off-by: Ilya Leoshkevich <iii@linux.ibm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20191115123722.58462-1-iii@linux.ibm.com

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7a6f8f6f1da4..ad80e9c6111c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -515,10 +515,12 @@ struct sock_fprog_kern {
 	struct sock_filter	*filter;
 };
 
+/* Some arches need doubleword alignment for their instructions and/or data */
+#define BPF_IMAGE_ALIGNMENT 8
+
 struct bpf_binary_header {
 	u32 pages;
-	/* Some arches need word alignment for their instructions */
-	u8 image[] __aligned(4);
+	u8 image[] __aligned(BPF_IMAGE_ALIGNMENT);
 };
 
 struct bpf_prog {

commit d31e95585ca697fb31440c6fe30113adc85ecfbd
Merge: c23fcbbc6aa4 1204c70d9dcb
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 2 13:12:51 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    The only slightly tricky merge conflict was the netdevsim because the
    mutex locking fix overlapped a lot of driver reload reorganization.
    
    The rest were (relatively) trivial in nature.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cd7455f1013ef96d5cbf5c05d2b7c06f273810a6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Oct 22 15:57:23 2019 +0200

    bpf: Fix use after free in subprog's jited symbol removal
    
    syzkaller managed to trigger the following crash:
    
      [...]
      BUG: unable to handle page fault for address: ffffc90001923030
      #PF: supervisor read access in kernel mode
      #PF: error_code(0x0000) - not-present page
      PGD aa551067 P4D aa551067 PUD aa552067 PMD a572b067 PTE 80000000a1173163
      Oops: 0000 [#1] PREEMPT SMP KASAN
      CPU: 0 PID: 7982 Comm: syz-executor912 Not tainted 5.4.0-rc3+ #0
      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
      RIP: 0010:bpf_jit_binary_hdr include/linux/filter.h:787 [inline]
      RIP: 0010:bpf_get_prog_addr_region kernel/bpf/core.c:531 [inline]
      RIP: 0010:bpf_tree_comp kernel/bpf/core.c:600 [inline]
      RIP: 0010:__lt_find include/linux/rbtree_latch.h:115 [inline]
      RIP: 0010:latch_tree_find include/linux/rbtree_latch.h:208 [inline]
      RIP: 0010:bpf_prog_kallsyms_find kernel/bpf/core.c:674 [inline]
      RIP: 0010:is_bpf_text_address+0x184/0x3b0 kernel/bpf/core.c:709
      [...]
      Call Trace:
       kernel_text_address kernel/extable.c:147 [inline]
       __kernel_text_address+0x9a/0x110 kernel/extable.c:102
       unwind_get_return_address+0x4c/0x90 arch/x86/kernel/unwind_frame.c:19
       arch_stack_walk+0x98/0xe0 arch/x86/kernel/stacktrace.c:26
       stack_trace_save+0xb6/0x150 kernel/stacktrace.c:123
       save_stack mm/kasan/common.c:69 [inline]
       set_track mm/kasan/common.c:77 [inline]
       __kasan_kmalloc+0x11c/0x1b0 mm/kasan/common.c:510
       kasan_slab_alloc+0xf/0x20 mm/kasan/common.c:518
       slab_post_alloc_hook mm/slab.h:584 [inline]
       slab_alloc mm/slab.c:3319 [inline]
       kmem_cache_alloc+0x1f5/0x2e0 mm/slab.c:3483
       getname_flags+0xba/0x640 fs/namei.c:138
       getname+0x19/0x20 fs/namei.c:209
       do_sys_open+0x261/0x560 fs/open.c:1091
       __do_sys_open fs/open.c:1115 [inline]
       __se_sys_open fs/open.c:1110 [inline]
       __x64_sys_open+0x87/0x90 fs/open.c:1110
       do_syscall_64+0xf7/0x1c0 arch/x86/entry/common.c:290
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [...]
    
    After further debugging it turns out that we walk kallsyms while in parallel
    we tear down a BPF program which contains subprograms that have been JITed
    though the program itself has not been fully exposed and is eventually bailing
    out with error.
    
    The bpf_prog_kallsyms_del_subprogs() in bpf_prog_load()'s error path removes
    the symbols, however, bpf_prog_free() tears down the JIT memory too early via
    scheduled work. Instead, it needs to properly respect RCU grace period as the
    kallsyms walk for BPF is under RCU.
    
    Fix it by refactoring __bpf_prog_put()'s tear down and reuse it in our error
    path where we defer final destruction when we have subprogs in the program.
    
    Fixes: 7d1982b4e335 ("bpf: fix panic in prog load calls cleanup")
    Fixes: 1c2a088a6626 ("bpf: x64: add JIT support for multi-function programs")
    Reported-by: syzbot+710043c5d1d5b5013bc7@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: syzbot+710043c5d1d5b5013bc7@syzkaller.appspotmail.com
    Link: https://lore.kernel.org/bpf/55f6367324c2d7e9583fa9ccf5385dcbba0d7a6e.1571752452.git.daniel@iogearbox.net

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2ce57645f3cd..0367a75f873b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1099,7 +1099,6 @@ static inline void bpf_get_prog_name(const struct bpf_prog *prog, char *sym)
 
 #endif /* CONFIG_BPF_JIT */
 
-void bpf_prog_kallsyms_del_subprogs(struct bpf_prog *fp);
 void bpf_prog_kallsyms_del_all(struct bpf_prog *fp);
 
 #define BPF_ANC		BIT(15)

commit 2a02759ef5f8a34792df22b41d5e10658fd7bbd3
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:25:02 2019 -0700

    bpf: Add support for BTF pointers to interpreter
    
    Pointer to BTF object is a pointer to kernel object or NULL.
    The memory access in the interpreter has to be done via probe_kernel_read
    to avoid page faults.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-9-ast@kernel.org

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d3d51d7aff2c..22ebea2e64ea 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -65,6 +65,9 @@ struct ctl_table_header;
 /* unused opcode to mark special call to bpf_tail_call() helper */
 #define BPF_TAIL_CALL	0xf0
 
+/* unused opcode to mark special load instruction. Same as BPF_ABS */
+#define BPF_PROBE_MEM	0x20
+
 /* unused opcode to mark call to interpreter with arguments */
 #define BPF_CALL_ARGS	0xe0
 

commit 7c6a469e3416fa23568c2395a3faa7dd6e376dcb
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:24:56 2019 -0700

    bpf: Add typecast to bpf helpers to help BTF generation
    
    When pahole converts dwarf to btf it emits only used types.
    Wrap existing bpf helper functions into typedef and use it in
    typecast to make gcc emits this type into dwarf.
    Then pahole will convert it to btf.
    The "btf_#name_of_helper" types will be used to figure out
    types of arguments of bpf helpers.
    The generated code before and after is the same.
    Only dwarf and btf sections are different.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-3-ast@kernel.org

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2ce57645f3cd..d3d51d7aff2c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -464,10 +464,11 @@ static inline bool insn_is_zext(const struct bpf_insn *insn)
 #define BPF_CALL_x(x, name, ...)					       \
 	static __always_inline						       \
 	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \
+	typedef u64 (*btf_##name)(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__)); \
 	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));	       \
 	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))	       \
 	{								       \
-		return ____##name(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
+		return ((btf_##name)____##name)(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
 	}								       \
 	static __always_inline						       \
 	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))

commit d895a0f16fadb26d22ab531c49768f7642ae5c3e
Author: Ilya Leoshkevich <iii@linux.ibm.com>
Date:   Fri Aug 16 12:53:00 2019 +0200

    bpf: fix accessing bpf_sysctl.file_pos on s390
    
    "ctx:file_pos sysctl:read write ok" fails on s390 with "Read value  !=
    nux". This is because verifier rewrites a complete 32-bit
    bpf_sysctl.file_pos update to a partial update of the first 32 bits of
    64-bit *bpf_sysctl_kern.ppos, which is not correct on big-endian
    systems.
    
    Fix by using an offset on big-endian systems.
    
    Ditto for bpf_sysctl.file_pos reads. Currently the test does not detect
    a problem there, since it expects to see 0, which it gets with high
    probability in error cases, so change it to seek to offset 3 and expect
    3 in bpf_sysctl.file_pos.
    
    Fixes: e1550bfe0de4 ("bpf: Add file_pos field to bpf_sysctl ctx")
    Signed-off-by: Ilya Leoshkevich <iii@linux.ibm.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20190816105300.49035-1-iii@linux.ibm.com/

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 92c6e31fb008..2ce57645f3cd 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -749,14 +749,14 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 }
 
 static inline u8
-bpf_ctx_narrow_load_shift(u32 off, u32 size, u32 size_default)
+bpf_ctx_narrow_access_offset(u32 off, u32 size, u32 size_default)
 {
-	u8 load_off = off & (size_default - 1);
+	u8 access_off = off & (size_default - 1);
 
 #ifdef __LITTLE_ENDIAN
-	return load_off * 8;
+	return access_off;
 #else
-	return (size_default - (load_off + size)) * 8;
+	return size_default - (access_off + size);
 #endif
 }
 

commit d9b8aadaffa65809d146cf0f8632a22a946367d7
Author: Ilya Leoshkevich <iii@linux.ibm.com>
Date:   Fri Jul 19 11:18:15 2019 +0200

    bpf: fix narrower loads on s390
    
    The very first check in test_pkt_md_access is failing on s390, which
    happens because loading a part of a struct __sk_buff field produces
    an incorrect result.
    
    The preprocessed code of the check is:
    
    {
            __u8 tmp = *((volatile __u8 *)&skb->len +
                    ((sizeof(skb->len) - sizeof(__u8)) / sizeof(__u8)));
            if (tmp != ((*(volatile __u32 *)&skb->len) & 0xFF)) return 2;
    };
    
    clang generates the following code for it:
    
          0:        71 21 00 03 00 00 00 00 r2 = *(u8 *)(r1 + 3)
          1:        61 31 00 00 00 00 00 00 r3 = *(u32 *)(r1 + 0)
          2:        57 30 00 00 00 00 00 ff r3 &= 255
          3:        5d 23 00 1d 00 00 00 00 if r2 != r3 goto +29 <LBB0_10>
    
    Finally, verifier transforms it to:
    
      0: (61) r2 = *(u32 *)(r1 +104)
      1: (bc) w2 = w2
      2: (74) w2 >>= 24
      3: (bc) w2 = w2
      4: (54) w2 &= 255
      5: (bc) w2 = w2
    
    The problem is that when verifier emits the code to replace a partial
    load of a struct __sk_buff field (*(u8 *)(r1 + 3)) with a full load of
    struct sk_buff field (*(u32 *)(r1 + 104)), an optional shift and a
    bitwise AND, it assumes that the machine is little endian and
    incorrectly decides to use a shift.
    
    Adjust shift count calculation to account for endianness.
    
    Fixes: 31fd85816dbe ("bpf: permits narrower load from bpf program context fields")
    Signed-off-by: Ilya Leoshkevich <iii@linux.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ff65d22cf336..92c6e31fb008 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -24,6 +24,7 @@
 
 #include <net/sch_generic.h>
 
+#include <asm/byteorder.h>
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
@@ -747,6 +748,18 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 	return size <= size_default && (size & (size - 1)) == 0;
 }
 
+static inline u8
+bpf_ctx_narrow_load_shift(u32 off, u32 size, u32 size_default)
+{
+	u8 load_off = off & (size_default - 1);
+
+#ifdef __LITTLE_ENDIAN
+	return load_off * 8;
+#else
+	return (size_default - (load_off + size)) * 8;
+#endif
+}
+
 #define bpf_ctx_wide_access_ok(off, size, type, field)			\
 	(size == sizeof(__u64) &&					\
 	off >= offsetof(type, field) &&					\

commit b43995469e5804636a55372e9bbb17ccb22441c5
Author: Stanislav Fomichev <sdf@google.com>
Date:   Mon Jul 15 09:39:52 2019 -0700

    bpf: rename bpf_ctx_wide_store_ok to bpf_ctx_wide_access_ok
    
    Rename bpf_ctx_wide_store_ok to bpf_ctx_wide_access_ok to indicate
    that it can be used for both loads and stores.
    
    Cc: Yonghong Song <yhs@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6d944369ca87..ff65d22cf336 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -747,7 +747,7 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 	return size <= size_default && (size & (size - 1)) == 0;
 }
 
-#define bpf_ctx_wide_store_ok(off, size, type, field)			\
+#define bpf_ctx_wide_access_ok(off, size, type, field)			\
 	(size == sizeof(__u64) &&					\
 	off >= offsetof(type, field) &&					\
 	off + sizeof(__u64) <= offsetofend(type, field) &&		\

commit 600c70bad6594cb124c641ed05355ca134650ea4
Author: Stanislav Fomichev <sdf@google.com>
Date:   Mon Jul 1 10:38:39 2019 -0700

    bpf: allow wide (u64) aligned stores for some fields of bpf_sock_addr
    
    Since commit cd17d7770578 ("bpf/tools: sync bpf.h") clang decided
    that it can do a single u64 store into user_ip6[2] instead of two
    separate u32 ones:
    
     #  17: (18) r2 = 0x100000000000000
     #  ; ctx->user_ip6[2] = bpf_htonl(DST_REWRITE_IP6_2);
     #  19: (7b) *(u64 *)(r1 +16) = r2
     #  invalid bpf_context access off=16 size=8
    
    >From the compiler point of view it does look like a correct thing
    to do, so let's support it on the kernel side.
    
    Credit to Andrii Nakryiko for a proper implementation of
    bpf_ctx_wide_store_ok.
    
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: Yonghong Song <yhs@fb.com>
    Fixes: cd17d7770578 ("bpf/tools: sync bpf.h")
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1fe53e78c7e3..6d944369ca87 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -747,6 +747,12 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 	return size <= size_default && (size & (size - 1)) == 0;
 }
 
+#define bpf_ctx_wide_store_ok(off, size, type, field)			\
+	(size == sizeof(__u64) &&					\
+	off >= offsetof(type, field) &&					\
+	off + sizeof(__u64) <= offsetofend(type, field) &&		\
+	off % sizeof(__u64) == 0)
+
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)

commit 43e74c0267a35d6f5127218054b2d80c7fe801f5
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Fri Jun 28 11:12:34 2019 +0200

    bpf_xdp_redirect_map: Perform map lookup in eBPF helper
    
    The bpf_redirect_map() helper used by XDP programs doesn't return any
    indication of whether it can successfully redirect to the map index it was
    given. Instead, BPF programs have to track this themselves, leading to
    programs using duplicate maps to track which entries are populated in the
    devmap.
    
    This patch fixes this by moving the map lookup into the bpf_redirect_map()
    helper, which makes it possible to return failure to the eBPF program. The
    lower bits of the flags argument is used as the return code, which means
    that existing users who pass a '0' flag argument will get XDP_ABORTED.
    
    With this, a BPF program can check the return code from the helper call and
    react by, for instance, substituting a different redirect. This works for
    any type of map used for redirect.
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 92bd192f7786..1fe53e78c7e3 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -580,6 +580,7 @@ struct bpf_skb_data_end {
 struct bpf_redirect_info {
 	u32 flags;
 	u32 tgt_index;
+	void *tgt_value;
 	struct bpf_map *map;
 	struct bpf_map *map_to_flush;
 	u32 kern_flags;

commit 4b55cf290dc6bd3a9e5da26d1ad60e77aa88c8cf
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Fri Jun 28 11:12:34 2019 +0200

    devmap: Rename ifindex member in bpf_redirect_info
    
    The bpf_redirect_info struct has an 'ifindex' member which was named back
    when the redirects could only target egress interfaces. Now that we can
    also redirect to sockets and CPUs, this is a bit misleading, so rename the
    member to tgt_index.
    
    Reorder the struct members so we can have 'tgt_index' and 'tgt_value' next
    to each other in a subsequent patch.
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 340f7d648974..92bd192f7786 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -578,8 +578,8 @@ struct bpf_skb_data_end {
 };
 
 struct bpf_redirect_info {
-	u32 ifindex;
 	u32 flags;
+	u32 tgt_index;
 	struct bpf_map *map;
 	struct bpf_map *map_to_flush;
 	u32 kern_flags;

commit 0d01da6afc5402f60325c5da31b22f7d56689b49
Author: Stanislav Fomichev <sdf@google.com>
Date:   Thu Jun 27 13:38:47 2019 -0700

    bpf: implement getsockopt and setsockopt hooks
    
    Implement new BPF_PROG_TYPE_CGROUP_SOCKOPT program type and
    BPF_CGROUP_{G,S}ETSOCKOPT cgroup hooks.
    
    BPF_CGROUP_SETSOCKOPT can modify user setsockopt arguments before
    passing them down to the kernel or bypass kernel completely.
    BPF_CGROUP_GETSOCKOPT can can inspect/modify getsockopt arguments that
    kernel returns.
    Both hooks reuse existing PTR_TO_PACKET{,_END} infrastructure.
    
    The buffer memory is pre-allocated (because I don't think there is
    a precedent for working with __user memory from bpf). This might be
    slow to do for each {s,g}etsockopt call, that's why I've added
    __cgroup_bpf_prog_array_is_empty that exits early if there is nothing
    attached to a cgroup. Note, however, that there is a race between
    __cgroup_bpf_prog_array_is_empty and BPF_PROG_RUN_ARRAY where cgroup
    program layout might have changed; this should not be a problem
    because in general there is a race between multiple calls to
    {s,g}etsocktop and user adding/removing bpf progs from a cgroup.
    
    The return code of the BPF program is handled as follows:
    * 0: EPERM
    * 1: success, continue with next BPF program in the cgroup chain
    
    v9:
    * allow overwriting setsockopt arguments (Alexei Starovoitov):
      * use set_fs (same as kernel_setsockopt)
      * buffer is always kzalloc'd (no small on-stack buffer)
    
    v8:
    * use s32 for optlen (Andrii Nakryiko)
    
    v7:
    * return only 0 or 1 (Alexei Starovoitov)
    * always run all progs (Alexei Starovoitov)
    * use optval=0 as kernel bypass in setsockopt (Alexei Starovoitov)
      (decided to use optval=-1 instead, optval=0 might be a valid input)
    * call getsockopt hook after kernel handlers (Alexei Starovoitov)
    
    v6:
    * rework cgroup chaining; stop as soon as bpf program returns
      0 or 2; see patch with the documentation for the details
    * drop Andrii's and Martin's Acked-by (not sure they are comfortable
      with the new state of things)
    
    v5:
    * skip copy_to_user() and put_user() when ret == 0 (Martin Lau)
    
    v4:
    * don't export bpf_sk_fullsock helper (Martin Lau)
    * size != sizeof(__u64) for uapi pointers (Martin Lau)
    * offsetof instead of bpf_ctx_range when checking ctx access (Martin Lau)
    
    v3:
    * typos in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY comments (Andrii Nakryiko)
    * reverse christmas tree in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY (Andrii
      Nakryiko)
    * use __bpf_md_ptr instead of __u32 for optval{,_end} (Martin Lau)
    * use BPF_FIELD_SIZEOF() for consistency (Martin Lau)
    * new CG_SOCKOPT_ACCESS macro to wrap repeated parts
    
    v2:
    * moved bpf_sockopt_kern fields around to remove a hole (Martin Lau)
    * aligned bpf_sockopt_kern->buf to 8 bytes (Martin Lau)
    * bpf_prog_array_is_empty instead of bpf_prog_array_length (Martin Lau)
    * added [0,2] return code check to verifier (Martin Lau)
    * dropped unused buf[64] from the stack (Martin Lau)
    * use PTR_TO_SOCKET for bpf_sockopt->sk (Martin Lau)
    * dropped bpf_target_off from ctx rewrites (Martin Lau)
    * use return code for kernel bypass (Martin Lau & Andrii Nakryiko)
    
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: Martin Lau <kafai@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 43b45d6db36d..340f7d648974 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1199,4 +1199,14 @@ struct bpf_sysctl_kern {
 	u64 tmp_reg;
 };
 
+struct bpf_sockopt_kern {
+	struct sock	*sk;
+	u8		*optval;
+	u8		*optval_end;
+	s32		level;
+	s32		optname;
+	s32		optlen;
+	s32		retval;
+};
+
 #endif /* __LINUX_FILTER_H__ */

commit 5cf1e91456301f8c4f6bbc63ff76cff12f92f31b
Author: brakmo <brakmo@fb.com>
Date:   Tue May 28 16:59:36 2019 -0700

    bpf: cgroup inet skb programs can return 0 to 3
    
    Allows cgroup inet skb programs to return values in the range [0, 3].
    The second bit is used to deterine if congestion occurred and higher
    level protocol should decrease rate. E.g. TCP would call tcp_enter_cwr()
    
    The bpf_prog must set expected_attach_type to BPF_CGROUP_INET_EGRESS
    at load time if it uses the new return values (i.e. 2 or 3).
    
    The expected_attach_type is currently not enforced for
    BPF_PROG_TYPE_CGROUP_SKB.  e.g Meaning the current bpf_prog with
    expected_attach_type setting to BPF_CGROUP_INET_EGRESS can attach to
    BPF_CGROUP_INET_INGRESS.  Blindly enforcing expected_attach_type will
    break backward compatibility.
    
    This patch adds a enforce_expected_attach_type bit to only
    enforce the expected_attach_type when it uses the new
    return value.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ba8b65270e0d..43b45d6db36d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -526,7 +526,8 @@ struct bpf_prog {
 				blinded:1,	/* Was blinded */
 				is_func:1,	/* program is a bpf function */
 				kprobe_override:1, /* Do we override a kprobe? */
-				has_callchain_buf:1; /* callchain buffer allocated? */
+				has_callchain_buf:1, /* callchain buffer allocated? */
+				enforce_expected_attach_type:1; /* Enforce expected_attach_type checking at attach time */
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	enum bpf_attach_type	expected_attach_type; /* For some prog types */
 	u32			len;		/* Number of filter blocks */

commit a4b1d3c1ddf6cb441187b6c130a473c16a05a356
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Fri May 24 23:25:15 2019 +0100

    bpf: verifier: insert zero extension according to analysis result
    
    After previous patches, verifier will mark a insn if it really needs zero
    extension on dst_reg.
    
    It is then for back-ends to decide how to use such information to eliminate
    unnecessary zero extension code-gen during JIT compilation.
    
    One approach is verifier insert explicit zero extension for those insns
    that need zero extension in a generic way, JIT back-ends then do not
    generate zero extension for sub-register write at default.
    
    However, only those back-ends which do not have hardware zero extension
    want this optimization. Back-ends like x86_64 and AArch64 have hardware
    zero extension support that the insertion should be disabled.
    
    This patch introduces new target hook "bpf_jit_needs_zext" which returns
    false at default, meaning verifier zero extension insertion is disabled at
    default. A back-end could override this hook to return true if it doesn't
    have hardware support and want verifier insert zero extension explicitly.
    
    Offload targets do not use this native target hook, instead, they could
    get the optimization results using bpf_prog_offload_ops.finalize.
    
    NOTE: arches could have diversified features, it is possible for one arch
    to have hardware zero extension support for some sub-register write insns
    but not for all. For example, PowerPC, SPARC have zero extended loads, but
    not for alu32. So when verifier zero extension insertion enabled, these JIT
    back-ends need to peephole insns to remove those zero extension inserted
    for insn that actually has hardware zero extension support. The peephole
    could be as simple as looking the next insn, if it is a special zero
    extension insn then it is safe to eliminate it if the current insn has
    hardware zero extension support.
    
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bb10ffb88452..ba8b65270e0d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -825,6 +825,7 @@ u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 
 struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
 void bpf_jit_compile(struct bpf_prog *prog);
+bool bpf_jit_needs_zext(void);
 bool bpf_helper_changes_pkt_data(void *func);
 
 static inline bool bpf_dump_raw_ok(void)

commit 7d134041a89610ae552501fc88652805addcdee4
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Fri May 24 23:25:14 2019 +0100

    bpf: introduce new mov32 variant for doing explicit zero extension
    
    The encoding for this new variant is based on BPF_X format. "imm" field was
    0 only, now it could be 1 which means doing zero extension unconditionally
    
      .code = BPF_ALU | BPF_MOV | BPF_X
      .dst_reg = DST
      .src_reg = SRC
      .imm  = 1
    
    We use this new form for doing zero extension for which verifier will
    guarantee SRC == DST.
    
    Implications on JIT back-ends when doing code-gen for
    BPF_ALU | BPF_MOV | BPF_X:
      1. No change if hardware already does zero extension unconditionally for
         sub-register write.
      2. Otherwise, when seeing imm == 1, just generate insns to clear high
         32-bit. No need to generate insns for the move because when imm == 1,
         dst_reg is the same as src_reg at the moment.
    
    Interpreter doesn't need change as well. It is doing unconditionally zero
    extension for mov32 already.
    
    One helper macro BPF_ZEXT_REG is added to help creating zero extension
    insn using this new mov32 variant.
    
    One helper function insn_is_zext is added for checking one insn is an
    zero extension on dst. This will be widely used by a few JIT back-ends in
    later patches in this set.
    
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7148bab96943..bb10ffb88452 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -160,6 +160,20 @@ struct ctl_table_header;
 		.off   = 0,					\
 		.imm   = IMM })
 
+/* Special form of mov32, used for doing explicit zero extension on dst. */
+#define BPF_ZEXT_REG(DST)					\
+	((struct bpf_insn) {					\
+		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
+		.dst_reg = DST,					\
+		.src_reg = DST,					\
+		.off   = 0,					\
+		.imm   = 1 })
+
+static inline bool insn_is_zext(const struct bpf_insn *insn)
+{
+	return insn->code == (BPF_ALU | BPF_MOV | BPF_X) && insn->imm == 1;
+}
+
 /* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
 #define BPF_LD_IMM64(DST, IMM)					\
 	BPF_LD_IMM64_RAW(DST, 0, IMM)

commit 80f232121b69cc69a31ccb2b38c1665d770b0710
Merge: 82efe4395994 a9e41a529681
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 22:03:58 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support AES128-CCM ciphers in kTLS, from Vakul Garg.
    
       2) Add fib_sync_mem to control the amount of dirty memory we allow to
          queue up between synchronize RCU calls, from David Ahern.
    
       3) Make flow classifier more lockless, from Vlad Buslov.
    
       4) Add PHY downshift support to aquantia driver, from Heiner
          Kallweit.
    
       5) Add SKB cache for TCP rx and tx, from Eric Dumazet. This reduces
          contention on SLAB spinlocks in heavy RPC workloads.
    
       6) Partial GSO offload support in XFRM, from Boris Pismenny.
    
       7) Add fast link down support to ethtool, from Heiner Kallweit.
    
       8) Use siphash for IP ID generator, from Eric Dumazet.
    
       9) Pull nexthops even further out from ipv4/ipv6 routes and FIB
          entries, from David Ahern.
    
      10) Move skb->xmit_more into a per-cpu variable, from Florian
          Westphal.
    
      11) Improve eBPF verifier speed and increase maximum program size,
          from Alexei Starovoitov.
    
      12) Eliminate per-bucket spinlocks in rhashtable, and instead use bit
          spinlocks. From Neil Brown.
    
      13) Allow tunneling with GUE encap in ipvs, from Jacky Hu.
    
      14) Improve link partner cap detection in generic PHY code, from
          Heiner Kallweit.
    
      15) Add layer 2 encap support to bpf_skb_adjust_room(), from Alan
          Maguire.
    
      16) Remove SKB list implementation assumptions in SCTP, your's truly.
    
      17) Various cleanups, optimizations, and simplifications in r8169
          driver. From Heiner Kallweit.
    
      18) Add memory accounting on TX and RX path of SCTP, from Xin Long.
    
      19) Switch PHY drivers over to use dynamic featue detection, from
          Heiner Kallweit.
    
      20) Support flow steering without masking in dpaa2-eth, from Ioana
          Ciocoi.
    
      21) Implement ndo_get_devlink_port in netdevsim driver, from Jiri
          Pirko.
    
      22) Increase the strict parsing of current and future netlink
          attributes, also export such policies to userspace. From Johannes
          Berg.
    
      23) Allow DSA tag drivers to be modular, from Andrew Lunn.
    
      24) Remove legacy DSA probing support, also from Andrew Lunn.
    
      25) Allow ll_temac driver to be used on non-x86 platforms, from Esben
          Haabendal.
    
      26) Add a generic tracepoint for TX queue timeouts to ease debugging,
          from Cong Wang.
    
      27) More indirect call optimizations, from Paolo Abeni"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1763 commits)
      cxgb4: Fix error path in cxgb4_init_module
      net: phy: improve pause mode reporting in phy_print_status
      dt-bindings: net: Fix a typo in the phy-mode list for ethernet bindings
      net: macb: Change interrupt and napi enable order in open
      net: ll_temac: Improve error message on error IRQ
      net/sched: remove block pointer from common offload structure
      net: ethernet: support of_get_mac_address new ERR_PTR error
      net: usb: smsc: fix warning reported by kbuild test robot
      staging: octeon-ethernet: Fix of_get_mac_address ERR_PTR check
      net: dsa: support of_get_mac_address new ERR_PTR error
      net: dsa: sja1105: Fix status initialization in sja1105_get_ethtool_stats
      vrf: sit mtu should not be updated when vrf netdev is the link
      net: dsa: Fix error cleanup path in dsa_init_module
      l2tp: Fix possible NULL pointer dereference
      taprio: add null check on sched_nest to avoid potential null pointer dereference
      net: mvpp2: cls: fix less than zero check on a u32 variable
      net_sched: sch_fq: handle non connected flows
      net_sched: sch_fq: do not assume EDT packets are ordered
      net: hns3: use devm_kcalloc when allocating desc_cb
      net: hns3: some cleanup for struct hns3_enet_ring
      ...

commit d53d2f78ceadba081fc7785570798c3c8d50a718
Author: Rick Edgecombe <rick.p.edgecombe@intel.com>
Date:   Thu Apr 25 17:11:38 2019 -0700

    bpf: Use vmalloc special flag
    
    Use new flag VM_FLUSH_RESET_PERMS for handling freeing of special
    permissioned memory in vmalloc and remove places where memory was set RW
    before freeing which is no longer needed. Don't track if the memory is RO
    anymore because it is now tracked in vmalloc.
    
    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <akpm@linux-foundation.org>
    Cc: <ard.biesheuvel@linaro.org>
    Cc: <deneen.t.dock@intel.com>
    Cc: <kernel-hardening@lists.openwall.com>
    Cc: <kristen@linux.intel.com>
    Cc: <linux_dti@icloud.com>
    Cc: <will.deacon@arm.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nadav Amit <nadav.amit@gmail.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190426001143.4983-19-namit@vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 14ec3bdad9a9..7d3abde3f183 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -20,6 +20,7 @@
 #include <linux/set_memory.h>
 #include <linux/kallsyms.h>
 #include <linux/if_vlan.h>
+#include <linux/vmalloc.h>
 
 #include <net/sch_generic.h>
 
@@ -503,7 +504,6 @@ struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	u16			jited:1,	/* Is our filter JIT'ed? */
 				jit_requested:1,/* archs need to JIT the prog */
-				undo_set_mem:1,	/* Passed set_memory_ro() checkpoint */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1,	/* Do we need dst entry? */
@@ -733,27 +733,17 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
-	fp->undo_set_mem = 1;
+	set_vm_flush_reset_perms(fp);
 	set_memory_ro((unsigned long)fp, fp->pages);
 }
 
-static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
-{
-	if (fp->undo_set_mem)
-		set_memory_rw((unsigned long)fp, fp->pages);
-}
-
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
+	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->pages);
 	set_memory_x((unsigned long)hdr, hdr->pages);
 }
 
-static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
-{
-	set_memory_rw((unsigned long)hdr, hdr->pages);
-}
-
 static inline struct bpf_binary_header *
 bpf_jit_binary_hdr(const struct bpf_prog *fp)
 {
@@ -789,7 +779,6 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
-	bpf_prog_unlock_ro(fp);
 	__bpf_prog_free(fp);
 }
 

commit f2c65fb3221adc6b73b0549fc7ba892022db9797
Author: Nadav Amit <namit@vmware.com>
Date:   Thu Apr 25 17:11:31 2019 -0700

    x86/modules: Avoid breaking W^X while loading modules
    
    When modules and BPF filters are loaded, there is a time window in
    which some memory is both writable and executable. An attacker that has
    already found another vulnerability (e.g., a dangling pointer) might be
    able to exploit this behavior to overwrite kernel code. Prevent having
    writable executable PTEs in this stage.
    
    In addition, avoiding having W+X mappings can also slightly simplify the
    patching of modules code on initialization (e.g., by alternatives and
    static-key), as would be done in the next patch. This was actually the
    main motivation for this patch.
    
    To avoid having W+X mappings, set them initially as RW (NX) and after
    they are set as RO set them as X as well. Setting them as executable is
    done as a separate step to avoid one core in which the old PTE is cached
    (hence writable), and another which sees the updated PTE (executable),
    which would break the W^X protection.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Suggested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <akpm@linux-foundation.org>
    Cc: <ard.biesheuvel@linaro.org>
    Cc: <deneen.t.dock@intel.com>
    Cc: <kernel-hardening@lists.openwall.com>
    Cc: <kristen@linux.intel.com>
    Cc: <linux_dti@icloud.com>
    Cc: <will.deacon@arm.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Rik van Riel <riel@surriel.com>
    Link: https://lkml.kernel.org/r/20190426001143.4983-12-namit@vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6074aa064b54..14ec3bdad9a9 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -746,6 +746,7 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
 	set_memory_ro((unsigned long)hdr, hdr->pages);
+	set_memory_x((unsigned long)hdr, hdr->pages);
 }
 
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)

commit e1550bfe0de47e30484ba91de1e50a91ec1c31f5
Author: Andrey Ignatov <rdna@fb.com>
Date:   Thu Mar 7 18:50:52 2019 -0800

    bpf: Add file_pos field to bpf_sysctl ctx
    
    Add file_pos field to bpf_sysctl context to read and write sysctl file
    position at which sysctl is being accessed (read or written).
    
    The field can be used to e.g. override whole sysctl value on write to
    sysctl even when sys_write is called by user space with file_pos > 0. Or
    BPF program may reject such accesses.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a23653f9460c..fb0edad75971 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1188,6 +1188,9 @@ struct bpf_sysctl_kern {
 	size_t new_len;
 	int new_updated;
 	int write;
+	loff_t *ppos;
+	/* Temporary "register" for indirect stores to ppos. */
+	u64 tmp_reg;
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit 4e63acdff864654cee0ac5aaeda3913798ee78f6
Author: Andrey Ignatov <rdna@fb.com>
Date:   Thu Mar 7 18:38:43 2019 -0800

    bpf: Introduce bpf_sysctl_{get,set}_new_value helpers
    
    Add helpers to work with new value being written to sysctl by user
    space.
    
    bpf_sysctl_get_new_value() copies value being written to sysctl into
    provided buffer.
    
    bpf_sysctl_set_new_value() overrides new value being written by user
    space with a one from provided buffer. Buffer should contain string
    representation of the value, similar to what can be seen in /proc/sys/.
    
    Both helpers can be used only on sysctl write.
    
    File position matters and can be managed by an interface that will be
    introduced separately. E.g. if user space calls sys_write to a file in
    /proc/sys/ at file position = X, where X > 0, then the value set by
    bpf_sysctl_set_new_value() will be written starting from X. If program
    wants to override whole value with specified buffer, file position has
    to be set to zero.
    
    Documentation for the new helpers is provided in bpf.h UAPI.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f254ff92819f..a23653f9460c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1184,6 +1184,9 @@ struct bpf_sysctl_kern {
 	struct ctl_table *table;
 	void *cur_val;
 	size_t cur_len;
+	void *new_val;
+	size_t new_len;
+	int new_updated;
 	int write;
 };
 

commit 1d11b3016cec4ed9770b98e82a61708c8f4926e7
Author: Andrey Ignatov <rdna@fb.com>
Date:   Thu Feb 28 19:22:15 2019 -0800

    bpf: Introduce bpf_sysctl_get_current_value helper
    
    Add bpf_sysctl_get_current_value() helper to copy current sysctl value
    into provided by BPF_PROG_TYPE_CGROUP_SYSCTL program buffer.
    
    It provides same string as user space can see by reading corresponding
    file in /proc/sys/, including new line, etc.
    
    Documentation for the new helper is provided in bpf.h UAPI.
    
    Since current value is kept in ctl_table->data in a parsed form,
    ctl_table->proc_handler() with write=0 is called to read that data and
    convert it to a string. Such a string can later be parsed by a program
    using helpers that will be introduced separately.
    
    Unfortunately it's not trivial to provide API to access parsed data due to
    variety of data representations (string, intvec, uintvec, ulongvec,
    custom structures, even NULL, etc). Instead it's assumed that user know
    how to handle specific sysctl they're interested in and appropriate
    helpers can be used.
    
    Since ctl_table->proc_handler() expects __user buffer, conversion to
    __user happens for kernel allocated one where the value is stored.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a17732057880..f254ff92819f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1182,6 +1182,8 @@ struct bpf_sock_ops_kern {
 struct bpf_sysctl_kern {
 	struct ctl_table_header *head;
 	struct ctl_table *table;
+	void *cur_val;
+	size_t cur_len;
 	int write;
 };
 

commit 7b146cebe30cb481b0f70d85779da938da818637
Author: Andrey Ignatov <rdna@fb.com>
Date:   Wed Feb 27 12:59:24 2019 -0800

    bpf: Sysctl hook
    
    Containerized applications may run as root and it may create problems
    for whole host. Specifically such applications may change a sysctl and
    affect applications in other containers.
    
    Furthermore in existing infrastructure it may not be possible to just
    completely disable writing to sysctl, instead such a process should be
    gradual with ability to log what sysctl are being changed by a
    container, investigate, limit the set of writable sysctl to currently
    used ones (so that new ones can not be changed) and eventually reduce
    this set to zero.
    
    The patch introduces new program type BPF_PROG_TYPE_CGROUP_SYSCTL and
    attach type BPF_CGROUP_SYSCTL to solve these problems on cgroup basis.
    
    New program type has access to following minimal context:
            struct bpf_sysctl {
                    __u32   write;
            };
    
    Where @write indicates whether sysctl is being read (= 0) or written (=
    1).
    
    Helpers to access sysctl name and value will be introduced separately.
    
    BPF_CGROUP_SYSCTL attach point is added to sysctl code right before
    passing control to ctl_table->proc_handler so that BPF program can
    either allow or deny access to sysctl.
    
    Suggested-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6074aa064b54..a17732057880 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -33,6 +33,8 @@ struct bpf_prog_aux;
 struct xdp_rxq_info;
 struct xdp_buff;
 struct sock_reuseport;
+struct ctl_table;
+struct ctl_table_header;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
@@ -1177,4 +1179,10 @@ struct bpf_sock_ops_kern {
 					 */
 };
 
+struct bpf_sysctl_kern {
+	struct ctl_table_header *head;
+	struct ctl_table *table;
+	int write;
+};
+
 #endif /* __LINUX_FILTER_H__ */

commit 203b6609e0ede49eb0b97008b1150c69e9d2ffd3
Merge: 3478588b5136 c978b9460fe1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 07:59:36 2019 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "Lots of tooling updates - too many to list, here's a few highlights:
    
       - Various subcommand updates to 'perf trace', 'perf report', 'perf
         record', 'perf annotate', 'perf script', 'perf test', etc.
    
       - CPU and NUMA topology and affinity handling improvements,
    
       - HW tracing and HW support updates:
          - Intel PT updates
          - ARM CoreSight updates
          - vendor HW event updates
    
       - BPF updates
    
       - Tons of infrastructure updates, both on the build system and the
         library support side
    
       - Documentation updates.
    
       - ... and lots of other changes, see the changelog for details.
    
      Kernel side updates:
    
       - Tighten up kprobes blacklist handling, reduce the number of places
         where developers can install a kprobe and hang/crash the system.
    
       - Fix/enhance vma address filter handling.
    
       - Various PMU driver updates, small fixes and additions.
    
       - refcount_t conversions
    
       - BPF updates
    
       - error code propagation enhancements
    
       - misc other changes"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (238 commits)
      perf script python: Add Python3 support to syscall-counts-by-pid.py
      perf script python: Add Python3 support to syscall-counts.py
      perf script python: Add Python3 support to stat-cpi.py
      perf script python: Add Python3 support to stackcollapse.py
      perf script python: Add Python3 support to sctop.py
      perf script python: Add Python3 support to powerpc-hcalls.py
      perf script python: Add Python3 support to net_dropmonitor.py
      perf script python: Add Python3 support to mem-phys-addr.py
      perf script python: Add Python3 support to failed-syscalls-by-pid.py
      perf script python: Add Python3 support to netdev-times.py
      perf tools: Add perf_exe() helper to find perf binary
      perf script: Handle missing fields with -F +..
      perf data: Add perf_data__open_dir_data function
      perf data: Add perf_data__(create_dir|close_dir) functions
      perf data: Fail check_backup in case of error
      perf data: Make check_backup work over directories
      perf tools: Add rm_rf_perf_data function
      perf tools: Add pattern name checking to rm_rf
      perf tools: Add depth checking to rm_rf
      perf data: Add global path holder
      ...

commit 9ed8f1a6e7670aadd5aef30456a90b456ed1b185
Merge: 43f4e6279f05 7d762d69145a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 28 08:27:17 2019 +0100

    Merge branch 'linus' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 492ecee892c2a4ba6a14903d5d586ff750b7e805
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Feb 25 14:28:39 2019 -0800

    bpf: enable program stats
    
    JITed BPF programs are indistinguishable from kernel functions, but unlike
    kernel code BPF code can be changed often.
    Typical approach of "perf record" + "perf report" profiling and tuning of
    kernel code works just as well for BPF programs, but kernel code doesn't
    need to be monitored whereas BPF programs do.
    Users load and run large amount of BPF programs.
    These BPF stats allow tools monitor the usage of BPF on the server.
    The monitoring tools will turn sysctl kernel.bpf_stats_enabled
    on and off for few seconds to sample average cost of the programs.
    Aggregated data over hours and days will provide an insight into cost of BPF
    and alarms can trigger in case given program suddenly gets more expensive.
    
    The cost of two sched_clock() per program invocation adds ~20 nsec.
    Fast BPF progs (like selftests/bpf/progs/test_pkt_access.c) will slow down
    from ~10 nsec to ~30 nsec.
    static_key minimizes the cost of the stats collection.
    There is no measurable difference before/after this patch
    with kernel.bpf_stats_enabled=0
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f32b3eca5a04..7e5e3db11106 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -533,7 +533,24 @@ struct sk_filter {
 	struct bpf_prog	*prog;
 };
 
-#define BPF_PROG_RUN(filter, ctx)  ({ cant_sleep(); (*(filter)->bpf_func)(ctx, (filter)->insnsi); })
+DECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);
+
+#define BPF_PROG_RUN(prog, ctx)	({				\
+	u32 ret;						\
+	cant_sleep();						\
+	if (static_branch_unlikely(&bpf_stats_enabled_key)) {	\
+		struct bpf_prog_stats *stats;			\
+		u64 start = sched_clock();			\
+		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
+		stats = this_cpu_ptr(prog->aux->stats);		\
+		u64_stats_update_begin(&stats->syncp);		\
+		stats->cnt++;					\
+		stats->nsecs += sched_clock() - start;		\
+		u64_stats_update_end(&stats->syncp);		\
+	} else {						\
+		ret = (*(prog)->bpf_func)(ctx, (prog)->insnsi);	\
+	}							\
+	ret; })
 
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 
@@ -764,6 +781,7 @@ void bpf_prog_free_jited_linfo(struct bpf_prog *prog);
 void bpf_prog_free_unused_jited_linfo(struct bpf_prog *prog);
 
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
+struct bpf_prog *bpf_prog_alloc_no_stats(unsigned int size, gfp_t gfp_extra_flags);
 struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);
 void __bpf_prog_free(struct bpf_prog *fp);

commit 568f196756ad9fe2b49c46bbf6a9de1b190438b4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 28 17:21:52 2019 -0800

    bpf: check that BPF programs run with preemption disabled
    
    Introduce cant_sleep() macro for annotation of functions that
    cannot sleep.
    
    Use it in BPF_PROG_RUN to catch execution of BPF programs in
    preemptable context.
    
    Suggested-by: Jann Horn <jannh@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 95e2d7ebdf21..f32b3eca5a04 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -533,7 +533,7 @@ struct sk_filter {
 	struct bpf_prog	*prog;
 };
 
-#define BPF_PROG_RUN(filter, ctx)  (*(filter)->bpf_func)(ctx, (filter)->insnsi)
+#define BPF_PROG_RUN(filter, ctx)  ({ cant_sleep(); (*(filter)->bpf_func)(ctx, (filter)->insnsi); })
 
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 

commit a655fe9f194842693258f43b5382855db1c2f654
Merge: 7499a288bf1a 27b4ad621e88
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 8 15:00:17 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    An ipvlan bug fix in 'net' conflicted with the abstraction away
    of the IPV6 specific support in 'net-next'.
    
    Similarly, a bug fix for mlx5 in 'net' conflicted with the flow
    action conversion in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6cab5e90ab2bd323c9f3811b6c70a4687df51e27
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Jan 28 18:43:34 2019 -0800

    bpf: run bpf programs with preemption disabled
    
    Disabled preemption is necessary for proper access to per-cpu maps
    from BPF programs.
    
    But the sender side of socket filters didn't have preemption disabled:
    unix_dgram_sendmsg->sk_filter->sk_filter_trim_cap->bpf_prog_run_save_cb->BPF_PROG_RUN
    
    and a combination of af_packet with tun device didn't disable either:
    tpacket_snd->packet_direct_xmit->packet_pick_tx_queue->ndo_select_queue->
      tun_select_queue->tun_ebpf_select_queue->bpf_prog_run_clear_cb->BPF_PROG_RUN
    
    Disable preemption before executing BPF programs (both classic and extended).
    
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ad106d845b22..e532fcc6e4b5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -591,8 +591,8 @@ static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 	return qdisc_skb_cb(skb)->data;
 }
 
-static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
-				       struct sk_buff *skb)
+static inline u32 __bpf_prog_run_save_cb(const struct bpf_prog *prog,
+					 struct sk_buff *skb)
 {
 	u8 *cb_data = bpf_skb_cb(skb);
 	u8 cb_saved[BPF_SKB_CB_LEN];
@@ -611,15 +611,30 @@ static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
 	return res;
 }
 
+static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
+				       struct sk_buff *skb)
+{
+	u32 res;
+
+	preempt_disable();
+	res = __bpf_prog_run_save_cb(prog, skb);
+	preempt_enable();
+	return res;
+}
+
 static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 					struct sk_buff *skb)
 {
 	u8 *cb_data = bpf_skb_cb(skb);
+	u32 res;
 
 	if (unlikely(prog->cb_access))
 		memset(cb_data, 0, BPF_SKB_CB_LEN);
 
-	return BPF_PROG_RUN(prog, skb);
+	preempt_disable();
+	res = BPF_PROG_RUN(prog, skb);
+	preempt_enable();
+	return res;
 }
 
 static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,

commit 116bfa96a255123ed209da6544f74a4f2eaca5da
Author: Valdis Kletnieks <valdis.kletnieks@vt.edu>
Date:   Tue Jan 29 01:04:25 2019 -0500

    bpf: fix missing prototype warnings
    
    Compiling with W=1 generates warnings:
    
      CC      kernel/bpf/core.o
    kernel/bpf/core.c:721:12: warning: no previous prototype for ?bpf_jit_alloc_exec_limit? [-Wmissing-prototypes]
      721 | u64 __weak bpf_jit_alloc_exec_limit(void)
          |            ^~~~~~~~~~~~~~~~~~~~~~~~
    kernel/bpf/core.c:757:14: warning: no previous prototype for ?bpf_jit_alloc_exec? [-Wmissing-prototypes]
      757 | void *__weak bpf_jit_alloc_exec(unsigned long size)
          |              ^~~~~~~~~~~~~~~~~~
    kernel/bpf/core.c:762:13: warning: no previous prototype for ?bpf_jit_free_exec? [-Wmissing-prototypes]
      762 | void __weak bpf_jit_free_exec(void *addr)
          |             ^~~~~~~~~~~~~~~~~
    
    All three are weak functions that archs can override, provide
    proper prototypes for when a new arch provides their own.
    
    Signed-off-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index e4b473f85b46..7317376734f7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -880,7 +880,9 @@ bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
 		     unsigned int alignment,
 		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
 void bpf_jit_binary_free(struct bpf_binary_header *hdr);
-
+u64 bpf_jit_alloc_exec_limit(void);
+void *bpf_jit_alloc_exec(unsigned long size);
+void bpf_jit_free_exec(void *addr);
 void bpf_jit_free(struct bpf_prog *fp);
 
 int bpf_jit_get_func_addr(const struct bpf_prog *prog,

commit a7b76c8857692b0fce063b94ed83da11c396d341
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Sat Jan 26 12:26:05 2019 -0500

    bpf: JIT blinds support JMP32
    
    This patch adds JIT blinds support for JMP32.
    
    Like BPF_JMP_REG/IMM, JMP32 version are needed for building raw bpf insn.
    They are added to both include/linux/filter.h and
    tools/include/linux/filter.h.
    
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index be9af6b4a9e4..e4b473f85b46 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -277,6 +277,26 @@ struct sock_reuseport;
 		.off   = OFF,					\
 		.imm   = IMM })
 
+/* Like BPF_JMP_REG, but with 32-bit wide operands for comparison. */
+
+#define BPF_JMP32_REG(OP, DST, SRC, OFF)			\
+	((struct bpf_insn) {					\
+		.code  = BPF_JMP32 | BPF_OP(OP) | BPF_X,	\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
+/* Like BPF_JMP_IMM, but with 32-bit wide operands for comparison. */
+
+#define BPF_JMP32_IMM(OP, DST, IMM, OFF)			\
+	((struct bpf_insn) {					\
+		.code  = BPF_JMP32 | BPF_OP(OP) | BPF_K,	\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
+		.off   = OFF,					\
+		.imm   = IMM })
+
 /* Unconditional jumps, goto pc + off16 */
 
 #define BPF_JMP_A(OFF)						\

commit 52875a04f4b26e7ef30a288ea096f7cfec0e93cd
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Jan 22 22:45:20 2019 -0800

    bpf: verifier: remove dead code
    
    Instead of overwriting dead code with jmp -1 instructions
    remove it completely for root.  Adjust verifier state and
    line info appropriately.
    
    v2:
     - adjust func_info (Alexei);
     - make sure first instruction retains line info (Alexei).
    v4: (Yonghong)
     - remove unnecessary if (!insn to remove) checks;
     - always keep last line info if first live instruction lacks one.
    v5: (Martin Lau)
     - improve and clarify comments.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ad106d845b22..be9af6b4a9e4 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -778,6 +778,7 @@ static inline bool bpf_dump_raw_ok(void)
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
+int bpf_remove_insns(struct bpf_prog *prog, u32 off, u32 cnt);
 
 void bpf_clear_redirect_map(struct bpf_map *map);
 

commit 6ee52e2a3fe4ea35520720736e6791df1fb67106
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:15 2019 -0800

    perf, bpf: Introduce PERF_RECORD_BPF_EVENT
    
    For better performance analysis of BPF programs, this patch introduces
    PERF_RECORD_BPF_EVENT, a new perf_event_type that exposes BPF program
    load/unload information to user space.
    
    Each BPF program may contain up to BPF_MAX_SUBPROGS (256) sub programs.
    The following example shows kernel symbols for a BPF program with 7 sub
    programs:
    
        ffffffffa0257cf9 t bpf_prog_b07ccb89267cf242_F
        ffffffffa02592e1 t bpf_prog_2dcecc18072623fc_F
        ffffffffa025b0e9 t bpf_prog_bb7a405ebaec5d5c_F
        ffffffffa025dd2c t bpf_prog_a7540d4a39ec1fc7_F
        ffffffffa025fcca t bpf_prog_05762d4ade0e3737_F
        ffffffffa026108f t bpf_prog_db4bd11e35df90d4_F
        ffffffffa0263f00 t bpf_prog_89d64e4abf0f0126_F
        ffffffffa0257cf9 t bpf_prog_ae31629322c4b018__dummy_tracepoi
    
    When a bpf program is loaded, PERF_RECORD_KSYMBOL is generated for each
    of these sub programs. Therefore, PERF_RECORD_BPF_EVENT is not needed
    for simple profiling.
    
    For annotation, user space need to listen to PERF_RECORD_BPF_EVENT and
    gather more information about these (sub) programs via sys_bpf.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradeaed.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-4-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ad106d845b22..d531d4250bff 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -951,6 +951,7 @@ bpf_address_lookup(unsigned long addr, unsigned long *size,
 
 void bpf_prog_kallsyms_add(struct bpf_prog *fp);
 void bpf_prog_kallsyms_del(struct bpf_prog *fp);
+void bpf_get_prog_name(const struct bpf_prog *prog, char *sym);
 
 #else /* CONFIG_BPF_JIT */
 
@@ -1006,6 +1007,12 @@ static inline void bpf_prog_kallsyms_add(struct bpf_prog *fp)
 static inline void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 {
 }
+
+static inline void bpf_get_prog_name(const struct bpf_prog *prog, char *sym)
+{
+	sym[0] = '\0';
+}
+
 #endif /* CONFIG_BPF_JIT */
 
 void bpf_prog_kallsyms_del_subprogs(struct bpf_prog *fp);

commit 9b73bfdd08e73231d6a90ae6db4b46b3fbf56c30
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jan 3 00:58:29 2019 +0100

    bpf: enable access to ax register also from verifier rewrite
    
    Right now we are using BPF ax register in JIT for constant blinding as
    well as in interpreter as temporary variable. Verifier will not be able
    to use it simply because its use will get overridden from the former in
    bpf_jit_blind_insn(). However, it can be made to work in that blinding
    will be skipped if there is prior use in either source or destination
    register on the instruction. Taking constraints of ax into account, the
    verifier is then open to use it in rewrites under some constraints. Note,
    ax register already has mappings in every eBPF JIT.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 84a6a98f8328..ad106d845b22 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -53,12 +53,7 @@ struct sock_reuseport;
 #define BPF_REG_D	BPF_REG_8	/* data, callee-saved */
 #define BPF_REG_H	BPF_REG_9	/* hlen, callee-saved */
 
-/* Kernel hidden auxiliary/helper register for hardening step.
- * Only used by eBPF JITs. It's nothing more than a temporary
- * register that JITs use internally, only that here it's part
- * of eBPF instructions that have been rewritten for blinding
- * constants. See JIT pre-step in bpf_jit_blind_constants().
- */
+/* Kernel hidden auxiliary/helper register. */
 #define BPF_REG_AX		MAX_BPF_REG
 #define MAX_BPF_EXT_REG		(MAX_BPF_REG + 1)
 #define MAX_BPF_JIT_REG		MAX_BPF_EXT_REG

commit 144cd91c4c2bced6eb8a7e25e590f6618a11e854
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jan 3 00:58:28 2019 +0100

    bpf: move tmp variable into ax register in interpreter
    
    This change moves the on-stack 64 bit tmp variable in ___bpf_prog_run()
    into the hidden ax register. The latter is currently only used in JITs
    for constant blinding as a temporary scratch register, meaning the BPF
    interpreter will never see the use of ax. Therefore it is safe to use
    it for the cases where tmp has been used earlier. This is needed to later
    on allow restricted hidden use of ax in both interpreter and JITs.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 8c8544b375eb..84a6a98f8328 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -60,7 +60,8 @@ struct sock_reuseport;
  * constants. See JIT pre-step in bpf_jit_blind_constants().
  */
 #define BPF_REG_AX		MAX_BPF_REG
-#define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
+#define MAX_BPF_EXT_REG		(MAX_BPF_REG + 1)
+#define MAX_BPF_JIT_REG		MAX_BPF_EXT_REG
 
 /* unused opcode to mark special call to bpf_tail_call() helper */
 #define BPF_TAIL_CALL	0xf0

commit 2be09de7d6a06f58e768de1255a687c9aaa66606
Merge: 44a7b3b6e3a4 1d51b4b1d3f2
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 20 10:53:28 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Lots of conflicts, by happily all cases of overlapping
    changes, parallel adds, things of that nature.
    
    Thanks to Stephen Rothwell, Saeed Mahameed, and others
    for their guidance in these resolutions.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit fdadd04931c2d7cd294dc5b2b342863f94be53a3
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Dec 11 12:14:12 2018 +0100

    bpf: fix bpf_jit_limit knob for PAGE_SIZE >= 64K
    
    Michael and Sandipan report:
    
      Commit ede95a63b5 introduced a bpf_jit_limit tuneable to limit BPF
      JIT allocations. At compile time it defaults to PAGE_SIZE * 40000,
      and is adjusted again at init time if MODULES_VADDR is defined.
    
      For ppc64 kernels, MODULES_VADDR isn't defined, so we're stuck with
      the compile-time default at boot-time, which is 0x9c400000 when
      using 64K page size. This overflows the signed 32-bit bpf_jit_limit
      value:
    
      root@ubuntu:/tmp# cat /proc/sys/net/core/bpf_jit_limit
      -1673527296
    
      and can cause various unexpected failures throughout the network
      stack. In one case `strace dhclient eth0` reported:
    
      setsockopt(5, SOL_SOCKET, SO_ATTACH_FILTER, {len=11, filter=0x105dd27f8},
                 16) = -1 ENOTSUPP (Unknown error 524)
    
      and similar failures can be seen with tools like tcpdump. This doesn't
      always reproduce however, and I'm not sure why. The more consistent
      failure I've seen is an Ubuntu 18.04 KVM guest booted on a POWER9
      host would time out on systemd/netplan configuring a virtio-net NIC
      with no noticeable errors in the logs.
    
    Given this and also given that in near future some architectures like
    arm64 will have a custom area for BPF JIT image allocations we should
    get rid of the BPF_JIT_LIMIT_DEFAULT fallback / default entirely. For
    4.21, we have an overridable bpf_jit_alloc_exec(), bpf_jit_free_exec()
    so therefore add another overridable bpf_jit_alloc_exec_limit() helper
    function which returns the possible size of the memory area for deriving
    the default heuristic in bpf_jit_charge_init().
    
    Like bpf_jit_alloc_exec() and bpf_jit_free_exec(), the new
    bpf_jit_alloc_exec_limit() assumes that module_alloc() is the default
    JIT memory provider, and therefore in case archs implement their custom
    module_alloc() we use MODULES_{END,_VADDR} for limits and otherwise for
    vmalloc_exec() cases like on ppc64 we use VMALLOC_{END,_START}.
    
    Additionally, for archs supporting large page sizes, we should change
    the sysctl to be handled as long to not run into sysctl restrictions
    in future.
    
    Fixes: ede95a63b5e8 ("bpf: add bpf_jit_limit knob to restrict unpriv allocations")
    Reported-by: Sandipan Das <sandipan@linux.ibm.com>
    Reported-by: Michael Roth <mdroth@linux.vnet.ibm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Tested-by: Michael Roth <mdroth@linux.vnet.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 795ff0b869bb..a8b9d90a8042 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -861,7 +861,7 @@ bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
 extern int bpf_jit_enable;
 extern int bpf_jit_harden;
 extern int bpf_jit_kallsyms;
-extern int bpf_jit_limit;
+extern long bpf_jit_limit;
 
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 

commit addb0679839a1f74da6ec742137558be244dd0e9
Merge: 8cc196d6ef86 aa570ff4fd36
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Dec 10 18:00:43 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-12-11
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    It has three minor merge conflicts, resolutions:
    
    1) tools/testing/selftests/bpf/test_verifier.c
    
     Take first chunk with alignment_prevented_execution.
    
    2) net/core/filter.c
    
      [...]
      case bpf_ctx_range_ptr(struct __sk_buff, flow_keys):
      case bpf_ctx_range(struct __sk_buff, wire_len):
            return false;
      [...]
    
    3) include/uapi/linux/bpf.h
    
      Take the second chunk for the two cases each.
    
    The main changes are:
    
    1) Add support for BPF line info via BTF and extend libbpf as well
       as bpftool's program dump to annotate output with BPF C code to
       facilitate debugging and introspection, from Martin.
    
    2) Add support for BPF_ALU | BPF_ARSH | BPF_{K,X} in interpreter
       and all JIT backends, from Jiong.
    
    3) Improve BPF test coverage on archs with no efficient unaligned
       access by adding an "any alignment" flag to the BPF program load
       to forcefully disable verifier alignment checks, from David.
    
    4) Add a new bpf_prog_test_run_xattr() API to libbpf which allows for
       proper use of BPF_PROG_TEST_RUN with data_out, from Lorenz.
    
    5) Extend tc BPF programs to use a new __sk_buff field called wire_len
       for more accurate accounting of packets going to wire, from Petar.
    
    6) Improve bpftool to allow dumping the trace pipe from it and add
       several improvements in bash completion and map/prog dump,
       from Quentin.
    
    7) Optimize arm64 BPF JIT to always emit movn/movk/movk sequence for
       kernel addresses and add a dedicated BPF JIT backend allocator,
       from Ard.
    
    8) Add a BPF helper function for IR remotes to report mouse movements,
       from Sean.
    
    9) Various cleanups in BPF prog dump e.g. to make UAPI bpf_prog_info
       member naming consistent with existing conventions, from Yonghong
       and Song.
    
    10) Misc cleanups and improvements in allowing to pass interface name
        via cmdline for xdp1 BPF example, from Matteo.
    
    11) Fix a potential segfault in BPF sample loader's kprobes handling,
        from Daniel T.
    
    12) Fix SPDX license in libbpf's README.rst, from Andrey.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4cc1feeb6ffc2799f8badb4dea77c637d340cb0d
Merge: a60956ed72f7 40e020c129cf
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Dec 9 21:27:48 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several conflicts, seemingly all over the place.
    
    I used Stephen Rothwell's sample resolutions for many of these, if not
    just to double check my own work, so definitely the credit largely
    goes to him.
    
    The NFP conflict consisted of a bug fix (moving operations
    past the rhashtable operation) while chaning the initial
    argument in the function call in the moved code.
    
    The net/dsa/master.c conflict had to do with a bug fix intermixing of
    making dsa_master_set_mtu() static with the fixing of the tagging
    attribute location.
    
    cls_flower had a conflict because the dup reject fix from Or
    overlapped with the addition of port range classifiction.
    
    __set_phy_supported()'s conflict was relatively easy to resolve
    because Andrew fixed it in both trees, so it was just a matter
    of taking the net-next copy.  Or at least I think it was :-)
    
    Joe Stringer's fix to the handling of netns id 0 in bpf_sk_lookup()
    intermixed with changes on how the sdif and caller_net are calculated
    in these code paths in net-next.
    
    The remaining BPF conflicts were largely about the addition of the
    __bpf_md_ptr stuff in 'net' overlapping with adjustments and additions
    to the relevant data structure where the MD pointer macros are used.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c454a46b5efd8eff8880e88ece2976e60a26bf35
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Dec 7 16:42:25 2018 -0800

    bpf: Add bpf_line_info support
    
    This patch adds bpf_line_info support.
    
    It accepts an array of bpf_line_info objects during BPF_PROG_LOAD.
    The "line_info", "line_info_cnt" and "line_info_rec_size" are added
    to the "union bpf_attr".  The "line_info_rec_size" makes
    bpf_line_info extensible in the future.
    
    The new "check_btf_line()" ensures the userspace line_info is valid
    for the kernel to use.
    
    When the verifier is translating/patching the bpf_prog (through
    "bpf_patch_insn_single()"), the line_infos' insn_off is also
    adjusted by the newly added "bpf_adj_linfo()".
    
    If the bpf_prog is jited, this patch also provides the jited addrs (in
    aux->jited_linfo) for the corresponding line_info.insn_off.
    "bpf_prog_fill_jited_linfo()" is added to fill the aux->jited_linfo.
    It is currently called by the x86 jit.  Other jits can also use
    "bpf_prog_fill_jited_linfo()" and it will be done in the followup patches.
    In the future, if it deemed necessary, a particular jit could also provide
    its own "bpf_prog_fill_jited_linfo()" implementation.
    
    A few "*line_info*" fields are added to the bpf_prog_info such
    that the user can get the xlated line_info back (i.e. the line_info
    with its insn_off reflecting the translated prog).  The jited_line_info
    is available if the prog is jited.  It is an array of __u64.
    If the prog is not jited, jited_line_info_cnt is 0.
    
    The verifier's verbose log with line_info will be done in
    a follow up patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d16deead65c6..29f21f9d7f68 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -718,6 +718,13 @@ void bpf_prog_free(struct bpf_prog *fp);
 
 bool bpf_opcode_in_insntable(u8 code);
 
+void bpf_prog_free_linfo(struct bpf_prog *prog);
+void bpf_prog_fill_jited_linfo(struct bpf_prog *prog,
+			       const u32 *insn_to_jit_off);
+int bpf_prog_alloc_jited_linfo(struct bpf_prog *prog);
+void bpf_prog_free_jited_linfo(struct bpf_prog *prog);
+void bpf_prog_free_unused_jited_linfo(struct bpf_prog *prog);
+
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
 struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);

commit b7df9ada9a7700dbcca1ba53d217c01e3d48179c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Dec 1 01:18:53 2018 +0100

    bpf: fix pointer offsets in context for 32 bit
    
    Currently, pointer offsets in three BPF context structures are
    broken in two scenarios: i) 32 bit compiled applications running
    on 64 bit kernels, and ii) LLVM compiled BPF programs running
    on 32 bit kernels. The latter is due to BPF target machine being
    strictly 64 bit. So in each of the cases the offsets will mismatch
    in verifier when checking / rewriting context access. Fix this by
    providing a helper macro __bpf_md_ptr() that will enforce padding
    up to 64 bit and proper alignment, and for context access a macro
    bpf_ctx_range_ptr() which will cover full 64 bit member range on
    32 bit archs. For flow_keys, we additionally need to force the
    size check to sizeof(__u64) as with other pointer types.
    
    Fixes: d58e468b1112 ("flow_dissector: implements flow dissector BPF hook")
    Fixes: 4f738adba30a ("bpf: create tcp_bpf_ulp allowing BPF to monitor socket TX/RX data")
    Fixes: 2dbb9b9e6df6 ("bpf: Introduce BPF_PROG_TYPE_SK_REUSEPORT")
    Reported-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: David S. Miller <davem@davemloft.net>
    Tested-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 448dcc448f1f..795ff0b869bb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -449,6 +449,13 @@ struct sock_reuseport;
 	offsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1
 #define bpf_ctx_range_till(TYPE, MEMBER1, MEMBER2)				\
 	offsetof(TYPE, MEMBER1) ... offsetofend(TYPE, MEMBER2) - 1
+#if BITS_PER_LONG == 64
+# define bpf_ctx_range_ptr(TYPE, MEMBER)					\
+	offsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1
+#else
+# define bpf_ctx_range_ptr(TYPE, MEMBER)					\
+	offsetof(TYPE, MEMBER) ... offsetof(TYPE, MEMBER) + 8 - 1
+#endif /* BITS_PER_LONG == 64 */
 
 #define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)				\
 	({									\

commit e561bb29b650d2817d10a4858f1817836ed08399
Merge: 62e3a9317882 60b548237fed
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Nov 28 22:10:54 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Trivial conflict in net/core/filter.c, a locally computed
    'sdif' is now an argument to the function.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e2c95a61656d29ceaac97b6a975c8a1f26e26f15
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Nov 26 14:05:38 2018 +0100

    bpf, ppc64: generalize fetching subprog into bpf_jit_get_func_addr
    
    Make fetching of the BPF call address from ppc64 JIT generic. ppc64
    was using a slightly different variant rather than through the insns'
    imm field encoding as the target address would not fit into that space.
    Therefore, the target subprog number was encoded into the insns' offset
    and fetched through fp->aux->func[off]->bpf_func instead. Given there
    are other JITs with this issue and the mechanism of fetching the address
    is JIT-generic, move it into the core as a helper instead. On the JIT
    side, we get information on whether the retrieved address is a fixed
    one, that is, not changing through JIT passes, or a dynamic one. For
    the former, JITs can optimize their imm emission because this doesn't
    change jump offsets throughout JIT process.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Sandipan Das <sandipan@linux.ibm.com>
    Tested-by: Sandipan Das <sandipan@linux.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index de629b706d1d..448dcc448f1f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -866,6 +866,10 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr);
 
 void bpf_jit_free(struct bpf_prog *fp);
 
+int bpf_jit_get_func_addr(const struct bpf_prog *prog,
+			  const struct bpf_insn *insn, bool extra_pass,
+			  u64 *func_addr, bool *func_addr_fixed);
+
 struct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *fp);
 void bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other);
 

commit 46f53a65d2de3e1591636c22b626b09d8684fd71
Author: Andrey Ignatov <rdna@fb.com>
Date:   Sat Nov 10 22:15:13 2018 -0800

    bpf: Allow narrow loads with offset > 0
    
    Currently BPF verifier allows narrow loads for a context field only with
    offset zero. E.g. if there is a __u32 field then only the following
    loads are permitted:
      * off=0, size=1 (narrow);
      * off=0, size=2 (narrow);
      * off=0, size=4 (full).
    
    On the other hand LLVM can generate a load with offset different than
    zero that make sense from program logic point of view, but verifier
    doesn't accept it.
    
    E.g. tools/testing/selftests/bpf/sendmsg4_prog.c has code:
    
      #define DST_IP4                       0xC0A801FEU /* 192.168.1.254 */
      ...
            if ((ctx->user_ip4 >> 24) == (bpf_htonl(DST_IP4) >> 24) &&
    
    where ctx is struct bpf_sock_addr.
    
    Some versions of LLVM can produce the following byte code for it:
    
           8:       71 12 07 00 00 00 00 00         r2 = *(u8 *)(r1 + 7)
           9:       67 02 00 00 18 00 00 00         r2 <<= 24
          10:       18 03 00 00 00 00 00 fe 00 00 00 00 00 00 00 00         r3 = 4261412864 ll
          12:       5d 32 07 00 00 00 00 00         if r2 != r3 goto +7 <LBB0_6>
    
    where `*(u8 *)(r1 + 7)` means narrow load for ctx->user_ip4 with size=1
    and offset=3 (7 - sizeof(ctx->user_family) = 3). This load is currently
    rejected by verifier.
    
    Verifier code that rejects such loads is in bpf_ctx_narrow_access_ok()
    what means any is_valid_access implementation, that uses the function,
    works this way, e.g. bpf_skb_is_valid_access() for __sk_buff or
    sock_addr_is_valid_access() for bpf_sock_addr.
    
    The patch makes such loads supported. Offset can be in [0; size_default)
    but has to be multiple of load size. E.g. for __u32 field the following
    loads are supported now:
      * off=0, size=1 (narrow);
      * off=1, size=1 (narrow);
      * off=2, size=1 (narrow);
      * off=3, size=1 (narrow);
      * off=0, size=2 (narrow);
      * off=2, size=2 (narrow);
      * off=0, size=4 (full).
    
    Reported-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index de629b706d1d..cc17f5f32fbb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -668,24 +668,10 @@ static inline u32 bpf_ctx_off_adjust_machine(u32 size)
 	return size;
 }
 
-static inline bool bpf_ctx_narrow_align_ok(u32 off, u32 size_access,
-					   u32 size_default)
-{
-	size_default = bpf_ctx_off_adjust_machine(size_default);
-	size_access  = bpf_ctx_off_adjust_machine(size_access);
-
-#ifdef __LITTLE_ENDIAN
-	return (off & (size_default - 1)) == 0;
-#else
-	return (off & (size_default - 1)) + size_access == size_default;
-#endif
-}
-
 static inline bool
 bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 {
-	return bpf_ctx_narrow_align_ok(off, size, size_default) &&
-	       size <= size_default && (size & (size - 1)) == 0;
+	return size <= size_default && (size & (size - 1)) == 0;
 }
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))

commit ede95a63b5e84ddeea6b0c473b36ab8bfd8c6ce3
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Oct 23 01:11:04 2018 +0200

    bpf: add bpf_jit_limit knob to restrict unpriv allocations
    
    Rick reported that the BPF JIT could potentially fill the entire module
    space with BPF programs from unprivileged users which would prevent later
    attempts to load normal kernel modules or privileged BPF programs, for
    example. If JIT was enabled but unsuccessful to generate the image, then
    before commit 290af86629b2 ("bpf: introduce BPF_JIT_ALWAYS_ON config")
    we would always fall back to the BPF interpreter. Nowadays in the case
    where the CONFIG_BPF_JIT_ALWAYS_ON could be set, then the load will abort
    with a failure since the BPF interpreter was compiled out.
    
    Add a global limit and enforce it for unprivileged users such that in case
    of BPF interpreter compiled out we fail once the limit has been reached
    or we fall back to BPF interpreter earlier w/o using module mem if latter
    was compiled in. In a next step, fair share among unprivileged users can
    be resolved in particular for the case where we would fail hard once limit
    is reached.
    
    Fixes: 290af86629b2 ("bpf: introduce BPF_JIT_ALWAYS_ON config")
    Fixes: 0a14842f5a3c ("net: filter: Just In Time compiler for x86-64")
    Co-Developed-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: LKML <linux-kernel@vger.kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 91b4c934f02e..de629b706d1d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -854,6 +854,7 @@ bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
 extern int bpf_jit_enable;
 extern int bpf_jit_harden;
 extern int bpf_jit_kallsyms;
+extern int bpf_jit_limit;
 
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 

commit b39b5f411dcfce28ff954e5d6acb2c11be3cb0ec
Author: Song Liu <songliubraving@fb.com>
Date:   Fri Oct 19 09:57:57 2018 -0700

    bpf: add cg_skb_is_valid_access for BPF_PROG_TYPE_CGROUP_SKB
    
    BPF programs of BPF_PROG_TYPE_CGROUP_SKB need to access headers in the
    skb. This patch enables direct access of skb for these programs.
    
    Two helper functions bpf_compute_and_save_data_end() and
    bpf_restore_data_end() are introduced. There are used in
    __cgroup_bpf_run_filter_skb(), to compute proper data_end for the
    BPF program, and restore original data afterwards.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 5771874bc01e..91b4c934f02e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -548,6 +548,27 @@ static inline void bpf_compute_data_pointers(struct sk_buff *skb)
 	cb->data_end  = skb->data + skb_headlen(skb);
 }
 
+/* Similar to bpf_compute_data_pointers(), except that save orginal
+ * data in cb->data and cb->meta_data for restore.
+ */
+static inline void bpf_compute_and_save_data_end(
+	struct sk_buff *skb, void **saved_data_end)
+{
+	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
+
+	*saved_data_end = cb->data_end;
+	cb->data_end  = skb->data + skb_headlen(skb);
+}
+
+/* Restore data saved by bpf_compute_data_pointers(). */
+static inline void bpf_restore_data_end(
+	struct sk_buff *skb, void *saved_data_end)
+{
+	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
+
+	cb->data_end = saved_data_end;
+}
+
 static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 {
 	/* eBPF programs may read/write skb->cb[] area to transfer meta

commit 604326b41a6fb9b4a78b6179335decee0365cd8c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:58 2018 +0200

    bpf, sockmap: convert to generic sk_msg interface
    
    Add a generic sk_msg layer, and convert current sockmap and later
    kTLS over to make use of it. While sk_buff handles network packet
    representation from netdevice up to socket, sk_msg handles data
    representation from application to socket layer.
    
    This means that sk_msg framework spans across ULP users in the
    kernel, and enables features such as introspection or filtering
    of data with the help of BPF programs that operate on this data
    structure.
    
    Latter becomes in particular useful for kTLS where data encryption
    is deferred into the kernel, and as such enabling the kernel to
    perform L7 introspection and policy based on BPF for TLS connections
    where the record is being encrypted after BPF has run and came to
    a verdict. In order to get there, first step is to transform open
    coding of scatter-gather list handling into a common core framework
    that subsystems can use.
    
    The code itself has been split and refactored into three bigger
    pieces: i) the generic sk_msg API which deals with managing the
    scatter gather ring, providing helpers for walking and mangling,
    transferring application data from user space into it, and preparing
    it for BPF pre/post-processing, ii) the plain sock map itself
    where sockets can be attached to or detached from; these bits
    are independent of i) which can now be used also without sock
    map, and iii) the integration with plain TCP as one protocol
    to be used for processing L7 application data (later this could
    e.g. also be extended to other protocols like UDP). The semantics
    are the same with the old sock map code and therefore no change
    of user facing behavior or APIs. While pursuing this work it
    also helped finding a number of bugs in the old sockmap code
    that we've fixed already in earlier commits. The test_sockmap
    kselftest suite passes through fine as well.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6791a0ac0139..5771874bc01e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -520,24 +520,6 @@ struct bpf_skb_data_end {
 	void *data_end;
 };
 
-struct sk_msg_buff {
-	void *data;
-	void *data_end;
-	__u32 apply_bytes;
-	__u32 cork_bytes;
-	int sg_copybreak;
-	int sg_start;
-	int sg_curr;
-	int sg_end;
-	struct scatterlist sg_data[MAX_SKB_FRAGS];
-	bool sg_copy[MAX_SKB_FRAGS];
-	__u32 flags;
-	struct sock *sk_redir;
-	struct sock *sk;
-	struct sk_buff *skb;
-	struct list_head list;
-};
-
 struct bpf_redirect_info {
 	u32 ifindex;
 	u32 flags;
@@ -833,9 +815,6 @@ void xdp_do_flush_map(void);
 
 void bpf_warn_invalid_xdp_action(u32 act);
 
-struct sock *do_sk_redirect_map(struct sk_buff *skb);
-struct sock *do_msg_redirect_map(struct sk_msg_buff *md);
-
 #ifdef CONFIG_INET
 struct sock *bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
 				  struct bpf_prog *prog, struct sk_buff *skb,

commit f6069b9aa9934ede26f41ac0781fce241279ad43
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Aug 17 23:26:14 2018 +0200

    bpf: fix redirect to map under tail calls
    
    Commits 109980b894e9 ("bpf: don't select potentially stale ri->map
    from buggy xdp progs") and 7c3001313396 ("bpf: fix ri->map_owner
    pointer on bpf_prog_realloc") tried to mitigate that buggy programs
    using bpf_redirect_map() helper call do not leave stale maps behind.
    Idea was to add a map_owner cookie into the per CPU struct redirect_info
    which was set to prog->aux by the prog making the helper call as a
    proof that the map is not stale since the prog is implicitly holding
    a reference to it. This owner cookie could later on get compared with
    the program calling into BPF whether they match and therefore the
    redirect could proceed with processing the map safely.
    
    In (obvious) hindsight, this approach breaks down when tail calls are
    involved since the original caller's prog->aux pointer does not have
    to match the one from one of the progs out of the tail call chain,
    and therefore the xdp buffer will be dropped instead of redirected.
    A way around that would be to fix the issue differently (which also
    allows to remove related work in fast path at the same time): once
    the life-time of a redirect map has come to its end we use it's map
    free callback where we need to wait on synchronize_rcu() for current
    outstanding xdp buffers and remove such a map pointer from the
    redirect info if found to be present. At that time no program is
    using this map anymore so we simply invalidate the map pointers to
    NULL iff they previously pointed to that instance while making sure
    that the redirect path only reads out the map once.
    
    Fixes: 97f91a7cf04f ("bpf: add bpf_redirect_map helper routine")
    Fixes: 109980b894e9 ("bpf: don't select potentially stale ri->map from buggy xdp progs")
    Reported-by: Sebastiano Miano <sebastiano.miano@polito.it>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 5d565c50bcb2..6791a0ac0139 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -543,7 +543,6 @@ struct bpf_redirect_info {
 	u32 flags;
 	struct bpf_map *map;
 	struct bpf_map *map_to_flush;
-	unsigned long   map_owner;
 	u32 kern_flags;
 };
 
@@ -781,6 +780,8 @@ static inline bool bpf_dump_raw_ok(void)
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
+void bpf_clear_redirect_map(struct bpf_map *map);
+
 static inline bool xdp_return_frame_no_direct(void)
 {
 	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);

commit 8217ca653ec601246832d562207bc24bdf652d2f
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:26 2018 -0700

    bpf: Enable BPF_PROG_TYPE_SK_REUSEPORT bpf prog in reuseport selection
    
    This patch allows a BPF_PROG_TYPE_SK_REUSEPORT bpf prog to select a
    SO_REUSEPORT sk from a BPF_MAP_TYPE_REUSEPORT_ARRAY introduced in
    the earlier patch.  "bpf_run_sk_reuseport()" will return -ECONNREFUSED
    when the BPF_PROG_TYPE_SK_REUSEPORT prog returns SK_DROP.
    The callers, in inet[6]_hashtable.c and ipv[46]/udp.c, are modified to
    handle this case and return NULL immediately instead of continuing the
    sk search from its hashtable.
    
    It re-uses the existing SO_ATTACH_REUSEPORT_EBPF setsockopt to attach
    BPF_PROG_TYPE_SK_REUSEPORT.  The "sk_reuseport_attach_bpf()" will check
    if the attaching bpf prog is in the new SK_REUSEPORT or the existing
    SOCKET_FILTER type and then check different things accordingly.
    
    One level of "__reuseport_attach_prog()" call is removed.  The
    "sk_unhashed() && ..." and "sk->sk_reuseport_cb" tests are pushed
    back to "reuseport_attach_prog()" in sock_reuseport.c.  sock_reuseport.c
    seems to have more knowledge on those test requirements than filter.c.
    In "reuseport_attach_prog()", after new_prog is attached to reuse->prog,
    the old_prog (if any) is also directly freed instead of returning the
    old_prog to the caller and asking the caller to free.
    
    The sysctl_optmem_max check is moved back to the
    "sk_reuseport_attach_filter()" and "sk_reuseport_attach_bpf()".
    As of other bpf prog types, the new BPF_PROG_TYPE_SK_REUSEPORT is only
    bounded by the usual "bpf_prog_charge_memlock()" during load time
    instead of bounded by both bpf_prog_charge_memlock and sysctl_optmem_max.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 70e9d57677fe..5d565c50bcb2 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -753,6 +753,7 @@ int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);
+void sk_reuseport_prog_free(struct bpf_prog *prog);
 int sk_detach_filter(struct sock *sk);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);

commit 2dbb9b9e6df67d444fbe425c7f6014858d337adf
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:25 2018 -0700

    bpf: Introduce BPF_PROG_TYPE_SK_REUSEPORT
    
    This patch adds a BPF_PROG_TYPE_SK_REUSEPORT which can select
    a SO_REUSEPORT sk from a BPF_MAP_TYPE_REUSEPORT_ARRAY.  Like other
    non SK_FILTER/CGROUP_SKB program, it requires CAP_SYS_ADMIN.
    
    BPF_PROG_TYPE_SK_REUSEPORT introduces "struct sk_reuseport_kern"
    to store the bpf context instead of using the skb->cb[48].
    
    At the SO_REUSEPORT sk lookup time, it is in the middle of transiting
    from a lower layer (ipv4/ipv6) to a upper layer (udp/tcp).  At this
    point,  it is not always clear where the bpf context can be appended
    in the skb->cb[48] to avoid saving-and-restoring cb[].  Even putting
    aside the difference between ipv4-vs-ipv6 and udp-vs-tcp.  It is not
    clear if the lower layer is only ipv4 and ipv6 in the future and
    will it not touch the cb[] again before transiting to the upper
    layer.
    
    For example, in udp_gro_receive(), it uses the 48 byte NAPI_GRO_CB
    instead of IP[6]CB and it may still modify the cb[] after calling
    the udp[46]_lib_lookup_skb().  Because of the above reason, if
    sk->cb is used for the bpf ctx, saving-and-restoring is needed
    and likely the whole 48 bytes cb[] has to be saved and restored.
    
    Instead of saving, setting and restoring the cb[], this patch opts
    to create a new "struct sk_reuseport_kern" and setting the needed
    values in there.
    
    The new BPF_PROG_TYPE_SK_REUSEPORT and "struct sk_reuseport_(kern|md)"
    will serve all ipv4/ipv6 + udp/tcp combinations.  There is no protocol
    specific usage at this point and it is also inline with the current
    sock_reuseport.c implementation (i.e. no protocol specific requirement).
    
    In "struct sk_reuseport_md", this patch exposes data/data_end/len
    with semantic similar to other existing usages.  Together
    with "bpf_skb_load_bytes()" and "bpf_skb_load_bytes_relative()",
    the bpf prog can peek anywhere in the skb.  The "bind_inany" tells
    the bpf prog that the reuseport group is bind-ed to a local
    INANY address which cannot be learned from skb.
    
    The new "bind_inany" is added to "struct sock_reuseport" which will be
    used when running the new "BPF_PROG_TYPE_SK_REUSEPORT" bpf prog in order
    to avoid repeating the "bind INANY" test on
    "sk_v6_rcv_saddr/sk->sk_rcv_saddr" every time a bpf prog is run.  It can
    only be properly initialized when a "sk->sk_reuseport" enabled sk is
    adding to a hashtable (i.e. during "reuseport_alloc()" and
    "reuseport_add_sock()").
    
    The new "sk_select_reuseport()" is the main helper that the
    bpf prog will use to select a SO_REUSEPORT sk.  It is the only function
    that can use the new BPF_MAP_TYPE_REUSEPORT_ARRAY.  As mentioned in
    the earlier patch, the validity of a selected sk is checked in
    run time in "sk_select_reuseport()".  Doing the check in
    verification time is difficult and inflexible (consider the map-in-map
    use case).  The runtime check is to compare the selected sk's reuseport_id
    with the reuseport_id that we want.  This helper will return -EXXX if the
    selected sk cannot serve the incoming request (e.g. reuseport_id
    not match).  The bpf prog can decide if it wants to do SK_DROP as its
    discretion.
    
    When the bpf prog returns SK_PASS, the kernel will check if a
    valid sk has been selected (i.e. "reuse_kern->selected_sk != NULL").
    If it does , it will use the selected sk.  If not, the kernel
    will select one from "reuse->socks[]" (as before this patch).
    
    The SK_DROP and SK_PASS handling logic will be in the next patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2b072dab32c0..70e9d57677fe 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -32,6 +32,7 @@ struct seccomp_data;
 struct bpf_prog_aux;
 struct xdp_rxq_info;
 struct xdp_buff;
+struct sock_reuseport;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
@@ -833,6 +834,20 @@ void bpf_warn_invalid_xdp_action(u32 act);
 struct sock *do_sk_redirect_map(struct sk_buff *skb);
 struct sock *do_msg_redirect_map(struct sk_msg_buff *md);
 
+#ifdef CONFIG_INET
+struct sock *bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
+				  struct bpf_prog *prog, struct sk_buff *skb,
+				  u32 hash);
+#else
+static inline struct sock *
+bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
+		     struct bpf_prog *prog, struct sk_buff *skb,
+		     u32 hash)
+{
+	return NULL;
+}
+#endif
+
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
 extern int bpf_jit_harden;

commit 2539650fadbf63a431e76535a9de7bff6ea5e409
Author: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
Date:   Fri Aug 3 16:58:16 2018 +0900

    xdp: Helpers for disabling napi_direct of xdp_return_frame
    
    We need some mechanism to disable napi_direct on calling
    xdp_return_frame_rx_napi() from some context.
    When veth gets support of XDP_REDIRECT, it will redirects packets which
    are redirected from other devices. On redirection veth will reuse
    xdp_mem_info of the redirection source device to make return_frame work.
    But in this case .ndo_xdp_xmit() called from veth redirection uses
    xdp_mem_info which is not guarded by NAPI, because the .ndo_xdp_xmit()
    is not called directly from the rxq which owns the xdp_mem_info.
    
    This approach introduces a flag in bpf_redirect_info to indicate that
    napi_direct should be disabled even when _rx_napi variant is used as
    well as helper functions to use it.
    
    A NAPI handler who wants to use this flag needs to call
    xdp_set_return_frame_no_direct() before processing packets, and call
    xdp_clear_return_frame_no_direct() after xdp_do_flush_map() before
    exiting NAPI.
    
    v4:
    - Use bpf_redirect_info for storing the flag instead of xdp_mem_info to
      avoid per-frame copy cost.
    
    Signed-off-by: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4717af8b95e6..2b072dab32c0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -543,10 +543,14 @@ struct bpf_redirect_info {
 	struct bpf_map *map;
 	struct bpf_map *map_to_flush;
 	unsigned long   map_owner;
+	u32 kern_flags;
 };
 
 DECLARE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);
 
+/* flags for bpf_redirect_info kern_flags */
+#define BPF_RI_F_RF_NO_DIRECT	BIT(0)	/* no napi_direct on return_frame */
+
 /* Compute the linear packet data range [data, data_end) which
  * will be accessed by various program types (cls_bpf, act_bpf,
  * lwt, ...). Subsystems allowing direct data access must (!)
@@ -775,6 +779,27 @@ static inline bool bpf_dump_raw_ok(void)
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
+static inline bool xdp_return_frame_no_direct(void)
+{
+	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+
+	return ri->kern_flags & BPF_RI_F_RF_NO_DIRECT;
+}
+
+static inline void xdp_set_return_frame_no_direct(void)
+{
+	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+
+	ri->kern_flags |= BPF_RI_F_RF_NO_DIRECT;
+}
+
+static inline void xdp_clear_return_frame_no_direct(void)
+{
+	struct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);
+
+	ri->kern_flags &= ~BPF_RI_F_RF_NO_DIRECT;
+}
+
 static inline int xdp_ok_fwd_dev(const struct net_device *fwd,
 				 unsigned int pktlen)
 {

commit 0b19cc0a8694d4295383f88bc3441765875a57bc
Author: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
Date:   Fri Aug 3 16:58:15 2018 +0900

    bpf: Make redirect_info accessible from modules
    
    We are going to add kern_flags field in redirect_info for kernel
    internal use.
    In order to avoid function call to access the flags, make redirect_info
    accessible from modules. Also as it is now non-static, add prefix bpf_
    to redirect_info.
    
    v6:
    - Fix sparse warning around EXPORT_SYMBOL.
    
    Signed-off-by: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c73dd7396886..4717af8b95e6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -537,6 +537,16 @@ struct sk_msg_buff {
 	struct list_head list;
 };
 
+struct bpf_redirect_info {
+	u32 ifindex;
+	u32 flags;
+	struct bpf_map *map;
+	struct bpf_map *map_to_flush;
+	unsigned long   map_owner;
+};
+
+DECLARE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);
+
 /* Compute the linear packet data range [data, data_end) which
  * will be accessed by various program types (cls_bpf, act_bpf,
  * lwt, ...). Subsystems allowing direct data access must (!)

commit d8d7218ad842e18fc6976b87c08ed749e8d56313
Author: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
Date:   Fri Jul 6 11:49:00 2018 +0900

    xdp: XDP_REDIRECT should check IFF_UP and MTU
    
    Otherwise we end up with attempting to send packets from down devices
    or to send oversized packets, which may cause unexpected driver/device
    behaviour. Generic XDP has already done this check, so reuse the logic
    in native XDP.
    
    Fixes: 814abfabef3c ("xdp: add bpf_redirect helper function")
    Signed-off-by: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 300baad62c88..c73dd7396886 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -765,8 +765,8 @@ static inline bool bpf_dump_raw_ok(void)
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
-static inline int __xdp_generic_ok_fwd_dev(struct sk_buff *skb,
-					   struct net_device *fwd)
+static inline int xdp_ok_fwd_dev(const struct net_device *fwd,
+				 unsigned int pktlen)
 {
 	unsigned int len;
 
@@ -774,7 +774,7 @@ static inline int __xdp_generic_ok_fwd_dev(struct sk_buff *skb,
 		return -ENETDOWN;
 
 	len = fwd->mtu + fwd->hard_header_len + VLAN_HLEN;
-	if (skb->len > len)
+	if (pktlen > len)
 		return -EMSGSIZE;
 
 	return 0;

commit 85782e037f8aba8922dadb24a1523ca0b82ab8bc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jun 28 23:34:59 2018 +0200

    bpf: undo prog rejection on read-only lock failure
    
    Partially undo commit 9facc336876f ("bpf: reject any prog that failed
    read-only lock") since it caused a regression, that is, syzkaller was
    able to manage to cause a panic via fault injection deep in set_memory_ro()
    path by letting an allocation fail: In x86's __change_page_attr_set_clr()
    it was able to change the attributes of the primary mapping but not in
    the alias mapping via cpa_process_alias(), so the second, inner call
    to the __change_page_attr() via __change_page_attr_set_clr() had to split
    a larger page and failed in the alloc_pages() with the artifically triggered
    allocation error which is then propagated down to the call site.
    
    Thus, for set_memory_ro() this means that it returned with an error, but
    from debugging a probe_kernel_write() revealed EFAULT on that memory since
    the primary mapping succeeded to get changed. Therefore the subsequent
    hdr->locked = 0 reset triggered the panic as it was performed on read-only
    memory, so call-site assumptions were infact wrong to assume that it would
    either succeed /or/ not succeed at all since there's no such rollback in
    set_memory_*() calls from partial change of mappings, in other words, we're
    left in a state that is "half done". A later undo via set_memory_rw() is
    succeeding though due to matching permissions on that part (aka due to the
    try_preserve_large_page() succeeding). While reproducing locally with
    explicitly triggering this error, the initial splitting only happens on
    rare occasions and in real world it would additionally need oom conditions,
    but that said, it could partially fail. Therefore, it is definitely wrong
    to bail out on set_memory_ro() error and reject the program with the
    set_memory_*() semantics we have today. Shouldn't have gone the extra mile
    since no other user in tree today infact checks for any set_memory_*()
    errors, e.g. neither module_enable_ro() / module_disable_ro() for module
    RO/NX handling which is mostly default these days nor kprobes core with
    alloc_insn_page() / free_insn_page() as examples that could be invoked long
    after bootup and original 314beb9bcabf ("x86: bpf_jit_comp: secure bpf jit
    against spraying attacks") did neither when it got first introduced to BPF
    so "improving" with bailing out was clearly not right when set_memory_*()
    cannot handle it today.
    
    Kees suggested that if set_memory_*() can fail, we should annotate it with
    __must_check, and all callers need to deal with it gracefully given those
    set_memory_*() markings aren't "advisory", but they're expected to actually
    do what they say. This might be an option worth to move forward in future
    but would at the same time require that set_memory_*() calls from supporting
    archs are guaranteed to be "atomic" in that they provide rollback if part
    of the range fails, once that happened, the transition from RW -> RO could
    be made more robust that way, while subsequent RO -> RW transition /must/
    continue guaranteeing to always succeed the undo part.
    
    Reported-by: syzbot+a4eb8c7766952a1ca872@syzkaller.appspotmail.com
    Reported-by: syzbot+d866d1925855328eac3b@syzkaller.appspotmail.com
    Fixes: 9facc336876f ("bpf: reject any prog that failed read-only lock")
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 20f2659dd829..300baad62c88 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -470,9 +470,7 @@ struct sock_fprog_kern {
 };
 
 struct bpf_binary_header {
-	u16 pages;
-	u16 locked:1;
-
+	u32 pages;
 	/* Some arches need word alignment for their instructions */
 	u8 image[] __aligned(4);
 };
@@ -481,7 +479,7 @@ struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	u16			jited:1,	/* Is our filter JIT'ed? */
 				jit_requested:1,/* archs need to JIT the prog */
-				locked:1,	/* Program image locked? */
+				undo_set_mem:1,	/* Passed set_memory_ro() checkpoint */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1,	/* Do we need dst entry? */
@@ -677,46 +675,24 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-	fp->locked = 1;
-	if (set_memory_ro((unsigned long)fp, fp->pages))
-		fp->locked = 0;
-#endif
+	fp->undo_set_mem = 1;
+	set_memory_ro((unsigned long)fp, fp->pages);
 }
 
 static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-	if (fp->locked) {
-		WARN_ON_ONCE(set_memory_rw((unsigned long)fp, fp->pages));
-		/* In case set_memory_rw() fails, we want to be the first
-		 * to crash here instead of some random place later on.
-		 */
-		fp->locked = 0;
-	}
-#endif
+	if (fp->undo_set_mem)
+		set_memory_rw((unsigned long)fp, fp->pages);
 }
 
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-	hdr->locked = 1;
-	if (set_memory_ro((unsigned long)hdr, hdr->pages))
-		hdr->locked = 0;
-#endif
+	set_memory_ro((unsigned long)hdr, hdr->pages);
 }
 
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 {
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-	if (hdr->locked) {
-		WARN_ON_ONCE(set_memory_rw((unsigned long)hdr, hdr->pages));
-		/* In case set_memory_rw() fails, we want to be the first
-		 * to crash here instead of some random place later on.
-		 */
-		hdr->locked = 0;
-	}
-#endif
+	set_memory_rw((unsigned long)hdr, hdr->pages);
 }
 
 static inline struct bpf_binary_header *
@@ -728,22 +704,6 @@ bpf_jit_binary_hdr(const struct bpf_prog *fp)
 	return (void *)addr;
 }
 
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-static inline int bpf_prog_check_pages_ro_single(const struct bpf_prog *fp)
-{
-	if (!fp->locked)
-		return -ENOLCK;
-	if (fp->jited) {
-		const struct bpf_binary_header *hdr = bpf_jit_binary_hdr(fp);
-
-		if (!hdr->locked)
-			return -ENOLCK;
-	}
-
-	return 0;
-}
-#endif
-
 int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 {

commit 9262478220eac908ae6e168c3df2c453c87e2da3
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 20 17:24:09 2018 -0700

    bpf: enforce correct alignment for instructions
    
    After commit 9facc336876f ("bpf: reject any prog that failed read-only lock")
    offsetof(struct bpf_binary_header, image) became 3 instead of 4,
    breaking powerpc BPF badly, since instructions need to be word aligned.
    
    Fixes: 9facc336876f ("bpf: reject any prog that failed read-only lock")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index b615df57b7d5..20f2659dd829 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -472,7 +472,9 @@ struct sock_fprog_kern {
 struct bpf_binary_header {
 	u16 pages;
 	u16 locked:1;
-	u8 image[];
+
+	/* Some arches need word alignment for their instructions */
+	u8 image[] __aligned(4);
 };
 
 struct bpf_prog {

commit 6d5fc1957989266006db6ef3dfb9159b42cf0189
Author: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
Date:   Thu Jun 14 11:07:42 2018 +0900

    xdp: Fix handling of devmap in generic XDP
    
    Commit 67f29e07e131 ("bpf: devmap introduce dev_map_enqueue") changed
    the return value type of __devmap_lookup_elem() from struct net_device *
    to struct bpf_dtab_netdev * but forgot to modify generic XDP code
    accordingly.
    
    Thus generic XDP incorrectly used struct bpf_dtab_netdev where struct
    net_device is expected, then skb->dev was set to invalid value.
    
    v2:
    - Fix compiler warning without CONFIG_BPF_SYSCALL.
    
    Fixes: 67f29e07e131 ("bpf: devmap introduce dev_map_enqueue")
    Signed-off-by: Toshiaki Makita <makita.toshiaki@lab.ntt.co.jp>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 108f9812e196..b615df57b7d5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -19,6 +19,7 @@
 #include <linux/cryptohash.h>
 #include <linux/set_memory.h>
 #include <linux/kallsyms.h>
+#include <linux/if_vlan.h>
 
 #include <net/sch_generic.h>
 
@@ -802,6 +803,21 @@ static inline bool bpf_dump_raw_ok(void)
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
+static inline int __xdp_generic_ok_fwd_dev(struct sk_buff *skb,
+					   struct net_device *fwd)
+{
+	unsigned int len;
+
+	if (unlikely(!(fwd->flags & IFF_UP)))
+		return -ENETDOWN;
+
+	len = fwd->mtu + fwd->hard_header_len + VLAN_HLEN;
+	if (skb->len > len)
+		return -EMSGSIZE;
+
+	return 0;
+}
+
 /* The pair of xdp_do_redirect and xdp_do_flush_map MUST be called in the
  * same cpu context. Further for best results no more than a single map
  * for the do_redirect/do_flush pair should be used. This limitation is

commit 9facc336876f7ecf9edba4c67b90426fde4ec898
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 15 02:30:48 2018 +0200

    bpf: reject any prog that failed read-only lock
    
    We currently lock any JITed image as read-only via bpf_jit_binary_lock_ro()
    as well as the BPF image as read-only through bpf_prog_lock_ro(). In
    the case any of these would fail we throw a WARN_ON_ONCE() in order to
    yell loudly to the log. Perhaps, to some extend, this may be comparable
    to an allocation where __GFP_NOWARN is explicitly not set.
    
    Added via 65869a47f348 ("bpf: improve read-only handling"), this behavior
    is slightly different compared to any of the other in-kernel set_memory_ro()
    users who do not check the return code of set_memory_ro() and friends /at
    all/ (e.g. in the case of module_enable_ro() / module_disable_ro()). Given
    in BPF this is mandatory hardening step, we want to know whether there
    are any issues that would leave both BPF data writable. So it happens
    that syzkaller enabled fault injection and it triggered memory allocation
    failure deep inside x86's change_page_attr_set_clr() which was triggered
    from set_memory_ro().
    
    Now, there are two options: i) leaving everything as is, and ii) reworking
    the image locking code in order to have a final checkpoint out of the
    central bpf_prog_select_runtime() which probes whether any of the calls
    during prog setup weren't successful, and then bailing out with an error.
    Option ii) is a better approach since this additional paranoia avoids
    altogether leaving any potential W+X pages from BPF side in the system.
    Therefore, lets be strict about it, and reject programs in such unlikely
    occasion. While testing I noticed also that one bpf_prog_lock_ro()
    call was missing on the outer dummy prog in case of calls, e.g. in the
    destructor we call bpf_prog_free_deferred() on the main prog where we
    try to bpf_prog_unlock_free() the program, and since we go via
    bpf_prog_select_runtime() do that as well.
    
    Reported-by: syzbot+3b889862e65a98317058@syzkaller.appspotmail.com
    Reported-by: syzbot+9e762b52dd17e616a7a5@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 297c56fa9cee..108f9812e196 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -469,7 +469,8 @@ struct sock_fprog_kern {
 };
 
 struct bpf_binary_header {
-	unsigned int pages;
+	u16 pages;
+	u16 locked:1;
 	u8 image[];
 };
 
@@ -671,15 +672,18 @@ bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
 	fp->locked = 1;
-	WARN_ON_ONCE(set_memory_ro((unsigned long)fp, fp->pages));
+	if (set_memory_ro((unsigned long)fp, fp->pages))
+		fp->locked = 0;
+#endif
 }
 
 static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
 	if (fp->locked) {
 		WARN_ON_ONCE(set_memory_rw((unsigned long)fp, fp->pages));
 		/* In case set_memory_rw() fails, we want to be the first
@@ -687,34 +691,30 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 		 */
 		fp->locked = 0;
 	}
+#endif
 }
 
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
-	WARN_ON_ONCE(set_memory_ro((unsigned long)hdr, hdr->pages));
-}
-
-static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
-{
-	WARN_ON_ONCE(set_memory_rw((unsigned long)hdr, hdr->pages));
-}
-#else
-static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
-{
-}
-
-static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
-{
-}
-
-static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
-{
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
+	hdr->locked = 1;
+	if (set_memory_ro((unsigned long)hdr, hdr->pages))
+		hdr->locked = 0;
+#endif
 }
 
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 {
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
+	if (hdr->locked) {
+		WARN_ON_ONCE(set_memory_rw((unsigned long)hdr, hdr->pages));
+		/* In case set_memory_rw() fails, we want to be the first
+		 * to crash here instead of some random place later on.
+		 */
+		hdr->locked = 0;
+	}
+#endif
 }
-#endif /* CONFIG_ARCH_HAS_SET_MEMORY */
 
 static inline struct bpf_binary_header *
 bpf_jit_binary_hdr(const struct bpf_prog *fp)
@@ -725,6 +725,22 @@ bpf_jit_binary_hdr(const struct bpf_prog *fp)
 	return (void *)addr;
 }
 
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
+static inline int bpf_prog_check_pages_ro_single(const struct bpf_prog *fp)
+{
+	if (!fp->locked)
+		return -ENOLCK;
+	if (fp->jited) {
+		const struct bpf_binary_header *hdr = bpf_jit_binary_hdr(fp);
+
+		if (!hdr->locked)
+			return -ENOLCK;
+	}
+
+	return 0;
+}
+#endif
+
 int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 {

commit 7d1982b4e335c1b184406b7566f6041bfe313c35
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 15 02:30:47 2018 +0200

    bpf: fix panic in prog load calls cleanup
    
    While testing I found that when hitting error path in bpf_prog_load()
    where we jump to free_used_maps and prog contained BPF to BPF calls
    that were JITed earlier, then we never clean up the bpf_prog_kallsyms_add()
    done under jit_subprogs(). Add proper API to make BPF kallsyms deletion
    more clear and fix that.
    
    Fixes: 1c2a088a6626 ("bpf: x64: add JIT support for multi-function programs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 45fc0f5000d8..297c56fa9cee 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -961,6 +961,9 @@ static inline void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 }
 #endif /* CONFIG_BPF_JIT */
 
+void bpf_prog_kallsyms_del_subprogs(struct bpf_prog *fp);
+void bpf_prog_kallsyms_del_all(struct bpf_prog *fp);
+
 #define BPF_ANC		BIT(15)
 
 static inline bool bpf_needs_clear_a(const struct sock_filter *first)

commit bc23105ca0abdeed98366af01c700c2c3aff5cd5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Jun 2 23:06:39 2018 +0200

    bpf: fix context access in tracing progs on 32 bit archs
    
    Wang reported that all the testcases for BPF_PROG_TYPE_PERF_EVENT
    program type in test_verifier report the following errors on x86_32:
    
      172/p unpriv: spill/fill of different pointers ldx FAIL
      Unexpected error message!
      0: (bf) r6 = r10
      1: (07) r6 += -8
      2: (15) if r1 == 0x0 goto pc+3
      R1=ctx(id=0,off=0,imm=0) R6=fp-8,call_-1 R10=fp0,call_-1
      3: (bf) r2 = r10
      4: (07) r2 += -76
      5: (7b) *(u64 *)(r6 +0) = r2
      6: (55) if r1 != 0x0 goto pc+1
      R1=ctx(id=0,off=0,imm=0) R2=fp-76,call_-1 R6=fp-8,call_-1 R10=fp0,call_-1 fp-8=fp
      7: (7b) *(u64 *)(r6 +0) = r1
      8: (79) r1 = *(u64 *)(r6 +0)
      9: (79) r1 = *(u64 *)(r1 +68)
      invalid bpf_context access off=68 size=8
    
      378/p check bpf_perf_event_data->sample_period byte load permitted FAIL
      Failed to load prog 'Permission denied'!
      0: (b7) r0 = 0
      1: (71) r0 = *(u8 *)(r1 +68)
      invalid bpf_context access off=68 size=1
    
      379/p check bpf_perf_event_data->sample_period half load permitted FAIL
      Failed to load prog 'Permission denied'!
      0: (b7) r0 = 0
      1: (69) r0 = *(u16 *)(r1 +68)
      invalid bpf_context access off=68 size=2
    
      380/p check bpf_perf_event_data->sample_period word load permitted FAIL
      Failed to load prog 'Permission denied'!
      0: (b7) r0 = 0
      1: (61) r0 = *(u32 *)(r1 +68)
      invalid bpf_context access off=68 size=4
    
      381/p check bpf_perf_event_data->sample_period dword load permitted FAIL
      Failed to load prog 'Permission denied'!
      0: (b7) r0 = 0
      1: (79) r0 = *(u64 *)(r1 +68)
      invalid bpf_context access off=68 size=8
    
    Reason is that struct pt_regs on x86_32 doesn't fully align to 8 byte
    boundary due to its size of 68 bytes. Therefore, bpf_ctx_narrow_access_ok()
    will then bail out saying that off & (size_default - 1) which is 68 & 7
    doesn't cleanly align in the case of sample_period access from struct
    bpf_perf_event_data, hence verifier wrongly thinks we might be doing an
    unaligned access here though underlying arch can handle it just fine.
    Therefore adjust this down to machine size and check and rewrite the
    offset for narrow access on that basis. We also need to fix corresponding
    pe_prog_is_valid_access(), since we hit the check for off % size != 0
    (e.g. 68 % 8 -> 4) in the first and last test. With that in place, progs
    for tracing work on x86_32.
    
    Reported-by: Wang YanQing <udknight@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Wang YanQing <udknight@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 8e60f1e9a702..45fc0f5000d8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -639,16 +639,34 @@ static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
 	return prog->type == BPF_PROG_TYPE_UNSPEC;
 }
 
-static inline bool
-bpf_ctx_narrow_access_ok(u32 off, u32 size, const u32 size_default)
+static inline u32 bpf_ctx_off_adjust_machine(u32 size)
+{
+	const u32 size_machine = sizeof(unsigned long);
+
+	if (size > size_machine && size % size_machine == 0)
+		size = size_machine;
+
+	return size;
+}
+
+static inline bool bpf_ctx_narrow_align_ok(u32 off, u32 size_access,
+					   u32 size_default)
 {
-	bool off_ok;
+	size_default = bpf_ctx_off_adjust_machine(size_default);
+	size_access  = bpf_ctx_off_adjust_machine(size_access);
+
 #ifdef __LITTLE_ENDIAN
-	off_ok = (off & (size_default - 1)) == 0;
+	return (off & (size_default - 1)) == 0;
 #else
-	off_ok = (off & (size_default - 1)) + size == size_default;
+	return (off & (size_default - 1)) + size_access == size_default;
 #endif
-	return off_ok && size <= size_default && (size & (size - 1)) == 0;
+}
+
+static inline bool
+bpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)
+{
+	return bpf_ctx_narrow_align_ok(off, size, size_default) &&
+	       size <= size_default && (size & (size - 1)) == 0;
 }
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))

commit 09772d92cd5ad998b0d5f6f46cd1658f8cb698cf
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Jun 2 23:06:35 2018 +0200

    bpf: avoid retpoline for lookup/update/delete calls on maps
    
    While some of the BPF map lookup helpers provide a ->map_gen_lookup()
    callback for inlining the map lookup altogether it is not available
    for every map, so the remaining ones have to call bpf_map_lookup_elem()
    helper which does a dispatch to map->ops->map_lookup_elem(). In
    times of retpolines, this will control and trap speculative execution
    rather than letting it do its work for the indirect call and will
    therefore cause a slowdown. Likewise, bpf_map_update_elem() and
    bpf_map_delete_elem() do not have an inlined version and need to call
    into their map->ops->map_update_elem() resp. map->ops->map_delete_elem()
    handlers.
    
    Before:
    
      # bpftool prog dump xlated id 1
        0: (bf) r2 = r10
        1: (07) r2 += -8
        2: (7a) *(u64 *)(r2 +0) = 0
        3: (18) r1 = map[id:1]
        5: (85) call __htab_map_lookup_elem#232656
        6: (15) if r0 == 0x0 goto pc+4
        7: (71) r1 = *(u8 *)(r0 +35)
        8: (55) if r1 != 0x0 goto pc+1
        9: (72) *(u8 *)(r0 +35) = 1
       10: (07) r0 += 56
       11: (15) if r0 == 0x0 goto pc+4
       12: (bf) r2 = r0
       13: (18) r1 = map[id:1]
       15: (85) call bpf_map_delete_elem#215008  <-- indirect call via
       16: (95) exit                                 helper
    
    After:
    
      # bpftool prog dump xlated id 1
        0: (bf) r2 = r10
        1: (07) r2 += -8
        2: (7a) *(u64 *)(r2 +0) = 0
        3: (18) r1 = map[id:1]
        5: (85) call __htab_map_lookup_elem#233328
        6: (15) if r0 == 0x0 goto pc+4
        7: (71) r1 = *(u8 *)(r0 +35)
        8: (55) if r1 != 0x0 goto pc+1
        9: (72) *(u8 *)(r0 +35) = 1
       10: (07) r0 += 56
       11: (15) if r0 == 0x0 goto pc+4
       12: (bf) r2 = r0
       13: (18) r1 = map[id:1]
       15: (85) call htab_lru_map_delete_elem#238240  <-- direct call
       16: (95) exit
    
    In all three lookup/update/delete cases however we can use the actual
    address of the map callback directly if we find that there's only a
    single path with a map pointer leading to the helper call, meaning
    when the map pointer has not been poisoned from verifier side.
    Example code can be seen above for the delete case.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6fd375fe7079..8e60f1e9a702 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -301,6 +301,9 @@ struct xdp_buff;
 
 /* Function call */
 
+#define BPF_CAST_CALL(x)					\
+		((u64 (*)(u64, u64, u64, u64, u64))(x))
+
 #define BPF_EMIT_CALL(FUNC)					\
 	((struct bpf_insn) {					\
 		.code  = BPF_JMP | BPF_CALL,			\

commit 06be0864c77ae6861632a678a6378511a4828d6e
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Jun 2 23:06:31 2018 +0200

    bpf: test case for map pointer poison with calls/branches
    
    Add several test cases where the same or different map pointers
    originate from different paths in the program and execute a map
    lookup or tail call at a common location.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d90abdaea94b..6fd375fe7079 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -289,6 +289,16 @@ struct xdp_buff;
 		.off   = OFF,					\
 		.imm   = 0 })
 
+/* Relative call */
+
+#define BPF_CALL_REL(TGT)					\
+	((struct bpf_insn) {					\
+		.code  = BPF_JMP | BPF_CALL,			\
+		.dst_reg = 0,					\
+		.src_reg = BPF_PSEUDO_CALL,			\
+		.off   = 0,					\
+		.imm   = TGT })
+
 /* Function call */
 
 #define BPF_EMIT_CALL(FUNC)					\

commit 1cedee13d25ab118d325f95588c1a084e9317229
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri May 25 08:55:23 2018 -0700

    bpf: Hooks for sys_sendmsg
    
    In addition to already existing BPF hooks for sys_bind and sys_connect,
    the patch provides new hooks for sys_sendmsg.
    
    It leverages existing BPF program type `BPF_PROG_TYPE_CGROUP_SOCK_ADDR`
    that provides access to socket itlself (properties like family, type,
    protocol) and user-passed `struct sockaddr *` so that BPF program can
    override destination IP and port for system calls such as sendto(2) or
    sendmsg(2) and/or assign source IP to the socket.
    
    The hooks are implemented as two new attach types:
    `BPF_CGROUP_UDP4_SENDMSG` and `BPF_CGROUP_UDP6_SENDMSG` for UDPv4 and
    UDPv6 correspondingly.
    
    UDPv4 and UDPv6 separate attach types for same reason as sys_bind and
    sys_connect hooks, i.e. to prevent reading from / writing to e.g.
    user_ip6 fields when user passes sockaddr_in since it'd be out-of-bound.
    
    The difference with already existing hooks is sys_sendmsg are
    implemented only for unconnected UDP.
    
    For TCP it doesn't make sense to change user-provided `struct sockaddr *`
    at sendto(2)/sendmsg(2) time since socket either was already connected
    and has source/destination set or wasn't connected and call to
    sendto(2)/sendmsg(2) would lead to ENOTCONN anyway.
    
    Connected UDP is already handled by sys_connect hooks that can override
    source/destination at connect time and use fast-path later, i.e. these
    hooks don't affect UDP fast-path.
    
    Rewriting source IP is implemented differently than that in sys_connect
    hooks. When sys_sendmsg is used with unconnected UDP it doesn't work to
    just bind socket to desired local IP address since source IP can be set
    on per-packet basis by using ancillary data (cmsg(3)). So no matter if
    socket is bound or not, source IP has to be rewritten on every call to
    sys_sendmsg.
    
    To do so two new fields are added to UAPI `struct bpf_sock_addr`;
    * `msg_src_ip4` to set source IPv4 for UDPv4;
    * `msg_src_ip6` to set source IPv6 for UDPv6.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d358d1815c16..d90abdaea94b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1010,6 +1010,7 @@ struct bpf_sock_addr_kern {
 	 * only two (src and dst) are available at convert_ctx_access time
 	 */
 	u64 tmp_reg;
+	void *t_ctx;	/* Attach type specific context. */
 };
 
 struct bpf_sock_ops_kern {

commit 303def35f64e37bcd5401d202889f5fbc0241179
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu May 17 14:16:58 2018 -0700

    bpf: allow sk_msg programs to read sock fields
    
    Currently sk_msg programs only have access to the raw data. However,
    it is often useful when building policies to have the policies specific
    to the socket endpoint. This allows using the socket tuple as input
    into filters, etc.
    
    This patch adds ctx access to the sock fields.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9dbcb9d55921..d358d1815c16 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -517,6 +517,7 @@ struct sk_msg_buff {
 	bool sg_copy[MAX_SKB_FRAGS];
 	__u32 flags;
 	struct sock *sk_redir;
+	struct sock *sk;
 	struct sk_buff *skb;
 	struct list_head list;
 };

commit e5cd3abcb31a48d4ea91bd32f0618802ca5f3592
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon May 14 10:00:16 2018 -0700

    bpf: sockmap, refactor sockmap routines to work with hashmap
    
    This patch only refactors the existing sockmap code. This will allow
    much of the psock initialization code path and bpf helper codes to
    work for both sockmap bpf map types that are backed by an array, the
    currently supported type, and the new hash backed bpf map type
    sockhash.
    
    Most the fallout comes from three changes,
    
      - Pushing bpf programs into an independent structure so we
        can use it from the htab struct in the next patch.
      - Generalizing helpers to use void *key instead of the hardcoded
        u32.
      - Instead of passing map/key through the metadata we now do
        the lookup inline. This avoids storing the key in the metadata
        which will be useful when keys can be longer than 4 bytes. We
        rename the sk pointers to sk_redir at this point as well to
        avoid any confusion between the current sk pointer and the
        redirect pointer sk_redir.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index da7e16523128..9dbcb9d55921 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -515,9 +515,8 @@ struct sk_msg_buff {
 	int sg_end;
 	struct scatterlist sg_data[MAX_SKB_FRAGS];
 	bool sg_copy[MAX_SKB_FRAGS];
-	__u32 key;
 	__u32 flags;
-	struct bpf_map *map;
+	struct sock *sk_redir;
 	struct sk_buff *skb;
 	struct list_head list;
 };

commit e0cea7ce988cf48cc4052235d2ad2550b3bc4fa0
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 4 01:08:14 2018 +0200

    bpf: implement ld_abs/ld_ind in native bpf
    
    The main part of this work is to finally allow removal of LD_ABS
    and LD_IND from the BPF core by reimplementing them through native
    eBPF instead. Both LD_ABS/LD_IND were carried over from cBPF and
    keeping them around in native eBPF caused way more trouble than
    actually worth it. To just list some of the security issues in
    the past:
    
      * fdfaf64e7539 ("x86: bpf_jit: support negative offsets")
      * 35607b02dbef ("sparc: bpf_jit: fix loads from negative offsets")
      * e0ee9c12157d ("x86: bpf_jit: fix two bugs in eBPF JIT compiler")
      * 07aee9439454 ("bpf, sparc: fix usage of wrong reg for load_skb_regs after call")
      * 6d59b7dbf72e ("bpf, s390x: do not reload skb pointers in non-skb context")
      * 87338c8e2cbb ("bpf, ppc64: do not reload skb pointers in non-skb context")
    
    For programs in native eBPF, LD_ABS/LD_IND are pretty much legacy
    these days due to their limitations and more efficient/flexible
    alternatives that have been developed over time such as direct
    packet access. LD_ABS/LD_IND only cover 1/2/4 byte loads into a
    register, the load happens in host endianness and its exception
    handling can yield unexpected behavior. The latter is explained
    in depth in f6b1b3bf0d5f ("bpf: fix subprog verifier bypass by
    div/mod by 0 exception") with similar cases of exceptions we had.
    In native eBPF more recent program types will disable LD_ABS/LD_IND
    altogether through may_access_skb() in verifier, and given the
    limitations in terms of exception handling, it's also disabled
    in programs that use BPF to BPF calls.
    
    In terms of cBPF, the LD_ABS/LD_IND is used in networking programs
    to access packet data. It is not used in seccomp-BPF but programs
    that use it for socket filtering or reuseport for demuxing with
    cBPF. This is mostly relevant for applications that have not yet
    migrated to native eBPF.
    
    The main complexity and source of bugs in LD_ABS/LD_IND is coming
    from their implementation in the various JITs. Most of them keep
    the model around from cBPF times by implementing a fastpath written
    in asm. They use typically two from the BPF program hidden CPU
    registers for caching the skb's headlen (skb->len - skb->data_len)
    and skb->data. Throughout the JIT phase this requires to keep track
    whether LD_ABS/LD_IND are used and if so, the two registers need
    to be recached each time a BPF helper would change the underlying
    packet data in native eBPF case. At least in eBPF case, available
    CPU registers are rare and the additional exit path out of the
    asm written JIT helper makes it also inflexible since not all
    parts of the JITer are in control from plain C. A LD_ABS/LD_IND
    implementation in eBPF therefore allows to significantly reduce
    the complexity in JITs with comparable performance results for
    them, e.g.:
    
    test_bpf             tcpdump port 22             tcpdump complex
    x64      - before    15 21 10                    14 19  18
             - after      7 10 10                     7 10  15
    arm64    - before    40 91 92                    40 91 151
             - after     51 64 73                    51 62 113
    
    For cBPF we now track any usage of LD_ABS/LD_IND in bpf_convert_filter()
    and cache the skb's headlen and data in the cBPF prologue. The
    BPF_REG_TMP gets remapped from R8 to R2 since it's mainly just
    used as a local temporary variable. This allows to shrink the
    image on x86_64 also for seccomp programs slightly since mapping
    to %rsi is not an ereg. In callee-saved R8 and R9 we now track
    skb data and headlen, respectively. For normal prologue emission
    in the JITs this does not add any extra instructions since R8, R9
    are pushed to stack in any case from eBPF side. cBPF uses the
    convert_bpf_ld_abs() emitter which probes the fast path inline
    already and falls back to bpf_skb_load_helper_{8,16,32}() helper
    relying on the cached skb data and headlen as well. R8 and R9
    never need to be reloaded due to bpf_helper_changes_pkt_data()
    since all skb access in cBPF is read-only. Then, for the case
    of native eBPF, we use the bpf_gen_ld_abs() emitter, which calls
    the bpf_skb_load_helper_{8,16,32}_no_cache() helper unconditionally,
    does neither cache skb data and headlen nor has an inlined fast
    path. The reason for the latter is that native eBPF does not have
    any extra registers available anyway, but even if there were, it
    avoids any reload of skb data and headlen in the first place.
    Additionally, for the negative offsets, we provide an alternative
    bpf_skb_load_bytes_relative() helper in eBPF which operates
    similarly as bpf_skb_load_bytes() and allows for more flexibility.
    Tested myself on x64, arm64, s390x, from Sandipan on ppc64.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index b7f81e3a70cb..da7e16523128 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -47,7 +47,9 @@ struct xdp_buff;
 /* Additional register mappings for converted user programs. */
 #define BPF_REG_A	BPF_REG_0
 #define BPF_REG_X	BPF_REG_7
-#define BPF_REG_TMP	BPF_REG_8
+#define BPF_REG_TMP	BPF_REG_2	/* scratch reg */
+#define BPF_REG_D	BPF_REG_8	/* data, callee-saved */
+#define BPF_REG_H	BPF_REG_9	/* hlen, callee-saved */
 
 /* Kernel hidden auxiliary/helper register for hardening step.
  * Only used by eBPF JITs. It's nothing more than a temporary

commit 02671e23e7b383763fe1ae4f20b56d8029f9dfc6
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 2 13:01:30 2018 +0200

    xsk: wire up XDP_SKB side of AF_XDP
    
    This commit wires up the xskmap to XDP_SKB layer.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 64899c04c1a6..b7f81e3a70cb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -760,7 +760,7 @@ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
  * This does not appear to be a real limitation for existing software.
  */
 int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,
-			    struct bpf_prog *prog);
+			    struct xdp_buff *xdp, struct bpf_prog *prog);
 int xdp_do_redirect(struct net_device *dev,
 		    struct xdp_buff *xdp,
 		    struct bpf_prog *prog);

commit c195651e565ae7f41a68acb7d4aa7390ad215de1
Author: Yonghong Song <yhs@fb.com>
Date:   Sat Apr 28 22:28:08 2018 -0700

    bpf: add bpf_get_stack helper
    
    Currently, stackmap and bpf_get_stackid helper are provided
    for bpf program to get the stack trace. This approach has
    a limitation though. If two stack traces have the same hash,
    only one will get stored in the stackmap table,
    so some stack traces are missing from user perspective.
    
    This patch implements a new helper, bpf_get_stack, will
    send stack traces directly to bpf program. The bpf program
    is able to see all stack traces, and then can do in-kernel
    processing or send stack traces to user space through
    shared map or bpf_perf_event_output.
    
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4da8b2308174..64899c04c1a6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -468,7 +468,8 @@ struct bpf_prog {
 				dst_needed:1,	/* Do we need dst entry? */
 				blinded:1,	/* Was blinded */
 				is_func:1,	/* program is a bpf function */
-				kprobe_override:1; /* Do we override a kprobe? */
+				kprobe_override:1, /* Do we override a kprobe? */
+				has_callchain_buf:1; /* callchain buffer allocated? */
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	enum bpf_attach_type	expected_attach_type; /* For some prog types */
 	u32			len;		/* Number of filter blocks */

commit 106ca27f2922e8de820d1bd3d79b1cbdf2d78eea
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Apr 17 16:45:37 2018 +0200

    xdp: move struct xdp_buff from filter.h to xdp.h
    
    This is done to prepare for the next patch, and it is also
    nice to move this XDP related struct out of filter.h.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index fc4e8f91b03d..4da8b2308174 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -30,6 +30,7 @@ struct sock;
 struct seccomp_data;
 struct bpf_prog_aux;
 struct xdp_rxq_info;
+struct xdp_buff;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
@@ -500,14 +501,6 @@ struct bpf_skb_data_end {
 	void *data_end;
 };
 
-struct xdp_buff {
-	void *data;
-	void *data_end;
-	void *data_meta;
-	void *data_hard_start;
-	struct xdp_rxq_info *rxq;
-};
-
 struct sk_msg_buff {
 	void *data;
 	void *data_end;
@@ -772,21 +765,6 @@ int xdp_do_redirect(struct net_device *dev,
 		    struct bpf_prog *prog);
 void xdp_do_flush_map(void);
 
-/* Drivers not supporting XDP metadata can use this helper, which
- * rejects any room expansion for metadata as a result.
- */
-static __always_inline void
-xdp_set_data_meta_invalid(struct xdp_buff *xdp)
-{
-	xdp->data_meta = xdp->data + 1;
-}
-
-static __always_inline bool
-xdp_data_meta_unsupported(const struct xdp_buff *xdp)
-{
-	return unlikely(xdp->data_meta > xdp->data);
-}
-
 void bpf_warn_invalid_xdp_action(u32 act);
 
 struct sock *do_sk_redirect_map(struct sk_buff *skb);

commit 4fbac77d2d092b475dda9eea66da674369665427
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:02 2018 -0700

    bpf: Hooks for sys_bind
    
    == The problem ==
    
    There is a use-case when all processes inside a cgroup should use one
    single IP address on a host that has multiple IP configured.  Those
    processes should use the IP for both ingress and egress, for TCP and UDP
    traffic. So TCP/UDP servers should be bound to that IP to accept
    incoming connections on it, and TCP/UDP clients should make outgoing
    connections from that IP. It should not require changing application
    code since it's often not possible.
    
    Currently it's solved by intercepting glibc wrappers around syscalls
    such as `bind(2)` and `connect(2)`. It's done by a shared library that
    is preloaded for every process in a cgroup so that whenever TCP/UDP
    server calls `bind(2)`, the library replaces IP in sockaddr before
    passing arguments to syscall. When application calls `connect(2)` the
    library transparently binds the local end of connection to that IP
    (`bind(2)` with `IP_BIND_ADDRESS_NO_PORT` to avoid performance penalty).
    
    Shared library approach is fragile though, e.g.:
    * some applications clear env vars (incl. `LD_PRELOAD`);
    * `/etc/ld.so.preload` doesn't help since some applications are linked
      with option `-z nodefaultlib`;
    * other applications don't use glibc and there is nothing to intercept.
    
    == The solution ==
    
    The patch provides much more reliable in-kernel solution for the 1st
    part of the problem: binding TCP/UDP servers on desired IP. It does not
    depend on application environment and implementation details (whether
    glibc is used or not).
    
    It adds new eBPF program type `BPF_PROG_TYPE_CGROUP_SOCK_ADDR` and
    attach types `BPF_CGROUP_INET4_BIND` and `BPF_CGROUP_INET6_BIND`
    (similar to already existing `BPF_CGROUP_INET_SOCK_CREATE`).
    
    The new program type is intended to be used with sockets (`struct sock`)
    in a cgroup and provided by user `struct sockaddr`. Pointers to both of
    them are parts of the context passed to programs of newly added types.
    
    The new attach types provides hooks in `bind(2)` system call for both
    IPv4 and IPv6 so that one can write a program to override IP addresses
    and ports user program tries to bind to and apply such a program for
    whole cgroup.
    
    == Implementation notes ==
    
    [1]
    Separate attach types for `AF_INET` and `AF_INET6` are added
    intentionally to prevent reading/writing to offsets that don't make
    sense for corresponding socket family. E.g. if user passes `sockaddr_in`
    it doesn't make sense to read from / write to `user_ip6[]` context
    fields.
    
    [2]
    The write access to `struct bpf_sock_addr_kern` is implemented using
    special field as an additional "register".
    
    There are just two registers in `sock_addr_convert_ctx_access`: `src`
    with value to write and `dst` with pointer to context that can't be
    changed not to break later instructions. But the fields, allowed to
    write to, are not available directly and to access them address of
    corresponding pointer has to be loaded first. To get additional register
    the 1st not used by `src` and `dst` one is taken, its content is saved
    to `bpf_sock_addr_kern.tmp_reg`, then the register is used to load
    address of pointer field, and finally the register's content is restored
    from the temporary field after writing `src` value.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 13c044e4832d..fc4e8f91b03d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1021,6 +1021,16 @@ static inline int bpf_tell_extensions(void)
 	return SKF_AD_MAX;
 }
 
+struct bpf_sock_addr_kern {
+	struct sock *sk;
+	struct sockaddr *uaddr;
+	/* Temporary "register" to make indirect stores to nested structures
+	 * defined above. We need three registers to make such a store, but
+	 * only two (src and dst) are available at convert_ctx_access time
+	 */
+	u64 tmp_reg;
+};
+
 struct bpf_sock_ops_kern {
 	struct	sock *sk;
 	u32	op;

commit 5e43f899b03a3492ce5fc44e8900becb04dae9c0
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:00 2018 -0700

    bpf: Check attach type at prog load time
    
    == The problem ==
    
    There are use-cases when a program of some type can be attached to
    multiple attach points and those attach points must have different
    permissions to access context or to call helpers.
    
    E.g. context structure may have fields for both IPv4 and IPv6 but it
    doesn't make sense to read from / write to IPv6 field when attach point
    is somewhere in IPv4 stack.
    
    Same applies to BPF-helpers: it may make sense to call some helper from
    some attach point, but not from other for same prog type.
    
    == The solution ==
    
    Introduce `expected_attach_type` field in in `struct bpf_attr` for
    `BPF_PROG_LOAD` command. If scenario described in "The problem" section
    is the case for some prog type, the field will be checked twice:
    
    1) At load time prog type is checked to see if attach type for it must
       be known to validate program permissions correctly. Prog will be
       rejected with EINVAL if it's the case and `expected_attach_type` is
       not specified or has invalid value.
    
    2) At attach time `attach_type` is compared with `expected_attach_type`,
       if prog type requires to have one, and, if they differ, attach will
       be rejected with EINVAL.
    
    The `expected_attach_type` is now available as part of `struct bpf_prog`
    in both `bpf_verifier_ops->is_valid_access()` and
    `bpf_verifier_ops->get_func_proto()` () and can be used to check context
    accesses and calls to helpers correspondingly.
    
    Initially the idea was discussed by Alexei Starovoitov <ast@fb.com> and
    Daniel Borkmann <daniel@iogearbox.net> here:
    https://marc.info/?l=linux-netdev&m=152107378717201&w=2
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 897ff3d95968..13c044e4832d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -469,6 +469,7 @@ struct bpf_prog {
 				is_func:1,	/* program is a bpf function */
 				kprobe_override:1; /* Do we override a kprobe? */
 	enum bpf_prog_type	type;		/* Type of BPF program */
+	enum bpf_attach_type	expected_attach_type; /* For some prog types */
 	u32			len;		/* Number of filter blocks */
 	u32			jited_len;	/* Size of jited insns in bytes */
 	u8			tag[BPF_TAG_SIZE];

commit fa246693a111fab32bd51d20f07a347e42773ee9
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Mar 28 12:49:25 2018 -0700

    bpf: sockmap, BPF_F_INGRESS flag for BPF_SK_SKB_STREAM_VERDICT:
    
    Add support for the BPF_F_INGRESS flag in skb redirect helper. To
    do this convert skb into a scatterlist and push into ingress queue.
    This is the same logic that is used in the sk_msg redirect helper
    so it should feel familiar.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 961cc5d53956..897ff3d95968 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -521,6 +521,7 @@ struct sk_msg_buff {
 	__u32 key;
 	__u32 flags;
 	struct bpf_map *map;
+	struct sk_buff *skb;
 	struct list_head list;
 };
 

commit 8934ce2fd08171e8605f7fada91ee7619fe17ab8
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Mar 28 12:49:15 2018 -0700

    bpf: sockmap redirect ingress support
    
    Add support for the BPF_F_INGRESS flag in sk_msg redirect helper.
    To do this add a scatterlist ring for receiving socks to check
    before calling into regular recvmsg call path. Additionally, because
    the poll wakeup logic only checked the skb recv queue we need to
    add a hook in TCP stack (similar to write side) so that we have
    a way to wake up polling socks when a scatterlist is redirected
    to that sock.
    
    After this all that is needed is for the redirect helper to
    push the scatterlist into the psock receive queue.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c2f167db8bd5..961cc5d53956 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -521,6 +521,7 @@ struct sk_msg_buff {
 	__u32 key;
 	__u32 flags;
 	struct bpf_map *map;
+	struct list_head list;
 };
 
 /* Compute the linear packet data range [data, data_end) which

commit e59ac634908f4ea90066e6db7dd7ae8ca02815ff
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Mar 28 17:48:33 2018 -0700

    bpf: add parenthesis around argument of BPF_LDST_BYTES()
    
    BPF_LDST_BYTES() does not put it's argument in parenthesis
    when referencing it.  This makes it impossible to pass pointers
    obtained by address-of operator (e.g. BPF_LDST_BYTES(&insn)).
    Add the parenthesis.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 109d05ccea9a..c2f167db8bd5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -372,7 +372,7 @@ struct xdp_rxq_info;
 
 #define BPF_LDST_BYTES(insn)					\
 	({							\
-		const int __size = bpf_size_to_bytes(BPF_SIZE(insn->code)); \
+		const int __size = bpf_size_to_bytes(BPF_SIZE((insn)->code)); \
 		WARN_ON(__size < 0);				\
 		__size;						\
 	})

commit 4f738adba30a7cfc006f605707e7aee847ffefa0
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Mar 18 12:57:10 2018 -0700

    bpf: create tcp_bpf_ulp allowing BPF to monitor socket TX/RX data
    
    This implements a BPF ULP layer to allow policy enforcement and
    monitoring at the socket layer. In order to support this a new
    program type BPF_PROG_TYPE_SK_MSG is used to run the policy at
    the sendmsg/sendpage hook. To attach the policy to sockets a
    sockmap is used with a new program attach type BPF_SK_MSG_VERDICT.
    
    Similar to previous sockmap usages when a sock is added to a
    sockmap, via a map update, if the map contains a BPF_SK_MSG_VERDICT
    program type attached then the BPF ULP layer is created on the
    socket and the attached BPF_PROG_TYPE_SK_MSG program is run for
    every msg in sendmsg case and page/offset in sendpage case.
    
    BPF_PROG_TYPE_SK_MSG Semantics/API:
    
    BPF_PROG_TYPE_SK_MSG supports only two return codes SK_PASS and
    SK_DROP. Returning SK_DROP free's the copied data in the sendmsg
    case and in the sendpage case leaves the data untouched. Both cases
    return -EACESS to the user. Returning SK_PASS will allow the msg to
    be sent.
    
    In the sendmsg case data is copied into kernel space buffers before
    running the BPF program. The kernel space buffers are stored in a
    scatterlist object where each element is a kernel memory buffer.
    Some effort is made to coalesce data from the sendmsg call here.
    For example a sendmsg call with many one byte iov entries will
    likely be pushed into a single entry. The BPF program is run with
    data pointers (start/end) pointing to the first sg element.
    
    In the sendpage case data is not copied. We opt not to copy the
    data by default here, because the BPF infrastructure does not
    know what bytes will be needed nor when they will be needed. So
    copying all bytes may be wasteful. Because of this the initial
    start/end data pointers are (0,0). Meaning no data can be read or
    written. This avoids reading data that may be modified by the
    user. A new helper is added later in this series if reading and
    writing the data is needed. The helper call will do a copy by
    default so that the page is exclusively owned by the BPF call.
    
    The verdict from the BPF_PROG_TYPE_SK_MSG applies to the entire msg
    in the sendmsg() case and the entire page/offset in the sendpage case.
    This avoids ambiguity on how to handle mixed return codes in the
    sendmsg case. Again a helper is added later in the series if
    a verdict needs to apply to multiple system calls and/or only
    a subpart of the currently being processed message.
    
    The helper msg_redirect_map() can be used to select the socket to
    send the data on. This is used similar to existing redirect use
    cases. This allows policy to redirect msgs.
    
    Pseudo code simple example:
    
    The basic logic to attach a program to a socket is as follows,
    
      // load the programs
      bpf_prog_load(SOCKMAP_TCP_MSG_PROG, BPF_PROG_TYPE_SK_MSG,
                    &obj, &msg_prog);
    
      // lookup the sockmap
      bpf_map_msg = bpf_object__find_map_by_name(obj, "my_sock_map");
    
      // get fd for sockmap
      map_fd_msg = bpf_map__fd(bpf_map_msg);
    
      // attach program to sockmap
      bpf_prog_attach(msg_prog, map_fd_msg, BPF_SK_MSG_VERDICT, 0);
    
    Adding sockets to the map is done in the normal way,
    
      // Add a socket 'fd' to sockmap at location 'i'
      bpf_map_update_elem(map_fd_msg, &i, fd, BPF_ANY);
    
    After the above any socket attached to "my_sock_map", in this case
    'fd', will run the BPF msg verdict program (msg_prog) on every
    sendmsg and sendpage system call.
    
    For a complete example see BPF selftests or sockmap samples.
    
    Implementation notes:
    
    It seemed the simplest, to me at least, to use a refcnt to ensure
    psock is not lost across the sendmsg copy into the sg, the bpf program
    running on the data in sg_data, and the final pass to the TCP stack.
    Some performance testing may show a better method to do this and avoid
    the refcnt cost, but for now use the simpler method.
    
    Another item that will come after basic support is in place is
    supporting MSG_MORE flag. At the moment we call sendpages even if
    the MSG_MORE flag is set. An enhancement would be to collect the
    pages into a larger scatterlist and pass down the stack. Notice that
    bpf_tcp_sendmsg() could support this with some additional state saved
    across sendmsg calls. I built the code to support this without having
    to do refactoring work. Other features TBD include ZEROCOPY and the
    TCP_RECV_QUEUE/TCP_NO_QUEUE support. This will follow initial series
    shortly.
    
    Future work could improve size limits on the scatterlist rings used
    here. Currently, we use MAX_SKB_FRAGS simply because this was being
    used already in the TLS case. Future work could extend the kernel sk
    APIs to tune this depending on workload. This is a trade-off
    between memory usage and throughput performance.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index fdb691b520c0..109d05ccea9a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -507,6 +507,22 @@ struct xdp_buff {
 	struct xdp_rxq_info *rxq;
 };
 
+struct sk_msg_buff {
+	void *data;
+	void *data_end;
+	__u32 apply_bytes;
+	__u32 cork_bytes;
+	int sg_copybreak;
+	int sg_start;
+	int sg_curr;
+	int sg_end;
+	struct scatterlist sg_data[MAX_SKB_FRAGS];
+	bool sg_copy[MAX_SKB_FRAGS];
+	__u32 key;
+	__u32 flags;
+	struct bpf_map *map;
+};
+
 /* Compute the linear packet data range [data, data_end) which
  * will be accessed by various program types (cls_bpf, act_bpf,
  * lwt, ...). Subsystems allowing direct data access must (!)
@@ -771,6 +787,7 @@ xdp_data_meta_unsupported(const struct xdp_buff *xdp)
 void bpf_warn_invalid_xdp_action(u32 act);
 
 struct sock *do_sk_redirect_map(struct sk_buff *skb);
+struct sock *do_msg_redirect_map(struct sk_msg_buff *md);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit 297dd12cb104151797fd649433a2157b585f1718
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Feb 13 14:15:36 2018 +0100

    net: avoid including xdp.h in filter.h
    
    If is sufficient with a forward declaration of struct xdp_rxq_info in
    linux/filter.h, which avoids including net/xdp.h.  This was originally
    suggested by John Fastabend during the review phase, but wasn't
    included in the final patchset revision.  Thus, this followup.
    
    Suggested-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 276932d75975..fdb691b520c0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -20,7 +20,6 @@
 #include <linux/set_memory.h>
 #include <linux/kallsyms.h>
 
-#include <net/xdp.h>
 #include <net/sch_generic.h>
 
 #include <uapi/linux/filter.h>
@@ -30,6 +29,7 @@ struct sk_buff;
 struct sock;
 struct seccomp_data;
 struct bpf_prog_aux;
+struct xdp_rxq_info;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function

commit 5e581dad4fec0e6d062740dc35b8dc248b39d224
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jan 26 23:33:38 2018 +0100

    bpf: make unknown opcode handling more robust
    
    Recent findings by syzcaller fixed in 7891a87efc71 ("bpf: arsh is
    not supported in 32 bit alu thus reject it") triggered a warning
    in the interpreter due to unknown opcode not being rejected by
    the verifier. The 'return 0' for an unknown opcode is really not
    optimal, since with BPF to BPF calls, this would go untracked by
    the verifier.
    
    Do two things here to improve the situation: i) perform basic insn
    sanity check early on in the verification phase and reject every
    non-uapi insn right there. The bpf_opcode_in_insntable() table
    reuses the same mapping as the jumptable in ___bpf_prog_run() sans
    the non-public mappings. And ii) in ___bpf_prog_run() we do need
    to BUG in the case where the verifier would ever create an unknown
    opcode due to some rewrites.
    
    Note that JITs do not have such issues since they would punt to
    interpreter in these situations. Moreover, the BPF_JIT_ALWAYS_ON
    would also help to avoid such unknown opcodes in the first place.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 20384c4bed25..276932d75975 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -688,6 +688,8 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 struct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err);
 void bpf_prog_free(struct bpf_prog *fp);
 
+bool bpf_opcode_in_insntable(u8 code);
+
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
 struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);

commit de525be2ca2734865d29c4b67ddd29913b214906
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Thu Jan 25 16:14:09 2018 -0800

    bpf: Support passing args to sock_ops bpf function
    
    Adds support for passing up to 4 arguments to sock_ops bpf functions. It
    reusues the reply union, so the bpf_sock_ops structures are not
    increased in size.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index daa5a676335f..20384c4bed25 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1003,6 +1003,7 @@ struct bpf_sock_ops_kern {
 	struct	sock *sk;
 	u32	op;
 	union {
+		u32 args[4];
 		u32 reply;
 		u32 replylong[4];
 	};

commit b73042b8a28e2603ac178295ab96c876ba5a97a1
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Thu Jan 25 16:14:08 2018 -0800

    bpf: Add write access to tcp_sock and sock fields
    
    This patch adds a macro, SOCK_OPS_SET_FIELD, for writing to
    struct tcp_sock or struct sock fields. This required adding a new
    field "temp" to struct bpf_sock_ops_kern for temporary storage that
    is used by sock_ops_convert_ctx_access. It is used to store and recover
    the contents of a register, so the register can be used to store the
    address of the sk. Since we cannot overwrite the dst_reg because it
    contains the pointer to ctx, nor the src_reg since it contains the value
    we want to store, we need an extra register to contain the address
    of the sk.
    
    Also adds the macro SOCK_OPS_GET_OR_SET_FIELD that calls one of the
    GET or SET macros depending on the value of the TYPE field.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 425056c7f96c..daa5a676335f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1007,6 +1007,15 @@ struct bpf_sock_ops_kern {
 		u32 replylong[4];
 	};
 	u32	is_fullsock;
+	u64	temp;			/* temp and everything after is not
+					 * initialized to 0 before calling
+					 * the BPF program. New fields that
+					 * should be initialized to 0 should
+					 * be inserted before temp.
+					 * temp is scratch storage used by
+					 * sock_ops_convert_ctx_access
+					 * as temporary storage of a register.
+					 */
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit aecd67b60722dd24353b0bc50e78a55b30707dcd
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Wed Jan 3 11:25:13 2018 +0100

    xdp: base API for new XDP rx-queue info concept
    
    This patch only introduce the core data structures and API functions.
    All XDP enabled drivers must use the API before this info can used.
    
    There is a need for XDP to know more about the RX-queue a given XDP
    frames have arrived on.  For both the XDP bpf-prog and kernel side.
    
    Instead of extending xdp_buff each time new info is needed, the patch
    creates a separate read-mostly struct xdp_rxq_info, that contains this
    info.  We stress this data/cache-line is for read-only info.  This is
    NOT for dynamic per packet info, use the data_meta for such use-cases.
    
    The performance advantage is this info can be setup at RX-ring init
    time, instead of updating N-members in xdp_buff.  A possible (driver
    level) micro optimization is that xdp_buff->rxq assignment could be
    done once per XDP/NAPI loop.  The extra pointer deref only happens for
    program needing access to this info (thus, no slowdown to existing
    use-cases).
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2b0df2703671..425056c7f96c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -20,6 +20,7 @@
 #include <linux/set_memory.h>
 #include <linux/kallsyms.h>
 
+#include <net/xdp.h>
 #include <net/sch_generic.h>
 
 #include <uapi/linux/filter.h>
@@ -503,6 +504,7 @@ struct xdp_buff {
 	void *data_end;
 	void *data_meta;
 	void *data_hard_start;
+	struct xdp_rxq_info *rxq;
 };
 
 /* Compute the linear packet data range [data, data_end) which

commit 7105e828c087de970fcb5a9509db51bfe6bd7894
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Dec 20 13:42:57 2017 +0100

    bpf: allow for correlation of maps and helpers in dump
    
    Currently a dump of an xlated prog (post verifier stage) doesn't
    correlate used helpers as well as maps. The prog info lists
    involved map ids, however there's no correlation of where in the
    program they are used as of today. Likewise, bpftool does not
    correlate helper calls with the target functions.
    
    The latter can be done w/o any kernel changes through kallsyms,
    and also has the advantage that this works with inlined helpers
    and BPF calls.
    
    Example, via interpreter:
    
      # tc filter show dev foo ingress
      filter protocol all pref 49152 bpf chain 0
      filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                          direct-action not_in_hw id 1 tag c74773051b364165   <-- prog id:1
    
      * Output before patch (calls/maps remain unclear):
    
      # bpftool prog dump xlated id 1             <-- dump prog id:1
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = 0xffff95c47a8d4800
       6: (85) call unknown#73040
       7: (15) if r0 == 0x0 goto pc+18
       8: (bf) r2 = r10
       9: (07) r2 += -4
      10: (bf) r1 = r0
      11: (85) call unknown#73040
      12: (15) if r0 == 0x0 goto pc+23
      [...]
    
      * Output after patch:
    
      # bpftool prog dump xlated id 1
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]                     <-- map id:2
       6: (85) call bpf_map_lookup_elem#73424     <-- helper call
       7: (15) if r0 == 0x0 goto pc+18
       8: (bf) r2 = r10
       9: (07) r2 += -4
      10: (bf) r1 = r0
      11: (85) call bpf_map_lookup_elem#73424
      12: (15) if r0 == 0x0 goto pc+23
      [...]
    
      # bpftool map show id 2                     <-- show/dump/etc map id:2
      2: hash_of_maps  flags 0x0
            key 4B  value 4B  max_entries 3  memlock 4096B
    
    Example, JITed, same prog:
    
      # tc filter show dev foo ingress
      filter protocol all pref 49152 bpf chain 0
      filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                      direct-action not_in_hw id 3 tag c74773051b364165 jited
    
      # bpftool prog show id 3
      3: sched_cls  tag c74773051b364165
            loaded_at Dec 19/13:48  uid 0
            xlated 384B  jited 257B  memlock 4096B  map_ids 2
    
      # bpftool prog dump xlated id 3
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]                      <-- map id:2
       6: (85) call __htab_map_lookup_elem#77408   <-+ inlined rewrite
       7: (15) if r0 == 0x0 goto pc+2                |
       8: (07) r0 += 56                              |
       9: (79) r0 = *(u64 *)(r0 +0)                <-+
      10: (15) if r0 == 0x0 goto pc+24
      11: (bf) r2 = r10
      12: (07) r2 += -4
      [...]
    
    Example, same prog, but kallsyms disabled (in that case we are
    also not allowed to pass any relative offsets, etc, so prog
    becomes pointer sanitized on dump):
    
      # sysctl kernel.kptr_restrict=2
      kernel.kptr_restrict = 2
    
      # bpftool prog dump xlated id 3
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]
       6: (85) call bpf_unspec#0
       7: (15) if r0 == 0x0 goto pc+2
      [...]
    
    Example, BPF calls via interpreter:
    
      # bpftool prog dump xlated id 1
       0: (85) call pc+2#__bpf_prog_run_args32
       1: (b7) r0 = 1
       2: (95) exit
       3: (b7) r0 = 2
       4: (95) exit
    
    Example, BPF calls via JIT:
    
      # sysctl net.core.bpf_jit_enable=1
      net.core.bpf_jit_enable = 1
      # sysctl net.core.bpf_jit_kallsyms=1
      net.core.bpf_jit_kallsyms = 1
    
      # bpftool prog dump xlated id 1
       0: (85) call pc+2#bpf_prog_3b185187f1855c4c_F
       1: (b7) r0 = 1
       2: (95) exit
       3: (b7) r0 = 2
       4: (95) exit
    
    And finally, an example for tail calls that is now working
    as well wrt correlation:
    
      # bpftool prog dump xlated id 2
      [...]
      10: (b7) r2 = 8
      11: (85) call bpf_trace_printk#-41312
      12: (bf) r1 = r6
      13: (18) r2 = map[id:1]
      15: (b7) r3 = 0
      16: (85) call bpf_tail_call#12
      17: (b7) r1 = 42
      18: (6b) *(u16 *)(r6 +46) = r1
      19: (b7) r0 = 0
      20: (95) exit
    
      # bpftool map show id 1
      1: prog_array  flags 0x0
            key 4B  value 4B  max_entries 1  memlock 4096B
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index e872b4ebaa57..2b0df2703671 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -18,6 +18,7 @@
 #include <linux/capability.h>
 #include <linux/cryptohash.h>
 #include <linux/set_memory.h>
+#include <linux/kallsyms.h>
 
 #include <net/sch_generic.h>
 
@@ -724,6 +725,14 @@ struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
 void bpf_jit_compile(struct bpf_prog *prog);
 bool bpf_helper_changes_pkt_data(void *func);
 
+static inline bool bpf_dump_raw_ok(void)
+{
+	/* Reconstruction of call-sites is dependent on kallsyms,
+	 * thus make dump the same restriction.
+	 */
+	return kallsyms_show_value() == 1;
+}
+
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 

commit 1c2a088a6626d4f51d2f2c97b0cbedbfbf3637f6
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:15 2017 -0800

    bpf: x64: add JIT support for multi-function programs
    
    Typical JIT does several passes over bpf instructions to
    compute total size and relative offsets of jumps and calls.
    With multitple bpf functions calling each other all relative calls
    will have invalid offsets intially therefore we need to additional
    last pass over the program to emit calls with correct offsets.
    For example in case of three bpf functions:
    main:
      call foo
      call bpf_map_lookup
      exit
    foo:
      call bar
      exit
    bar:
      exit
    
    We will call bpf_int_jit_compile() indepedently for main(), foo() and bar()
    x64 JIT typically does 4-5 passes to converge.
    After these initial passes the image for these 3 functions
    will be good except call targets, since start addresses of
    foo() and bar() are unknown when we were JITing main()
    (note that call bpf_map_lookup will be resolved properly
    during initial passes).
    Once start addresses of 3 functions are known we patch
    call_insn->imm to point to right functions and call
    bpf_int_jit_compile() again which needs only one pass.
    Additional safety checks are done to make sure this
    last pass doesn't produce image that is larger or smaller
    than previous pass.
    
    When constant blinding is on it's applied to all functions
    at the first pass, since doing it once again at the last
    pass can change size of the JITed code.
    
    Tested on x64 and arm64 hw with JIT on/off, blinding on/off.
    x64 jits bpf-to-bpf calls correctly while arm64 falls back to interpreter.
    All other JITs that support normal BPF_CALL will behave the same way
    since bpf-to-bpf call is equivalent to bpf-to-kernel call from
    JITs point of view.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3d6edc34932c..e872b4ebaa57 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -463,6 +463,8 @@ struct bpf_prog {
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1,	/* Do we need dst entry? */
+				blinded:1,	/* Was blinded */
+				is_func:1,	/* program is a bpf function */
 				kprobe_override:1; /* Do we override a kprobe? */
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */

commit 60b58afc96c9df71871df2dbad42037757ceef26
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:14 2017 -0800

    bpf: fix net.core.bpf_jit_enable race
    
    global bpf_jit_enable variable is tested multiple times in JITs,
    blinding and verifier core. The malicious root can try to toggle
    it while loading the programs. This race condition was accounted
    for and there should be no issues, but it's safer to avoid
    this race condition.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f26e6da1007b..3d6edc34932c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -458,6 +458,7 @@ struct bpf_binary_header {
 struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	u16			jited:1,	/* Is our filter JIT'ed? */
+				jit_requested:1,/* archs need to JIT the prog */
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
@@ -804,7 +805,7 @@ static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
 	return fp->jited && bpf_jit_is_ebpf();
 }
 
-static inline bool bpf_jit_blinding_enabled(void)
+static inline bool bpf_jit_blinding_enabled(struct bpf_prog *prog)
 {
 	/* These are the prerequisites, should someone ever have the
 	 * idea to call blinding outside of them, we make sure to
@@ -812,7 +813,7 @@ static inline bool bpf_jit_blinding_enabled(void)
 	 */
 	if (!bpf_jit_is_ebpf())
 		return false;
-	if (!bpf_jit_enable)
+	if (!prog->jit_requested)
 		return false;
 	if (!bpf_jit_harden)
 		return false;

commit 1ea47e01ad6ea0fe99697c54c2413d81dd21fe32
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:13 2017 -0800

    bpf: add support for bpf_call to interpreter
    
    though bpf_call is still the same call instruction and
    calling convention 'bpf to bpf' and 'bpf to helper' is the same
    the interpreter has to oparate on 'struct bpf_insn *'.
    To distinguish these two cases add a kernel internal opcode and
    mark call insns with it.
    This opcode is seen by interpreter only. JITs will never see it.
    Also add tiny bit of debug code to aid interpreter debugging.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 5feb441d3dd9..f26e6da1007b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -58,6 +58,9 @@ struct bpf_prog_aux;
 /* unused opcode to mark special call to bpf_tail_call() helper */
 #define BPF_TAIL_CALL	0xf0
 
+/* unused opcode to mark call to interpreter with arguments */
+#define BPF_CALL_ARGS	0xe0
+
 /* As per nm, we expose JITed images as text (code) section for
  * kallsyms. That way, tools like perf can find it to match
  * addresses.
@@ -710,6 +713,9 @@ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+#define __bpf_call_base_args \
+	((u64 (*)(u64, u64, u64, u64, u64, const struct bpf_insn *)) \
+	 __bpf_call_base)
 
 struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
 void bpf_jit_compile(struct bpf_prog *prog);

commit 9802d86585db91655c7d1929a4f6bbe0952ea88e
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Dec 11 11:36:48 2017 -0500

    bpf: add a bpf_override_function helper
    
    Error injection is sloppy and very ad-hoc.  BPF could fill this niche
    perfectly with it's kprobe functionality.  We could make sure errors are
    only triggered in specific call chains that we care about with very
    specific situations.  Accomplish this with the bpf_override_funciton
    helper.  This will modify the probe'd callers return value to the
    specified value and set the PC to an override function that simply
    returns, bypassing the originally probed function.  This gives us a nice
    clean way to implement systematic error injection for all of our code
    paths.
    
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0062302e1285..5feb441d3dd9 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -458,7 +458,8 @@ struct bpf_prog {
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
-				dst_needed:1;	/* Do we need dst entry? */
+				dst_needed:1,	/* Do we need dst entry? */
+				kprobe_override:1; /* Do we override a kprobe? */
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */
 	u32			jited_len;	/* Size of jited insns in bytes */

commit f19397a5c65665d66e3866b42056f1f58b7a366b
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Dec 1 10:15:04 2017 -0800

    bpf: Add access to snd_cwnd and others in sock_ops
    
    Adds read access to snd_cwnd and srtt_us fields of tcp_sock. Since these
    fields are only valid if the socket associated with the sock_ops program
    call is a full socket, the field is_fullsock is also added to the
    bpf_sock_ops struct. If the socket is not a full socket, reading these
    fields returns 0.
    
    Note that in most cases it will not be necessary to check is_fullsock to
    know if there is a full socket. The context of the call, as specified by
    the 'op' field, can sometimes determine whether there is a full socket.
    
    The struct bpf_sock_ops has the following fields added:
    
      __u32 is_fullsock;      /* Some TCP fields are only valid if
                               * there is a full socket. If not, the
                               * fields read as zero.
                               */
      __u32 snd_cwnd;
      __u32 srtt_us;          /* Averaged RTT << 3 in usecs */
    
    There is a new macro, SOCK_OPS_GET_TCP32(NAME), to make it easier to add
    read access to more 32 bit tcp_sock fields.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 80b5b482cb46..0062302e1285 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -985,6 +985,7 @@ struct bpf_sock_ops_kern {
 		u32 reply;
 		u32 replylong[4];
 	};
+	u32	is_fullsock;
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit 7c225c69f86c934e3be9be63ecde754e286838d7
Merge: 6363b3f3ac5b 1b7176aea0a9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 19:42:40 2017 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - a few misc bits
    
     - ocfs2 updates
    
     - almost all of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (131 commits)
      memory hotplug: fix comments when adding section
      mm: make alloc_node_mem_map a void call if we don't have CONFIG_FLAT_NODE_MEM_MAP
      mm: simplify nodemask printing
      mm,oom_reaper: remove pointless kthread_run() error check
      mm/page_ext.c: check if page_ext is not prepared
      writeback: remove unused function parameter
      mm: do not rely on preempt_count in print_vma_addr
      mm, sparse: do not swamp log with huge vmemmap allocation failures
      mm/hmm: remove redundant variable align_end
      mm/list_lru.c: mark expected switch fall-through
      mm/shmem.c: mark expected switch fall-through
      mm/page_alloc.c: broken deferred calculation
      mm: don't warn about allocations which stall for too long
      fs: fuse: account fuse_inode slab memory as reclaimable
      mm, page_alloc: fix potential false positive in __zone_watermark_ok
      mm: mlock: remove lru_add_drain_all()
      mm, sysctl: make NUMA stats configurable
      shmem: convert shmem_init_inodecache() to void
      Unify migrate_pages and move_pages access checks
      mm, pagevec: rename pagevec drained field
      ...

commit 4950276672fce5c241857540f8561c440663673d
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:51 2017 -0800

    kmemcheck: remove annotations
    
    Patch series "kmemcheck: kill kmemcheck", v2.
    
    As discussed at LSF/MM, kill kmemcheck.
    
    KASan is a replacement that is able to work without the limitation of
    kmemcheck (single CPU, slow).  KASan is already upstream.
    
    We are also not aware of any users of kmemcheck (or users who don't
    consider KASan as a suitable replacement).
    
    The only objection was that since KASAN wasn't supported by all GCC
    versions provided by distros at that time we should hold off for 2
    years, and try again.
    
    Now that 2 years have passed, and all distros provide gcc that supports
    KASAN, kill kmemcheck again for the very same reasons.
    
    This patch (of 4):
    
    Remove kmemcheck annotations, and calls to kmemcheck from the kernel.
    
    [alexander.levin@verizon.com: correctly remove kmemcheck call from dma_map_sg_attrs]
      Link: http://lkml.kernel.org/r/20171012192151.26531-1-alexander.levin@verizon.com
    Link: http://lkml.kernel.org/r/20171007030159.22241-2-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 48ec57e70f9f..42197b16dd78 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -454,13 +454,11 @@ struct bpf_binary_header {
 
 struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
-	kmemcheck_bitfield_begin(meta);
 	u16			jited:1,	/* Is our filter JIT'ed? */
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1;	/* Do we need dst entry? */
-	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */
 	u32			jited_len;	/* Size of jited insns in bytes */

commit f3edacbd697f94a743fff1a3d26910ab99948ba7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 11 18:24:55 2017 +0900

    bpf: Revert bpf_overrid_function() helper changes.
    
    NACK'd by x86 maintainer.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index eaec066f99e8..0cd02ff4ae30 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -459,8 +459,7 @@ struct bpf_prog {
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
-				dst_needed:1,	/* Do we need dst entry? */
-				kprobe_override:1; /* Do we override a kprobe? */
+				dst_needed:1;	/* Do we need dst entry? */
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */

commit dd0bb688eaa241b5655d396d45366cba9225aed9
Author: Josef Bacik <jbacik@fb.com>
Date:   Tue Nov 7 15:28:42 2017 -0500

    bpf: add a bpf_override_function helper
    
    Error injection is sloppy and very ad-hoc.  BPF could fill this niche
    perfectly with it's kprobe functionality.  We could make sure errors are
    only triggered in specific call chains that we care about with very
    specific situations.  Accomplish this with the bpf_override_funciton
    helper.  This will modify the probe'd callers return value to the
    specified value and set the PC to an override function that simply
    returns, bypassing the originally probed function.  This gives us a nice
    clean way to implement systematic error injection for all of our code
    paths.
    
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0cd02ff4ae30..eaec066f99e8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -459,7 +459,8 @@ struct bpf_prog {
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
-				dst_needed:1;	/* Do we need dst entry? */
+				dst_needed:1,	/* Do we need dst entry? */
+				kprobe_override:1; /* Do we override a kprobe? */
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */

commit 2a171788ba7bb61995e98e8163204fc7880f63b2
Merge: bf5345882bd1 d4c2e9fca5b7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 4 09:26:51 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Files removed in 'net-next' had their license header updated
    in 'net'.  We take the remove from 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 818a0b26249e..48ec57e70f9f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Linux Socket Filter Data Structures
  */

commit f8ddadc4db6c7b7029b6d0e0d9af24f74ad27ca2
Merge: bdd091bab8c6 b5ac3beb5a9f
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Oct 22 13:36:53 2017 +0100

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    There were quite a few overlapping sets of changes here.
    
    Daniel's bug fix for off-by-ones in the new BPF branch instructions,
    along with the added allowances for "data_end > ptr + x" forms
    collided with the metadata additions.
    
    Along with those three changes came veritifer test cases, which in
    their final form I tried to group together properly.  If I had just
    trimmed GIT's conflict tags as-is, this would have split up the
    meta tests unnecessarily.
    
    In the socketmap code, a set of preemption disabling changes
    overlapped with the rename of bpf_compute_data_end() to
    bpf_compute_data_pointers().
    
    Changes were made to the mv88e6060.c driver set addr method
    which got removed in net-next.
    
    The hyperv transport socket layer had a locking change in 'net'
    which overlapped with a change of socket state macro usage
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 34f79502bbcfab659b8729da68b5e387f96eb4c1
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Oct 18 07:10:36 2017 -0700

    bpf: avoid preempt enable/disable in sockmap using tcp_skb_cb region
    
    SK_SKB BPF programs are run from the socket/tcp context but early in
    the stack before much of the TCP metadata is needed in tcp_skb_cb. So
    we can use some unused fields to place BPF metadata needed for SK_SKB
    programs when implementing the redirect function.
    
    This allows us to drop the preempt disable logic. It does however
    require an API change so sk_redirect_map() has been updated to
    additionally provide ctx_ptr to skb. Note, we do however continue to
    disable/enable preemption around actual BPF program running to account
    for map updates.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d29e58fde364..818a0b26249e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -728,7 +728,7 @@ void xdp_do_flush_map(void);
 void bpf_warn_invalid_xdp_action(u32 act);
 void bpf_warn_invalid_xdp_redirect(u32 ifindex);
 
-struct sock *do_sk_redirect_map(void);
+struct sock *do_sk_redirect_map(struct sk_buff *skb);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit 324bda9e6c5add86ba2e1066476481c48132aca0
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Oct 2 22:50:21 2017 -0700

    bpf: multi program support for cgroup+bpf
    
    introduce BPF_F_ALLOW_MULTI flag that can be used to attach multiple
    bpf programs to a cgroup.
    
    The difference between three possible flags for BPF_PROG_ATTACH command:
    - NONE(default): No further bpf programs allowed in the subtree.
    - BPF_F_ALLOW_OVERRIDE: If a sub-cgroup installs some bpf program,
      the program in this cgroup yields to sub-cgroup program.
    - BPF_F_ALLOW_MULTI: If a sub-cgroup installs some bpf program,
      that cgroup program gets run in addition to the program in this cgroup.
    
    NONE and BPF_F_ALLOW_OVERRIDE existed before. This patch doesn't
    change their behavior. It only clarifies the semantics in relation
    to new flag.
    
    Only one program is allowed to be attached to a cgroup with
    NONE or BPF_F_ALLOW_OVERRIDE flag.
    Multiple programs are allowed to be attached to a cgroup with
    BPF_F_ALLOW_MULTI flag. They are executed in FIFO order
    (those that were attached first, run first)
    The programs of sub-cgroup are executed first, then programs of
    this cgroup and then programs of parent cgroup.
    All eligible programs are executed regardless of return code from
    earlier programs.
    
    To allow efficient execution of multiple programs attached to a cgroup
    and to avoid penalizing cgroups without any programs attached
    introduce 'struct bpf_prog_array' which is RCU protected array
    of pointers to bpf programs.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    for cgroup bits
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 911d454af107..2d2db394b0ca 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -481,7 +481,7 @@ struct sk_filter {
 	struct bpf_prog	*prog;
 };
 
-#define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
+#define BPF_PROG_RUN(filter, ctx)  (*(filter)->bpf_func)(ctx, (filter)->insnsi)
 
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 

commit de8f3a83b0a0fddb2cf56e7a718127e9619ea3da
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Sep 25 02:25:51 2017 +0200

    bpf: add meta pointer for direct access
    
    This work enables generic transfer of metadata from XDP into skb. The
    basic idea is that we can make use of the fact that the resulting skb
    must be linear and already comes with a larger headroom for supporting
    bpf_xdp_adjust_head(), which mangles xdp->data. Here, we base our work
    on a similar principle and introduce a small helper bpf_xdp_adjust_meta()
    for adjusting a new pointer called xdp->data_meta. Thus, the packet has
    a flexible and programmable room for meta data, followed by the actual
    packet data. struct xdp_buff is therefore laid out that we first point
    to data_hard_start, then data_meta directly prepended to data followed
    by data_end marking the end of packet. bpf_xdp_adjust_head() takes into
    account whether we have meta data already prepended and if so, memmove()s
    this along with the given offset provided there's enough room.
    
    xdp->data_meta is optional and programs are not required to use it. The
    rationale is that when we process the packet in XDP (e.g. as DoS filter),
    we can push further meta data along with it for the XDP_PASS case, and
    give the guarantee that a clsact ingress BPF program on the same device
    can pick this up for further post-processing. Since we work with skb
    there, we can also set skb->mark, skb->priority or other skb meta data
    out of BPF, thus having this scratch space generic and programmable
    allows for more flexibility than defining a direct 1:1 transfer of
    potentially new XDP members into skb (it's also more efficient as we
    don't need to initialize/handle each of such new members). The facility
    also works together with GRO aggregation. The scratch space at the head
    of the packet can be multiple of 4 byte up to 32 byte large. Drivers not
    yet supporting xdp->data_meta can simply be set up with xdp->data_meta
    as xdp->data + 1 as bpf_xdp_adjust_meta() will detect this and bail out,
    such that the subsequent match against xdp->data for later access is
    guaranteed to fail.
    
    The verifier treats xdp->data_meta/xdp->data the same way as we treat
    xdp->data/xdp->data_end pointer comparisons. The requirement for doing
    the compare against xdp->data is that it hasn't been modified from it's
    original address we got from ctx access. It may have a range marking
    already from prior successful xdp->data/xdp->data_end pointer comparisons
    though.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 052bab3d62e7..911d454af107 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -487,12 +487,14 @@ struct sk_filter {
 
 struct bpf_skb_data_end {
 	struct qdisc_skb_cb qdisc_cb;
+	void *data_meta;
 	void *data_end;
 };
 
 struct xdp_buff {
 	void *data;
 	void *data_end;
+	void *data_meta;
 	void *data_hard_start;
 };
 
@@ -507,7 +509,8 @@ static inline void bpf_compute_data_pointers(struct sk_buff *skb)
 	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
 
 	BUILD_BUG_ON(sizeof(*cb) > FIELD_SIZEOF(struct sk_buff, cb));
-	cb->data_end = skb->data + skb_headlen(skb);
+	cb->data_meta = skb->data - skb_metadata_len(skb);
+	cb->data_end  = skb->data + skb_headlen(skb);
 }
 
 static inline u8 *bpf_skb_cb(struct sk_buff *skb)
@@ -728,8 +731,22 @@ int xdp_do_redirect(struct net_device *dev,
 		    struct bpf_prog *prog);
 void xdp_do_flush_map(void);
 
+/* Drivers not supporting XDP metadata can use this helper, which
+ * rejects any room expansion for metadata as a result.
+ */
+static __always_inline void
+xdp_set_data_meta_invalid(struct xdp_buff *xdp)
+{
+	xdp->data_meta = xdp->data + 1;
+}
+
+static __always_inline bool
+xdp_data_meta_unsupported(const struct xdp_buff *xdp)
+{
+	return unlikely(xdp->data_meta > xdp->data);
+}
+
 void bpf_warn_invalid_xdp_action(u32 act);
-void bpf_warn_invalid_xdp_redirect(u32 ifindex);
 
 struct sock *do_sk_redirect_map(void);
 

commit 6aaae2b6c4330a46204bca042f1d2f41e8e18dea
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Sep 25 02:25:50 2017 +0200

    bpf: rename bpf_compute_data_end into bpf_compute_data_pointers
    
    Just do the rename into bpf_compute_data_pointers() as we'll add
    one more pointer here to recompute.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d29e58fde364..052bab3d62e7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -496,10 +496,13 @@ struct xdp_buff {
 	void *data_hard_start;
 };
 
-/* compute the linear packet data range [data, data_end) which
- * will be accessed by cls_bpf, act_bpf and lwt programs
+/* Compute the linear packet data range [data, data_end) which
+ * will be accessed by various program types (cls_bpf, act_bpf,
+ * lwt, ...). Subsystems allowing direct data access must (!)
+ * ensure that cb[] area can be written to when BPF program is
+ * invoked (otherwise cb[] save/restore is necessary).
  */
-static inline void bpf_compute_data_end(struct sk_buff *skb)
+static inline void bpf_compute_data_pointers(struct sk_buff *skb)
 {
 	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
 

commit 2facaad6000f2322eb40ca379aced31c957f0a41
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Thu Aug 24 12:33:08 2017 +0200

    xdp: make generic xdp redirect use tracepoint trace_xdp_redirect
    
    If the xdp_do_generic_redirect() call fails, it trigger the
    trace_xdp_exception tracepoint.  It seems better to use the same
    tracepoint trace_xdp_redirect, as the native xdp_do_redirect{,_map} does.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7015116331af..d29e58fde364 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -718,7 +718,8 @@ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
  * because we only track one map and force a flush when the map changes.
  * This does not appear to be a real limitation for existing software.
  */
-int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb);
+int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,
+			    struct bpf_prog *prog);
 int xdp_do_redirect(struct net_device *dev,
 		    struct xdp_buff *xdp,
 		    struct bpf_prog *prog);

commit 174a79ff9515f400b9a6115643dafd62a635b7e6
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Aug 15 22:32:47 2017 -0700

    bpf: sockmap with sk redirect support
    
    Recently we added a new map type called dev map used to forward XDP
    packets between ports (6093ec2dc313). This patches introduces a
    similar notion for sockets.
    
    A sockmap allows users to add participating sockets to a map. When
    sockets are added to the map enough context is stored with the
    map entry to use the entry with a new helper
    
      bpf_sk_redirect_map(map, key, flags)
    
    This helper (analogous to bpf_redirect_map in XDP) is given the map
    and an entry in the map. When called from a sockmap program, discussed
    below, the skb will be sent on the socket using skb_send_sock().
    
    With the above we need a bpf program to call the helper from that will
    then implement the send logic. The initial site implemented in this
    series is the recv_sock hook. For this to work we implemented a map
    attach command to add attributes to a map. In sockmap we add two
    programs a parse program and a verdict program. The parse program
    uses strparser to build messages and pass them to the verdict program.
    The parse programs use the normal strparser semantics. The verdict
    program is of type SK_SKB.
    
    The verdict program returns a verdict SK_DROP, or  SK_REDIRECT for
    now. Additional actions may be added later. When SK_REDIRECT is
    returned, expected when bpf program uses bpf_sk_redirect_map(), the
    sockmap logic will consult per cpu variables set by the helper routine
    and pull the sock entry out of the sock map. This pattern follows the
    existing redirect logic in cls and xdp programs.
    
    This gives the flow,
    
     recv_sock -> str_parser (parse_prog) -> verdict_prog -> skb_send_sock
                                                         \
                                                          -> kfree_skb
    
    As an example use case a message based load balancer may use specific
    logic in the verdict program to select the sock to send on.
    
    Sample programs are provided in future patches that hopefully illustrate
    the user interfaces. Also selftests are in follow-on patches.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d19ed3c15e1e..7015116331af 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -727,6 +727,8 @@ void xdp_do_flush_map(void);
 void bpf_warn_invalid_xdp_action(u32 act);
 void bpf_warn_invalid_xdp_redirect(u32 ifindex);
 
+struct sock *do_sk_redirect_map(void);
+
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
 extern int bpf_jit_harden;

commit 2ddf71e23cc246e95af72a6deed67b4a50a7b81c
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:30:02 2017 -0700

    net: add notifier hooks for devmap bpf map
    
    The BPF map devmap holds a refcnt on the net_device structure when
    it is in the map. We need to do this to ensure on driver unload we
    don't lose a dev reference.
    
    However, its not very convenient to have to manually unload the map
    when destroying a net device so add notifier handlers to do the cleanup
    automatically. But this creates a race between update/destroy BPF
    syscall and programs and the unregister netdev hook.
    
    Unfortunately, the best I could come up with is either to live with
    requiring manual removal of net devices from the map before removing
    the net device OR to add a mutex in devmap to ensure the map is not
    modified while we are removing a device. The fallout also requires
    that BPF programs no longer update/delete the map from the BPF program
    side because the mutex may sleep and this can not be done from inside
    an rcu critical section.  This is not a real problem though because I
    have not come up with any use cases where this is actually useful in
    practice. If/when we come up with a compelling user for this we may
    need to revisit this.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3323ee91c172..d19ed3c15e1e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -716,7 +716,7 @@ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
  * same cpu context. Further for best results no more than a single map
  * for the do_redirect/do_flush pair should be used. This limitation is
  * because we only track one map and force a flush when the map changes.
- * This does not appear to be a real limiation for existing software.
+ * This does not appear to be a real limitation for existing software.
  */
 int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb);
 int xdp_do_redirect(struct net_device *dev,

commit 11393cc9b9be2a1f61559e6fb9c27bc8fa20b1ff
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:29:40 2017 -0700

    xdp: Add batching support to redirect map
    
    For performance reasons we want to avoid updating the tail pointer in
    the driver tx ring as much as possible. To accomplish this we add
    batching support to the redirect path in XDP.
    
    This adds another ndo op "xdp_flush" that is used to inform the driver
    that it should bump the tail pointer on the TX ring.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ce8211fa91c7..3323ee91c172 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -712,10 +712,17 @@ bool bpf_helper_changes_pkt_data(void *func);
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
+/* The pair of xdp_do_redirect and xdp_do_flush_map MUST be called in the
+ * same cpu context. Further for best results no more than a single map
+ * for the do_redirect/do_flush pair should be used. This limitation is
+ * because we only track one map and force a flush when the map changes.
+ * This does not appear to be a real limiation for existing software.
+ */
 int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb);
 int xdp_do_redirect(struct net_device *dev,
 		    struct xdp_buff *xdp,
 		    struct bpf_prog *prog);
+void xdp_do_flush_map(void);
 
 void bpf_warn_invalid_xdp_action(u32 act);
 void bpf_warn_invalid_xdp_redirect(u32 ifindex);

commit 5acaee0a8964c9bab7775ab8bedcd1f66a2a1011
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:28:35 2017 -0700

    xdp: add trace event for xdp redirect
    
    This adds a trace event for xdp redirect which may help when debugging
    XDP programs that use redirect bpf commands.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 10df7daf5ec6..ce8211fa91c7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -713,7 +713,9 @@ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
 int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb);
-int xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp);
+int xdp_do_redirect(struct net_device *dev,
+		    struct xdp_buff *xdp,
+		    struct bpf_prog *prog);
 
 void bpf_warn_invalid_xdp_action(u32 act);
 void bpf_warn_invalid_xdp_redirect(u32 ifindex);

commit 6103aa96ec077c976e851e0b89cc2446cb76573d
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:27:50 2017 -0700

    net: implement XDP_REDIRECT for xdp generic
    
    Add support for redirect to xdp generic creating a fall back for
    devices that do not yet have support and allowing test infrastructure
    using veth pairs to be built.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Tested-by: Andy Gospodarek <andy@greyhouse.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 64cae7a08148..10df7daf5ec6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -712,6 +712,7 @@ bool bpf_helper_changes_pkt_data(void *func);
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 
+int xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb);
 int xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp);
 
 void bpf_warn_invalid_xdp_action(u32 act);

commit 814abfabef3ceed390c10d06a0cc69a86454b6cf
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:27:07 2017 -0700

    xdp: add bpf_redirect helper function
    
    This adds support for a bpf_redirect helper function to the XDP
    infrastructure. For now this only supports redirecting to the egress
    path of a port.
    
    In order to support drivers handling a xdp_buff natively this patches
    uses a new ndo operation ndo_xdp_xmit() that takes pushes a xdp_buff
    to the specified device.
    
    If the program specifies either (a) an unknown device or (b) a device
    that does not support the operation a BPF warning is thrown and the
    XDP_ABORTED error code is returned.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bfef1e5734f8..64cae7a08148 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -711,7 +711,11 @@ bool bpf_helper_changes_pkt_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
+
+int xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp);
+
 void bpf_warn_invalid_xdp_action(u32 act);
+void bpf_warn_invalid_xdp_redirect(u32 ifindex);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit 820a0b24b261c650cb07ea0f60aea9191f658f25
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 6 15:36:01 2017 -0700

    include/linux/filter.h: use linux/set_memory.h
    
    This header always exists, so doesn't require an ifdef around its
    inclusion.  When CONFIG_ARCH_HAS_SET_MEMORY=y it includes the asm
    header, otherwise it provides empty versions of the set_memory_xx()
    routines.
    
    Link: http://lkml.kernel.org/r/1498717781-29151-4-git-send-email-mpe@ellerman.id.au
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f1fc9baa3509..bfef1e5734f8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -16,13 +16,10 @@
 #include <linux/sched.h>
 #include <linux/capability.h>
 #include <linux/cryptohash.h>
+#include <linux/set_memory.h>
 
 #include <net/sch_generic.h>
 
-#ifdef CONFIG_ARCH_HAS_SET_MEMORY
-#include <asm/set_memory.h>
-#endif
-
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 

commit f96da09473b52c09125cc9bf7d7d4576ae8229e0
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Jul 2 02:13:27 2017 +0200

    bpf: simplify narrower ctx access
    
    This work tries to make the semantics and code around the
    narrower ctx access a bit easier to follow. Right now
    everything is done inside the .is_valid_access(). Offset
    matching is done differently for read/write types, meaning
    writes don't support narrower access and thus matching only
    on offsetof(struct foo, bar) is enough whereas for read
    case that supports narrower access we must check for
    offsetof(struct foo, bar) + offsetof(struct foo, bar) +
    sizeof(<bar>) - 1 for each of the cases. For read cases of
    individual members that don't support narrower access (like
    packet pointers or skb->cb[] case which has its own narrow
    access logic), we check as usual only offsetof(struct foo,
    bar) like in write case. Then, for the case where narrower
    access is allowed, we also need to set the aux info for the
    access. Meaning, ctx_field_size and converted_op_size have
    to be set. First is the original field size e.g. sizeof(<bar>)
    as in above example from the user facing ctx, and latter
    one is the target size after actual rewrite happened, thus
    for the kernel facing ctx. Also here we need the range match
    and we need to keep track changing convert_ctx_access() and
    converted_op_size from is_valid_access() as both are not at
    the same location.
    
    We can simplify the code a bit: check_ctx_access() becomes
    simpler in that we only store ctx_field_size as a meta data
    and later in convert_ctx_accesses() we fetch the target_size
    right from the location where we do convert. Should the verifier
    be misconfigured we do reject for BPF_WRITE cases or target_size
    that are not provided. For the subsystems, we always work on
    ranges in is_valid_access() and add small helpers for ranges
    and narrow access, convert_ctx_accesses() sets target_size
    for the relevant instruction.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Cc: Yonghong Song <yhs@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 738f8b14f025..f1fc9baa3509 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -337,6 +337,22 @@ struct bpf_prog_aux;
 	bpf_size;						\
 })
 
+#define bpf_size_to_bytes(bpf_size)				\
+({								\
+	int bytes = -EINVAL;					\
+								\
+	if (bpf_size == BPF_B)					\
+		bytes = sizeof(u8);				\
+	else if (bpf_size == BPF_H)				\
+		bytes = sizeof(u16);				\
+	else if (bpf_size == BPF_W)				\
+		bytes = sizeof(u32);				\
+	else if (bpf_size == BPF_DW)				\
+		bytes = sizeof(u64);				\
+								\
+	bytes;							\
+})
+
 #define BPF_SIZEOF(type)					\
 	({							\
 		const int __size = bytes_to_bpf_size(sizeof(type)); \
@@ -351,6 +367,13 @@ struct bpf_prog_aux;
 		__size;						\
 	})
 
+#define BPF_LDST_BYTES(insn)					\
+	({							\
+		const int __size = bpf_size_to_bytes(BPF_SIZE(insn->code)); \
+		WARN_ON(__size < 0);				\
+		__size;						\
+	})
+
 #define __BPF_MAP_0(m, v, ...) v
 #define __BPF_MAP_1(m, v, t, a, ...) m(t, a)
 #define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)
@@ -401,6 +424,18 @@ struct bpf_prog_aux;
 #define BPF_CALL_4(name, ...)	BPF_CALL_x(4, name, __VA_ARGS__)
 #define BPF_CALL_5(name, ...)	BPF_CALL_x(5, name, __VA_ARGS__)
 
+#define bpf_ctx_range(TYPE, MEMBER)						\
+	offsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1
+#define bpf_ctx_range_till(TYPE, MEMBER1, MEMBER2)				\
+	offsetof(TYPE, MEMBER1) ... offsetofend(TYPE, MEMBER2) - 1
+
+#define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)				\
+	({									\
+		BUILD_BUG_ON(FIELD_SIZEOF(TYPE, MEMBER) != (SIZE));		\
+		*(PTR_SIZE) = (SIZE);						\
+		offsetof(TYPE, MEMBER);						\
+	})
+
 #ifdef CONFIG_COMPAT
 /* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {
@@ -564,6 +599,18 @@ static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
 	return prog->type == BPF_PROG_TYPE_UNSPEC;
 }
 
+static inline bool
+bpf_ctx_narrow_access_ok(u32 off, u32 size, const u32 size_default)
+{
+	bool off_ok;
+#ifdef __LITTLE_ENDIAN
+	off_ok = (off & (size_default - 1)) == 0;
+#else
+	off_ok = (off & (size_default - 1)) + size == size_default;
+#endif
+	return off_ok && size <= size_default && (size & (size - 1)) == 0;
+}
+
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
 #ifdef CONFIG_ARCH_HAS_SET_MEMORY

commit 40304b2a1567fecc321f640ee4239556dd0f3ee0
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:40 2017 -0700

    bpf: BPF support for sock_ops
    
    Created a new BPF program type, BPF_PROG_TYPE_SOCK_OPS, and a corresponding
    struct that allows BPF programs of this type to access some of the
    socket's fields (such as IP addresses, ports, etc.). It uses the
    existing bpf cgroups infrastructure so the programs can be attached per
    cgroup with full inheritance support. The program will be called at
    appropriate times to set relevant connections parameters such as buffer
    sizes, SYN and SYN-ACK RTOs, etc., based on connection information such
    as IP addresses, port numbers, etc.
    
    Alghough there are already 3 mechanisms to set parameters (sysctls,
    route metrics and setsockopts), this new mechanism provides some
    distinct advantages. Unlike sysctls, it can set parameters per
    connection. In contrast to route metrics, it can also use port numbers
    and information provided by a user level program. In addition, it could
    set parameters probabilistically for evaluation purposes (i.e. do
    something different on 10% of the flows and compare results with the
    other 90% of the flows). Also, in cases where IPv6 addresses contain
    geographic information, the rules to make changes based on the distance
    (or RTT) between the hosts are much easier than route metric rules and
    can be global. Finally, unlike setsockopt, it oes not require
    application changes and it can be updated easily at any time.
    
    Although the bpf cgroup framework already contains a sock related
    program type (BPF_PROG_TYPE_CGROUP_SOCK), I created the new type
    (BPF_PROG_TYPE_SOCK_OPS) beccause the existing type expects to be called
    only once during the connections's lifetime. In contrast, the new
    program type will be called multiple times from different places in the
    network stack code.  For example, before sending SYN and SYN-ACKs to set
    an appropriate timeout, when the connection is established to set
    congestion control, etc. As a result it has "op" field to specify the
    type of operation requested.
    
    The purpose of this new program type is to simplify setting connection
    parameters, such as buffer sizes, TCP's SYN RTO, etc. For example, it is
    easy to use facebook's internal IPv6 addresses to determine if both hosts
    of a connection are in the same datacenter. Therefore, it is easy to
    write a BPF program to choose a small SYN RTO value when both hosts are
    in the same datacenter.
    
    This patch only contains the framework to support the new BPF program
    type, following patches add the functionality to set various connection
    parameters.
    
    This patch defines a new BPF program type: BPF_PROG_TYPE_SOCKET_OPS
    and a new bpf syscall command to load a new program of this type:
    BPF_PROG_LOAD_SOCKET_OPS.
    
    Two new corresponding structs (one for the kernel one for the user/BPF
    program):
    
    /* kernel version */
    struct bpf_sock_ops_kern {
            struct sock *sk;
            __u32  op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
    };
    
    /* user version
     * Some fields are in network byte order reflecting the sock struct
     * Use the bpf_ntohl helper macro in samples/bpf/bpf_endian.h to
     * convert them to host byte order.
     */
    struct bpf_sock_ops {
            __u32 op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
            __u32 family;
            __u32 remote_ip4;     /* In network byte order */
            __u32 local_ip4;      /* In network byte order */
            __u32 remote_ip6[4];  /* In network byte order */
            __u32 local_ip6[4];   /* In network byte order */
            __u32 remote_port;    /* In network byte order */
            __u32 local_port;     /* In host byte horder */
    };
    
    Currently there are two types of ops. The first type expects the BPF
    program to return a value which is then used by the caller (or a
    negative value to indicate the operation is not supported). The second
    type expects state changes to be done by the BPF program, for example
    through a setsockopt BPF helper function, and they ignore the return
    value.
    
    The reply fields of the bpf_sockt_ops struct are there in case a bpf
    program needs to return a value larger than an integer.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1fa26dc562ce..738f8b14f025 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -898,4 +898,13 @@ static inline int bpf_tell_extensions(void)
 	return SKF_AD_MAX;
 }
 
+struct bpf_sock_ops_kern {
+	struct	sock *sk;
+	u32	op;
+	union {
+		u32 reply;
+		u32 replylong[4];
+	};
+};
+
 #endif /* __LINUX_FILTER_H__ */

commit 1e270976908686ec25fb91b8a34145be54137976
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:52 2017 -0700

    bpf: Add BPF_OBJ_GET_INFO_BY_FD
    
    A single BPF_OBJ_GET_INFO_BY_FD cmd is used to obtain the info
    for both bpf_prog and bpf_map.  The kernel can figure out the
    fd is associated with a bpf_prog or bpf_map.
    
    The suggested struct bpf_prog_info and struct bpf_map_info are
    not meant to be a complete list and it is not the goal of this patch.
    New fields can be added in the future patch.
    
    The focus of this patch is to create the interface,
    BPF_OBJ_GET_INFO_BY_FD cmd for exposing the bpf_prog's and
    bpf_map's info.
    
    The obj's info, which will be extended (and get bigger) over time, is
    separated from the bpf_attr to avoid bloating the bpf_attr.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1e2dddf21f3b..1fa26dc562ce 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -69,8 +69,6 @@ struct bpf_prog_aux;
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
-#define BPF_TAG_SIZE	8
-
 /* Helper macros for filter block array initializers. */
 
 /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */

commit 783d28dd11f68fb25d1f2e0de7c42336394ef128
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:51 2017 -0700

    bpf: Add jited_len to struct bpf_prog
    
    Add jited_len to struct bpf_prog.  It will be
    useful for the struct bpf_prog_info which will
    be added in the later patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a20ba40fcb73..1e2dddf21f3b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -432,6 +432,7 @@ struct bpf_prog {
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */
+	u32			jited_len;	/* Size of jited insns in bytes */
 	u8			tag[BPF_TAG_SIZE];
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */

commit 71189fa9b092ef125ee741eccb2f5fa916798afd
Author: Alexei Starovoitov <ast@fb.com>
Date:   Tue May 30 13:31:27 2017 -0700

    bpf: free up BPF_JMP | BPF_CALL | BPF_X opcode
    
    free up BPF_JMP | BPF_CALL | BPF_X opcode to be used by actual
    indirect call by register and use kernel internal opcode to
    mark call instruction into bpf_tail_call() helper.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 62d948f80730..a20ba40fcb73 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -57,6 +57,9 @@ struct bpf_prog_aux;
 #define BPF_REG_AX		MAX_BPF_REG
 #define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
 
+/* unused opcode to mark special call to bpf_tail_call() helper */
+#define BPF_TAIL_CALL	0xf0
+
 /* As per nm, we expose JITed images as text (code) section for
  * kallsyms. That way, tools like perf can find it to match
  * addresses.

commit 614d0d77b49a9b131e58b77473698ab5b2c525b7
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu May 25 01:05:09 2017 +0200

    bpf: add various verifier test cases
    
    This patch adds various verifier test cases:
    
    1) A test case for the pruning issue when tracking alignment
       is used.
    2) Various PTR_TO_MAP_VALUE_OR_NULL tests to make sure pointer
       arithmetic turns such register into UNKNOWN_VALUE type.
    3) Test cases for the special treatment of LD_ABS/LD_IND to
       make sure verifier doesn't break calling convention here.
       Latter is needed, since f.e. arm64 JIT uses r1 - r5 for
       storing temporary data, so they really must be marked as
       NOT_INIT.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 56197f82af45..62d948f80730 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -272,6 +272,16 @@ struct bpf_prog_aux;
 		.off   = OFF,					\
 		.imm   = IMM })
 
+/* Unconditional jumps, goto pc + off16 */
+
+#define BPF_JMP_A(OFF)						\
+	((struct bpf_insn) {					\
+		.code  = BPF_JMP | BPF_JA,			\
+		.dst_reg = 0,					\
+		.src_reg = 0,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
 /* Function call */
 
 #define BPF_EMIT_CALL(FUNC)					\

commit 2d0bde57f3527ffac9279b4c8ba61060ba395b1a
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:58:26 2017 -0700

    include/linux/filter.h: use set_memory.h header
    
    set_memory_* functions have moved to set_memory.h.  Switch to this
    explicitly.
    
    Link: http://lkml.kernel.org/r/1488920133-27229-11-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9a7786db14fa..56197f82af45 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -19,7 +19,9 @@
 
 #include <net/sch_generic.h>
 
-#include <asm/cacheflush.h>
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
+#include <asm/set_memory.h>
+#endif
 
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>

commit e390b55d5aefe2b51569068b2a505d19d72afbf1
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Apr 24 22:14:35 2017 +0200

    bpf: make bpf_xdp_adjust_head support mandatory
    
    Now that also the last in-tree user of the xdp_adjust_head bit has
    been removed, we can remove the flag from struct bpf_prog altogether.
    
    This, at the same time, also makes sure that any future driver for
    XDP comes with bpf_xdp_adjust_head() support right away.
    
    A rejection based on this flag would also mean that tail calls
    couldn't be used with such driver as per c2002f983767 ("bpf: fix
    checking xdp_adjust_head on tail calls") fix, thus lets not allow
    for it in the first place.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 511fe910bf1d..9a7786db14fa 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -413,8 +413,7 @@ struct bpf_prog {
 				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
-				dst_needed:1,	/* Do we need dst entry? */
-				xdp_adjust_head:1; /* Adjusting pkt head? */
+				dst_needed:1;	/* Do we need dst entry? */
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */

commit 4c355cdfbba537971b5c3849680b1b6453a7a383
Author: Reshetova, Elena <elena.reshetova@intel.com>
Date:   Tue Mar 21 13:59:19 2017 +0200

    net: convert sk_filter.refcnt from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index dffa072b7b79..511fe910bf1d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -7,6 +7,7 @@
 #include <stdarg.h>
 
 #include <linux/atomic.h>
+#include <linux/refcount.h>
 #include <linux/compat.h>
 #include <linux/skbuff.h>
 #include <linux/linkage.h>
@@ -430,7 +431,7 @@ struct bpf_prog {
 };
 
 struct sk_filter {
-	atomic_t	refcnt;
+	refcount_t	refcnt;
 	struct rcu_head	rcu;
 	struct bpf_prog	*prog;
 };

commit 81ed18ab3098b6519274545e80a29caacb77d160
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Mar 15 18:26:42 2017 -0700

    bpf: add helper inlining infra and optimize map_array lookup
    
    Optimize bpf_call -> bpf_map_lookup_elem() -> array_map_lookup_elem()
    into a sequence of bpf instructions.
    When JIT is on the sequence of bpf instructions is the sequence
    of native cpu instructions with significantly faster performance
    than indirect call and two function's prologue/epilogue.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index fbf7b39e8103..dffa072b7b79 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -693,6 +693,11 @@ static inline bool bpf_jit_is_ebpf(void)
 # endif
 }
 
+static inline bool ebpf_jit_enabled(void)
+{
+	return bpf_jit_enable && bpf_jit_is_ebpf();
+}
+
 static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
 {
 	return fp->jited && bpf_jit_is_ebpf();
@@ -753,6 +758,11 @@ void bpf_prog_kallsyms_del(struct bpf_prog *fp);
 
 #else /* CONFIG_BPF_JIT */
 
+static inline bool ebpf_jit_enabled(void)
+{
+	return false;
+}
+
 static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
 {
 	return false;

commit 65869a47f3488253f5fd88cc4f14e0a4e2601a55
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Mar 11 16:55:49 2017 +0100

    bpf: improve read-only handling
    
    Improve bpf_{prog,jit_binary}_{un,}lock_ro() by throwing a
    one-time warning in case of an error when the image couldn't
    be set read-only, and also mark struct bpf_prog as locked when
    bpf_prog_lock_ro() was called.
    
    Reason for the latter is that bpf_prog_unlock_ro() is called from
    various places including error paths, and we shouldn't mess with
    page attributes when really not needed.
    
    For bpf_jit_binary_unlock_ro() this is not needed as jited flag
    implicitly indicates this, thus for archs with ARCH_HAS_SET_MEMORY
    we're guaranteed to have a previously locked image. Overall, this
    should also help us to identify any further potential issues with
    set_memory_*() helpers.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0c167fdee5f7..fbf7b39e8103 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -409,6 +409,7 @@ struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	kmemcheck_bitfield_begin(meta);
 	u16			jited:1,	/* Is our filter JIT'ed? */
+				locked:1,	/* Program image locked? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1,	/* Do we need dst entry? */
@@ -554,22 +555,29 @@ static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
 #ifdef CONFIG_ARCH_HAS_SET_MEMORY
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
-	set_memory_ro((unsigned long)fp, fp->pages);
+	fp->locked = 1;
+	WARN_ON_ONCE(set_memory_ro((unsigned long)fp, fp->pages));
 }
 
 static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
-	set_memory_rw((unsigned long)fp, fp->pages);
+	if (fp->locked) {
+		WARN_ON_ONCE(set_memory_rw((unsigned long)fp, fp->pages));
+		/* In case set_memory_rw() fails, we want to be the first
+		 * to crash here instead of some random place later on.
+		 */
+		fp->locked = 0;
+	}
 }
 
 static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
-	set_memory_ro((unsigned long)hdr, hdr->pages);
+	WARN_ON_ONCE(set_memory_ro((unsigned long)hdr, hdr->pages));
 }
 
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 {
-	set_memory_rw((unsigned long)hdr, hdr->pages);
+	WARN_ON_ONCE(set_memory_rw((unsigned long)hdr, hdr->pages));
 }
 #else
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)

commit 9d876e79df6a2f364b9f2737eacd72ceb27da53a
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Feb 21 16:09:34 2017 +0100

    bpf: fix unlocking of jited image when module ronx not set
    
    Eric and Willem reported that they recently saw random crashes when
    JIT was in use and bisected this to 74451e66d516 ("bpf: make jited
    programs visible in traces"). Issue was that the consolidation part
    added bpf_jit_binary_unlock_ro() that would unlock previously made
    read-only memory back to read-write. However, DEBUG_SET_MODULE_RONX
    cannot be used for this to test for presence of set_memory_*()
    functions. We need to use ARCH_HAS_SET_MEMORY instead to fix this;
    also add the corresponding bpf_jit_binary_lock_ro() to filter.h.
    
    Fixes: 74451e66d516 ("bpf: make jited programs visible in traces")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Willem de Bruijn <willemb@google.com>
    Bisected-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Tested-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0c1cc9143cb2..0c167fdee5f7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -551,7 +551,7 @@ static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
-#ifdef CONFIG_DEBUG_SET_MODULE_RONX
+#ifdef CONFIG_ARCH_HAS_SET_MEMORY
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
 	set_memory_ro((unsigned long)fp, fp->pages);
@@ -562,6 +562,11 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 	set_memory_rw((unsigned long)fp, fp->pages);
 }
 
+static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
+{
+	set_memory_ro((unsigned long)hdr, hdr->pages);
+}
+
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 {
 	set_memory_rw((unsigned long)hdr, hdr->pages);
@@ -575,10 +580,14 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
 }
 
+static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
+{
+}
+
 static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
 {
 }
-#endif /* CONFIG_DEBUG_SET_MODULE_RONX */
+#endif /* CONFIG_ARCH_HAS_SET_MEMORY */
 
 static inline struct bpf_binary_header *
 bpf_jit_binary_hdr(const struct bpf_prog *fp)

commit 74451e66d516c55e309e8d89a4a1e7596e46aacd
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Feb 16 22:24:50 2017 +0100

    bpf: make jited programs visible in traces
    
    Long standing issue with JITed programs is that stack traces from
    function tracing check whether a given address is kernel code
    through {__,}kernel_text_address(), which checks for code in core
    kernel, modules and dynamically allocated ftrace trampolines. But
    what is still missing is BPF JITed programs (interpreted programs
    are not an issue as __bpf_prog_run() will be attributed to them),
    thus when a stack trace is triggered, the code walking the stack
    won't see any of the JITed ones. The same for address correlation
    done from user space via reading /proc/kallsyms. This is read by
    tools like perf, but the latter is also useful for permanent live
    tracing with eBPF itself in combination with stack maps when other
    eBPF types are part of the callchain. See offwaketime example on
    dumping stack from a map.
    
    This work tries to tackle that issue by making the addresses and
    symbols known to the kernel. The lookup from *kernel_text_address()
    is implemented through a latched RB tree that can be read under
    RCU in fast-path that is also shared for symbol/size/offset lookup
    for a specific given address in kallsyms. The slow-path iteration
    through all symbols in the seq file done via RCU list, which holds
    a tiny fraction of all exported ksyms, usually below 0.1 percent.
    Function symbols are exported as bpf_prog_<tag>, in order to aide
    debugging and attribution. This facility is currently enabled for
    root-only when bpf_jit_kallsyms is set to 1, and disabled if hardening
    is active in any mode. The rationale behind this is that still a lot
    of systems ship with world read permissions on kallsyms thus addresses
    should not get suddenly exposed for them. If that situation gets
    much better in future, we always have the option to change the
    default on this. Likewise, unprivileged programs are not allowed
    to add entries there either, but that is less of a concern as most
    such programs types relevant in this context are for root-only anyway.
    If enabled, call graphs and stack traces will then show a correct
    attribution; one example is illustrated below, where the trace is
    now visible in tooling such as perf script --kallsyms=/proc/kallsyms
    and friends.
    
    Before:
    
      7fff8166889d bpf_clone_redirect+0x80007f0020ed (/lib/modules/4.9.0-rc8+/build/vmlinux)
             f5d80 __sendmsg_nocancel+0xffff006451f1a007 (/usr/lib64/libc-2.18.so)
    
    After:
    
      7fff816688b7 bpf_clone_redirect+0x80007f002107 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fffa0575728 bpf_prog_33c45a467c9e061a+0x8000600020fb (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fffa07ef1fc cls_bpf_classify+0x8000600020dc (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff81678b68 tc_classify+0x80007f002078 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164d40b __netif_receive_skb_core+0x80007f0025fb (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164d718 __netif_receive_skb+0x80007f002018 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164e565 process_backlog+0x80007f002095 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164dc71 net_rx_action+0x80007f002231 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff81767461 __softirqentry_text_start+0x80007f0020d1 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff817658ac do_softirq_own_stack+0x80007f00201c (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff810a2c20 do_softirq+0x80007f002050 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff810a2cb5 __local_bh_enable_ip+0x80007f002085 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168d452 ip_finish_output2+0x80007f002152 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168ea3d ip_finish_output+0x80007f00217d (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168f2af ip_output+0x80007f00203f (/lib/modules/4.9.0-rc8+/build/vmlinux)
      [...]
      7fff81005854 do_syscall_64+0x80007f002054 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff817649eb return_from_SYSCALL_64+0x80007f002000 (/lib/modules/4.9.0-rc8+/build/vmlinux)
             f5d80 __sendmsg_nocancel+0xffff01c484812007 (/usr/lib64/libc-2.18.so)
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c7a70e0cc3a0..0c1cc9143cb2 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -54,6 +54,12 @@ struct bpf_prog_aux;
 #define BPF_REG_AX		MAX_BPF_REG
 #define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
 
+/* As per nm, we expose JITed images as text (code) section for
+ * kallsyms. That way, tools like perf can find it to match
+ * addresses.
+ */
+#define BPF_SYM_ELF_TYPE	't'
+
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
@@ -555,6 +561,11 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
 	set_memory_rw((unsigned long)fp, fp->pages);
 }
+
+static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
+{
+	set_memory_rw((unsigned long)hdr, hdr->pages);
+}
 #else
 static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 {
@@ -563,8 +574,21 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 {
 }
+
+static inline void bpf_jit_binary_unlock_ro(struct bpf_binary_header *hdr)
+{
+}
 #endif /* CONFIG_DEBUG_SET_MODULE_RONX */
 
+static inline struct bpf_binary_header *
+bpf_jit_binary_hdr(const struct bpf_prog *fp)
+{
+	unsigned long real_start = (unsigned long)fp->bpf_func;
+	unsigned long addr = real_start & PAGE_MASK;
+
+	return (void *)addr;
+}
+
 int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
 static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 {
@@ -617,6 +641,7 @@ void bpf_warn_invalid_xdp_action(u32 act);
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
 extern int bpf_jit_harden;
+extern int bpf_jit_kallsyms;
 
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 
@@ -651,6 +676,11 @@ static inline bool bpf_jit_is_ebpf(void)
 # endif
 }
 
+static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
+{
+	return fp->jited && bpf_jit_is_ebpf();
+}
+
 static inline bool bpf_jit_blinding_enabled(void)
 {
 	/* These are the prerequisites, should someone ever have the
@@ -668,11 +698,91 @@ static inline bool bpf_jit_blinding_enabled(void)
 
 	return true;
 }
-#else
+
+static inline bool bpf_jit_kallsyms_enabled(void)
+{
+	/* There are a couple of corner cases where kallsyms should
+	 * not be enabled f.e. on hardening.
+	 */
+	if (bpf_jit_harden)
+		return false;
+	if (!bpf_jit_kallsyms)
+		return false;
+	if (bpf_jit_kallsyms == 1)
+		return true;
+
+	return false;
+}
+
+const char *__bpf_address_lookup(unsigned long addr, unsigned long *size,
+				 unsigned long *off, char *sym);
+bool is_bpf_text_address(unsigned long addr);
+int bpf_get_kallsym(unsigned int symnum, unsigned long *value, char *type,
+		    char *sym);
+
+static inline const char *
+bpf_address_lookup(unsigned long addr, unsigned long *size,
+		   unsigned long *off, char **modname, char *sym)
+{
+	const char *ret = __bpf_address_lookup(addr, size, off, sym);
+
+	if (ret && modname)
+		*modname = NULL;
+	return ret;
+}
+
+void bpf_prog_kallsyms_add(struct bpf_prog *fp);
+void bpf_prog_kallsyms_del(struct bpf_prog *fp);
+
+#else /* CONFIG_BPF_JIT */
+
+static inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)
+{
+	return false;
+}
+
 static inline void bpf_jit_free(struct bpf_prog *fp)
 {
 	bpf_prog_unlock_free(fp);
 }
+
+static inline bool bpf_jit_kallsyms_enabled(void)
+{
+	return false;
+}
+
+static inline const char *
+__bpf_address_lookup(unsigned long addr, unsigned long *size,
+		     unsigned long *off, char *sym)
+{
+	return NULL;
+}
+
+static inline bool is_bpf_text_address(unsigned long addr)
+{
+	return false;
+}
+
+static inline int bpf_get_kallsym(unsigned int symnum, unsigned long *value,
+				  char *type, char *sym)
+{
+	return -ERANGE;
+}
+
+static inline const char *
+bpf_address_lookup(unsigned long addr, unsigned long *size,
+		   unsigned long *off, char **modname, char *sym)
+{
+	return NULL;
+}
+
+static inline void bpf_prog_kallsyms_add(struct bpf_prog *fp)
+{
+}
+
+static inline void bpf_prog_kallsyms_del(struct bpf_prog *fp)
+{
+}
 #endif /* CONFIG_BPF_JIT */
 
 #define BPF_ANC		BIT(15)

commit 9383191da4e40360a5d880fbe6bb03911c61621b
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Feb 16 22:24:49 2017 +0100

    bpf: remove stubs for cBPF from arch code
    
    Remove the dummy bpf_jit_compile() stubs for eBPF JITs and make
    that a single __weak function in the core that can be overridden
    similarly to the eBPF one. Also remove stale pr_err() mentions
    of bpf_jit_compile.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index e4eb2546339a..c7a70e0cc3a0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -607,6 +607,7 @@ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 
 struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
+void bpf_jit_compile(struct bpf_prog *prog);
 bool bpf_helper_changes_pkt_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
@@ -625,7 +626,6 @@ bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
 		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
 void bpf_jit_binary_free(struct bpf_binary_header *hdr);
 
-void bpf_jit_compile(struct bpf_prog *fp);
 void bpf_jit_free(struct bpf_prog *fp);
 
 struct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *fp);
@@ -669,10 +669,6 @@ static inline bool bpf_jit_blinding_enabled(void)
 	return true;
 }
 #else
-static inline void bpf_jit_compile(struct bpf_prog *fp)
-{
-}
-
 static inline void bpf_jit_free(struct bpf_prog *fp)
 {
 	bpf_prog_unlock_free(fp);

commit f1f7714ea51c56b7163fb1a5acf39c6a204dd758
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jan 13 23:38:15 2017 +0100

    bpf: rework prog_digest into prog_tag
    
    Commit 7bd509e311f4 ("bpf: add prog_digest and expose it via
    fdinfo/netlink") was recently discussed, partially due to
    admittedly suboptimal name of "prog_digest" in combination
    with sha1 hash usage, thus inevitably and rightfully concerns
    about its security in terms of collision resistance were
    raised with regards to use-cases.
    
    The intended use cases are for debugging resp. introspection
    only for providing a stable "tag" over the instruction sequence
    that both kernel and user space can calculate independently.
    It's not usable at all for making a security relevant decision.
    So collisions where two different instruction sequences generate
    the same tag can happen, but ideally at a rather low rate. The
    "tag" will be dumped in hex and is short enough to introspect
    in tracepoints or kallsyms output along with other data such
    as stack trace, etc. Thus, this patch performs a rename into
    prog_tag and truncates the tag to a short output (64 bits) to
    make it obvious it's not collision-free.
    
    Should in future a hash or facility be needed with a security
    relevant focus, then we can think about requirements, constraints,
    etc that would fit to that situation. For now, rework the exposed
    parts for the current use cases as long as nothing has been
    released yet. Tested on x86_64 and s390x.
    
    Fixes: 7bd509e311f4 ("bpf: add prog_digest and expose it via fdinfo/netlink")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a0934e6c9bab..e4eb2546339a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -57,6 +57,8 @@ struct bpf_prog_aux;
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
+#define BPF_TAG_SIZE	8
+
 /* Helper macros for filter block array initializers. */
 
 /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
@@ -408,7 +410,7 @@ struct bpf_prog {
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */
-	u32			digest[SHA_DIGEST_WORDS]; /* Program digest */
+	u8			tag[BPF_TAG_SIZE];
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	unsigned int		(*bpf_func)(const void *ctx,
@@ -519,7 +521,7 @@ static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
 	return prog->len * sizeof(struct bpf_insn);
 }
 
-static inline u32 bpf_prog_digest_scratch_size(const struct bpf_prog *prog)
+static inline u32 bpf_prog_tag_scratch_size(const struct bpf_prog *prog)
 {
 	return round_up(bpf_prog_insn_size(prog) +
 			sizeof(__be64) + 1, SHA_MESSAGE_BYTES);

commit be26727772cd86979255dfaf1eea967338dc0c9b
Author: Jason Wang <jasowang@redhat.com>
Date:   Tue Dec 27 10:49:54 2016 +0800

    net: xdp: remove unused bfp_warn_invalid_xdp_buffer()
    
    After commit 73b62bd085f4737679ea9afc7867fa5f99ba7d1b ("virtio-net:
    remove the warning before XDP linearizing"), there's no users for
    bpf_warn_invalid_xdp_buffer(), so remove it. This is a revert for
    commit f23bc46c30ca5ef58b8549434899fcbac41b2cfc.
    
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: John Fastabend <john.r.fastabend@intel.com>
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 702314253797..a0934e6c9bab 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -610,7 +610,6 @@ bool bpf_helper_changes_pkt_data(void *func);
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 void bpf_warn_invalid_xdp_action(u32 act);
-void bpf_warn_invalid_xdp_buffer(void);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit aafe6ae9cee32df85eb5e8bb6dd1d918e6807b09
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 18 01:52:57 2016 +0100

    bpf: dynamically allocate digest scratch buffer
    
    Geert rightfully complained that 7bd509e311f4 ("bpf: add prog_digest
    and expose it via fdinfo/netlink") added a too large allocation of
    variable 'raw' from bss section, and should instead be done dynamically:
    
      # ./scripts/bloat-o-meter kernel/bpf/core.o.1 kernel/bpf/core.o.2
      add/remove: 3/0 grow/shrink: 0/0 up/down: 33291/0 (33291)
      function                                     old     new   delta
      raw                                            -   32832  +32832
      [...]
    
    Since this is only relevant during program creation path, which can be
    considered slow-path anyway, lets allocate that dynamically and be not
    implicitly dependent on verifier mutex. Move bpf_prog_calc_digest() at
    the beginning of replace_map_fd_with_map_ptr() and also error handling
    stays straight forward.
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index af8a1804cac6..702314253797 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -57,9 +57,6 @@ struct bpf_prog_aux;
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
-/* Maximum BPF program size in bytes. */
-#define MAX_BPF_SIZE	(BPF_MAXINSNS * sizeof(struct bpf_insn))
-
 /* Helper macros for filter block array initializers. */
 
 /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
@@ -517,6 +514,17 @@ static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 	return BPF_PROG_RUN(prog, xdp);
 }
 
+static inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)
+{
+	return prog->len * sizeof(struct bpf_insn);
+}
+
+static inline u32 bpf_prog_digest_scratch_size(const struct bpf_prog *prog)
+{
+	return round_up(bpf_prog_insn_size(prog) +
+			sizeof(__be64) + 1, SHA_MESSAGE_BYTES);
+}
+
 static inline unsigned int bpf_prog_size(unsigned int proglen)
 {
 	return max(sizeof(struct bpf_prog),

commit f23bc46c30ca5ef58b8549434899fcbac41b2cfc
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Dec 15 12:12:54 2016 -0800

    net: xdp: add invalid buffer warning
    
    This adds a warning for drivers to use when encountering an invalid
    buffer for XDP. For normal cases this should not happen but to catch
    this in virtual/qemu setups that I may not have expected from the
    emulation layer having a standard warning is useful.
    
    Signed-off-by: John Fastabend <john.r.fastabend@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6a1658308612..af8a1804cac6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -602,6 +602,7 @@ bool bpf_helper_changes_pkt_data(void *func);
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
 void bpf_warn_invalid_xdp_action(u32 act);
+void bpf_warn_invalid_xdp_buffer(void);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit 17bedab2723145d17b14084430743549e6943d03
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Dec 7 15:53:11 2016 -0800

    bpf: xdp: Allow head adjustment in XDP prog
    
    This patch allows XDP prog to extend/remove the packet
    data at the head (like adding or removing header).  It is
    done by adding a new XDP helper bpf_xdp_adjust_head().
    
    It also renames bpf_helper_changes_skb_data() to
    bpf_helper_changes_pkt_data() to better reflect
    that XDP prog does not work on skb.
    
    This patch adds one "xdp_adjust_head" bit to bpf_prog for the
    XDP-capable driver to check if the XDP prog requires
    bpf_xdp_adjust_head() support.  The driver can then decide
    to error out during XDP_SETUP_PROG.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.r.fastabend@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f078d2b1cff6..6a1658308612 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -406,7 +406,8 @@ struct bpf_prog {
 	u16			jited:1,	/* Is our filter JIT'ed? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
 				cb_access:1,	/* Is control block accessed? */
-				dst_needed:1;	/* Do we need dst entry? */
+				dst_needed:1,	/* Do we need dst entry? */
+				xdp_adjust_head:1; /* Adjusting pkt head? */
 	kmemcheck_bitfield_end(meta);
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	u32			len;		/* Number of filter blocks */
@@ -440,6 +441,7 @@ struct bpf_skb_data_end {
 struct xdp_buff {
 	void *data;
 	void *data_end;
+	void *data_hard_start;
 };
 
 /* compute the linear packet data range [data, data_end) which
@@ -595,7 +597,7 @@ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 
 struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
-bool bpf_helper_changes_skb_data(void *func);
+bool bpf_helper_changes_pkt_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);

commit 7bd509e311f408f7a5132fcdde2069af65fa05ae
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 4 23:19:41 2016 +0100

    bpf: add prog_digest and expose it via fdinfo/netlink
    
    When loading a BPF program via bpf(2), calculate the digest over
    the program's instruction stream and store it in struct bpf_prog's
    digest member. This is done at a point in time before any instructions
    are rewritten by the verifier. Any unstable map file descriptor
    number part of the imm field will be zeroed for the hash.
    
    fdinfo example output for progs:
    
      # cat /proc/1590/fdinfo/5
      pos:          0
      flags:        02000002
      mnt_id:       11
      prog_type:    1
      prog_jited:   1
      prog_digest:  b27e8b06da22707513aa97363dfb11c7c3675d28
      memlock:      4096
    
    When programs are pinned and retrieved by an ELF loader, the loader
    can check the program's digest through fdinfo and compare it against
    one that was generated over the ELF file's program section to see
    if the program needs to be reloaded. Furthermore, this can also be
    exposed through other means such as netlink in case of a tc cls/act
    dump (or xdp in future), but also through tracepoints or other
    facilities to identify the program. Other than that, the digest can
    also serve as a base name for the work in progress kallsyms support
    of programs. The digest doesn't depend/select the crypto layer, since
    we need to keep dependencies to a minimum. iproute2 will get support
    for this facility.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 97338134398f..f078d2b1cff6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -14,6 +14,7 @@
 #include <linux/workqueue.h>
 #include <linux/sched.h>
 #include <linux/capability.h>
+#include <linux/cryptohash.h>
 
 #include <net/sch_generic.h>
 
@@ -56,6 +57,9 @@ struct bpf_prog_aux;
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
+/* Maximum BPF program size in bytes. */
+#define MAX_BPF_SIZE	(BPF_MAXINSNS * sizeof(struct bpf_insn))
+
 /* Helper macros for filter block array initializers. */
 
 /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
@@ -404,8 +408,9 @@ struct bpf_prog {
 				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1;	/* Do we need dst entry? */
 	kmemcheck_bitfield_end(meta);
-	u32			len;		/* Number of filter blocks */
 	enum bpf_prog_type	type;		/* Type of BPF program */
+	u32			len;		/* Number of filter blocks */
+	u32			digest[SHA_DIGEST_WORDS]; /* Program digest */
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	unsigned int		(*bpf_func)(const void *ctx,

commit 366cbf2f46048d70005c6c33dc289330f24b54b0
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Nov 30 22:16:06 2016 +0100

    bpf, xdp: drop rcu_read_lock from bpf_prog_run_xdp and move to caller
    
    After 326fe02d1ed6 ("net/mlx4_en: protect ring->xdp_prog with rcu_read_lock"),
    the rcu_read_lock() in bpf_prog_run_xdp() is superfluous, since callers
    need to hold rcu_read_lock() already to make sure BPF program doesn't
    get released in the background.
    
    Thus, drop it from bpf_prog_run_xdp(), as it can otherwise be misleading.
    Still keeping the bpf_prog_run_xdp() is useful as it allows for grepping
    in XDP supported drivers and to keep the typecheck on the context intact.
    For mlx4, this means we don't have a double rcu_read_lock() anymore. nfp can
    just make use of bpf_prog_run_xdp(), too. For qede, just move rcu_read_lock()
    out of the helper. When the driver gets atomic replace support, this will
    move to call-sites eventually.
    
    mlx5 needs actual fixing as it has the same issue as described already in
    326fe02d1ed6 ("net/mlx4_en: protect ring->xdp_prog with rcu_read_lock"),
    that is, we're under RCU bh at this time, BPF programs are released via
    call_rcu(), and call_rcu() != call_rcu_bh(), so we need to properly mark
    read side as programs can get xchg()'ed in mlx5e_xdp_set() without queue
    reset.
    
    Fixes: 86994156c736 ("net/mlx5e: XDP fast RX drop bpf programs support")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7ba644626553..97338134398f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -498,16 +498,16 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return BPF_PROG_RUN(prog, skb);
 }
 
-static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
-				   struct xdp_buff *xdp)
+static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
+					    struct xdp_buff *xdp)
 {
-	u32 ret;
-
-	rcu_read_lock();
-	ret = BPF_PROG_RUN(prog, xdp);
-	rcu_read_unlock();
-
-	return ret;
+	/* Caller needs to hold rcu_read_lock() (!), otherwise program
+	 * can be released while still running, or map elements could be
+	 * freed early while still having concurrent users. XDP fastpath
+	 * already takes rcu_read_lock() when fetching the program, so
+	 * it's not necessary here anymore.
+	 */
+	return BPF_PROG_RUN(prog, xdp);
 }
 
 static inline unsigned int bpf_prog_size(unsigned int proglen)

commit 3a0af8fd61f90920f6fa04e4f1e9a6a73c1b4fd2
Author: Thomas Graf <tgraf@suug.ch>
Date:   Wed Nov 30 17:10:10 2016 +0100

    bpf: BPF for lightweight tunnel infrastructure
    
    Registers new BPF program types which correspond to the LWT hooks:
      - BPF_PROG_TYPE_LWT_IN   => dst_input()
      - BPF_PROG_TYPE_LWT_OUT  => dst_output()
      - BPF_PROG_TYPE_LWT_XMIT => lwtunnel_xmit()
    
    The separate program types are required to differentiate between the
    capabilities each LWT hook allows:
    
     * Programs attached to dst_input() or dst_output() are restricted and
       may only read the data of an skb. This prevent modification and
       possible invalidation of already validated packet headers on receive
       and the construction of illegal headers while the IP headers are
       still being assembled.
    
     * Programs attached to lwtunnel_xmit() are allowed to modify packet
       content as well as prepending an L2 header via a newly introduced
       helper bpf_skb_change_head(). This is safe as lwtunnel_xmit() is
       invoked after the IP header has been assembled completely.
    
    All BPF programs receive an skb with L3 headers attached and may return
    one of the following error codes:
    
     BPF_OK - Continue routing as per nexthop
     BPF_DROP - Drop skb and return EPERM
     BPF_REDIRECT - Redirect skb to device as per redirect() helper.
                    (Only valid in lwtunnel_xmit() context)
    
    The return codes are binary compatible with their TC_ACT_
    relatives to ease compatibility.
    
    Signed-off-by: Thomas Graf <tgraf@suug.ch>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7f246a281435..7ba644626553 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -438,7 +438,7 @@ struct xdp_buff {
 };
 
 /* compute the linear packet data range [data, data_end) which
- * will be accessed by cls_bpf and act_bpf programs
+ * will be accessed by cls_bpf, act_bpf and lwt programs
  */
 static inline void bpf_compute_data_end(struct sk_buff *skb)
 {

commit 88575199cc65de99a156888629a68180c830eff2
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Nov 26 01:28:04 2016 +0100

    bpf: drop unnecessary context cast from BPF_PROG_RUN
    
    Since long already bpf_func is not only about struct sk_buff * as
    input anymore. Make it generic as void *, so that callers don't
    need to cast for it each time they call BPF_PROG_RUN().
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1f09c521adfe..7f246a281435 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -408,8 +408,8 @@ struct bpf_prog {
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
-	unsigned int		(*bpf_func)(const struct sk_buff *skb,
-					    const struct bpf_insn *filter);
+	unsigned int		(*bpf_func)(const void *ctx,
+					    const struct bpf_insn *insn);
 	/* Instructions for interpreter */
 	union {
 		struct sock_filter	insns[0];
@@ -504,7 +504,7 @@ static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
 	u32 ret;
 
 	rcu_read_lock();
-	ret = BPF_PROG_RUN(prog, (void *)xdp);
+	ret = BPF_PROG_RUN(prog, xdp);
 	rcu_read_unlock();
 
 	return ret;

commit f3694e00123802d688180e7ae90b240669910e3c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Sep 9 02:45:31 2016 +0200

    bpf: add BPF_CALL_x macros for declaring helpers
    
    This work adds BPF_CALL_<n>() macros and converts all the eBPF helper functions
    to use them, in a similar fashion like we do with SYSCALL_DEFINE<n>() macros
    that are used today. Motivation for this is to hide all the register handling
    and all necessary casts from the user, so that it is done automatically in the
    background when adding a BPF_CALL_<n>() call.
    
    This makes current helpers easier to review, eases to write future helpers,
    avoids getting the casting mess wrong, and allows for extending all helpers at
    once (f.e. build time checks, etc). It also helps detecting more easily in
    code reviews that unused registers are not instrumented in the code by accident,
    breaking compatibility with existing programs.
    
    BPF_CALL_<n>() internals are quite similar to SYSCALL_DEFINE<n>() ones with some
    fundamental differences, for example, for generating the actual helper function
    that carries all u64 regs, we need to fill unused regs, so that we always end up
    with 5 u64 regs as an argument.
    
    I reviewed several 0-5 generated BPF_CALL_<n>() variants of the .i results and
    they look all as expected. No sparse issue spotted. We let this also sit for a
    few days with Fengguang's kbuild test robot, and there were no issues seen. On
    s390, it barked on the "uses dynamic stack allocation" notice, which is an old
    one from bpf_perf_event_output{,_tp}() reappearing here due to the conversion
    to the call wrapper, just telling that the perf raw record/frag sits on stack
    (gcc with s390's -mwarn-dynamicstack), but that's all. Did various runtime tests
    and they were fine as well. All eBPF helpers are now converted to use these
    macros, getting rid of a good chunk of all the raw castings.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7fabad8dc3fc..1f09c521adfe 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -328,6 +328,56 @@ struct bpf_prog_aux;
 		__size;						\
 	})
 
+#define __BPF_MAP_0(m, v, ...) v
+#define __BPF_MAP_1(m, v, t, a, ...) m(t, a)
+#define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)
+#define __BPF_MAP_3(m, v, t, a, ...) m(t, a), __BPF_MAP_2(m, v, __VA_ARGS__)
+#define __BPF_MAP_4(m, v, t, a, ...) m(t, a), __BPF_MAP_3(m, v, __VA_ARGS__)
+#define __BPF_MAP_5(m, v, t, a, ...) m(t, a), __BPF_MAP_4(m, v, __VA_ARGS__)
+
+#define __BPF_REG_0(...) __BPF_PAD(5)
+#define __BPF_REG_1(...) __BPF_MAP(1, __VA_ARGS__), __BPF_PAD(4)
+#define __BPF_REG_2(...) __BPF_MAP(2, __VA_ARGS__), __BPF_PAD(3)
+#define __BPF_REG_3(...) __BPF_MAP(3, __VA_ARGS__), __BPF_PAD(2)
+#define __BPF_REG_4(...) __BPF_MAP(4, __VA_ARGS__), __BPF_PAD(1)
+#define __BPF_REG_5(...) __BPF_MAP(5, __VA_ARGS__)
+
+#define __BPF_MAP(n, ...) __BPF_MAP_##n(__VA_ARGS__)
+#define __BPF_REG(n, ...) __BPF_REG_##n(__VA_ARGS__)
+
+#define __BPF_CAST(t, a)						       \
+	(__force t)							       \
+	(__force							       \
+	 typeof(__builtin_choose_expr(sizeof(t) == sizeof(unsigned long),      \
+				      (unsigned long)0, (t)0))) a
+#define __BPF_V void
+#define __BPF_N
+
+#define __BPF_DECL_ARGS(t, a) t   a
+#define __BPF_DECL_REGS(t, a) u64 a
+
+#define __BPF_PAD(n)							       \
+	__BPF_MAP(n, __BPF_DECL_ARGS, __BPF_N, u64, __ur_1, u64, __ur_2,       \
+		  u64, __ur_3, u64, __ur_4, u64, __ur_5)
+
+#define BPF_CALL_x(x, name, ...)					       \
+	static __always_inline						       \
+	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \
+	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));	       \
+	u64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))	       \
+	{								       \
+		return ____##name(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\
+	}								       \
+	static __always_inline						       \
+	u64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))
+
+#define BPF_CALL_0(name, ...)	BPF_CALL_x(0, name, __VA_ARGS__)
+#define BPF_CALL_1(name, ...)	BPF_CALL_x(1, name, __VA_ARGS__)
+#define BPF_CALL_2(name, ...)	BPF_CALL_x(2, name, __VA_ARGS__)
+#define BPF_CALL_3(name, ...)	BPF_CALL_x(3, name, __VA_ARGS__)
+#define BPF_CALL_4(name, ...)	BPF_CALL_x(4, name, __VA_ARGS__)
+#define BPF_CALL_5(name, ...)	BPF_CALL_x(5, name, __VA_ARGS__)
+
 #ifdef CONFIG_COMPAT
 /* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {

commit f035a51536af9802f55d8c79bd87f184ebffb093
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Sep 9 02:45:29 2016 +0200

    bpf: add BPF_SIZEOF and BPF_FIELD_SIZEOF macros
    
    Add BPF_SIZEOF() and BPF_FIELD_SIZEOF() macros to improve the code a bit
    which otherwise often result in overly long bytes_to_bpf_size(sizeof())
    and bytes_to_bpf_size(FIELD_SIZEOF()) lines. So place them into a macro
    helper instead. Moreover, we currently have a BUILD_BUG_ON(BPF_FIELD_SIZEOF())
    check in convert_bpf_extensions(), but we should rather make that generic
    as well and add a BUILD_BUG_ON() test in all BPF_SIZEOF()/BPF_FIELD_SIZEOF()
    users to detect any rewriter size issues at compile time. Note, there are
    currently none, but we want to assert that it stays this way.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a16439b99fd9..7fabad8dc3fc 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -314,6 +314,20 @@ struct bpf_prog_aux;
 	bpf_size;						\
 })
 
+#define BPF_SIZEOF(type)					\
+	({							\
+		const int __size = bytes_to_bpf_size(sizeof(type)); \
+		BUILD_BUG_ON(__size < 0);			\
+		__size;						\
+	})
+
+#define BPF_FIELD_SIZEOF(type, field)				\
+	({							\
+		const int __size = bytes_to_bpf_size(FIELD_SIZEOF(type, field)); \
+		BUILD_BUG_ON(__size < 0);			\
+		__size;						\
+	})
+
 #ifdef CONFIG_COMPAT
 /* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {

commit de0ba9a0d8909996f9e293d311c2cc459fa77d67
Merge: d95a93a9b716 107df03203bb
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jul 23 19:31:37 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Just several instances of overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6a773a15a1e8874e5eccd2f29190c31085912c95
Author: Brenden Blanco <bblanco@plumgrid.com>
Date:   Tue Jul 19 12:16:47 2016 -0700

    bpf: add XDP prog type for early driver filter
    
    Add a new bpf prog type that is intended to run in early stages of the
    packet rx path. Only minimal packet metadata will be available, hence a
    new context type, struct xdp_md, is exposed to userspace. So far only
    expose the packet start and end pointers, and only in read mode.
    
    An XDP program must return one of the well known enum values, all other
    return codes are reserved for future use. Unfortunately, this
    restriction is hard to enforce at verification time, so take the
    approach of warning at runtime when such programs are encountered. Out
    of bounds return codes should alias to XDP_ABORTED.
    
    Signed-off-by: Brenden Blanco <bblanco@plumgrid.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6fc31ef1da2d..15d816a8b755 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -368,6 +368,11 @@ struct bpf_skb_data_end {
 	void *data_end;
 };
 
+struct xdp_buff {
+	void *data;
+	void *data_end;
+};
+
 /* compute the linear packet data range [data, data_end) which
  * will be accessed by cls_bpf and act_bpf programs
  */
@@ -429,6 +434,18 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 	return BPF_PROG_RUN(prog, skb);
 }
 
+static inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
+				   struct xdp_buff *xdp)
+{
+	u32 ret;
+
+	rcu_read_lock();
+	ret = BPF_PROG_RUN(prog, (void *)xdp);
+	rcu_read_unlock();
+
+	return ret;
+}
+
 static inline unsigned int bpf_prog_size(unsigned int proglen)
 {
 	return max(sizeof(struct bpf_prog),
@@ -509,6 +526,7 @@ bool bpf_helper_changes_skb_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 				       const struct bpf_insn *patch, u32 len);
+void bpf_warn_invalid_xdp_action(u32 act);
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;

commit f4979fcea7fd36d8e2f556abef86f80e0d5af1ba
Author: Willem de Bruijn <willemb@google.com>
Date:   Tue Jul 12 18:18:56 2016 -0400

    rose: limit sk_filter trim to payload
    
    Sockets can have a filter program attached that drops or trims
    incoming packets based on the filter program return value.
    
    Rose requires data packets to have at least ROSE_MIN_LEN bytes. It
    verifies this on arrival in rose_route_frame and unconditionally pulls
    the bytes in rose_recvmsg. The filter can trim packets to below this
    value in-between, causing pull to fail, leaving the partial header at
    the time of skb_copy_datagram_msg.
    
    Place a lower bound on the size to which sk_filter may trim packets
    by introducing sk_filter_trim_cap and call this for rose packets.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6fc31ef1da2d..8f74f3d61894 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -467,7 +467,11 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 }
 #endif /* CONFIG_DEBUG_SET_MODULE_RONX */
 
-int sk_filter(struct sock *sk, struct sk_buff *skb);
+int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
+static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
+{
+	return sk_filter_trim_cap(sk, skb, 1);
+}
 
 struct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err);
 void bpf_prog_free(struct bpf_prog *fp);

commit 4f3446bb809f20ad56cadf712e6006815ae7a8f9
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 13 19:08:32 2016 +0200

    bpf: add generic constant blinding for use in jits
    
    This work adds a generic facility for use from eBPF JIT compilers
    that allows for further hardening of JIT generated images through
    blinding constants. In response to the original work on BPF JIT
    spraying published by Keegan McAllister [1], most BPF JITs were
    changed to make images read-only and start at a randomized offset
    in the page, where the rest was filled with trap instructions. We
    have this nowadays in x86, arm, arm64 and s390 JIT compilers.
    Additionally, later work also made eBPF interpreter images read
    only for kernels supporting DEBUG_SET_MODULE_RONX, that is, x86,
    arm, arm64 and s390 archs as well currently. This is done by
    default for mentioned JITs when JITing is enabled. Furthermore,
    we had a generic and configurable constant blinding facility on our
    todo for quite some time now to further make spraying harder, and
    first implementation since around netconf 2016.
    
    We found that for systems where untrusted users can load cBPF/eBPF
    code where JIT is enabled, start offset randomization helps a bit
    to make jumps into crafted payload harder, but in case where larger
    programs that cross page boundary are injected, we again have some
    part of the program opcodes at a page start offset. With improved
    guessing and more reliable payload injection, chances can increase
    to jump into such payload. Elena Reshetova recently wrote a test
    case for it [2, 3]. Moreover, eBPF comes with 64 bit constants, which
    can leave some more room for payloads. Note that for all this,
    additional bugs in the kernel are still required to make the jump
    (and of course to guess right, to not jump into a trap) and naturally
    the JIT must be enabled, which is disabled by default.
    
    For helping mitigation, the general idea is to provide an option
    bpf_jit_harden that admins can tweak along with bpf_jit_enable, so
    that for cases where JIT should be enabled for performance reasons,
    the generated image can be further hardened with blinding constants
    for unpriviledged users (bpf_jit_harden == 1), with trading off
    performance for these, but not for privileged ones. We also added
    the option of blinding for all users (bpf_jit_harden == 2), which
    is quite helpful for testing f.e. with test_bpf.ko. There are no
    further e.g. hardening levels of bpf_jit_harden switch intended,
    rationale is to have it dead simple to use as on/off. Since this
    functionality would need to be duplicated over and over for JIT
    compilers to use, which are already complex enough, we provide a
    generic eBPF byte-code level based blinding implementation, which is
    then just transparently JITed. JIT compilers need to make only a few
    changes to integrate this facility and can be migrated one by one.
    
    This option is for eBPF JITs and will be used in x86, arm64, s390
    without too much effort, and soon ppc64 JITs, thus that native eBPF
    can be blinded as well as cBPF to eBPF migrations, so that both can
    be covered with a single implementation. The rule for JITs is that
    bpf_jit_blind_constants() must be called from bpf_int_jit_compile(),
    and in case blinding is disabled, we follow normally with JITing the
    passed program. In case blinding is enabled and we fail during the
    process of blinding itself, we must return with the interpreter.
    Similarly, in case the JITing process after the blinding failed, we
    return normally to the interpreter with the non-blinded code. Meaning,
    interpreter doesn't change in any way and operates on eBPF code as
    usual. For doing this pre-JIT blinding step, we need to make use of
    a helper/auxiliary register, here BPF_REG_AX. This is strictly internal
    to the JIT and not in any way part of the eBPF architecture. Just like
    in the same way as JITs internally make use of some helper registers
    when emitting code, only that here the helper register is one
    abstraction level higher in eBPF bytecode, but nevertheless in JIT
    phase. That helper register is needed since f.e. manually written
    program can issue loads to all registers of eBPF architecture.
    
    The core concept with the additional register is: blind out all 32
    and 64 bit constants by converting BPF_K based instructions into a
    small sequence from K_VAL into ((RND ^ K_VAL) ^ RND). Therefore, this
    is transformed into: BPF_REG_AX := (RND ^ K_VAL), BPF_REG_AX ^= RND,
    and REG <OP> BPF_REG_AX, so actual operation on the target register
    is translated from BPF_K into BPF_X one that is operating on
    BPF_REG_AX's content. During rewriting phase when blinding, RND is
    newly generated via prandom_u32() for each processed instruction.
    64 bit loads are split into two 32 bit loads to make translation and
    patching not too complex. Only basic thing required by JITs is to
    call the helper bpf_jit_blind_constants()/bpf_jit_prog_release_other()
    pair, and to map BPF_REG_AX into an unused register.
    
    Small bpf_jit_disasm extract from [2] when applied to x86 JIT:
    
    echo 0 > /proc/sys/net/core/bpf_jit_harden
    
      ffffffffa034f5e9 + <x>:
      [...]
      39:   mov    $0xa8909090,%eax
      3e:   mov    $0xa8909090,%eax
      43:   mov    $0xa8ff3148,%eax
      48:   mov    $0xa89081b4,%eax
      4d:   mov    $0xa8900bb0,%eax
      52:   mov    $0xa810e0c1,%eax
      57:   mov    $0xa8908eb4,%eax
      5c:   mov    $0xa89020b0,%eax
      [...]
    
    echo 1 > /proc/sys/net/core/bpf_jit_harden
    
      ffffffffa034f1e5 + <x>:
      [...]
      39:   mov    $0xe1192563,%r10d
      3f:   xor    $0x4989b5f3,%r10d
      46:   mov    %r10d,%eax
      49:   mov    $0xb8296d93,%r10d
      4f:   xor    $0x10b9fd03,%r10d
      56:   mov    %r10d,%eax
      59:   mov    $0x8c381146,%r10d
      5f:   xor    $0x24c7200e,%r10d
      66:   mov    %r10d,%eax
      69:   mov    $0xeb2a830e,%r10d
      6f:   xor    $0x43ba02ba,%r10d
      76:   mov    %r10d,%eax
      79:   mov    $0xd9730af,%r10d
      7f:   xor    $0xa5073b1f,%r10d
      86:   mov    %r10d,%eax
      89:   mov    $0x9a45662b,%r10d
      8f:   xor    $0x325586ea,%r10d
      96:   mov    %r10d,%eax
      [...]
    
    As can be seen, original constants that carry payload are hidden
    when enabled, actual operations are transformed from constant-based
    to register-based ones, making jumps into constants ineffective.
    Above extract/example uses single BPF load instruction over and
    over, but of course all instructions with constants are blinded.
    
    Performance wise, JIT with blinding performs a bit slower than just
    JIT and faster than interpreter case. This is expected, since we
    still get all the performance benefits from JITing and in normal
    use-cases not every single instruction needs to be blinded. Summing
    up all 296 test cases averaged over multiple runs from test_bpf.ko
    suite, interpreter was 55% slower than JIT only and JIT with blinding
    was 8% slower than JIT only. Since there are also some extremes in
    the test suite, I expect for ordinary workloads that the performance
    for the JIT with blinding case is even closer to JIT only case,
    f.e. nmap test case from suite has averaged timings in ns 29 (JIT),
    35 (+ blinding), and 151 (interpreter).
    
    BPF test suite, seccomp test suite, eBPF sample code and various
    bigger networking eBPF programs have been tested with this and were
    running fine. For testing purposes, I also adapted interpreter and
    redirected blinded eBPF image to interpreter and also here all tests
    pass.
    
      [1] http://mainisusuallyafunction.blogspot.com/2012/11/attacking-hardened-linux-systems-with.html
      [2] https://github.com/01org/jit-spray-poc-for-ksp/
      [3] http://www.openwall.com/lists/kernel-hardening/2016/05/03/5
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Elena Reshetova <elena.reshetova@intel.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 891852cf7716..6fc31ef1da2d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -13,6 +13,8 @@
 #include <linux/printk.h>
 #include <linux/workqueue.h>
 #include <linux/sched.h>
+#include <linux/capability.h>
+
 #include <net/sch_generic.h>
 
 #include <asm/cacheflush.h>
@@ -42,6 +44,15 @@ struct bpf_prog_aux;
 #define BPF_REG_X	BPF_REG_7
 #define BPF_REG_TMP	BPF_REG_8
 
+/* Kernel hidden auxiliary/helper register for hardening step.
+ * Only used by eBPF JITs. It's nothing more than a temporary
+ * register that JITs use internally, only that here it's part
+ * of eBPF instructions that have been rewritten for blinding
+ * constants. See JIT pre-step in bpf_jit_blind_constants().
+ */
+#define BPF_REG_AX		MAX_BPF_REG
+#define MAX_BPF_JIT_REG		(MAX_BPF_REG + 1)
+
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
@@ -501,6 +512,7 @@ struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
+extern int bpf_jit_harden;
 
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 
@@ -513,6 +525,9 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr);
 void bpf_jit_compile(struct bpf_prog *fp);
 void bpf_jit_free(struct bpf_prog *fp);
 
+struct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *fp);
+void bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other);
+
 static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 				u32 pass, void *image)
 {
@@ -523,6 +538,33 @@ static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
 			       16, 1, image, proglen, false);
 }
+
+static inline bool bpf_jit_is_ebpf(void)
+{
+# ifdef CONFIG_HAVE_EBPF_JIT
+	return true;
+# else
+	return false;
+# endif
+}
+
+static inline bool bpf_jit_blinding_enabled(void)
+{
+	/* These are the prerequisites, should someone ever have the
+	 * idea to call blinding outside of them, we make sure to
+	 * bail out.
+	 */
+	if (!bpf_jit_is_ebpf())
+		return false;
+	if (!bpf_jit_enable)
+		return false;
+	if (!bpf_jit_harden)
+		return false;
+	if (bpf_jit_harden == 1 && capable(CAP_SYS_ADMIN))
+		return false;
+
+	return true;
+}
 #else
 static inline void bpf_jit_compile(struct bpf_prog *fp)
 {

commit d1c55ab5e41fcd72cb0a8bef86d3f652ad9ad9f5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 13 19:08:31 2016 +0200

    bpf: prepare bpf_int_jit_compile/bpf_prog_select_runtime apis
    
    Since the blinding is strictly only called from inside eBPF JITs,
    we need to change signatures for bpf_int_jit_compile() and
    bpf_prog_select_runtime() first in order to prepare that the
    eBPF program we're dealing with can change underneath. Hence,
    for call sites, we need to return the latest prog. No functional
    change in this patch.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c4aae496f376..891852cf7716 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -458,7 +458,7 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
-int bpf_prog_select_runtime(struct bpf_prog *fp);
+struct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err);
 void bpf_prog_free(struct bpf_prog *fp);
 
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
@@ -492,7 +492,8 @@ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
-void bpf_int_jit_compile(struct bpf_prog *fp);
+
+struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);
 bool bpf_helper_changes_skb_data(void *func);
 
 struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,

commit c237ee5eb33bf19fe0591c04ff8db19da7323a83
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 13 19:08:30 2016 +0200

    bpf: add bpf_patch_insn_single helper
    
    Move the functionality to patch instructions out of the verifier
    code and into the core as the new bpf_patch_insn_single() helper
    will be needed later on for blinding as well. No changes in
    functionality.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4ff0e647598f..c4aae496f376 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -495,6 +495,9 @@ u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 void bpf_int_jit_compile(struct bpf_prog *fp);
 bool bpf_helper_changes_skb_data(void *func);
 
+struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
+				       const struct bpf_insn *patch, u32 len);
+
 #ifdef CONFIG_BPF_JIT
 extern int bpf_jit_enable;
 

commit c94987e40ebbae3b7b6c3ece37b6f8338830f6b1
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 13 19:08:27 2016 +0200

    bpf: move bpf_jit_enable declaration
    
    Move the bpf_jit_enable declaration to the filter.h file where
    most other core code is declared, also since we're going to add
    a second knob there.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ec1411c89105..4ff0e647598f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -496,6 +496,8 @@ void bpf_int_jit_compile(struct bpf_prog *fp);
 bool bpf_helper_changes_skb_data(void *func);
 
 #ifdef CONFIG_BPF_JIT
+extern int bpf_jit_enable;
+
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
 
 struct bpf_binary_header *

commit db58ba45920255e967cc1d62a430cebd634b5046
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu May 5 19:49:12 2016 -0700

    bpf: wire in data and data_end for cls_act_bpf
    
    allow cls_bpf and act_bpf programs access skb->data and skb->data_end pointers.
    The bpf helpers that change skb->data need to update data_end pointer as well.
    The verifier checks that programs always reload data, data_end pointers
    after calls to such bpf helpers.
    We cannot add 'data_end' pointer to struct qdisc_skb_cb directly,
    since it's embedded as-is by infiniband ipoib, so wrapper struct is needed.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 43aa1f8855c7..ec1411c89105 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -352,6 +352,22 @@ struct sk_filter {
 
 #define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
 
+struct bpf_skb_data_end {
+	struct qdisc_skb_cb qdisc_cb;
+	void *data_end;
+};
+
+/* compute the linear packet data range [data, data_end) which
+ * will be accessed by cls_bpf and act_bpf programs
+ */
+static inline void bpf_compute_data_end(struct sk_buff *skb)
+{
+	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;
+
+	BUILD_BUG_ON(sizeof(*cb) > FIELD_SIZEOF(struct sk_buff, cb));
+	cb->data_end = skb->data + skb_headlen(skb);
+}
+
 static inline u8 *bpf_skb_cb(struct sk_buff *skb)
 {
 	/* eBPF programs may read/write skb->cb[] area to transfer meta

commit 8ced425ee630c03beea06c1dfa35190bf8395d07
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Tue Apr 5 17:10:16 2016 +0200

    tun: use socket locks for sk_{attach,detatch}_filter
    
    This reverts commit 5a5abb1fa3b05dd ("tun, bpf: fix suspicious RCU usage
    in tun_{attach, detach}_filter") and replaces it to use lock_sock around
    sk_{attach,detach}_filter. The checks inside filter.c are updated with
    lockdep_sock_is_held to check for proper socket locks.
    
    It keeps the code cleaner by ensuring that only one lock governs the
    socket filter instead of two independent locks.
    
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a51a5361695f..43aa1f8855c7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -465,14 +465,10 @@ int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
 void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
-int __sk_attach_filter(struct sock_fprog *fprog, struct sock *sk,
-		       bool locked);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
-int __sk_detach_filter(struct sock *sk, bool locked);
-
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit 5a5abb1fa3b05dd6aa821525832644c1e7d2905f
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Mar 31 02:13:18 2016 +0200

    tun, bpf: fix suspicious RCU usage in tun_{attach, detach}_filter
    
    Sasha Levin reported a suspicious rcu_dereference_protected() warning
    found while fuzzing with trinity that is similar to this one:
    
      [   52.765684] net/core/filter.c:2262 suspicious rcu_dereference_protected() usage!
      [   52.765688] other info that might help us debug this:
      [   52.765695] rcu_scheduler_active = 1, debug_locks = 1
      [   52.765701] 1 lock held by a.out/1525:
      [   52.765704]  #0:  (rtnl_mutex){+.+.+.}, at: [<ffffffff816a64b7>] rtnl_lock+0x17/0x20
      [   52.765721] stack backtrace:
      [   52.765728] CPU: 1 PID: 1525 Comm: a.out Not tainted 4.5.0+ #264
      [...]
      [   52.765768] Call Trace:
      [   52.765775]  [<ffffffff813e488d>] dump_stack+0x85/0xc8
      [   52.765784]  [<ffffffff810f2fa5>] lockdep_rcu_suspicious+0xd5/0x110
      [   52.765792]  [<ffffffff816afdc2>] sk_detach_filter+0x82/0x90
      [   52.765801]  [<ffffffffa0883425>] tun_detach_filter+0x35/0x90 [tun]
      [   52.765810]  [<ffffffffa0884ed4>] __tun_chr_ioctl+0x354/0x1130 [tun]
      [   52.765818]  [<ffffffff8136fed0>] ? selinux_file_ioctl+0x130/0x210
      [   52.765827]  [<ffffffffa0885ce3>] tun_chr_ioctl+0x13/0x20 [tun]
      [   52.765834]  [<ffffffff81260ea6>] do_vfs_ioctl+0x96/0x690
      [   52.765843]  [<ffffffff81364af3>] ? security_file_ioctl+0x43/0x60
      [   52.765850]  [<ffffffff81261519>] SyS_ioctl+0x79/0x90
      [   52.765858]  [<ffffffff81003ba2>] do_syscall_64+0x62/0x140
      [   52.765866]  [<ffffffff817d563f>] entry_SYSCALL64_slow_path+0x25/0x25
    
    Same can be triggered with PROVE_RCU (+ PROVE_RCU_REPEATEDLY) enabled
    from tun_attach_filter() when user space calls ioctl(tun_fd, TUN{ATTACH,
    DETACH}FILTER, ...) for adding/removing a BPF filter on tap devices.
    
    Since the fix in f91ff5b9ff52 ("net: sk_{detach|attach}_filter() rcu
    fixes") sk_attach_filter()/sk_detach_filter() now dereferences the
    filter with rcu_dereference_protected(), checking whether socket lock
    is held in control path.
    
    Since its introduction in 994051625981 ("tun: socket filter support"),
    tap filters are managed under RTNL lock from __tun_chr_ioctl(). Thus the
    sock_owned_by_user(sk) doesn't apply in this specific case and therefore
    triggers the false positive.
    
    Extend the BPF API with __sk_attach_filter()/__sk_detach_filter() pair
    that is used by tap filters and pass in lockdep_rtnl_is_held() for the
    rcu_dereference_protected() checks instead.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 43aa1f8855c7..a51a5361695f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -465,10 +465,14 @@ int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
 void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+int __sk_attach_filter(struct sock_fprog *fprog, struct sock *sk,
+		       bool locked);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
+int __sk_detach_filter(struct sock *sk, bool locked);
+
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit 01dd194c387af5b3c4c1f6459d30f596565e466c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jan 6 22:32:16 2016 +0100

    bpf: cleanup bpf_prog_run_{save,clear}_cb helpers
    
    Move the details behind the cb[] access into a small helper to decouple
    and make them generic for bpf_prog_run_save_cb()/bpf_prog_run_clear_cb()
    that was introduced via commit ff936a04e5f2 ("bpf: fix cb access in socket
    filter programs"). Also add a comment to better clarify what is done in
    bpf_skb_cb().
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f5b5891ed1ba..43aa1f8855c7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -350,25 +350,43 @@ struct sk_filter {
 
 #define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 
+#define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN
+
+static inline u8 *bpf_skb_cb(struct sk_buff *skb)
+{
+	/* eBPF programs may read/write skb->cb[] area to transfer meta
+	 * data between tail calls. Since this also needs to work with
+	 * tc, that scratch memory is mapped to qdisc_skb_cb's data area.
+	 *
+	 * In some socket filter cases, the cb unfortunately needs to be
+	 * saved/restored so that protocol specific skb->cb[] data won't
+	 * be lost. In any case, due to unpriviledged eBPF programs
+	 * attached to sockets, we need to clear the bpf_skb_cb() area
+	 * to not leak previous contents to user space.
+	 */
+	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) != BPF_SKB_CB_LEN);
+	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) !=
+		     FIELD_SIZEOF(struct qdisc_skb_cb, data));
+
+	return qdisc_skb_cb(skb)->data;
+}
+
 static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
 				       struct sk_buff *skb)
 {
-	u8 *cb_data = qdisc_skb_cb(skb)->data;
-	u8 saved_cb[QDISC_CB_PRIV_LEN];
+	u8 *cb_data = bpf_skb_cb(skb);
+	u8 cb_saved[BPF_SKB_CB_LEN];
 	u32 res;
 
-	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) !=
-		     QDISC_CB_PRIV_LEN);
-
 	if (unlikely(prog->cb_access)) {
-		memcpy(saved_cb, cb_data, sizeof(saved_cb));
-		memset(cb_data, 0, sizeof(saved_cb));
+		memcpy(cb_saved, cb_data, sizeof(cb_saved));
+		memset(cb_data, 0, sizeof(cb_saved));
 	}
 
 	res = BPF_PROG_RUN(prog, skb);
 
 	if (unlikely(prog->cb_access))
-		memcpy(cb_data, saved_cb, sizeof(saved_cb));
+		memcpy(cb_data, cb_saved, sizeof(cb_saved));
 
 	return res;
 }
@@ -376,10 +394,11 @@ static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
 static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
 					struct sk_buff *skb)
 {
-	u8 *cb_data = qdisc_skb_cb(skb)->data;
+	u8 *cb_data = bpf_skb_cb(skb);
 
 	if (unlikely(prog->cb_access))
-		memset(cb_data, 0, QDISC_CB_PRIV_LEN);
+		memset(cb_data, 0, BPF_SKB_CB_LEN);
+
 	return BPF_PROG_RUN(prog, skb);
 }
 

commit 9e0efaf6b41bf22c2eb81258fc2a6f1538a643e5
Merge: c7f5d105495a 51cb67c0b0fc
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jan 6 22:54:18 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 55795ef5469290f89f04e12e662ded604909e462
Author: Rabin Vincent <rabin@rab.in>
Date:   Tue Jan 5 16:23:07 2016 +0100

    net: filter: make JITs zero A for SKF_AD_ALU_XOR_X
    
    The SKF_AD_ALU_XOR_X ancillary is not like the other ancillary data
    instructions since it XORs A with X while all the others replace A with
    some loaded value.  All the BPF JITs fail to clear A if this is used as
    the first instruction in a filter.  This was found using american fuzzy
    lop.
    
    Add a helper to determine if A needs to be cleared given the first
    instruction in a filter, and use this in the JITs.  Except for ARM, the
    rest have only been compile-tested.
    
    Fixes: 3480593131e0 ("net: filter: get rid of BPF_S_* enum")
    Signed-off-by: Rabin Vincent <rabin@rab.in>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4165e9ac9e36..5972ffe5719a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -493,6 +493,25 @@ static inline void bpf_jit_free(struct bpf_prog *fp)
 
 #define BPF_ANC		BIT(15)
 
+static inline bool bpf_needs_clear_a(const struct sock_filter *first)
+{
+	switch (first->code) {
+	case BPF_RET | BPF_K:
+	case BPF_LD | BPF_W | BPF_LEN:
+		return false;
+
+	case BPF_LD | BPF_W | BPF_ABS:
+	case BPF_LD | BPF_H | BPF_ABS:
+	case BPF_LD | BPF_B | BPF_ABS:
+		if (first->k == SKF_AD_OFF + SKF_AD_ALU_XOR_X)
+			return true;
+		return false;
+
+	default:
+		return true;
+	}
+}
+
 static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
 {
 	BUG_ON(ftest->code & BPF_ANC);

commit 538950a1b7527a0a52ccd9337e3fcd304f027f13
Author: Craig Gallek <kraig@google.com>
Date:   Mon Jan 4 17:41:47 2016 -0500

    soreuseport: setsockopt SO_ATTACH_REUSEPORT_[CE]BPF
    
    Expose socket options for setting a classic or extended BPF program
    for use when selecting sockets in an SO_REUSEPORT group.  These options
    can be used on the first socket to belong to a group before bind or
    on any socket in the group after bind.
    
    This change includes refactoring of the existing sk_filter code to
    allow reuse of the existing BPF filter validation checks.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4165e9ac9e36..294c3cdf07b3 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -447,6 +447,8 @@ void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
+int sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+int sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);

commit ff936a04e5f28b7e0455be0e7fa91334f89e4b44
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Oct 7 10:55:41 2015 -0700

    bpf: fix cb access in socket filter programs
    
    eBPF socket filter programs may see junk in 'u32 cb[5]' area,
    since it could have been used by protocol layers earlier.
    
    For socket filter programs used in af_packet we need to clean
    20 bytes of skb->cb area if it could be used by the program.
    For programs attached to TCP/UDP sockets we need to save/restore
    these 20 bytes, since it's used by protocol layers.
    
    Remove SK_RUN_FILTER macro, since it's no longer used.
    
    Long term we may move this bpf cb area to per-cpu scratch, but that
    requires addition of new 'per-cpu load/store' instructions,
    so not suitable as a short term fix.
    
    Fixes: d691f9e8d440 ("bpf: allow programs to write to certain skb fields")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1bbce14bcf17..4165e9ac9e36 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -13,6 +13,7 @@
 #include <linux/printk.h>
 #include <linux/workqueue.h>
 #include <linux/sched.h>
+#include <net/sch_generic.h>
 
 #include <asm/cacheflush.h>
 
@@ -302,10 +303,6 @@ struct bpf_prog_aux;
 	bpf_size;						\
 })
 
-/* Macro to invoke filter function. */
-#define SK_RUN_FILTER(filter, ctx) \
-	(*filter->prog->bpf_func)(ctx, filter->prog->insnsi)
-
 #ifdef CONFIG_COMPAT
 /* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {
@@ -329,6 +326,7 @@ struct bpf_prog {
 	kmemcheck_bitfield_begin(meta);
 	u16			jited:1,	/* Is our filter JIT'ed? */
 				gpl_compatible:1, /* Is filter GPL compatible? */
+				cb_access:1,	/* Is control block accessed? */
 				dst_needed:1;	/* Do we need dst entry? */
 	kmemcheck_bitfield_end(meta);
 	u32			len;		/* Number of filter blocks */
@@ -352,6 +350,39 @@ struct sk_filter {
 
 #define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 
+static inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,
+				       struct sk_buff *skb)
+{
+	u8 *cb_data = qdisc_skb_cb(skb)->data;
+	u8 saved_cb[QDISC_CB_PRIV_LEN];
+	u32 res;
+
+	BUILD_BUG_ON(FIELD_SIZEOF(struct __sk_buff, cb) !=
+		     QDISC_CB_PRIV_LEN);
+
+	if (unlikely(prog->cb_access)) {
+		memcpy(saved_cb, cb_data, sizeof(saved_cb));
+		memset(cb_data, 0, sizeof(saved_cb));
+	}
+
+	res = BPF_PROG_RUN(prog, skb);
+
+	if (unlikely(prog->cb_access))
+		memcpy(cb_data, saved_cb, sizeof(saved_cb));
+
+	return res;
+}
+
+static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
+					struct sk_buff *skb)
+{
+	u8 *cb_data = qdisc_skb_cb(skb)->data;
+
+	if (unlikely(prog->cb_access))
+		memset(cb_data, 0, QDISC_CB_PRIV_LEN);
+	return BPF_PROG_RUN(prog, skb);
+}
+
 static inline unsigned int bpf_prog_size(unsigned int proglen)
 {
 	return max(sizeof(struct bpf_prog),

commit bab18991871545dfbd10c931eb0fe8f7637156a9
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Oct 2 15:17:33 2015 +0200

    bpf, seccomp: prepare for upcoming criu support
    
    The current ongoing effort to dump existing cBPF seccomp filters back
    to user space requires to hold the pre-transformed instructions like
    we do in case of socket filters from sk_attach_filter() side, so they
    can be reloaded in original form at a later point in time by utilities
    such as criu.
    
    To prepare for this, simply extend the bpf_prog_create_from_user()
    API to hold a flag that tells whether we should store the original
    or not. Also, fanout filters could make use of that in future for
    things like diag. While fanout filters already use bpf_prog_destroy(),
    move seccomp over to them as well to handle original programs when
    present.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Tycho Andersen <tycho.andersen@canonical.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Tested-by: Tycho Andersen <tycho.andersen@canonical.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3d5fd24b321b..1bbce14bcf17 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -411,7 +411,7 @@ typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
 
 int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
-			      bpf_aux_classic_check_t trans);
+			      bpf_aux_classic_check_t trans, bool save_orig);
 void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);

commit c46646d0484f5d08e2bede9b45034ba5b8b489cc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Sep 30 01:41:51 2015 +0200

    sched, bpf: add helper for retrieving routing realms
    
    Using routing realms as part of the classifier is quite useful, it
    can be viewed as a tag for one or multiple routing entries (think of
    an analogy to net_cls cgroup for processes), set by user space routing
    daemons or via iproute2 as an indicator for traffic classifiers and
    later on processed in the eBPF program.
    
    Unlike actions, the classifier can inspect device flags and enable
    netif_keep_dst() if necessary. tc actions don't have that possibility,
    but in case people know what they are doing, it can be used from there
    as well (e.g. via devs that must keep dsts by design anyway).
    
    If a realm is set, the handler returns the non-zero realm. User space
    can set the full 32bit realm for the dst.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bad618f316d7..3d5fd24b321b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -328,7 +328,8 @@ struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	kmemcheck_bitfield_begin(meta);
 	u16			jited:1,	/* Is our filter JIT'ed? */
-				gpl_compatible:1; /* Is filter GPL compatible? */
+				gpl_compatible:1, /* Is filter GPL compatible? */
+				dst_needed:1;	/* Do we need dst entry? */
 	kmemcheck_bitfield_end(meta);
 	u32			len;		/* Number of filter blocks */
 	enum bpf_prog_type	type;		/* Type of BPF program */

commit a91263d520246b63c63e75ddfb072ee6a853fe15
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Sep 30 01:41:50 2015 +0200

    ebpf: migrate bpf_prog's flags to bitfield
    
    As we need to add further flags to the bpf_prog structure, lets migrate
    both bools to a bitfield representation. The size of the base structure
    (excluding insns) remains unchanged at 40 bytes.
    
    Add also tags for the kmemchecker, so that it doesn't throw false
    positives. Even in case gcc would generate suboptimal code, it's not
    being accessed in performance critical paths.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index fa2cab985e57..bad618f316d7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -326,8 +326,10 @@ struct bpf_binary_header {
 
 struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
-	bool			jited;		/* Is our filter JIT'ed? */
-	bool			gpl_compatible;	/* Is our filter GPL compatible? */
+	kmemcheck_bitfield_begin(meta);
+	u16			jited:1,	/* Is our filter JIT'ed? */
+				gpl_compatible:1; /* Is filter GPL compatible? */
+	kmemcheck_bitfield_end(meta);
 	u32			len;		/* Number of filter blocks */
 	enum bpf_prog_type	type;		/* Type of BPF program */
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */

commit b13138ef72178a13f34e33883f9f093f9e3b1bda
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jul 30 12:42:49 2015 +0200

    bpf: also show process name/pid in bpf_jit_dump
    
    It can be useful for testing to see the actual process/pid who is loading
    a given filter. I was running some BPF test program and noticed unusual
    filter loads from time to time, triggered by some other application in the
    background. bpf_jit_disasm is still working after this change.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 6b025491120d..fa2cab985e57 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -12,6 +12,7 @@
 #include <linux/linkage.h>
 #include <linux/printk.h>
 #include <linux/workqueue.h>
+#include <linux/sched.h>
 
 #include <asm/cacheflush.h>
 
@@ -438,8 +439,9 @@ void bpf_jit_free(struct bpf_prog *fp);
 static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 				u32 pass, void *image)
 {
-	pr_err("flen=%u proglen=%u pass=%u image=%pK\n",
-	       flen, proglen, pass, image);
+	pr_err("flen=%u proglen=%u pass=%u image=%pK from=%s pid=%d\n", flen,
+	       proglen, pass, image, current->comm, task_pid_nr(current));
+
 	if (image)
 		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
 			       16, 1, image, proglen, false);

commit 7b36f92934e40d1ee24e5617ddedb852e10086ca
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jul 30 12:42:47 2015 +0200

    bpf: provide helper that indicates eBPF was migrated
    
    During recent discussions we had with Michael, we found that it would
    be useful to have an indicator that tells the JIT that an eBPF program
    had been migrated from classic instructions into eBPF instructions, as
    only in that case A and X need to be cleared in the prologue. Such eBPF
    programs do not set a particular type, but all have BPF_PROG_TYPE_UNSPEC.
    Thus, introduce a small helper for cde66c2d88da ("s390/bpf: Only clear
    A and X for converted BPF programs") and possibly others in future.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 69d00555ce35..6b025491120d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -354,6 +354,16 @@ static inline unsigned int bpf_prog_size(unsigned int proglen)
 		   offsetof(struct bpf_prog, insns[proglen]));
 }
 
+static inline bool bpf_prog_was_classic(const struct bpf_prog *prog)
+{
+	/* When classic BPF programs have been loaded and the arch
+	 * does not have a classic BPF JIT (anymore), they have been
+	 * converted via bpf_migrate_filter() to eBPF and thus always
+	 * have an unspec program type.
+	 */
+	return prog->type == BPF_PROG_TYPE_UNSPEC;
+}
+
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
 #ifdef CONFIG_DEBUG_SET_MODULE_RONX

commit 4e10df9a60d96ced321dd2af71da558c6b750078
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Mon Jul 20 20:34:18 2015 -0700

    bpf: introduce bpf_skb_vlan_push/pop() helpers
    
    Allow eBPF programs attached to TC qdiscs call skb_vlan_push/pop via
    helper functions. These functions may change skb->data/hlen which are
    cached by some JITs to improve performance of ld_abs/ld_ind instructions.
    Therefore JITs need to recognize bpf_skb_vlan_push/pop() calls,
    re-compute header len and re-cache skb->data/hlen back into cpu registers.
    Note, skb->data/hlen are not directly accessible from the programs,
    so any changes to skb->data done either by these helpers or by other
    TC actions are safe.
    
    eBPF JIT supported by three architectures:
    - arm64 JIT is using bpf_load_pointer() without caching, so it's ok as-is.
    - x64 JIT re-caches skb->data/hlen unconditionally after vlan_push/pop calls
      (experiments showed that conditional re-caching is slower).
    - s390 JIT falls back to interpreter for now when bpf_skb_vlan_push() is present
      in the program (re-caching is tbd).
    
    These helpers allow more scalable handling of vlan from the programs.
    Instead of creating thousands of vlan netdevs on top of eth0 and attaching
    TC+ingress+bpf to all of them, the program can be attached to eth0 directly
    and manipulate vlans as necessary.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 17724f6ea983..69d00555ce35 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -411,6 +411,7 @@ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 void bpf_int_jit_compile(struct bpf_prog *fp);
+bool bpf_helper_changes_skb_data(void *func);
 
 #ifdef CONFIG_BPF_JIT
 typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);

commit 04fd61ab36ec065e194ab5e74ae34a5240d992bb
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Tue May 19 16:59:03 2015 -0700

    bpf: allow bpf programs to tail-call other bpf programs
    
    introduce bpf_tail_call(ctx, &jmp_table, index) helper function
    which can be used from BPF programs like:
    int bpf_prog(struct pt_regs *ctx)
    {
      ...
      bpf_tail_call(ctx, &jmp_table, index);
      ...
    }
    that is roughly equivalent to:
    int bpf_prog(struct pt_regs *ctx)
    {
      ...
      if (jmp_table[index])
        return (*jmp_table[index])(ctx);
      ...
    }
    The important detail that it's not a normal call, but a tail call.
    The kernel stack is precious, so this helper reuses the current
    stack frame and jumps into another BPF program without adding
    extra call frame.
    It's trivially done in interpreter and a bit trickier in JITs.
    In case of x64 JIT the bigger part of generated assembler prologue
    is common for all programs, so it is simply skipped while jumping.
    Other JITs can do similar prologue-skipping optimization or
    do stack unwind before jumping into the next program.
    
    bpf_tail_call() arguments:
    ctx - context pointer
    jmp_table - one of BPF_MAP_TYPE_PROG_ARRAY maps used as the jump table
    index - index in the jump table
    
    Since all BPF programs are idenitified by file descriptor, user space
    need to populate the jmp_table with FDs of other BPF programs.
    If jmp_table[index] is empty the bpf_tail_call() doesn't jump anywhere
    and program execution continues as normal.
    
    New BPF_MAP_TYPE_PROG_ARRAY map type is introduced so that user space can
    populate this jmp_table array with FDs of other bpf programs.
    Programs can share the same jmp_table array or use multiple jmp_tables.
    
    The chain of tail calls can form unpredictable dynamic loops therefore
    tail_call_cnt is used to limit the number of calls and currently is set to 32.
    
    Use cases:
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    
    ==========
    - simplify complex programs by splitting them into a sequence of small programs
    
    - dispatch routine
      For tracing and future seccomp the program may be triggered on all system
      calls, but processing of syscall arguments will be different. It's more
      efficient to implement them as:
      int syscall_entry(struct seccomp_data *ctx)
      {
         bpf_tail_call(ctx, &syscall_jmp_table, ctx->nr /* syscall number */);
         ... default: process unknown syscall ...
      }
      int sys_write_event(struct seccomp_data *ctx) {...}
      int sys_read_event(struct seccomp_data *ctx) {...}
      syscall_jmp_table[__NR_write] = sys_write_event;
      syscall_jmp_table[__NR_read] = sys_read_event;
    
      For networking the program may call into different parsers depending on
      packet format, like:
      int packet_parser(struct __sk_buff *skb)
      {
         ... parse L2, L3 here ...
         __u8 ipproto = load_byte(skb, ... offsetof(struct iphdr, protocol));
         bpf_tail_call(skb, &ipproto_jmp_table, ipproto);
         ... default: process unknown protocol ...
      }
      int parse_tcp(struct __sk_buff *skb) {...}
      int parse_udp(struct __sk_buff *skb) {...}
      ipproto_jmp_table[IPPROTO_TCP] = parse_tcp;
      ipproto_jmp_table[IPPROTO_UDP] = parse_udp;
    
    - for TC use case, bpf_tail_call() allows to implement reclassify-like logic
    
    - bpf_map_update_elem/delete calls into BPF_MAP_TYPE_PROG_ARRAY jump table
      are atomic, so user space can build chains of BPF programs on the fly
    
    Implementation details:
    =======================
    - high performance of bpf_tail_call() is the goal.
      It could have been implemented without JIT changes as a wrapper on top of
      BPF_PROG_RUN() macro, but with two downsides:
      . all programs would have to pay performance penalty for this feature and
        tail call itself would be slower, since mandatory stack unwind, return,
        stack allocate would be done for every tailcall.
      . tailcall would be limited to programs running preempt_disabled, since
        generic 'void *ctx' doesn't have room for 'tail_call_cnt' and it would
        need to be either global per_cpu variable accessed by helper and by wrapper
        or global variable protected by locks.
    
      In this implementation x64 JIT bypasses stack unwind and jumps into the
      callee program after prologue.
    
    - bpf_prog_array_compatible() ensures that prog_type of callee and caller
      are the same and JITed/non-JITed flag is the same, since calling JITed
      program from non-JITed is invalid, since stack frames are different.
      Similarly calling kprobe type program from socket type program is invalid.
    
    - jump table is implemented as BPF_MAP_TYPE_PROG_ARRAY to reuse 'map'
      abstraction, its user space API and all of verifier logic.
      It's in the existing arraymap.c file, since several functions are
      shared with regular array map.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 200be4a74a33..17724f6ea983 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -378,7 +378,7 @@ static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
 
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
-void bpf_prog_select_runtime(struct bpf_prog *fp);
+int bpf_prog_select_runtime(struct bpf_prog *fp);
 void bpf_prog_free(struct bpf_prog *fp);
 
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);

commit a4afd37b26f4b9f640310a89b7f8d176ae3460b1
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed May 13 13:12:43 2015 +0200

    test_bpf: add tests related to BPF_MAXINSNS
    
    Couple of torture test cases related to the bug fixed in 0b59d8806a31
    ("ARM: net: delegate filter to kernel interpreter when imm_offset()
    return value can't fit into 12bits.").
    
    I've added a helper to allocate and fill the insn space. Output on
    x86_64 from my laptop:
    
    test_bpf: #233 BPF_MAXINSNS: Maximum possible literals jited:0 7 PASS
    test_bpf: #234 BPF_MAXINSNS: Single literal jited:0 8 PASS
    test_bpf: #235 BPF_MAXINSNS: Run/add until end jited:0 11553 PASS
    test_bpf: #236 BPF_MAXINSNS: Too many instructions PASS
    test_bpf: #237 BPF_MAXINSNS: Very long jump jited:0 9 PASS
    test_bpf: #238 BPF_MAXINSNS: Ctx heavy transformations jited:0 20329 20398 PASS
    test_bpf: #239 BPF_MAXINSNS: Call heavy transformations jited:0 32178 32475 PASS
    test_bpf: #240 BPF_MAXINSNS: Jump heavy test jited:0 10518 PASS
    
    test_bpf: #233 BPF_MAXINSNS: Maximum possible literals jited:1 4 PASS
    test_bpf: #234 BPF_MAXINSNS: Single literal jited:1 4 PASS
    test_bpf: #235 BPF_MAXINSNS: Run/add until end jited:1 1625 PASS
    test_bpf: #236 BPF_MAXINSNS: Too many instructions PASS
    test_bpf: #237 BPF_MAXINSNS: Very long jump jited:1 8 PASS
    test_bpf: #238 BPF_MAXINSNS: Ctx heavy transformations jited:1 3301 3174 PASS
    test_bpf: #239 BPF_MAXINSNS: Call heavy transformations jited:1 24107 23491 PASS
    test_bpf: #240 BPF_MAXINSNS: Jump heavy test jited:1 8651 PASS
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Nicolas Schichan <nschichan@freebox.fr>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ce1d72d34382..200be4a74a33 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -277,6 +277,14 @@ struct bpf_prog_aux;
 		.off   = 0,					\
 		.imm   = 0 })
 
+/* Internal classic blocks for direct assignment */
+
+#define __BPF_STMT(CODE, K)					\
+	((struct sock_filter) BPF_STMT(CODE, K))
+
+#define __BPF_JUMP(CODE, K, JT, JF)				\
+	((struct sock_filter) BPF_JUMP(CODE, K, JT, JF))
+
 #define bytes_to_bpf_size(bytes)				\
 ({								\
 	int bpf_size = -EINVAL;					\

commit cffc642d93f9324a06dfbd7da9af29652952a248
Author: Michael Holzheu <holzheu@linux.vnet.ibm.com>
Date:   Mon May 11 22:22:44 2015 -0700

    test_bpf: add 173 new testcases for eBPF
    
    add an exhaustive set of eBPF tests bringing total to:
    test_bpf: Summary: 233 PASSED, 0 FAILED, [0/226 JIT'ed]
    
    Signed-off-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3c03a6085b82..ce1d72d34382 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -207,6 +207,16 @@ struct bpf_prog_aux;
 		.off   = OFF,					\
 		.imm   = 0 })
 
+/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
+
+#define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
+	((struct bpf_insn) {					\
+		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
 /* Memory store, *(uint *) (dst_reg + off16) = imm32 */
 
 #define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\

commit ac67eb2c5347bd9976308c0e0cf1d9e7ca690342
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed May 6 16:12:30 2015 +0200

    seccomp, filter: add and use bpf_prog_create_from_user from seccomp
    
    Seccomp has always been a special candidate when it comes to preparation
    of its filters in seccomp_prepare_filter(). Due to the extra checks and
    filter rewrite it partially duplicates code and has BPF internals exposed.
    
    This patch adds a generic API inside the BPF code code that seccomp can use
    and thus keep it's filter preparation code minimal and better maintainable.
    The other side-effect is that now classic JITs can add seccomp support as
    well by only providing a BPF_LDX | BPF_W | BPF_ABS translation.
    
    Tested with seccomp and BPF test suites.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Nicolas Schichan <nschichan@freebox.fr>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Kees Cook <keescook@chromium.org>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0dcb44bcfc5f..3c03a6085b82 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -374,19 +374,17 @@ static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 	__bpf_prog_free(fp);
 }
 
+typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
+				       unsigned int flen);
+
 int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
+int bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,
+			      bpf_aux_classic_check_t trans);
 void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
-
-typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
-				       unsigned int flen);
-
-struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,
-				    bpf_aux_classic_check_t trans);
-
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit d9e12f42e58da475379b9080708b94f2095904af
Author: Nicolas Schichan <nschichan@freebox.fr>
Date:   Wed May 6 16:12:28 2015 +0200

    seccomp: simplify seccomp_prepare_filter and reuse bpf_prepare_filter
    
    Remove the calls to bpf_check_classic(), bpf_convert_filter() and
    bpf_migrate_runtime() and let bpf_prepare_filter() take care of that
    instead.
    
    seccomp_check_filter() is passed to bpf_prepare_filter() so that it
    gets called from there, after bpf_check_classic().
    
    We can now remove exposure of two internal classic BPF functions
    previously used by seccomp. The export of bpf_check_classic() symbol,
    previously known as sk_chk_filter(), was there since pre git times,
    and no in-tree module was using it, therefore remove it.
    
    Joint work with Daniel Borkmann.
    
    Signed-off-by: Nicolas Schichan <nschichan@freebox.fr>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Kees Cook <keescook@chromium.org>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 91996247cb55..0dcb44bcfc5f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -363,9 +363,6 @@ int sk_filter(struct sock *sk, struct sk_buff *skb);
 void bpf_prog_select_runtime(struct bpf_prog *fp);
 void bpf_prog_free(struct bpf_prog *fp);
 
-int bpf_convert_filter(struct sock_filter *prog, int len,
-		       struct bpf_insn *new_prog, int *new_len);
-
 struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
 struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);
@@ -387,7 +384,6 @@ int sk_detach_filter(struct sock *sk);
 typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
 				       unsigned int flen);
 
-int bpf_check_classic(const struct sock_filter *filter, unsigned int flen);
 struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,
 				    bpf_aux_classic_check_t trans);
 

commit 4ae92bc77ac8e620f7c8d59b5882a4cb0d1c4ef1
Author: Nicolas Schichan <nschichan@freebox.fr>
Date:   Wed May 6 16:12:27 2015 +0200

    net: filter: add a callback to allow classic post-verifier transformations
    
    This is in preparation for use by the seccomp code, the rationale is
    not to duplicate additional code within the seccomp layer, but instead,
    have it abstracted and hidden within the classic BPF API.
    
    As an interim step, this now also makes bpf_prepare_filter() visible
    (not as exported symbol though), so that seccomp can reuse that code
    path instead of reimplementing it.
    
    Joint work with Daniel Borkmann.
    
    Signed-off-by: Nicolas Schichan <nschichan@freebox.fr>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Kees Cook <keescook@chromium.org>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index fa11b3a367be..91996247cb55 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -384,7 +384,13 @@ int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
 
+typedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,
+				       unsigned int flen);
+
 int bpf_check_classic(const struct sock_filter *filter, unsigned int flen);
+struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,
+				    bpf_aux_classic_check_t trans);
+
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit 27cd5452476978283decb19e429e81fc6c71e74b
Author: Michal Sekletar <msekleta@redhat.com>
Date:   Tue Mar 24 14:48:41 2015 +0100

    filter: introduce SKF_AD_VLAN_TPID BPF extension
    
    If vlan offloading takes place then vlan header is removed from frame
    and its contents, both vlan_tci and vlan_proto, is available to user
    space via TPACKET interface. However, only vlan_tci can be used in BPF
    filters.
    
    This commit introduces a new BPF extension. It makes possible to load
    the value of vlan_proto (vlan TPID) to register A. Support for classic
    BPF and eBPF is being added, analogous to skb->protocol.
    
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Jiri Pirko <jpirko@redhat.com>
    
    Signed-off-by: Michal Sekletar <msekleta@redhat.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Reviewed-by: Jiri Pirko <jiri@resnulli.us>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9ee8c67ea249..fa11b3a367be 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -454,6 +454,7 @@ static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
 		BPF_ANCILLARY(VLAN_TAG_PRESENT);
 		BPF_ANCILLARY(PAY_OFFSET);
 		BPF_ANCILLARY(RANDOM);
+		BPF_ANCILLARY(VLAN_TPID);
 		}
 		/* Fallthrough. */
 	default:

commit 24701ecea76b0b93bd9667486934ec310825f558
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Mar 1 12:31:47 2015 +0100

    ebpf: move read-only fields to bpf_prog and shrink bpf_prog_aux
    
    is_gpl_compatible and prog_type should be moved directly into bpf_prog
    as they stay immutable during bpf_prog's lifetime, are core attributes
    and they can be locked as read-only later on via bpf_prog_select_runtime().
    
    With a bit of rearranging, this also allows us to shrink bpf_prog_aux
    to exactly 1 cacheline.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 5e3863d5f666..9ee8c67ea249 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -308,9 +308,11 @@ struct bpf_binary_header {
 struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	bool			jited;		/* Is our filter JIT'ed? */
+	bool			gpl_compatible;	/* Is our filter GPL compatible? */
 	u32			len;		/* Number of filter blocks */
-	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
+	enum bpf_prog_type	type;		/* Type of BPF program */
 	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
+	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct bpf_insn *filter);
 	/* Instructions for interpreter */

commit f1a66f85b74c5ef7b503f746ea97742dacd56419
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Mar 1 12:31:43 2015 +0100

    ebpf: export BPF_PSEUDO_MAP_FD to uapi
    
    We need to export BPF_PSEUDO_MAP_FD to user space, as it's used in the
    ELF BPF loader where instructions are being loaded that need map fixups.
    
    An initial stage loads all maps into the kernel, and later on replaces
    related instructions in the eBPF blob with BPF_PSEUDO_MAP_FD as source
    register and the actual fd as immediate value.
    
    The kernel verifier recognizes this keyword and replaces the map fd with
    a real pointer internally.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index caac2087a4d5..5e3863d5f666 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -145,8 +145,6 @@ struct bpf_prog_aux;
 		.off   = 0,					\
 		.imm   = ((__u64) (IMM)) >> 32 })
 
-#define BPF_PSEUDO_MAP_FD	1
-
 /* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
 #define BPF_LD_MAP_FD(DST, MAP_FD)				\
 	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)

commit 89aa075832b0da4402acebd698d0411dcc82d03e
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Mon Dec 1 15:06:35 2014 -0800

    net: sock: allow eBPF programs to be attached to sockets
    
    introduce new setsockopt() command:
    
    setsockopt(sock, SOL_SOCKET, SO_ATTACH_BPF, &prog_fd, sizeof(prog_fd))
    
    where prog_fd was received from syscall bpf(BPF_PROG_LOAD, attr, ...)
    and attr->prog_type == BPF_PROG_TYPE_SOCKET_FILTER
    
    setsockopt() calls bpf_prog_get() which increments refcnt of the program,
    so it doesn't get unloaded while socket is using the program.
    
    The same eBPF program can be attached to multiple sockets.
    
    User task exit automatically closes socket which calls sk_filter_uncharge()
    which decrements refcnt of eBPF program
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ca95abd2bed1..caac2087a4d5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -381,6 +381,7 @@ int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+int sk_attach_bpf(u32 ufd, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
 
 int bpf_check_classic(const struct sock_filter *filter, unsigned int flen);

commit 0246e64d9a5fcd4805198de59b9b5cf1f974eb41
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:04 2014 -0700

    bpf: handle pseudo BPF_LD_IMM64 insn
    
    eBPF programs passed from userspace are using pseudo BPF_LD_IMM64 instructions
    to refer to process-local map_fd. Scan the program for such instructions and
    if FDs are valid, convert them to 'struct bpf_map' pointers which will be used
    by verifier to check access to maps in bpf_map_lookup/update() calls.
    If program passes verifier, convert pseudo BPF_LD_IMM64 into generic by dropping
    BPF_PSEUDO_MAP_FD flag.
    
    Note that eBPF interpreter is generic and knows nothing about pseudo insns.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4ffc0958d85e..ca95abd2bed1 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -145,6 +145,12 @@ struct bpf_prog_aux;
 		.off   = 0,					\
 		.imm   = ((__u64) (IMM)) >> 32 })
 
+#define BPF_PSEUDO_MAP_FD	1
+
+/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
+#define BPF_LD_MAP_FD(DST, MAP_FD)				\
+	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)
+
 /* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 
 #define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\

commit 09756af46893c18839062976c3252e93a1beeba7
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:00 2014 -0700

    bpf: expand BPF syscall with program load/unload
    
    eBPF programs are similar to kernel modules. They are loaded by the user
    process and automatically unloaded when process exits. Each eBPF program is
    a safe run-to-completion set of instructions. eBPF verifier statically
    determines that the program terminates and is safe to execute.
    
    The following syscall wrapper can be used to load the program:
    int bpf_prog_load(enum bpf_prog_type prog_type,
                      const struct bpf_insn *insns, int insn_cnt,
                      const char *license)
    {
        union bpf_attr attr = {
            .prog_type = prog_type,
            .insns = ptr_to_u64(insns),
            .insn_cnt = insn_cnt,
            .license = ptr_to_u64(license),
        };
    
        return bpf(BPF_PROG_LOAD, &attr, sizeof(attr));
    }
    where 'insns' is an array of eBPF instructions and 'license' is a string
    that must be GPL compatible to call helper functions marked gpl_only
    
    Upon succesful load the syscall returns prog_fd.
    Use close(prog_fd) to unload the program.
    
    User space tests and examples follow in the later patches
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1a0bc6d134d7..4ffc0958d85e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -21,6 +21,7 @@
 struct sk_buff;
 struct sock;
 struct seccomp_data;
+struct bpf_prog_aux;
 
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
@@ -300,17 +301,12 @@ struct bpf_binary_header {
 	u8 image[];
 };
 
-struct bpf_work_struct {
-	struct bpf_prog *prog;
-	struct work_struct work;
-};
-
 struct bpf_prog {
 	u16			pages;		/* Number of allocated pages */
 	bool			jited;		/* Is our filter JIT'ed? */
 	u32			len;		/* Number of filter blocks */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
-	struct bpf_work_struct	*work;		/* Deferred free work struct */
+	struct bpf_prog_aux	*aux;		/* Auxiliary fields */
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct bpf_insn *filter);
 	/* Instructions for interpreter */

commit b954d83421d51d822c42e5ab7b65069b25ad3005
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Wed Sep 10 15:01:02 2014 +0200

    net: bpf: only build bpf_jit_binary_{alloc, free}() when jit selected
    
    Since BPF JIT depends on the availability of module_alloc() and
    module_free() helpers (HAVE_BPF_JIT and MODULES), we better build
    that code only in case we have BPF_JIT in our config enabled, just
    like with other JIT code. Fixes builds for arm/marzen_defconfig
    and sh/rsk7269_defconfig.
    
    ====================
    kernel/built-in.o: In function `bpf_jit_binary_alloc':
    /home/cwang/linux/kernel/bpf/core.c:144: undefined reference to `module_alloc'
    kernel/built-in.o: In function `bpf_jit_binary_free':
    /home/cwang/linux/kernel/bpf/core.c:164: undefined reference to `module_free'
    make: *** [vmlinux] Error 1
    ====================
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Fixes: 738cbe72adc5 ("net: bpf: consolidate JIT binary allocator")
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4b59edead908..1a0bc6d134d7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -4,12 +4,18 @@
 #ifndef __LINUX_FILTER_H__
 #define __LINUX_FILTER_H__
 
+#include <stdarg.h>
+
 #include <linux/atomic.h>
 #include <linux/compat.h>
 #include <linux/skbuff.h>
+#include <linux/linkage.h>
+#include <linux/printk.h>
 #include <linux/workqueue.h>
-#include <uapi/linux/filter.h>
+
 #include <asm/cacheflush.h>
+
+#include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
 struct sk_buff;
@@ -363,14 +369,6 @@ struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);
 void __bpf_prog_free(struct bpf_prog *fp);
 
-typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
-
-struct bpf_binary_header *
-bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
-		     unsigned int alignment,
-		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
-void bpf_jit_binary_free(struct bpf_binary_header *hdr);
-
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
 	bpf_prog_unlock_ro(fp);
@@ -393,6 +391,38 @@ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 void bpf_int_jit_compile(struct bpf_prog *fp);
 
+#ifdef CONFIG_BPF_JIT
+typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
+
+struct bpf_binary_header *
+bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
+		     unsigned int alignment,
+		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
+void bpf_jit_binary_free(struct bpf_binary_header *hdr);
+
+void bpf_jit_compile(struct bpf_prog *fp);
+void bpf_jit_free(struct bpf_prog *fp);
+
+static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
+				u32 pass, void *image)
+{
+	pr_err("flen=%u proglen=%u pass=%u image=%pK\n",
+	       flen, proglen, pass, image);
+	if (image)
+		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
+			       16, 1, image, proglen, false);
+}
+#else
+static inline void bpf_jit_compile(struct bpf_prog *fp)
+{
+}
+
+static inline void bpf_jit_free(struct bpf_prog *fp)
+{
+	bpf_prog_unlock_free(fp);
+}
+#endif /* CONFIG_BPF_JIT */
+
 #define BPF_ANC		BIT(15)
 
 static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
@@ -440,36 +470,6 @@ static inline void *bpf_load_pointer(const struct sk_buff *skb, int k,
 	return bpf_internal_load_pointer_neg_helper(skb, k, size);
 }
 
-#ifdef CONFIG_BPF_JIT
-#include <stdarg.h>
-#include <linux/linkage.h>
-#include <linux/printk.h>
-
-void bpf_jit_compile(struct bpf_prog *fp);
-void bpf_jit_free(struct bpf_prog *fp);
-
-static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
-				u32 pass, void *image)
-{
-	pr_err("flen=%u proglen=%u pass=%u image=%pK\n",
-	       flen, proglen, pass, image);
-	if (image)
-		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
-			       16, 1, image, proglen, false);
-}
-#else
-#include <linux/slab.h>
-
-static inline void bpf_jit_compile(struct bpf_prog *fp)
-{
-}
-
-static inline void bpf_jit_free(struct bpf_prog *fp)
-{
-	bpf_prog_unlock_free(fp);
-}
-#endif /* CONFIG_BPF_JIT */
-
 static inline int bpf_tell_extensions(void)
 {
 	return SKF_AD_MAX;

commit 286aad3c4014ca825c447e07e24f8929e6d266d2
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Sep 8 08:04:49 2014 +0200

    net: bpf: be friendly to kmemcheck
    
    Reported by Mikulas Patocka, kmemcheck currently barks out a
    false positive since we don't have special kmemcheck annotation
    for bitfields used in bpf_prog structure.
    
    We currently have jited:1, len:31 and thus when accessing len
    while CONFIG_KMEMCHECK enabled, kmemcheck throws a warning that
    we're reading uninitialized memory.
    
    As we don't need the whole bit universe for pages member, we
    can just split it to u16 and use a bool flag for jited instead
    of a bitfield.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 868764fcffb8..4b59edead908 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -300,9 +300,9 @@ struct bpf_work_struct {
 };
 
 struct bpf_prog {
-	u32			pages;		/* Number of allocated pages */
-	u32			jited:1,	/* Is our filter JIT'ed? */
-				len:31;		/* Number of filter blocks */
+	u16			pages;		/* Number of allocated pages */
+	bool			jited;		/* Is our filter JIT'ed? */
+	u32			len;		/* Number of filter blocks */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	struct bpf_work_struct	*work;		/* Deferred free work struct */
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,

commit 738cbe72adc5c8f2016c4c68aa5162631d4f27e1
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Sep 8 08:04:47 2014 +0200

    net: bpf: consolidate JIT binary allocator
    
    Introduced in commit 314beb9bcabf ("x86: bpf_jit_comp: secure bpf jit
    against spraying attacks") and later on replicated in aa2d2c73c21f
    ("s390/bpf,jit: address randomize and write protect jit code") for
    s390 architecture, write protection for BPF JIT images got added and
    a random start address of the JIT code, so that it's not on a page
    boundary anymore.
    
    Since both use a very similar allocator for the BPF binary header,
    we can consolidate this code into the BPF core as it's mostly JIT
    independant anyway.
    
    This will also allow for future archs that support DEBUG_SET_MODULE_RONX
    to just reuse instead of reimplementing it.
    
    JIT tested on x86_64 and s390x with BPF test suite.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 8f82ef3f1cdd..868764fcffb8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -289,6 +289,11 @@ struct sock_fprog_kern {
 	struct sock_filter	*filter;
 };
 
+struct bpf_binary_header {
+	unsigned int pages;
+	u8 image[];
+};
+
 struct bpf_work_struct {
 	struct bpf_prog *prog;
 	struct work_struct work;
@@ -358,6 +363,14 @@ struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
 				  gfp_t gfp_extra_flags);
 void __bpf_prog_free(struct bpf_prog *fp);
 
+typedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);
+
+struct bpf_binary_header *
+bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
+		     unsigned int alignment,
+		     bpf_jit_fill_hole_t bpf_fill_ill_insns);
+void bpf_jit_binary_free(struct bpf_binary_header *hdr);
+
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
 	bpf_prog_unlock_ro(fp);

commit daedfb22451dd02b35c0549566cbb7cc06bdd53b
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Sep 4 22:17:18 2014 -0700

    net: filter: split filter.h and expose eBPF to user space
    
    allow user space to generate eBPF programs
    
    uapi/linux/bpf.h: eBPF instruction set definition
    
    linux/filter.h: the rest
    
    This patch only moves macro definitions, but practically it freezes existing
    eBPF instruction set, though new instructions can still be added in the future.
    
    These eBPF definitions cannot go into uapi/linux/filter.h, since the names
    may conflict with existing applications.
    
    Full eBPF ISA description is in Documentation/networking/filter.txt
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bf323da77950..8f82ef3f1cdd 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -10,58 +10,12 @@
 #include <linux/workqueue.h>
 #include <uapi/linux/filter.h>
 #include <asm/cacheflush.h>
+#include <uapi/linux/bpf.h>
 
 struct sk_buff;
 struct sock;
 struct seccomp_data;
 
-/* Internally used and optimized filter representation with extended
- * instruction set based on top of classic BPF.
- */
-
-/* instruction classes */
-#define BPF_ALU64	0x07	/* alu mode in double word width */
-
-/* ld/ldx fields */
-#define BPF_DW		0x18	/* double word */
-#define BPF_XADD	0xc0	/* exclusive add */
-
-/* alu/jmp fields */
-#define BPF_MOV		0xb0	/* mov reg to reg */
-#define BPF_ARSH	0xc0	/* sign extending arithmetic shift right */
-
-/* change endianness of a register */
-#define BPF_END		0xd0	/* flags for endianness conversion: */
-#define BPF_TO_LE	0x00	/* convert to little-endian */
-#define BPF_TO_BE	0x08	/* convert to big-endian */
-#define BPF_FROM_LE	BPF_TO_LE
-#define BPF_FROM_BE	BPF_TO_BE
-
-#define BPF_JNE		0x50	/* jump != */
-#define BPF_JSGT	0x60	/* SGT is signed '>', GT in x86 */
-#define BPF_JSGE	0x70	/* SGE is signed '>=', GE in x86 */
-#define BPF_CALL	0x80	/* function call */
-#define BPF_EXIT	0x90	/* function return */
-
-/* Register numbers */
-enum {
-	BPF_REG_0 = 0,
-	BPF_REG_1,
-	BPF_REG_2,
-	BPF_REG_3,
-	BPF_REG_4,
-	BPF_REG_5,
-	BPF_REG_6,
-	BPF_REG_7,
-	BPF_REG_8,
-	BPF_REG_9,
-	BPF_REG_10,
-	__MAX_BPF_REG,
-};
-
-/* BPF has 10 general purpose 64-bit registers and stack frame. */
-#define MAX_BPF_REG	__MAX_BPF_REG
-
 /* ArgX, context and stack frame pointer register positions. Note,
  * Arg1, Arg2, Arg3, etc are used as argument mappings of function
  * calls in BPF_CALL instruction.
@@ -322,14 +276,6 @@ enum {
 #define SK_RUN_FILTER(filter, ctx) \
 	(*filter->prog->bpf_func)(ctx, filter->prog->insnsi)
 
-struct bpf_insn {
-	__u8	code;		/* opcode */
-	__u8	dst_reg:4;	/* dest register */
-	__u8	src_reg:4;	/* source register */
-	__s16	off;		/* signed offset */
-	__s32	imm;		/* signed immediate constant */
-};
-
 #ifdef CONFIG_COMPAT
 /* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {

commit 02ab695bb37ee9ad515df0d0790d5977505dd04a
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Sep 4 22:17:17 2014 -0700

    net: filter: add "load 64-bit immediate" eBPF instruction
    
    add BPF_LD_IMM64 instruction to load 64-bit immediate value into a register.
    All previous instructions were 8-byte. This is first 16-byte instruction.
    Two consecutive 'struct bpf_insn' blocks are interpreted as single instruction:
    insn[0].code = BPF_LD | BPF_DW | BPF_IMM
    insn[0].dst_reg = destination register
    insn[0].imm = lower 32-bit
    insn[1].code = 0
    insn[1].imm = upper 32-bit
    All unused fields must be zero.
    
    Classic BPF has similar instruction: BPF_LD | BPF_W | BPF_IMM
    which loads 32-bit immediate value into a register.
    
    x64 JITs it as single 'movabsq %rax, imm64'
    arm64 may JIT as sequence of four 'movk x0, #imm16, lsl #shift' insn
    
    Note that old eBPF programs are binary compatible with new interpreter.
    
    It helps eBPF programs load 64-bit constant into a register with one
    instruction instead of using two registers and 4 instructions:
    BPF_MOV32_IMM(R1, imm32)
    BPF_ALU64_IMM(BPF_LSH, R1, 32)
    BPF_MOV32_IMM(R2, imm32)
    BPF_ALU64_REG(BPF_OR, R1, R2)
    
    User space generated programs will use this instruction to load constants only.
    
    To tell kernel that user space needs a pointer the _pseudo_ variant of
    this instruction may be added later, which will use extra bits of encoding
    to indicate what type of pointer user space is asking kernel to provide.
    For example 'off' or 'src_reg' fields can be used for such purpose.
    src_reg = 1 could mean that user space is asking kernel to validate and
    load in-kernel map pointer.
    src_reg = 2 could mean that user space needs readonly data section pointer
    src_reg = 3 could mean that user space needs a pointer to per-cpu local data
    All such future pseudo instructions will not be carrying the actual pointer
    as part of the instruction, but rather will be treated as a request to kernel
    to provide one. The kernel will verify the request_for_a_pointer, then
    will drop _pseudo_ marking and will store actual internal pointer inside
    the instruction, so the end result is the interpreter and JITs never
    see pseudo BPF_LD_IMM64 insns and only operate on generic BPF_LD_IMM64 that
    loads 64-bit immediate into a register. User space never operates on direct
    pointers and verifier can easily recognize request_for_pointer vs other
    instructions.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c78994593355..bf323da77950 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -166,6 +166,24 @@ enum {
 		.off   = 0,					\
 		.imm   = IMM })
 
+/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
+#define BPF_LD_IMM64(DST, IMM)					\
+	BPF_LD_IMM64_RAW(DST, 0, IMM)
+
+#define BPF_LD_IMM64_RAW(DST, SRC, IMM)				\
+	((struct bpf_insn) {					\
+		.code  = BPF_LD | BPF_DW | BPF_IMM,		\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
+		.off   = 0,					\
+		.imm   = (__u32) (IMM) }),			\
+	((struct bpf_insn) {					\
+		.code  = 0, /* zero is reserved opcode */	\
+		.dst_reg = 0,					\
+		.src_reg = 0,					\
+		.off   = 0,					\
+		.imm   = ((__u64) (IMM)) >> 32 })
+
 /* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 
 #define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\

commit 60a3b2253c413cf601783b070507d7dd6620c954
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Tue Sep 2 22:53:44 2014 +0200

    net: bpf: make eBPF interpreter images read-only
    
    With eBPF getting more extended and exposure to user space is on it's way,
    hardening the memory range the interpreter uses to steer its command flow
    seems appropriate.  This patch moves the to be interpreted bytecode to
    read-only pages.
    
    In case we execute a corrupted BPF interpreter image for some reason e.g.
    caused by an attacker which got past a verifier stage, it would not only
    provide arbitrary read/write memory access but arbitrary function calls
    as well. After setting up the BPF interpreter image, its contents do not
    change until destruction time, thus we can setup the image on immutable
    made pages in order to mitigate modifications to that code. The idea
    is derived from commit 314beb9bcabf ("x86: bpf_jit_comp: secure bpf jit
    against spraying attacks").
    
    This is possible because bpf_prog is not part of sk_filter anymore.
    After setup bpf_prog cannot be altered during its life-time. This prevents
    any modifications to the entire bpf_prog structure (incl. function/JIT
    image pointer).
    
    Every eBPF program (including classic BPF that are migrated) have to call
    bpf_prog_select_runtime() to select either interpreter or a JIT image
    as a last setup step, and they all are being freed via bpf_prog_free(),
    including non-JIT. Therefore, we can easily integrate this into the
    eBPF life-time, plus since we directly allocate a bpf_prog, we have no
    performance penalty.
    
    Tested with seccomp and test_bpf testsuite in JIT/non-JIT mode and manual
    inspection of kernel_page_tables.  Brad Spengler proposed the same idea
    via Twitter during development of this patch.
    
    Joint work with Hannes Frederic Sowa.
    
    Suggested-by: Brad Spengler <spender@grsecurity.net>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Kees Cook <keescook@chromium.org>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a5227ab8ccb1..c78994593355 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -9,6 +9,11 @@
 #include <linux/skbuff.h>
 #include <linux/workqueue.h>
 #include <uapi/linux/filter.h>
+#include <asm/cacheflush.h>
+
+struct sk_buff;
+struct sock;
+struct seccomp_data;
 
 /* Internally used and optimized filter representation with extended
  * instruction set based on top of classic BPF.
@@ -320,20 +325,23 @@ struct sock_fprog_kern {
 	struct sock_filter	*filter;
 };
 
-struct sk_buff;
-struct sock;
-struct seccomp_data;
+struct bpf_work_struct {
+	struct bpf_prog *prog;
+	struct work_struct work;
+};
 
 struct bpf_prog {
+	u32			pages;		/* Number of allocated pages */
 	u32			jited:1,	/* Is our filter JIT'ed? */
 				len:31;		/* Number of filter blocks */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
+	struct bpf_work_struct	*work;		/* Deferred free work struct */
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct bpf_insn *filter);
+	/* Instructions for interpreter */
 	union {
 		struct sock_filter	insns[0];
 		struct bpf_insn		insnsi[0];
-		struct work_struct	work;
 	};
 };
 
@@ -353,6 +361,26 @@ static inline unsigned int bpf_prog_size(unsigned int proglen)
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
+#ifdef CONFIG_DEBUG_SET_MODULE_RONX
+static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
+{
+	set_memory_ro((unsigned long)fp, fp->pages);
+}
+
+static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
+{
+	set_memory_rw((unsigned long)fp, fp->pages);
+}
+#else
+static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
+{
+}
+
+static inline void bpf_prog_unlock_ro(struct bpf_prog *fp)
+{
+}
+#endif /* CONFIG_DEBUG_SET_MODULE_RONX */
+
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
 void bpf_prog_select_runtime(struct bpf_prog *fp);
@@ -361,6 +389,17 @@ void bpf_prog_free(struct bpf_prog *fp);
 int bpf_convert_filter(struct sock_filter *prog, int len,
 		       struct bpf_insn *new_prog, int *new_len);
 
+struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);
+struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
+				  gfp_t gfp_extra_flags);
+void __bpf_prog_free(struct bpf_prog *fp);
+
+static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
+{
+	bpf_prog_unlock_ro(fp);
+	__bpf_prog_free(fp);
+}
+
 int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
 void bpf_prog_destroy(struct bpf_prog *fp);
 
@@ -450,7 +489,7 @@ static inline void bpf_jit_compile(struct bpf_prog *fp)
 
 static inline void bpf_jit_free(struct bpf_prog *fp)
 {
-	kfree(fp);
+	bpf_prog_unlock_free(fp);
 }
 #endif /* CONFIG_BPF_JIT */
 

commit 7ae457c1e5b45a1b826fad9d62b32191d2bdcfdb
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:16 2014 -0700

    net: filter: split 'struct sk_filter' into socket and bpf parts
    
    clean up names related to socket filtering and bpf in the following way:
    - everything that deals with sockets keeps 'sk_*' prefix
    - everything that is pure BPF is changed to 'bpf_*' prefix
    
    split 'struct sk_filter' into
    struct sk_filter {
            atomic_t        refcnt;
            struct rcu_head rcu;
            struct bpf_prog *prog;
    };
    and
    struct bpf_prog {
            u32                     jited:1,
                                    len:31;
            struct sock_fprog_kern  *orig_prog;
            unsigned int            (*bpf_func)(const struct sk_buff *skb,
                                                const struct bpf_insn *filter);
            union {
                    struct sock_filter      insns[0];
                    struct bpf_insn         insnsi[0];
                    struct work_struct      work;
            };
    };
    so that 'struct bpf_prog' can be used independent of sockets and cleans up
    'unattached' bpf use cases
    
    split SK_RUN_FILTER macro into:
        SK_RUN_FILTER to be used with 'struct sk_filter *' and
        BPF_PROG_RUN to be used with 'struct bpf_prog *'
    
    __sk_filter_release(struct sk_filter *) gains
    __bpf_prog_release(struct bpf_prog *) helper function
    
    also perform related renames for the functions that work
    with 'struct bpf_prog *', since they're on the same lines:
    
    sk_filter_size -> bpf_prog_size
    sk_filter_select_runtime -> bpf_prog_select_runtime
    sk_filter_free -> bpf_prog_free
    sk_unattached_filter_create -> bpf_prog_create
    sk_unattached_filter_destroy -> bpf_prog_destroy
    sk_store_orig_filter -> bpf_prog_store_orig_filter
    sk_release_orig_filter -> bpf_release_orig_filter
    __sk_migrate_filter -> bpf_migrate_filter
    __sk_prepare_filter -> bpf_prepare_filter
    
    API for attaching classic BPF to a socket stays the same:
    sk_attach_filter(prog, struct sock *)/sk_detach_filter(struct sock *)
    and SK_RUN_FILTER(struct sk_filter *, ctx) to execute a program
    which is used by sockets, tun, af_packet
    
    API for 'unattached' BPF programs becomes:
    bpf_prog_create(struct bpf_prog **)/bpf_prog_destroy(struct bpf_prog *)
    and BPF_PROG_RUN(struct bpf_prog *, ctx) to execute a program
    which is used by isdn, ppp, team, seccomp, ptp, xt_bpf, cls_bpf, test_bpf
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7cb9b40e9a2f..a5227ab8ccb1 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -296,7 +296,8 @@ enum {
 })
 
 /* Macro to invoke filter function. */
-#define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
+#define SK_RUN_FILTER(filter, ctx) \
+	(*filter->prog->bpf_func)(ctx, filter->prog->insnsi)
 
 struct bpf_insn {
 	__u8	code;		/* opcode */
@@ -323,12 +324,10 @@ struct sk_buff;
 struct sock;
 struct seccomp_data;
 
-struct sk_filter {
-	atomic_t		refcnt;
+struct bpf_prog {
 	u32			jited:1,	/* Is our filter JIT'ed? */
 				len:31;		/* Number of filter blocks */
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
-	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct bpf_insn *filter);
 	union {
@@ -338,25 +337,32 @@ struct sk_filter {
 	};
 };
 
-static inline unsigned int sk_filter_size(unsigned int proglen)
+struct sk_filter {
+	atomic_t	refcnt;
+	struct rcu_head	rcu;
+	struct bpf_prog	*prog;
+};
+
+#define BPF_PROG_RUN(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
+
+static inline unsigned int bpf_prog_size(unsigned int proglen)
 {
-	return max(sizeof(struct sk_filter),
-		   offsetof(struct sk_filter, insns[proglen]));
+	return max(sizeof(struct bpf_prog),
+		   offsetof(struct bpf_prog, insns[proglen]));
 }
 
 #define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
-void sk_filter_select_runtime(struct sk_filter *fp);
-void sk_filter_free(struct sk_filter *fp);
+void bpf_prog_select_runtime(struct bpf_prog *fp);
+void bpf_prog_free(struct bpf_prog *fp);
 
 int bpf_convert_filter(struct sock_filter *prog, int len,
 		       struct bpf_insn *new_prog, int *new_len);
 
-int sk_unattached_filter_create(struct sk_filter **pfp,
-				struct sock_fprog_kern *fprog);
-void sk_unattached_filter_destroy(struct sk_filter *fp);
+int bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);
+void bpf_prog_destroy(struct bpf_prog *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
@@ -369,7 +375,7 @@ bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
-void bpf_int_jit_compile(struct sk_filter *fp);
+void bpf_int_jit_compile(struct bpf_prog *fp);
 
 #define BPF_ANC		BIT(15)
 
@@ -423,8 +429,8 @@ static inline void *bpf_load_pointer(const struct sk_buff *skb, int k,
 #include <linux/linkage.h>
 #include <linux/printk.h>
 
-void bpf_jit_compile(struct sk_filter *fp);
-void bpf_jit_free(struct sk_filter *fp);
+void bpf_jit_compile(struct bpf_prog *fp);
+void bpf_jit_free(struct bpf_prog *fp);
 
 static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 				u32 pass, void *image)
@@ -438,11 +444,11 @@ static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 #else
 #include <linux/slab.h>
 
-static inline void bpf_jit_compile(struct sk_filter *fp)
+static inline void bpf_jit_compile(struct bpf_prog *fp)
 {
 }
 
-static inline void bpf_jit_free(struct sk_filter *fp)
+static inline void bpf_jit_free(struct bpf_prog *fp)
 {
 	kfree(fp);
 }

commit 8fb575ca396bc31d9fa99c26336e2432b41d1bfc
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:15 2014 -0700

    net: filter: rename sk_convert_filter() -> bpf_convert_filter()
    
    to indicate that this function is converting classic BPF into eBPF
    and not related to sockets
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c4d0be4c5e75..7cb9b40e9a2f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -351,8 +351,8 @@ int sk_filter(struct sock *sk, struct sk_buff *skb);
 void sk_filter_select_runtime(struct sk_filter *fp);
 void sk_filter_free(struct sk_filter *fp);
 
-int sk_convert_filter(struct sock_filter *prog, int len,
-		      struct bpf_insn *new_prog, int *new_len);
+int bpf_convert_filter(struct sock_filter *prog, int len,
+		       struct bpf_insn *new_prog, int *new_len);
 
 int sk_unattached_filter_create(struct sk_filter **pfp,
 				struct sock_fprog_kern *fprog);

commit 4df95ff488eb796aab9566652c250330179def17
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:14 2014 -0700

    net: filter: rename sk_chk_filter() -> bpf_check_classic()
    
    trivial rename to indicate that this functions performs classic BPF checking
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3769341a745d..c4d0be4c5e75 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -361,7 +361,7 @@ void sk_unattached_filter_destroy(struct sk_filter *fp);
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
 
-int sk_chk_filter(const struct sock_filter *filter, unsigned int flen);
+int bpf_check_classic(const struct sock_filter *filter, unsigned int flen);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit 009937e78a45553a86d26654f192b2fd9ebe289d
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:13 2014 -0700

    net: filter: rename sk_filter_proglen -> bpf_classic_proglen
    
    trivial rename to better match semantics of macro
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 00640edc166f..3769341a745d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -344,8 +344,7 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 		   offsetof(struct sk_filter, insns[proglen]));
 }
 
-#define sk_filter_proglen(fprog)			\
-		(fprog->len * sizeof(fprog->filter[0]))
+#define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))
 
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 

commit 278571baca2aecf5fb5cb5c8b002dbfa0a6c524c
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:12 2014 -0700

    net: filter: simplify socket charging
    
    attaching bpf program to a socket involves multiple socket memory arithmetic,
    since size of 'sk_filter' is changing when classic BPF is converted to eBPF.
    Also common path of program creation has to deal with two ways of freeing
    the memory.
    
    Simplify the code by delaying socket charging until program is ready and
    its size is known
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 20dd50ef7271..00640edc166f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -366,7 +366,7 @@ int sk_chk_filter(const struct sock_filter *filter, unsigned int flen);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 
-void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
+bool sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);

commit 2695fb552cbef1029aa025a98acb80cc51d66de5
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Jul 24 16:38:21 2014 -0700

    net: filter: rename 'struct sock_filter_int' into 'struct bpf_insn'
    
    eBPF is used by socket filtering, seccomp and soon by tracing and
    exposed to userspace, therefore 'sock_filter_int' name is not accurate.
    Rename it to 'bpf_insn'
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c43c8258e682..20dd50ef7271 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -82,7 +82,7 @@ enum {
 /* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
 
 #define BPF_ALU64_REG(OP, DST, SRC)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -90,7 +90,7 @@ enum {
 		.imm   = 0 })
 
 #define BPF_ALU32_REG(OP, DST, SRC)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -100,7 +100,7 @@ enum {
 /* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
 
 #define BPF_ALU64_IMM(OP, DST, IMM)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -108,7 +108,7 @@ enum {
 		.imm   = IMM })
 
 #define BPF_ALU32_IMM(OP, DST, IMM)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -118,7 +118,7 @@ enum {
 /* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
 
 #define BPF_ENDIAN(TYPE, DST, LEN)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -128,7 +128,7 @@ enum {
 /* Short form of mov, dst_reg = src_reg */
 
 #define BPF_MOV64_REG(DST, SRC)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -136,7 +136,7 @@ enum {
 		.imm   = 0 })
 
 #define BPF_MOV32_REG(DST, SRC)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -146,7 +146,7 @@ enum {
 /* Short form of mov, dst_reg = imm32 */
 
 #define BPF_MOV64_IMM(DST, IMM)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -154,7 +154,7 @@ enum {
 		.imm   = IMM })
 
 #define BPF_MOV32_IMM(DST, IMM)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -164,7 +164,7 @@ enum {
 /* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 
 #define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -172,7 +172,7 @@ enum {
 		.imm   = IMM })
 
 #define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -182,7 +182,7 @@ enum {
 /* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
 
 #define BPF_LD_ABS(SIZE, IMM)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
 		.dst_reg = 0,					\
 		.src_reg = 0,					\
@@ -192,7 +192,7 @@ enum {
 /* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
 
 #define BPF_LD_IND(SIZE, SRC, IMM)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
 		.dst_reg = 0,					\
 		.src_reg = SRC,					\
@@ -202,7 +202,7 @@ enum {
 /* Memory load, dst_reg = *(uint *) (src_reg + off16) */
 
 #define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -212,7 +212,7 @@ enum {
 /* Memory store, *(uint *) (dst_reg + off16) = src_reg */
 
 #define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -222,7 +222,7 @@ enum {
 /* Memory store, *(uint *) (dst_reg + off16) = imm32 */
 
 #define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -232,7 +232,7 @@ enum {
 /* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
 
 #define BPF_JMP_REG(OP, DST, SRC, OFF)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -242,7 +242,7 @@ enum {
 /* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
 
 #define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
 		.dst_reg = DST,					\
 		.src_reg = 0,					\
@@ -252,7 +252,7 @@ enum {
 /* Function call */
 
 #define BPF_EMIT_CALL(FUNC)					\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_JMP | BPF_CALL,			\
 		.dst_reg = 0,					\
 		.src_reg = 0,					\
@@ -262,7 +262,7 @@ enum {
 /* Raw code statement block */
 
 #define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = CODE,					\
 		.dst_reg = DST,					\
 		.src_reg = SRC,					\
@@ -272,7 +272,7 @@ enum {
 /* Program exit */
 
 #define BPF_EXIT_INSN()						\
-	((struct sock_filter_int) {				\
+	((struct bpf_insn) {					\
 		.code  = BPF_JMP | BPF_EXIT,			\
 		.dst_reg = 0,					\
 		.src_reg = 0,					\
@@ -298,7 +298,7 @@ enum {
 /* Macro to invoke filter function. */
 #define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 
-struct sock_filter_int {
+struct bpf_insn {
 	__u8	code;		/* opcode */
 	__u8	dst_reg:4;	/* dest register */
 	__u8	src_reg:4;	/* source register */
@@ -330,10 +330,10 @@ struct sk_filter {
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
-					    const struct sock_filter_int *filter);
+					    const struct bpf_insn *filter);
 	union {
 		struct sock_filter	insns[0];
-		struct sock_filter_int	insnsi[0];
+		struct bpf_insn		insnsi[0];
 		struct work_struct	work;
 	};
 };
@@ -353,7 +353,7 @@ void sk_filter_select_runtime(struct sk_filter *fp);
 void sk_filter_free(struct sk_filter *fp);
 
 int sk_convert_filter(struct sock_filter *prog, int len,
-		      struct sock_filter_int *new_prog, int *new_len);
+		      struct bpf_insn *new_prog, int *new_len);
 
 int sk_unattached_filter_create(struct sk_filter **pfp,
 				struct sock_fprog_kern *fprog);

commit ec31a05c4dfa95149b1754d9de92831a5a95c636
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Jul 12 15:49:16 2014 +0200

    net: filter: sk_chk_filter() no longer mangles filter
    
    Add const attribute to filter argument to make clear it is no
    longer modified.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index b885dcb7eaca..c43c8258e682 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -362,7 +362,7 @@ void sk_unattached_filter_destroy(struct sk_filter *fp);
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 int sk_detach_filter(struct sock *sk);
 
-int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
+int sk_chk_filter(const struct sock_filter *filter, unsigned int flen);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
 

commit 9f12fbe603f7ae346b2b46008e325f0c9a68e55d
Author: Zi Shen Lim <zlim.lnx@gmail.com>
Date:   Thu Jul 3 07:56:54 2014 -0700

    net: filter: move load_pointer() into filter.h
    
    load_pointer() is already a static inline function.
    Let's move it into filter.h so BPF JIT implementations can reuse this
    function.
    
    Since we're exporting this function, let's also rename it to
    bpf_load_pointer() for clarity.
    
    Signed-off-by: Zi Shen Lim <zlim.lnx@gmail.com>
    Reviewed-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a7e3c48d73a7..b885dcb7eaca 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -6,6 +6,7 @@
 
 #include <linux/atomic.h>
 #include <linux/compat.h>
+#include <linux/skbuff.h>
 #include <linux/workqueue.h>
 #include <uapi/linux/filter.h>
 
@@ -406,6 +407,18 @@ static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
 	}
 }
 
+void *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb,
+					   int k, unsigned int size);
+
+static inline void *bpf_load_pointer(const struct sk_buff *skb, int k,
+				     unsigned int size, void *buffer)
+{
+	if (k >= 0)
+		return skb_header_pointer(skb, k, size, buffer);
+
+	return bpf_internal_load_pointer_neg_helper(skb, k, size);
+}
+
 #ifdef CONFIG_BPF_JIT
 #include <stdarg.h>
 #include <linux/linkage.h>

commit e430f34ee5192c84bcabd3c79ab7e2388b5eec74
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Jun 6 14:46:06 2014 -0700

    net: filter: cleanup A/X name usage
    
    The macro 'A' used in internal BPF interpreter:
     #define A regs[insn->a_reg]
    was easily confused with the name of classic BPF register 'A', since
    'A' would mean two different things depending on context.
    
    This patch is trying to clean up the naming and clarify its usage in the
    following way:
    
    - A and X are names of two classic BPF registers
    
    - BPF_REG_A denotes internal BPF register R0 used to map classic register A
      in internal BPF programs generated from classic
    
    - BPF_REG_X denotes internal BPF register R7 used to map classic register X
      in internal BPF programs generated from classic
    
    - internal BPF instruction format:
    struct sock_filter_int {
            __u8    code;           /* opcode */
            __u8    dst_reg:4;      /* dest register */
            __u8    src_reg:4;      /* source register */
            __s16   off;            /* signed offset */
            __s32   imm;            /* signed immediate constant */
    };
    
    - BPF_X/BPF_K is 1 bit used to encode source operand of instruction
    In classic:
      BPF_X - means use register X as source operand
      BPF_K - means use 32-bit immediate as source operand
    In internal:
      BPF_X - means use 'src_reg' register as source operand
      BPF_K - means use 32-bit immediate as source operand
    
    Suggested-by: Chema Gonzalez <chema@google.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Chema Gonzalez <chema@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f0c2ad43b4af..a7e3c48d73a7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -78,161 +78,173 @@ enum {
 
 /* Helper macros for filter block array initializers. */
 
-/* ALU ops on registers, bpf_add|sub|...: A += X */
+/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
 
-#define BPF_ALU64_REG(OP, A, X)					\
+#define BPF_ALU64_REG(OP, DST, SRC)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = 0 })
 
-#define BPF_ALU32_REG(OP, A, X)					\
+#define BPF_ALU32_REG(OP, DST, SRC)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = 0 })
 
-/* ALU ops on immediates, bpf_add|sub|...: A += IMM */
+/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
 
-#define BPF_ALU64_IMM(OP, A, IMM)				\
+#define BPF_ALU64_IMM(OP, DST, IMM)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
-#define BPF_ALU32_IMM(OP, A, IMM)				\
+#define BPF_ALU32_IMM(OP, DST, IMM)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
 /* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
 
-#define BPF_ENDIAN(TYPE, A, LEN)				\
+#define BPF_ENDIAN(TYPE, DST, LEN)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = LEN })
 
-/* Short form of mov, A = X */
+/* Short form of mov, dst_reg = src_reg */
 
-#define BPF_MOV64_REG(A, X)					\
+#define BPF_MOV64_REG(DST, SRC)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = 0 })
 
-#define BPF_MOV32_REG(A, X)					\
+#define BPF_MOV32_REG(DST, SRC)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = 0 })
 
-/* Short form of mov, A = IMM */
+/* Short form of mov, dst_reg = imm32 */
 
-#define BPF_MOV64_IMM(A, IMM)					\
+#define BPF_MOV64_IMM(DST, IMM)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
-#define BPF_MOV32_IMM(A, IMM)					\
+#define BPF_MOV32_IMM(DST, IMM)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
-/* Short form of mov based on type, BPF_X: A = X,  BPF_K: A = IMM */
+/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
 
-#define BPF_MOV64_RAW(TYPE, A, X, IMM)				\
+#define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)			\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
-#define BPF_MOV32_RAW(TYPE, A, X, IMM)				\
+#define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)			\
 	((struct sock_filter_int) {				\
 		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
 		.imm   = IMM })
 
-/* Direct packet access, R0 = *(uint *) (skb->data + OFF) */
+/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
 
-#define BPF_LD_ABS(SIZE, OFF)					\
+#define BPF_LD_ABS(SIZE, IMM)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
-		.a_reg = 0,					\
-		.x_reg = 0,					\
+		.dst_reg = 0,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
-		.imm   = OFF })
+		.imm   = IMM })
 
-/* Indirect packet access, R0 = *(uint *) (skb->data + X + OFF) */
+/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
 
-#define BPF_LD_IND(SIZE, X, OFF)				\
+#define BPF_LD_IND(SIZE, SRC, IMM)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
-		.a_reg = 0,					\
-		.x_reg = X,					\
+		.dst_reg = 0,					\
+		.src_reg = SRC,					\
 		.off   = 0,					\
-		.imm   = OFF })
+		.imm   = IMM })
 
-/* Memory store, A = *(uint *) (X + OFF), and vice versa */
+/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
 
-#define BPF_LDX_MEM(SIZE, A, X, OFF)				\
+#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
 	((struct sock_filter_int) {				\
 		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = OFF,					\
 		.imm   = 0 })
 
-#define BPF_STX_MEM(SIZE, A, X, OFF)				\
+/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
+
+#define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
 	((struct sock_filter_int) {				\
 		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = OFF,					\
 		.imm   = 0 })
 
-/* Conditional jumps against registers, if (A 'op' X) goto pc + OFF */
+/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
+
+#define BPF_ST_MEM(SIZE, DST, OFF, IMM)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,	\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
+		.off   = OFF,					\
+		.imm   = IMM })
+
+/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
 
-#define BPF_JMP_REG(OP, A, X, OFF)				\
+#define BPF_JMP_REG(OP, DST, SRC, OFF)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = OFF,					\
 		.imm   = 0 })
 
-/* Conditional jumps against immediates, if (A 'op' IMM) goto pc + OFF */
+/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
 
-#define BPF_JMP_IMM(OP, A, IMM, OFF)				\
+#define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
 	((struct sock_filter_int) {				\
 		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
-		.a_reg = A,					\
-		.x_reg = 0,					\
+		.dst_reg = DST,					\
+		.src_reg = 0,					\
 		.off   = OFF,					\
 		.imm   = IMM })
 
@@ -241,18 +253,18 @@ enum {
 #define BPF_EMIT_CALL(FUNC)					\
 	((struct sock_filter_int) {				\
 		.code  = BPF_JMP | BPF_CALL,			\
-		.a_reg = 0,					\
-		.x_reg = 0,					\
+		.dst_reg = 0,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = ((FUNC) - __bpf_call_base) })
 
 /* Raw code statement block */
 
-#define BPF_RAW_INSN(CODE, A, X, OFF, IMM)			\
+#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
 	((struct sock_filter_int) {				\
 		.code  = CODE,					\
-		.a_reg = A,					\
-		.x_reg = X,					\
+		.dst_reg = DST,					\
+		.src_reg = SRC,					\
 		.off   = OFF,					\
 		.imm   = IMM })
 
@@ -261,8 +273,8 @@ enum {
 #define BPF_EXIT_INSN()						\
 	((struct sock_filter_int) {				\
 		.code  = BPF_JMP | BPF_EXIT,			\
-		.a_reg = 0,					\
-		.x_reg = 0,					\
+		.dst_reg = 0,					\
+		.src_reg = 0,					\
 		.off   = 0,					\
 		.imm   = 0 })
 
@@ -287,8 +299,8 @@ enum {
 
 struct sock_filter_int {
 	__u8	code;		/* opcode */
-	__u8	a_reg:4;	/* dest register */
-	__u8	x_reg:4;	/* source register */
+	__u8	dst_reg:4;	/* dest register */
+	__u8	src_reg:4;	/* source register */
 	__s16	off;		/* signed offset */
 	__s32	imm;		/* signed immediate constant */
 };

commit f8f6d679aaa78b989d9aee8d2935066fbdca2a30
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Thu May 29 10:22:51 2014 +0200

    net: filter: improve filter block macros
    
    Commit 9739eef13c92 ("net: filter: make BPF conversion more readable")
    started to introduce helper macros similar to BPF_STMT()/BPF_JUMP()
    macros from classic BPF.
    
    However, quite some statements in the filter conversion functions
    remained in the old style which gives a mixture of block macros and
    non block macros in the code. This patch makes the block macros itself
    more readable by using explicit member initialization, and converts
    the remaining ones where possible to remain in a more consistent state.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 49ef7a298c92..f0c2ad43b4af 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -76,56 +76,211 @@ enum {
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
-/* bpf_add|sub|...: a += x, bpf_mov: a = x */
-#define BPF_ALU64_REG(op, a, x) \
-	((struct sock_filter_int) {BPF_ALU64|BPF_OP(op)|BPF_X, a, x, 0, 0})
-#define BPF_ALU32_REG(op, a, x) \
-	((struct sock_filter_int) {BPF_ALU|BPF_OP(op)|BPF_X, a, x, 0, 0})
-
-/* bpf_add|sub|...: a += imm, bpf_mov: a = imm */
-#define BPF_ALU64_IMM(op, a, imm) \
-	((struct sock_filter_int) {BPF_ALU64|BPF_OP(op)|BPF_K, a, 0, 0, imm})
-#define BPF_ALU32_IMM(op, a, imm) \
-	((struct sock_filter_int) {BPF_ALU|BPF_OP(op)|BPF_K, a, 0, 0, imm})
-
-/* R0 = *(uint *) (skb->data + off) */
-#define BPF_LD_ABS(size, off) \
-	((struct sock_filter_int) {BPF_LD|BPF_SIZE(size)|BPF_ABS, 0, 0, 0, off})
-
-/* R0 = *(uint *) (skb->data + x + off) */
-#define BPF_LD_IND(size, x, off) \
-	((struct sock_filter_int) {BPF_LD|BPF_SIZE(size)|BPF_IND, 0, x, 0, off})
-
-/* a = *(uint *) (x + off) */
-#define BPF_LDX_MEM(sz, a, x, off) \
-	((struct sock_filter_int) {BPF_LDX|BPF_SIZE(sz)|BPF_MEM, a, x, off, 0})
-
-/* if (a 'op' x) goto pc+off */
-#define BPF_JMP_REG(op, a, x, off) \
-	((struct sock_filter_int) {BPF_JMP|BPF_OP(op)|BPF_X, a, x, off, 0})
-
-/* if (a 'op' imm) goto pc+off */
-#define BPF_JMP_IMM(op, a, imm, off) \
-	((struct sock_filter_int) {BPF_JMP|BPF_OP(op)|BPF_K, a, 0, off, imm})
-
-#define BPF_EXIT_INSN() \
-	((struct sock_filter_int) {BPF_JMP|BPF_EXIT, 0, 0, 0, 0})
-
-static inline int size_to_bpf(int size)
-{
-	switch (size) {
-	case 1:
-		return BPF_B;
-	case 2:
-		return BPF_H;
-	case 4:
-		return BPF_W;
-	case 8:
-		return BPF_DW;
-	default:
-		return -EINVAL;
-	}
-}
+/* Helper macros for filter block array initializers. */
+
+/* ALU ops on registers, bpf_add|sub|...: A += X */
+
+#define BPF_ALU64_REG(OP, A, X)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = 0 })
+
+#define BPF_ALU32_REG(OP, A, X)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_OP(OP) | BPF_X,		\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = 0 })
+
+/* ALU ops on immediates, bpf_add|sub|...: A += IMM */
+
+#define BPF_ALU64_IMM(OP, A, IMM)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+#define BPF_ALU32_IMM(OP, A, IMM)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_OP(OP) | BPF_K,		\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
+
+#define BPF_ENDIAN(TYPE, A, LEN)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),	\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = LEN })
+
+/* Short form of mov, A = X */
+
+#define BPF_MOV64_REG(A, X)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = 0 })
+
+#define BPF_MOV32_REG(A, X)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = 0 })
+
+/* Short form of mov, A = IMM */
+
+#define BPF_MOV64_IMM(A, IMM)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+#define BPF_MOV32_IMM(A, IMM)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_MOV | BPF_K,		\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+/* Short form of mov based on type, BPF_X: A = X,  BPF_K: A = IMM */
+
+#define BPF_MOV64_RAW(TYPE, A, X, IMM)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),	\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+#define BPF_MOV32_RAW(TYPE, A, X, IMM)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),	\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = IMM })
+
+/* Direct packet access, R0 = *(uint *) (skb->data + OFF) */
+
+#define BPF_LD_ABS(SIZE, OFF)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,	\
+		.a_reg = 0,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = OFF })
+
+/* Indirect packet access, R0 = *(uint *) (skb->data + X + OFF) */
+
+#define BPF_LD_IND(SIZE, X, OFF)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,	\
+		.a_reg = 0,					\
+		.x_reg = X,					\
+		.off   = 0,					\
+		.imm   = OFF })
+
+/* Memory store, A = *(uint *) (X + OFF), and vice versa */
+
+#define BPF_LDX_MEM(SIZE, A, X, OFF)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
+#define BPF_STX_MEM(SIZE, A, X, OFF)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
+/* Conditional jumps against registers, if (A 'op' X) goto pc + OFF */
+
+#define BPF_JMP_REG(OP, A, X, OFF)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = OFF,					\
+		.imm   = 0 })
+
+/* Conditional jumps against immediates, if (A 'op' IMM) goto pc + OFF */
+
+#define BPF_JMP_IMM(OP, A, IMM, OFF)				\
+	((struct sock_filter_int) {				\
+		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
+		.a_reg = A,					\
+		.x_reg = 0,					\
+		.off   = OFF,					\
+		.imm   = IMM })
+
+/* Function call */
+
+#define BPF_EMIT_CALL(FUNC)					\
+	((struct sock_filter_int) {				\
+		.code  = BPF_JMP | BPF_CALL,			\
+		.a_reg = 0,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = ((FUNC) - __bpf_call_base) })
+
+/* Raw code statement block */
+
+#define BPF_RAW_INSN(CODE, A, X, OFF, IMM)			\
+	((struct sock_filter_int) {				\
+		.code  = CODE,					\
+		.a_reg = A,					\
+		.x_reg = X,					\
+		.off   = OFF,					\
+		.imm   = IMM })
+
+/* Program exit */
+
+#define BPF_EXIT_INSN()						\
+	((struct sock_filter_int) {				\
+		.code  = BPF_JMP | BPF_EXIT,			\
+		.a_reg = 0,					\
+		.x_reg = 0,					\
+		.off   = 0,					\
+		.imm   = 0 })
+
+#define bytes_to_bpf_size(bytes)				\
+({								\
+	int bpf_size = -EINVAL;					\
+								\
+	if (bytes == sizeof(u8))				\
+		bpf_size = BPF_B;				\
+	else if (bytes == sizeof(u16))				\
+		bpf_size = BPF_H;				\
+	else if (bytes == sizeof(u32))				\
+		bpf_size = BPF_W;				\
+	else if (bytes == sizeof(u64))				\
+		bpf_size = BPF_DW;				\
+								\
+	bpf_size;						\
+})
 
 /* Macro to invoke filter function. */
 #define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)

commit 3480593131e0b781287dae0139bf7ccee7cba7ff
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Thu May 29 10:22:50 2014 +0200

    net: filter: get rid of BPF_S_* enum
    
    This patch finally allows us to get rid of the BPF_S_* enum.
    Currently, the code performs unnecessary encode and decode
    workarounds in seccomp and filter migration itself when a filter
    is being attached in order to overcome BPF_S_* encoding which
    is not used anymore by the new interpreter resp. JIT compilers.
    
    Keeping it around would mean that also in future we would need
    to extend and maintain this enum and related encoders/decoders.
    We can get rid of all that and save us these operations during
    filter attaching. Naturally, also JIT compilers need to be updated
    by this.
    
    Before JIT conversion is being done, each compiler checks if A
    is being loaded at startup to obtain information if it needs to
    emit instructions to clear A first. Since BPF extensions are a
    subset of BPF_LD | BPF_{W,H,B} | BPF_ABS variants, case statements
    for extensions can be removed at that point. To ease and minimalize
    code changes in the classic JITs, we have introduced bpf_anc_helper().
    
    Tested with test_bpf on x86_64 (JIT, int), s390x (JIT, int),
    arm (JIT, int), i368 (int), ppc64 (JIT, int); for sparc we
    unfortunately didn't have access, but changes are analogous to
    the rest.
    
    Joint work with Alexei Starovoitov.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mircea Gherzan <mgherzan@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Acked-by: Chema Gonzalez <chemag@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 625f4de9bdf2..49ef7a298c92 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -197,7 +197,6 @@ int sk_detach_filter(struct sock *sk);
 int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
 		  unsigned int len);
-void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 
 void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
@@ -205,6 +204,41 @@ void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
 void bpf_int_jit_compile(struct sk_filter *fp);
 
+#define BPF_ANC		BIT(15)
+
+static inline u16 bpf_anc_helper(const struct sock_filter *ftest)
+{
+	BUG_ON(ftest->code & BPF_ANC);
+
+	switch (ftest->code) {
+	case BPF_LD | BPF_W | BPF_ABS:
+	case BPF_LD | BPF_H | BPF_ABS:
+	case BPF_LD | BPF_B | BPF_ABS:
+#define BPF_ANCILLARY(CODE)	case SKF_AD_OFF + SKF_AD_##CODE:	\
+				return BPF_ANC | SKF_AD_##CODE
+		switch (ftest->k) {
+		BPF_ANCILLARY(PROTOCOL);
+		BPF_ANCILLARY(PKTTYPE);
+		BPF_ANCILLARY(IFINDEX);
+		BPF_ANCILLARY(NLATTR);
+		BPF_ANCILLARY(NLATTR_NEST);
+		BPF_ANCILLARY(MARK);
+		BPF_ANCILLARY(QUEUE);
+		BPF_ANCILLARY(HATYPE);
+		BPF_ANCILLARY(RXHASH);
+		BPF_ANCILLARY(CPU);
+		BPF_ANCILLARY(ALU_XOR_X);
+		BPF_ANCILLARY(VLAN_TAG);
+		BPF_ANCILLARY(VLAN_TAG_PRESENT);
+		BPF_ANCILLARY(PAY_OFFSET);
+		BPF_ANCILLARY(RANDOM);
+		}
+		/* Fallthrough. */
+	default:
+		return ftest->code;
+	}
+}
+
 #ifdef CONFIG_BPF_JIT
 #include <stdarg.h>
 #include <linux/linkage.h>
@@ -224,86 +258,20 @@ static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 }
 #else
 #include <linux/slab.h>
+
 static inline void bpf_jit_compile(struct sk_filter *fp)
 {
 }
+
 static inline void bpf_jit_free(struct sk_filter *fp)
 {
 	kfree(fp);
 }
-#endif
+#endif /* CONFIG_BPF_JIT */
 
 static inline int bpf_tell_extensions(void)
 {
 	return SKF_AD_MAX;
 }
 
-enum {
-	BPF_S_RET_K = 1,
-	BPF_S_RET_A,
-	BPF_S_ALU_ADD_K,
-	BPF_S_ALU_ADD_X,
-	BPF_S_ALU_SUB_K,
-	BPF_S_ALU_SUB_X,
-	BPF_S_ALU_MUL_K,
-	BPF_S_ALU_MUL_X,
-	BPF_S_ALU_DIV_X,
-	BPF_S_ALU_MOD_K,
-	BPF_S_ALU_MOD_X,
-	BPF_S_ALU_AND_K,
-	BPF_S_ALU_AND_X,
-	BPF_S_ALU_OR_K,
-	BPF_S_ALU_OR_X,
-	BPF_S_ALU_XOR_K,
-	BPF_S_ALU_XOR_X,
-	BPF_S_ALU_LSH_K,
-	BPF_S_ALU_LSH_X,
-	BPF_S_ALU_RSH_K,
-	BPF_S_ALU_RSH_X,
-	BPF_S_ALU_NEG,
-	BPF_S_LD_W_ABS,
-	BPF_S_LD_H_ABS,
-	BPF_S_LD_B_ABS,
-	BPF_S_LD_W_LEN,
-	BPF_S_LD_W_IND,
-	BPF_S_LD_H_IND,
-	BPF_S_LD_B_IND,
-	BPF_S_LD_IMM,
-	BPF_S_LDX_W_LEN,
-	BPF_S_LDX_B_MSH,
-	BPF_S_LDX_IMM,
-	BPF_S_MISC_TAX,
-	BPF_S_MISC_TXA,
-	BPF_S_ALU_DIV_K,
-	BPF_S_LD_MEM,
-	BPF_S_LDX_MEM,
-	BPF_S_ST,
-	BPF_S_STX,
-	BPF_S_JMP_JA,
-	BPF_S_JMP_JEQ_K,
-	BPF_S_JMP_JEQ_X,
-	BPF_S_JMP_JGE_K,
-	BPF_S_JMP_JGE_X,
-	BPF_S_JMP_JGT_K,
-	BPF_S_JMP_JGT_X,
-	BPF_S_JMP_JSET_K,
-	BPF_S_JMP_JSET_X,
-	/* Ancillary data */
-	BPF_S_ANC_PROTOCOL,
-	BPF_S_ANC_PKTTYPE,
-	BPF_S_ANC_IFINDEX,
-	BPF_S_ANC_NLATTR,
-	BPF_S_ANC_NLATTR_NEST,
-	BPF_S_ANC_MARK,
-	BPF_S_ANC_QUEUE,
-	BPF_S_ANC_HATYPE,
-	BPF_S_ANC_RXHASH,
-	BPF_S_ANC_CPU,
-	BPF_S_ANC_ALU_XOR_X,
-	BPF_S_ANC_VLAN_TAG,
-	BPF_S_ANC_VLAN_TAG_PRESENT,
-	BPF_S_ANC_PAY_OFFSET,
-	BPF_S_ANC_RANDOM,
-};
-
 #endif /* __LINUX_FILTER_H__ */

commit b1fcd35cf53553a0a3ef949b05106d921446abc3
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri May 23 18:43:58 2014 +0200

    net: filter: let unattached filters use sock_fprog_kern
    
    The sk_unattached_filter_create() API is used by BPF filters that
    are not directly attached or related to sockets, and are used in
    team, ptp, xt_bpf, cls_bpf, etc. As such all users do their own
    internal managment of obtaining filter blocks and thus already
    have them in kernel memory and set up before calling into
    sk_unattached_filter_create(). As a result, due to __user annotation
    in sock_fprog, sparse triggers false positives (incorrect type in
    assignment [different address space]) when filters are set up before
    passing them to sk_unattached_filter_create(). Therefore, let
    sk_unattached_filter_create() API use sock_fprog_kern to overcome
    this issue.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2b0056afd1f7..625f4de9bdf2 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -188,7 +188,7 @@ int sk_convert_filter(struct sock_filter *prog, int len,
 		      struct sock_filter_int *new_prog, int *new_len);
 
 int sk_unattached_filter_create(struct sk_filter **pfp,
-				struct sock_fprog *fprog);
+				struct sock_fprog_kern *fprog);
 void sk_unattached_filter_destroy(struct sk_filter *fp);
 
 int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);

commit 8556ce79d5986a87fee4c29300b4efee07c0f15e
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri May 23 18:43:57 2014 +0200

    net: filter: remove DL macro
    
    Lets get rid of this macro. After commit 5bcfedf06f7f ("net: filter:
    simplify label names from jump-table"), labels have become more
    readable due to omission of BPF_ prefix but at the same time more
    generic, so that things like `git grep -n` would not find them. As
    a middle path, lets get rid of the DL macro as it's not strictly
    needed and would otherwise just hide the full name.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 7977b3958e25..2b0056afd1f7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -37,9 +37,6 @@
 #define BPF_CALL	0x80	/* function call */
 #define BPF_EXIT	0x90	/* function return */
 
-/* Placeholder/dummy for 0 */
-#define BPF_0		0
-
 /* Register numbers */
 enum {
 	BPF_REG_0 = 0,

commit 5fe821a9dee241fa450703ab7015d970ee0cfb8d
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Mon May 19 14:56:14 2014 -0700

    net: filter: cleanup invocation of internal BPF
    
    Kernel API for classic BPF socket filters is:
    
    sk_unattached_filter_create() - validate classic BPF, convert, JIT
    SK_RUN_FILTER() - run it
    sk_unattached_filter_destroy() - destroy socket filter
    
    Cleanup internal BPF kernel API as following:
    
    sk_filter_select_runtime() - final step of internal BPF creation.
      Try to JIT internal BPF program, if JIT is not available select interpreter
    SK_RUN_FILTER() - run it
    sk_filter_free() - free internal BPF program
    
    Disallow direct calls to BPF interpreter. Execution of the BPF program should
    be done with SK_RUN_FILTER() macro.
    
    Example of internal BPF create, run, destroy:
    
      struct sk_filter *fp;
    
      fp = kzalloc(sk_filter_size(prog_len), GFP_KERNEL);
      memcpy(fp->insni, prog, prog_len * sizeof(fp->insni[0]));
      fp->len = prog_len;
    
      sk_filter_select_runtime(fp);
    
      SK_RUN_FILTER(fp, ctx);
    
      sk_filter_free(fp);
    
    Sockets, seccomp, testsuite, tracing are using different ways to populate
    sk_filter, so first steps of program creation are not common.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9d5ae0a2c954..7977b3958e25 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -184,10 +184,8 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
-u32 sk_run_filter_int_seccomp(const struct seccomp_data *ctx,
-			      const struct sock_filter_int *insni);
-u32 sk_run_filter_int_skb(const struct sk_buff *ctx,
-			  const struct sock_filter_int *insni);
+void sk_filter_select_runtime(struct sk_filter *fp);
+void sk_filter_free(struct sk_filter *fp);
 
 int sk_convert_filter(struct sock_filter *prog, int len,
 		      struct sock_filter_int *new_prog, int *new_len);

commit 622582786c9e041d0bd52bde201787adeab249f8
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Tue May 13 19:50:46 2014 -0700

    net: filter: x86: internal BPF JIT
    
    Maps all internal BPF instructions into x86_64 instructions.
    This patch replaces original BPF x64 JIT with internal BPF x64 JIT.
    sysctl net.core.bpf_jit_enable is reused as on/off switch.
    
    Performance:
    
    1. old BPF JIT and internal BPF JIT generate equivalent x86_64 code.
      No performance difference is observed for filters that were JIT-able before
    
    Example assembler code for BPF filter "tcpdump port 22"
    
    original BPF -> old JIT:            original BPF -> internal BPF -> new JIT:
       0:   push   %rbp                      0:     push   %rbp
       1:   mov    %rsp,%rbp                 1:     mov    %rsp,%rbp
       4:   sub    $0x60,%rsp                4:     sub    $0x228,%rsp
       8:   mov    %rbx,-0x8(%rbp)           b:     mov    %rbx,-0x228(%rbp) // prologue
                                            12:     mov    %r13,-0x220(%rbp)
                                            19:     mov    %r14,-0x218(%rbp)
                                            20:     mov    %r15,-0x210(%rbp)
                                            27:     xor    %eax,%eax         // clear A
       c:   xor    %ebx,%ebx                29:     xor    %r13,%r13         // clear X
       e:   mov    0x68(%rdi),%r9d          2c:     mov    0x68(%rdi),%r9d
      12:   sub    0x6c(%rdi),%r9d          30:     sub    0x6c(%rdi),%r9d
      16:   mov    0xd8(%rdi),%r8           34:     mov    0xd8(%rdi),%r10
                                            3b:     mov    %rdi,%rbx
      1d:   mov    $0xc,%esi                3e:     mov    $0xc,%esi
      22:   callq  0xffffffffe1021e15       43:     callq  0xffffffffe102bd75
      27:   cmp    $0x86dd,%eax             48:     cmp    $0x86dd,%rax
      2c:   jne    0x0000000000000069       4f:     jne    0x000000000000009a
      2e:   mov    $0x14,%esi               51:     mov    $0x14,%esi
      33:   callq  0xffffffffe1021e31       56:     callq  0xffffffffe102bd91
      38:   cmp    $0x84,%eax               5b:     cmp    $0x84,%rax
      3d:   je     0x0000000000000049       62:     je     0x0000000000000074
      3f:   cmp    $0x6,%eax                64:     cmp    $0x6,%rax
      42:   je     0x0000000000000049       68:     je     0x0000000000000074
      44:   cmp    $0x11,%eax               6a:     cmp    $0x11,%rax
      47:   jne    0x00000000000000c6       6e:     jne    0x0000000000000117
      49:   mov    $0x36,%esi               74:     mov    $0x36,%esi
      4e:   callq  0xffffffffe1021e15       79:     callq  0xffffffffe102bd75
      53:   cmp    $0x16,%eax               7e:     cmp    $0x16,%rax
      56:   je     0x00000000000000bf       82:     je     0x0000000000000110
      58:   mov    $0x38,%esi               88:     mov    $0x38,%esi
      5d:   callq  0xffffffffe1021e15       8d:     callq  0xffffffffe102bd75
      62:   cmp    $0x16,%eax               92:     cmp    $0x16,%rax
      65:   je     0x00000000000000bf       96:     je     0x0000000000000110
      67:   jmp    0x00000000000000c6       98:     jmp    0x0000000000000117
      69:   cmp    $0x800,%eax              9a:     cmp    $0x800,%rax
      6e:   jne    0x00000000000000c6       a1:     jne    0x0000000000000117
      70:   mov    $0x17,%esi               a3:     mov    $0x17,%esi
      75:   callq  0xffffffffe1021e31       a8:     callq  0xffffffffe102bd91
      7a:   cmp    $0x84,%eax               ad:     cmp    $0x84,%rax
      7f:   je     0x000000000000008b       b4:     je     0x00000000000000c2
      81:   cmp    $0x6,%eax                b6:     cmp    $0x6,%rax
      84:   je     0x000000000000008b       ba:     je     0x00000000000000c2
      86:   cmp    $0x11,%eax               bc:     cmp    $0x11,%rax
      89:   jne    0x00000000000000c6       c0:     jne    0x0000000000000117
      8b:   mov    $0x14,%esi               c2:     mov    $0x14,%esi
      90:   callq  0xffffffffe1021e15       c7:     callq  0xffffffffe102bd75
      95:   test   $0x1fff,%ax              cc:     test   $0x1fff,%rax
      99:   jne    0x00000000000000c6       d3:     jne    0x0000000000000117
                                            d5:     mov    %rax,%r14
      9b:   mov    $0xe,%esi                d8:     mov    $0xe,%esi
      a0:   callq  0xffffffffe1021e44       dd:     callq  0xffffffffe102bd91 // MSH
                                            e2:     and    $0xf,%eax
                                            e5:     shl    $0x2,%eax
                                            e8:     mov    %rax,%r13
                                            eb:     mov    %r14,%rax
                                            ee:     mov    %r13,%rsi
      a5:   lea    0xe(%rbx),%esi           f1:     add    $0xe,%esi
      a8:   callq  0xffffffffe1021e0d       f4:     callq  0xffffffffe102bd6d
      ad:   cmp    $0x16,%eax               f9:     cmp    $0x16,%rax
      b0:   je     0x00000000000000bf       fd:     je     0x0000000000000110
                                            ff:     mov    %r13,%rsi
      b2:   lea    0x10(%rbx),%esi         102:     add    $0x10,%esi
      b5:   callq  0xffffffffe1021e0d      105:     callq  0xffffffffe102bd6d
      ba:   cmp    $0x16,%eax              10a:     cmp    $0x16,%rax
      bd:   jne    0x00000000000000c6      10e:     jne    0x0000000000000117
      bf:   mov    $0xffff,%eax            110:     mov    $0xffff,%eax
      c4:   jmp    0x00000000000000c8      115:     jmp    0x000000000000011c
      c6:   xor    %eax,%eax               117:     mov    $0x0,%eax
      c8:   mov    -0x8(%rbp),%rbx         11c:     mov    -0x228(%rbp),%rbx // epilogue
      cc:   leaveq                         123:     mov    -0x220(%rbp),%r13
      cd:   retq                           12a:     mov    -0x218(%rbp),%r14
                                           131:     mov    -0x210(%rbp),%r15
                                           138:     leaveq
                                           139:     retq
    
    On fully cached SKBs both JITed functions take 12 nsec to execute.
    BPF interpreter executes the program in 30 nsec.
    
    The difference in generated assembler is due to the following:
    
    Old BPF imlements LDX_MSH instruction via sk_load_byte_msh() helper function
    inside bpf_jit.S.
    New JIT removes the helper and does it explicitly, so ldx_msh cost
    is the same for both JITs, but generated code looks longer.
    
    New JIT has 4 registers to save, so prologue/epilogue are larger,
    but the cost is within noise on x64.
    
    Old JIT checks whether first insn clears A and if not emits 'xor %eax,%eax'.
    New JIT clears %rax unconditionally.
    
    2. old BPF JIT doesn't support ANC_NLATTR, ANC_PAY_OFFSET, ANC_RANDOM
      extensions. New JIT supports all BPF extensions.
      Performance of such filters improves 2-4 times depending on a filter.
      The longer the filter the higher performance gain.
      Synthetic benchmarks with many ancillary loads see 20x speedup
      which seems to be the maximum gain from JIT
    
    Notes:
    
    . net.core.bpf_jit_enable=2 + tools/net/bpf_jit_disasm is still functional
      and can be used to see generated assembler
    
    . there are two jit_compile() functions and code flow for classic filters is:
      sk_attach_filter() - load classic BPF
      bpf_jit_compile() - try to JIT from classic BPF
      sk_convert_filter() - convert classic to internal
      bpf_int_jit_compile() - JIT from internal BPF
    
      seccomp and tracing filters will just call bpf_int_jit_compile()
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4457b383961c..9d5ae0a2c954 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -207,6 +207,9 @@ void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
 void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
+u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);
+void bpf_int_jit_compile(struct sk_filter *fp);
+
 #ifdef CONFIG_BPF_JIT
 #include <stdarg.h>
 #include <linux/linkage.h>

commit 9739eef13c926645fbf88bcb77e66442fa75d688
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu May 8 14:10:51 2014 -0700

    net: filter: make BPF conversion more readable
    
    Introduce BPF helper macros to define instructions
    (similar to old BPF_STMT/BPF_JUMP macros)
    
    Use them while converting classic BPF to internal
    and in BPF testsuite later.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ed1efab10b8f..4457b383961c 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -79,6 +79,57 @@ enum {
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
+/* bpf_add|sub|...: a += x, bpf_mov: a = x */
+#define BPF_ALU64_REG(op, a, x) \
+	((struct sock_filter_int) {BPF_ALU64|BPF_OP(op)|BPF_X, a, x, 0, 0})
+#define BPF_ALU32_REG(op, a, x) \
+	((struct sock_filter_int) {BPF_ALU|BPF_OP(op)|BPF_X, a, x, 0, 0})
+
+/* bpf_add|sub|...: a += imm, bpf_mov: a = imm */
+#define BPF_ALU64_IMM(op, a, imm) \
+	((struct sock_filter_int) {BPF_ALU64|BPF_OP(op)|BPF_K, a, 0, 0, imm})
+#define BPF_ALU32_IMM(op, a, imm) \
+	((struct sock_filter_int) {BPF_ALU|BPF_OP(op)|BPF_K, a, 0, 0, imm})
+
+/* R0 = *(uint *) (skb->data + off) */
+#define BPF_LD_ABS(size, off) \
+	((struct sock_filter_int) {BPF_LD|BPF_SIZE(size)|BPF_ABS, 0, 0, 0, off})
+
+/* R0 = *(uint *) (skb->data + x + off) */
+#define BPF_LD_IND(size, x, off) \
+	((struct sock_filter_int) {BPF_LD|BPF_SIZE(size)|BPF_IND, 0, x, 0, off})
+
+/* a = *(uint *) (x + off) */
+#define BPF_LDX_MEM(sz, a, x, off) \
+	((struct sock_filter_int) {BPF_LDX|BPF_SIZE(sz)|BPF_MEM, a, x, off, 0})
+
+/* if (a 'op' x) goto pc+off */
+#define BPF_JMP_REG(op, a, x, off) \
+	((struct sock_filter_int) {BPF_JMP|BPF_OP(op)|BPF_X, a, x, off, 0})
+
+/* if (a 'op' imm) goto pc+off */
+#define BPF_JMP_IMM(op, a, imm, off) \
+	((struct sock_filter_int) {BPF_JMP|BPF_OP(op)|BPF_K, a, 0, off, imm})
+
+#define BPF_EXIT_INSN() \
+	((struct sock_filter_int) {BPF_JMP|BPF_EXIT, 0, 0, 0, 0})
+
+static inline int size_to_bpf(int size)
+{
+	switch (size) {
+	case 1:
+		return BPF_B;
+	case 2:
+		return BPF_H;
+	case 4:
+		return BPF_W;
+	case 8:
+		return BPF_DW;
+	default:
+		return -EINVAL;
+	}
+}
+
 /* Macro to invoke filter function. */
 #define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 

commit 30743837dd204d2b04fd4e9d3db78cc7b118c81a
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Thu May 1 18:34:19 2014 +0200

    net: filter: make register naming more comprehensible
    
    The current code is a bit hard to parse on which registers can be used,
    how they are mapped and all play together. It makes much more sense to
    define this a bit more clearly so that the code is a bit more intuitive.
    This patch cleans this up, and makes naming a bit more consistent among
    the code. This also allows for moving some of the defines into the header
    file. Clearing of A and X registers in __sk_run_filter() do not get a
    particular register name assigned as they have not an 'official' function,
    but rather just result from the concrete initial mapping of old BPF
    programs. Since for BPF helper functions for BPF_CALL we already use
    small letters, so be consistent here as well. No functional changes.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index b042d1db127f..ed1efab10b8f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -40,16 +40,47 @@
 /* Placeholder/dummy for 0 */
 #define BPF_0		0
 
+/* Register numbers */
+enum {
+	BPF_REG_0 = 0,
+	BPF_REG_1,
+	BPF_REG_2,
+	BPF_REG_3,
+	BPF_REG_4,
+	BPF_REG_5,
+	BPF_REG_6,
+	BPF_REG_7,
+	BPF_REG_8,
+	BPF_REG_9,
+	BPF_REG_10,
+	__MAX_BPF_REG,
+};
+
 /* BPF has 10 general purpose 64-bit registers and stack frame. */
-#define MAX_BPF_REG	11
+#define MAX_BPF_REG	__MAX_BPF_REG
+
+/* ArgX, context and stack frame pointer register positions. Note,
+ * Arg1, Arg2, Arg3, etc are used as argument mappings of function
+ * calls in BPF_CALL instruction.
+ */
+#define BPF_REG_ARG1	BPF_REG_1
+#define BPF_REG_ARG2	BPF_REG_2
+#define BPF_REG_ARG3	BPF_REG_3
+#define BPF_REG_ARG4	BPF_REG_4
+#define BPF_REG_ARG5	BPF_REG_5
+#define BPF_REG_CTX	BPF_REG_6
+#define BPF_REG_FP	BPF_REG_10
+
+/* Additional register mappings for converted user programs. */
+#define BPF_REG_A	BPF_REG_0
+#define BPF_REG_X	BPF_REG_7
+#define BPF_REG_TMP	BPF_REG_8
 
 /* BPF program can access up to 512 bytes of stack space. */
 #define MAX_BPF_STACK	512
 
-/* Arg1, context and stack frame pointer register positions. */
-#define ARG1_REG	1
-#define CTX_REG		6
-#define FP_REG		10
+/* Macro to invoke filter function. */
+#define SK_RUN_FILTER(filter, ctx)  (*filter->bpf_func)(ctx, filter->insnsi)
 
 struct sock_filter_int {
 	__u8	code;		/* opcode */
@@ -100,9 +131,6 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 #define sk_filter_proglen(fprog)			\
 		(fprog->len * sizeof(fprog->filter[0]))
 
-#define SK_RUN_FILTER(filter, ctx)			\
-		(*filter->bpf_func)(ctx, filter->insnsi)
-
 int sk_filter(struct sock *sk, struct sk_buff *skb);
 
 u32 sk_run_filter_int_seccomp(const struct seccomp_data *ctx,

commit 5bcfedf06f7fdf9efcf65dc11198e9012f7530f4
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Thu May 1 18:34:18 2014 +0200

    net: filter: simplify label names from jump-table
    
    This patch simplifies label naming for the BPF jump-table.
    When we define labels via DL(), we just concatenate/textify
    the combination of instruction opcode which consists of the
    class, subclass, word size, target register and so on. Each
    time we leave BPF_ prefix intact, so that e.g. the preprocessor
    generates a label BPF_ALU_BPF_ADD_BPF_X for DL(BPF_ALU, BPF_ADD,
    BPF_X) whereas a label name of ALU_ADD_X is much more easy
    to grasp. Pure cleanup only.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 759abf78dd61..b042d1db127f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -37,6 +37,9 @@
 #define BPF_CALL	0x80	/* function call */
 #define BPF_EXIT	0x90	/* function return */
 
+/* Placeholder/dummy for 0 */
+#define BPF_0		0
+
 /* BPF has 10 general purpose 64-bit registers and stack frame. */
 #define MAX_BPF_REG	11
 

commit 4cd3675ebf74d7f559038ded6aa8088e4099a83d
Author: Chema Gonzalez <chema@google.com>
Date:   Mon Apr 21 09:21:24 2014 -0700

    filter: added BPF random opcode
    
    Added a new ancillary load (bpf call in eBPF parlance) that produces
    a 32-bit random number. We are implementing it as an ancillary load
    (instead of an ISA opcode) because (a) it is simpler, (b) allows easy
    JITing, and (c) seems more in line with generic ISAs that do not have
    "get a random number" as a instruction, but as an OS call.
    
    The main use for this ancillary load is to perform random packet sampling.
    
    Signed-off-by: Chema Gonzalez <chema@google.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 024fd03e5d18..759abf78dd61 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -223,6 +223,7 @@ enum {
 	BPF_S_ANC_VLAN_TAG,
 	BPF_S_ANC_VLAN_TAG_PRESENT,
 	BPF_S_ANC_PAY_OFFSET,
+	BPF_S_ANC_RANDOM,
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit 8c482cdc358ef931ee02262e0a4ef0f29946aa0c
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Apr 14 21:20:12 2014 +0200

    net: filter: seccomp: fix wrong decoding of BPF_S_ANC_SECCOMP_LD_W
    
    While reviewing seccomp code, we found that BPF_S_ANC_SECCOMP_LD_W has
    been wrongly decoded by commit a8fc927780 ("sk-filter: Add ability to
    get socket filter program (v2)") into the opcode BPF_LD|BPF_B|BPF_ABS
    although it should have been decoded as BPF_LD|BPF_W|BPF_ABS.
    
    In practice, this should not have much side-effect though, as such
    conversion is/was being done through prctl(2) PR_SET_SECCOMP. Reverse
    operation PR_GET_SECCOMP will only return the current seccomp mode, but
    not the filter itself. Since the transition to the new BPF infrastructure,
    it's also not used anymore, so we can simply remove this as it's
    unreachable.
    
    Fixes: a8fc927780 ("sk-filter: Add ability to get socket filter program (v2)")
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 262dcbb75ffe..024fd03e5d18 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -220,7 +220,6 @@ enum {
 	BPF_S_ANC_RXHASH,
 	BPF_S_ANC_CPU,
 	BPF_S_ANC_ALU_XOR_X,
-	BPF_S_ANC_SECCOMP_LD_W,
 	BPF_S_ANC_VLAN_TAG,
 	BPF_S_ANC_VLAN_TAG_PRESENT,
 	BPF_S_ANC_PAY_OFFSET,

commit bd4cf0ed331a275e9bf5a49e6d0fd55dffc551b8
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Mar 28 18:58:25 2014 +0100

    net: filter: rework/optimize internal BPF interpreter's instruction set
    
    This patch replaces/reworks the kernel-internal BPF interpreter with
    an optimized BPF instruction set format that is modelled closer to
    mimic native instruction sets and is designed to be JITed with one to
    one mapping. Thus, the new interpreter is noticeably faster than the
    current implementation of sk_run_filter(); mainly for two reasons:
    
    1. Fall-through jumps:
    
      BPF jump instructions are forced to go either 'true' or 'false'
      branch which causes branch-miss penalty. The new BPF jump
      instructions have only one branch and fall-through otherwise,
      which fits the CPU branch predictor logic better. `perf stat`
      shows drastic difference for branch-misses between the old and
      new code.
    
    2. Jump-threaded implementation of interpreter vs switch
       statement:
    
      Instead of single table-jump at the top of 'switch' statement,
      gcc will now generate multiple table-jump instructions, which
      helps CPU branch predictor logic.
    
    Note that the verification of filters is still being done through
    sk_chk_filter() in classical BPF format, so filters from user- or
    kernel space are verified in the same way as we do now, and same
    restrictions/constraints hold as well.
    
    We reuse current BPF JIT compilers in a way that this upgrade would
    even be fine as is, but nevertheless allows for a successive upgrade
    of BPF JIT compilers to the new format.
    
    The internal instruction set migration is being done after the
    probing for JIT compilation, so in case JIT compilers are able to
    create a native opcode image, we're going to use that, and in all
    other cases we're doing a follow-up migration of the BPF program's
    instruction set, so that it can be transparently run in the new
    interpreter.
    
    In short, the *internal* format extends BPF in the following way (more
    details can be taken from the appended documentation):
    
      - Number of registers increase from 2 to 10
      - Register width increases from 32-bit to 64-bit
      - Conditional jt/jf targets replaced with jt/fall-through
      - Adds signed > and >= insns
      - 16 4-byte stack slots for register spill-fill replaced
        with up to 512 bytes of multi-use stack space
      - Introduction of bpf_call insn and register passing convention
        for zero overhead calls from/to other kernel functions
      - Adds arithmetic right shift and endianness conversion insns
      - Adds atomic_add insn
      - Old tax/txa insns are replaced with 'mov dst,src' insn
    
    Performance of two BPF filters generated by libpcap resp. bpf_asm
    was measured on x86_64, i386 and arm32 (other libpcap programs
    have similar performance differences):
    
    fprog #1 is taken from Documentation/networking/filter.txt:
    tcpdump -i eth0 port 22 -dd
    
    fprog #2 is taken from 'man tcpdump':
    tcpdump -i eth0 'tcp port 22 and (((ip[2:2] - ((ip[0]&0xf)<<2)) -
       ((tcp[12]&0xf0)>>2)) != 0)' -dd
    
    Raw performance data from BPF micro-benchmark: SK_RUN_FILTER on the
    same SKB (cache-hit) or 10k SKBs (cache-miss); time in ns per call,
    smaller is better:
    
    --x86_64--
             fprog #1  fprog #1   fprog #2  fprog #2
             cache-hit cache-miss cache-hit cache-miss
    old BPF      90       101        192       202
    new BPF      31        71         47        97
    old BPF jit  12        34         17        44
    new BPF jit TBD
    
    --i386--
             fprog #1  fprog #1   fprog #2  fprog #2
             cache-hit cache-miss cache-hit cache-miss
    old BPF     107       136        227       252
    new BPF      40       119         69       172
    
    --arm32--
             fprog #1  fprog #1   fprog #2  fprog #2
             cache-hit cache-miss cache-hit cache-miss
    old BPF     202       300        475       540
    new BPF     180       270        330       470
    old BPF jit  26       182         37       202
    new BPF jit TBD
    
    Thus, without changing any userland BPF filters, applications on
    top of AF_PACKET (or other families) such as libpcap/tcpdump, cls_bpf
    classifier, netfilter's xt_bpf, team driver's load-balancing mode,
    and many more will have better interpreter filtering performance.
    
    While we are replacing the internal BPF interpreter, we also need
    to convert seccomp BPF in the same step to make use of the new
    internal structure since it makes use of lower-level API details
    without being further decoupled through higher-level calls like
    sk_unattached_filter_{create,destroy}(), for example.
    
    Just as for normal socket filtering, also seccomp BPF experiences
    a time-to-verdict speedup:
    
    05-sim-long_jumps.c of libseccomp was used as micro-benchmark:
    
      seccomp_rule_add_exact(ctx,...
      seccomp_rule_add_exact(ctx,...
    
      rc = seccomp_load(ctx);
    
      for (i = 0; i < 10000000; i++)
         syscall(199, 100);
    
    'short filter' has 2 rules
    'large filter' has 200 rules
    
    'short filter' performance is slightly better on x86_64/i386/arm32
    'large filter' is much faster on x86_64 and i386 and shows no
                   difference on arm32
    
    --x86_64-- short filter
    old BPF: 2.7 sec
     39.12%  bench  libc-2.15.so       [.] syscall
      8.10%  bench  [kernel.kallsyms]  [k] sk_run_filter
      6.31%  bench  [kernel.kallsyms]  [k] system_call
      5.59%  bench  [kernel.kallsyms]  [k] trace_hardirqs_on_caller
      4.37%  bench  [kernel.kallsyms]  [k] trace_hardirqs_off_caller
      3.70%  bench  [kernel.kallsyms]  [k] __secure_computing
      3.67%  bench  [kernel.kallsyms]  [k] lock_is_held
      3.03%  bench  [kernel.kallsyms]  [k] seccomp_bpf_load
    new BPF: 2.58 sec
     42.05%  bench  libc-2.15.so       [.] syscall
      6.91%  bench  [kernel.kallsyms]  [k] system_call
      6.25%  bench  [kernel.kallsyms]  [k] trace_hardirqs_on_caller
      6.07%  bench  [kernel.kallsyms]  [k] __secure_computing
      5.08%  bench  [kernel.kallsyms]  [k] sk_run_filter_int_seccomp
    
    --arm32-- short filter
    old BPF: 4.0 sec
     39.92%  bench  [kernel.kallsyms]  [k] vector_swi
     16.60%  bench  [kernel.kallsyms]  [k] sk_run_filter
     14.66%  bench  libc-2.17.so       [.] syscall
      5.42%  bench  [kernel.kallsyms]  [k] seccomp_bpf_load
      5.10%  bench  [kernel.kallsyms]  [k] __secure_computing
    new BPF: 3.7 sec
     35.93%  bench  [kernel.kallsyms]  [k] vector_swi
     21.89%  bench  libc-2.17.so       [.] syscall
     13.45%  bench  [kernel.kallsyms]  [k] sk_run_filter_int_seccomp
      6.25%  bench  [kernel.kallsyms]  [k] __secure_computing
      3.96%  bench  [kernel.kallsyms]  [k] syscall_trace_exit
    
    --x86_64-- large filter
    old BPF: 8.6 seconds
        73.38%    bench  [kernel.kallsyms]  [k] sk_run_filter
        10.70%    bench  libc-2.15.so       [.] syscall
         5.09%    bench  [kernel.kallsyms]  [k] seccomp_bpf_load
         1.97%    bench  [kernel.kallsyms]  [k] system_call
    new BPF: 5.7 seconds
        66.20%    bench  [kernel.kallsyms]  [k] sk_run_filter_int_seccomp
        16.75%    bench  libc-2.15.so       [.] syscall
         3.31%    bench  [kernel.kallsyms]  [k] system_call
         2.88%    bench  [kernel.kallsyms]  [k] __secure_computing
    
    --i386-- large filter
    old BPF: 5.4 sec
    new BPF: 3.8 sec
    
    --arm32-- large filter
    old BPF: 13.5 sec
     73.88%  bench  [kernel.kallsyms]  [k] sk_run_filter
     10.29%  bench  [kernel.kallsyms]  [k] vector_swi
      6.46%  bench  libc-2.17.so       [.] syscall
      2.94%  bench  [kernel.kallsyms]  [k] seccomp_bpf_load
      1.19%  bench  [kernel.kallsyms]  [k] __secure_computing
      0.87%  bench  [kernel.kallsyms]  [k] sys_getuid
    new BPF: 13.5 sec
     76.08%  bench  [kernel.kallsyms]  [k] sk_run_filter_int_seccomp
     10.98%  bench  [kernel.kallsyms]  [k] vector_swi
      5.87%  bench  libc-2.17.so       [.] syscall
      1.77%  bench  [kernel.kallsyms]  [k] __secure_computing
      0.93%  bench  [kernel.kallsyms]  [k] sys_getuid
    
    BPF filters generated by seccomp are very branchy, so the new
    internal BPF performance is better than the old one. Performance
    gains will be even higher when BPF JIT is committed for the
    new structure, which is planned in future work (as successive
    JIT migrations).
    
    BPF has also been stress-tested with trinity's BPF fuzzer.
    
    Joint work with Daniel Borkmann.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Cc: Hagen Paul Pfeifer <hagen@jauu.net>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Paul Moore <pmoore@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9bde3ed19fe6..262dcbb75ffe 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -9,13 +9,58 @@
 #include <linux/workqueue.h>
 #include <uapi/linux/filter.h>
 
-#ifdef CONFIG_COMPAT
-/*
- * A struct sock_filter is architecture independent.
+/* Internally used and optimized filter representation with extended
+ * instruction set based on top of classic BPF.
  */
+
+/* instruction classes */
+#define BPF_ALU64	0x07	/* alu mode in double word width */
+
+/* ld/ldx fields */
+#define BPF_DW		0x18	/* double word */
+#define BPF_XADD	0xc0	/* exclusive add */
+
+/* alu/jmp fields */
+#define BPF_MOV		0xb0	/* mov reg to reg */
+#define BPF_ARSH	0xc0	/* sign extending arithmetic shift right */
+
+/* change endianness of a register */
+#define BPF_END		0xd0	/* flags for endianness conversion: */
+#define BPF_TO_LE	0x00	/* convert to little-endian */
+#define BPF_TO_BE	0x08	/* convert to big-endian */
+#define BPF_FROM_LE	BPF_TO_LE
+#define BPF_FROM_BE	BPF_TO_BE
+
+#define BPF_JNE		0x50	/* jump != */
+#define BPF_JSGT	0x60	/* SGT is signed '>', GT in x86 */
+#define BPF_JSGE	0x70	/* SGE is signed '>=', GE in x86 */
+#define BPF_CALL	0x80	/* function call */
+#define BPF_EXIT	0x90	/* function return */
+
+/* BPF has 10 general purpose 64-bit registers and stack frame. */
+#define MAX_BPF_REG	11
+
+/* BPF program can access up to 512 bytes of stack space. */
+#define MAX_BPF_STACK	512
+
+/* Arg1, context and stack frame pointer register positions. */
+#define ARG1_REG	1
+#define CTX_REG		6
+#define FP_REG		10
+
+struct sock_filter_int {
+	__u8	code;		/* opcode */
+	__u8	a_reg:4;	/* dest register */
+	__u8	x_reg:4;	/* source register */
+	__s16	off;		/* signed offset */
+	__s32	imm;		/* signed immediate constant */
+};
+
+#ifdef CONFIG_COMPAT
+/* A struct sock_filter is architecture independent. */
 struct compat_sock_fprog {
 	u16		len;
-	compat_uptr_t	filter;		/* struct sock_filter * */
+	compat_uptr_t	filter;	/* struct sock_filter * */
 };
 #endif
 
@@ -26,6 +71,7 @@ struct sock_fprog_kern {
 
 struct sk_buff;
 struct sock;
+struct seccomp_data;
 
 struct sk_filter {
 	atomic_t		refcnt;
@@ -34,9 +80,10 @@ struct sk_filter {
 	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
-					    const struct sock_filter *filter);
+					    const struct sock_filter_int *filter);
 	union {
-		struct sock_filter     	insns[0];
+		struct sock_filter	insns[0];
+		struct sock_filter_int	insnsi[0];
 		struct work_struct	work;
 	};
 };
@@ -50,9 +97,18 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 #define sk_filter_proglen(fprog)			\
 		(fprog->len * sizeof(fprog->filter[0]))
 
+#define SK_RUN_FILTER(filter, ctx)			\
+		(*filter->bpf_func)(ctx, filter->insnsi)
+
 int sk_filter(struct sock *sk, struct sk_buff *skb);
-unsigned int sk_run_filter(const struct sk_buff *skb,
-			   const struct sock_filter *filter);
+
+u32 sk_run_filter_int_seccomp(const struct seccomp_data *ctx,
+			      const struct sock_filter_int *insni);
+u32 sk_run_filter_int_skb(const struct sk_buff *ctx,
+			  const struct sock_filter_int *insni);
+
+int sk_convert_filter(struct sock_filter *prog, int len,
+		      struct sock_filter_int *new_prog, int *new_len);
 
 int sk_unattached_filter_create(struct sk_filter **pfp,
 				struct sock_fprog *fprog);
@@ -86,7 +142,6 @@ static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
 			       16, 1, image, proglen, false);
 }
-#define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)
 #else
 #include <linux/slab.h>
 static inline void bpf_jit_compile(struct sk_filter *fp)
@@ -96,7 +151,6 @@ static inline void bpf_jit_free(struct sk_filter *fp)
 {
 	kfree(fp);
 }
-#define SK_RUN_FILTER(FILTER, SKB) sk_run_filter(SKB, FILTER->insns)
 #endif
 
 static inline int bpf_tell_extensions(void)

commit fbc907f0b1386c02e00516aa78a0fa6b0454fd0b
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Mar 28 18:58:20 2014 +0100

    net: filter: move filter accounting to filter core
    
    This patch basically does two things, i) removes the extern keyword
    from the include/linux/filter.h file to be more consistent with the
    rest of Joe's changes, and ii) moves filter accounting into the filter
    core framework.
    
    Filter accounting mainly done through sk_filter_{un,}charge() take
    care of the case when sockets are being cloned through sk_clone_lock()
    so that removal of the filter on one socket won't result in eviction
    as it's still referenced by the other.
    
    These functions actually belong to net/core/filter.c and not
    include/net/sock.h as we want to keep all that in a central place.
    It's also not in fast-path so uninlining them is fine and even allows
    us to get rd of sk_filter_release_rcu()'s EXPORT_SYMBOL and a forward
    declaration.
    
    Joint work with Alexei Starovoitov.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 93a9792e27bc..9bde3ed19fe6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -50,28 +50,32 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 #define sk_filter_proglen(fprog)			\
 		(fprog->len * sizeof(fprog->filter[0]))
 
-extern int sk_filter(struct sock *sk, struct sk_buff *skb);
-extern unsigned int sk_run_filter(const struct sk_buff *skb,
-				  const struct sock_filter *filter);
+int sk_filter(struct sock *sk, struct sk_buff *skb);
+unsigned int sk_run_filter(const struct sk_buff *skb,
+			   const struct sock_filter *filter);
 
-extern int sk_unattached_filter_create(struct sk_filter **pfp,
-				       struct sock_fprog *fprog);
-extern void sk_unattached_filter_destroy(struct sk_filter *fp);
+int sk_unattached_filter_create(struct sk_filter **pfp,
+				struct sock_fprog *fprog);
+void sk_unattached_filter_destroy(struct sk_filter *fp);
 
-extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
-extern int sk_detach_filter(struct sock *sk);
+int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+int sk_detach_filter(struct sock *sk);
 
-extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
-extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
-extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
+int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
+int sk_get_filter(struct sock *sk, struct sock_filter __user *filter,
+		  unsigned int len);
+void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
+
+void sk_filter_charge(struct sock *sk, struct sk_filter *fp);
+void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);
 
 #ifdef CONFIG_BPF_JIT
 #include <stdarg.h>
 #include <linux/linkage.h>
 #include <linux/printk.h>
 
-extern void bpf_jit_compile(struct sk_filter *fp);
-extern void bpf_jit_free(struct sk_filter *fp);
+void bpf_jit_compile(struct sk_filter *fp);
+void bpf_jit_free(struct sk_filter *fp);
 
 static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 				u32 pass, void *image)

commit a3ea269b8bcdbb0c5fa2fd449a436e7987446975
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Mar 28 18:58:19 2014 +0100

    net: filter: keep original BPF program around
    
    In order to open up the possibility to internally transform a BPF program
    into an alternative and possibly non-trivial reversible representation, we
    need to keep the original BPF program around, so that it can be passed back
    to user space w/o the need of a complex decoder.
    
    The reason for that use case resides in commit a8fc92778080 ("sk-filter:
    Add ability to get socket filter program (v2)"), that is, the ability
    to retrieve the currently attached BPF filter from a given socket used
    mainly by the checkpoint-restore project, for example.
    
    Therefore, we add two helpers sk_{store,release}_orig_filter for taking
    care of that. In the sk_unattached_filter_create() case, there's no such
    possibility/requirement to retrieve a loaded BPF program. Therefore, we
    can spare us the work in that case.
    
    This approach will simplify and slightly speed up both, sk_get_filter()
    and sock_diag_put_filterinfo() handlers as we won't need to successively
    decode filters anymore through sk_decode_filter(). As we still need
    sk_decode_filter() later on, we're keeping it around.
    
    Joint work with Alexei Starovoitov.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index e65e23087367..93a9792e27bc 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -19,14 +19,19 @@ struct compat_sock_fprog {
 };
 #endif
 
+struct sock_fprog_kern {
+	u16			len;
+	struct sock_filter	*filter;
+};
+
 struct sk_buff;
 struct sock;
 
-struct sk_filter
-{
+struct sk_filter {
 	atomic_t		refcnt;
 	u32			jited:1,	/* Is our filter JIT'ed? */
 				len:31;		/* Number of filter blocks */
+	struct sock_fprog_kern	*orig_prog;	/* Original BPF program */
 	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct sock_filter *filter);
@@ -42,14 +47,20 @@ static inline unsigned int sk_filter_size(unsigned int proglen)
 		   offsetof(struct sk_filter, insns[proglen]));
 }
 
+#define sk_filter_proglen(fprog)			\
+		(fprog->len * sizeof(fprog->filter[0]))
+
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
 extern unsigned int sk_run_filter(const struct sk_buff *skb,
 				  const struct sock_filter *filter);
+
 extern int sk_unattached_filter_create(struct sk_filter **pfp,
 				       struct sock_fprog *fprog);
 extern void sk_unattached_filter_destroy(struct sk_filter *fp);
+
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
+
 extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);

commit f8bbbfc3b97f4c7a6c7c23185e520b22bfc3a21d
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Mar 28 18:58:18 2014 +0100

    net: filter: add jited flag to indicate jit compiled filters
    
    This patch adds a jited flag into sk_filter struct in order to indicate
    whether a filter is currently jited or not. The size of sk_filter is
    not being expanded as the 32 bit 'len' member allows upper bits to be
    reused since a filter can currently only grow as large as BPF_MAXINSNS.
    
    Therefore, there's enough room also for other in future needed flags to
    reuse 'len' field if necessary. The jited flag also allows for having
    alternative interpreter functions running as currently, we can only
    detect jit compiled filters by testing fp->bpf_func to not equal the
    address of sk_run_filter().
    
    Joint work with Alexei Starovoitov.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Cc: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index e568c8ef896b..e65e23087367 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -25,7 +25,8 @@ struct sock;
 struct sk_filter
 {
 	atomic_t		refcnt;
-	unsigned int         	len;	/* Number of filter blocks */
+	u32			jited:1,	/* Is our filter JIT'ed? */
+				len:31;		/* Number of filter blocks */
 	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct sock_filter *filter);

commit 37692299319db31f33f04ce418604781fa5710fb
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Tue Jan 21 00:19:37 2014 +0100

    net: filter: let bpf_tell_extensions return SKF_AD_MAX
    
    Michal Sekletar added in commit ea02f9411d9f ("net: introduce
    SO_BPF_EXTENSIONS") a facility where user space can enquire
    the BPF ancillary instruction set, which is imho a step into
    the right direction for letting user space high-level to BPF
    optimizers make an informed decision for possibly using these
    extensions.
    
    The original rationale was to return through a getsockopt(2)
    a bitfield of which instructions are supported and which
    are not, as of right now, we just return 0 to indicate a
    base support for SKF_AD_PROTOCOL up to SKF_AD_PAY_OFFSET.
    Limitations of this approach are that this API which we need
    to maintain for a long time can only support a maximum of 32
    extensions, and needs to be additionally maintained/updated
    when each new extension that comes in.
    
    I thought about this a bit more and what we can do here to
    overcome this is to just return SKF_AD_MAX. Since we never
    remove any extension since we cannot break user space and
    always linearly increase SKF_AD_MAX on each newly added
    extension, user space can make a decision on what extensions
    are supported in the whole set of extensions and which aren't,
    by just checking which of them from the whole set have an
    offset < SKF_AD_MAX of the underlying kernel.
    
    Since SKF_AD_MAX must be updated each time we add new ones,
    we don't need to introduce an additional enum and got
    maintenance for free. At some point in time when
    SO_BPF_EXTENSIONS becomes ubiquitous for most kernels, then
    an application can simply make use of this and easily be run
    on newer or older underlying kernels without needing to be
    recompiled, of course. Since that is for 3.14, it's not too
    late to do this change.
    
    Cc: Michal Sekletar <msekleta@redhat.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Michal Sekletar <msekleta@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1a95a2dcf113..e568c8ef896b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -85,13 +85,7 @@ static inline void bpf_jit_free(struct sk_filter *fp)
 
 static inline int bpf_tell_extensions(void)
 {
-	/* When adding new BPF extension it is necessary to enumerate
-	 * it here, so userspace software which wants to know what is
-	 * supported can do so by inspecting return value of this
-	 * function
-	 */
-
-	return 0;
+	return SKF_AD_MAX;
 }
 
 enum {

commit ea02f9411d9faa3553ed09ce0ec9f00ceae9885e
Author: Michal Sekletar <msekleta@redhat.com>
Date:   Fri Jan 17 17:09:45 2014 +0100

    net: introduce SO_BPF_EXTENSIONS
    
    For user space packet capturing libraries such as libpcap, there's
    currently only one way to check which BPF extensions are supported
    by the kernel, that is, commit aa1113d9f85d ("net: filter: return
    -EINVAL if BPF_S_ANC* operation is not supported"). For querying all
    extensions at once this might be rather inconvenient.
    
    Therefore, this patch introduces a new option which can be used as
    an argument for getsockopt(), and allows one to obtain information
    about which BPF extensions are supported by the current kernel.
    
    As David Miller suggests, we do not need to define any bits right
    now and status quo can just return 0 in order to state that this
    versions supports SKF_AD_PROTOCOL up to SKF_AD_PAY_OFFSET. Later
    additions to BPF extensions need to add their bits to the
    bpf_tell_extensions() function, as documented in the comment.
    
    Signed-off-by: Michal Sekletar <msekleta@redhat.com>
    Cc: David Miller <davem@davemloft.net>
    Reviewed-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ff4e40cd45b1..1a95a2dcf113 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -83,6 +83,17 @@ static inline void bpf_jit_free(struct sk_filter *fp)
 #define SK_RUN_FILTER(FILTER, SKB) sk_run_filter(SKB, FILTER->insns)
 #endif
 
+static inline int bpf_tell_extensions(void)
+{
+	/* When adding new BPF extension it is necessary to enumerate
+	 * it here, so userspace software which wants to know what is
+	 * supported can do so by inspecting return value of this
+	 * function
+	 */
+
+	return 0;
+}
+
 enum {
 	BPF_S_RET_K = 1,
 	BPF_S_RET_A,

commit d45ed4a4e33ae103053c0a53d280014e7101bb5c
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Oct 4 00:14:06 2013 -0700

    net: fix unsafe set_memory_rw from softirq
    
    on x86 system with net.core.bpf_jit_enable = 1
    
    sudo tcpdump -i eth1 'tcp port 22'
    
    causes the warning:
    [   56.766097]  Possible unsafe locking scenario:
    [   56.766097]
    [   56.780146]        CPU0
    [   56.786807]        ----
    [   56.793188]   lock(&(&vb->lock)->rlock);
    [   56.799593]   <Interrupt>
    [   56.805889]     lock(&(&vb->lock)->rlock);
    [   56.812266]
    [   56.812266]  *** DEADLOCK ***
    [   56.812266]
    [   56.830670] 1 lock held by ksoftirqd/1/13:
    [   56.836838]  #0:  (rcu_read_lock){.+.+..}, at: [<ffffffff8118f44c>] vm_unmap_aliases+0x8c/0x380
    [   56.849757]
    [   56.849757] stack backtrace:
    [   56.862194] CPU: 1 PID: 13 Comm: ksoftirqd/1 Not tainted 3.12.0-rc3+ #45
    [   56.868721] Hardware name: System manufacturer System Product Name/P8Z77 WS, BIOS 3007 07/26/2012
    [   56.882004]  ffffffff821944c0 ffff88080bbdb8c8 ffffffff8175a145 0000000000000007
    [   56.895630]  ffff88080bbd5f40 ffff88080bbdb928 ffffffff81755b14 0000000000000001
    [   56.909313]  ffff880800000001 ffff880800000000 ffffffff8101178f 0000000000000001
    [   56.923006] Call Trace:
    [   56.929532]  [<ffffffff8175a145>] dump_stack+0x55/0x76
    [   56.936067]  [<ffffffff81755b14>] print_usage_bug+0x1f7/0x208
    [   56.942445]  [<ffffffff8101178f>] ? save_stack_trace+0x2f/0x50
    [   56.948932]  [<ffffffff810cc0a0>] ? check_usage_backwards+0x150/0x150
    [   56.955470]  [<ffffffff810ccb52>] mark_lock+0x282/0x2c0
    [   56.961945]  [<ffffffff810ccfed>] __lock_acquire+0x45d/0x1d50
    [   56.968474]  [<ffffffff810cce6e>] ? __lock_acquire+0x2de/0x1d50
    [   56.975140]  [<ffffffff81393bf5>] ? cpumask_next_and+0x55/0x90
    [   56.981942]  [<ffffffff810cef72>] lock_acquire+0x92/0x1d0
    [   56.988745]  [<ffffffff8118f52a>] ? vm_unmap_aliases+0x16a/0x380
    [   56.995619]  [<ffffffff817628f1>] _raw_spin_lock+0x41/0x50
    [   57.002493]  [<ffffffff8118f52a>] ? vm_unmap_aliases+0x16a/0x380
    [   57.009447]  [<ffffffff8118f52a>] vm_unmap_aliases+0x16a/0x380
    [   57.016477]  [<ffffffff8118f44c>] ? vm_unmap_aliases+0x8c/0x380
    [   57.023607]  [<ffffffff810436b0>] change_page_attr_set_clr+0xc0/0x460
    [   57.030818]  [<ffffffff810cfb8d>] ? trace_hardirqs_on+0xd/0x10
    [   57.037896]  [<ffffffff811a8330>] ? kmem_cache_free+0xb0/0x2b0
    [   57.044789]  [<ffffffff811b59c3>] ? free_object_rcu+0x93/0xa0
    [   57.051720]  [<ffffffff81043d9f>] set_memory_rw+0x2f/0x40
    [   57.058727]  [<ffffffff8104e17c>] bpf_jit_free+0x2c/0x40
    [   57.065577]  [<ffffffff81642cba>] sk_filter_release_rcu+0x1a/0x30
    [   57.072338]  [<ffffffff811108e2>] rcu_process_callbacks+0x202/0x7c0
    [   57.078962]  [<ffffffff81057f17>] __do_softirq+0xf7/0x3f0
    [   57.085373]  [<ffffffff81058245>] run_ksoftirqd+0x35/0x70
    
    cannot reuse jited filter memory, since it's readonly,
    so use original bpf insns memory to hold work_struct
    
    defer kfree of sk_filter until jit completed freeing
    
    tested on x86_64 and i386
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index a6ac84871d6d..ff4e40cd45b1 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -6,6 +6,7 @@
 
 #include <linux/atomic.h>
 #include <linux/compat.h>
+#include <linux/workqueue.h>
 #include <uapi/linux/filter.h>
 
 #ifdef CONFIG_COMPAT
@@ -25,15 +26,19 @@ struct sk_filter
 {
 	atomic_t		refcnt;
 	unsigned int         	len;	/* Number of filter blocks */
+	struct rcu_head		rcu;
 	unsigned int		(*bpf_func)(const struct sk_buff *skb,
 					    const struct sock_filter *filter);
-	struct rcu_head		rcu;
-	struct sock_filter     	insns[0];
+	union {
+		struct sock_filter     	insns[0];
+		struct work_struct	work;
+	};
 };
 
-static inline unsigned int sk_filter_len(const struct sk_filter *fp)
+static inline unsigned int sk_filter_size(unsigned int proglen)
 {
-	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
+	return max(sizeof(struct sk_filter),
+		   offsetof(struct sk_filter, insns[proglen]));
 }
 
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
@@ -67,11 +72,13 @@ static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 }
 #define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)
 #else
+#include <linux/slab.h>
 static inline void bpf_jit_compile(struct sk_filter *fp)
 {
 }
 static inline void bpf_jit_free(struct sk_filter *fp)
 {
+	kfree(fp);
 }
 #define SK_RUN_FILTER(FILTER, SKB) sk_run_filter(SKB, FILTER->insns)
 #endif

commit d98cae64e4a733ff377184d78aa0b1f2b54faede
Merge: 646093a29f85 4067c666f2dc
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jun 19 16:49:39 2013 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/wireless/ath/ath9k/Kconfig
            drivers/net/xen-netback/netback.c
            net/batman-adv/bat_iv_ogm.c
            net/wireless/nl80211.c
    
    The ath9k Kconfig conflict was a change of a Kconfig option name right
    next to the deletion of another option.
    
    The xen-netback conflict was overlapping changes involving the
    handling of the notify list in xen_netbk_rx_action().
    
    Batman conflict resolution provided by Antonio Quartulli, basically
    keep everything in both conflict hunks.
    
    The nl80211 conflict is a little more involved.  In 'net' we added a
    dynamic memory allocation to nl80211_dump_wiphy() to fix a race that
    Linus reported.  Meanwhile in 'net-next' the handlers were converted
    to use pre and post doit handlers which use a flag to determine
    whether to hold the RTNL mutex around the operation.
    
    However, the dump handlers to not use this logic.  Instead they have
    to explicitly do the locking.  There were apparent bugs in the
    conversion of nl80211_dump_wiphy() in that we were not dropping the
    RTNL mutex in all the return paths, and it seems we very much should
    be doing so.  So I fixed that whilst handling the overlapping changes.
    
    To simplify the initial returns, I take the RTNL mutex after we try
    to allocate 'tb'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ed13998c319b050fc9abdb73915859dfdbe1fb38
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Wed Jun 5 15:30:55 2013 +0200

    sock_diag: fix filter code sent to userspace
    
    Filters need to be translated to real BPF code for userland, like SO_GETFILTER.
    
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c050dcc322a4..f65f5a69db8f 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -46,6 +46,7 @@ extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
+extern void sk_decode_filter(struct sock_filter *filt, struct sock_filter *to);
 
 #ifdef CONFIG_BPF_JIT
 #include <stdarg.h>

commit 164954454a4b1000eb022415654001cceb9259a7
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 17 16:57:37 2013 +0000

    filter: do not output bpf image address for security reason
    
    Do not leak starting address of BPF JIT code for non root users,
    as it might help intruders to perform an attack.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c050dcc322a4..56a6b7fbb3c6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -58,10 +58,10 @@ extern void bpf_jit_free(struct sk_filter *fp);
 static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
 				u32 pass, void *image)
 {
-	pr_err("flen=%u proglen=%u pass=%u image=%p\n",
+	pr_err("flen=%u proglen=%u pass=%u image=%pK\n",
 	       flen, proglen, pass, image);
 	if (image)
-		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_ADDRESS,
+		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_OFFSET,
 			       16, 1, image, proglen, false);
 }
 #define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)

commit 20074f357da4a637430aec2879c9d864c5d2c23c
Author: Xi Wang <xi.wang@gmail.com>
Date:   Wed May 1 16:24:08 2013 -0400

    filter: fix va_list build error
    
    This patch fixes the following build error.
    
    In file included from include/linux/filter.h:52:0,
                     from arch/arm/net/bpf_jit_32.c:14:
    include/linux/printk.h:54:2: error: unknown type name ‘va_list’
    include/linux/printk.h:105:21: error: unknown type name ‘va_list’
    include/linux/printk.h:108:30: error: unknown type name ‘va_list’
    
    Signed-off-by: Xi Wang <xi.wang@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d1248f401a56..c050dcc322a4 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -48,6 +48,7 @@ extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 
 #ifdef CONFIG_BPF_JIT
+#include <stdarg.h>
 #include <linux/linkage.h>
 #include <linux/printk.h>
 

commit a691ce7fe451363d2f1fa48d30c8f4b87c2475d4
Author: Chen Gang <gang.chen@asianux.com>
Date:   Thu Mar 28 15:24:53 2013 +0000

    include/linux: printk is needed in filter.h when CONFIG_BPF_JIT is defined
    
    for make V=1 EXTRA_CFLAGS=-W ARCH=arm allmodconfig
        printk is need when CONFIG_BPF_JIT is defined
        or it will report pr_err and print_hex_dump are implicit declaration
    
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d7d25083130b..d1248f401a56 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -48,6 +48,9 @@ extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 
 #ifdef CONFIG_BPF_JIT
+#include <linux/linkage.h>
+#include <linux/printk.h>
+
 extern void bpf_jit_compile(struct sk_filter *fp);
 extern void bpf_jit_free(struct sk_filter *fp);
 

commit 79617801ea0c0e6664cb497d4c1892c2ff407364
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Thu Mar 21 22:22:03 2013 +0100

    filter: bpf_jit_comp: refactor and unify BPF JIT image dump output
    
    If bpf_jit_enable > 1, then we dump the emitted JIT compiled image
    after creation. Currently, only SPARC and PowerPC has similar output
    as in the reference implementation on x86_64. Make a small helper
    function in order to reduce duplicated code and make the dump output
    uniform across architectures x86_64, SPARC, PPC, ARM (e.g. on ARM
    flen, pass and proglen are currently not shown, but would be
    interesting to know as well), also for future BPF JIT implementations
    on other archs.
    
    Cc: Mircea Gherzan <mgherzan@gmail.com>
    Cc: Matt Evans <matt@ozlabs.org>
    Cc: Eric Dumazet <eric.dumazet@google.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index d2059cb4e465..d7d25083130b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -50,6 +50,16 @@ extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, uns
 #ifdef CONFIG_BPF_JIT
 extern void bpf_jit_compile(struct sk_filter *fp);
 extern void bpf_jit_free(struct sk_filter *fp);
+
+static inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,
+				u32 pass, void *image)
+{
+	pr_err("flen=%u proglen=%u pass=%u image=%p\n",
+	       flen, proglen, pass, image);
+	if (image)
+		print_hex_dump(KERN_ERR, "JIT code: ", DUMP_PREFIX_ADDRESS,
+			       16, 1, image, proglen, false);
+}
 #define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)
 #else
 static inline void bpf_jit_compile(struct sk_filter *fp)

commit 3e5289d5e3f98b7b5b8cac32e9e5a7004c067436
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Tue Mar 19 06:39:31 2013 +0000

    filter: add ANC_PAY_OFFSET instruction for loading payload start offset
    
    It is very useful to do dynamic truncation of packets. In particular,
    we're interested to push the necessary header bytes to the user space and
    cut off user payload that should probably not be transferred for some reasons
    (e.g. privacy, speed, or others). With the ancillary extension PAY_OFFSET,
    we can load it into the accumulator, and return it. E.g. in bpfc syntax ...
    
            ld #poff        ; { 0x20, 0, 0, 0xfffff034 },
            ret a           ; { 0x16, 0, 0, 0x00000000 },
    
    ... as a filter will accomplish this without having to do a big hackery in
    a BPF filter itself. Follow-up JIT implementations are welcome.
    
    Thanks to Eric Dumazet for suggesting and discussing this during the
    Netfilter Workshop in Copenhagen.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c45eabc135e1..d2059cb4e465 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -126,6 +126,7 @@ enum {
 	BPF_S_ANC_SECCOMP_LD_W,
 	BPF_S_ANC_VLAN_TAG,
 	BPF_S_ANC_VLAN_TAG_PRESENT,
+	BPF_S_ANC_PAY_OFFSET,
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit a8fc92778080c845eaadc369a0ecf5699a03bef0
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Nov 1 02:01:48 2012 +0000

    sk-filter: Add ability to get socket filter program (v2)
    
    The SO_ATTACH_FILTER option is set only. I propose to add the get
    ability by using SO_ATTACH_FILTER in getsockopt. To be less
    irritating to eyes the SO_GET_FILTER alias to it is declared. This
    ability is required by checkpoint-restore project to be able to
    save full state of a socket.
    
    There are two issues with getting filter back.
    
    First, kernel modifies the sock_filter->code on filter load, thus in
    order to return the filter element back to user we have to decode it
    into user-visible constants. Fortunately the modification in question
    is interconvertible.
    
    Second, the BPF_S_ALU_DIV_K code modifies the command argument k to
    speed up the run-time division by doing kernel_k = reciprocal(user_k).
    Bad news is that different user_k may result in same kernel_k, so we
    can't get the original user_k back. Good news is that we don't have
    to do it. What we need to is calculate a user2_k so, that
    
      reciprocal(user2_k) == reciprocal(user_k) == kernel_k
    
    i.e. if it's re-loaded back the compiled again value will be exactly
    the same as it was. That said, the user2_k can be calculated like this
    
      user2_k = reciprocal(kernel_k)
    
    with an exception, that if kernel_k == 0, then user2_k == 1.
    
    The optlen argument is treated like this -- when zero, kernel returns
    the amount of sock_fprog elements in filter, otherwise it should be
    large enough for the sock_fprog array.
    
    changes since v1:
    * Declared SO_GET_FILTER in all arch headers
    * Added decode of vlan-tag codes
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c9f0005c35e2..c45eabc135e1 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -45,6 +45,7 @@ extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
+extern int sk_get_filter(struct sock *sk, struct sock_filter __user *filter, unsigned len);
 
 #ifdef CONFIG_BPF_JIT
 extern void bpf_jit_compile(struct sk_filter *fp);

commit f3335031b9452baebfe49b8b5e55d3fe0c4677d1
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Oct 27 02:26:17 2012 +0000

    net: filter: add vlan tag access
    
    BPF filters lack ability to access skb->vlan_tci
    
    This patch adds two new ancillary accessors :
    
    SKF_AD_VLAN_TAG         (44) mapped to vlan_tx_tag_get(skb)
    
    SKF_AD_VLAN_TAG_PRESENT (48) mapped to vlan_tx_tag_present(skb)
    
    This allows libpcap/tcpdump to use a kernel filter instead of
    having to fallback to accept all packets, then filter them in
    user space.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Suggested-by: Ani Sinha <ani@aristanetworks.com>
    Suggested-by: Daniel Borkmann <danborkmann@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 24d251f3bab0..c9f0005c35e2 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -123,6 +123,8 @@ enum {
 	BPF_S_ANC_CPU,
 	BPF_S_ANC_ALU_XOR_X,
 	BPF_S_ANC_SECCOMP_LD_W,
+	BPF_S_ANC_VLAN_TAG,
+	BPF_S_ANC_VLAN_TAG_PRESENT,
 };
 
 #endif /* __LINUX_FILTER_H__ */

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 2ded090e10f4..24d251f3bab0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1,141 +1,12 @@
 /*
  * Linux Socket Filter Data Structures
  */
-
 #ifndef __LINUX_FILTER_H__
 #define __LINUX_FILTER_H__
 
-#include <linux/compiler.h>
-#include <linux/types.h>
-
-#ifdef __KERNEL__
 #include <linux/atomic.h>
 #include <linux/compat.h>
-#endif
-
-/*
- * Current version of the filter code architecture.
- */
-#define BPF_MAJOR_VERSION 1
-#define BPF_MINOR_VERSION 1
-
-/*
- *	Try and keep these values and structures similar to BSD, especially
- *	the BPF code definitions which need to match so you can share filters
- */
- 
-struct sock_filter {	/* Filter block */
-	__u16	code;   /* Actual filter code */
-	__u8	jt;	/* Jump true */
-	__u8	jf;	/* Jump false */
-	__u32	k;      /* Generic multiuse field */
-};
-
-struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
-	unsigned short		len;	/* Number of filter blocks */
-	struct sock_filter __user *filter;
-};
-
-/*
- * Instruction classes
- */
-
-#define BPF_CLASS(code) ((code) & 0x07)
-#define         BPF_LD          0x00
-#define         BPF_LDX         0x01
-#define         BPF_ST          0x02
-#define         BPF_STX         0x03
-#define         BPF_ALU         0x04
-#define         BPF_JMP         0x05
-#define         BPF_RET         0x06
-#define         BPF_MISC        0x07
-
-/* ld/ldx fields */
-#define BPF_SIZE(code)  ((code) & 0x18)
-#define         BPF_W           0x00
-#define         BPF_H           0x08
-#define         BPF_B           0x10
-#define BPF_MODE(code)  ((code) & 0xe0)
-#define         BPF_IMM         0x00
-#define         BPF_ABS         0x20
-#define         BPF_IND         0x40
-#define         BPF_MEM         0x60
-#define         BPF_LEN         0x80
-#define         BPF_MSH         0xa0
-
-/* alu/jmp fields */
-#define BPF_OP(code)    ((code) & 0xf0)
-#define         BPF_ADD         0x00
-#define         BPF_SUB         0x10
-#define         BPF_MUL         0x20
-#define         BPF_DIV         0x30
-#define         BPF_OR          0x40
-#define         BPF_AND         0x50
-#define         BPF_LSH         0x60
-#define         BPF_RSH         0x70
-#define         BPF_NEG         0x80
-#define		BPF_MOD		0x90
-#define		BPF_XOR		0xa0
-
-#define         BPF_JA          0x00
-#define         BPF_JEQ         0x10
-#define         BPF_JGT         0x20
-#define         BPF_JGE         0x30
-#define         BPF_JSET        0x40
-#define BPF_SRC(code)   ((code) & 0x08)
-#define         BPF_K           0x00
-#define         BPF_X           0x08
-
-/* ret - BPF_K and BPF_X also apply */
-#define BPF_RVAL(code)  ((code) & 0x18)
-#define         BPF_A           0x10
-
-/* misc */
-#define BPF_MISCOP(code) ((code) & 0xf8)
-#define         BPF_TAX         0x00
-#define         BPF_TXA         0x80
-
-#ifndef BPF_MAXINSNS
-#define BPF_MAXINSNS 4096
-#endif
-
-/*
- * Macros for filter block array initializers.
- */
-#ifndef BPF_STMT
-#define BPF_STMT(code, k) { (unsigned short)(code), 0, 0, k }
-#endif
-#ifndef BPF_JUMP
-#define BPF_JUMP(code, k, jt, jf) { (unsigned short)(code), jt, jf, k }
-#endif
-
-/*
- * Number of scratch memory words for: BPF_ST and BPF_STX
- */
-#define BPF_MEMWORDS 16
-
-/* RATIONALE. Negative offsets are invalid in BPF.
-   We use them to reference ancillary data.
-   Unlike introduction new instructions, it does not break
-   existing compilers/optimizers.
- */
-#define SKF_AD_OFF    (-0x1000)
-#define SKF_AD_PROTOCOL 0
-#define SKF_AD_PKTTYPE 	4
-#define SKF_AD_IFINDEX 	8
-#define SKF_AD_NLATTR	12
-#define SKF_AD_NLATTR_NEST	16
-#define SKF_AD_MARK 	20
-#define SKF_AD_QUEUE	24
-#define SKF_AD_HATYPE	28
-#define SKF_AD_RXHASH	32
-#define SKF_AD_CPU	36
-#define SKF_AD_ALU_XOR_X	40
-#define SKF_AD_MAX	44
-#define SKF_NET_OFF   (-0x100000)
-#define SKF_LL_OFF    (-0x200000)
-
-#ifdef __KERNEL__
+#include <uapi/linux/filter.h>
 
 #ifdef CONFIG_COMPAT
 /*
@@ -254,6 +125,4 @@ enum {
 	BPF_S_ANC_SECCOMP_LD_W,
 };
 
-#endif /* __KERNEL__ */
-
 #endif /* __LINUX_FILTER_H__ */

commit 9e49e88958feb41ec701fa34b44723dabadbc28c
Author: Daniel Borkmann <dxchgb@gmail.com>
Date:   Mon Sep 24 02:23:59 2012 +0000

    filter: add XOR instruction for use with X/K
    
    SKF_AD_ALU_XOR_X has been added a while ago, but as an 'ancillary'
    operation that is invoked through a negative offset in K within BPF
    load operations. Since BPF_MOD has recently been added, BPF_XOR should
    also be part of the common ALU operations. Removing SKF_AD_ALU_XOR_X
    might not be an option since this is exposed to user space.
    
    Signed-off-by: Daniel Borkmann <daniel.borkmann@tik.ee.ethz.ch>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3cf5fd561d86..2ded090e10f4 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -75,6 +75,7 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define         BPF_RSH         0x70
 #define         BPF_NEG         0x80
 #define		BPF_MOD		0x90
+#define		BPF_XOR		0xa0
 
 #define         BPF_JA          0x00
 #define         BPF_JEQ         0x10
@@ -204,6 +205,8 @@ enum {
 	BPF_S_ALU_AND_X,
 	BPF_S_ALU_OR_K,
 	BPF_S_ALU_OR_X,
+	BPF_S_ALU_XOR_K,
+	BPF_S_ALU_XOR_X,
 	BPF_S_ALU_LSH_K,
 	BPF_S_ALU_LSH_X,
 	BPF_S_ALU_RSH_K,

commit b6069a95706ca5738be3f5d90fd286cbd13ac695
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 7 22:03:35 2012 +0000

    filter: add MOD operation
    
    Add a new ALU opcode, to compute a modulus.
    
    Commit ffe06c17afbbb used an ancillary to implement XOR_X,
    but here we reserve one of the available ALU opcode to implement both
    MOD_X and MOD_K
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Suggested-by: George Bakos <gbakos@alpinista.org>
    Cc: Jay Schulist <jschlst@samba.org>
    Cc: Jiri Pirko <jpirko@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 82b01357af8b..3cf5fd561d86 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -74,6 +74,8 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define         BPF_LSH         0x60
 #define         BPF_RSH         0x70
 #define         BPF_NEG         0x80
+#define		BPF_MOD		0x90
+
 #define         BPF_JA          0x00
 #define         BPF_JEQ         0x10
 #define         BPF_JGT         0x20
@@ -196,6 +198,8 @@ enum {
 	BPF_S_ALU_MUL_K,
 	BPF_S_ALU_MUL_X,
 	BPF_S_ALU_DIV_X,
+	BPF_S_ALU_MOD_K,
+	BPF_S_ALU_MOD_X,
 	BPF_S_ALU_AND_K,
 	BPF_S_ALU_AND_X,
 	BPF_S_ALU_OR_K,

commit cb60e3e65c1b96a4d6444a7a13dc7dd48bc15a2b
Merge: 99262a3dafa3 ff2bb047c4bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 21 20:27:36 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "New notable features:
       - The seccomp work from Will Drewry
       - PR_{GET,SET}_NO_NEW_PRIVS from Andy Lutomirski
       - Longer security labels for Smack from Casey Schaufler
       - Additional ptrace restriction modes for Yama by Kees Cook"
    
    Fix up trivial context conflicts in arch/x86/Kconfig and include/linux/filter.h
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (65 commits)
      apparmor: fix long path failure due to disconnected path
      apparmor: fix profile lookup for unconfined
      ima: fix filename hint to reflect script interpreter name
      KEYS: Don't check for NULL key pointer in key_validate()
      Smack: allow for significantly longer Smack labels v4
      gfp flags for security_inode_alloc()?
      Smack: recursive tramsmute
      Yama: replace capable() with ns_capable()
      TOMOYO: Accept manager programs which do not start with / .
      KEYS: Add invalidation support
      KEYS: Do LRU discard in full keyrings
      KEYS: Permit in-place link replacement in keyring list
      KEYS: Perform RCU synchronisation on keys prior to key destruction
      KEYS: Announce key type (un)registration
      KEYS: Reorganise keys Makefile
      KEYS: Move the key config into security/keys/Kconfig
      KEYS: Use the compat keyctl() syscall wrapper on Sparc64 for Sparc32 compat
      Yama: remove an unused variable
      samples/seccomp: fix dependencies on arch macros
      Yama: add additional ptrace scopes
      ...

commit 0c5fe1b4221c6701224c2601cf3c692e5721103e
Author: Will Drewry <wad@chromium.org>
Date:   Thu Apr 12 16:47:53 2012 -0500

    net/compat.c,linux/filter.h: share compat_sock_fprog
    
    Any other users of bpf_*_filter that take a struct sock_fprog from
    userspace will need to be able to also accept a compat_sock_fprog
    if the arch supports compat calls.  This change allows the existing
    compat_sock_fprog be shared.
    
    Signed-off-by: Will Drewry <wad@chromium.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Eric Paris <eparis@redhat.com>
    
    v18: tasered by the apostrophe police
    v14: rebase/nochanges
    v13: rebase on to 88ebdda6159ffc15699f204c33feb3e431bf9bdc
    v12: rebase on to linux-next
    v11: introduction
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index aaa2e80630b8..f2e53152e835 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -10,6 +10,7 @@
 
 #ifdef __KERNEL__
 #include <linux/atomic.h>
+#include <linux/compat.h>
 #endif
 
 /*
@@ -132,6 +133,16 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 
 #ifdef __KERNEL__
 
+#ifdef CONFIG_COMPAT
+/*
+ * A struct sock_filter is architecture independent.
+ */
+struct compat_sock_fprog {
+	u16		len;
+	compat_uptr_t	filter;		/* struct sock_filter * */
+};
+#endif
+
 struct sk_buff;
 struct sock;
 

commit 46b325c7eb01482674406701825ff67f561ccdd4
Author: Will Drewry <wad@chromium.org>
Date:   Thu Apr 12 16:47:52 2012 -0500

    sk_run_filter: add BPF_S_ANC_SECCOMP_LD_W
    
    Introduces a new BPF ancillary instruction that all LD calls will be
    mapped through when skb_run_filter() is being used for seccomp BPF.  The
    rewriting will be done using a secondary chk_filter function that is run
    after skb_chk_filter.
    
    The code change is guarded by CONFIG_SECCOMP_FILTER which is added,
    along with the seccomp_bpf_load() function later in this series.
    
    This is based on http://lkml.org/lkml/2012/3/2/141
    
    Suggested-by: Indan Zupancic <indan@nul.nu>
    Signed-off-by: Will Drewry <wad@chromium.org>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Eric Paris <eparis@redhat.com>
    
    v18: rebase
    ...
    v15: include seccomp.h explicitly for when seccomp_bpf_load exists.
    v14: First cut using a single additional instruction
    ... v13: made bpf functions generic.
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 8eeb205f298b..aaa2e80630b8 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -228,6 +228,7 @@ enum {
 	BPF_S_ANC_HATYPE,
 	BPF_S_ANC_RXHASH,
 	BPF_S_ANC_CPU,
+	BPF_S_ANC_SECCOMP_LD_W,
 };
 
 #endif /* __KERNEL__ */

commit ffe06c17afbbbd4d73cdc339419be232847d667a
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Sat Mar 31 11:01:20 2012 +0000

    filter: add XOR operation
    
    Add XOR instruction fo BPF machine. Needed for computing packet hashes.
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 92dd9933c43d..72090994d789 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -126,7 +126,8 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_HATYPE	28
 #define SKF_AD_RXHASH	32
 #define SKF_AD_CPU	36
-#define SKF_AD_MAX	40
+#define SKF_AD_ALU_XOR_X	40
+#define SKF_AD_MAX	44
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 
@@ -231,6 +232,7 @@ enum {
 	BPF_S_ANC_HATYPE,
 	BPF_S_ANC_RXHASH,
 	BPF_S_ANC_CPU,
+	BPF_S_ANC_ALU_XOR_X,
 };
 
 #endif /* __KERNEL__ */

commit 302d663740cfaf2c364df6bb61cd339014ed714c
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Sat Mar 31 11:01:19 2012 +0000

    filter: Allow to create sk-unattached filters
    
    Today, BPF filters are bind to sockets. Since BPF machine becomes handy
    for other purposes, this patch allows to create unattached filter.
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 8eeb205f298b..92dd9933c43d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -153,6 +153,9 @@ static inline unsigned int sk_filter_len(const struct sk_filter *fp)
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
 extern unsigned int sk_run_filter(const struct sk_buff *skb,
 				  const struct sock_filter *filter);
+extern int sk_unattached_filter_create(struct sk_filter **pfp,
+				       struct sock_fprog *fprog);
+extern void sk_unattached_filter_destroy(struct sk_filter *fp);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);

commit 4f25af27827080c3163e59c7af1ca84a05ce121c
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Mon Oct 17 21:04:20 2011 +0000

    filter: use unsigned int to silence static checker warning
    
    This is just a cleanup.
    
    My testing version of Smatch warns about this:
    net/core/filter.c +380 check_load_and_stores(6)
            warn: check 'flen' for negative values
    
    flen comes from the user.  We try to clamp the values here between 1
    and BPF_MAXINSNS but the clamp doesn't work because it could be
    negative.  This is a bug, but it's not exploitable.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 741956fa5bfd..8eeb205f298b 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -155,7 +155,7 @@ extern unsigned int sk_run_filter(const struct sk_buff *skb,
 				  const struct sock_filter *filter);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
-extern int sk_chk_filter(struct sock_filter *filter, int flen);
+extern int sk_chk_filter(struct sock_filter *filter, unsigned int flen);
 
 #ifdef CONFIG_BPF_JIT
 extern void bpf_jit_compile(struct sk_filter *fp);

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 9ee3f9fb0b4a..741956fa5bfd 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -9,7 +9,7 @@
 #include <linux/types.h>
 
 #ifdef __KERNEL__
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #endif
 
 /*

commit 792d4b5cb16b684958c2590f77688667ddec1f61
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sun May 22 07:08:11 2011 +0000

    net: filter: move forward declarations to avoid compile warnings
    
    Get rid of this compile warning:
    
    In file included from arch/s390/kernel/compat_linux.c:37:0:
    include/linux/filter.h:139:23: warning: 'struct sk_buff' declared inside parameter list
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 4609b85e559d..9ee3f9fb0b4a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -131,6 +131,10 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define SKF_LL_OFF    (-0x200000)
 
 #ifdef __KERNEL__
+
+struct sk_buff;
+struct sock;
+
 struct sk_filter
 {
 	atomic_t		refcnt;
@@ -146,9 +150,6 @@ static inline unsigned int sk_filter_len(const struct sk_filter *fp)
 	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
 }
 
-struct sk_buff;
-struct sock;
-
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
 extern unsigned int sk_run_filter(const struct sk_buff *skb,
 				  const struct sock_filter *filter);

commit 0a14842f5a3c0e88a1e59fac5c3025db39721f74
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Apr 20 09:27:32 2011 +0000

    net: filter: Just In Time compiler for x86-64
    
    In order to speedup packet filtering, here is an implementation of a
    JIT compiler for x86_64
    
    It is disabled by default, and must be enabled by the admin.
    
    echo 1 >/proc/sys/net/core/bpf_jit_enable
    
    It uses module_alloc() and module_free() to get memory in the 2GB text
    kernel range since we call helpers functions from the generated code.
    
    EAX : BPF A accumulator
    EBX : BPF X accumulator
    RDI : pointer to skb   (first argument given to JIT function)
    RBP : frame pointer (even if CONFIG_FRAME_POINTER=n)
    r9d : skb->len - skb->data_len (headlen)
    r8  : skb->data
    
    To get a trace of generated code, use :
    
    echo 2 >/proc/sys/net/core/bpf_jit_enable
    
    Example of generated code :
    
    # tcpdump -p -n -s 0 -i eth1 host 192.168.20.0/24
    
    flen=18 proglen=147 pass=3 image=ffffffffa00b5000
    JIT code: ffffffffa00b5000: 55 48 89 e5 48 83 ec 60 48 89 5d f8 44 8b 4f 60
    JIT code: ffffffffa00b5010: 44 2b 4f 64 4c 8b 87 b8 00 00 00 be 0c 00 00 00
    JIT code: ffffffffa00b5020: e8 24 7b f7 e0 3d 00 08 00 00 75 28 be 1a 00 00
    JIT code: ffffffffa00b5030: 00 e8 fe 7a f7 e0 24 00 3d 00 14 a8 c0 74 49 be
    JIT code: ffffffffa00b5040: 1e 00 00 00 e8 eb 7a f7 e0 24 00 3d 00 14 a8 c0
    JIT code: ffffffffa00b5050: 74 36 eb 3b 3d 06 08 00 00 74 07 3d 35 80 00 00
    JIT code: ffffffffa00b5060: 75 2d be 1c 00 00 00 e8 c8 7a f7 e0 24 00 3d 00
    JIT code: ffffffffa00b5070: 14 a8 c0 74 13 be 26 00 00 00 e8 b5 7a f7 e0 24
    JIT code: ffffffffa00b5080: 00 3d 00 14 a8 c0 75 07 b8 ff ff 00 00 eb 02 31
    JIT code: ffffffffa00b5090: c0 c9 c3
    
    BPF program is 144 bytes long, so native program is almost same size ;)
    
    (000) ldh      [12]
    (001) jeq      #0x800           jt 2    jf 8
    (002) ld       [26]
    (003) and      #0xffffff00
    (004) jeq      #0xc0a81400      jt 16   jf 5
    (005) ld       [30]
    (006) and      #0xffffff00
    (007) jeq      #0xc0a81400      jt 16   jf 17
    (008) jeq      #0x806           jt 10   jf 9
    (009) jeq      #0x8035          jt 10   jf 17
    (010) ld       [28]
    (011) and      #0xffffff00
    (012) jeq      #0xc0a81400      jt 16   jf 13
    (013) ld       [38]
    (014) and      #0xffffff00
    (015) jeq      #0xc0a81400      jt 16   jf 17
    (016) ret      #65535
    (017) ret      #0
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Hagen Paul Pfeifer <hagen@jauu.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 45266b75409a..4609b85e559d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -135,6 +135,8 @@ struct sk_filter
 {
 	atomic_t		refcnt;
 	unsigned int         	len;	/* Number of filter blocks */
+	unsigned int		(*bpf_func)(const struct sk_buff *skb,
+					    const struct sock_filter *filter);
 	struct rcu_head		rcu;
 	struct sock_filter     	insns[0];
 };
@@ -153,6 +155,80 @@ extern unsigned int sk_run_filter(const struct sk_buff *skb,
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, int flen);
+
+#ifdef CONFIG_BPF_JIT
+extern void bpf_jit_compile(struct sk_filter *fp);
+extern void bpf_jit_free(struct sk_filter *fp);
+#define SK_RUN_FILTER(FILTER, SKB) (*FILTER->bpf_func)(SKB, FILTER->insns)
+#else
+static inline void bpf_jit_compile(struct sk_filter *fp)
+{
+}
+static inline void bpf_jit_free(struct sk_filter *fp)
+{
+}
+#define SK_RUN_FILTER(FILTER, SKB) sk_run_filter(SKB, FILTER->insns)
+#endif
+
+enum {
+	BPF_S_RET_K = 1,
+	BPF_S_RET_A,
+	BPF_S_ALU_ADD_K,
+	BPF_S_ALU_ADD_X,
+	BPF_S_ALU_SUB_K,
+	BPF_S_ALU_SUB_X,
+	BPF_S_ALU_MUL_K,
+	BPF_S_ALU_MUL_X,
+	BPF_S_ALU_DIV_X,
+	BPF_S_ALU_AND_K,
+	BPF_S_ALU_AND_X,
+	BPF_S_ALU_OR_K,
+	BPF_S_ALU_OR_X,
+	BPF_S_ALU_LSH_K,
+	BPF_S_ALU_LSH_X,
+	BPF_S_ALU_RSH_K,
+	BPF_S_ALU_RSH_X,
+	BPF_S_ALU_NEG,
+	BPF_S_LD_W_ABS,
+	BPF_S_LD_H_ABS,
+	BPF_S_LD_B_ABS,
+	BPF_S_LD_W_LEN,
+	BPF_S_LD_W_IND,
+	BPF_S_LD_H_IND,
+	BPF_S_LD_B_IND,
+	BPF_S_LD_IMM,
+	BPF_S_LDX_W_LEN,
+	BPF_S_LDX_B_MSH,
+	BPF_S_LDX_IMM,
+	BPF_S_MISC_TAX,
+	BPF_S_MISC_TXA,
+	BPF_S_ALU_DIV_K,
+	BPF_S_LD_MEM,
+	BPF_S_LDX_MEM,
+	BPF_S_ST,
+	BPF_S_STX,
+	BPF_S_JMP_JA,
+	BPF_S_JMP_JEQ_K,
+	BPF_S_JMP_JEQ_X,
+	BPF_S_JMP_JGE_K,
+	BPF_S_JMP_JGE_X,
+	BPF_S_JMP_JGT_K,
+	BPF_S_JMP_JGT_X,
+	BPF_S_JMP_JSET_K,
+	BPF_S_JMP_JSET_X,
+	/* Ancillary data */
+	BPF_S_ANC_PROTOCOL,
+	BPF_S_ANC_PKTTYPE,
+	BPF_S_ANC_IFINDEX,
+	BPF_S_ANC_NLATTR,
+	BPF_S_ANC_NLATTR_NEST,
+	BPF_S_ANC_MARK,
+	BPF_S_ANC_QUEUE,
+	BPF_S_ANC_HATYPE,
+	BPF_S_ANC_RXHASH,
+	BPF_S_ANC_CPU,
+};
+
 #endif /* __KERNEL__ */
 
 #endif /* __LINUX_FILTER_H__ */

commit 62ab0812137ec4f9884dd7de346238841ac03283
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Dec 6 20:50:09 2010 +0000

    filter: constify sk_run_filter()
    
    sk_run_filter() doesnt write on skb, change its prototype to reflect
    this.
    
    Fix two af_packet comments.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 5334adaf4072..45266b75409a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -148,7 +148,7 @@ struct sk_buff;
 struct sock;
 
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
-extern unsigned int sk_run_filter(struct sk_buff *skb,
+extern unsigned int sk_run_filter(const struct sk_buff *skb,
 				  const struct sock_filter *filter);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);

commit da2033c282264bfba4e339b7cb3df62adb5c5fc8
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 30 21:45:56 2010 +0000

    filter: add SKF_AD_RXHASH and SKF_AD_CPU
    
    Add SKF_AD_RXHASH and SKF_AD_CPU to filter ancillary mechanism,
    to be able to build advanced filters.
    
    This can help spreading packets on several sockets with a fast
    selection, after RPS dispatch to N cpus for example, or to catch a
    percentage of flows in one queue.
    
    tcpdump -s 500 "cpu = 1" :
    
    [0] ld CPU
    [1] jeq #1  jt 2  jf 3
    [2] ret #500
    [3] ret #0
    
    # take 12.5 % of flows (average)
    tcpdump -s 1000 "rxhash & 7 = 2" :
    
    [0] ld RXHASH
    [1] and #7
    [2] jeq #2  jt 3  jf 4
    [3] ret #1000
    [4] ret #0
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Rui <wirelesser@gmail.com>
    Acked-by: Changli Gao <xiaosuo@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 447a775878fb..5334adaf4072 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -124,7 +124,9 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_MARK 	20
 #define SKF_AD_QUEUE	24
 #define SKF_AD_HATYPE	28
-#define SKF_AD_MAX	32
+#define SKF_AD_RXHASH	32
+#define SKF_AD_CPU	36
+#define SKF_AD_MAX	40
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit 93aaae2e01e57483256b7da05c9a7ebd65ad4686
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Nov 19 09:49:59 2010 -0800

    filter: optimize sk_run_filter
    
    Remove pc variable to avoid arithmetic to compute fentry at each filter
    instruction. Jumps directly manipulate fentry pointer.
    
    As the last instruction of filter[] is guaranteed to be a RETURN, and
    all jumps are before the last instruction, we dont need to check filter
    bounds (number of instructions in filter array) at each iteration, so we
    remove it from sk_run_filter() params.
    
    On x86_32 remove f_k var introduced in commit 57fe93b374a6b871
    (filter: make sure filters dont read uninitialized memory)
    
    Note : We could use a CONFIG_ARCH_HAS_{FEW|MANY}_REGISTERS in order to
    avoid too many ifdefs in this code.
    
    This helps compiler to use cpu registers to hold fentry and A
    accumulator.
    
    On x86_32, this saves 401 bytes, and more important, sk_run_filter()
    runs much faster because less register pressure (One less conditional
    branch per BPF instruction)
    
    # size net/core/filter.o net/core/filter_pre.o
       text    data     bss     dec     hex filename
       2948       0       0    2948     b84 net/core/filter.o
       3349       0       0    3349     d15 net/core/filter_pre.o
    
    on x86_64 :
    # size net/core/filter.o net/core/filter_pre.o
       text    data     bss     dec     hex filename
       5173       0       0    5173    1435 net/core/filter.o
       5224       0       0    5224    1468 net/core/filter_pre.o
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Changli Gao <xiaosuo@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 151f5d703b7e..447a775878fb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -147,7 +147,7 @@ struct sock;
 
 extern int sk_filter(struct sock *sk, struct sk_buff *skb);
 extern unsigned int sk_run_filter(struct sk_buff *skb,
-				  struct sock_filter *filter, int flen);
+				  const struct sock_filter *filter);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, int flen);

commit 4c3710afbc333c33100739dec10662b4ee64e219
Author: Changli Gao <xiaosuo@gmail.com>
Date:   Tue Nov 16 20:28:24 2010 +0000

    net: move definitions of BPF_S_* to net/core/filter.c
    
    BPF_S_* are used internally, should not be exposed to the others.
    
    Signed-off-by: Changli Gao <xiaosuo@gmail.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Hagen Paul Pfeifer <hagen@jauu.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 69b43dbea6c6..151f5d703b7e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -91,54 +91,6 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define         BPF_TAX         0x00
 #define         BPF_TXA         0x80
 
-enum {
-	BPF_S_RET_K = 0,
-	BPF_S_RET_A,
-	BPF_S_ALU_ADD_K,
-	BPF_S_ALU_ADD_X,
-	BPF_S_ALU_SUB_K,
-	BPF_S_ALU_SUB_X,
-	BPF_S_ALU_MUL_K,
-	BPF_S_ALU_MUL_X,
-	BPF_S_ALU_DIV_X,
-	BPF_S_ALU_AND_K,
-	BPF_S_ALU_AND_X,
-	BPF_S_ALU_OR_K,
-	BPF_S_ALU_OR_X,
-	BPF_S_ALU_LSH_K,
-	BPF_S_ALU_LSH_X,
-	BPF_S_ALU_RSH_K,
-	BPF_S_ALU_RSH_X,
-	BPF_S_ALU_NEG,
-	BPF_S_LD_W_ABS,
-	BPF_S_LD_H_ABS,
-	BPF_S_LD_B_ABS,
-	BPF_S_LD_W_LEN,
-	BPF_S_LD_W_IND,
-	BPF_S_LD_H_IND,
-	BPF_S_LD_B_IND,
-	BPF_S_LD_IMM,
-	BPF_S_LDX_W_LEN,
-	BPF_S_LDX_B_MSH,
-	BPF_S_LDX_IMM,
-	BPF_S_MISC_TAX,
-	BPF_S_MISC_TXA,
-	BPF_S_ALU_DIV_K,
-	BPF_S_LD_MEM,
-	BPF_S_LDX_MEM,
-	BPF_S_ST,
-	BPF_S_STX,
-	BPF_S_JMP_JA,
-	BPF_S_JMP_JEQ_K,
-	BPF_S_JMP_JEQ_X,
-	BPF_S_JMP_JGE_K,
-	BPF_S_JMP_JGE_X,
-	BPF_S_JMP_JGT_K,
-	BPF_S_JMP_JGT_X,
-	BPF_S_JMP_JSET_K,
-	BPF_S_JMP_JSET_X,
-};
-
 #ifndef BPF_MAXINSNS
 #define BPF_MAXINSNS 4096
 #endif

commit 01f2f3f6ef4d076c0c10a8a7b42624416d56b523
Author: Hagen Paul Pfeifer <hagen@jauu.net>
Date:   Sat Jun 19 17:05:36 2010 +0000

    net: optimize Berkeley Packet Filter (BPF) processing
    
    Gcc is currenlty not in the ability to optimize the switch statement in
    sk_run_filter() because of dense case labels. This patch replace the
    OR'd labels with ordered sequenced case labels. The sk_chk_filter()
    function is modified to patch/replace the original OPCODES in a
    ordered but equivalent form. gcc is now in the ability to transform the
    switch statement in sk_run_filter into a jump table of complexity O(1).
    
    Until this patch gcc generates a sequence of conditional branches (O(n) of 567
    byte .text segment size (arch x86_64):
    
    7ff: 8b 06                 mov    (%rsi),%eax
    801: 66 83 f8 35           cmp    $0x35,%ax
    805: 0f 84 d0 02 00 00     je     adb <sk_run_filter+0x31d>
    80b: 0f 87 07 01 00 00     ja     918 <sk_run_filter+0x15a>
    811: 66 83 f8 15           cmp    $0x15,%ax
    815: 0f 84 c5 02 00 00     je     ae0 <sk_run_filter+0x322>
    81b: 77 73                 ja     890 <sk_run_filter+0xd2>
    81d: 66 83 f8 04           cmp    $0x4,%ax
    821: 0f 84 17 02 00 00     je     a3e <sk_run_filter+0x280>
    827: 77 29                 ja     852 <sk_run_filter+0x94>
    829: 66 83 f8 01           cmp    $0x1,%ax
    [...]
    
    With the modification the compiler translate the switch statement into
    the following jump table fragment:
    
    7ff: 66 83 3e 2c           cmpw   $0x2c,(%rsi)
    803: 0f 87 1f 02 00 00     ja     a28 <sk_run_filter+0x26a>
    809: 0f b7 06              movzwl (%rsi),%eax
    80c: ff 24 c5 00 00 00 00  jmpq   *0x0(,%rax,8)
    813: 44 89 e3              mov    %r12d,%ebx
    816: e9 43 03 00 00        jmpq   b5e <sk_run_filter+0x3a0>
    81b: 41 89 dc              mov    %ebx,%r12d
    81e: e9 3b 03 00 00        jmpq   b5e <sk_run_filter+0x3a0>
    
    Furthermore, I reordered the instructions to reduce cache line misses by
    order the most common instruction to the start.
    
    Signed-off-by: Hagen Paul Pfeifer <hagen@jauu.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 151f5d703b7e..69b43dbea6c6 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -91,6 +91,54 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define         BPF_TAX         0x00
 #define         BPF_TXA         0x80
 
+enum {
+	BPF_S_RET_K = 0,
+	BPF_S_RET_A,
+	BPF_S_ALU_ADD_K,
+	BPF_S_ALU_ADD_X,
+	BPF_S_ALU_SUB_K,
+	BPF_S_ALU_SUB_X,
+	BPF_S_ALU_MUL_K,
+	BPF_S_ALU_MUL_X,
+	BPF_S_ALU_DIV_X,
+	BPF_S_ALU_AND_K,
+	BPF_S_ALU_AND_X,
+	BPF_S_ALU_OR_K,
+	BPF_S_ALU_OR_X,
+	BPF_S_ALU_LSH_K,
+	BPF_S_ALU_LSH_X,
+	BPF_S_ALU_RSH_K,
+	BPF_S_ALU_RSH_X,
+	BPF_S_ALU_NEG,
+	BPF_S_LD_W_ABS,
+	BPF_S_LD_H_ABS,
+	BPF_S_LD_B_ABS,
+	BPF_S_LD_W_LEN,
+	BPF_S_LD_W_IND,
+	BPF_S_LD_H_IND,
+	BPF_S_LD_B_IND,
+	BPF_S_LD_IMM,
+	BPF_S_LDX_W_LEN,
+	BPF_S_LDX_B_MSH,
+	BPF_S_LDX_IMM,
+	BPF_S_MISC_TAX,
+	BPF_S_MISC_TXA,
+	BPF_S_ALU_DIV_K,
+	BPF_S_LD_MEM,
+	BPF_S_LDX_MEM,
+	BPF_S_ST,
+	BPF_S_STX,
+	BPF_S_JMP_JA,
+	BPF_S_JMP_JEQ_K,
+	BPF_S_JMP_JEQ_X,
+	BPF_S_JMP_JGE_K,
+	BPF_S_JMP_JGE_X,
+	BPF_S_JMP_JGT_K,
+	BPF_S_JMP_JGT_X,
+	BPF_S_JMP_JSET_K,
+	BPF_S_JMP_JSET_X,
+};
+
 #ifndef BPF_MAXINSNS
 #define BPF_MAXINSNS 4096
 #endif

commit 40eaf96271526a9f71030dd1a199ce46c045752e
Author: Paul LeoNerd Evans <leonerd@leonerd.org.uk>
Date:   Thu Apr 22 03:32:22 2010 +0000

    net: Socket filter ancilliary data access for skb->dev->type
    
    Add an SKF_AD_HATYPE field to the packet ancilliary data area, giving
    access to skb->dev->type, as reported in the sll_hatype field.
    
    When capturing packets on a PF_PACKET/SOCK_RAW socket bound to all
    interfaces, there doesn't appear to be a way for the filter program to
    actually find out the underlying hardware type the packet was captured
    on. This patch adds such ability.
    
    This patch also handles the case where skb->dev can be NULL, such as on
    netlink sockets.
    
    Signed-off-by: Paul Evans <leonerd@leonerd.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 29a0e3db9f43..151f5d703b7e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -123,7 +123,8 @@ struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_NLATTR_NEST	16
 #define SKF_AD_MARK 	20
 #define SKF_AD_QUEUE	24
-#define SKF_AD_MAX	28
+#define SKF_AD_HATYPE	28
+#define SKF_AD_MAX	32
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit d94d9fee9fa4e66a0b91640a694b8b10177075b3
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Nov 4 09:50:58 2009 -0800

    net: cleanup include/linux
    
    This cleanup patch puts struct/union/enum opening braces,
    in first line to ease grep games.
    
    struct something
    {
    
    becomes :
    
    struct something {
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bb3b4352978a..29a0e3db9f43 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -23,16 +23,14 @@
  *	the BPF code definitions which need to match so you can share filters
  */
  
-struct sock_filter	/* Filter block */
-{
+struct sock_filter {	/* Filter block */
 	__u16	code;   /* Actual filter code */
 	__u8	jt;	/* Jump true */
 	__u8	jf;	/* Jump false */
 	__u32	k;      /* Generic multiuse field */
 };
 
-struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
-{
+struct sock_fprog {	/* Required for SO_ATTACH_FILTER. */
 	unsigned short		len;	/* Number of filter blocks */
 	struct sock_filter __user *filter;
 };

commit d19742fb1c68e6db83b76e06dea5a374c99e104f
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Oct 20 01:06:22 2009 -0700

    filter: Add SKF_AD_QUEUE instruction
    
    It can help being able to filter packets on their queue_mapping.
    
    If filter performance is not good, we could add a "numqueue" field
    in struct packet_type, so that netif_nit_deliver() and other functions
    can directly ignore packets with not expected queue number.
    
    Lets experiment this simple filter extension first.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 909193e25395..bb3b4352978a 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -124,7 +124,8 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_NLATTR	12
 #define SKF_AD_NLATTR_NEST	16
 #define SKF_AD_MARK 	20
-#define SKF_AD_MAX	24
+#define SKF_AD_QUEUE	24
+#define SKF_AD_MAX	28
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit 7e75f93eda027d9f9e0203ee6ffd210ea92e98f3
Author: jamal <hadi@cyberus.ca>
Date:   Mon Oct 19 02:17:56 2009 +0000

    pkt_sched: ingress socket filter by mark
    
    Allow bpf to set a filter to drop packets that dont
    match a specific mark
    
    Signed-off-by: Jamal Hadi Salim <hadi@cyberus.ca>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 1354aaf6abbe..909193e25395 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -123,7 +123,8 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_IFINDEX 	8
 #define SKF_AD_NLATTR	12
 #define SKF_AD_NLATTR_NEST	16
-#define SKF_AD_MAX	20
+#define SKF_AD_MARK 	20
+#define SKF_AD_MAX	24
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit d214c7537bbf2f247991fb65b3420b0b3d712c67
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Thu Nov 20 00:49:27 2008 -0800

    filter: add SKF_AD_NLATTR_NEST to look for nested attributes
    
    SKF_AD_NLATTR allows us to find the first matching attribute in a
    stream of netlink attributes from one offset to the end of the
    netlink message. This is not suitable to look for a specific
    matching inside a set of nested attributes.
    
    For example, in ctnetlink messages, if we look for the CTA_V6_SRC
    attribute in a message that talks about an IPv4 connection,
    SKF_AD_NLATTR returns the offset of CTA_STATUS which has the same
    value of CTA_V6_SRC but outside the nest. To differenciate
    CTA_STATUS and CTA_V6_SRC, we would have to make assumptions on the
    size of the attribute and the usual offset, resulting in horrible
    BSF code.
    
    This patch adds SKF_AD_NLATTR_NEST, which is a variant of
    SKF_AD_NLATTR, that looks for an attribute inside the limits of
    a nested attributes, but not further.
    
    This patch validates that we have enough room to look for the
    nested attributes - based on a suggestion from Patrick McHardy.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Acked-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index b6ea9aa9e853..1354aaf6abbe 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -122,7 +122,8 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_PKTTYPE 	4
 #define SKF_AD_IFINDEX 	8
 #define SKF_AD_NLATTR	12
-#define SKF_AD_MAX 	16
+#define SKF_AD_NLATTR_NEST	16
+#define SKF_AD_MAX	20
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit 4738c1db1593687713869fa69e733eebc7b0d6d8
Author: Patrick McHardy <kaber@trash.net>
Date:   Thu Apr 10 02:02:28 2008 -0700

    [SKFILTER]: Add SKF_ADF_NLATTR instruction
    
    SKF_ADF_NLATTR searches for a netlink attribute, which avoids manually
    parsing and walking attributes. It takes the offset at which to start
    searching in the 'A' register and the attribute type in the 'X' register
    and returns the offset in the 'A' register. When the attribute is not
    found it returns zero.
    
    A top-level attribute can be located using a filter like this
    (example for nfnetlink, using struct nfgenmsg):
    
            ...
            {
                    /* A = offset of first attribute */
                    .code   = BPF_LD | BPF_IMM,
                    .k      = sizeof(struct nlmsghdr) + sizeof(struct nfgenmsg)
            },
            {
                    /* X = CTA_PROTOINFO */
                    .code   = BPF_LDX | BPF_IMM,
                    .k      = CTA_PROTOINFO,
            },
            {
                    /* A = netlink attribute offset */
                    .code   = BPF_LD | BPF_B | BPF_ABS,
                    .k      = SKF_AD_OFF + SKF_AD_NLATTR
            },
            {
                    /* Exit if not found */
                    .code   = BPF_JMP | BPF_JEQ | BPF_K,
                    .k      = 0,
                    .jt     = <error>
            },
            ...
    
    A nested attribute below the CTA_PROTOINFO attribute would then
    be parsed like this:
    
            ...
            {
                    /* A += sizeof(struct nlattr) */
                    .code   = BPF_ALU | BPF_ADD | BPF_K,
                    .k      = sizeof(struct nlattr),
            },
            {
                    /* X = CTA_PROTOINFO_TCP */
                    .code   = BPF_LDX | BPF_IMM,
                    .k      = CTA_PROTOINFO_TCP,
            },
            {
                    /* A = netlink attribute offset */
                    .code   = BPF_LD | BPF_B | BPF_ABS,
                    .k      = SKF_AD_OFF + SKF_AD_NLATTR
            },
            ...
    
    The data of an attribute can be loaded into 'A' like this:
    
            ...
            {
                    /* X = A (attribute offset) */
                    .code   = BPF_MISC | BPF_TAX,
            },
            {
                    /* A = skb->data[X + k] */
                    .code   = BPF_LD | BPF_B | BPF_IND,
                    .k      = sizeof(struct nlattr),
            },
            ...
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 673e5677ebcc..b6ea9aa9e853 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -121,7 +121,8 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 #define SKF_AD_PROTOCOL 0
 #define SKF_AD_PKTTYPE 	4
 #define SKF_AD_IFINDEX 	8
-#define SKF_AD_MAX 	12
+#define SKF_AD_NLATTR	12
+#define SKF_AD_MAX 	16
 #define SKF_NET_OFF   (-0x100000)
 #define SKF_LL_OFF    (-0x200000)
 

commit 43db6d65e0ef943a361cb91f8baa49132009227b
Author: Stephen Hemminger <shemminger@vyatta.com>
Date:   Thu Apr 10 01:43:09 2008 -0700

    socket: sk_filter deinline
    
    The sk_filter function is too big to be inlined. This saves 2296 bytes
    of text on allyesconfig.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index bfc5d319b946..673e5677ebcc 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -142,6 +142,7 @@ static inline unsigned int sk_filter_len(const struct sk_filter *fp)
 struct sk_buff;
 struct sock;
 
+extern int sk_filter(struct sock *sk, struct sk_buff *skb);
 extern unsigned int sk_run_filter(struct sk_buff *skb,
 				  struct sock_filter *filter, int flen);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);

commit b715631fad3ed320b85d386a84a6fb0b3f86b0b9
Author: Stephen Hemminger <shemminger@vyatta.com>
Date:   Thu Apr 10 01:33:47 2008 -0700

    socket: sk_filter minor cleanups
    
    Some minor style cleanups:
      * Move __KERNEL__ definitions to one place in filter.h
      * Use const for sk_filter_len
      * Line wrapping
      * Put EXPORT_SYMBOL next to function definition
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index ddfa0372a3b7..bfc5d319b946 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -37,21 +37,6 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 	struct sock_filter __user *filter;
 };
 
-#ifdef __KERNEL__
-struct sk_filter
-{
-	atomic_t		refcnt;
-	unsigned int         	len;	/* Number of filter blocks */
-	struct rcu_head		rcu;
-	struct sock_filter     	insns[0];
-};
-
-static inline unsigned int sk_filter_len(struct sk_filter *fp)
-{
-	return fp->len*sizeof(struct sock_filter) + sizeof(*fp);
-}
-#endif
-
 /*
  * Instruction classes
  */
@@ -141,10 +126,24 @@ static inline unsigned int sk_filter_len(struct sk_filter *fp)
 #define SKF_LL_OFF    (-0x200000)
 
 #ifdef __KERNEL__
+struct sk_filter
+{
+	atomic_t		refcnt;
+	unsigned int         	len;	/* Number of filter blocks */
+	struct rcu_head		rcu;
+	struct sock_filter     	insns[0];
+};
+
+static inline unsigned int sk_filter_len(const struct sk_filter *fp)
+{
+	return fp->len * sizeof(struct sock_filter) + sizeof(*fp);
+}
+
 struct sk_buff;
 struct sock;
 
-extern unsigned int sk_run_filter(struct sk_buff *skb, struct sock_filter *filter, int flen);
+extern unsigned int sk_run_filter(struct sk_buff *skb,
+				  struct sock_filter *filter, int flen);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, int flen);

commit 55b333253d5bcafbe187b50474e40789301c53c6
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Oct 17 21:21:26 2007 -0700

    [NET]: Introduce the sk_detach_filter() call
    
    Filter is attached in a separate function, so do the
    same for filter detaching.
    
    This also removes one variable sock_setsockopt().
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 91b2e3b9251e..ddfa0372a3b7 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -146,6 +146,7 @@ struct sock;
 
 extern unsigned int sk_run_filter(struct sk_buff *skb, struct sock_filter *filter, int flen);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+extern int sk_detach_filter(struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, int flen);
 #endif /* __KERNEL__ */
 

commit fda9ef5d679b07c9d9097aaf6ef7f069d794a8f9
Author: Dmitry Mishin <dim@openvz.org>
Date:   Thu Aug 31 15:28:39 2006 -0700

    [NET]: Fix sk->sk_filter field access
    
    Function sk_filter() is called from tcp_v{4,6}_rcv() functions with arg
    needlock = 0, while socket is not locked at that moment. In order to avoid
    this and similar issues in the future, use rcu for sk->sk_filter field read
    protection.
    
    Signed-off-by: Dmitry Mishin <dim@openvz.org>
    Signed-off-by: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index c6cb8f095088..91b2e3b9251e 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -25,10 +25,10 @@
  
 struct sock_filter	/* Filter block */
 {
-        __u16	code;   /* Actual filter code */
-        __u8	jt;	/* Jump true */
-        __u8	jf;	/* Jump false */
-        __u32	k;      /* Generic multiuse field */
+	__u16	code;   /* Actual filter code */
+	__u8	jt;	/* Jump true */
+	__u8	jf;	/* Jump false */
+	__u32	k;      /* Generic multiuse field */
 };
 
 struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
@@ -41,8 +41,9 @@ struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
 struct sk_filter
 {
 	atomic_t		refcnt;
-        unsigned int         	len;	/* Number of filter blocks */
-        struct sock_filter     	insns[0];
+	unsigned int         	len;	/* Number of filter blocks */
+	struct rcu_head		rcu;
+	struct sock_filter     	insns[0];
 };
 
 static inline unsigned int sk_filter_len(struct sk_filter *fp)

commit 4bad4dc919573dbe9a5b41dd9edff279e99822d7
Author: Kris Katterjohn <kjak@ispwest.com>
Date:   Fri Jan 6 13:08:20 2006 -0800

    [NET]: Change sk_run_filter()'s return type in net/core/filter.c
    
    It should return an unsigned value, and fix sk_filter() as well.
    
    Signed-off-by: Kris Katterjohn <kjak@ispwest.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/filter.h b/include/linux/filter.h
index 3ba843c46382..c6cb8f095088 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -143,7 +143,7 @@ static inline unsigned int sk_filter_len(struct sk_filter *fp)
 struct sk_buff;
 struct sock;
 
-extern int sk_run_filter(struct sk_buff *skb, struct sock_filter *filter, int flen);
+extern unsigned int sk_run_filter(struct sk_buff *skb, struct sock_filter *filter, int flen);
 extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
 extern int sk_chk_filter(struct sock_filter *filter, int flen);
 #endif /* __KERNEL__ */

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/linux/filter.h b/include/linux/filter.h
new file mode 100644
index 000000000000..3ba843c46382
--- /dev/null
+++ b/include/linux/filter.h
@@ -0,0 +1,151 @@
+/*
+ * Linux Socket Filter Data Structures
+ */
+
+#ifndef __LINUX_FILTER_H__
+#define __LINUX_FILTER_H__
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+
+#ifdef __KERNEL__
+#include <asm/atomic.h>
+#endif
+
+/*
+ * Current version of the filter code architecture.
+ */
+#define BPF_MAJOR_VERSION 1
+#define BPF_MINOR_VERSION 1
+
+/*
+ *	Try and keep these values and structures similar to BSD, especially
+ *	the BPF code definitions which need to match so you can share filters
+ */
+ 
+struct sock_filter	/* Filter block */
+{
+        __u16	code;   /* Actual filter code */
+        __u8	jt;	/* Jump true */
+        __u8	jf;	/* Jump false */
+        __u32	k;      /* Generic multiuse field */
+};
+
+struct sock_fprog	/* Required for SO_ATTACH_FILTER. */
+{
+	unsigned short		len;	/* Number of filter blocks */
+	struct sock_filter __user *filter;
+};
+
+#ifdef __KERNEL__
+struct sk_filter
+{
+	atomic_t		refcnt;
+        unsigned int         	len;	/* Number of filter blocks */
+        struct sock_filter     	insns[0];
+};
+
+static inline unsigned int sk_filter_len(struct sk_filter *fp)
+{
+	return fp->len*sizeof(struct sock_filter) + sizeof(*fp);
+}
+#endif
+
+/*
+ * Instruction classes
+ */
+
+#define BPF_CLASS(code) ((code) & 0x07)
+#define         BPF_LD          0x00
+#define         BPF_LDX         0x01
+#define         BPF_ST          0x02
+#define         BPF_STX         0x03
+#define         BPF_ALU         0x04
+#define         BPF_JMP         0x05
+#define         BPF_RET         0x06
+#define         BPF_MISC        0x07
+
+/* ld/ldx fields */
+#define BPF_SIZE(code)  ((code) & 0x18)
+#define         BPF_W           0x00
+#define         BPF_H           0x08
+#define         BPF_B           0x10
+#define BPF_MODE(code)  ((code) & 0xe0)
+#define         BPF_IMM         0x00
+#define         BPF_ABS         0x20
+#define         BPF_IND         0x40
+#define         BPF_MEM         0x60
+#define         BPF_LEN         0x80
+#define         BPF_MSH         0xa0
+
+/* alu/jmp fields */
+#define BPF_OP(code)    ((code) & 0xf0)
+#define         BPF_ADD         0x00
+#define         BPF_SUB         0x10
+#define         BPF_MUL         0x20
+#define         BPF_DIV         0x30
+#define         BPF_OR          0x40
+#define         BPF_AND         0x50
+#define         BPF_LSH         0x60
+#define         BPF_RSH         0x70
+#define         BPF_NEG         0x80
+#define         BPF_JA          0x00
+#define         BPF_JEQ         0x10
+#define         BPF_JGT         0x20
+#define         BPF_JGE         0x30
+#define         BPF_JSET        0x40
+#define BPF_SRC(code)   ((code) & 0x08)
+#define         BPF_K           0x00
+#define         BPF_X           0x08
+
+/* ret - BPF_K and BPF_X also apply */
+#define BPF_RVAL(code)  ((code) & 0x18)
+#define         BPF_A           0x10
+
+/* misc */
+#define BPF_MISCOP(code) ((code) & 0xf8)
+#define         BPF_TAX         0x00
+#define         BPF_TXA         0x80
+
+#ifndef BPF_MAXINSNS
+#define BPF_MAXINSNS 4096
+#endif
+
+/*
+ * Macros for filter block array initializers.
+ */
+#ifndef BPF_STMT
+#define BPF_STMT(code, k) { (unsigned short)(code), 0, 0, k }
+#endif
+#ifndef BPF_JUMP
+#define BPF_JUMP(code, k, jt, jf) { (unsigned short)(code), jt, jf, k }
+#endif
+
+/*
+ * Number of scratch memory words for: BPF_ST and BPF_STX
+ */
+#define BPF_MEMWORDS 16
+
+/* RATIONALE. Negative offsets are invalid in BPF.
+   We use them to reference ancillary data.
+   Unlike introduction new instructions, it does not break
+   existing compilers/optimizers.
+ */
+#define SKF_AD_OFF    (-0x1000)
+#define SKF_AD_PROTOCOL 0
+#define SKF_AD_PKTTYPE 	4
+#define SKF_AD_IFINDEX 	8
+#define SKF_AD_MAX 	12
+#define SKF_NET_OFF   (-0x100000)
+#define SKF_LL_OFF    (-0x200000)
+
+#ifdef __KERNEL__
+struct sk_buff;
+struct sock;
+
+extern int sk_run_filter(struct sk_buff *skb, struct sock_filter *filter, int flen);
+extern int sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);
+extern int sk_chk_filter(struct sock_filter *filter, int flen);
+#endif /* __KERNEL__ */
+
+#endif /* __LINUX_FILTER_H__ */
