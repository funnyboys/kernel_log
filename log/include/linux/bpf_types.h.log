commit 7f045a49fee04b5662cbdeaf0838f9322ae8c63a
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Sun May 31 10:28:38 2020 +0200

    bpf: Add link-based BPF program attachment to network namespace
    
    Extend bpf() syscall subcommands that operate on bpf_link, that is
    LINK_CREATE, LINK_UPDATE, OBJ_GET_INFO, to accept attach types tied to
    network namespaces (only flow dissector at the moment).
    
    Link-based and prog-based attachment can be used interchangeably, but only
    one can exist at a time. Attempts to attach a link when a prog is already
    attached directly, and the other way around, will be met with -EEXIST.
    Attempts to detach a program when link exists result in -EINVAL.
    
    Attachment of multiple links of same attach type to one netns is not
    supported with the intention to lift the restriction when a use-case
    presents itself. Because of that link create returns -E2BIG when trying to
    create another netns link, when one already exists.
    
    Link-based attachments to netns don't keep a netns alive by holding a ref
    to it. Instead links get auto-detached from netns when the latter is being
    destroyed, using a pernet pre_exit callback.
    
    When auto-detached, link lives in defunct state as long there are open FDs
    for it. -ENOLINK is returned if a user tries to update a defunct link.
    
    Because bpf_link to netns doesn't hold a ref to struct net, special care is
    taken when releasing, updating, or filling link info. The netns might be
    getting torn down when any of these link operations are in progress. That
    is why auto-detach and update/release/fill_info are synchronized by the
    same mutex. Also, link ops have to always check if auto-detach has not
    happened yet and if netns is still alive (refcnt > 0).
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200531082846.2117903-5-jakub@cloudflare.com

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index fa8e1b552acd..a18ae82a298a 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -126,3 +126,6 @@ BPF_LINK_TYPE(BPF_LINK_TYPE_TRACING, tracing)
 BPF_LINK_TYPE(BPF_LINK_TYPE_CGROUP, cgroup)
 #endif
 BPF_LINK_TYPE(BPF_LINK_TYPE_ITER, iter)
+#ifdef CONFIG_NET
+BPF_LINK_TYPE(BPF_LINK_TYPE_NETNS, netns)
+#endif

commit 457f44363a8894135c85b7a9afd2bd8196db24ab
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 29 00:54:20 2020 -0700

    bpf: Implement BPF ring buffer and verifier support for it
    
    This commit adds a new MPSC ring buffer implementation into BPF ecosystem,
    which allows multiple CPUs to submit data to a single shared ring buffer. On
    the consumption side, only single consumer is assumed.
    
    Motivation
    ----------
    There are two distinctive motivators for this work, which are not satisfied by
    existing perf buffer, which prompted creation of a new ring buffer
    implementation.
      - more efficient memory utilization by sharing ring buffer across CPUs;
      - preserving ordering of events that happen sequentially in time, even
      across multiple CPUs (e.g., fork/exec/exit events for a task).
    
    These two problems are independent, but perf buffer fails to satisfy both.
    Both are a result of a choice to have per-CPU perf ring buffer.  Both can be
    also solved by having an MPSC implementation of ring buffer. The ordering
    problem could technically be solved for perf buffer with some in-kernel
    counting, but given the first one requires an MPSC buffer, the same solution
    would solve the second problem automatically.
    
    Semantics and APIs
    ------------------
    Single ring buffer is presented to BPF programs as an instance of BPF map of
    type BPF_MAP_TYPE_RINGBUF. Two other alternatives considered, but ultimately
    rejected.
    
    One way would be to, similar to BPF_MAP_TYPE_PERF_EVENT_ARRAY, make
    BPF_MAP_TYPE_RINGBUF could represent an array of ring buffers, but not enforce
    "same CPU only" rule. This would be more familiar interface compatible with
    existing perf buffer use in BPF, but would fail if application needed more
    advanced logic to lookup ring buffer by arbitrary key. HASH_OF_MAPS addresses
    this with current approach. Additionally, given the performance of BPF
    ringbuf, many use cases would just opt into a simple single ring buffer shared
    among all CPUs, for which current approach would be an overkill.
    
    Another approach could introduce a new concept, alongside BPF map, to
    represent generic "container" object, which doesn't necessarily have key/value
    interface with lookup/update/delete operations. This approach would add a lot
    of extra infrastructure that has to be built for observability and verifier
    support. It would also add another concept that BPF developers would have to
    familiarize themselves with, new syntax in libbpf, etc. But then would really
    provide no additional benefits over the approach of using a map.
    BPF_MAP_TYPE_RINGBUF doesn't support lookup/update/delete operations, but so
    doesn't few other map types (e.g., queue and stack; array doesn't support
    delete, etc).
    
    The approach chosen has an advantage of re-using existing BPF map
    infrastructure (introspection APIs in kernel, libbpf support, etc), being
    familiar concept (no need to teach users a new type of object in BPF program),
    and utilizing existing tooling (bpftool). For common scenario of using
    a single ring buffer for all CPUs, it's as simple and straightforward, as
    would be with a dedicated "container" object. On the other hand, by being
    a map, it can be combined with ARRAY_OF_MAPS and HASH_OF_MAPS map-in-maps to
    implement a wide variety of topologies, from one ring buffer for each CPU
    (e.g., as a replacement for perf buffer use cases), to a complicated
    application hashing/sharding of ring buffers (e.g., having a small pool of
    ring buffers with hashed task's tgid being a look up key to preserve order,
    but reduce contention).
    
    Key and value sizes are enforced to be zero. max_entries is used to specify
    the size of ring buffer and has to be a power of 2 value.
    
    There are a bunch of similarities between perf buffer
    (BPF_MAP_TYPE_PERF_EVENT_ARRAY) and new BPF ring buffer semantics:
      - variable-length records;
      - if there is no more space left in ring buffer, reservation fails, no
        blocking;
      - memory-mappable data area for user-space applications for ease of
        consumption and high performance;
      - epoll notifications for new incoming data;
      - but still the ability to do busy polling for new data to achieve the
        lowest latency, if necessary.
    
    BPF ringbuf provides two sets of APIs to BPF programs:
      - bpf_ringbuf_output() allows to *copy* data from one place to a ring
        buffer, similarly to bpf_perf_event_output();
      - bpf_ringbuf_reserve()/bpf_ringbuf_commit()/bpf_ringbuf_discard() APIs
        split the whole process into two steps. First, a fixed amount of space is
        reserved. If successful, a pointer to a data inside ring buffer data area
        is returned, which BPF programs can use similarly to a data inside
        array/hash maps. Once ready, this piece of memory is either committed or
        discarded. Discard is similar to commit, but makes consumer ignore the
        record.
    
    bpf_ringbuf_output() has disadvantage of incurring extra memory copy, because
    record has to be prepared in some other place first. But it allows to submit
    records of the length that's not known to verifier beforehand. It also closely
    matches bpf_perf_event_output(), so will simplify migration significantly.
    
    bpf_ringbuf_reserve() avoids the extra copy of memory by providing a memory
    pointer directly to ring buffer memory. In a lot of cases records are larger
    than BPF stack space allows, so many programs have use extra per-CPU array as
    a temporary heap for preparing sample. bpf_ringbuf_reserve() avoid this needs
    completely. But in exchange, it only allows a known constant size of memory to
    be reserved, such that verifier can verify that BPF program can't access
    memory outside its reserved record space. bpf_ringbuf_output(), while slightly
    slower due to extra memory copy, covers some use cases that are not suitable
    for bpf_ringbuf_reserve().
    
    The difference between commit and discard is very small. Discard just marks
    a record as discarded, and such records are supposed to be ignored by consumer
    code. Discard is useful for some advanced use-cases, such as ensuring
    all-or-nothing multi-record submission, or emulating temporary malloc()/free()
    within single BPF program invocation.
    
    Each reserved record is tracked by verifier through existing
    reference-tracking logic, similar to socket ref-tracking. It is thus
    impossible to reserve a record, but forget to submit (or discard) it.
    
    bpf_ringbuf_query() helper allows to query various properties of ring buffer.
    Currently 4 are supported:
      - BPF_RB_AVAIL_DATA returns amount of unconsumed data in ring buffer;
      - BPF_RB_RING_SIZE returns the size of ring buffer;
      - BPF_RB_CONS_POS/BPF_RB_PROD_POS returns current logical possition of
        consumer/producer, respectively.
    Returned values are momentarily snapshots of ring buffer state and could be
    off by the time helper returns, so this should be used only for
    debugging/reporting reasons or for implementing various heuristics, that take
    into account highly-changeable nature of some of those characteristics.
    
    One such heuristic might involve more fine-grained control over poll/epoll
    notifications about new data availability in ring buffer. Together with
    BPF_RB_NO_WAKEUP/BPF_RB_FORCE_WAKEUP flags for output/commit/discard helpers,
    it allows BPF program a high degree of control and, e.g., more efficient
    batched notifications. Default self-balancing strategy, though, should be
    adequate for most applications and will work reliable and efficiently already.
    
    Design and implementation
    -------------------------
    This reserve/commit schema allows a natural way for multiple producers, either
    on different CPUs or even on the same CPU/in the same BPF program, to reserve
    independent records and work with them without blocking other producers. This
    means that if BPF program was interruped by another BPF program sharing the
    same ring buffer, they will both get a record reserved (provided there is
    enough space left) and can work with it and submit it independently. This
    applies to NMI context as well, except that due to using a spinlock during
    reservation, in NMI context, bpf_ringbuf_reserve() might fail to get a lock,
    in which case reservation will fail even if ring buffer is not full.
    
    The ring buffer itself internally is implemented as a power-of-2 sized
    circular buffer, with two logical and ever-increasing counters (which might
    wrap around on 32-bit architectures, that's not a problem):
      - consumer counter shows up to which logical position consumer consumed the
        data;
      - producer counter denotes amount of data reserved by all producers.
    
    Each time a record is reserved, producer that "owns" the record will
    successfully advance producer counter. At that point, data is still not yet
    ready to be consumed, though. Each record has 8 byte header, which contains
    the length of reserved record, as well as two extra bits: busy bit to denote
    that record is still being worked on, and discard bit, which might be set at
    commit time if record is discarded. In the latter case, consumer is supposed
    to skip the record and move on to the next one. Record header also encodes
    record's relative offset from the beginning of ring buffer data area (in
    pages). This allows bpf_ringbuf_commit()/bpf_ringbuf_discard() to accept only
    the pointer to the record itself, without requiring also the pointer to ring
    buffer itself. Ring buffer memory location will be restored from record
    metadata header. This significantly simplifies verifier, as well as improving
    API usability.
    
    Producer counter increments are serialized under spinlock, so there is
    a strict ordering between reservations. Commits, on the other hand, are
    completely lockless and independent. All records become available to consumer
    in the order of reservations, but only after all previous records where
    already committed. It is thus possible for slow producers to temporarily hold
    off submitted records, that were reserved later.
    
    Reservation/commit/consumer protocol is verified by litmus tests in
    Documentation/litmus-test/bpf-rb.
    
    One interesting implementation bit, that significantly simplifies (and thus
    speeds up as well) implementation of both producers and consumers is how data
    area is mapped twice contiguously back-to-back in the virtual memory. This
    allows to not take any special measures for samples that have to wrap around
    at the end of the circular buffer data area, because the next page after the
    last data page would be first data page again, and thus the sample will still
    appear completely contiguous in virtual memory. See comment and a simple ASCII
    diagram showing this visually in bpf_ringbuf_area_alloc().
    
    Another feature that distinguishes BPF ringbuf from perf ring buffer is
    a self-pacing notifications of new data being availability.
    bpf_ringbuf_commit() implementation will send a notification of new record
    being available after commit only if consumer has already caught up right up
    to the record being committed. If not, consumer still has to catch up and thus
    will see new data anyways without needing an extra poll notification.
    Benchmarks (see tools/testing/selftests/bpf/benchs/bench_ringbuf.c) show that
    this allows to achieve a very high throughput without having to resort to
    tricks like "notify only every Nth sample", which are necessary with perf
    buffer. For extreme cases, when BPF program wants more manual control of
    notifications, commit/discard/output helpers accept BPF_RB_NO_WAKEUP and
    BPF_RB_FORCE_WAKEUP flags, which give full control over notifications of data
    availability, but require extra caution and diligence in using this API.
    
    Comparison to alternatives
    --------------------------
    Before considering implementing BPF ring buffer from scratch existing
    alternatives in kernel were evaluated, but didn't seem to meet the needs. They
    largely fell into few categores:
      - per-CPU buffers (perf, ftrace, etc), which don't satisfy two motivations
        outlined above (ordering and memory consumption);
      - linked list-based implementations; while some were multi-producer designs,
        consuming these from user-space would be very complicated and most
        probably not performant; memory-mapping contiguous piece of memory is
        simpler and more performant for user-space consumers;
      - io_uring is SPSC, but also requires fixed-sized elements. Naively turning
        SPSC queue into MPSC w/ lock would have subpar performance compared to
        locked reserve + lockless commit, as with BPF ring buffer. Fixed sized
        elements would be too limiting for BPF programs, given existing BPF
        programs heavily rely on variable-sized perf buffer already;
      - specialized implementations (like a new printk ring buffer, [0]) with lots
        of printk-specific limitations and implications, that didn't seem to fit
        well for intended use with BPF programs.
    
      [0] https://lwn.net/Articles/779550/
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200529075424.3139988-2-andriin@fb.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 29d22752fc87..fa8e1b552acd 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -118,6 +118,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_STACK, stack_map_ops)
 #if defined(CONFIG_BPF_JIT)
 BPF_MAP_TYPE(BPF_MAP_TYPE_STRUCT_OPS, bpf_struct_ops_map_ops)
 #endif
+BPF_MAP_TYPE(BPF_MAP_TYPE_RINGBUF, ringbuf_map_ops)
 
 BPF_LINK_TYPE(BPF_LINK_TYPE_RAW_TRACEPOINT, raw_tracepoint)
 BPF_LINK_TYPE(BPF_LINK_TYPE_TRACING, tracing)

commit de4e05cac46d206f9090051ef09930514bff73e4
Author: Yonghong Song <yhs@fb.com>
Date:   Sat May 9 10:59:01 2020 -0700

    bpf: Support bpf tracing/iter programs for BPF_LINK_CREATE
    
    Given a bpf program, the step to create an anonymous bpf iterator is:
      - create a bpf_iter_link, which combines bpf program and the target.
        In the future, there could be more information recorded in the link.
        A link_fd will be returned to the user space.
      - create an anonymous bpf iterator with the given link_fd.
    
    The bpf_iter_link can be pinned to bpffs mount file system to
    create a file based bpf iterator as well.
    
    The benefit to use of bpf_iter_link:
      - using bpf link simplifies design and implementation as bpf link
        is used for other tracing bpf programs.
      - for file based bpf iterator, bpf_iter_link provides a standard
        way to replace underlying bpf programs.
      - for both anonymous and free based iterators, bpf link query
        capability can be leveraged.
    
    The patch added support of tracing/iter programs for BPF_LINK_CREATE.
    A new link type BPF_LINK_TYPE_ITER is added to facilitate link
    querying. Currently, only prog_id is needed, so there is no
    additional in-kernel show_fdinfo() and fill_link_info() hook
    is needed for BPF_LINK_TYPE_ITER link.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200509175901.2475084-1-yhs@fb.com

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 8345cdf553b8..29d22752fc87 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -124,3 +124,4 @@ BPF_LINK_TYPE(BPF_LINK_TYPE_TRACING, tracing)
 #ifdef CONFIG_CGROUP_BPF
 BPF_LINK_TYPE(BPF_LINK_TYPE_CGROUP, cgroup)
 #endif
+BPF_LINK_TYPE(BPF_LINK_TYPE_ITER, iter)

commit f2e10bff16a0fdd41ba278c84da9813700e356af
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Apr 28 17:16:08 2020 -0700

    bpf: Add support for BPF_OBJ_GET_INFO_BY_FD for bpf_link
    
    Add ability to fetch bpf_link details through BPF_OBJ_GET_INFO_BY_FD command.
    Also enhance show_fdinfo to potentially include bpf_link type-specific
    information (similarly to obj_info).
    
    Also introduce enum bpf_link_type stored in bpf_link itself and expose it in
    UAPI. bpf_link_tracing also now will store and return bpf_attach_type.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200429001614.1544-5-andriin@fb.com

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index ba0c2d56f8a3..8345cdf553b8 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -118,3 +118,9 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_STACK, stack_map_ops)
 #if defined(CONFIG_BPF_JIT)
 BPF_MAP_TYPE(BPF_MAP_TYPE_STRUCT_OPS, bpf_struct_ops_map_ops)
 #endif
+
+BPF_LINK_TYPE(BPF_LINK_TYPE_RAW_TRACEPOINT, raw_tracepoint)
+BPF_LINK_TYPE(BPF_LINK_TYPE_TRACING, tracing)
+#ifdef CONFIG_CGROUP_BPF
+BPF_LINK_TYPE(BPF_LINK_TYPE_CGROUP, cgroup)
+#endif

commit fc611f47f2188ade2b48ff6902d5cce8baac0c58
Author: KP Singh <kpsingh@google.com>
Date:   Sun Mar 29 01:43:49 2020 +0100

    bpf: Introduce BPF_PROG_TYPE_LSM
    
    Introduce types and configs for bpf programs that can be attached to
    LSM hooks. The programs can be enabled by the config option
    CONFIG_BPF_LSM.
    
    Signed-off-by: KP Singh <kpsingh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Brendan Jackman <jackmanb@google.com>
    Reviewed-by: Florent Revest <revest@google.com>
    Reviewed-by: Thomas Garnier <thgarnie@google.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: James Morris <jamorris@linux.microsoft.com>
    Link: https://lore.kernel.org/bpf/20200329004356.27286-2-kpsingh@chromium.org

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index c81d4ece79a4..ba0c2d56f8a3 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -70,6 +70,10 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_STRUCT_OPS, bpf_struct_ops,
 	      void *, void *)
 BPF_PROG_TYPE(BPF_PROG_TYPE_EXT, bpf_extension,
 	      void *, void *)
+#ifdef CONFIG_BPF_LSM
+BPF_PROG_TYPE(BPF_PROG_TYPE_LSM, lsm,
+	       void *, void *)
+#endif /* CONFIG_BPF_LSM */
 #endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)

commit be8704ff07d2374bcc5c675526f95e70c6459683
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Jan 20 16:53:46 2020 -0800

    bpf: Introduce dynamic program extensions
    
    Introduce dynamic program extensions. The users can load additional BPF
    functions and replace global functions in previously loaded BPF programs while
    these programs are executing.
    
    Global functions are verified individually by the verifier based on their types only.
    Hence the global function in the new program which types match older function can
    safely replace that corresponding function.
    
    This new function/program is called 'an extension' of old program. At load time
    the verifier uses (attach_prog_fd, attach_btf_id) pair to identify the function
    to be replaced. The BPF program type is derived from the target program into
    extension program. Technically bpf_verifier_ops is copied from target program.
    The BPF_PROG_TYPE_EXT program type is a placeholder. It has empty verifier_ops.
    The extension program can call the same bpf helper functions as target program.
    Single BPF_PROG_TYPE_EXT type is used to extend XDP, SKB and all other program
    types. The verifier allows only one level of replacement. Meaning that the
    extension program cannot recursively extend an extension. That also means that
    the maximum stack size is increasing from 512 to 1024 bytes and maximum
    function nesting level from 8 to 16. The programs don't always consume that
    much. The stack usage is determined by the number of on-stack variables used by
    the program. The verifier could have enforced 512 limit for combined original
    plus extension program, but it makes for difficult user experience. The main
    use case for extensions is to provide generic mechanism to plug external
    programs into policy program or function call chaining.
    
    BPF trampoline is used to track both fentry/fexit and program extensions
    because both are using the same nop slot at the beginning of every BPF
    function. Attaching fentry/fexit to a function that was replaced is not
    allowed. The opposite is true as well. Replacing a function that currently
    being analyzed with fentry/fexit is not allowed. The executable page allocated
    by BPF trampoline is not used by program extensions. This inefficiency will be
    optimized in future patches.
    
    Function by function verification of global function supports scalars and
    pointer to context only. Hence program extensions are supported for such class
    of global functions only. In the future the verifier will be extended with
    support to pointers to structures, arrays with sizes, etc.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Link: https://lore.kernel.org/bpf/20200121005348.2769920-2-ast@kernel.org

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 9f326e6ef885..c81d4ece79a4 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -68,6 +68,8 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport,
 #if defined(CONFIG_BPF_JIT)
 BPF_PROG_TYPE(BPF_PROG_TYPE_STRUCT_OPS, bpf_struct_ops,
 	      void *, void *)
+BPF_PROG_TYPE(BPF_PROG_TYPE_EXT, bpf_extension,
+	      void *, void *)
 #endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)

commit 85d33df357b634649ddbe0a20fd2d0fc5732c3cb
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:05 2020 -0800

    bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS
    
    The patch introduces BPF_MAP_TYPE_STRUCT_OPS.  The map value
    is a kernel struct with its func ptr implemented in bpf prog.
    This new map is the interface to register/unregister/introspect
    a bpf implemented kernel struct.
    
    The kernel struct is actually embedded inside another new struct
    (or called the "value" struct in the code).  For example,
    "struct tcp_congestion_ops" is embbeded in:
    struct bpf_struct_ops_tcp_congestion_ops {
            refcount_t refcnt;
            enum bpf_struct_ops_state state;
            struct tcp_congestion_ops data;  /* <-- kernel subsystem struct here */
    }
    The map value is "struct bpf_struct_ops_tcp_congestion_ops".
    The "bpftool map dump" will then be able to show the
    state ("inuse"/"tobefree") and the number of subsystem's refcnt (e.g.
    number of tcp_sock in the tcp_congestion_ops case).  This "value" struct
    is created automatically by a macro.  Having a separate "value" struct
    will also make extending "struct bpf_struct_ops_XYZ" easier (e.g. adding
    "void (*init)(void)" to "struct bpf_struct_ops_XYZ" to do some
    initialization works before registering the struct_ops to the kernel
    subsystem).  The libbpf will take care of finding and populating the
    "struct bpf_struct_ops_XYZ" from "struct XYZ".
    
    Register a struct_ops to a kernel subsystem:
    1. Load all needed BPF_PROG_TYPE_STRUCT_OPS prog(s)
    2. Create a BPF_MAP_TYPE_STRUCT_OPS with attr->btf_vmlinux_value_type_id
       set to the btf id "struct bpf_struct_ops_tcp_congestion_ops" of the
       running kernel.
       Instead of reusing the attr->btf_value_type_id,
       btf_vmlinux_value_type_id s added such that attr->btf_fd can still be
       used as the "user" btf which could store other useful sysadmin/debug
       info that may be introduced in the furture,
       e.g. creation-date/compiler-details/map-creator...etc.
    3. Create a "struct bpf_struct_ops_tcp_congestion_ops" object as described
       in the running kernel btf.  Populate the value of this object.
       The function ptr should be populated with the prog fds.
    4. Call BPF_MAP_UPDATE with the object created in (3) as
       the map value.  The key is always "0".
    
    During BPF_MAP_UPDATE, the code that saves the kernel-func-ptr's
    args as an array of u64 is generated.  BPF_MAP_UPDATE also allows
    the specific struct_ops to do some final checks in "st_ops->init_member()"
    (e.g. ensure all mandatory func ptrs are implemented).
    If everything looks good, it will register this kernel struct
    to the kernel subsystem.  The map will not allow further update
    from this point.
    
    Unregister a struct_ops from the kernel subsystem:
    BPF_MAP_DELETE with key "0".
    
    Introspect a struct_ops:
    BPF_MAP_LOOKUP_ELEM with key "0".  The map value returned will
    have the prog _id_ populated as the func ptr.
    
    The map value state (enum bpf_struct_ops_state) will transit from:
    INIT (map created) =>
    INUSE (map updated, i.e. reg) =>
    TOBEFREE (map value deleted, i.e. unreg)
    
    The kernel subsystem needs to call bpf_struct_ops_get() and
    bpf_struct_ops_put() to manage the "refcnt" in the
    "struct bpf_struct_ops_XYZ".  This patch uses a separate refcnt
    for the purose of tracking the subsystem usage.  Another approach
    is to reuse the map->refcnt and then "show" (i.e. during map_lookup)
    the subsystem's usage by doing map->refcnt - map->usercnt to filter out
    the map-fd/pinned-map usage.  However, that will also tie down the
    future semantics of map->refcnt and map->usercnt.
    
    The very first subsystem's refcnt (during reg()) holds one
    count to map->refcnt.  When the very last subsystem's refcnt
    is gone, it will also release the map->refcnt.  All bpf_prog will be
    freed when the map->refcnt reaches 0 (i.e. during map_free()).
    
    Here is how the bpftool map command will look like:
    [root@arch-fb-vm1 bpf]# bpftool map show
    6: struct_ops  name dctcp  flags 0x0
            key 4B  value 256B  max_entries 1  memlock 4096B
            btf_id 6
    [root@arch-fb-vm1 bpf]# bpftool map dump id 6
    [{
            "value": {
                "refcnt": {
                    "refs": {
                        "counter": 1
                    }
                },
                "state": 1,
                "data": {
                    "list": {
                        "next": 0,
                        "prev": 0
                    },
                    "key": 0,
                    "flags": 2,
                    "init": 24,
                    "release": 0,
                    "ssthresh": 25,
                    "cong_avoid": 30,
                    "set_state": 27,
                    "cwnd_event": 28,
                    "in_ack_event": 26,
                    "undo_cwnd": 29,
                    "pkts_acked": 0,
                    "min_tso_segs": 0,
                    "sndbuf_expand": 0,
                    "cong_control": 0,
                    "get_info": 0,
                    "name": [98,112,102,95,100,99,116,99,112,0,0,0,0,0,0,0
                    ],
                    "owner": 0
                }
            }
        }
    ]
    
    Misc Notes:
    * bpf_struct_ops_map_sys_lookup_elem() is added for syscall lookup.
      It does an inplace update on "*value" instead returning a pointer
      to syscall.c.  Otherwise, it needs a separate copy of "zero" value
      for the BPF_STRUCT_OPS_STATE_INIT to avoid races.
    
    * The bpf_struct_ops_map_delete_elem() is also called without
      preempt_disable() from map_delete_elem().  It is because
      the "->unreg()" may requires sleepable context, e.g.
      the "tcp_unregister_congestion_control()".
    
    * "const" is added to some of the existing "struct btf_func_model *"
      function arg to avoid a compiler warning caused by this patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003505.3855919-1-kafai@fb.com

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index fadd243ffa2d..9f326e6ef885 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -109,3 +109,6 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, reuseport_array_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_QUEUE, queue_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_STACK, stack_map_ops)
+#if defined(CONFIG_BPF_JIT)
+BPF_MAP_TYPE(BPF_MAP_TYPE_STRUCT_OPS, bpf_struct_ops_map_ops)
+#endif

commit 27ae7997a66174cb8afd6a75b3989f5e0c1b9e5a
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:03 2020 -0800

    bpf: Introduce BPF_PROG_TYPE_STRUCT_OPS
    
    This patch allows the kernel's struct ops (i.e. func ptr) to be
    implemented in BPF.  The first use case in this series is the
    "struct tcp_congestion_ops" which will be introduced in a
    latter patch.
    
    This patch introduces a new prog type BPF_PROG_TYPE_STRUCT_OPS.
    The BPF_PROG_TYPE_STRUCT_OPS prog is verified against a particular
    func ptr of a kernel struct.  The attr->attach_btf_id is the btf id
    of a kernel struct.  The attr->expected_attach_type is the member
    "index" of that kernel struct.  The first member of a struct starts
    with member index 0.  That will avoid ambiguity when a kernel struct
    has multiple func ptrs with the same func signature.
    
    For example, a BPF_PROG_TYPE_STRUCT_OPS prog is written
    to implement the "init" func ptr of the "struct tcp_congestion_ops".
    The attr->attach_btf_id is the btf id of the "struct tcp_congestion_ops"
    of the _running_ kernel.  The attr->expected_attach_type is 3.
    
    The ctx of BPF_PROG_TYPE_STRUCT_OPS is an array of u64 args saved
    by arch_prepare_bpf_trampoline that will be done in the next
    patch when introducing BPF_MAP_TYPE_STRUCT_OPS.
    
    "struct bpf_struct_ops" is introduced as a common interface for the kernel
    struct that supports BPF_PROG_TYPE_STRUCT_OPS prog.  The supporting kernel
    struct will need to implement an instance of the "struct bpf_struct_ops".
    
    The supporting kernel struct also needs to implement a bpf_verifier_ops.
    During BPF_PROG_LOAD, bpf_struct_ops_find() will find the right
    bpf_verifier_ops by searching the attr->attach_btf_id.
    
    A new "btf_struct_access" is also added to the bpf_verifier_ops such
    that the supporting kernel struct can optionally provide its own specific
    check on accessing the func arg (e.g. provide limited write access).
    
    After btf_vmlinux is parsed, the new bpf_struct_ops_init() is called
    to initialize some values (e.g. the btf id of the supporting kernel
    struct) and it can only be done once the btf_vmlinux is available.
    
    The R0 checks at BPF_EXIT is excluded for the BPF_PROG_TYPE_STRUCT_OPS prog
    if the return type of the prog->aux->attach_func_proto is "void".
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003503.3855825-1-kafai@fb.com

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 93740b3614d7..fadd243ffa2d 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -65,6 +65,10 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2,
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport,
 	      struct sk_reuseport_md, struct sk_reuseport_kern)
 #endif
+#if defined(CONFIG_BPF_JIT)
+BPF_PROG_TYPE(BPF_PROG_TYPE_STRUCT_OPS, bpf_struct_ops,
+	      void *, void *)
+#endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit 91cc1a99740e2ed1d903b5906afb470cc5a07379
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:15 2019 -0800

    bpf: Annotate context types
    
    Annotate BPF program context types with program-side type and kernel-side type.
    This type information is used by the verifier. btf_get_prog_ctx_type() is
    used in the later patches to verify that BTF type of ctx in BPF program matches to
    kernel expected ctx type. For example, the XDP program type is:
    BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp, struct xdp_md, struct xdp_buff)
    That means that XDP program should be written as:
    int xdp_prog(struct xdp_md *ctx) { ... }
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-16-ast@kernel.org

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index de14872b01ba..93740b3614d7 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -2,42 +2,68 @@
 /* internal file - do not include directly */
 
 #ifdef CONFIG_NET
-BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act)
-BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp,
+	      struct xdp_md, struct xdp_buff)
 #ifdef CONFIG_CGROUP_BPF
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock,
+	      struct bpf_sock, struct sock)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr,
+	      struct bpf_sock_addr, struct bpf_sock_addr_kern)
 #endif
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_in)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_out)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_SEG6LOCAL, lwt_seg6local)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg)
-BPF_PROG_TYPE(BPF_PROG_TYPE_FLOW_DISSECTOR, flow_dissector)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_in,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_out,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_SEG6LOCAL, lwt_seg6local,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops,
+	      struct bpf_sock_ops, struct bpf_sock_ops_kern)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb,
+	      struct __sk_buff, struct sk_buff)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg,
+	      struct sk_msg_md, struct sk_msg)
+BPF_PROG_TYPE(BPF_PROG_TYPE_FLOW_DISSECTOR, flow_dissector,
+	      struct __sk_buff, struct bpf_flow_dissector)
 #endif
 #ifdef CONFIG_BPF_EVENTS
-BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
-BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
-BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
-BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
-BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, raw_tracepoint_writable)
-BPF_PROG_TYPE(BPF_PROG_TYPE_TRACING, tracing)
+BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe,
+	      bpf_user_pt_regs_t, struct pt_regs)
+BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint,
+	      __u64, u64)
+BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event,
+	      struct bpf_perf_event_data, struct bpf_perf_event_data_kern)
+BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint,
+	      struct bpf_raw_tracepoint_args, u64)
+BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, raw_tracepoint_writable,
+	      struct bpf_raw_tracepoint_args, u64)
+BPF_PROG_TYPE(BPF_PROG_TYPE_TRACING, tracing,
+	      void *, void *)
 #endif
 #ifdef CONFIG_CGROUP_BPF
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SYSCTL, cg_sysctl)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCKOPT, cg_sockopt)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev,
+	      struct bpf_cgroup_dev_ctx, struct bpf_cgroup_dev_ctx)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SYSCTL, cg_sysctl,
+	      struct bpf_sysctl, struct bpf_sysctl_kern)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCKOPT, cg_sockopt,
+	      struct bpf_sockopt, struct bpf_sockopt_kern)
 #endif
 #ifdef CONFIG_BPF_LIRC_MODE2
-BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2,
+	      __u32, u32)
 #endif
 #ifdef CONFIG_INET
-BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport,
+	      struct sk_reuseport_md, struct sk_reuseport_kern)
 #endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)

commit f1b9509c2fb0ef4db8d22dac9aef8e856a5d81f6
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Oct 30 15:32:11 2019 -0700

    bpf: Replace prog_raw_tp+btf_id with prog_tracing
    
    The bpf program type raw_tp together with 'expected_attach_type'
    was the most appropriate api to indicate BTF-enabled raw_tp programs.
    But during development it became apparent that 'expected_attach_type'
    cannot be used and new 'attach_btf_id' field had to be introduced.
    Which means that the information is duplicated in two fields where
    one of them is ignored.
    Clean it up by introducing new program type where both
    'expected_attach_type' and 'attach_btf_id' fields have
    specific meaning.
    In the future 'expected_attach_type' will be extended
    with other attach points that have similar semantics to raw_tp.
    This patch is replacing BTF-enabled BPF_PROG_TYPE_RAW_TRACEPOINT with
    prog_type = BPF_RPOG_TYPE_TRACING
    expected_attach_type = BPF_TRACE_RAW_TP
    attach_btf_id = btf_id of raw tracepoint inside the kernel
    Future patches will add
    expected_attach_type = BPF_TRACE_FENTRY or BPF_TRACE_FEXIT
    where programs have the same input context and the same helpers,
    but different attach points.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191030223212.953010-2-ast@kernel.org

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 36a9c2325176..de14872b01ba 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -26,6 +26,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
 BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
 BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
 BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, raw_tracepoint_writable)
+BPF_PROG_TYPE(BPF_PROG_TYPE_TRACING, tracing)
 #endif
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)

commit 6f9d451ab1a33728adb72d7ff66a7b374d665176
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Fri Jul 26 18:06:55 2019 +0200

    xdp: Add devmap_hash map type for looking up devices by hashed index
    
    A common pattern when using xdp_redirect_map() is to create a device map
    where the lookup key is simply ifindex. Because device maps are arrays,
    this leaves holes in the map, and the map has to be sized to fit the
    largest ifindex, regardless of how many devices actually are actually
    needed in the map.
    
    This patch adds a second type of device map where the key is looked up
    using a hashmap, instead of being used as an array index. This allows maps
    to be densely packed, so they can be smaller.
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index eec5aeeeaf92..36a9c2325176 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -62,6 +62,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP_HASH, dev_map_hash_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SK_STORAGE, sk_storage_map_ops)
 #if defined(CONFIG_BPF_STREAM_PARSER)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)

commit 0d01da6afc5402f60325c5da31b22f7d56689b49
Author: Stanislav Fomichev <sdf@google.com>
Date:   Thu Jun 27 13:38:47 2019 -0700

    bpf: implement getsockopt and setsockopt hooks
    
    Implement new BPF_PROG_TYPE_CGROUP_SOCKOPT program type and
    BPF_CGROUP_{G,S}ETSOCKOPT cgroup hooks.
    
    BPF_CGROUP_SETSOCKOPT can modify user setsockopt arguments before
    passing them down to the kernel or bypass kernel completely.
    BPF_CGROUP_GETSOCKOPT can can inspect/modify getsockopt arguments that
    kernel returns.
    Both hooks reuse existing PTR_TO_PACKET{,_END} infrastructure.
    
    The buffer memory is pre-allocated (because I don't think there is
    a precedent for working with __user memory from bpf). This might be
    slow to do for each {s,g}etsockopt call, that's why I've added
    __cgroup_bpf_prog_array_is_empty that exits early if there is nothing
    attached to a cgroup. Note, however, that there is a race between
    __cgroup_bpf_prog_array_is_empty and BPF_PROG_RUN_ARRAY where cgroup
    program layout might have changed; this should not be a problem
    because in general there is a race between multiple calls to
    {s,g}etsocktop and user adding/removing bpf progs from a cgroup.
    
    The return code of the BPF program is handled as follows:
    * 0: EPERM
    * 1: success, continue with next BPF program in the cgroup chain
    
    v9:
    * allow overwriting setsockopt arguments (Alexei Starovoitov):
      * use set_fs (same as kernel_setsockopt)
      * buffer is always kzalloc'd (no small on-stack buffer)
    
    v8:
    * use s32 for optlen (Andrii Nakryiko)
    
    v7:
    * return only 0 or 1 (Alexei Starovoitov)
    * always run all progs (Alexei Starovoitov)
    * use optval=0 as kernel bypass in setsockopt (Alexei Starovoitov)
      (decided to use optval=-1 instead, optval=0 might be a valid input)
    * call getsockopt hook after kernel handlers (Alexei Starovoitov)
    
    v6:
    * rework cgroup chaining; stop as soon as bpf program returns
      0 or 2; see patch with the documentation for the details
    * drop Andrii's and Martin's Acked-by (not sure they are comfortable
      with the new state of things)
    
    v5:
    * skip copy_to_user() and put_user() when ret == 0 (Martin Lau)
    
    v4:
    * don't export bpf_sk_fullsock helper (Martin Lau)
    * size != sizeof(__u64) for uapi pointers (Martin Lau)
    * offsetof instead of bpf_ctx_range when checking ctx access (Martin Lau)
    
    v3:
    * typos in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY comments (Andrii Nakryiko)
    * reverse christmas tree in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY (Andrii
      Nakryiko)
    * use __bpf_md_ptr instead of __u32 for optval{,_end} (Martin Lau)
    * use BPF_FIELD_SIZEOF() for consistency (Martin Lau)
    * new CG_SOCKOPT_ACCESS macro to wrap repeated parts
    
    v2:
    * moved bpf_sockopt_kern fields around to remove a hole (Martin Lau)
    * aligned bpf_sockopt_kern->buf to 8 bytes (Martin Lau)
    * bpf_prog_array_is_empty instead of bpf_prog_array_length (Martin Lau)
    * added [0,2] return code check to verifier (Martin Lau)
    * dropped unused buf[64] from the stack (Martin Lau)
    * use PTR_TO_SOCKET for bpf_sockopt->sk (Martin Lau)
    * dropped bpf_target_off from ctx rewrites (Martin Lau)
    * use return code for kernel bypass (Martin Lau & Andrii Nakryiko)
    
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: Martin Lau <kafai@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 5a9975678d6f..eec5aeeeaf92 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -30,6 +30,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, raw_tracepoint_writable)
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SYSCTL, cg_sysctl)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCKOPT, cg_sockopt)
 #endif
 #ifdef CONFIG_BPF_LIRC_MODE2
 BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)

commit 6ac99e8f23d4b10258406ca0dd7bffca5f31da9d
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Apr 26 16:39:39 2019 -0700

    bpf: Introduce bpf sk local storage
    
    After allowing a bpf prog to
    - directly read the skb->sk ptr
    - get the fullsock bpf_sock by "bpf_sk_fullsock()"
    - get the bpf_tcp_sock by "bpf_tcp_sock()"
    - get the listener sock by "bpf_get_listener_sock()"
    - avoid duplicating the fields of "(bpf_)sock" and "(bpf_)tcp_sock"
      into different bpf running context.
    
    this patch is another effort to make bpf's network programming
    more intuitive to do (together with memory and performance benefit).
    
    When bpf prog needs to store data for a sk, the current practice is to
    define a map with the usual 4-tuples (src/dst ip/port) as the key.
    If multiple bpf progs require to store different sk data, multiple maps
    have to be defined.  Hence, wasting memory to store the duplicated
    keys (i.e. 4 tuples here) in each of the bpf map.
    [ The smallest key could be the sk pointer itself which requires
      some enhancement in the verifier and it is a separate topic. ]
    
    Also, the bpf prog needs to clean up the elem when sk is freed.
    Otherwise, the bpf map will become full and un-usable quickly.
    The sk-free tracking currently could be done during sk state
    transition (e.g. BPF_SOCK_OPS_STATE_CB).
    
    The size of the map needs to be predefined which then usually ended-up
    with an over-provisioned map in production.  Even the map was re-sizable,
    while the sk naturally come and go away already, this potential re-size
    operation is arguably redundant if the data can be directly connected
    to the sk itself instead of proxy-ing through a bpf map.
    
    This patch introduces sk->sk_bpf_storage to provide local storage space
    at sk for bpf prog to use.  The space will be allocated when the first bpf
    prog has created data for this particular sk.
    
    The design optimizes the bpf prog's lookup (and then optionally followed by
    an inline update).  bpf_spin_lock should be used if the inline update needs
    to be protected.
    
    BPF_MAP_TYPE_SK_STORAGE:
    -----------------------
    To define a bpf "sk-local-storage", a BPF_MAP_TYPE_SK_STORAGE map (new in
    this patch) needs to be created.  Multiple BPF_MAP_TYPE_SK_STORAGE maps can
    be created to fit different bpf progs' needs.  The map enforces
    BTF to allow printing the sk-local-storage during a system-wise
    sk dump (e.g. "ss -ta") in the future.
    
    The purpose of a BPF_MAP_TYPE_SK_STORAGE map is not for lookup/update/delete
    a "sk-local-storage" data from a particular sk.
    Think of the map as a meta-data (or "type") of a "sk-local-storage".  This
    particular "type" of "sk-local-storage" data can then be stored in any sk.
    
    The main purposes of this map are mostly:
    1. Define the size of a "sk-local-storage" type.
    2. Provide a similar syscall userspace API as the map (e.g. lookup/update,
       map-id, map-btf...etc.)
    3. Keep track of all sk's storages of this "type" and clean them up
       when the map is freed.
    
    sk->sk_bpf_storage:
    ------------------
    The main lookup/update/delete is done on sk->sk_bpf_storage (which
    is a "struct bpf_sk_storage").  When doing a lookup,
    the "map" pointer is now used as the "key" to search on the
    sk_storage->list.  The "map" pointer is actually serving
    as the "type" of the "sk-local-storage" that is being
    requested.
    
    To allow very fast lookup, it should be as fast as looking up an
    array at a stable-offset.  At the same time, it is not ideal to
    set a hard limit on the number of sk-local-storage "type" that the
    system can have.  Hence, this patch takes a cache approach.
    The last search result from sk_storage->list is cached in
    sk_storage->cache[] which is a stable sized array.  Each
    "sk-local-storage" type has a stable offset to the cache[] array.
    In the future, a map's flag could be introduced to do cache
    opt-out/enforcement if it became necessary.
    
    The cache size is 16 (i.e. 16 types of "sk-local-storage").
    Programs can share map.  On the program side, having a few bpf_progs
    running in the networking hotpath is already a lot.  The bpf_prog
    should have already consolidated the existing sock-key-ed map usage
    to minimize the map lookup penalty.  16 has enough runway to grow.
    
    All sk-local-storage data will be removed from sk->sk_bpf_storage
    during sk destruction.
    
    bpf_sk_storage_get() and bpf_sk_storage_delete():
    ------------------------------------------------
    Instead of using bpf_map_(lookup|update|delete)_elem(),
    the bpf prog needs to use the new helper bpf_sk_storage_get() and
    bpf_sk_storage_delete().  The verifier can then enforce the
    ARG_PTR_TO_SOCKET argument.  The bpf_sk_storage_get() also allows to
    "create" new elem if one does not exist in the sk.  It is done by
    the new BPF_SK_STORAGE_GET_F_CREATE flag.  An optional value can also be
    provided as the initial value during BPF_SK_STORAGE_GET_F_CREATE.
    The BPF_MAP_TYPE_SK_STORAGE also supports bpf_spin_lock.  Together,
    it has eliminated the potential use cases for an equivalent
    bpf_map_update_elem() API (for bpf_prog) in this patch.
    
    Misc notes:
    ----------
    1. map_get_next_key is not supported.  From the userspace syscall
       perspective,  the map has the socket fd as the key while the map
       can be shared by pinned-file or map-id.
    
       Since btf is enforced, the existing "ss" could be enhanced to pretty
       print the local-storage.
    
       Supporting a kernel defined btf with 4 tuples as the return key could
       be explored later also.
    
    2. The sk->sk_lock cannot be acquired.  Atomic operations is used instead.
       e.g. cmpxchg is done on the sk->sk_bpf_storage ptr.
       Please refer to the source code comments for the details in
       synchronization cases and considerations.
    
    3. The mem is charged to the sk->sk_omem_alloc as the sk filter does.
    
    Benchmark:
    ---------
    Here is the benchmark data collected by turning on
    the "kernel.bpf_stats_enabled" sysctl.
    Two bpf progs are tested:
    
    One bpf prog with the usual bpf hashmap (max_entries = 8192) with the
    sk ptr as the key. (verifier is modified to support sk ptr as the key
    That should have shortened the key lookup time.)
    
    Another bpf prog is with the new BPF_MAP_TYPE_SK_STORAGE.
    
    Both are storing a "u32 cnt", do a lookup on "egress_skb/cgroup" for
    each egress skb and then bump the cnt.  netperf is used to drive
    data with 4096 connected UDP sockets.
    
    BPF_MAP_TYPE_HASH with a modifier verifier (152ns per bpf run)
    27: cgroup_skb  name egress_sk_map  tag 74f56e832918070b run_time_ns 58280107540 run_cnt 381347633
        loaded_at 2019-04-15T13:46:39-0700  uid 0
        xlated 344B  jited 258B  memlock 4096B  map_ids 16
        btf_id 5
    
    BPF_MAP_TYPE_SK_STORAGE in this patch (66ns per bpf run)
    30: cgroup_skb  name egress_sk_stora  tag d4aa70984cc7bbf6 run_time_ns 25617093319 run_cnt 390989739
        loaded_at 2019-04-15T13:47:54-0700  uid 0
        xlated 168B  jited 156B  memlock 4096B  map_ids 17
        btf_id 6
    
    Here is a high-level picture on how are the objects organized:
    
           sk
        ┌──────┐
        │      │
        │      │
        │      │
        │*sk_bpf_storage─────▶ bpf_sk_storage
        └──────┘                 ┌───────┐
                     ┌───────────┤ list  │
                     │           │       │
                     │           │       │
                     │           │       │
                     │           └───────┘
                     │
                     │     elem
                     │  ┌────────┐
                     ├─▶│ snode  │
                     │  ├────────┤
                     │  │  data  │          bpf_map
                     │  ├────────┤        ┌─────────┐
                     │  │map_node│◀─┬─────┤  list   │
                     │  └────────┘  │     │         │
                     │              │     │         │
                     │     elem     │     │         │
                     │  ┌────────┐  │     └─────────┘
                     └─▶│ snode  │  │
                        ├────────┤  │
       bpf_map          │  data  │  │
     ┌─────────┐        ├────────┤  │
     │  list   ├───────▶│map_node│  │
     │         │        └────────┘  │
     │         │                    │
     │         │           elem     │
     └─────────┘        ┌────────┐  │
                     ┌─▶│ snode  │  │
                     │  ├────────┤  │
                     │  │  data  │  │
                     │  ├────────┤  │
                     │  │map_node│◀─┘
                     │  └────────┘
                     │
                     │
                     │          ┌───────┐
         sk          └──────────│ list  │
      ┌──────┐                  │       │
      │      │                  │       │
      │      │                  │       │
      │      │                  └───────┘
      │*sk_bpf_storage───────▶bpf_sk_storage
      └──────┘
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index a10d37bce364..5a9975678d6f 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -61,6 +61,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_SK_STORAGE, sk_storage_map_ops)
 #if defined(CONFIG_BPF_STREAM_PARSER)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKHASH, sock_hash_ops)

commit 9df1c28bb75217b244257152ab7d788bb2a386d0
Author: Matt Mullins <mmullins@fb.com>
Date:   Fri Apr 26 11:49:47 2019 -0700

    bpf: add writable context for raw tracepoints
    
    This is an opt-in interface that allows a tracepoint to provide a safe
    buffer that can be written from a BPF_PROG_TYPE_RAW_TRACEPOINT program.
    The size of the buffer must be a compile-time constant, and is checked
    before allowing a BPF program to attach to a tracepoint that uses this
    feature.
    
    The pointer to this buffer will be the first argument of tracepoints
    that opt in; the pointer is valid and can be bpf_probe_read() by both
    BPF_PROG_TYPE_RAW_TRACEPOINT and BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE
    programs that attach to such a tracepoint, but the buffer to which it
    points may only be written by the latter.
    
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index d26991a16894..a10d37bce364 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -25,6 +25,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
 BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
 BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
 BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
+BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE, raw_tracepoint_writable)
 #endif
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)

commit 7b146cebe30cb481b0f70d85779da938da818637
Author: Andrey Ignatov <rdna@fb.com>
Date:   Wed Feb 27 12:59:24 2019 -0800

    bpf: Sysctl hook
    
    Containerized applications may run as root and it may create problems
    for whole host. Specifically such applications may change a sysctl and
    affect applications in other containers.
    
    Furthermore in existing infrastructure it may not be possible to just
    completely disable writing to sysctl, instead such a process should be
    gradual with ability to log what sysctl are being changed by a
    container, investigate, limit the set of writable sysctl to currently
    used ones (so that new ones can not be changed) and eventually reduce
    this set to zero.
    
    The patch introduces new program type BPF_PROG_TYPE_CGROUP_SYSCTL and
    attach type BPF_CGROUP_SYSCTL to solve these problems on cgroup basis.
    
    New program type has access to following minimal context:
            struct bpf_sysctl {
                    __u32   write;
            };
    
    Where @write indicates whether sysctl is being read (= 0) or written (=
    1).
    
    Helpers to access sysctl name and value will be introduced separately.
    
    BPF_CGROUP_SYSCTL attach point is added to sysctl code right before
    passing control to ctl_table->proc_handler so that BPF program can
    either allow or deny access to sysctl.
    
    Suggested-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 08bf2f1fe553..d26991a16894 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -28,6 +28,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
 #endif
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SYSCTL, cg_sysctl)
 #endif
 #ifdef CONFIG_BPF_LIRC_MODE2
 BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)

commit befa618112a0a4590ce21d70aa35c9d341337774
Author: Stanislav Fomichev <sdf@google.com>
Date:   Mon Jan 28 09:21:19 2019 -0800

    bpf: BPF_PROG_TYPE_CGROUP_{SKB, SOCK, SOCK_ADDR} require cgroups enabled
    
    There is no way to exercise appropriate attach points without cgroups
    enabled. This lets test_verifier correctly skip tests for these
    prog_types if kernel was compiled without BPF cgroup support.
    
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 44d9ab4809bd..08bf2f1fe553 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -6,9 +6,11 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act)
 BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
+#ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
+#endif
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_in)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_out)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)

commit f1a2e44a3aeccb3ff18d3ccc0b0203e70b95bd92
Author: Mauricio Vasquez B <mauricio.vasquez@polito.it>
Date:   Thu Oct 18 15:16:25 2018 +0200

    bpf: add queue and stack maps
    
    Queue/stack maps implement a FIFO/LIFO data storage for ebpf programs.
    These maps support peek, pop and push operations that are exposed to eBPF
    programs through the new bpf_map[peek/pop/push] helpers.  Those operations
    are exposed to userspace applications through the already existing
    syscalls in the following way:
    
    BPF_MAP_LOOKUP_ELEM            -> peek
    BPF_MAP_LOOKUP_AND_DELETE_ELEM -> pop
    BPF_MAP_UPDATE_ELEM            -> push
    
    Queue/stack maps are implemented using a buffer, tail and head indexes,
    hence BPF_F_NO_PREALLOC is not supported.
    
    As opposite to other maps, queue and stack do not use RCU for protecting
    maps values, the bpf_map[peek/pop] have a ARG_PTR_TO_UNINIT_MAP_VALUE
    argument that is a pointer to a memory zone where to save the value of a
    map.  Basically the same as ARG_PTR_TO_UNINIT_MEM, but the size has not
    be passed as an extra argument.
    
    Our main motivation for implementing queue/stack maps was to keep track
    of a pool of elements, like network ports in a SNAT, however we forsee
    other use cases, like for exampling saving last N kernel events in a map
    and then analysing from userspace.
    
    Signed-off-by: Mauricio Vasquez B <mauricio.vasquez@polito.it>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 7bad4e1947ed..44d9ab4809bd 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -69,3 +69,5 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_XSKMAP, xsk_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, reuseport_array_ops)
 #endif
 #endif
+BPF_MAP_TYPE(BPF_MAP_TYPE_QUEUE, queue_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_STACK, stack_map_ops)

commit 144991602e6a14d667b295f1b099e609ce857772
Author: Mauricio Vasquez B <mauricio.vasquez@polito.it>
Date:   Thu Oct 18 15:16:09 2018 +0200

    bpf: rename stack trace map operations
    
    In the following patches queue and stack maps (FIFO and LIFO
    datastructures) will be implemented.  In order to avoid confusion and
    a possible name clash rename stack_map_ops to stack_trace_map_ops
    
    Signed-off-by: Mauricio Vasquez B <mauricio.vasquez@polito.it>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index fa48343a5ea1..7bad4e1947ed 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -51,7 +51,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_LRU_HASH, htab_lru_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_LRU_PERCPU_HASH, htab_lru_percpu_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_LPM_TRIE, trie_map_ops)
 #ifdef CONFIG_PERF_EVENTS
-BPF_MAP_TYPE(BPF_MAP_TYPE_STACK_TRACE, stack_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_STACK_TRACE, stack_trace_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)

commit 604326b41a6fb9b4a78b6179335decee0365cd8c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:58 2018 +0200

    bpf, sockmap: convert to generic sk_msg interface
    
    Add a generic sk_msg layer, and convert current sockmap and later
    kTLS over to make use of it. While sk_buff handles network packet
    representation from netdevice up to socket, sk_msg handles data
    representation from application to socket layer.
    
    This means that sk_msg framework spans across ULP users in the
    kernel, and enables features such as introspection or filtering
    of data with the help of BPF programs that operate on this data
    structure.
    
    Latter becomes in particular useful for kTLS where data encryption
    is deferred into the kernel, and as such enabling the kernel to
    perform L7 introspection and policy based on BPF for TLS connections
    where the record is being encrypted after BPF has run and came to
    a verdict. In order to get there, first step is to transform open
    coding of scatter-gather list handling into a common core framework
    that subsystems can use.
    
    The code itself has been split and refactored into three bigger
    pieces: i) the generic sk_msg API which deals with managing the
    scatter gather ring, providing helpers for walking and mangling,
    transferring application data from user space into it, and preparing
    it for BPF pre/post-processing, ii) the plain sock map itself
    where sockets can be attached to or detached from; these bits
    are independent of i) which can now be used also without sock
    map, and iii) the integration with plain TCP as one protocol
    to be used for processing L7 application data (later this could
    e.g. also be extended to other protocols like UDP). The semantics
    are the same with the old sock map code and therefore no change
    of user facing behavior or APIs. While pursuing this work it
    also helped finding a number of bugs in the old sockmap code
    that we've fixed already in earlier commits. The test_sockmap
    kselftest suite passes through fine as well.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 5432f4c9f50e..fa48343a5ea1 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -57,7 +57,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
-#if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_INET)
+#if defined(CONFIG_BPF_STREAM_PARSER)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKHASH, sock_hash_ops)
 #endif

commit b741f1630346defcbc8cc60f1a2bdae8b3b0036f
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Sep 28 14:45:43 2018 +0000

    bpf: introduce per-cpu cgroup local storage
    
    This commit introduced per-cpu cgroup local storage.
    
    Per-cpu cgroup local storage is very similar to simple cgroup storage
    (let's call it shared), except all the data is per-cpu.
    
    The main goal of per-cpu variant is to implement super fast
    counters (e.g. packet counters), which don't require neither
    lookups, neither atomic operations.
    
    >From userspace's point of view, accessing a per-cpu cgroup storage
    is similar to other per-cpu map types (e.g. per-cpu hashmaps and
    arrays).
    
    Writing to a per-cpu cgroup storage is not atomic, but is performed
    by copying longs, so some minimal atomicity is here, exactly
    as with other per-cpu maps.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index c9bd6fb765b0..5432f4c9f50e 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -43,6 +43,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_CGROUP_ARRAY, cgroup_array_map_ops)
 #endif
 #ifdef CONFIG_CGROUP_BPF
 BPF_MAP_TYPE(BPF_MAP_TYPE_CGROUP_STORAGE, cgroup_storage_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE, cgroup_storage_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH, htab_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_HASH, htab_percpu_map_ops)

commit 2dfd184abd38fd72d80715fa8b00c9de78490200
Author: Willem de Bruijn <willemb@google.com>
Date:   Tue Sep 18 16:20:18 2018 -0400

    flow_dissector: fix build failure without CONFIG_NET
    
    If boolean CONFIG_BPF_SYSCALL is enabled, kernel/bpf/syscall.c will
    call flow_dissector functions from net/core/flow_dissector.c.
    
    This causes this build failure if CONFIG_NET is disabled:
    
        kernel/bpf/syscall.o: In function `__x64_sys_bpf':
        syscall.c:(.text+0x3278): undefined reference to
        `skb_flow_dissector_bpf_prog_attach'
        syscall.c:(.text+0x3310): undefined reference to
        `skb_flow_dissector_bpf_prog_detach'
        kernel/bpf/syscall.o:(.rodata+0x3f0): undefined reference to
        `flow_dissector_prog_ops'
        kernel/bpf/verifier.o:(.rodata+0x250): undefined reference to
        `flow_dissector_verifier_ops'
    
    Analogous to other optional BPF program types in syscall.c, add stubs
    if the relevant functions are not compiled and move the BPF_PROG_TYPE
    definition in the #ifdef CONFIG_NET block.
    
    Fixes: d58e468b1112 ("flow_dissector: implements flow dissector BPF hook")
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org> # build-tested
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 22083712dd18..c9bd6fb765b0 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -16,6 +16,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_SEG6LOCAL, lwt_seg6local)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg)
+BPF_PROG_TYPE(BPF_PROG_TYPE_FLOW_DISSECTOR, flow_dissector)
 #endif
 #ifdef CONFIG_BPF_EVENTS
 BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
@@ -32,7 +33,6 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)
 #ifdef CONFIG_INET
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport)
 #endif
-BPF_PROG_TYPE(BPF_PROG_TYPE_FLOW_DISSECTOR, flow_dissector)
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit d58e468b1112dcd1d5193c0a89ff9f98b5a3e8b9
Author: Petar Penkov <ppenkov@google.com>
Date:   Fri Sep 14 07:46:18 2018 -0700

    flow_dissector: implements flow dissector BPF hook
    
    Adds a hook for programs of type BPF_PROG_TYPE_FLOW_DISSECTOR and
    attach type BPF_FLOW_DISSECTOR that is executed in the flow dissector
    path. The BPF program is per-network namespace.
    
    Signed-off-by: Petar Penkov <ppenkov@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index cd26c090e7c0..22083712dd18 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -32,6 +32,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)
 #ifdef CONFIG_INET
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport)
 #endif
+BPF_PROG_TYPE(BPF_PROG_TYPE_FLOW_DISSECTOR, flow_dissector)
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit 2dbb9b9e6df67d444fbe425c7f6014858d337adf
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:25 2018 -0700

    bpf: Introduce BPF_PROG_TYPE_SK_REUSEPORT
    
    This patch adds a BPF_PROG_TYPE_SK_REUSEPORT which can select
    a SO_REUSEPORT sk from a BPF_MAP_TYPE_REUSEPORT_ARRAY.  Like other
    non SK_FILTER/CGROUP_SKB program, it requires CAP_SYS_ADMIN.
    
    BPF_PROG_TYPE_SK_REUSEPORT introduces "struct sk_reuseport_kern"
    to store the bpf context instead of using the skb->cb[48].
    
    At the SO_REUSEPORT sk lookup time, it is in the middle of transiting
    from a lower layer (ipv4/ipv6) to a upper layer (udp/tcp).  At this
    point,  it is not always clear where the bpf context can be appended
    in the skb->cb[48] to avoid saving-and-restoring cb[].  Even putting
    aside the difference between ipv4-vs-ipv6 and udp-vs-tcp.  It is not
    clear if the lower layer is only ipv4 and ipv6 in the future and
    will it not touch the cb[] again before transiting to the upper
    layer.
    
    For example, in udp_gro_receive(), it uses the 48 byte NAPI_GRO_CB
    instead of IP[6]CB and it may still modify the cb[] after calling
    the udp[46]_lib_lookup_skb().  Because of the above reason, if
    sk->cb is used for the bpf ctx, saving-and-restoring is needed
    and likely the whole 48 bytes cb[] has to be saved and restored.
    
    Instead of saving, setting and restoring the cb[], this patch opts
    to create a new "struct sk_reuseport_kern" and setting the needed
    values in there.
    
    The new BPF_PROG_TYPE_SK_REUSEPORT and "struct sk_reuseport_(kern|md)"
    will serve all ipv4/ipv6 + udp/tcp combinations.  There is no protocol
    specific usage at this point and it is also inline with the current
    sock_reuseport.c implementation (i.e. no protocol specific requirement).
    
    In "struct sk_reuseport_md", this patch exposes data/data_end/len
    with semantic similar to other existing usages.  Together
    with "bpf_skb_load_bytes()" and "bpf_skb_load_bytes_relative()",
    the bpf prog can peek anywhere in the skb.  The "bind_inany" tells
    the bpf prog that the reuseport group is bind-ed to a local
    INANY address which cannot be learned from skb.
    
    The new "bind_inany" is added to "struct sock_reuseport" which will be
    used when running the new "BPF_PROG_TYPE_SK_REUSEPORT" bpf prog in order
    to avoid repeating the "bind INANY" test on
    "sk_v6_rcv_saddr/sk->sk_rcv_saddr" every time a bpf prog is run.  It can
    only be properly initialized when a "sk->sk_reuseport" enabled sk is
    adding to a hashtable (i.e. during "reuseport_alloc()" and
    "reuseport_add_sock()").
    
    The new "sk_select_reuseport()" is the main helper that the
    bpf prog will use to select a SO_REUSEPORT sk.  It is the only function
    that can use the new BPF_MAP_TYPE_REUSEPORT_ARRAY.  As mentioned in
    the earlier patch, the validity of a selected sk is checked in
    run time in "sk_select_reuseport()".  Doing the check in
    verification time is difficult and inflexible (consider the map-in-map
    use case).  The runtime check is to compare the selected sk's reuseport_id
    with the reuseport_id that we want.  This helper will return -EXXX if the
    selected sk cannot serve the incoming request (e.g. reuseport_id
    not match).  The bpf prog can decide if it wants to do SK_DROP as its
    discretion.
    
    When the bpf prog returns SK_PASS, the kernel will check if a
    valid sk has been selected (i.e. "reuse_kern->selected_sk != NULL").
    If it does , it will use the selected sk.  If not, the kernel
    will select one from "reuse->socks[]" (as before this patch).
    
    The SK_DROP and SK_PASS handling logic will be in the next patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 14fd6c02d258..cd26c090e7c0 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -29,6 +29,9 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
 #ifdef CONFIG_BPF_LIRC_MODE2
 BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)
 #endif
+#ifdef CONFIG_INET
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport)
+#endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit 5dc4c4b7d4e8115e7cde96a030f98cb3ab2e458c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:24 2018 -0700

    bpf: Introduce BPF_MAP_TYPE_REUSEPORT_SOCKARRAY
    
    This patch introduces a new map type BPF_MAP_TYPE_REUSEPORT_SOCKARRAY.
    
    To unleash the full potential of a bpf prog, it is essential for the
    userspace to be capable of directly setting up a bpf map which can then
    be consumed by the bpf prog to make decision.  In this case, decide which
    SO_REUSEPORT sk to serve the incoming request.
    
    By adding BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, the userspace has total control
    and visibility on where a SO_REUSEPORT sk should be located in a bpf map.
    The later patch will introduce BPF_PROG_TYPE_SK_REUSEPORT such that
    the bpf prog can directly select a sk from the bpf map.  That will
    raise the programmability of the bpf prog attached to a reuseport
    group (a group of sk serving the same IP:PORT).
    
    For example, in UDP, the bpf prog can peek into the payload (e.g.
    through the "data" pointer introduced in the later patch) to learn
    the application level's connection information and then decide which sk
    to pick from a bpf map.  The userspace can tightly couple the sk's location
    in a bpf map with the application logic in generating the UDP payload's
    connection information.  This connection info contact/API stays within the
    userspace.
    
    Also, when used with map-in-map, the userspace can switch the
    old-server-process's inner map to a new-server-process's inner map
    in one call "bpf_map_update_elem(outer_map, &index, &new_reuseport_array)".
    The bpf prog will then direct incoming requests to the new process instead
    of the old process.  The old process can finish draining the pending
    requests (e.g. by "accept()") before closing the old-fds.  [Note that
    deleting a fd from a bpf map does not necessary mean the fd is closed]
    
    During map_update_elem(),
    Only SO_REUSEPORT sk (i.e. which has already been added
    to a reuse->socks[]) can be used.  That means a SO_REUSEPORT sk that is
    "bind()" for UDP or "bind()+listen()" for TCP.  These conditions are
    ensured in "reuseport_array_update_check()".
    
    A SO_REUSEPORT sk can only be added once to a map (i.e. the
    same sk cannot be added twice even to the same map).  SO_REUSEPORT
    already allows another sk to be created for the same IP:PORT.
    There is no need to re-create a similar usage in the BPF side.
    
    When a SO_REUSEPORT is deleted from the "reuse->socks[]" (e.g. "close()"),
    it will notify the bpf map to remove it from the map also.  It is
    done through "bpf_sk_reuseport_detach()" and it will only be called
    if >=1 of the "reuse->sock[]" has ever been added to a bpf map.
    
    The map_update()/map_delete() has to be in-sync with the
    "reuse->socks[]".  Hence, the same "reuseport_lock" used
    by "reuse->socks[]" has to be used here also. Care has
    been taken to ensure the lock is only acquired when the
    adding sk passes some strict tests. and
    freeing the map does not require the reuseport_lock.
    
    The reuseport_array will also support lookup from the syscall
    side.  It will return a sock_gen_cookie().  The sock_gen_cookie()
    is on-demand (i.e. a sk's cookie is not generated until the very
    first map_lookup_elem()).
    
    The lookup cookie is 64bits but it goes against the logical userspace
    expectation on 32bits sizeof(fd) (and as other fd based bpf maps do also).
    It may catch user in surprise if we enforce value_size=8 while
    userspace still pass a 32bits fd during update.  Supporting different
    value_size between lookup and update seems unintuitive also.
    
    We also need to consider what if other existing fd based maps want
    to return 64bits value from syscall's lookup in the future.
    Hence, reuseport_array supports both value_size 4 and 8, and
    assuming user will usually use value_size=4.  The syscall's lookup
    will return ENOSPC on value_size=4.  It will will only
    return 64bits value from sock_gen_cookie() when user consciously
    choose value_size=8 (as a signal that lookup is desired) which then
    requires a 64bits value in both lookup and update.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index add08be53b6f..14fd6c02d258 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -60,4 +60,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)
 #if defined(CONFIG_XDP_SOCKETS)
 BPF_MAP_TYPE(BPF_MAP_TYPE_XSKMAP, xsk_map_ops)
 #endif
+#ifdef CONFIG_INET
+BPF_MAP_TYPE(BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, reuseport_array_ops)
+#endif
 #endif

commit de9cbbaadba5adf88a19e46df61f7054000838f6
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Aug 2 14:27:18 2018 -0700

    bpf: introduce cgroup storage maps
    
    This commit introduces BPF_MAP_TYPE_CGROUP_STORAGE maps:
    a special type of maps which are implementing the cgroup storage.
    
    >From the userspace point of view it's almost a generic
    hash map with the (cgroup inode id, attachment type) pair
    used as a key.
    
    The only difference is that some operations are restricted:
      1) a user can't create new entries,
      2) a user can't remove existing entries.
    
    The lookup from userspace is o(log(n)).
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index c5700c2d5549..add08be53b6f 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -37,6 +37,9 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_PERF_EVENT_ARRAY, perf_event_array_map_ops)
 #ifdef CONFIG_CGROUPS
 BPF_MAP_TYPE(BPF_MAP_TYPE_CGROUP_ARRAY, cgroup_array_map_ops)
 #endif
+#ifdef CONFIG_CGROUP_BPF
+BPF_MAP_TYPE(BPF_MAP_TYPE_CGROUP_STORAGE, cgroup_storage_map_ops)
+#endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH, htab_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_HASH, htab_percpu_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_LRU_HASH, htab_lru_map_ops)

commit f4364dcfc86df7c1ca47b256eaf6b6d0cdd0d936
Author: Sean Young <sean@mess.org>
Date:   Sun May 27 12:24:09 2018 +0100

    media: rc: introduce BPF_PROG_LIRC_MODE2
    
    Add support for BPF_PROG_LIRC_MODE2. This type of BPF program can call
    rc_keydown() to reported decoded IR scancodes, or rc_repeat() to report
    that the last key should be repeated.
    
    The bpf program can be attached to using the bpf(BPF_PROG_ATTACH) syscall;
    the target_fd must be the /dev/lircN device.
    
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Sean Young <sean@mess.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index b161e506dcfc..c5700c2d5549 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -26,6 +26,9 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
 #endif
+#ifdef CONFIG_BPF_LIRC_MODE2
+BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2)
+#endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit 004d4b274e2a1a895a0e5dc66158b90a7d463d44
Author: Mathieu Xhonneux <m.xhonneux@gmail.com>
Date:   Sun May 20 14:58:16 2018 +0100

    ipv6: sr: Add seg6local action End.BPF
    
    This patch adds the End.BPF action to the LWT seg6local infrastructure.
    This action works like any other seg6local End action, meaning that an IPv6
    header with SRH is needed, whose DA has to be equal to the SID of the
    action. It will also advance the SRH to the next segment, the BPF program
    does not have to take care of this.
    
    Since the BPF program may not be a source of instability in the kernel, it
    is important to ensure that the integrity of the packet is maintained
    before yielding it back to the IPv6 layer. The hook hence keeps track if
    the SRH has been altered through the helpers, and re-validates its
    content if needed with seg6_validate_srh. The state kept for validation is
    stored in a per-CPU buffer. The BPF program is not allowed to directly
    write into the packet, and only some fields of the SRH can be altered
    through the helper bpf_lwt_seg6_store_bytes.
    
    Performances profiling has shown that the SRH re-validation does not induce
    a significant overhead. If the altered SRH is deemed as invalid, the packet
    is dropped.
    
    This validation is also done before executing any action through
    bpf_lwt_seg6_action, and will not be performed again if the SRH is not
    modified after calling the action.
    
    The BPF program may return 3 types of return codes:
        - BPF_OK: the End.BPF action will look up the next destination through
                 seg6_lookup_nexthop.
        - BPF_REDIRECT: if an action has been executed through the
              bpf_lwt_seg6_action helper, the BPF program should return this
              value, as the skb's destination is already set and the default
              lookup should not be performed.
        - BPF_DROP : the packet will be dropped.
    
    Signed-off-by: Mathieu Xhonneux <m.xhonneux@gmail.com>
    Acked-by: David Lebrun <dlebrun@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index aa5c8b878474..b161e506dcfc 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -12,6 +12,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_in)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_out)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_SEG6LOCAL, lwt_seg6local)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg)

commit cd3092c7f8db8c320ea7f1aa7c1adeac2450f43a
Author: Mathieu Xhonneux <m.xhonneux@gmail.com>
Date:   Sun May 20 14:58:15 2018 +0100

    bpf: Split lwt inout verifier structures
    
    The new bpf_lwt_push_encap helper should only be accessible within the
    LWT BPF IN hook, and not the OUT one, as this may lead to a skb under
    panic.
    
    At the moment, both LWT BPF IN and OUT share the same list of helpers,
    whose calls are authorized by the verifier. This patch separates the
    verifier ops for the IN and OUT hooks, and allows the IN hook to call the
    bpf_lwt_push_encap helper.
    
    This patch is also the occasion to put all lwt_*_func_proto functions
    together for clarity. At the moment, socks_op_func_proto is in the middle
    of lwt_inout_func_proto and lwt_xmit_func_proto.
    
    Signed-off-by: Mathieu Xhonneux <m.xhonneux@gmail.com>
    Acked-by: David Lebrun <dlebrun@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index b67f8793de0d..aa5c8b878474 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -9,8 +9,8 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_in)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_out)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)

commit 81110384441a59cff47430f20f049e69b98c17f4
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon May 14 10:00:17 2018 -0700

    bpf: sockmap, add hash map support
    
    Sockmap is currently backed by an array and enforces keys to be
    four bytes. This works well for many use cases and was originally
    modeled after devmap which also uses four bytes keys. However,
    this has become limiting in larger use cases where a hash would
    be more appropriate. For example users may want to use the 5-tuple
    of the socket as the lookup key.
    
    To support this add hash support.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index d7df1b323082..b67f8793de0d 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -47,6 +47,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
 #if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_INET)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKHASH, sock_hash_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)
 #if defined(CONFIG_XDP_SOCKETS)

commit fbfc504a24f53f7ebe128ab55cb5dba634f4ece8
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 2 13:01:28 2018 +0200

    bpf: introduce new bpf AF_XDP map type BPF_MAP_TYPE_XSKMAP
    
    The xskmap is yet another BPF map, very much inspired by
    dev/cpu/sockmap, and is a holder of AF_XDP sockets. A user application
    adds AF_XDP sockets into the map, and by using the bpf_redirect_map
    helper, an XDP program can redirect XDP frames to an AF_XDP socket.
    
    Note that a socket that is bound to certain ifindex/queue index will
    *only* accept XDP frames from that netdev/queue index. If an XDP
    program tries to redirect from a netdev/queue index other than what
    the socket is bound to, the frame will not be received on the socket.
    
    A socket can reside in multiple maps.
    
    v3: Fixed race and simplified code.
    v2: Removed one indirection in map lookup.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 2b28fcf6f6ae..d7df1b323082 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -49,4 +49,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)
+#if defined(CONFIG_XDP_SOCKETS)
+BPF_MAP_TYPE(BPF_MAP_TYPE_XSKMAP, xsk_map_ops)
+#endif
 #endif

commit 4fbac77d2d092b475dda9eea66da674369665427
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:02 2018 -0700

    bpf: Hooks for sys_bind
    
    == The problem ==
    
    There is a use-case when all processes inside a cgroup should use one
    single IP address on a host that has multiple IP configured.  Those
    processes should use the IP for both ingress and egress, for TCP and UDP
    traffic. So TCP/UDP servers should be bound to that IP to accept
    incoming connections on it, and TCP/UDP clients should make outgoing
    connections from that IP. It should not require changing application
    code since it's often not possible.
    
    Currently it's solved by intercepting glibc wrappers around syscalls
    such as `bind(2)` and `connect(2)`. It's done by a shared library that
    is preloaded for every process in a cgroup so that whenever TCP/UDP
    server calls `bind(2)`, the library replaces IP in sockaddr before
    passing arguments to syscall. When application calls `connect(2)` the
    library transparently binds the local end of connection to that IP
    (`bind(2)` with `IP_BIND_ADDRESS_NO_PORT` to avoid performance penalty).
    
    Shared library approach is fragile though, e.g.:
    * some applications clear env vars (incl. `LD_PRELOAD`);
    * `/etc/ld.so.preload` doesn't help since some applications are linked
      with option `-z nodefaultlib`;
    * other applications don't use glibc and there is nothing to intercept.
    
    == The solution ==
    
    The patch provides much more reliable in-kernel solution for the 1st
    part of the problem: binding TCP/UDP servers on desired IP. It does not
    depend on application environment and implementation details (whether
    glibc is used or not).
    
    It adds new eBPF program type `BPF_PROG_TYPE_CGROUP_SOCK_ADDR` and
    attach types `BPF_CGROUP_INET4_BIND` and `BPF_CGROUP_INET6_BIND`
    (similar to already existing `BPF_CGROUP_INET_SOCK_CREATE`).
    
    The new program type is intended to be used with sockets (`struct sock`)
    in a cgroup and provided by user `struct sockaddr`. Pointers to both of
    them are parts of the context passed to programs of newly added types.
    
    The new attach types provides hooks in `bind(2)` system call for both
    IPv4 and IPv6 so that one can write a program to override IP addresses
    and ports user program tries to bind to and apply such a program for
    whole cgroup.
    
    == Implementation notes ==
    
    [1]
    Separate attach types for `AF_INET` and `AF_INET6` are added
    intentionally to prevent reading/writing to offsets that don't make
    sense for corresponding socket family. E.g. if user passes `sockaddr_in`
    it doesn't make sense to read from / write to `user_ip6[]` context
    fields.
    
    [2]
    The write access to `struct bpf_sock_addr_kern` is implemented using
    special field as an additional "register".
    
    There are just two registers in `sock_addr_convert_ctx_access`: `src`
    with value to write and `dst` with pointer to context that can't be
    changed not to break later instructions. But the fields, allowed to
    write to, are not available directly and to access them address of
    corresponding pointer has to be loaded first. To get additional register
    the 1st not used by `src` and `dst` one is taken, its content is saved
    to `bpf_sock_addr_kern.tmp_reg`, then the register is used to load
    address of pointer field, and finally the register's content is restored
    from the temporary field after writing `src` value.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 6d7243bfb0ff..2b28fcf6f6ae 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -8,6 +8,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act)
 BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK_ADDR, cg_sock_addr)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)

commit c4f6699dfcb8558d138fe838f741b2c10f416cf9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Mar 28 12:05:37 2018 -0700

    bpf: introduce BPF_RAW_TRACEPOINT
    
    Introduce BPF_PROG_TYPE_RAW_TRACEPOINT bpf program type to access
    kernel internal arguments of the tracepoints in their raw form.
    
    >From bpf program point of view the access to the arguments look like:
    struct bpf_raw_tracepoint_args {
           __u64 args[0];
    };
    
    int bpf_prog(struct bpf_raw_tracepoint_args *ctx)
    {
      // program can read args[N] where N depends on tracepoint
      // and statically verified at program load+attach time
    }
    
    kprobe+bpf infrastructure allows programs access function arguments.
    This feature allows programs access raw tracepoint arguments.
    
    Similar to proposed 'dynamic ftrace events' there are no abi guarantees
    to what the tracepoints arguments are and what their meaning is.
    The program needs to type cast args properly and use bpf_probe_read()
    helper to access struct fields when argument is a pointer.
    
    For every tracepoint __bpf_trace_##call function is prepared.
    In assembler it looks like:
    (gdb) disassemble __bpf_trace_xdp_exception
    Dump of assembler code for function __bpf_trace_xdp_exception:
       0xffffffff81132080 <+0>:     mov    %ecx,%ecx
       0xffffffff81132082 <+2>:     jmpq   0xffffffff811231f0 <bpf_trace_run3>
    
    where
    
    TRACE_EVENT(xdp_exception,
            TP_PROTO(const struct net_device *dev,
                     const struct bpf_prog *xdp, u32 act),
    
    The above assembler snippet is casting 32-bit 'act' field into 'u64'
    to pass into bpf_trace_run3(), while 'dev' and 'xdp' args are passed as-is.
    All of ~500 of __bpf_trace_*() functions are only 5-10 byte long
    and in total this approach adds 7k bytes to .text.
    
    This approach gives the lowest possible overhead
    while calling trace_xdp_exception() from kernel C code and
    transitioning into bpf land.
    Since tracepoint+bpf are used at speeds of 1M+ events per second
    this is valuable optimization.
    
    The new BPF_RAW_TRACEPOINT_OPEN sys_bpf command is introduced
    that returns anon_inode FD of 'bpf-raw-tracepoint' object.
    
    The user space looks like:
    // load bpf prog with BPF_PROG_TYPE_RAW_TRACEPOINT type
    prog_fd = bpf_prog_load(...);
    // receive anon_inode fd for given bpf_raw_tracepoint with prog attached
    raw_tp_fd = bpf_raw_tracepoint_open("xdp_exception", prog_fd);
    
    Ctrl-C of tracing daemon or cmdline tool that uses this feature
    will automatically detach bpf program, unload it and
    unregister tracepoint probe.
    
    On the kernel side the __bpf_raw_tp_map section of pointers to
    tracepoint definition and to __bpf_trace_*() probe function is used
    to find a tracepoint with "xdp_exception" name and
    corresponding __bpf_trace_xdp_exception() probe function
    which are passed to tracepoint_probe_register() to connect probe
    with tracepoint.
    
    Addition of bpf_raw_tracepoint doesn't interfere with ftrace and perf
    tracepoint mechanisms. perf_event_open() can be used in parallel
    on the same tracepoint.
    Multiple bpf_raw_tracepoint_open("xdp_exception", prog_fd) are permitted.
    Each with its own bpf program. The kernel will execute
    all tracepoint probes and all attached bpf programs.
    
    In the future bpf_raw_tracepoints can be extended with
    query/introspection logic.
    
    __bpf_raw_tp_map section logic was contributed by Steven Rostedt
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 5e2e8a49fb21..6d7243bfb0ff 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -19,6 +19,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg)
 BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
 BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
 BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
+BPF_PROG_TYPE(BPF_PROG_TYPE_RAW_TRACEPOINT, raw_tracepoint)
 #endif
 #ifdef CONFIG_CGROUP_BPF
 BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)

commit 4f738adba30a7cfc006f605707e7aee847ffefa0
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Mar 18 12:57:10 2018 -0700

    bpf: create tcp_bpf_ulp allowing BPF to monitor socket TX/RX data
    
    This implements a BPF ULP layer to allow policy enforcement and
    monitoring at the socket layer. In order to support this a new
    program type BPF_PROG_TYPE_SK_MSG is used to run the policy at
    the sendmsg/sendpage hook. To attach the policy to sockets a
    sockmap is used with a new program attach type BPF_SK_MSG_VERDICT.
    
    Similar to previous sockmap usages when a sock is added to a
    sockmap, via a map update, if the map contains a BPF_SK_MSG_VERDICT
    program type attached then the BPF ULP layer is created on the
    socket and the attached BPF_PROG_TYPE_SK_MSG program is run for
    every msg in sendmsg case and page/offset in sendpage case.
    
    BPF_PROG_TYPE_SK_MSG Semantics/API:
    
    BPF_PROG_TYPE_SK_MSG supports only two return codes SK_PASS and
    SK_DROP. Returning SK_DROP free's the copied data in the sendmsg
    case and in the sendpage case leaves the data untouched. Both cases
    return -EACESS to the user. Returning SK_PASS will allow the msg to
    be sent.
    
    In the sendmsg case data is copied into kernel space buffers before
    running the BPF program. The kernel space buffers are stored in a
    scatterlist object where each element is a kernel memory buffer.
    Some effort is made to coalesce data from the sendmsg call here.
    For example a sendmsg call with many one byte iov entries will
    likely be pushed into a single entry. The BPF program is run with
    data pointers (start/end) pointing to the first sg element.
    
    In the sendpage case data is not copied. We opt not to copy the
    data by default here, because the BPF infrastructure does not
    know what bytes will be needed nor when they will be needed. So
    copying all bytes may be wasteful. Because of this the initial
    start/end data pointers are (0,0). Meaning no data can be read or
    written. This avoids reading data that may be modified by the
    user. A new helper is added later in this series if reading and
    writing the data is needed. The helper call will do a copy by
    default so that the page is exclusively owned by the BPF call.
    
    The verdict from the BPF_PROG_TYPE_SK_MSG applies to the entire msg
    in the sendmsg() case and the entire page/offset in the sendpage case.
    This avoids ambiguity on how to handle mixed return codes in the
    sendmsg case. Again a helper is added later in the series if
    a verdict needs to apply to multiple system calls and/or only
    a subpart of the currently being processed message.
    
    The helper msg_redirect_map() can be used to select the socket to
    send the data on. This is used similar to existing redirect use
    cases. This allows policy to redirect msgs.
    
    Pseudo code simple example:
    
    The basic logic to attach a program to a socket is as follows,
    
      // load the programs
      bpf_prog_load(SOCKMAP_TCP_MSG_PROG, BPF_PROG_TYPE_SK_MSG,
                    &obj, &msg_prog);
    
      // lookup the sockmap
      bpf_map_msg = bpf_object__find_map_by_name(obj, "my_sock_map");
    
      // get fd for sockmap
      map_fd_msg = bpf_map__fd(bpf_map_msg);
    
      // attach program to sockmap
      bpf_prog_attach(msg_prog, map_fd_msg, BPF_SK_MSG_VERDICT, 0);
    
    Adding sockets to the map is done in the normal way,
    
      // Add a socket 'fd' to sockmap at location 'i'
      bpf_map_update_elem(map_fd_msg, &i, fd, BPF_ANY);
    
    After the above any socket attached to "my_sock_map", in this case
    'fd', will run the BPF msg verdict program (msg_prog) on every
    sendmsg and sendpage system call.
    
    For a complete example see BPF selftests or sockmap samples.
    
    Implementation notes:
    
    It seemed the simplest, to me at least, to use a refcnt to ensure
    psock is not lost across the sendmsg copy into the sg, the bpf program
    running on the data in sg_data, and the final pass to the TCP stack.
    Some performance testing may show a better method to do this and avoid
    the refcnt cost, but for now use the simpler method.
    
    Another item that will come after basic support is in place is
    supporting MSG_MORE flag. At the moment we call sendpages even if
    the MSG_MORE flag is set. An enhancement would be to collect the
    pages into a larger scatterlist and pass down the stack. Notice that
    bpf_tcp_sendmsg() could support this with some additional state saved
    across sendmsg calls. I built the code to support this without having
    to do refactoring work. Other features TBD include ZEROCOPY and the
    TCP_RECV_QUEUE/TCP_NO_QUEUE support. This will follow initial series
    shortly.
    
    Future work could improve size limits on the scatterlist rings used
    here. Currently, we use MAX_SKB_FRAGS simply because this was being
    used already in the TLS case. Future work could extend the kernel sk
    APIs to tune this depending on workload. This is a trade-off
    between memory usage and throughput performance.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 19b8349a3809..5e2e8a49fb21 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -13,6 +13,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_MSG, sk_msg)
 #endif
 #ifdef CONFIG_BPF_EVENTS
 BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)

commit 5f103c5d4dbadec0f2cacd39b6429e1b8a8cf983
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Jan 3 17:57:56 2018 -0800

    bpf: only build sockmap with CONFIG_INET
    
    The sockmap infrastructure is only aware of TCP sockets at the
    moment. In the future we plan to add UDP. In both cases CONFIG_NET
    should be built-in.
    
    So lets only build sockmap if CONFIG_INET is enabled.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 978c1d9c9383..19b8349a3809 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -42,7 +42,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
-#ifdef CONFIG_STREAM_PARSER
+#if defined(CONFIG_STREAM_PARSER) && defined(CONFIG_INET)
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)

commit ebc614f687369f9df99828572b1d85a7c2de3d92
Author: Roman Gushchin <guro@fb.com>
Date:   Sun Nov 5 08:15:32 2017 -0500

    bpf, cgroup: implement eBPF-based device controller for cgroup v2
    
    Cgroup v2 lacks the device controller, provided by cgroup v1.
    This patch adds a new eBPF program type, which in combination
    of previously added ability to attach multiple eBPF programs
    to a cgroup, will provide a similar functionality, but with some
    additional flexibility.
    
    This patch introduces a BPF_PROG_TYPE_CGROUP_DEVICE program type.
    A program takes major and minor device numbers, device type
    (block/character) and access type (mknod/read/write) as parameters
    and returns an integer which defines if the operation should be
    allowed or terminated with -EPERM.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 53c5b9ad7220..978c1d9c9383 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -19,6 +19,9 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
 BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
 BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
 #endif
+#ifdef CONFIG_CGROUP_BPF
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_DEVICE, cg_dev)
+#endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)

commit 2a171788ba7bb61995e98e8163204fc7880f63b2
Merge: bf5345882bd1 d4c2e9fca5b7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 4 09:26:51 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Files removed in 'net-next' had their license header updated
    in 'net'.  We take the remove from 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 6f1a567667b8..e1149327a0c0 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /* internal file - do not include directly */
 
 #ifdef CONFIG_NET

commit 7de16e3a35578f4f5accc6f5f23970310483d0a2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 16 16:40:53 2017 -0700

    bpf: split verifier and program ops
    
    struct bpf_verifier_ops contains both verifier ops and operations
    used later during program's lifetime (test_run).  Split the runtime
    ops into a different structure.
    
    BPF_PROG_TYPE() will now append ## _prog_ops or ## _verifier_ops
    to the names.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 814c1081a4a9..36418ad43245 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -1,22 +1,22 @@
 /* internal file - do not include directly */
 
 #ifdef CONFIG_NET
-BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act)
+BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb)
 #endif
 #ifdef CONFIG_BPF_EVENTS
-BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint_prog_ops)
-BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe)
+BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint)
+BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event)
 #endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)

commit 6710e1126934d8b4372b4d2f9ae1646cd3f151bf
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Mon Oct 16 12:19:28 2017 +0200

    bpf: introduce new bpf cpu map type BPF_MAP_TYPE_CPUMAP
    
    The 'cpumap' is primarily used as a backend map for XDP BPF helper
    call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'.
    
    This patch implement the main part of the map.  It is not connected to
    the XDP redirect system yet, and no SKB allocation are done yet.
    
    The main concern in this patch is to ensure the datapath can run
    without any locking.  This adds complexity to the setup and tear-down
    procedure, which assumptions are extra carefully documented in the
    code comments.
    
    V2:
     - make sure array isn't larger than NR_CPUS
     - make sure CPUs added is a valid possible CPU
    
    V3: fix nitpicks from Jakub Kicinski <kubakici@wp.pl>
    
    V5:
     - Restrict map allocation to root / CAP_SYS_ADMIN
     - WARN_ON_ONCE if queue is not empty on tear-down
     - Return -EPERM on memlock limit instead of -ENOMEM
     - Error code in __cpu_map_entry_alloc() also handle ptr_ring_cleanup()
     - Moved cpu_map_enqueue() to next patch
    
    V6: all notice by Daniel Borkmann
     - Fix err return code in cpu_map_alloc() introduced in V5
     - Move cpu_possible() check after max_entries boundary check
     - Forbid usage initially in check_map_func_compatibility()
    
    V7:
     - Fix alloc error path spotted by Daniel Borkmann
     - Did stress test adding+removing CPUs from the map concurrently
     - Fixed refcnt issue on cpu_map_entry, kthread started too soon
     - Make sure packets are flushed during tear-down, involved use of
       rcu_barrier() and kthread_run only exit after queue is empty
     - Fix alloc error path in __cpu_map_entry_alloc() for ptr_ring
    
    V8:
     - Nitpicking comments and gramma by Edward Cree
     - Fix missing semi-colon introduced in V7 due to rebasing
     - Move struct bpf_cpu_map_entry members cpu+map_id to tracepoint patch
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 6f1a567667b8..814c1081a4a9 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -41,4 +41,5 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
 #ifdef CONFIG_STREAM_PARSER
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif
+BPF_MAP_TYPE(BPF_MAP_TYPE_CPUMAP, cpu_map_ops)
 #endif

commit 6bdc9c4c31c81688e19cb186d49be01bbb6a1618
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Aug 16 15:02:32 2017 -0700

    bpf: sock_map fixes for !CONFIG_BPF_SYSCALL and !STREAM_PARSER
    
    Resolve issues with !CONFIG_BPF_SYSCALL and !STREAM_PARSER
    
    net/core/filter.c: In function ‘do_sk_redirect_map’:
    net/core/filter.c:1881:3: error: implicit declaration of function ‘__sock_map_lookup_elem’ [-Werror=implicit-function-declaration]
       sk = __sock_map_lookup_elem(ri->map, ri->ifindex);
       ^
    net/core/filter.c:1881:6: warning: assignment makes pointer from integer without a cast [enabled by default]
       sk = __sock_map_lookup_elem(ri->map, ri->ifindex);
    
    Fixes: 174a79ff9515 ("bpf: sockmap with sk redirect support")
    Reported-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index fa805074d168..6f1a567667b8 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -38,5 +38,7 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
+#ifdef CONFIG_STREAM_PARSER
 BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif
+#endif

commit 174a79ff9515f400b9a6115643dafd62a635b7e6
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Aug 15 22:32:47 2017 -0700

    bpf: sockmap with sk redirect support
    
    Recently we added a new map type called dev map used to forward XDP
    packets between ports (6093ec2dc313). This patches introduces a
    similar notion for sockets.
    
    A sockmap allows users to add participating sockets to a map. When
    sockets are added to the map enough context is stored with the
    map entry to use the entry with a new helper
    
      bpf_sk_redirect_map(map, key, flags)
    
    This helper (analogous to bpf_redirect_map in XDP) is given the map
    and an entry in the map. When called from a sockmap program, discussed
    below, the skb will be sent on the socket using skb_send_sock().
    
    With the above we need a bpf program to call the helper from that will
    then implement the send logic. The initial site implemented in this
    series is the recv_sock hook. For this to work we implemented a map
    attach command to add attributes to a map. In sockmap we add two
    programs a parse program and a verdict program. The parse program
    uses strparser to build messages and pass them to the verdict program.
    The parse programs use the normal strparser semantics. The verdict
    program is of type SK_SKB.
    
    The verdict program returns a verdict SK_DROP, or  SK_REDIRECT for
    now. Additional actions may be added later. When SK_REDIRECT is
    returned, expected when bpf program uses bpf_sk_redirect_map(), the
    sockmap logic will consult per cpu variables set by the helper routine
    and pull the sock entry out of the sock map. This pattern follows the
    existing redirect logic in cls and xdp programs.
    
    This gives the flow,
    
     recv_sock -> str_parser (parse_prog) -> verdict_prog -> skb_send_sock
                                                         \
                                                          -> kfree_skb
    
    As an example use case a message based load balancer may use specific
    logic in the verdict program to select the sock to send on.
    
    Sample programs are provided in future patches that hopefully illustrate
    the user interfaces. Also selftests are in follow-on patches.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 4b72db30dacf..fa805074d168 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -38,4 +38,5 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
 #ifdef CONFIG_NET
 BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_SOCKMAP, sock_map_ops)
 #endif

commit b005fd189cec9407b700599e1e80e0552446ee79
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Aug 15 22:31:58 2017 -0700

    bpf: introduce new program type for skbs on sockets
    
    A class of programs, run from strparser and soon from a new map type
    called sock map, are used with skb as the context but on established
    sockets. By creating a specific program type for these we can use
    bpf helpers that expect full sockets and get the verifier to ensure
    these helpers are not used out of context.
    
    The new type is BPF_PROG_TYPE_SK_SKB. This patch introduces the
    infrastructure and type.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index b1e1035ca24b..4b72db30dacf 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -11,6 +11,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_SKB, sk_skb_prog_ops)
 #endif
 #ifdef CONFIG_BPF_EVENTS
 BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe_prog_ops)

commit 546ac1ffb70d25b56c1126940e5ec639c4dd7413
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Jul 17 09:28:56 2017 -0700

    bpf: add devmap, a map for storing net device references
    
    Device map (devmap) is a BPF map, primarily useful for networking
    applications, that uses a key to lookup a reference to a netdevice.
    
    The map provides a clean way for BPF programs to build virtual port
    to physical port maps. Additionally, it provides a scoping function
    for the redirect action itself allowing multiple optimizations. Future
    patches will leverage the map to provide batching at the XDP layer.
    
    Another optimization/feature, that is not yet implemented, would be
    to support multiple netdevices per key to support efficient multicast
    and broadcast support.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 3d137c33d664..b1e1035ca24b 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -35,3 +35,6 @@ BPF_MAP_TYPE(BPF_MAP_TYPE_STACK_TRACE, stack_map_ops)
 #endif
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)
+#ifdef CONFIG_NET
+BPF_MAP_TYPE(BPF_MAP_TYPE_DEVMAP, dev_map_ops)
+#endif

commit 40304b2a1567fecc321f640ee4239556dd0f3ee0
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:40 2017 -0700

    bpf: BPF support for sock_ops
    
    Created a new BPF program type, BPF_PROG_TYPE_SOCK_OPS, and a corresponding
    struct that allows BPF programs of this type to access some of the
    socket's fields (such as IP addresses, ports, etc.). It uses the
    existing bpf cgroups infrastructure so the programs can be attached per
    cgroup with full inheritance support. The program will be called at
    appropriate times to set relevant connections parameters such as buffer
    sizes, SYN and SYN-ACK RTOs, etc., based on connection information such
    as IP addresses, port numbers, etc.
    
    Alghough there are already 3 mechanisms to set parameters (sysctls,
    route metrics and setsockopts), this new mechanism provides some
    distinct advantages. Unlike sysctls, it can set parameters per
    connection. In contrast to route metrics, it can also use port numbers
    and information provided by a user level program. In addition, it could
    set parameters probabilistically for evaluation purposes (i.e. do
    something different on 10% of the flows and compare results with the
    other 90% of the flows). Also, in cases where IPv6 addresses contain
    geographic information, the rules to make changes based on the distance
    (or RTT) between the hosts are much easier than route metric rules and
    can be global. Finally, unlike setsockopt, it oes not require
    application changes and it can be updated easily at any time.
    
    Although the bpf cgroup framework already contains a sock related
    program type (BPF_PROG_TYPE_CGROUP_SOCK), I created the new type
    (BPF_PROG_TYPE_SOCK_OPS) beccause the existing type expects to be called
    only once during the connections's lifetime. In contrast, the new
    program type will be called multiple times from different places in the
    network stack code.  For example, before sending SYN and SYN-ACKs to set
    an appropriate timeout, when the connection is established to set
    congestion control, etc. As a result it has "op" field to specify the
    type of operation requested.
    
    The purpose of this new program type is to simplify setting connection
    parameters, such as buffer sizes, TCP's SYN RTO, etc. For example, it is
    easy to use facebook's internal IPv6 addresses to determine if both hosts
    of a connection are in the same datacenter. Therefore, it is easy to
    write a BPF program to choose a small SYN RTO value when both hosts are
    in the same datacenter.
    
    This patch only contains the framework to support the new BPF program
    type, following patches add the functionality to set various connection
    parameters.
    
    This patch defines a new BPF program type: BPF_PROG_TYPE_SOCKET_OPS
    and a new bpf syscall command to load a new program of this type:
    BPF_PROG_LOAD_SOCKET_OPS.
    
    Two new corresponding structs (one for the kernel one for the user/BPF
    program):
    
    /* kernel version */
    struct bpf_sock_ops_kern {
            struct sock *sk;
            __u32  op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
    };
    
    /* user version
     * Some fields are in network byte order reflecting the sock struct
     * Use the bpf_ntohl helper macro in samples/bpf/bpf_endian.h to
     * convert them to host byte order.
     */
    struct bpf_sock_ops {
            __u32 op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
            __u32 family;
            __u32 remote_ip4;     /* In network byte order */
            __u32 local_ip4;      /* In network byte order */
            __u32 remote_ip6[4];  /* In network byte order */
            __u32 local_ip6[4];   /* In network byte order */
            __u32 remote_port;    /* In network byte order */
            __u32 local_port;     /* In host byte horder */
    };
    
    Currently there are two types of ops. The first type expects the BPF
    program to return a value which is then used by the caller (or a
    negative value to indicate the operation is not supported). The second
    type expects state changes to be done by the BPF program, for example
    through a setsockopt BPF helper function, and they ignore the return
    value.
    
    The reply fields of the bpf_sockt_ops struct are there in case a bpf
    program needs to return a value larger than an integer.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 03bf223f18be..3d137c33d664 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -10,6 +10,7 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops_prog_ops)
 #endif
 #ifdef CONFIG_BPF_EVENTS
 BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe_prog_ops)

commit 40077e0cf62206ac3c315b6991d8dcddb3703286
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Tue Apr 11 15:34:58 2017 +0200

    bpf: remove struct bpf_map_type_list
    
    There's no need to have struct bpf_map_type_list since
    it just contains a list_head, the type, and the ops
    pointer. Since the types are densely packed and not
    actually dynamically registered, it's much easier and
    smaller to have an array of type->ops pointer. Also
    initialize this array statically to remove code needed
    to initialize it.
    
    In order to save duplicating the list, move it to the
    types header file added by the previous patch and
    include it in the same fashion.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 68b0a9811216..03bf223f18be 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -16,3 +16,21 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint_prog_ops)
 BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event_prog_ops)
 #endif
+
+BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PROG_ARRAY, prog_array_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PERF_EVENT_ARRAY, perf_event_array_map_ops)
+#ifdef CONFIG_CGROUPS
+BPF_MAP_TYPE(BPF_MAP_TYPE_CGROUP_ARRAY, cgroup_array_map_ops)
+#endif
+BPF_MAP_TYPE(BPF_MAP_TYPE_HASH, htab_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_HASH, htab_percpu_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_LRU_HASH, htab_lru_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_LRU_PERCPU_HASH, htab_lru_percpu_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_LPM_TRIE, trie_map_ops)
+#ifdef CONFIG_PERF_EVENTS
+BPF_MAP_TYPE(BPF_MAP_TYPE_STACK_TRACE, stack_map_ops)
+#endif
+BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY_OF_MAPS, array_of_maps_map_ops)
+BPF_MAP_TYPE(BPF_MAP_TYPE_HASH_OF_MAPS, htab_of_maps_map_ops)

commit be9370a7d8614d1fa54649c75de14458e79b91ec
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Tue Apr 11 15:34:57 2017 +0200

    bpf: remove struct bpf_prog_type_list
    
    There's no need to have struct bpf_prog_type_list since
    it just contains a list_head, the type, and the ops
    pointer. Since the types are densely packed and not
    actually dynamically registered, it's much easier and
    smaller to have an array of type->ops pointer. Also
    initialize this array statically to remove code needed
    to initialize it.
    
    In order to save duplicating the list, move it to a new
    header file and include it in the places needing it.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
new file mode 100644
index 000000000000..68b0a9811216
--- /dev/null
+++ b/include/linux/bpf_types.h
@@ -0,0 +1,18 @@
+/* internal file - do not include directly */
+
+#ifdef CONFIG_NET
+BPF_PROG_TYPE(BPF_PROG_TYPE_SOCKET_FILTER, sk_filter_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_CLS, tc_cls_act_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SCHED_ACT, tc_cls_act_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SKB, cg_skb_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_CGROUP_SOCK, cg_sock_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_IN, lwt_inout_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_OUT, lwt_inout_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_LWT_XMIT, lwt_xmit_prog_ops)
+#endif
+#ifdef CONFIG_BPF_EVENTS
+BPF_PROG_TYPE(BPF_PROG_TYPE_KPROBE, kprobe_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_TRACEPOINT, tracepoint_prog_ops)
+BPF_PROG_TYPE(BPF_PROG_TYPE_PERF_EVENT, perf_event_prog_ops)
+#endif
