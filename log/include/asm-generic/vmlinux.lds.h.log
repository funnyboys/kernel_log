commit 6553896666433e7efec589838b400a2a652b3ffa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 9 22:47:17 2020 +0100

    vmlinux.lds.h: Create section for protection against instrumentation
    
    Some code pathes, especially the low level entry code, must be protected
    against instrumentation for various reasons:
    
     - Low level entry code can be a fragile beast, especially on x86.
    
     - With NO_HZ_FULL RCU state needs to be established before using it.
    
    Having a dedicated section for such code allows to validate with tooling
    that no unsafe functions are invoked.
    
    Add the .noinstr.text section and the noinstr attribute to mark
    functions. noinstr implies notrace. Kprobes will gain a section check
    later.
    
    Provide also a set of markers: instrumentation_begin()/end()
    
    These are used to mark code inside a noinstr function which calls
    into regular instrumentable text section as safe.
    
    The instrumentation markers are only active when CONFIG_DEBUG_ENTRY is
    enabled as the end marker emits a NOP to prevent the compiler from merging
    the annotation points. This means the objtool verification requires a
    kernel compiled with this option.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134100.075416272@linutronix.de

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 71e387a5fe90..db600ef218d7 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -540,6 +540,15 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
+/*
+ * Non-instrumentable text section
+ */
+#define NOINSTR_TEXT							\
+		ALIGN_FUNCTION();					\
+		__noinstr_text_start = .;				\
+		*(.noinstr.text)					\
+		__noinstr_text_end = .;
+
 /*
  * .text section. Map to function alignment to avoid address changes
  * during second ld run in second ld pass when generating System.map
@@ -551,6 +560,7 @@
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
 		*(.text.hot TEXT_MAIN .text.fixup .text.unlikely)	\
+		NOINSTR_TEXT						\
 		*(.text..refcount)					\
 		*(.ref.text)						\
 	MEM_KEEP(init.text*)						\

commit 29d9f30d4ce6c7a38745a54a8cddface10013490
Merge: 56a451b78067 7f80ccfe9968
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 17:29:33 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Fix the iwlwifi regression, from Johannes Berg.
    
       2) Support BSS coloring and 802.11 encapsulation offloading in
          hardware, from John Crispin.
    
       3) Fix some potential Spectre issues in qtnfmac, from Sergey
          Matyukevich.
    
       4) Add TTL decrement action to openvswitch, from Matteo Croce.
    
       5) Allow paralleization through flow_action setup by not taking the
          RTNL mutex, from Vlad Buslov.
    
       6) A lot of zero-length array to flexible-array conversions, from
          Gustavo A. R. Silva.
    
       7) Align XDP statistics names across several drivers for consistency,
          from Lorenzo Bianconi.
    
       8) Add various pieces of infrastructure for offloading conntrack, and
          make use of it in mlx5 driver, from Paul Blakey.
    
       9) Allow using listening sockets in BPF sockmap, from Jakub Sitnicki.
    
      10) Lots of parallelization improvements during configuration changes
          in mlxsw driver, from Ido Schimmel.
    
      11) Add support to devlink for generic packet traps, which report
          packets dropped during ACL processing. And use them in mlxsw
          driver. From Jiri Pirko.
    
      12) Support bcmgenet on ACPI, from Jeremy Linton.
    
      13) Make BPF compatible with RT, from Thomas Gleixnet, Alexei
          Starovoitov, and your's truly.
    
      14) Support XDP meta-data in virtio_net, from Yuya Kusakabe.
    
      15) Fix sysfs permissions when network devices change namespaces, from
          Christian Brauner.
    
      16) Add a flags element to ethtool_ops so that drivers can more simply
          indicate which coalescing parameters they actually support, and
          therefore the generic layer can validate the user's ethtool
          request. Use this in all drivers, from Jakub Kicinski.
    
      17) Offload FIFO qdisc in mlxsw, from Petr Machata.
    
      18) Support UDP sockets in sockmap, from Lorenz Bauer.
    
      19) Fix stretch ACK bugs in several TCP congestion control modules,
          from Pengcheng Yang.
    
      20) Support virtual functiosn in octeontx2 driver, from Tomasz
          Duszynski.
    
      21) Add region operations for devlink and use it in ice driver to dump
          NVM contents, from Jacob Keller.
    
      22) Add support for hw offload of MACSEC, from Antoine Tenart.
    
      23) Add support for BPF programs that can be attached to LSM hooks,
          from KP Singh.
    
      24) Support for multiple paths, path managers, and counters in MPTCP.
          From Peter Krystad, Paolo Abeni, Florian Westphal, Davide Caratti,
          and others.
    
      25) More progress on adding the netlink interface to ethtool, from
          Michal Kubecek"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2121 commits)
      net: ipv6: rpl_iptunnel: Fix potential memory leak in rpl_do_srh_inline
      cxgb4/chcr: nic-tls stats in ethtool
      net: dsa: fix oops while probing Marvell DSA switches
      net/bpfilter: remove superfluous testing message
      net: macb: Fix handling of fixed-link node
      net: dsa: ksz: Select KSZ protocol tag
      netdevsim: dev: Fix memory leak in nsim_dev_take_snapshot_write
      net: stmmac: add EHL 2.5Gbps PCI info and PCI ID
      net: stmmac: add EHL PSE0 & PSE1 1Gbps PCI info and PCI ID
      net: stmmac: create dwmac-intel.c to contain all Intel platform
      net: dsa: bcm_sf2: Support specifying VLAN tag egress rule
      net: dsa: bcm_sf2: Add support for matching VLAN TCI
      net: dsa: bcm_sf2: Move writing of CFP_DATA(5) into slicing functions
      net: dsa: bcm_sf2: Check earlier for FLOW_EXT and FLOW_MAC_EXT
      net: dsa: bcm_sf2: Disable learning for ASP port
      net: dsa: b53: Deny enslaving port 7 for 7278 into a bridge
      net: dsa: b53: Prevent tagged VLAN on port 7 for 7278
      net: dsa: b53: Restore VLAN entries upon (re)configuration
      net: dsa: bcm_sf2: Fix overflow checks
      hv_netvsc: Remove unnecessary round_up for recv_completion_cnt
      ...

commit 84d5f77fc2ee4e010c2c037750e32f06e55224b0
Author: H.J. Lu <hjl.tools@gmail.com>
Date:   Thu Mar 26 12:30:20 2020 -0700

    x86, vmlinux.lds: Add RUNTIME_DISCARD_EXIT to generic DISCARDS
    
    In the x86 kernel, .exit.text and .exit.data sections are discarded at
    runtime, not by the linker. Add RUNTIME_DISCARD_EXIT to generic DISCARDS
    and define it in the x86 kernel linker script to keep them.
    
    The sections are added before the DISCARD directive so document here
    only the situation explicitly as this change doesn't have any effect on
    the generated kernel. Also, other architectures like ARM64 will use it
    too so generalize the approach with the RUNTIME_DISCARD_EXIT define.
    
     [ bp: Massage and extend commit message. ]
    
    Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Link: https://lkml.kernel.org/r/20200326193021.255002-1-hjl.tools@gmail.com

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e00f41aa8ec4..2444336ef04c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -894,10 +894,17 @@
  * section definitions so that such archs put those in earlier section
  * definitions.
  */
+#ifdef RUNTIME_DISCARD_EXIT
+#define EXIT_DISCARDS
+#else
+#define EXIT_DISCARDS							\
+	EXIT_TEXT							\
+	EXIT_DATA
+#endif
+
 #define DISCARDS							\
 	/DISCARD/ : {							\
-	EXIT_TEXT							\
-	EXIT_DATA							\
+	EXIT_DISCARDS							\
 	EXIT_CALL							\
 	*(.discard)							\
 	*(.discard.*)							\

commit 90ceddcb495008ac8ba7a3dce297841efcd7d584
Author: Fangrui Song <maskray@google.com>
Date:   Wed Mar 18 15:27:46 2020 -0700

    bpf: Support llvm-objcopy for vmlinux BTF
    
    Simplify gen_btf logic to make it work with llvm-objcopy. The existing
    'file format' and 'architecture' parsing logic is brittle and does not
    work with llvm-objcopy/llvm-objdump.
    
    'file format' output of llvm-objdump>=11 will match GNU objdump, but
    'architecture' (bfdarch) may not.
    
    .BTF in .tmp_vmlinux.btf is non-SHF_ALLOC. Add the SHF_ALLOC flag
    because it is part of vmlinux image used for introspection. C code
    can reference the section via linker script defined __start_BTF and
    __stop_BTF. This fixes a small problem that previous .BTF had the
    SHF_WRITE flag (objcopy -I binary -O elf* synthesized .data).
    
    Additionally, `objcopy -I binary` synthesized symbols
    _binary__btf_vmlinux_bin_start and _binary__btf_vmlinux_bin_stop (not
    used elsewhere) are replaced with more commonplace __start_BTF and
    __stop_BTF.
    
    Add 2>/dev/null because GNU objcopy (but not llvm-objcopy) warns
    "empty loadable segment detected at vaddr=0xffffffff81000000, is this intentional?"
    
    We use a dd command to change the e_type field in the ELF header from
    ET_EXEC to ET_REL so that lld will accept .btf.vmlinux.bin.o.  Accepting
    ET_EXEC as an input file is an extremely rare GNU ld feature that lld
    does not intend to support, because this is error-prone.
    
    The output section description .BTF in include/asm-generic/vmlinux.lds.h
    avoids potential subtle orphan section placement issues and suppresses
    --orphan-handling=warn warnings.
    
    Fixes: df786c9b9476 ("bpf: Force .BTF section start to zero when dumping from vmlinux")
    Fixes: cb0cc635c7a9 ("powerpc: Include .BTF section")
    Reported-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Fangrui Song <maskray@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Tested-by: Stanislav Fomichev <sdf@google.com>
    Tested-by: Andrii Nakryiko <andriin@fb.com>
    Reviewed-by: Stanislav Fomichev <sdf@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Link: https://github.com/ClangBuiltLinux/linux/issues/871
    Link: https://lore.kernel.org/bpf/20200318222746.173648-1-maskray@google.com

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e00f41aa8ec4..39da8d8b561d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -535,6 +535,7 @@
 									\
 	RO_EXCEPTION_TABLE						\
 	NOTES								\
+	BTF								\
 									\
 	. = ALIGN((align));						\
 	__end_rodata = .;
@@ -621,6 +622,20 @@
 		__stop___ex_table = .;					\
 	}
 
+/*
+ * .BTF
+ */
+#ifdef CONFIG_DEBUG_INFO_BTF
+#define BTF								\
+	.BTF : AT(ADDR(.BTF) - LOAD_OFFSET) {				\
+		__start_BTF = .;					\
+		*(.BTF)							\
+		__stop_BTF = .;						\
+	}
+#else
+#define BTF
+#endif
+
 /*
  * Init task
  */

commit 95f1fa9e3418d50ce099e67280b5497b9c93843b
Merge: 477093b3e144 16c0f03f629a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 27 11:42:01 2019 -0800

    Merge tag 'trace-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "New tracing features:
    
       - New PERMANENT flag to ftrace_ops when attaching a callback to a
         function.
    
         As /proc/sys/kernel/ftrace_enabled when set to zero will disable
         all attached callbacks in ftrace, this has a detrimental impact on
         live kernel tracing, as it disables all that it patched. If a
         ftrace_ops is registered to ftrace with the PERMANENT flag set, it
         will prevent ftrace_enabled from being disabled, and if
         ftrace_enabled is already disabled, it will prevent a ftrace_ops
         with PREMANENT flag set from being registered.
    
       - New register_ftrace_direct().
    
         As eBPF would like to register its own trampolines to be called by
         the ftrace nop locations directly, without going through the ftrace
         trampoline, this function has been added. This allows for eBPF
         trampolines to live along side of ftrace, perf, kprobe and live
         patching. It also utilizes the ftrace enabled_functions file that
         keeps track of functions that have been modified in the kernel, to
         allow for security auditing.
    
       - Allow for kernel internal use of ftrace instances.
    
         Subsystems in the kernel can now create and destroy their own
         tracing instances which allows them to have their own tracing
         buffer, and be able to record events without worrying about other
         users from writing over their data.
    
       - New seq_buf_hex_dump() that lets users use the hex_dump() in their
         seq_buf usage.
    
       - Notifications now added to tracing_max_latency to allow user space
         to know when a new max latency is hit by one of the latency
         tracers.
    
       - Wider spread use of generic compare operations for use of bsearch
         and friends.
    
       - More synthetic event fields may be defined (32 up from 16)
    
       - Use of xarray for architectures with sparse system calls, for the
         system call trace events.
    
      This along with small clean ups and fixes"
    
    * tag 'trace-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (51 commits)
      tracing: Enable syscall optimization for MIPS
      tracing: Use xarray for syscall trace events
      tracing: Sample module to demonstrate kernel access to Ftrace instances.
      tracing: Adding new functions for kernel access to Ftrace instances
      tracing: Fix Kconfig indentation
      ring-buffer: Fix typos in function ring_buffer_producer
      ftrace: Use BIT() macro
      ftrace: Return ENOTSUPP when DYNAMIC_FTRACE_WITH_DIRECT_CALLS is not configured
      ftrace: Rename ftrace_graph_stub to ftrace_stub_graph
      ftrace: Add a helper function to modify_ftrace_direct() to allow arch optimization
      ftrace: Add helper find_direct_entry() to consolidate code
      ftrace: Add another check for match in register_ftrace_direct()
      ftrace: Fix accounting bug with direct->count in register_ftrace_direct()
      ftrace/selftests: Fix spelling mistake "wakeing" -> "waking"
      tracing: Increase SYNTH_FIELDS_MAX for synthetic_events
      ftrace/samples: Add a sample module that implements modify_ftrace_direct()
      ftrace: Add modify_ftrace_direct()
      tracing: Add missing "inline" in stub function of latency_fsnotify()
      tracing: Remove stray tab in TRACE_EVAL_MAP_FILE's help text
      tracing: Use seq_buf_hex_dump() to dump buffers
      ...

commit 1d87200446f1d10dfe9672ca8edb027a82612f8c
Merge: 5c4a1c090d86 f01ec4fca820
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 10:42:40 2019 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Cross-arch changes to move the linker sections for NOTES and
         EXCEPTION_TABLE into the RO_DATA area, where they belong on most
         architectures. (Kees Cook)
    
       - Switch the x86 linker fill byte from x90 (NOP) to 0xcc (INT3), to
         trap jumps into the middle of those padding areas instead of
         sliding execution. (Kees Cook)
    
       - A thorough cleanup of symbol definitions within x86 assembler code.
         The rather randomly named macros got streamlined around a
         (hopefully) straightforward naming scheme:
    
            SYM_START(name, linkage, align...)
            SYM_END(name, sym_type)
    
            SYM_FUNC_START(name)
            SYM_FUNC_END(name)
    
            SYM_CODE_START(name)
            SYM_CODE_END(name)
    
            SYM_DATA_START(name)
            SYM_DATA_END(name)
    
         etc - with about three times of these basic primitives with some
         label, local symbol or attribute variant, expressed via postfixes.
    
         No change in functionality intended. (Jiri Slaby)
    
       - Misc other changes, cleanups and smaller fixes"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (67 commits)
      x86/entry/64: Remove pointless jump in paranoid_exit
      x86/entry/32: Remove unused resume_userspace label
      x86/build/vdso: Remove meaningless CFLAGS_REMOVE_*.o
      m68k: Convert missed RODATA to RO_DATA
      x86/vmlinux: Use INT3 instead of NOP for linker fill bytes
      x86/mm: Report actual image regions in /proc/iomem
      x86/mm: Report which part of kernel image is freed
      x86/mm: Remove redundant address-of operators on addresses
      xtensa: Move EXCEPTION_TABLE to RO_DATA segment
      powerpc: Move EXCEPTION_TABLE to RO_DATA segment
      parisc: Move EXCEPTION_TABLE to RO_DATA segment
      microblaze: Move EXCEPTION_TABLE to RO_DATA segment
      ia64: Move EXCEPTION_TABLE to RO_DATA segment
      h8300: Move EXCEPTION_TABLE to RO_DATA segment
      c6x: Move EXCEPTION_TABLE to RO_DATA segment
      arm64: Move EXCEPTION_TABLE to RO_DATA segment
      alpha: Move EXCEPTION_TABLE to RO_DATA segment
      x86/vmlinux: Move EXCEPTION_TABLE to RO_DATA segment
      x86/vmlinux: Actually use _etext for the end of the text segment
      vmlinux.lds.h: Allow EXCEPTION_TABLE to live in RO_DATA
      ...

commit 46f9469247c6f4697cbbf37e4b3961120bf07f29
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Nov 18 10:41:29 2019 -0500

    ftrace: Rename ftrace_graph_stub to ftrace_stub_graph
    
    The ftrace_graph_stub was created and points to ftrace_stub as a way to
    assign the functon graph tracer function pointer to a stub function with a
    different prototype than what ftrace_stub has and not trigger the C
    verifier. The ftrace_graph_stub was created via the linker script
    vmlinux.lds.h. Unfortunately, powerpc already uses the name
    ftrace_graph_stub for its internal implementation of the function graph
    tracer, and even though powerpc would still build, the change via the linker
    script broke function tracer on powerpc from working.
    
    By using the name ftrace_stub_graph, which does not exist anywhere else in
    the kernel, this should not be a problem.
    
    Link: https://lore.kernel.org/r/1573849732.5937.136.camel@lca.pw
    
    Fixes: b83b43ffc6e4 ("fgraph: Fix function type mismatches of ftrace_graph_return using ftrace_stub")
    Reorted-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0f358be551cd..996db32c491b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -112,7 +112,7 @@
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 #ifdef CC_USING_PATCHABLE_FUNCTION_ENTRY
 /*
- * Need to also make ftrace_graph_stub point to ftrace_stub
+ * Need to also make ftrace_stub_graph point to ftrace_stub
  * so that the same stub location may have different protocols
  * and not mess up with C verifiers.
  */
@@ -120,17 +120,17 @@
 			__start_mcount_loc = .;			\
 			KEEP(*(__patchable_function_entries))	\
 			__stop_mcount_loc = .;			\
-			ftrace_graph_stub = ftrace_stub;
+			ftrace_stub_graph = ftrace_stub;
 #else
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
 			KEEP(*(__mcount_loc))			\
 			__stop_mcount_loc = .;			\
-			ftrace_graph_stub = ftrace_stub;
+			ftrace_stub_graph = ftrace_stub;
 #endif
 #else
 # ifdef CONFIG_FUNCTION_TRACER
-#  define MCOUNT_REC()	ftrace_graph_stub = ftrace_stub;
+#  define MCOUNT_REC()	ftrace_stub_graph = ftrace_stub;
 # else
 #  define MCOUNT_REC()
 # endif

commit b83b43ffc6e4b514ca034a0fbdee01322e2f7022
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Oct 15 09:00:55 2019 -0400

    fgraph: Fix function type mismatches of ftrace_graph_return using ftrace_stub
    
    The C compiler is allowing more checks to make sure that function pointers
    are assigned to the correct prototype function. Unfortunately, the function
    graph tracer uses a special name with its assigned ftrace_graph_return
    function pointer that maps to a stub function used by the function tracer
    (ftrace_stub). The ftrace_graph_return variable is compared to the
    ftrace_stub in some archs to know if the function graph tracer is enabled or
    not. This means we can not just simply create a new function stub that
    compares it without modifying all the archs.
    
    Instead, have the linker script create a function_graph_stub that maps to
    ftrace_stub, and this way we can define the prototype for it to match the
    prototype of ftrace_graph_return, and make the compiler checks all happy!
    
    Link: http://lkml.kernel.org/r/20191015090055.789a0aed@gandalf.local.home
    
    Cc: linux-sh@vger.kernel.org
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc:  Rich Felker <dalias@libc.org>
    Reported-by: Sami Tolvanen <samitolvanen@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index dae64600ccbf..0f358be551cd 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -111,18 +111,29 @@
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 #ifdef CC_USING_PATCHABLE_FUNCTION_ENTRY
+/*
+ * Need to also make ftrace_graph_stub point to ftrace_stub
+ * so that the same stub location may have different protocols
+ * and not mess up with C verifiers.
+ */
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
 			KEEP(*(__patchable_function_entries))	\
-			__stop_mcount_loc = .;
+			__stop_mcount_loc = .;			\
+			ftrace_graph_stub = ftrace_stub;
 #else
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
 			KEEP(*(__mcount_loc))			\
-			__stop_mcount_loc = .;
+			__stop_mcount_loc = .;			\
+			ftrace_graph_stub = ftrace_stub;
 #endif
 #else
-#define MCOUNT_REC()
+# ifdef CONFIG_FUNCTION_TRACER
+#  define MCOUNT_REC()	ftrace_graph_stub = ftrace_stub;
+# else
+#  define MCOUNT_REC()
+# endif
 #endif
 
 #ifdef CONFIG_TRACE_BRANCH_PROFILING

commit a1326b17ac03a9012cb3d01e434aacb4d67a416c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Oct 16 18:17:11 2019 +0100

    module/ftrace: handle patchable-function-entry
    
    When using patchable-function-entry, the compiler will record the
    callsites into a section named "__patchable_function_entries" rather
    than "__mcount_loc". Let's abstract this difference behind a new
    FTRACE_CALLSITE_SECTION, so that architectures don't have to handle this
    explicitly (e.g. with custom module linker scripts).
    
    As parisc currently handles this explicitly, it is fixed up accordingly,
    with its custom linker script removed. Since FTRACE_CALLSITE_SECTION is
    only defined when DYNAMIC_FTRACE is selected, the parisc module loading
    code is updated to only use the definition in that case. When
    DYNAMIC_FTRACE is not selected, modules shouldn't have this section, so
    this removes some redundant work in that case.
    
    To make sure that this is keep up-to-date for modules and the main
    kernel, a comment is added to vmlinux.lds.h, with the existing ifdeffery
    simplified for legibility.
    
    I built parisc generic-{32,64}bit_defconfig with DYNAMIC_FTRACE enabled,
    and verified that the section made it into the .ko files for modules.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Helge Deller <deller@gmx.de>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Tested-by: Sven Schnelle <svens@stackframe.org>
    Tested-by: Torsten Duwe <duwe@suse.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James E.J. Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: linux-parisc@vger.kernel.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index dae64600ccbf..a9c4e4721434 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -110,17 +110,17 @@
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
-#ifdef CC_USING_PATCHABLE_FUNCTION_ENTRY
-#define MCOUNT_REC()	. = ALIGN(8);				\
-			__start_mcount_loc = .;			\
-			KEEP(*(__patchable_function_entries))	\
-			__stop_mcount_loc = .;
-#else
+/*
+ * The ftrace call sites are logged to a section whose name depends on the
+ * compiler option used. A given kernel image will only use one, AKA
+ * FTRACE_CALLSITE_SECTION. We capture all of them here to avoid header
+ * dependencies for FTRACE_CALLSITE_SECTION's definition.
+ */
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
 			KEEP(*(__mcount_loc))			\
+			KEEP(*(__patchable_function_entries))	\
 			__stop_mcount_loc = .;
-#endif
 #else
 #define MCOUNT_REC()
 #endif

commit b8c2f776164c8f74ac31c5e370ca3f029be0aa19
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:36 2019 -0700

    vmlinux.lds.h: Allow EXCEPTION_TABLE to live in RO_DATA
    
    Many architectures have an EXCEPTION_TABLE that needs to be only
    readable. As such, it should live in RO_DATA. Create a macro to identify
    this case for the architectures that can move EXCEPTION_TABLE into
    RO_DATA.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-15-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 356078e50a5c..9867d8e41eed 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -69,6 +69,17 @@
 #define NOTES_HEADERS_RESTORE
 #endif
 
+/*
+ * Some architectures have non-executable read-only exception tables.
+ * They can be added to the RO_DATA segment by specifying their desired
+ * alignment.
+ */
+#ifdef RO_EXCEPTION_TABLE_ALIGN
+#define RO_EXCEPTION_TABLE	EXCEPTION_TABLE(RO_EXCEPTION_TABLE_ALIGN)
+#else
+#define RO_EXCEPTION_TABLE
+#endif
+
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
@@ -513,6 +524,7 @@
 		__stop___modver = .;					\
 	}								\
 									\
+	RO_EXCEPTION_TABLE						\
 	NOTES								\
 									\
 	. = ALIGN((align));						\

commit c9174047b48d700a785b633319dd7d27288b86be
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:35 2019 -0700

    vmlinux.lds.h: Replace RW_DATA_SECTION with RW_DATA
    
    Rename RW_DATA_SECTION to RW_DATA. (Calling this a "section" is a lie,
    since it's multiple sections and section flags cannot be applied to
    the macro.)
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> # m68k
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-14-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 061e57c609f6..356078e50a5c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -24,7 +24,7 @@
  *
  *      _sdata = .;
  *	RO_DATA(PAGE_SIZE)
- *	RW_DATA_SECTION(...)
+ *	RW_DATA(...)
  *	_edata = .;
  *
  *	EXCEPTION_TABLE(...)
@@ -975,7 +975,7 @@
  * matches the requirement of PAGE_ALIGNED_DATA.
  *
  * use 0 as page_align if page_aligned data is not used */
-#define RW_DATA_SECTION(cacheline, pagealigned, inittask)		\
+#define RW_DATA(cacheline, pagealigned, inittask)			\
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
 		INIT_TASK_DATA(inittask)				\

commit 93240b327929ff03c1878ea8badc5c6bd86f053f
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:34 2019 -0700

    vmlinux.lds.h: Replace RO_DATA_SECTION with RO_DATA
    
    Finish renaming RO_DATA_SECTION to RO_DATA. (Calling this a "section"
    is a lie, since it's multiple sections and section flags cannot be
    applied to the macro.)
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> # m68k
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-13-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a0a989fbe411..061e57c609f6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -23,7 +23,7 @@
  *	_etext = .;
  *
  *      _sdata = .;
- *	RO_DATA_SECTION(PAGE_SIZE)
+ *	RO_DATA(PAGE_SIZE)
  *	RW_DATA_SECTION(...)
  *	_edata = .;
  *
@@ -363,7 +363,7 @@
 /*
  * Read only Data
  */
-#define RO_DATA_SECTION(align)						\
+#define RO_DATA(align)							\
 	. = ALIGN((align));						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		__start_rodata = .;					\
@@ -518,9 +518,6 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
-/* All archs are supposed to use RO_DATA() */
-#define RO_DATA(align)  RO_DATA_SECTION(align)
-
 /*
  * .text section. Map to function alignment to avoid address changes
  * during second ld run in second ld pass when generating System.map

commit c82318254d15e5f83c75f60aedf2bb9eb408308f
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:33 2019 -0700

    vmlinux.lds.h: Replace RODATA with RO_DATA
    
    There's no reason to keep the RODATA macro: replace the callers with
    the expected RO_DATA macro.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-12-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index dc3390ec6b60..a0a989fbe411 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -518,9 +518,7 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
-/* RODATA & RO_DATA provided for backward compatibility.
- * All archs are supposed to use RO_DATA() */
-#define RODATA          RO_DATA_SECTION(4096)
+/* All archs are supposed to use RO_DATA() */
 #define RO_DATA(align)  RO_DATA_SECTION(align)
 
 /*

commit eaf937075c9a42eb8ba51eb3050773d7205d3595
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:32 2019 -0700

    vmlinux.lds.h: Move NOTES into RO_DATA
    
    The .notes section should be non-executable read-only data. As such,
    move it to the RO_DATA macro instead of being per-architecture defined.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-11-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 97d4299f14dc..dc3390ec6b60 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -28,7 +28,6 @@
  *	_edata = .;
  *
  *	EXCEPTION_TABLE(...)
- *	NOTES
  *
  *	BSS_SECTION(0, 0, 0)
  *	_end = .;
@@ -512,10 +511,12 @@
 		__start___modver = .;					\
 		KEEP(*(__modver))					\
 		__stop___modver = .;					\
-		. = ALIGN((align));					\
-		__end_rodata = .;					\
 	}								\
-	. = ALIGN((align));
+									\
+	NOTES								\
+									\
+	. = ALIGN((align));						\
+	__end_rodata = .;
 
 /* RODATA & RO_DATA provided for backward compatibility.
  * All archs are supposed to use RO_DATA() */

commit fbe6a8e618a2d70621cff277e24f6eb338d3d149
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:31 2019 -0700

    vmlinux.lds.h: Move Program Header restoration into NOTES macro
    
    In preparation for moving NOTES into RO_DATA, make the Program Header
    assignment restoration be part of the NOTES macro itself.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-10-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f5dd45ce73f1..97d4299f14dc 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -56,10 +56,18 @@
 
 /*
  * Only some architectures want to have the .notes segment visible in
- * a separate PT_NOTE ELF Program Header.
+ * a separate PT_NOTE ELF Program Header. When this happens, it needs
+ * to be visible in both the kernel text's PT_LOAD and the PT_NOTE
+ * Program Headers. In this case, though, the PT_LOAD needs to be made
+ * the default again so that all the following sections don't also end
+ * up in the PT_NOTE Program Header.
  */
 #ifdef EMITS_PT_NOTE
 #define NOTES_HEADERS		:text :note
+#define NOTES_HEADERS_RESTORE	__restore_ph : { *(.__restore_ph) } :text
+#else
+#define NOTES_HEADERS
+#define NOTES_HEADERS_RESTORE
 #endif
 
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
@@ -798,7 +806,8 @@
 		__start_notes = .;					\
 		KEEP(*(.note.*))					\
 		__stop_notes = .;					\
-	}
+	} NOTES_HEADERS							\
+	NOTES_HEADERS_RESTORE
 
 #define INIT_SETUP(initsetup_align)					\
 		. = ALIGN(initsetup_align);				\

commit 441110a547f86a2fd0c40bf04b274853622c53cc
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 29 14:13:30 2019 -0700

    vmlinux.lds.h: Provide EMIT_PT_NOTE to indicate export of .notes
    
    In preparation for moving NOTES into RO_DATA, provide a mechanism for
    architectures that want to emit a PT_NOTE Program Header to do so.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # s390
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Segher Boessenkool <segher@kernel.crashing.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: https://lkml.kernel.org/r/20191029211351.13243-9-keescook@chromium.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index dae64600ccbf..f5dd45ce73f1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -54,6 +54,14 @@
 #define LOAD_OFFSET 0
 #endif
 
+/*
+ * Only some architectures want to have the .notes segment visible in
+ * a separate PT_NOTE ELF Program Header.
+ */
+#ifdef EMITS_PT_NOTE
+#define NOTES_HEADERS		:text :note
+#endif
+
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 

commit aefcf2f4b58155d27340ba5f9ddbe9513da8286d
Merge: f1f2f614d535 45893a0abee6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 28 08:14:15 2019 -0700

    Merge branch 'next-lockdown' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull kernel lockdown mode from James Morris:
     "This is the latest iteration of the kernel lockdown patchset, from
      Matthew Garrett, David Howells and others.
    
      From the original description:
    
        This patchset introduces an optional kernel lockdown feature,
        intended to strengthen the boundary between UID 0 and the kernel.
        When enabled, various pieces of kernel functionality are restricted.
        Applications that rely on low-level access to either hardware or the
        kernel may cease working as a result - therefore this should not be
        enabled without appropriate evaluation beforehand.
    
        The majority of mainstream distributions have been carrying variants
        of this patchset for many years now, so there's value in providing a
        doesn't meet every distribution requirement, but gets us much closer
        to not requiring external patches.
    
      There are two major changes since this was last proposed for mainline:
    
       - Separating lockdown from EFI secure boot. Background discussion is
         covered here: https://lwn.net/Articles/751061/
    
       -  Implementation as an LSM, with a default stackable lockdown LSM
          module. This allows the lockdown feature to be policy-driven,
          rather than encoding an implicit policy within the mechanism.
    
      The new locked_down LSM hook is provided to allow LSMs to make a
      policy decision around whether kernel functionality that would allow
      tampering with or examining the runtime state of the kernel should be
      permitted.
    
      The included lockdown LSM provides an implementation with a simple
      policy intended for general purpose use. This policy provides a coarse
      level of granularity, controllable via the kernel command line:
    
        lockdown={integrity|confidentiality}
    
      Enable the kernel lockdown feature. If set to integrity, kernel features
      that allow userland to modify the running kernel are disabled. If set to
      confidentiality, kernel features that allow userland to extract
      confidential information from the kernel are also disabled.
    
      This may also be controlled via /sys/kernel/security/lockdown and
      overriden by kernel configuration.
    
      New or existing LSMs may implement finer-grained controls of the
      lockdown features. Refer to the lockdown_reason documentation in
      include/linux/security.h for details.
    
      The lockdown feature has had signficant design feedback and review
      across many subsystems. This code has been in linux-next for some
      weeks, with a few fixes applied along the way.
    
      Stephen Rothwell noted that commit 9d1f8be5cf42 ("bpf: Restrict bpf
      when kernel lockdown is in confidentiality mode") is missing a
      Signed-off-by from its author. Matthew responded that he is providing
      this under category (c) of the DCO"
    
    * 'next-lockdown' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (31 commits)
      kexec: Fix file verification on S390
      security: constify some arrays in lockdown LSM
      lockdown: Print current->comm in restriction messages
      efi: Restrict efivar_ssdt_load when the kernel is locked down
      tracefs: Restrict tracefs when the kernel is locked down
      debugfs: Restrict debugfs when the kernel is locked down
      kexec: Allow kexec_file() with appropriate IMA policy when locked down
      lockdown: Lock down perf when in confidentiality mode
      bpf: Restrict bpf when kernel lockdown is in confidentiality mode
      lockdown: Lock down tracing and perf kprobes when in confidentiality mode
      lockdown: Lock down /proc/kcore
      x86/mmiotrace: Lock down the testmmiotrace module
      lockdown: Lock down module params that specify hardware parameters (eg. ioport)
      lockdown: Lock down TIOCSSERIAL
      lockdown: Prohibit PCMCIA CIS storage when the kernel is locked down
      acpi: Disable ACPI table override if the kernel is locked down
      acpi: Ignore acpi_rsdp kernel param when the kernel has been locked down
      ACPI: Limit access to custom_method when the kernel is locked down
      x86/msr: Restrict MSR access when the kernel is locked down
      x86: Lock down IO port access when the kernel is locked down
      ...

commit e6b1db98cf4d54d9ea59cfcc195f70dc946fdd38
Author: Matthew Garrett <matthewgarrett@google.com>
Date:   Mon Aug 19 17:17:37 2019 -0700

    security: Support early LSMs
    
    The lockdown module is intended to allow for kernels to be locked down
    early in boot - sufficiently early that we don't have the ability to
    kmalloc() yet. Add support for early initialisation of some LSMs, and
    then add them to the list of names when we do full initialisation later.
    Early LSMs are initialised in link order and cannot be overridden via
    boot parameters, and cannot make use of kmalloc() (since the allocator
    isn't initialised yet).
    
    (Fixed by Stephen Rothwell to include a stub to fix builds when
    !CONFIG_SECURITY)
    
    Signed-off-by: Matthew Garrett <mjg59@google.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Casey Schaufler <casey@schaufler-ca.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 088987e9a3ea..c1807d14daa3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -208,8 +208,13 @@
 			__start_lsm_info = .;				\
 			KEEP(*(.lsm_info.init))				\
 			__end_lsm_info = .;
+#define EARLY_LSM_TABLE()	. = ALIGN(8);				\
+			__start_early_lsm_info = .;			\
+			KEEP(*(.early_lsm_info.init))			\
+			__end_early_lsm_info = .;
 #else
 #define LSM_TABLE()
+#define EARLY_LSM_TABLE()
 #endif
 
 #define ___OF_TABLE(cfg, name)	_OF_TABLE_##cfg(name)
@@ -609,7 +614,8 @@
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(timer)						\
 	EARLYCON_TABLE()						\
-	LSM_TABLE()
+	LSM_TABLE()							\
+	EARLY_LSM_TABLE()
 
 #define INIT_TEXT							\
 	*(.init.text .init.text.*)					\

commit aac09ce27556f79f20a860ae89d790d7bfbf1747
Merge: c3c08f939abe 6c395f66e98c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 17 13:13:41 2019 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/rzhang/linux
    
    Pull thermal management updates from Zhang Rui:
    
     - Convert thermal documents to ReST (Mauro Carvalho Chehab)
    
     - Fix a cyclic depedency in between thermal core and governors (Daniel
       Lezcano)
    
     - Fix processor_thermal_device driver to re-evaluate power limits after
       resume (Srinivas Pandruvada, Zhang Rui)
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/rzhang/linux:
      drivers: thermal: processor_thermal_device: Fix build warning
      docs: thermal: convert to ReST
      thermal/drivers/core: Use governor table to initialize
      thermal/drivers/core: Add init section table for self-encapsulation
      drivers: thermal: processor_thermal: Read PPCC on resume

commit 980af75ede4f36107b98aa5c247359b87c6afc30
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Jun 12 22:13:24 2019 +0200

    thermal/drivers/core: Add init section table for self-encapsulation
    
    Currently the governors are declared in their respective files but they
    export their [un]register functions which in turn call the [un]register
    governors core's functions. That implies a cyclic dependency which is
    not desirable. There is a way to self-encapsulate the governors by letting
    them to declare themselves in a __init section table.
    
    Define the table in the asm generic linker description like the other
    tables and provide the specific macros to deal with.
    
    Reviewed-by: Amit Kucheria <amit.kucheria@linaro.org>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 088987e9a3ea..5a06822fcd6c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -239,6 +239,16 @@
 #define ACPI_PROBE_TABLE(name)
 #endif
 
+#ifdef CONFIG_THERMAL
+#define THERMAL_TABLE(name)						\
+	. = ALIGN(8);							\
+	__##name##_thermal_table = .;					\
+	KEEP(*(__##name##_thermal_table))				\
+	__##name##_thermal_table_end = .;
+#else
+#define THERMAL_TABLE(name)
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	__dtb_start = .;						\
@@ -608,6 +618,7 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(timer)						\
+	THERMAL_TABLE(governor)						\
 	EARLYCON_TABLE()						\
 	LSM_TABLE()
 

commit 6ca6366220ed285e29ee22f4cf5c68a0397cb005
Author: Sven Schnelle <svens@stackframe.org>
Date:   Wed Jun 5 22:32:22 2019 +0200

    parisc: add dynamic ftrace
    
    This patch implements dynamic ftrace for PA-RISC. The required mcount
    call sequences can get pretty long, so instead of patching the
    whole call sequence out of the functions, we are using
    -fpatchable-function-entry from gcc. This puts a configurable amount of
    NOPS before/at the start of the function. Taking do_sys_open() as example,
    which would look like this when the call is patched out:
    
    1036b248:       08 00 02 40     nop
    1036b24c:       08 00 02 40     nop
    1036b250:       08 00 02 40     nop
    1036b254:       08 00 02 40     nop
    
    1036b258 <do_sys_open>:
    1036b258:       08 00 02 40     nop
    1036b25c:       08 03 02 41     copy r3,r1
    1036b260:       6b c2 3f d9     stw rp,-14(sp)
    1036b264:       08 1e 02 43     copy sp,r3
    1036b268:       6f c1 01 00     stw,ma r1,80(sp)
    
    When ftrace gets enabled for this function the kernel will patch these
    NOPs to:
    
    1036b248:       10 19 57 20     <address of ftrace>
    1036b24c:       6f c1 00 80     stw,ma r1,40(sp)
    1036b250:       48 21 3f d1     ldw -18(r1),r1
    1036b254:       e8 20 c0 02     bv,n r0(r1)
    
    1036b258 <do_sys_open>:
    1036b258:       e8 3f 1f df     b,l,n .-c,r1
    1036b25c:       08 03 02 41     copy r3,r1
    1036b260:       6b c2 3f d9     stw rp,-14(sp)
    1036b264:       08 1e 02 43     copy sp,r3
    1036b268:       6f c1 01 00     stw,ma r1,80(sp)
    
    So the first NOP in do_sys_open() will be patched to jump backwards into
    some minimal trampoline code which pushes a stackframe, saves r1 which
    holds the return address, loads the address of the real ftrace function,
    and branches to that location. For 64 Bit things are getting a bit more
    complicated (and longer) because we must make sure that the address of
    ftrace location is 8 byte aligned, and the offset passed to ldd for
    fetching the address is 8 byte aligned as well.
    
    Note that gcc has a bug which misplaces the function label, and needs a
    patch to make dynamic ftrace work. See
    https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90751 for details.
    
    Signed-off-by: Sven Schnelle <svens@stackframe.org>
    Signed-off-by: Helge Deller <deller@gmx.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 088987e9a3ea..ca42182992a5 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -110,10 +110,17 @@
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
+#ifdef CC_USING_PATCHABLE_FUNCTION_ENTRY
+#define MCOUNT_REC()	. = ALIGN(8);				\
+			__start_mcount_loc = .;			\
+			KEEP(*(__patchable_function_entries))	\
+			__stop_mcount_loc = .;
+#else
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
 			KEEP(*(__mcount_loc))			\
 			__stop_mcount_loc = .;
+#endif
 #else
 #define MCOUNT_REC()
 #endif

commit 280664f558c9d973315d48f125eb664cc607d089
Merge: e0654264c480 dadec066d8fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 14 10:55:54 2019 -0700

    Merge tag 'modules-for-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeyu/linux
    
    Pull modules updates from Jessica Yu:
    
     - Use a separate table to store symbol types instead of hijacking
       fields in struct Elf_Sym
    
     - Trivial code cleanups
    
    * tag 'modules-for-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/jeyu/linux:
      module: add stubs for within_module functions
      kallsyms: store type information in its own array
      vmlinux.lds.h: drop unused __vermagic

commit 898490c010b5d2e499e03b7e815fc214209ac583
Author: Alexey Gladkov <gladkov.alexey@gmail.com>
Date:   Mon Apr 29 18:11:14 2019 +0200

    moduleparam: Save information about built-in modules in separate file
    
    Problem:
    
    When a kernel module is compiled as a separate module, some important
    information about the kernel module is available via .modinfo section of
    the module.  In contrast, when the kernel module is compiled into the
    kernel, that information is not available.
    
    Information about built-in modules is necessary in the following cases:
    
    1. When it is necessary to find out what additional parameters can be
    passed to the kernel at boot time.
    
    2. When you need to know which module names and their aliases are in
    the kernel. This is very useful for creating an initrd image.
    
    Proposal:
    
    The proposed patch does not remove .modinfo section with module
    information from the vmlinux at the build time and saves it into a
    separate file after kernel linking. So, the kernel does not increase in
    size and no additional information remains in it. Information is stored
    in the same format as in the separate modules (null-terminated string
    array). Because the .modinfo section is already exported with a separate
    modules, we are not creating a new API.
    
    It can be easily read in the userspace:
    
    $ tr '\0' '\n' < modules.builtin.modinfo
    ext4.softdep=pre: crc32c
    ext4.license=GPL
    ext4.description=Fourth Extended Filesystem
    ext4.author=Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others
    ext4.alias=fs-ext4
    ext4.alias=ext3
    ext4.alias=fs-ext3
    ext4.alias=ext2
    ext4.alias=fs-ext2
    md_mod.alias=block-major-9-*
    md_mod.alias=md
    md_mod.description=MD RAID framework
    md_mod.license=GPL
    md_mod.parmtype=create_on_open:bool
    md_mod.parmtype=start_dirty_degraded:int
    ...
    
    Co-Developed-by: Gleb Fotengauer-Malinovskiy <glebfm@altlinux.org>
    Signed-off-by: Gleb Fotengauer-Malinovskiy <glebfm@altlinux.org>
    Signed-off-by: Alexey Gladkov <gladkov.alexey@gmail.com>
    Acked-by: Jessica Yu <jeyu@kernel.org>
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f8f6f04c4453..bbb9e332f2fe 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -844,6 +844,7 @@
 	EXIT_CALL							\
 	*(.discard)							\
 	*(.discard.*)							\
+	*(.modinfo)							\
 	}
 
 /**

commit 9672e2cb0fbdcb11d64ac43bcb4ee86a76b4221f
Author: Mathias Krause <minipli@googlemail.com>
Date:   Sun Dec 30 13:40:04 2018 +0100

    vmlinux.lds.h: drop unused __vermagic
    
    The reference to '__vermagic' is a relict from v2.5 times. And even
    there it had a very short life time, from v2.5.59 (commit 1d411b80ee18
    ("Module Sanity Check") in the historic tree) to v2.5.69 (commit
    67ac5b866bda ("[PATCH] complete modinfo section")).
    
    Neither current kernels nor modules contain a '__vermagic' section any
    more, so get rid of it.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jessica Yu <jeyu@kernel.org>
    Signed-off-by: Jessica Yu <jeyu@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f8f6f04c4453..42844931b851 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -332,7 +332,6 @@
 		__start_rodata = .;					\
 		*(.rodata) *(.rodata.*)					\
 		RO_AFTER_INIT_DATA	/* Read only after init */	\
-		KEEP(*(__vermagic))	/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		__start___tracepoints_ptrs = .;				\
 		KEEP(*(__tracepoints_ptrs)) /* Tracepoints: pointer array */ \

commit f76a16adc485699f95bb71fce114f97c832fe664
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Mar 6 11:07:24 2019 -0600

    x86/unwind/orc: Fix ORC unwind table alignment
    
    The .orc_unwind section is a packed array of 6-byte structs.  It's
    currently aligned to 6 bytes, which is causing warnings in the LLD
    linker.
    
    Six isn't a power of two, so it's not a valid alignment value.  The
    actual alignment doesn't matter much because it's an array of packed
    structs.  An alignment of two is sufficient.  In reality it always gets
    aligned to four bytes because it comes immediately after the
    4-byte-aligned .orc_unwind_ip section.
    
    Fixes: ee9f8fce9964 ("x86/unwind: Add the ORC unwinder")
    Reported-by: Nick Desaulniers <ndesaulniers@google.com>
    Reported-by: Dmitry Golovin <dima@golovin.in>
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Link: https://github.com/ClangBuiltLinux/linux/issues/218
    Link: https://lkml.kernel.org/r/d55027ee95fe73e952dcd8be90aebd31b0095c45.1551892041.git.jpoimboe@redhat.com

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 3d7a6a9c2370..f8f6f04c4453 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -733,7 +733,7 @@
 		KEEP(*(.orc_unwind_ip))					\
 		__stop_orc_unwind_ip = .;				\
 	}								\
-	. = ALIGN(6);							\
+	. = ALIGN(2);							\
 	.orc_unwind : AT(ADDR(.orc_unwind) - LOAD_OFFSET) {		\
 		__start_orc_unwind = .;					\
 		KEEP(*(.orc_unwind))					\

commit 638820d8da8ededd6dc609beaef02d5396599c03
Merge: d5e4d81da4d4 3f6caaf5ff33
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:49:35 2018 +0100

    Merge branch 'next-general' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "In this patchset, there are a couple of minor updates, as well as some
      reworking of the LSM initialization code from Kees Cook (these prepare
      the way for ordered stackable LSMs, but are a valuable cleanup on
      their own)"
    
    * 'next-general' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security:
      LSM: Don't ignore initialization failures
      LSM: Provide init debugging infrastructure
      LSM: Record LSM name in struct lsm_info
      LSM: Convert security_initcall() into DEFINE_LSM()
      vmlinux.lds.h: Move LSM_TABLE into INIT_DATA
      LSM: Convert from initcall to struct lsm_info
      LSM: Remove initcall tracing
      LSM: Rename .security_initcall section to .lsm_info
      vmlinux.lds.h: Avoid copy/paste of security_init section
      LSM: Correctly announce start of LSM initialization
      security: fix LSM description location
      keys: Fix the use of the C++ keyword "private" in uapi/linux/keyctl.h
      seccomp: remove unnecessary unlikely()
      security: tomoyo: Fix obsolete function
      security/capabilities: remove check for -EINVAL

commit 0200fbdd431519d730b5d399a12840ec832b27cc
Merge: de3fbb2aa802 01a14bda11ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 13:08:53 2018 +0100

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and misc x86 updates from Ingo Molnar:
     "Lots of changes in this cycle - in part because locking/core attracted
      a number of related x86 low level work which was easier to handle in a
      single tree:
    
       - Linux Kernel Memory Consistency Model updates (Alan Stern, Paul E.
         McKenney, Andrea Parri)
    
       - lockdep scalability improvements and micro-optimizations (Waiman
         Long)
    
       - rwsem improvements (Waiman Long)
    
       - spinlock micro-optimization (Matthew Wilcox)
    
       - qspinlocks: Provide a liveness guarantee (more fairness) on x86.
         (Peter Zijlstra)
    
       - Add support for relative references in jump tables on arm64, x86
         and s390 to optimize jump labels (Ard Biesheuvel, Heiko Carstens)
    
       - Be a lot less permissive on weird (kernel address) uaccess faults
         on x86: BUG() when uaccess helpers fault on kernel addresses (Jann
         Horn)
    
       - macrofy x86 asm statements to un-confuse the GCC inliner. (Nadav
         Amit)
    
       - ... and a handful of other smaller changes as well"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      locking/lockdep: Make global debug_locks* variables read-mostly
      locking/lockdep: Fix debug_locks off performance problem
      locking/pvqspinlock: Extend node size when pvqspinlock is configured
      locking/qspinlock_stat: Count instances of nested lock slowpaths
      locking/qspinlock, x86: Provide liveness guarantee
      x86/asm: 'Simplify' GEN_*_RMWcc() macros
      locking/qspinlock: Rework some comments
      locking/qspinlock: Re-order code
      locking/lockdep: Remove duplicated 'lock_class_ops' percpu array
      x86/defconfig: Enable CONFIG_USB_XHCI_HCD=y
      futex: Replace spin_is_locked() with lockdep
      locking/lockdep: Make class->ops a percpu counter and move it under CONFIG_DEBUG_LOCKDEP=y
      x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs
      x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs
      x86/extable: Macrofy inline assembly code to work around GCC inlining bugs
      x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops
      x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs
      x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs
      x86/refcount: Work around GCC inlining bug
      x86/objtool: Use asm macros to work around GCC inlining bugs
      ...

commit 52c8ee5bad8f33d02c567f6609f43d69303fc48d
Author: Peter Oberparleiter <oberpar@linux.ibm.com>
Date:   Thu Sep 13 13:00:00 2018 +0200

    vmlinux.lds.h: Fix linker warnings about orphan .LPBX sections
    
    Enabling both CONFIG_LD_DEAD_CODE_DATA_ELIMINATION=y and
    CONFIG_GCOV_PROFILE_ALL=y results in linker warnings:
    
      warning: orphan section `.data..LPBX1' being placed in
      section `.data..LPBX1'.
    
    LD_DEAD_CODE_DATA_ELIMINATION adds compiler flag -fdata-sections. This
    option causes GCC to create separate data sections for data objects,
    including those generated by GCC internally for gcov profiling. The
    names of these objects start with a dot (.LPBX0, .LPBX1), resulting in
    section names starting with 'data..'.
    
    As section names starting with 'data..' are used for specific purposes
    in the Linux kernel, the linker script does not automatically include
    them in the output data section, resulting in the "orphan section"
    linker warnings.
    
    Fix this by specifically including sections named "data..LPBX*" in the
    data section.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Tested-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Tested-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Peter Oberparleiter <oberpar@linux.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b4d74b1c1e1d..d7701d466b60 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -68,7 +68,7 @@
  */
 #ifdef CONFIG_LD_DEAD_CODE_DATA_ELIMINATION
 #define TEXT_MAIN .text .text.[0-9a-zA-Z_]*
-#define DATA_MAIN .data .data.[0-9a-zA-Z_]*
+#define DATA_MAIN .data .data.[0-9a-zA-Z_]* .data..LPBX*
 #define SDATA_MAIN .sdata .sdata.[0-9a-zA-Z_]*
 #define RODATA_MAIN .rodata .rodata.[0-9a-zA-Z_]*
 #define BSS_MAIN .bss .bss.[0-9a-zA-Z_]*

commit 8dcf86caa1e3daf4a6ccf38e97f4f752b411f829
Author: Peter Oberparleiter <oberpar@linux.ibm.com>
Date:   Thu Sep 13 12:59:59 2018 +0200

    vmlinux.lds.h: Fix incomplete .text.exit discards
    
    Enabling CONFIG_GCOV_PROFILE_ALL=y causes linker errors on ARM:
    
      `.text.exit' referenced in section `.ARM.exidx.text.exit':
      defined in discarded section `.text.exit'
    
      `.text.exit' referenced in section `.fini_array.00100':
      defined in discarded section `.text.exit'
    
    And related errors on NDS32:
    
      `.text.exit' referenced in section `.dtors.65435':
      defined in discarded section `.text.exit'
    
    The gcov compiler flags cause certain compiler versions to generate
    additional destructor-related sections that are not yet handled by the
    linker script, resulting in references between discarded and
    non-discarded sections.
    
    Since destructors are not used in the Linux kernel, fix this by
    discarding these additional sections.
    
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Tested-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Reported-by: Greentime Hu <green.hu@gmail.com>
    Tested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Peter Oberparleiter <oberpar@linux.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7b75ff6e2fce..b4d74b1c1e1d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -613,8 +613,8 @@
 
 #define EXIT_DATA							\
 	*(.exit.data .exit.data.*)					\
-	*(.fini_array)							\
-	*(.dtors)							\
+	*(.fini_array .fini_array.*)					\
+	*(.dtors .dtors.*)						\
 	MEM_DISCARD(exit.data*)						\
 	MEM_DISCARD(exit.rodata*)
 

commit 3ac946d12e344a48c1192ef8910c6095a0d6a8ac
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 10 17:18:22 2018 -0700

    vmlinux.lds.h: Move LSM_TABLE into INIT_DATA
    
    Since the struct lsm_info table is not an initcall, we can just move it
    into INIT_DATA like all the other tables.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Casey Schaufler <casey@schaufler-ca.com>
    Reviewed-by: John Johansen <john.johansen@canonical.com>
    Reviewed-by: James Morris <james.morris@microsoft.com>
    Signed-off-by: James Morris <james.morris@microsoft.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 5079a969e612..b31ea8bdfef9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -203,6 +203,15 @@
 #define EARLYCON_TABLE()
 #endif
 
+#ifdef CONFIG_SECURITY
+#define LSM_TABLE()	. = ALIGN(8);					\
+			__start_lsm_info = .;				\
+			KEEP(*(.lsm_info.init))				\
+			__end_lsm_info = .;
+#else
+#define LSM_TABLE()
+#endif
+
 #define ___OF_TABLE(cfg, name)	_OF_TABLE_##cfg(name)
 #define __OF_TABLE(cfg, name)	___OF_TABLE(cfg, name)
 #define OF_TABLE(cfg, name)	__OF_TABLE(IS_ENABLED(cfg), name)
@@ -597,7 +606,8 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(timer)						\
-	EARLYCON_TABLE()
+	EARLYCON_TABLE()						\
+	LSM_TABLE()
 
 #define INIT_TEXT							\
 	*(.init.text .init.text.*)					\
@@ -786,17 +796,6 @@
 		KEEP(*(.con_initcall.init))				\
 		__con_initcall_end = .;
 
-#define SECURITY_INITCALL						\
-		__start_lsm_info = .;					\
-		KEEP(*(.lsm_info.init))					\
-		__end_lsm_info = .;
-
-/* Older linker script style for security init. */
-#define SECURITY_INIT							\
-	.lsm_info.init : AT(ADDR(.lsm_info.init) - LOAD_OFFSET) {	\
-		LSM_INFO						\
-	}
-
 #ifdef CONFIG_BLK_DEV_INITRD
 #define INIT_RAM_FS							\
 	. = ALIGN(4);							\
@@ -963,7 +962,6 @@
 		INIT_SETUP(initsetup_align)				\
 		INIT_CALLS						\
 		CON_INITCALL						\
-		SECURITY_INITCALL					\
 		INIT_RAM_FS						\
 	}
 

commit b048ae6e6c7062809e4398f4d0bfe80870715d3c
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 10 17:18:19 2018 -0700

    LSM: Rename .security_initcall section to .lsm_info
    
    In preparation for switching from initcall to just a regular set of
    pointers in a section, rename the internal section name.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Casey Schaufler <casey@schaufler-ca.com>
    Reviewed-by: James Morris <james.morris@microsoft.com>
    Reviewed-by: John Johansen <john.johansen@canonical.com>
    Signed-off-by: James Morris <james.morris@microsoft.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 934a45395547..5079a969e612 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -787,14 +787,14 @@
 		__con_initcall_end = .;
 
 #define SECURITY_INITCALL						\
-		__security_initcall_start = .;				\
-		KEEP(*(.security_initcall.init))			\
-		__security_initcall_end = .;
+		__start_lsm_info = .;					\
+		KEEP(*(.lsm_info.init))					\
+		__end_lsm_info = .;
 
 /* Older linker script style for security init. */
 #define SECURITY_INIT							\
-	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
-		SECURITY_INITCALL					\
+	.lsm_info.init : AT(ADDR(.lsm_info.init) - LOAD_OFFSET) {	\
+		LSM_INFO						\
 	}
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 1e80cd1672bc77c96fa72205ba6db78dc10825b4
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 10 17:18:18 2018 -0700

    vmlinux.lds.h: Avoid copy/paste of security_init section
    
    Avoid copy/paste by defining SECURITY_INIT in terms of SECURITY_INITCALL.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Casey Schaufler <casey@schaufler-ca.com>
    Reviewed-by: James Morris <james.morris@microsoft.com>
    Reviewed-by: John Johansen <john.johansen@canonical.com>
    Signed-off-by: James Morris <james.morris@microsoft.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7b75ff6e2fce..934a45395547 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -473,13 +473,6 @@
 #define RODATA          RO_DATA_SECTION(4096)
 #define RO_DATA(align)  RO_DATA_SECTION(align)
 
-#define SECURITY_INIT							\
-	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
-		__security_initcall_start = .;				\
-		KEEP(*(.security_initcall.init))			\
-		__security_initcall_end = .;				\
-	}
-
 /*
  * .text section. Map to function alignment to avoid address changes
  * during second ld run in second ld pass when generating System.map
@@ -798,6 +791,12 @@
 		KEEP(*(.security_initcall.init))			\
 		__security_initcall_end = .;
 
+/* Older linker script style for security init. */
+#define SECURITY_INIT							\
+	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
+		SECURITY_INITCALL					\
+	}
+
 #ifdef CONFIG_BLK_DEV_INITRD
 #define INIT_RAM_FS							\
 	. = ALIGN(4);							\

commit e872267b8bcbb179e21ccc7118f258873d6e7a59
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:43 2018 -0700

    jump_table: Move entries into ro_after_init region
    
    The __jump_table sections emitted into the core kernel and into
    each module consist of statically initialized references into
    other parts of the code, and with the exception of entries that
    point into init code, which are defused at post-init time, these
    data structures are never modified.
    
    So let's move them into the ro_after_init section, to prevent them
    from being corrupted inadvertently by buggy code, or deliberately
    by an attacker.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Jessica Yu <jeyu@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Link: https://lkml.kernel.org/r/20180919065144.25010-9-ard.biesheuvel@linaro.org

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7b75ff6e2fce..f09ee3c544bc 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -253,10 +253,6 @@
 	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
 	/* implement dynamic printk debug */				\
-	. = ALIGN(8);                                                   \
-	__start___jump_table = .;					\
-	KEEP(*(__jump_table))                                           \
-	__stop___jump_table = .;					\
 	. = ALIGN(8);							\
 	__start___verbose = .;						\
 	KEEP(*(__verbose))                                              \
@@ -300,6 +296,12 @@
 	. = __start_init_task + THREAD_SIZE;				\
 	__end_init_task = .;
 
+#define JUMP_TABLE_DATA							\
+	. = ALIGN(8);							\
+	__start___jump_table = .;					\
+	KEEP(*(__jump_table))						\
+	__stop___jump_table = .;
+
 /*
  * Allow architectures to handle ro_after_init data on their
  * own by defining an empty RO_AFTER_INIT_DATA.
@@ -308,6 +310,7 @@
 #define RO_AFTER_INIT_DATA						\
 	__start_ro_after_init = .;					\
 	*(.data..ro_after_init)						\
+	JUMP_TABLE_DATA							\
 	__end_ro_after_init = .;
 #endif
 

commit 7953002a7c6561c93defd19c81737012ef5a10dc
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Aug 21 00:06:24 2018 +0900

    vmlinux.lds.h: remove stale <linux/export.h> include
    
    This is unneeded since commit a62143850053 ("vmlinux.lds.h: remove
    no-op macro VMLINUX_SYMBOL()").
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f173b5f30dbe..7b75ff6e2fce 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -54,8 +54,6 @@
 #define LOAD_OFFSET 0
 #endif
 
-#include <linux/export.h>
-
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 

commit ac6bbf0cdf4206c517ac9789814c23e372ebce4d
Author: Rob Herring <robh@kernel.org>
Date:   Mon Jul 9 09:41:52 2018 -0600

    iommu: Remove IOMMU_OF_DECLARE
    
    Now that we use the driver core to stop deferred probe for missing
    drivers, IOMMU_OF_DECLARE can be removed.
    
    This is slightly less optimal than having a list of built-in drivers in
    that we'll now defer probe twice before giving up. This shouldn't have a
    significant impact on boot times as past discussions about deferred
    probe have given no evidence of deferred probe having a substantial
    impact.
    
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Kukjin Kim <kgene@kernel.org>
    Cc: Krzysztof Kozlowski <krzk@kernel.org>
    Cc: Rob Clark <robdclark@gmail.com>
    Cc: Heiko Stuebner <heiko@sntech.de>
    Cc: Frank Rowand <frowand.list@gmail.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-samsung-soc@vger.kernel.org
    Cc: linux-arm-msm@vger.kernel.org
    Cc: linux-rockchip@lists.infradead.org
    Cc: devicetree@vger.kernel.org
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e373e2e10f6a..f173b5f30dbe 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -218,7 +218,6 @@
 #define TIMER_OF_TABLES()	OF_TABLE(CONFIG_TIMER_OF, timer)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
-#define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
 #define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
 #define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
 #define CPUIDLE_METHOD_OF_TABLES() OF_TABLE(CONFIG_CPU_IDLE, cpuidle_method)
@@ -601,7 +600,6 @@
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\
 	TIMER_OF_TABLES()						\
-	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	CPUIDLE_METHOD_OF_TABLES()					\
 	KERNEL_DTB()							\

commit 266ff2a8f51f02b429a987d87634697eb0d01d6a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed May 9 22:59:58 2018 +1000

    kbuild: Fix asm-generic/vmlinux.lds.h for LD_DEAD_CODE_DATA_ELIMINATION
    
    KEEP more tables, and add the function/data section wildcard to more
    section selections.
    
    This is a little ad-hoc at the moment, but kernel code should be moved
    to consistently use .text..x (note: double dots) for explicit sections
    and all references to it in the linker script can be made with
    TEXT_MAIN, and similarly for other sections.
    
    For now, let's see if major architectures move to enabling this option
    then we can do some refactoring passes. Otherwise if it remains unused
    or superseded by LTO, this may not be required.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f4980c72d389..e373e2e10f6a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -64,15 +64,24 @@
  * generates .data.identifier sections, which need to be pulled in with
  * .data. We don't want to pull in .data..other sections, which Linux
  * has defined. Same for text and bss.
+ *
+ * RODATA_MAIN is not used because existing code already defines .rodata.x
+ * sections to be brought in with rodata.
  */
 #ifdef CONFIG_LD_DEAD_CODE_DATA_ELIMINATION
 #define TEXT_MAIN .text .text.[0-9a-zA-Z_]*
 #define DATA_MAIN .data .data.[0-9a-zA-Z_]*
+#define SDATA_MAIN .sdata .sdata.[0-9a-zA-Z_]*
+#define RODATA_MAIN .rodata .rodata.[0-9a-zA-Z_]*
 #define BSS_MAIN .bss .bss.[0-9a-zA-Z_]*
+#define SBSS_MAIN .sbss .sbss.[0-9a-zA-Z_]*
 #else
 #define TEXT_MAIN .text
 #define DATA_MAIN .data
+#define SDATA_MAIN .sdata
+#define RODATA_MAIN .rodata
 #define BSS_MAIN .bss
+#define SBSS_MAIN .sbss
 #endif
 
 /*
@@ -105,7 +114,7 @@
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 #define MCOUNT_REC()	. = ALIGN(8);				\
 			__start_mcount_loc = .;			\
-			*(__mcount_loc)				\
+			KEEP(*(__mcount_loc))			\
 			__stop_mcount_loc = .;
 #else
 #define MCOUNT_REC()
@@ -113,7 +122,7 @@
 
 #ifdef CONFIG_TRACE_BRANCH_PROFILING
 #define LIKELY_PROFILE()	__start_annotated_branch_profile = .;	\
-				*(_ftrace_annotated_branch)		\
+				KEEP(*(_ftrace_annotated_branch))	\
 				__stop_annotated_branch_profile = .;
 #else
 #define LIKELY_PROFILE()
@@ -121,7 +130,7 @@
 
 #ifdef CONFIG_PROFILE_ALL_BRANCHES
 #define BRANCH_PROFILE()	__start_branch_profile = .;		\
-				*(_ftrace_branch)			\
+				KEEP(*(_ftrace_branch))			\
 				__stop_branch_profile = .;
 #else
 #define BRANCH_PROFILE()
@@ -238,8 +247,8 @@
 	*(DATA_MAIN)							\
 	*(.ref.data)							\
 	*(.data..shared_aligned) /* percpu related */			\
-	MEM_KEEP(init.data)						\
-	MEM_KEEP(exit.data)						\
+	MEM_KEEP(init.data*)						\
+	MEM_KEEP(exit.data*)						\
 	*(.data.unlikely)						\
 	__start_once = .;						\
 	*(.data.once)							\
@@ -289,8 +298,8 @@
 	__start_init_task = .;						\
 	init_thread_union = .;						\
 	init_stack = .;							\
-	*(.data..init_task)						\
-	*(.data..init_thread_info)					\
+	KEEP(*(.data..init_task))					\
+	KEEP(*(.data..init_thread_info))				\
 	. = __start_init_task + THREAD_SIZE;				\
 	__end_init_task = .;
 
@@ -487,8 +496,8 @@
 		*(.text.hot TEXT_MAIN .text.fixup .text.unlikely)	\
 		*(.text..refcount)					\
 		*(.ref.text)						\
-	MEM_KEEP(init.text)						\
-	MEM_KEEP(exit.text)						\
+	MEM_KEEP(init.text*)						\
+	MEM_KEEP(exit.text*)						\
 
 
 /* sched.text is aling to function alignment to secure we have same
@@ -538,7 +547,7 @@
 		__softirqentry_text_end = .;
 
 /* Section used for early init (in .S files) */
-#define HEAD_TEXT  *(.head.text)
+#define HEAD_TEXT  KEEP(*(.head.text))
 
 #define HEAD_TEXT_SECTION							\
 	.head.text : AT(ADDR(.head.text) - LOAD_OFFSET) {		\
@@ -579,11 +588,11 @@
 /* init and exit section handling */
 #define INIT_DATA							\
 	KEEP(*(SORT(___kentry+*)))					\
-	*(.init.data)							\
-	MEM_DISCARD(init.data)						\
+	*(.init.data init.data.*)					\
+	MEM_DISCARD(init.data*)						\
 	KERNEL_CTORS()							\
 	MCOUNT_REC()							\
-	*(.init.rodata)							\
+	*(.init.rodata .init.rodata.*)					\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
 	KPROBE_BLACKLIST()						\
@@ -602,16 +611,16 @@
 	EARLYCON_TABLE()
 
 #define INIT_TEXT							\
-	*(.init.text)							\
+	*(.init.text .init.text.*)					\
 	*(.text.startup)						\
-	MEM_DISCARD(init.text)
+	MEM_DISCARD(init.text*)
 
 #define EXIT_DATA							\
-	*(.exit.data)							\
+	*(.exit.data .exit.data.*)					\
 	*(.fini_array)							\
 	*(.dtors)							\
-	MEM_DISCARD(exit.data)						\
-	MEM_DISCARD(exit.rodata)
+	MEM_DISCARD(exit.data*)						\
+	MEM_DISCARD(exit.rodata*)
 
 #define EXIT_TEXT							\
 	*(.exit.text)							\
@@ -629,7 +638,7 @@
 	. = ALIGN(sbss_align);						\
 	.sbss : AT(ADDR(.sbss) - LOAD_OFFSET) {				\
 		*(.dynsbss)						\
-		*(.sbss)						\
+		*(SBSS_MAIN)						\
 		*(.scommon)						\
 	}
 
@@ -754,7 +763,7 @@
 #define NOTES								\
 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
 		__start_notes = .;					\
-		*(.note.*)						\
+		KEEP(*(.note.*))					\
 		__stop_notes = .;					\
 	}
 

commit a6214385005333202c8cc1744c7075a9e1a26b9a
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Wed May 9 16:23:51 2018 +0900

    vmlinux.lds.h: remove no-op macro VMLINUX_SYMBOL()
    
    Now that VMLINUX_SYMBOL() is no-op, clean up the linker script.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Reviewed-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index af240573e482..f4980c72d389 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -104,66 +104,66 @@
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
 #define MCOUNT_REC()	. = ALIGN(8);				\
-			VMLINUX_SYMBOL(__start_mcount_loc) = .; \
+			__start_mcount_loc = .;			\
 			*(__mcount_loc)				\
-			VMLINUX_SYMBOL(__stop_mcount_loc) = .;
+			__stop_mcount_loc = .;
 #else
 #define MCOUNT_REC()
 #endif
 
 #ifdef CONFIG_TRACE_BRANCH_PROFILING
-#define LIKELY_PROFILE()	VMLINUX_SYMBOL(__start_annotated_branch_profile) = .; \
-				*(_ftrace_annotated_branch)			      \
-				VMLINUX_SYMBOL(__stop_annotated_branch_profile) = .;
+#define LIKELY_PROFILE()	__start_annotated_branch_profile = .;	\
+				*(_ftrace_annotated_branch)		\
+				__stop_annotated_branch_profile = .;
 #else
 #define LIKELY_PROFILE()
 #endif
 
 #ifdef CONFIG_PROFILE_ALL_BRANCHES
-#define BRANCH_PROFILE()	VMLINUX_SYMBOL(__start_branch_profile) = .;   \
-				*(_ftrace_branch)			      \
-				VMLINUX_SYMBOL(__stop_branch_profile) = .;
+#define BRANCH_PROFILE()	__start_branch_profile = .;		\
+				*(_ftrace_branch)			\
+				__stop_branch_profile = .;
 #else
 #define BRANCH_PROFILE()
 #endif
 
 #ifdef CONFIG_KPROBES
 #define KPROBE_BLACKLIST()	. = ALIGN(8);				      \
-				VMLINUX_SYMBOL(__start_kprobe_blacklist) = .; \
+				__start_kprobe_blacklist = .;		      \
 				KEEP(*(_kprobe_blacklist))		      \
-				VMLINUX_SYMBOL(__stop_kprobe_blacklist) = .;
+				__stop_kprobe_blacklist = .;
 #else
 #define KPROBE_BLACKLIST()
 #endif
 
 #ifdef CONFIG_FUNCTION_ERROR_INJECTION
 #define ERROR_INJECT_WHITELIST()	STRUCT_ALIGN();			      \
-			VMLINUX_SYMBOL(__start_error_injection_whitelist) = .;\
+			__start_error_injection_whitelist = .;		      \
 			KEEP(*(_error_injection_whitelist))		      \
-			VMLINUX_SYMBOL(__stop_error_injection_whitelist) = .;
+			__stop_error_injection_whitelist = .;
 #else
 #define ERROR_INJECT_WHITELIST()
 #endif
 
 #ifdef CONFIG_EVENT_TRACING
 #define FTRACE_EVENTS()	. = ALIGN(8);					\
-			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
+			__start_ftrace_events = .;			\
 			KEEP(*(_ftrace_events))				\
-			VMLINUX_SYMBOL(__stop_ftrace_events) = .;	\
-			VMLINUX_SYMBOL(__start_ftrace_eval_maps) = .;	\
+			__stop_ftrace_events = .;			\
+			__start_ftrace_eval_maps = .;			\
 			KEEP(*(_ftrace_eval_map))			\
-			VMLINUX_SYMBOL(__stop_ftrace_eval_maps) = .;
+			__stop_ftrace_eval_maps = .;
 #else
 #define FTRACE_EVENTS()
 #endif
 
 #ifdef CONFIG_TRACING
-#define TRACE_PRINTKS() VMLINUX_SYMBOL(__start___trace_bprintk_fmt) = .;      \
+#define TRACE_PRINTKS()	 __start___trace_bprintk_fmt = .;      \
 			 KEEP(*(__trace_printk_fmt)) /* Trace_printk fmt' pointer */ \
-			 VMLINUX_SYMBOL(__stop___trace_bprintk_fmt) = .;
-#define TRACEPOINT_STR() VMLINUX_SYMBOL(__start___tracepoint_str) = .;	\
+			 __stop___trace_bprintk_fmt = .;
+#define TRACEPOINT_STR() __start___tracepoint_str = .;	\
 			 KEEP(*(__tracepoint_str)) /* Trace_printk fmt' pointer */ \
-			 VMLINUX_SYMBOL(__stop___tracepoint_str) = .;
+			 __stop___tracepoint_str = .;
 #else
 #define TRACE_PRINTKS()
 #define TRACEPOINT_STR()
@@ -171,27 +171,27 @@
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 #define TRACE_SYSCALLS() . = ALIGN(8);					\
-			 VMLINUX_SYMBOL(__start_syscalls_metadata) = .;	\
+			 __start_syscalls_metadata = .;			\
 			 KEEP(*(__syscalls_metadata))			\
-			 VMLINUX_SYMBOL(__stop_syscalls_metadata) = .;
+			 __stop_syscalls_metadata = .;
 #else
 #define TRACE_SYSCALLS()
 #endif
 
 #ifdef CONFIG_BPF_EVENTS
 #define BPF_RAW_TP() STRUCT_ALIGN();					\
-			 VMLINUX_SYMBOL(__start__bpf_raw_tp) = .;	\
+			 __start__bpf_raw_tp = .;			\
 			 KEEP(*(__bpf_raw_tp_map))			\
-			 VMLINUX_SYMBOL(__stop__bpf_raw_tp) = .;
+			 __stop__bpf_raw_tp = .;
 #else
 #define BPF_RAW_TP()
 #endif
 
 #ifdef CONFIG_SERIAL_EARLYCON
 #define EARLYCON_TABLE() . = ALIGN(8);				\
-			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
+			 __earlycon_table = .;			\
 			 KEEP(*(__earlycon_table))		\
-			 VMLINUX_SYMBOL(__earlycon_table_end) = .;
+			 __earlycon_table_end = .;
 #else
 #define EARLYCON_TABLE()
 #endif
@@ -202,7 +202,7 @@
 #define _OF_TABLE_0(name)
 #define _OF_TABLE_1(name)						\
 	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__##name##_of_table) = .;			\
+	__##name##_of_table = .;					\
 	KEEP(*(__##name##_of_table))					\
 	KEEP(*(__##name##_of_table_end))
 
@@ -217,18 +217,18 @@
 #ifdef CONFIG_ACPI
 #define ACPI_PROBE_TABLE(name)						\
 	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__##name##_acpi_probe_table) = .;		\
+	__##name##_acpi_probe_table = .;				\
 	KEEP(*(__##name##_acpi_probe_table))				\
-	VMLINUX_SYMBOL(__##name##_acpi_probe_table_end) = .;
+	__##name##_acpi_probe_table_end = .;
 #else
 #define ACPI_PROBE_TABLE(name)
 #endif
 
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
-	VMLINUX_SYMBOL(__dtb_start) = .;				\
+	__dtb_start = .;						\
 	KEEP(*(.dtb.init.rodata))					\
-	VMLINUX_SYMBOL(__dtb_end) = .;
+	__dtb_end = .;
 
 /*
  * .data section
@@ -241,20 +241,20 @@
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
 	*(.data.unlikely)						\
-	VMLINUX_SYMBOL(__start_once) = .;				\
+	__start_once = .;						\
 	*(.data.once)							\
-	VMLINUX_SYMBOL(__end_once) = .;					\
+	__end_once = .;							\
 	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
 	/* implement dynamic printk debug */				\
 	. = ALIGN(8);                                                   \
-	VMLINUX_SYMBOL(__start___jump_table) = .;                       \
+	__start___jump_table = .;					\
 	KEEP(*(__jump_table))                                           \
-	VMLINUX_SYMBOL(__stop___jump_table) = .;                        \
+	__stop___jump_table = .;					\
 	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__start___verbose) = .;                          \
+	__start___verbose = .;						\
 	KEEP(*(__verbose))                                              \
-	VMLINUX_SYMBOL(__stop___verbose) = .;				\
+	__stop___verbose = .;						\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
 	TRACE_PRINTKS()							\
@@ -266,10 +266,10 @@
  */
 #define NOSAVE_DATA							\
 	. = ALIGN(PAGE_SIZE);						\
-	VMLINUX_SYMBOL(__nosave_begin) = .;				\
+	__nosave_begin = .;						\
 	*(.data..nosave)						\
 	. = ALIGN(PAGE_SIZE);						\
-	VMLINUX_SYMBOL(__nosave_end) = .;
+	__nosave_end = .;
 
 #define PAGE_ALIGNED_DATA(page_align)					\
 	. = ALIGN(page_align);						\
@@ -286,13 +286,13 @@
 
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
-	VMLINUX_SYMBOL(__start_init_task) = .;				\
-	VMLINUX_SYMBOL(init_thread_union) = .;				\
-	VMLINUX_SYMBOL(init_stack) = .;					\
+	__start_init_task = .;						\
+	init_thread_union = .;						\
+	init_stack = .;							\
 	*(.data..init_task)						\
 	*(.data..init_thread_info)					\
-	. = VMLINUX_SYMBOL(__start_init_task) + THREAD_SIZE;		\
-	VMLINUX_SYMBOL(__end_init_task) = .;
+	. = __start_init_task + THREAD_SIZE;				\
+	__end_init_task = .;
 
 /*
  * Allow architectures to handle ro_after_init data on their
@@ -300,9 +300,9 @@
  */
 #ifndef RO_AFTER_INIT_DATA
 #define RO_AFTER_INIT_DATA						\
-	VMLINUX_SYMBOL(__start_ro_after_init) = .;			\
+	__start_ro_after_init = .;					\
 	*(.data..ro_after_init)						\
-	VMLINUX_SYMBOL(__end_ro_after_init) = .;
+	__end_ro_after_init = .;
 #endif
 
 /*
@@ -311,14 +311,14 @@
 #define RO_DATA_SECTION(align)						\
 	. = ALIGN((align));						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start_rodata) = .;			\
+		__start_rodata = .;					\
 		*(.rodata) *(.rodata.*)					\
 		RO_AFTER_INIT_DATA	/* Read only after init */	\
 		KEEP(*(__vermagic))	/* Kernel version magic */	\
 		. = ALIGN(8);						\
-		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\
+		__start___tracepoints_ptrs = .;				\
 		KEEP(*(__tracepoints_ptrs)) /* Tracepoints: pointer array */ \
-		VMLINUX_SYMBOL(__stop___tracepoints_ptrs) = .;		\
+		__stop___tracepoints_ptrs = .;				\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
 	}								\
 									\
@@ -328,109 +328,109 @@
 									\
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
+		__start_pci_fixups_early = .;				\
 		KEEP(*(.pci_fixup_early))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_early) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_header) = .;		\
+		__end_pci_fixups_early = .;				\
+		__start_pci_fixups_header = .;				\
 		KEEP(*(.pci_fixup_header))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_header) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_final) = .;		\
+		__end_pci_fixups_header = .;				\
+		__start_pci_fixups_final = .;				\
 		KEEP(*(.pci_fixup_final))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_final) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_enable) = .;		\
+		__end_pci_fixups_final = .;				\
+		__start_pci_fixups_enable = .;				\
 		KEEP(*(.pci_fixup_enable))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_enable) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_resume) = .;		\
+		__end_pci_fixups_enable = .;				\
+		__start_pci_fixups_resume = .;				\
 		KEEP(*(.pci_fixup_resume))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_resume) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_resume_early) = .;	\
+		__end_pci_fixups_resume = .;				\
+		__start_pci_fixups_resume_early = .;			\
 		KEEP(*(.pci_fixup_resume_early))			\
-		VMLINUX_SYMBOL(__end_pci_fixups_resume_early) = .;	\
-		VMLINUX_SYMBOL(__start_pci_fixups_suspend) = .;		\
+		__end_pci_fixups_resume_early = .;			\
+		__start_pci_fixups_suspend = .;				\
 		KEEP(*(.pci_fixup_suspend))				\
-		VMLINUX_SYMBOL(__end_pci_fixups_suspend) = .;		\
-		VMLINUX_SYMBOL(__start_pci_fixups_suspend_late) = .;	\
+		__end_pci_fixups_suspend = .;				\
+		__start_pci_fixups_suspend_late = .;			\
 		KEEP(*(.pci_fixup_suspend_late))			\
-		VMLINUX_SYMBOL(__end_pci_fixups_suspend_late) = .;	\
+		__end_pci_fixups_suspend_late = .;			\
 	}								\
 									\
 	/* Built-in firmware blobs */					\
 	.builtin_fw        : AT(ADDR(.builtin_fw) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start_builtin_fw) = .;			\
+		__start_builtin_fw = .;					\
 		KEEP(*(.builtin_fw))					\
-		VMLINUX_SYMBOL(__end_builtin_fw) = .;			\
+		__end_builtin_fw = .;					\
 	}								\
 									\
 	TRACEDATA							\
 									\
 	/* Kernel symbol table: Normal symbols */			\
 	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start___ksymtab) = .;			\
+		__start___ksymtab = .;					\
 		KEEP(*(SORT(___ksymtab+*)))				\
-		VMLINUX_SYMBOL(__stop___ksymtab) = .;			\
+		__stop___ksymtab = .;					\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__ksymtab_gpl     : AT(ADDR(__ksymtab_gpl) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start___ksymtab_gpl) = .;		\
+		__start___ksymtab_gpl = .;				\
 		KEEP(*(SORT(___ksymtab_gpl+*)))				\
-		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
+		__stop___ksymtab_gpl = .;				\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__ksymtab_unused  : AT(ADDR(__ksymtab_unused) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start___ksymtab_unused) = .;		\
+		__start___ksymtab_unused = .;				\
 		KEEP(*(SORT(___ksymtab_unused+*)))			\
-		VMLINUX_SYMBOL(__stop___ksymtab_unused) = .;		\
+		__stop___ksymtab_unused = .;				\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__ksymtab_unused_gpl : AT(ADDR(__ksymtab_unused_gpl) - LOAD_OFFSET) { \
-		VMLINUX_SYMBOL(__start___ksymtab_unused_gpl) = .;	\
+		__start___ksymtab_unused_gpl = .;			\
 		KEEP(*(SORT(___ksymtab_unused_gpl+*)))			\
-		VMLINUX_SYMBOL(__stop___ksymtab_unused_gpl) = .;	\
+		__stop___ksymtab_unused_gpl = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__ksymtab_gpl_future : AT(ADDR(__ksymtab_gpl_future) - LOAD_OFFSET) { \
-		VMLINUX_SYMBOL(__start___ksymtab_gpl_future) = .;	\
+		__start___ksymtab_gpl_future = .;			\
 		KEEP(*(SORT(___ksymtab_gpl_future+*)))			\
-		VMLINUX_SYMBOL(__stop___ksymtab_gpl_future) = .;	\
+		__stop___ksymtab_gpl_future = .;			\
 	}								\
 									\
 	/* Kernel symbol table: Normal symbols */			\
 	__kcrctab         : AT(ADDR(__kcrctab) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start___kcrctab) = .;			\
+		__start___kcrctab = .;					\
 		KEEP(*(SORT(___kcrctab+*)))				\
-		VMLINUX_SYMBOL(__stop___kcrctab) = .;			\
+		__stop___kcrctab = .;					\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__kcrctab_gpl     : AT(ADDR(__kcrctab_gpl) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start___kcrctab_gpl) = .;		\
+		__start___kcrctab_gpl = .;				\
 		KEEP(*(SORT(___kcrctab_gpl+*)))				\
-		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
+		__stop___kcrctab_gpl = .;				\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__kcrctab_unused  : AT(ADDR(__kcrctab_unused) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start___kcrctab_unused) = .;		\
+		__start___kcrctab_unused = .;				\
 		KEEP(*(SORT(___kcrctab_unused+*)))			\
-		VMLINUX_SYMBOL(__stop___kcrctab_unused) = .;		\
+		__stop___kcrctab_unused = .;				\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__kcrctab_unused_gpl : AT(ADDR(__kcrctab_unused_gpl) - LOAD_OFFSET) { \
-		VMLINUX_SYMBOL(__start___kcrctab_unused_gpl) = .;	\
+		__start___kcrctab_unused_gpl = .;			\
 		KEEP(*(SORT(___kcrctab_unused_gpl+*)))			\
-		VMLINUX_SYMBOL(__stop___kcrctab_unused_gpl) = .;	\
+		__stop___kcrctab_unused_gpl = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__kcrctab_gpl_future : AT(ADDR(__kcrctab_gpl_future) - LOAD_OFFSET) { \
-		VMLINUX_SYMBOL(__start___kcrctab_gpl_future) = .;	\
+		__start___kcrctab_gpl_future = .;			\
 		KEEP(*(SORT(___kcrctab_gpl_future+*)))			\
-		VMLINUX_SYMBOL(__stop___kcrctab_gpl_future) = .;	\
+		__stop___kcrctab_gpl_future = .;			\
 	}								\
 									\
 	/* Kernel symbol table: strings */				\
@@ -447,18 +447,18 @@
 									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
-		VMLINUX_SYMBOL(__start___param) = .;			\
+		__start___param = .;					\
 		KEEP(*(__param))					\
-		VMLINUX_SYMBOL(__stop___param) = .;			\
+		__stop___param = .;					\
 	}								\
 									\
 	/* Built-in module versions. */					\
 	__modver : AT(ADDR(__modver) - LOAD_OFFSET) {			\
-		VMLINUX_SYMBOL(__start___modver) = .;			\
+		__start___modver = .;					\
 		KEEP(*(__modver))					\
-		VMLINUX_SYMBOL(__stop___modver) = .;			\
+		__stop___modver = .;					\
 		. = ALIGN((align));					\
-		VMLINUX_SYMBOL(__end_rodata) = .;			\
+		__end_rodata = .;					\
 	}								\
 	. = ALIGN((align));
 
@@ -469,9 +469,9 @@
 
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
-		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
+		__security_initcall_start = .;				\
 		KEEP(*(.security_initcall.init))			\
-		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
+		__security_initcall_end = .;				\
 	}
 
 /*
@@ -495,47 +495,47 @@
  * address even at second ld pass when generating System.map */
 #define SCHED_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__sched_text_start) = .;			\
+		__sched_text_start = .;					\
 		*(.sched.text)						\
-		VMLINUX_SYMBOL(__sched_text_end) = .;
+		__sched_text_end = .;
 
 /* spinlock.text is aling to function alignment to secure we have same
  * address even at second ld pass when generating System.map */
 #define LOCK_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__lock_text_start) = .;			\
+		__lock_text_start = .;					\
 		*(.spinlock.text)					\
-		VMLINUX_SYMBOL(__lock_text_end) = .;
+		__lock_text_end = .;
 
 #define CPUIDLE_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__cpuidle_text_start) = .;		\
+		__cpuidle_text_start = .;				\
 		*(.cpuidle.text)					\
-		VMLINUX_SYMBOL(__cpuidle_text_end) = .;
+		__cpuidle_text_end = .;
 
 #define KPROBES_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__kprobes_text_start) = .;		\
+		__kprobes_text_start = .;				\
 		*(.kprobes.text)					\
-		VMLINUX_SYMBOL(__kprobes_text_end) = .;
+		__kprobes_text_end = .;
 
 #define ENTRY_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__entry_text_start) = .;			\
+		__entry_text_start = .;					\
 		*(.entry.text)						\
-		VMLINUX_SYMBOL(__entry_text_end) = .;
+		__entry_text_end = .;
 
 #define IRQENTRY_TEXT							\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__irqentry_text_start) = .;		\
+		__irqentry_text_start = .;				\
 		*(.irqentry.text)					\
-		VMLINUX_SYMBOL(__irqentry_text_end) = .;
+		__irqentry_text_end = .;
 
 #define SOFTIRQENTRY_TEXT						\
 		ALIGN_FUNCTION();					\
-		VMLINUX_SYMBOL(__softirqentry_text_start) = .;		\
+		__softirqentry_text_start = .;				\
 		*(.softirqentry.text)					\
-		VMLINUX_SYMBOL(__softirqentry_text_end) = .;
+		__softirqentry_text_end = .;
 
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  *(.head.text)
@@ -551,9 +551,9 @@
 #define EXCEPTION_TABLE(align)						\
 	. = ALIGN(align);						\
 	__ex_table : AT(ADDR(__ex_table) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start___ex_table) = .;			\
+		__start___ex_table = .;					\
 		KEEP(*(__ex_table))					\
-		VMLINUX_SYMBOL(__stop___ex_table) = .;			\
+		__stop___ex_table = .;					\
 	}
 
 /*
@@ -567,11 +567,11 @@
 
 #ifdef CONFIG_CONSTRUCTORS
 #define KERNEL_CTORS()	. = ALIGN(8);			   \
-			VMLINUX_SYMBOL(__ctors_start) = .; \
+			__ctors_start = .;		   \
 			KEEP(*(.ctors))			   \
 			KEEP(*(SORT(.init_array.*)))	   \
 			KEEP(*(.init_array))		   \
-			VMLINUX_SYMBOL(__ctors_end) = .;
+			__ctors_end = .;
 #else
 #define KERNEL_CTORS()
 #endif
@@ -706,9 +706,9 @@
 #define BUG_TABLE							\
 	. = ALIGN(8);							\
 	__bug_table : AT(ADDR(__bug_table) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start___bug_table) = .;		\
+		__start___bug_table = .;				\
 		KEEP(*(__bug_table))					\
-		VMLINUX_SYMBOL(__stop___bug_table) = .;			\
+		__stop___bug_table = .;					\
 	}
 #else
 #define BUG_TABLE
@@ -718,22 +718,22 @@
 #define ORC_UNWIND_TABLE						\
 	. = ALIGN(4);							\
 	.orc_unwind_ip : AT(ADDR(.orc_unwind_ip) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__start_orc_unwind_ip) = .;		\
+		__start_orc_unwind_ip = .;				\
 		KEEP(*(.orc_unwind_ip))					\
-		VMLINUX_SYMBOL(__stop_orc_unwind_ip) = .;		\
+		__stop_orc_unwind_ip = .;				\
 	}								\
 	. = ALIGN(6);							\
 	.orc_unwind : AT(ADDR(.orc_unwind) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start_orc_unwind) = .;			\
+		__start_orc_unwind = .;					\
 		KEEP(*(.orc_unwind))					\
-		VMLINUX_SYMBOL(__stop_orc_unwind) = .;			\
+		__stop_orc_unwind = .;					\
 	}								\
 	. = ALIGN(4);							\
 	.orc_lookup : AT(ADDR(.orc_lookup) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(orc_lookup) = .;				\
+		orc_lookup = .;						\
 		. += (((SIZEOF(.text) + LOOKUP_BLOCK_SIZE - 1) /	\
 			LOOKUP_BLOCK_SIZE) + 1) * 4;			\
-		VMLINUX_SYMBOL(orc_lookup_end) = .;			\
+		orc_lookup_end = .;					\
 	}
 #else
 #define ORC_UNWIND_TABLE
@@ -743,9 +743,9 @@
 #define TRACEDATA							\
 	. = ALIGN(4);							\
 	.tracedata : AT(ADDR(.tracedata) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__tracedata_start) = .;			\
+		__tracedata_start = .;					\
 		KEEP(*(.tracedata))					\
-		VMLINUX_SYMBOL(__tracedata_end) = .;			\
+		__tracedata_end = .;					\
 	}
 #else
 #define TRACEDATA
@@ -753,24 +753,24 @@
 
 #define NOTES								\
 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
-		VMLINUX_SYMBOL(__start_notes) = .;			\
+		__start_notes = .;					\
 		*(.note.*)						\
-		VMLINUX_SYMBOL(__stop_notes) = .;			\
+		__stop_notes = .;					\
 	}
 
 #define INIT_SETUP(initsetup_align)					\
 		. = ALIGN(initsetup_align);				\
-		VMLINUX_SYMBOL(__setup_start) = .;			\
+		__setup_start = .;					\
 		KEEP(*(.init.setup))					\
-		VMLINUX_SYMBOL(__setup_end) = .;
+		__setup_end = .;
 
 #define INIT_CALLS_LEVEL(level)						\
-		VMLINUX_SYMBOL(__initcall##level##_start) = .;		\
+		__initcall##level##_start = .;				\
 		KEEP(*(.initcall##level##.init))			\
 		KEEP(*(.initcall##level##s.init))			\
 
 #define INIT_CALLS							\
-		VMLINUX_SYMBOL(__initcall_start) = .;			\
+		__initcall_start = .;					\
 		KEEP(*(.initcallearly.init))				\
 		INIT_CALLS_LEVEL(0)					\
 		INIT_CALLS_LEVEL(1)					\
@@ -781,22 +781,22 @@
 		INIT_CALLS_LEVEL(rootfs)				\
 		INIT_CALLS_LEVEL(6)					\
 		INIT_CALLS_LEVEL(7)					\
-		VMLINUX_SYMBOL(__initcall_end) = .;
+		__initcall_end = .;
 
 #define CON_INITCALL							\
-		VMLINUX_SYMBOL(__con_initcall_start) = .;		\
+		__con_initcall_start = .;				\
 		KEEP(*(.con_initcall.init))				\
-		VMLINUX_SYMBOL(__con_initcall_end) = .;
+		__con_initcall_end = .;
 
 #define SECURITY_INITCALL						\
-		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
+		__security_initcall_start = .;				\
 		KEEP(*(.security_initcall.init))			\
-		VMLINUX_SYMBOL(__security_initcall_end) = .;
+		__security_initcall_end = .;
 
 #ifdef CONFIG_BLK_DEV_INITRD
 #define INIT_RAM_FS							\
 	. = ALIGN(4);							\
-	VMLINUX_SYMBOL(__initramfs_start) = .;				\
+	__initramfs_start = .;						\
 	KEEP(*(.init.ramfs))						\
 	. = ALIGN(8);							\
 	KEEP(*(.init.ramfs.info))
@@ -851,7 +851,7 @@
  * sharing between subsections for different purposes.
  */
 #define PERCPU_INPUT(cacheline)						\
-	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
+	__per_cpu_start = .;						\
 	*(.data..percpu..first)						\
 	. = ALIGN(PAGE_SIZE);						\
 	*(.data..percpu..page_aligned)					\
@@ -861,7 +861,7 @@
 	*(.data..percpu)						\
 	*(.data..percpu..shared_aligned)				\
 	PERCPU_DECRYPTED_SECTION					\
-	VMLINUX_SYMBOL(__per_cpu_end) = .;
+	__per_cpu_end = .;
 
 /**
  * PERCPU_VADDR - define output section for percpu area
@@ -888,12 +888,11 @@
  * address, use PERCPU_SECTION.
  */
 #define PERCPU_VADDR(cacheline, vaddr, phdr)				\
-	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
-	.data..percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
-				- LOAD_OFFSET) {			\
+	__per_cpu_load = .;						\
+	.data..percpu vaddr : AT(__per_cpu_load - LOAD_OFFSET) {	\
 		PERCPU_INPUT(cacheline)					\
 	} phdr								\
-	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data..percpu);
+	. = __per_cpu_load + SIZEOF(.data..percpu);
 
 /**
  * PERCPU_SECTION - define output section for percpu area, simple version
@@ -910,7 +909,7 @@
 #define PERCPU_SECTION(cacheline)					\
 	. = ALIGN(PAGE_SIZE);						\
 	.data..percpu	: AT(ADDR(.data..percpu) - LOAD_OFFSET) {	\
-		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
+		__per_cpu_load = .;					\
 		PERCPU_INPUT(cacheline)					\
 	}
 
@@ -949,9 +948,9 @@
 #define INIT_TEXT_SECTION(inittext_align)				\
 	. = ALIGN(inittext_align);					\
 	.init.text : AT(ADDR(.init.text) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(_sinittext) = .;				\
+		_sinittext = .;						\
 		INIT_TEXT						\
-		VMLINUX_SYMBOL(_einittext) = .;				\
+		_einittext = .;						\
 	}
 
 #define INIT_DATA_SECTION(initsetup_align)				\
@@ -966,8 +965,8 @@
 
 #define BSS_SECTION(sbss_align, bss_align, stop_align)			\
 	. = ALIGN(sbss_align);						\
-	VMLINUX_SYMBOL(__bss_start) = .;				\
+	__bss_start = .;						\
 	SBSS(sbss_align)						\
 	BSS(bss_align)							\
 	. = ALIGN(stop_align);						\
-	VMLINUX_SYMBOL(__bss_stop) = .;
+	__bss_stop = .;

commit dd709e72cb934eefd44de8d9969097173fbf45dc
Author: Daniel Kurtz <djkurtz@chromium.org>
Date:   Fri Apr 6 17:21:53 2018 -0600

    earlycon: Use a pointer table to fix __earlycon_table stride
    
    Commit 99492c39f39f ("earlycon: Fix __earlycon_table stride") tried to fix
    __earlycon_table stride by forcing the earlycon_id struct alignment to 32
    and asking the linker to 32-byte align the __earlycon_table symbol.  This
    fix was based on commit 07fca0e57fca92 ("tracing: Properly align linker
    defined symbols") which tried a similar fix for the tracing subsystem.
    
    However, this fix doesn't quite work because there is no guarantee that
    gcc will place structures packed into an array format.  In fact, gcc 4.9
    chooses to 64-byte align these structs by inserting additional padding
    between the entries because it has no clue that they are supposed to be in
    an array.  If we are unlucky, the linker will assign symbol
    "__earlycon_table" to a 32-byte aligned address which does not correspond
    to the 64-byte aligned contents of section "__earlycon_table".
    
    To address this same problem, the fix to the tracing system was
    subsequently re-implemented using a more robust table of pointers approach
    by commits:
     3d56e331b653 ("tracing: Replace syscall_meta_data struct array with pointer array")
     654986462939 ("tracepoints: Fix section alignment using pointer array")
     e4a9ea5ee7c8 ("tracing: Replace trace_event struct array with pointer array")
    
    Let's use this same "array of pointers to structs" approach for
    EARLYCON_TABLE.
    
    Fixes: 99492c39f39f ("earlycon: Fix __earlycon_table stride")
    Signed-off-by: Daniel Kurtz <djkurtz@chromium.org>
    Suggested-by: Aaron Durbin <adurbin@chromium.org>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Tested-by: Guenter Roeck <groeck@chromium.org>
    Reviewed-by: Guenter Roeck <groeck@chromium.org>
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 278841c75b97..af240573e482 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -188,7 +188,7 @@
 #endif
 
 #ifdef CONFIG_SERIAL_EARLYCON
-#define EARLYCON_TABLE() STRUCT_ALIGN();			\
+#define EARLYCON_TABLE() . = ALIGN(8);				\
 			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
 			 KEEP(*(__earlycon_table))		\
 			 VMLINUX_SYMBOL(__earlycon_table_end) = .;

commit 23221d997b3d28cb80c4d4d1b4bd36610f8e12fc
Merge: 5b1f3dc927a2 65896545b69f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 4 16:01:43 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Nothing particularly stands out here, probably because people were
      tied up with spectre/meltdown stuff last time around. Still, the main
      pieces are:
    
       - Rework of our CPU features framework so that we can whitelist CPUs
         that don't require kpti even in a heterogeneous system
    
       - Support for the IDC/DIC architecture extensions, which allow us to
         elide instruction and data cache maintenance when writing out
         instructions
    
       - Removal of the large memory model which resulted in suboptimal
         codegen by the compiler and increased the use of literal pools,
         which could potentially be used as ROP gadgets since they are
         mapped as executable
    
       - Rework of forced signal delivery so that the siginfo_t is
         well-formed and handling of show_unhandled_signals is consolidated
         and made consistent between different fault types
    
       - More siginfo cleanup based on the initial patches from Eric
         Biederman
    
       - Workaround for Cortex-A55 erratum #1024718
    
       - Some small ACPI IORT updates and cleanups from Lorenzo Pieralisi
    
       - Misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (70 commits)
      arm64: uaccess: Fix omissions from usercopy whitelist
      arm64: fpsimd: Split cpu field out from struct fpsimd_state
      arm64: tlbflush: avoid writing RES0 bits
      arm64: cmpxchg: Include linux/compiler.h in asm/cmpxchg.h
      arm64: move percpu cmpxchg implementation from cmpxchg.h to percpu.h
      arm64: cmpxchg: Include build_bug.h instead of bug.h for BUILD_BUG
      arm64: lse: Include compiler_types.h and export.h for out-of-line LL/SC
      arm64: fpsimd: include <linux/init.h> in fpsimd.h
      drivers/perf: arm_pmu_platform: do not warn about affinity on uniprocessor
      perf: arm_spe: include linux/vmalloc.h for vmap()
      Revert "arm64: Revert L1_CACHE_SHIFT back to 6 (64-byte cache line size)"
      arm64: cpufeature: Avoid warnings due to unused symbols
      arm64: Add work around for Arm Cortex-A55 Erratum 1024718
      arm64: Delay enabling hardware DBM feature
      arm64: Add MIDR encoding for Arm Cortex-A55 and Cortex-A35
      arm64: capabilities: Handle shared entries
      arm64: capabilities: Add support for checks based on a list of MIDRs
      arm64: Add helpers for checking CPU MIDR against a range
      arm64: capabilities: Clean up midr range helpers
      arm64: capabilities: Change scope of VHE to Boot CPU feature
      ...

commit c4f6699dfcb8558d138fe838f741b2c10f416cf9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Mar 28 12:05:37 2018 -0700

    bpf: introduce BPF_RAW_TRACEPOINT
    
    Introduce BPF_PROG_TYPE_RAW_TRACEPOINT bpf program type to access
    kernel internal arguments of the tracepoints in their raw form.
    
    >From bpf program point of view the access to the arguments look like:
    struct bpf_raw_tracepoint_args {
           __u64 args[0];
    };
    
    int bpf_prog(struct bpf_raw_tracepoint_args *ctx)
    {
      // program can read args[N] where N depends on tracepoint
      // and statically verified at program load+attach time
    }
    
    kprobe+bpf infrastructure allows programs access function arguments.
    This feature allows programs access raw tracepoint arguments.
    
    Similar to proposed 'dynamic ftrace events' there are no abi guarantees
    to what the tracepoints arguments are and what their meaning is.
    The program needs to type cast args properly and use bpf_probe_read()
    helper to access struct fields when argument is a pointer.
    
    For every tracepoint __bpf_trace_##call function is prepared.
    In assembler it looks like:
    (gdb) disassemble __bpf_trace_xdp_exception
    Dump of assembler code for function __bpf_trace_xdp_exception:
       0xffffffff81132080 <+0>:     mov    %ecx,%ecx
       0xffffffff81132082 <+2>:     jmpq   0xffffffff811231f0 <bpf_trace_run3>
    
    where
    
    TRACE_EVENT(xdp_exception,
            TP_PROTO(const struct net_device *dev,
                     const struct bpf_prog *xdp, u32 act),
    
    The above assembler snippet is casting 32-bit 'act' field into 'u64'
    to pass into bpf_trace_run3(), while 'dev' and 'xdp' args are passed as-is.
    All of ~500 of __bpf_trace_*() functions are only 5-10 byte long
    and in total this approach adds 7k bytes to .text.
    
    This approach gives the lowest possible overhead
    while calling trace_xdp_exception() from kernel C code and
    transitioning into bpf land.
    Since tracepoint+bpf are used at speeds of 1M+ events per second
    this is valuable optimization.
    
    The new BPF_RAW_TRACEPOINT_OPEN sys_bpf command is introduced
    that returns anon_inode FD of 'bpf-raw-tracepoint' object.
    
    The user space looks like:
    // load bpf prog with BPF_PROG_TYPE_RAW_TRACEPOINT type
    prog_fd = bpf_prog_load(...);
    // receive anon_inode fd for given bpf_raw_tracepoint with prog attached
    raw_tp_fd = bpf_raw_tracepoint_open("xdp_exception", prog_fd);
    
    Ctrl-C of tracing daemon or cmdline tool that uses this feature
    will automatically detach bpf program, unload it and
    unregister tracepoint probe.
    
    On the kernel side the __bpf_raw_tp_map section of pointers to
    tracepoint definition and to __bpf_trace_*() probe function is used
    to find a tracepoint with "xdp_exception" name and
    corresponding __bpf_trace_xdp_exception() probe function
    which are passed to tracepoint_probe_register() to connect probe
    with tracepoint.
    
    Addition of bpf_raw_tracepoint doesn't interfere with ftrace and perf
    tracepoint mechanisms. perf_event_open() can be used in parallel
    on the same tracepoint.
    Multiple bpf_raw_tracepoint_open("xdp_exception", prog_fd) are permitted.
    Each with its own bpf program. The kernel will execute
    all tracepoint probes and all attached bpf programs.
    
    In the future bpf_raw_tracepoints can be extended with
    query/introspection logic.
    
    __bpf_raw_tp_map section logic was contributed by Steven Rostedt
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 1ab0e520d6fc..8add3493a202 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -178,6 +178,15 @@
 #define TRACE_SYSCALLS()
 #endif
 
+#ifdef CONFIG_BPF_EVENTS
+#define BPF_RAW_TP() STRUCT_ALIGN();					\
+			 VMLINUX_SYMBOL(__start__bpf_raw_tp) = .;	\
+			 KEEP(*(__bpf_raw_tp_map))			\
+			 VMLINUX_SYMBOL(__stop__bpf_raw_tp) = .;
+#else
+#define BPF_RAW_TP()
+#endif
+
 #ifdef CONFIG_SERIAL_EARLYCON
 #define EARLYCON_TABLE() STRUCT_ALIGN();			\
 			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
@@ -249,6 +258,7 @@
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
 	TRACE_PRINTKS()							\
+	BPF_RAW_TP()							\
 	TRACEPOINT_STR()
 
 /*

commit c38d08526bc9cadfc64065afd219090a8df97f48
Author: Jia He <jia.he@hxt-semitech.com>
Date:   Tue Feb 6 20:11:34 2018 -0800

    ACPI/IORT: Remove linker section for IORT entries again
    
    In commit 316ca8804ea8 ("ACPI/IORT: Remove linker section for IORT entries
    probing"), iort entries was removed in vmlinux.lds.h. But in
    commit 2fcc112af37f ("clocksource/drivers: Rename clksrc table to timer"),
    this line was back incorrectly.
    
    It does no harm except for adding some useless symbols, so fix it.
    
    Signed-off-by: Jia He <jia.he@hxt-semitech.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 1ab0e520d6fc..58b1dab0cf59 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -589,7 +589,6 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(timer)						\
-	ACPI_PROBE_TABLE(iort)						\
 	EARLYCON_TABLE()
 
 #define INIT_TEXT							\

commit b2fe5fa68642860e7de76167c3111623aa0d5de1
Merge: a103950e0dd2 a54667f6728c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 31 14:31:10 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Significantly shrink the core networking routing structures. Result
        of http://vger.kernel.org/~davem/seoul2017_netdev_keynote.pdf
    
     2) Add netdevsim driver for testing various offloads, from Jakub
        Kicinski.
    
     3) Support cross-chip FDB operations in DSA, from Vivien Didelot.
    
     4) Add a 2nd listener hash table for TCP, similar to what was done for
        UDP. From Martin KaFai Lau.
    
     5) Add eBPF based queue selection to tun, from Jason Wang.
    
     6) Lockless qdisc support, from John Fastabend.
    
     7) SCTP stream interleave support, from Xin Long.
    
     8) Smoother TCP receive autotuning, from Eric Dumazet.
    
     9) Lots of erspan tunneling enhancements, from William Tu.
    
    10) Add true function call support to BPF, from Alexei Starovoitov.
    
    11) Add explicit support for GRO HW offloading, from Michael Chan.
    
    12) Support extack generation in more netlink subsystems. From Alexander
        Aring, Quentin Monnet, and Jakub Kicinski.
    
    13) Add 1000BaseX, flow control, and EEE support to mvneta driver. From
        Russell King.
    
    14) Add flow table abstraction to netfilter, from Pablo Neira Ayuso.
    
    15) Many improvements and simplifications to the NFP driver bpf JIT,
        from Jakub Kicinski.
    
    16) Support for ipv6 non-equal cost multipath routing, from Ido
        Schimmel.
    
    17) Add resource abstration to devlink, from Arkadi Sharshevsky.
    
    18) Packet scheduler classifier shared filter block support, from Jiri
        Pirko.
    
    19) Avoid locking in act_csum, from Davide Caratti.
    
    20) devinet_ioctl() simplifications from Al viro.
    
    21) More TCP bpf improvements from Lawrence Brakmo.
    
    22) Add support for onlink ipv6 route flag, similar to ipv4, from David
        Ahern.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1925 commits)
      tls: Add support for encryption using async offload accelerator
      ip6mr: fix stale iterator
      net/sched: kconfig: Remove blank help texts
      openvswitch: meter: Use 64-bit arithmetic instead of 32-bit
      tcp_nv: fix potential integer overflow in tcpnv_acked
      r8169: fix RTL8168EP take too long to complete driver initialization.
      qmi_wwan: Add support for Quectel EP06
      rtnetlink: enable IFLA_IF_NETNSID for RTM_NEWLINK
      ipmr: Fix ptrdiff_t print formatting
      ibmvnic: Wait for device response when changing MAC
      qlcnic: fix deadlock bug
      tcp: release sk_frag.page in tcp_disconnect
      ipv4: Get the address of interface correctly.
      net_sched: gen_estimator: fix lockdep splat
      net: macb: Handle HRESP error
      net/mlx5e: IPoIB, Fix copy-paste bug in flow steering refactoring
      ipv6: addrconf: break critical section in addrconf_verify_rtnl()
      ipv6: change route cache aging logic
      i40e/i40evf: Update DESC_NEEDED value to reflect larger value
      bnxt_en: cleanup DIM work on device shutdown
      ...

commit 663faf9f7beeaca4ad0176bb96c776eed9dad0c5
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sat Jan 13 02:55:33 2018 +0900

    error-injection: Add injectable error types
    
    Add injectable error types for each error-injectable function.
    
    One motivation of error injection test is to find software flaws,
    mistakes or mis-handlings of expectable errors. If we find such
    flaws by the test, that is a program bug, so we need to fix it.
    
    But if the tester miss input the error (e.g. just return success
    code without processing anything), it causes unexpected behavior
    even if the caller is correctly programmed to handle any errors.
    That is not what we want to test by error injection.
    
    To clarify what type of errors the caller must expect for each
    injectable function, this introduces injectable error types:
    
     - EI_ETYPE_NULL : means the function will return NULL if it
                        fails. No ERR_PTR, just a NULL.
     - EI_ETYPE_ERRNO : means the function will return -ERRNO
                        if it fails.
     - EI_ETYPE_ERRNO_NULL : means the function will return -ERRNO
                           (ERR_PTR) or NULL.
    
    ALLOW_ERROR_INJECTION() macro is expanded to get one of
    NULL, ERRNO, ERRNO_NULL to record the error type for
    each function. e.g.
    
     ALLOW_ERROR_INJECTION(open_ctree, ERRNO)
    
    This error types are shown in debugfs as below.
    
      ====
      / # cat /sys/kernel/debug/error_injection/list
      open_ctree [btrfs]    ERRNO
      io_ctl_init [btrfs]   ERRNO
      ====
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f2068cca5206..ebe544e048cd 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -137,7 +137,7 @@
 #endif
 
 #ifdef CONFIG_FUNCTION_ERROR_INJECTION
-#define ERROR_INJECT_WHITELIST()	. = ALIGN(8);			      \
+#define ERROR_INJECT_WHITELIST()	STRUCT_ALIGN();			      \
 			VMLINUX_SYMBOL(__start_error_injection_whitelist) = .;\
 			KEEP(*(_error_injection_whitelist))		      \
 			VMLINUX_SYMBOL(__stop_error_injection_whitelist) = .;

commit 540adea3809f61115d2a1ea4ed6e627613452ba1
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sat Jan 13 02:55:03 2018 +0900

    error-injection: Separate error-injection from kprobe
    
    Since error-injection framework is not limited to be used
    by kprobes, nor bpf. Other kernel subsystems can use it
    freely for checking safeness of error-injection, e.g.
    livepatch, ftrace etc.
    So this separate error-injection framework from kprobes.
    
    Some differences has been made:
    
    - "kprobe" word is removed from any APIs/structures.
    - BPF_ALLOW_ERROR_INJECTION() is renamed to
      ALLOW_ERROR_INJECTION() since it is not limited for BPF too.
    - CONFIG_FUNCTION_ERROR_INJECTION is the config item of this
      feature. It is automatically enabled if the arch supports
      error injection feature for kprobe or ftrace etc.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a2e8582d094a..f2068cca5206 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -136,13 +136,13 @@
 #define KPROBE_BLACKLIST()
 #endif
 
-#ifdef CONFIG_BPF_KPROBE_OVERRIDE
-#define ERROR_INJECT_LIST()	. = ALIGN(8);						\
-				VMLINUX_SYMBOL(__start_kprobe_error_inject_list) = .;	\
-				KEEP(*(_kprobe_error_inject_list))			\
-				VMLINUX_SYMBOL(__stop_kprobe_error_inject_list) = .;
+#ifdef CONFIG_FUNCTION_ERROR_INJECTION
+#define ERROR_INJECT_WHITELIST()	. = ALIGN(8);			      \
+			VMLINUX_SYMBOL(__start_error_injection_whitelist) = .;\
+			KEEP(*(_error_injection_whitelist))		      \
+			VMLINUX_SYMBOL(__stop_error_injection_whitelist) = .;
 #else
-#define ERROR_INJECT_LIST()
+#define ERROR_INJECT_WHITELIST()
 #endif
 
 #ifdef CONFIG_EVENT_TRACING
@@ -573,7 +573,7 @@
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
 	KPROBE_BLACKLIST()						\
-	ERROR_INJECT_LIST()						\
+	ERROR_INJECT_WHITELIST()					\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ee8b707d9fa9..a564b83bf013 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -268,7 +268,11 @@
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
 	VMLINUX_SYMBOL(__start_init_task) = .;				\
+	VMLINUX_SYMBOL(init_thread_union) = .;				\
+	VMLINUX_SYMBOL(init_stack) = .;					\
 	*(.data..init_task)						\
+	*(.data..init_thread_info)					\
+	. = VMLINUX_SYMBOL(__start_init_task) + THREAD_SIZE;		\
 	VMLINUX_SYMBOL(__end_init_task) = .;
 
 /*

commit 92ace9991da08827e809c2d120108a96a281e7fc
Author: Josef Bacik <jbacik@fb.com>
Date:   Mon Dec 11 11:36:46 2017 -0500

    add infrastructure for tagging functions as error injectable
    
    Using BPF we can override kprob'ed functions and return arbitrary
    values.  Obviously this can be a bit unsafe, so make this feature opt-in
    for functions.  Simply tag a function with KPROBE_ERROR_INJECT_SYMBOL in
    order to give BPF access to that function for error injection purposes.
    
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ee8b707d9fa9..a2e8582d094a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -136,6 +136,15 @@
 #define KPROBE_BLACKLIST()
 #endif
 
+#ifdef CONFIG_BPF_KPROBE_OVERRIDE
+#define ERROR_INJECT_LIST()	. = ALIGN(8);						\
+				VMLINUX_SYMBOL(__start_kprobe_error_inject_list) = .;	\
+				KEEP(*(_kprobe_error_inject_list))			\
+				VMLINUX_SYMBOL(__stop_kprobe_error_inject_list) = .;
+#else
+#define ERROR_INJECT_LIST()
+#endif
+
 #ifdef CONFIG_EVENT_TRACING
 #define FTRACE_EVENTS()	. = ALIGN(8);					\
 			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
@@ -564,6 +573,7 @@
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
 	KPROBE_BLACKLIST()						\
+	ERROR_INJECT_LIST()						\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\

commit b1fca27d384e8418aac84b39f6f5179aecc1b64f
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Nov 17 15:27:03 2017 -0800

    kernel debug: support resetting WARN*_ONCE
    
    I like _ONCE warnings because it's guaranteed that they don't flood the
    log.
    
    During testing I find it useful to reset the state of the once warnings,
    so that I can rerun tests and see if they trigger again, or can
    guarantee that a test run always hits the same warnings.
    
    This patch adds a debugfs interface to reset all the _ONCE warnings so
    that they appear again:
    
      echo 1 > /sys/kernel/debug/clear_warn_once
    
    This is implemented by putting all the warning booleans into a special
    section, and clearing it.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/20171017221455.6740-1-andi@firstfloor.org
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bdcd1caae092..ee8b707d9fa9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -223,6 +223,9 @@
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
 	*(.data.unlikely)						\
+	VMLINUX_SYMBOL(__start_once) = .;				\
+	*(.data.once)							\
+	VMLINUX_SYMBOL(__end_once) = .;					\
 	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
 	/* implement dynamic printk debug */				\

commit d6ec9d9a4def52a5094237564eaf6f6979fd7a27
Merge: 3e2014637c50 91a6a6cfee8a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 14:13:48 2017 -0800

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 core updates from Ingo Molnar:
     "Note that in this cycle most of the x86 topics interacted at a level
      that caused them to be merged into tip:x86/asm - but this should be a
      temporary phenomenon, hopefully we'll back to the usual patterns in
      the next merge window.
    
      The main changes in this cycle were:
    
      Hardware enablement:
    
       - Add support for the Intel UMIP (User Mode Instruction Prevention)
         CPU feature. This is a security feature that disables certain
         instructions such as SGDT, SLDT, SIDT, SMSW and STR. (Ricardo Neri)
    
         [ Note that this is disabled by default for now, there are some
           smaller enhancements in the pipeline that I'll follow up with in
           the next 1-2 days, which allows this to be enabled by default.]
    
       - Add support for the AMD SEV (Secure Encrypted Virtualization) CPU
         feature, on top of SME (Secure Memory Encryption) support that was
         added in v4.14. (Tom Lendacky, Brijesh Singh)
    
       - Enable new SSE/AVX/AVX512 CPU features: AVX512_VBMI2, GFNI, VAES,
         VPCLMULQDQ, AVX512_VNNI, AVX512_BITALG. (Gayatri Kammela)
    
      Other changes:
    
       - A big series of entry code simplifications and enhancements (Andy
         Lutomirski)
    
       - Make the ORC unwinder default on x86 and various objtool
         enhancements. (Josh Poimboeuf)
    
       - 5-level paging enhancements (Kirill A. Shutemov)
    
       - Micro-optimize the entry code a bit (Borislav Petkov)
    
       - Improve the handling of interdependent CPU features in the early
         FPU init code (Andi Kleen)
    
       - Build system enhancements (Changbin Du, Masahiro Yamada)
    
       - ... plus misc enhancements, fixes and cleanups"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (118 commits)
      x86/build: Make the boot image generation less verbose
      selftests/x86: Add tests for the STR and SLDT instructions
      selftests/x86: Add tests for User-Mode Instruction Prevention
      x86/traps: Fix up general protection faults caused by UMIP
      x86/umip: Enable User-Mode Instruction Prevention at runtime
      x86/umip: Force a page fault when unable to copy emulated result to user
      x86/umip: Add emulation code for UMIP instructions
      x86/cpufeature: Add User-Mode Instruction Prevention definitions
      x86/insn-eval: Add support to resolve 16-bit address encodings
      x86/insn-eval: Handle 32-bit address encodings in virtual-8086 mode
      x86/insn-eval: Add wrapper function for 32 and 64-bit addresses
      x86/insn-eval: Add support to resolve 32-bit address encodings
      x86/insn-eval: Compute linear address in several utility functions
      resource: Fix resource_size.cocci warnings
      X86/KVM: Clear encryption attribute when SEV is active
      X86/KVM: Decrypt shared per-cpu variables when SEV is active
      percpu: Introduce DEFINE_PER_CPU_DECRYPTED
      x86: Add support for changing memory encryption attribute in early boot
      x86/io: Unroll string I/O when SEV is active
      x86/boot: Add early boot support when running with SEV active
      ...

commit ac26963a1175c813e3ed21c0d2435b083173136e
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Fri Oct 20 09:30:57 2017 -0500

    percpu: Introduce DEFINE_PER_CPU_DECRYPTED
    
    KVM guest defines three per-CPU variables (steal-time, apf_reason, and
    kvm_pic_eoi) which are shared between a guest and a hypervisor.
    
    When SEV is active, memory is encrypted with a guest-specific key, and if
    the guest OS wants to share the memory region with the hypervisor then it
    must clear the C-bit (i.e set decrypted) before sharing it.
    
    DEFINE_PER_CPU_DECRYPTED can be used to define the per-CPU variables
    which will be shared between a guest and a hypervisor.
    
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Borislav Petkov <bp@suse.de>
    Acked-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: kvm@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Christoph Lameter <cl@linux.com>
    Link: https://lkml.kernel.org/r/20171020143059.3291-16-brijesh.singh@amd.com

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 63e56f6c1877..c58f3805e348 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -777,6 +777,24 @@
 #define INIT_RAM_FS
 #endif
 
+/*
+ * Memory encryption operates on a page basis. Since we need to clear
+ * the memory encryption mask for this section, it needs to be aligned
+ * on a page boundary and be a page-size multiple in length.
+ *
+ * Note: We use a separate section so that only this section gets
+ * decrypted to avoid exposing more than we wish.
+ */
+#ifdef CONFIG_AMD_MEM_ENCRYPT
+#define PERCPU_DECRYPTED_SECTION					\
+	. = ALIGN(PAGE_SIZE);						\
+	*(.data..percpu..decrypted)					\
+	. = ALIGN(PAGE_SIZE);
+#else
+#define PERCPU_DECRYPTED_SECTION
+#endif
+
+
 /*
  * Default discarded sections.
  *
@@ -815,6 +833,7 @@
 	. = ALIGN(cacheline);						\
 	*(.data..percpu)						\
 	*(.data..percpu..shared_aligned)				\
+	PERCPU_DECRYPTED_SECTION					\
 	VMLINUX_SYMBOL(__per_cpu_end) = .;
 
 /**

commit 11af847446ed0d131cf24d16a7ef3d5ea7a49554
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Oct 13 15:02:00 2017 -0500

    x86/unwind: Rename unwinder config options to 'CONFIG_UNWINDER_*'
    
    Rename the unwinder config options from:
    
      CONFIG_ORC_UNWINDER
      CONFIG_FRAME_POINTER_UNWINDER
      CONFIG_GUESS_UNWINDER
    
    to:
    
      CONFIG_UNWINDER_ORC
      CONFIG_UNWINDER_FRAME_POINTER
      CONFIG_UNWINDER_GUESS
    
    ... in order to give them a more logical config namespace.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/73972fc7e2762e91912c6b9584582703d6f1b8cc.1507924831.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8acfc1e099e1..63e56f6c1877 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -687,7 +687,7 @@
 #define BUG_TABLE
 #endif
 
-#ifdef CONFIG_ORC_UNWINDER
+#ifdef CONFIG_UNWINDER_ORC
 #define ORC_UNWIND_TABLE						\
 	. = ALIGN(4);							\
 	.orc_unwind_ip : AT(ADDR(.orc_unwind_ip) - LOAD_OFFSET) {	\

commit 564c9cc84e2adf8a6671c1937f0a9fe3da2a4b0e
Author: Kees Cook <keescook@chromium.org>
Date:   Sat Sep 2 13:09:45 2017 -0700

    locking/refcounts, x86/asm: Use unique .text section for refcount exceptions
    
    Using .text.unlikely for refcount exceptions isn't safe because gcc may
    move entire functions into .text.unlikely (e.g. in6_dev_dev()), which
    would cause any uses of a protected refcount_t function to stay inline
    with the function, triggering the protection unconditionally:
    
            .section        .text.unlikely,"ax",@progbits
            .type   in6_dev_get, @function
    in6_dev_getx:
    .LFB4673:
            .loc 2 4128 0
            .cfi_startproc
    ...
            lock; incl 480(%rbx)
            js 111f
            .pushsection .text.unlikely
    111:    lea 480(%rbx), %rcx
    112:    .byte 0x0f, 0xff
    .popsection
    113:
    
    This creates a unique .text..refcount section and adds an additional
    test to the exception handler to WARN in the case of having none of OF,
    SF, nor ZF set so we can see things like this more easily in the future.
    
    The double dot for the section name keeps it out of the TEXT_MAIN macro
    namespace, to avoid collisions and so it can be put at the end with
    text.unlikely to keep the cold code together.
    
    See commit:
    
      cb87481ee89db ("kbuild: linker script do not match C names unless LD_DEAD_CODE_DATA_ELIMINATION is configured")
    
    ... which matches C names: [a-zA-Z0-9_] but not ".".
    
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Elena <elena.reshetova@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Fixes: 7a46ec0e2f48 ("locking/refcounts, x86/asm: Implement fast refcount overflow protection")
    Link: http://lkml.kernel.org/r/1504382986-49301-2-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8acfc1e099e1..e549bff87c5b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -459,6 +459,7 @@
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
 		*(.text.hot TEXT_MAIN .text.fixup .text.unlikely)	\
+		*(.text..refcount)					\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\

commit a59e57da49f7c3f3de8cf4b7568a0c6c82f5b242
Merge: 0ce5c79f384b d1f936d73683
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 9 14:48:21 2017 -0700

    Merge tag 'for-linus-20170904' of git://git.infradead.org/linux-mtd
    
    Pull MTD updates from Boris Brezillon:
     "General updates:
       - Constify pci_device_id in various drivers
       - Constify device_type
       - Remove pad control code from the Gemini driver
       - Use %pOF to print OF node full_name
       - Various fixes in the physmap_of driver
       - Remove unused vars in mtdswap
       - Check devm_kzalloc() return value in the spear_smi driver
       - Check clk_prepare_enable() return code in the st_spi_fsm driver
       - Create per MTD device debugfs enties
    
      NAND updates, from Boris Brezillon:
       - Fix memory leaks in the core
       - Remove unused NAND locking support
       - Rename nand.h into rawnand.h (preparing support for spi NANDs)
       - Use NAND_MAX_ID_LEN where appropriate
       - Fix support for 20nm Hynix chips
       - Fix support for Samsung and Hynix SLC NANDs
       - Various cleanup, improvements and fixes in the qcom driver
       - Fixes for bugs detected by various static code analysis tools
       - Fix mxc ooblayout definition
       - Add a new part_parsers to tmio and sharpsl platform data in order
         to define a custom list of partition parsers
       - Request the reset line in exclusive mode in the sunxi driver
       - Fix a build error in the orion-nand driver when compiled for ARMv4
       - Allow 64-bit mvebu platforms to select the PXA3XX driver
    
      SPI NOR updates, from Cyrille Pitchen and Marek Vasut:
       - add support to the JEDEC JESD216B specification (SFDP tables).
       - add support to the Intel Denverton SPI flash controller.
       - fix error recovery for Spansion/Cypress SPI NOR memories.
       - fix 4-byte address management for the Aspeed SPI controller.
       - add support to some Microchip SST26 memory parts
       - remove unneeded pinctrl header Write a message for tag:"
    
    * tag 'for-linus-20170904' of git://git.infradead.org/linux-mtd: (74 commits)
      mtd: nand: complain loudly when chip->bits_per_cell is not correctly initialized
      mtd: nand: make Samsung SLC NAND usable again
      mtd: nand: tmio: Register partitions using the parsers
      mfd: tmio: Add partition parsers platform data
      mtd: nand: sharpsl: Register partitions using the parsers
      mtd: nand: sharpsl: Add partition parsers platform data
      mtd: nand: qcom: Support for IPQ8074 QPIC NAND controller
      mtd: nand: qcom: support for IPQ4019 QPIC NAND controller
      dt-bindings: qcom_nandc: IPQ8074 QPIC NAND documentation
      dt-bindings: qcom_nandc: IPQ4019 QPIC NAND documentation
      dt-bindings: qcom_nandc: fix the ipq806x device tree example
      mtd: nand: qcom: support for different DEV_CMD register offsets
      mtd: nand: qcom: QPIC data descriptors handling
      mtd: nand: qcom: enable BAM or ADM mode
      mtd: nand: qcom: erased codeword detection configuration
      mtd: nand: qcom: support for read location registers
      mtd: nand: qcom: support for passing flags in DMA helper functions
      mtd: nand: qcom: add BAM DMA descriptor handling
      mtd: nand: qcom: allocate BAM transaction
      mtd: nand: qcom: DMA mapping support for register read buffer
      ...

commit b0c79f49c343cda8954b3322984c32f258ca4ccb
Merge: f213a6c84c1b dd88a0a0c861
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 09:52:57 2017 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
    
     - Introduce the ORC unwinder, which can be enabled via
       CONFIG_ORC_UNWINDER=y.
    
       The ORC unwinder is a lightweight, Linux kernel specific debuginfo
       implementation, which aims to be DWARF done right for unwinding.
       Objtool is used to generate the ORC unwinder tables during build, so
       the data format is flexible and kernel internal: there's no
       dependency on debuginfo created by an external toolchain.
    
       The ORC unwinder is almost two orders of magnitude faster than the
       (out of tree) DWARF unwinder - which is important for perf call graph
       profiling. It is also significantly simpler and is coded defensively:
       there has not been a single ORC related kernel crash so far, even
       with early versions. (knock on wood!)
    
       But the main advantage is that enabling the ORC unwinder allows
       CONFIG_FRAME_POINTERS to be turned off - which speeds up the kernel
       measurably:
    
       With frame pointers disabled, GCC does not have to add frame pointer
       instrumentation code to every function in the kernel. The kernel's
       .text size decreases by about 3.2%, resulting in better cache
       utilization and fewer instructions executed, resulting in a broad
       kernel-wide speedup. Average speedup of system calls should be
       roughly in the 1-3% range - measurements by Mel Gorman [1] have shown
       a speedup of 5-10% for some function execution intense workloads.
    
       The main cost of the unwinder is that the unwinder data has to be
       stored in RAM: the memory cost is 2-4MB of RAM, depending on kernel
       config - which is a modest cost on modern x86 systems.
    
       Given how young the ORC unwinder code is it's not enabled by default
       - but given the performance advantages the plan is to eventually make
       it the default unwinder on x86.
    
       See Documentation/x86/orc-unwinder.txt for more details.
    
     - Remove lguest support: its intended role was that of a temporary
       proof of concept for virtualization, plus its removal will enable the
       reduction (removal) of the paravirt API as well, so Rusty agreed to
       its removal. (Juergen Gross)
    
     - Clean up and fix FSGS related functionality (Andy Lutomirski)
    
     - Clean up IO access APIs (Andy Shevchenko)
    
     - Enhance the symbol namespace (Jiri Slaby)
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (47 commits)
      objtool: Handle GCC stack pointer adjustment bug
      x86/entry/64: Use ENTRY() instead of ALIGN+GLOBAL for stub32_clone()
      x86/fpu/math-emu: Add ENDPROC to functions
      x86/boot/64: Extract efi_pe_entry() from startup_64()
      x86/boot/32: Extract efi_pe_entry() from startup_32()
      x86/lguest: Remove lguest support
      x86/paravirt/xen: Remove xen_patch()
      objtool: Fix objtool fallthrough detection with function padding
      x86/xen/64: Fix the reported SS and CS in SYSCALL
      objtool: Track DRAP separately from callee-saved registers
      objtool: Fix validate_branch() return codes
      x86: Clarify/fix no-op barriers for text_poke_bp()
      x86/switch_to/64: Rewrite FS/GS switching yet again to fix AMD CPUs
      selftests/x86/fsgsbase: Test selectors 1, 2, and 3
      x86/fsgsbase/64: Report FSBASE and GSBASE correctly in core dumps
      x86/fsgsbase/64: Fully initialize FS and GS state in start_thread_common
      x86/asm: Fix UNWIND_HINT_REGS macro for older binutils
      x86/asm/32: Fix regs_get_register() on segment registers
      x86/xen/64: Rearrange the SYSCALL entries
      x86/asm/32: Remove a bunch of '& 0xffff' from pt_regs segment reads
      ...

commit 5ffa70b2a38befadb920b85fe87f5da2b4cff623
Merge: 75864b301c91 cc4a41fe5541
Author: Boris Brezillon <boris.brezillon@free-electrons.com>
Date:   Mon Aug 28 18:04:15 2017 +0200

    Merge tag 'v4.13-rc7' into mtd/next
    
    Merge v4.13-rc7 back to resolve merge conflicts in
    drivers/mtd/nand/nandsim.c and include/asm-generic/vmlinux.lds.h.

commit 290d9bf2811bd83ae907232176d75690c0f7d82b
Merge: c7f4f994dea2 64aee2a965cf
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Aug 25 11:01:05 2017 +0200

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 129f6c4820dd68be26b516ba6d6e471e63855d47
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 21 22:26:25 2017 +0200

    mtd: only use __xipram annotation when XIP_KERNEL is set
    
    When XIP_KERNEL is enabled, some functions are defined in the .data
    ELF section because we require them to be in RAM whenever we communicate
    with the flash chip. However this causes problems when FTRACE is
    enabled and gcc emits calls to __gnu_mcount_nc in the function
    prolog:
    
    drivers/built-in.o: In function `cfi_chip_setup':
    :(.data+0x272fc): relocation truncated to fit: R_ARM_CALL against symbol `__gnu_mcount_nc' defined in .text section in arch/arm/kernel/built-in.o
    drivers/built-in.o: In function `cfi_probe_chip':
    :(.data+0x27de8): relocation truncated to fit: R_ARM_CALL against symbol `__gnu_mcount_nc' defined in .text section in arch/arm/kernel/built-in.o
    /tmp/ccY172rP.s: Assembler messages:
    /tmp/ccY172rP.s:70: Warning: ignoring changed section attributes for .data
    /tmp/ccY172rP.s: Error: 1 warning, treating warnings as errors
    make[5]: *** [drivers/mtd/chips/cfi_probe.o] Error 1
    /tmp/ccK4rjeO.s: Assembler messages:
    /tmp/ccK4rjeO.s:421: Warning: ignoring changed section attributes for .data
    /tmp/ccK4rjeO.s: Error: 1 warning, treating warnings as errors
    make[5]: *** [drivers/mtd/chips/cfi_util.o] Error 1
    /tmp/ccUvhCYR.s: Assembler messages:
    /tmp/ccUvhCYR.s:1895: Warning: ignoring changed section attributes for .data
    /tmp/ccUvhCYR.s: Error: 1 warning, treating warnings as errors
    
    Specifically, this does not work because the .data section is not
    marked executable, which leads LD to not generate trampolines for
    long calls.
    
    This moves the __xipram functions into their own .xiptext section instead.
    The section is still placed next to .data and located in RAM but is marked
    executable, which avoids the build errors.
    
    Also, we only need to place the XIP functions into a separate section
    if both CONFIG_XIP_KERNEL and CONFIG_MTD_XIP are set: When only MTD_XIP
    is used, the whole kernel is still in RAM and we do not need to worry
    about pulling out the rug under it. When only XIP_KERNEL but not MTD_XIP
    is set, the kernel is in some form of ROM, but we never write to it.
    
    Note that MTD_XIP has been broken on ARM since around 2011 or 2012. I
    have sent another patch[2] to fix compilation, which I plan to merge
    through arm-soc unless there are objections. The obvious alternative
    to that would be to completely rip out the MTD_XIP support from the
    kernel, since obviously nobody has been using it in a long while.
    
    Link: [1] https://patchwork.kernel.org/patch/8109771/
    Link: [2] https://patchwork.kernel.org/patch/9855225/
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Boris Brezillon <boris.brezillon@free-electrons.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index da0be9a8d1de..ff507c178327 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -203,6 +203,7 @@
  * pull in .data..stuff which has its own requirements. Same for bss.
  */
 #define DATA_DATA							\
+	*(.xiptext)							\
 	*(.data .data.[0-9a-zA-Z_]*)					\
 	*(.ref.data)							\
 	*(.data..shared_aligned) /* percpu related */			\

commit 229a71860547ec856b156179a9c6bef2de426f66
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Aug 3 11:38:21 2017 +0900

    irq: Make the irqentry text section unconditional
    
    Generate irqentry and softirqentry text sections without
    any Kconfig dependencies. This will add extra sections, but
    there should be no performace impact.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David S . Miller <davem@davemloft.net>
    Cc: Francis Deslauriers <francis.deslauriers@efficios.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-cris-kernel@axis.com
    Cc: mathieu.desnoyers@efficios.com
    Link: http://lkml.kernel.org/r/150172789110.27216.3955739126693102122.stgit@devbox
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index da0be9a8d1de..62e2395f0a82 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -483,25 +483,17 @@
 		*(.entry.text)						\
 		VMLINUX_SYMBOL(__entry_text_end) = .;
 
-#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)
 #define IRQENTRY_TEXT							\
 		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__irqentry_text_start) = .;		\
 		*(.irqentry.text)					\
 		VMLINUX_SYMBOL(__irqentry_text_end) = .;
-#else
-#define IRQENTRY_TEXT
-#endif
 
-#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)
 #define SOFTIRQENTRY_TEXT						\
 		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__softirqentry_text_start) = .;		\
 		*(.softirqentry.text)					\
 		VMLINUX_SYMBOL(__softirqentry_text_end) = .;
-#else
-#define SOFTIRQENTRY_TEXT
-#endif
 
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  *(.head.text)

commit cb87481ee89dbd6609e227afbf64900fb4e5c930
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 26 22:46:27 2017 +1000

    kbuild: linker script do not match C names unless LD_DEAD_CODE_DATA_ELIMINATION is configured
    
    The .data and .bss sections were modified in the generic linker script to
    pull in sections named .data.<C identifier>, which are generated by gcc with
    -ffunction-sections and -fdata-sections options.
    
    The problem with this pattern is it can also match section names that Linux
    defines explicitly, e.g., .data.unlikely. This can cause Linux sections to
    get moved into the wrong place.
    
    The way to avoid this is to use ".." separators for explicit section names
    (the dot character is valid in a section name but not a C identifier).
    However currently there are sections which don't follow this rule, so for
    now just disable the wild card by default.
    
    Example: http://marc.info/?l=linux-arm-kernel&m=150106824024221&w=2
    
    Cc: <stable@vger.kernel.org> # 4.9
    Fixes: b67067f1176df ("kbuild: allow archs to select link dead code/data elimination")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index da0be9a8d1de..9623d78f8494 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -59,6 +59,22 @@
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
+/*
+ * LD_DEAD_CODE_DATA_ELIMINATION option enables -fdata-sections, which
+ * generates .data.identifier sections, which need to be pulled in with
+ * .data. We don't want to pull in .data..other sections, which Linux
+ * has defined. Same for text and bss.
+ */
+#ifdef CONFIG_LD_DEAD_CODE_DATA_ELIMINATION
+#define TEXT_MAIN .text .text.[0-9a-zA-Z_]*
+#define DATA_MAIN .data .data.[0-9a-zA-Z_]*
+#define BSS_MAIN .bss .bss.[0-9a-zA-Z_]*
+#else
+#define TEXT_MAIN .text
+#define DATA_MAIN .data
+#define BSS_MAIN .bss
+#endif
+
 /*
  * Align to a 32 byte boundary equal to the
  * alignment gcc 4.5 uses for a struct
@@ -198,12 +214,9 @@
 
 /*
  * .data section
- * LD_DEAD_CODE_DATA_ELIMINATION option enables -fdata-sections generates
- * .data.identifier which needs to be pulled in with .data, but don't want to
- * pull in .data..stuff which has its own requirements. Same for bss.
  */
 #define DATA_DATA							\
-	*(.data .data.[0-9a-zA-Z_]*)					\
+	*(DATA_MAIN)							\
 	*(.ref.data)							\
 	*(.data..shared_aligned) /* percpu related */			\
 	MEM_KEEP(init.data)						\
@@ -434,16 +447,17 @@
 		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
 	}
 
-/* .text section. Map to function alignment to avoid address changes
+/*
+ * .text section. Map to function alignment to avoid address changes
  * during second ld run in second ld pass when generating System.map
- * LD_DEAD_CODE_DATA_ELIMINATION option enables -ffunction-sections generates
- * .text.identifier which needs to be pulled in with .text , but some
- * architectures define .text.foo which is not intended to be pulled in here.
- * Those enabling LD_DEAD_CODE_DATA_ELIMINATION must ensure they don't have
- * conflicting section names, and must pull in .text.[0-9a-zA-Z_]* */
+ *
+ * TEXT_MAIN here will match .text.fixup and .text.unlikely if dead
+ * code elimination is enabled, so these sections should be converted
+ * to use ".." first.
+ */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
-		*(.text.hot .text .text.fixup .text.unlikely)		\
+		*(.text.hot TEXT_MAIN .text.fixup .text.unlikely)	\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\
@@ -613,7 +627,7 @@
 		BSS_FIRST_SECTIONS					\
 		*(.bss..page_aligned)					\
 		*(.dynbss)						\
-		*(.bss .bss.[0-9a-zA-Z_]*)				\
+		*(BSS_MAIN)						\
 		*(COMMON)						\
 	}
 

commit ee9f8fce99640811b2b8e79d0d1dbe8bab69ba67
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Jul 24 18:36:57 2017 -0500

    x86/unwind: Add the ORC unwinder
    
    Add the new ORC unwinder which is enabled by CONFIG_ORC_UNWINDER=y.
    It plugs into the existing x86 unwinder framework.
    
    It relies on objtool to generate the needed .orc_unwind and
    .orc_unwind_ip sections.
    
    For more details on why ORC is used instead of DWARF, see
    Documentation/x86/orc-unwinder.txt - but the short version is
    that it's a simplified, fundamentally more robust debugninfo
    data structure, which also allows up to two orders of magnitude
    faster lookups than the DWARF unwinder - which matters to
    profiling workloads like perf.
    
    Thanks to Andy Lutomirski for the performance improvement ideas:
    splitting the ORC unwind table into two parallel arrays and creating a
    fast lookup table to search a subset of the unwind table.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: live-patching@vger.kernel.org
    Link: http://lkml.kernel.org/r/0a6cbfb40f8da99b7a45a1a8302dc6aef16ec812.1500938583.git.jpoimboe@redhat.com
    [ Extended the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index da0be9a8d1de..fffc9bdae025 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -680,6 +680,31 @@
 #define BUG_TABLE
 #endif
 
+#ifdef CONFIG_ORC_UNWINDER
+#define ORC_UNWIND_TABLE						\
+	. = ALIGN(4);							\
+	.orc_unwind_ip : AT(ADDR(.orc_unwind_ip) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start_orc_unwind_ip) = .;		\
+		KEEP(*(.orc_unwind_ip))					\
+		VMLINUX_SYMBOL(__stop_orc_unwind_ip) = .;		\
+	}								\
+	. = ALIGN(6);							\
+	.orc_unwind : AT(ADDR(.orc_unwind) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start_orc_unwind) = .;			\
+		KEEP(*(.orc_unwind))					\
+		VMLINUX_SYMBOL(__stop_orc_unwind) = .;			\
+	}								\
+	. = ALIGN(4);							\
+	.orc_lookup : AT(ADDR(.orc_lookup) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(orc_lookup) = .;				\
+		. += (((SIZEOF(.text) + LOOKUP_BLOCK_SIZE - 1) /	\
+			LOOKUP_BLOCK_SIZE) + 1) * 4;			\
+		VMLINUX_SYMBOL(orc_lookup_end) = .;			\
+	}
+#else
+#define ORC_UNWIND_TABLE
+#endif
+
 #ifdef CONFIG_PM_TRACE
 #define TRACEDATA							\
 	. = ALIGN(4);							\
@@ -866,7 +891,7 @@
 		DATA_DATA						\
 		CONSTRUCTORS						\
 	}								\
-	BUG_TABLE
+	BUG_TABLE							\
 
 #define INIT_TEXT_SECTION(inittext_align)				\
 	. = ALIGN(inittext_align);					\

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit 2074006dace5d289d90f2bd31ae1e4bc94965f55
Merge: f72e24a1240b 69d71879d2cf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 6 19:45:45 2017 -0700

    Merge tag 'trace-v4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "The new features of this release:
    
       - Added TRACE_DEFINE_SIZEOF() which allows trace events that use
         sizeof() it the TP_printk() to be converted to the actual size such
         that trace-cmd and perf can parse them correctly.
    
       - Some rework of the TRACE_DEFINE_ENUM() such that the above
         TRACE_DEFINE_SIZEOF() could reuse the same code.
    
       - Recording of tgid (Thread Group ID). This is similar to how task
         COMMs are recorded (cached at sched_switch), where it is in a table
         and used on output of the trace and trace_pipe files.
    
       - Have ":mod:<module>" be cached when written into set_ftrace_filter.
         Then the functions of the module will be traced at module load.
    
       - Some random clean ups and small fixes"
    
    * tag 'trace-v4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (26 commits)
      ftrace: Test for NULL iter->tr in regex for stack_trace_filter changes
      ftrace: Decrement count for dyn_ftrace_total_info for init functions
      ftrace: Unlock hash mutex on failed allocation in process_mod_list()
      tracing: Add support for display of tgid in trace output
      tracing: Add support for recording tgid of tasks
      ftrace: Decrement count for dyn_ftrace_total_info file
      ftrace: Remove unused function ftrace_arch_read_dyn_info()
      sh/ftrace: Remove only user of ftrace_arch_read_dyn_info()
      ftrace: Have cached module filters be an active filter
      ftrace: Implement cached modules tracing on module load
      ftrace: Have the cached module list show in set_ftrace_filter
      ftrace: Add :mod: caching infrastructure to trace_array
      tracing: Show address when function names are not found
      ftrace: Add missing comment for FTRACE_OPS_FL_RCU
      tracing: Rename update the enum_map file
      tracing: Add TRACE_DEFINE_SIZEOF() macros
      tracing: define TRACE_DEFINE_SIZEOF() macro to map sizeof's to their values
      tracing: Rename enum_replace to eval_replace
      trace: rename enum_map functions
      trace: rename trace.c enum functions
      ...

commit bb0eb050a577a866cb47c2dc37596f1207f4c2d9
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri May 26 19:34:11 2017 +0200

    clocksource/drivers: Rename CLKSRC_OF to TIMER_OF
    
    The config option name is now renamed to 'TIMER_OF' for consistency with
    the CLOCKSOURCE_OF_DECLARE => TIMER_OF_DECLARE change.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c6a4ef50bbe6..0d64658a224f 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -172,7 +172,7 @@
 	KEEP(*(__##name##_of_table))					\
 	KEEP(*(__##name##_of_table_end))
 
-#define TIMER_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, timer)
+#define TIMER_OF_TABLES()	OF_TABLE(CONFIG_TIMER_OF, timer)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
 #define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)

commit 2fcc112af37fa88f8da077d6dd3bb8e38e75adb1
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri May 26 18:33:27 2017 +0200

    clocksource/drivers: Rename clksrc table to timer
    
    The table name is now renamed to 'timer' for consistency with
    the CLOCKSOURCE_OF_DECLARE => TIMER_OF_DECLARE change.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 401d324bcede..c6a4ef50bbe6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -172,7 +172,7 @@
 	KEEP(*(__##name##_of_table))					\
 	KEEP(*(__##name##_of_table_end))
 
-#define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
+#define TIMER_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, timer)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
 #define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
@@ -556,14 +556,15 @@
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\
-	CLKSRC_OF_TABLES()						\
+	TIMER_OF_TABLES()						\
 	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	CPUIDLE_METHOD_OF_TABLES()					\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
-	ACPI_PROBE_TABLE(clksrc)					\
+	ACPI_PROBE_TABLE(timer)						\
+	ACPI_PROBE_TABLE(iort)						\
 	EARLYCON_TABLE()
 
 #define INIT_TEXT							\

commit 02fd7f68f5342bc7e8054cb05ea4a07f26d41d12
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:42 2017 -0500

    trace: rename kernel enum section to eval
    
    The kernel and its modules have sections containing the enum
    string to value conversions. Rename this section because we
    intend to store more than enums in it.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-2-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 314a0b9219c6..800f9f9677a6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -125,9 +125,9 @@
 			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
 			KEEP(*(_ftrace_events))				\
 			VMLINUX_SYMBOL(__stop_ftrace_events) = .;	\
-			VMLINUX_SYMBOL(__start_ftrace_enum_maps) = .;	\
-			KEEP(*(_ftrace_enum_map))			\
-			VMLINUX_SYMBOL(__stop_ftrace_enum_maps) = .;
+			VMLINUX_SYMBOL(__start_ftrace_eval_maps) = .;	\
+			KEEP(*(_ftrace_eval_map))			\
+			VMLINUX_SYMBOL(__stop_ftrace_eval_maps) = .;
 #else
 #define FTRACE_EVENTS()
 #endif

commit 8e0931022e12e45bab9afe01e830d697d9c8e73d
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri May 26 15:30:34 2017 +0200

    Revert "clockevents: Add a clkevt-of mechanism like clksrc-of"
    
    After discussing it, this feature is dropped as it is not considered
    adequate:
    
            https://patchwork.kernel.org/patch/9639317/
    
    There is no user of this macro yet, so there is no impact on the drivers.
    
    This reverts commit 376bc27150f180d9f5eddec6a14117780177589d.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 314a0b9219c6..401d324bcede 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -173,7 +173,6 @@
 	KEEP(*(__##name##_of_table_end))
 
 #define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
-#define CLKEVT_OF_TABLES()	OF_TABLE(CONFIG_CLKEVT_OF, clkevt)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
 #define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
@@ -558,7 +557,6 @@
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\
 	CLKSRC_OF_TABLES()						\
-	CLKEVT_OF_TABLES()						\
 	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	CPUIDLE_METHOD_OF_TABLES()					\

commit 83a092cf95f28696ddc36c8add0cf03ac034897f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri May 12 03:40:40 2017 +1000

    powerpc: Link warning for orphan sections
    
    Add --orphan-handling=warn to final link flags. This ensures we can
    handle all sections explicitly. This would have caught subtle breakage
    such as 7de3b27bac47da9de08409df1d69664acbb72197 at build-time.
    
    Also bring existing orphan sections into the fold:
    - .text.hot and .text.unlikely are compiler generated sections.
    - .sdata2, .dynsbss, .plt are used by PPC32
    - We previously did not specify DWARF_DEBUG or STABS_DEBUG
    - DWARF_DEBUG did not include all DWARF sections that can be emitted
    - A number of sections are unused and can be discarded.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 314a0b9219c6..9862afb3ae05 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -595,6 +595,7 @@
 #define SBSS(sbss_align)						\
 	. = ALIGN(sbss_align);						\
 	.sbss : AT(ADDR(.sbss) - LOAD_OFFSET) {				\
+		*(.dynsbss)						\
 		*(.sbss)						\
 		*(.scommon)						\
 	}
@@ -641,11 +642,22 @@
 		.debug_str      0 : { *(.debug_str) }			\
 		.debug_loc      0 : { *(.debug_loc) }			\
 		.debug_macinfo  0 : { *(.debug_macinfo) }		\
+		.debug_pubtypes 0 : { *(.debug_pubtypes) }		\
+		/* DWARF 3 */						\
+		.debug_ranges	0 : { *(.debug_ranges) }		\
 		/* SGI/MIPS DWARF 2 extensions */			\
 		.debug_weaknames 0 : { *(.debug_weaknames) }		\
 		.debug_funcnames 0 : { *(.debug_funcnames) }		\
 		.debug_typenames 0 : { *(.debug_typenames) }		\
 		.debug_varnames  0 : { *(.debug_varnames) }		\
+		/* GNU DWARF 2 extensions */				\
+		.debug_gnu_pubnames 0 : { *(.debug_gnu_pubnames) }	\
+		.debug_gnu_pubtypes 0 : { *(.debug_gnu_pubtypes) }	\
+		/* DWARF 4 */						\
+		.debug_types	0 : { *(.debug_types) }			\
+		/* DWARF 5 */						\
+		.debug_macro	0 : { *(.debug_macro) }			\
+		.debug_addr	0 : { *(.debug_addr) }
 
 		/* Stabs debugging sections.  */
 #define STABS_DEBUG							\

commit 28b47809b2171a6cfbab839936b24280639c9f85
Merge: 4a1e31c68e9f 2c0248d68880
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 9 15:15:47 2017 -0700

    Merge tag 'iommu-updates-v4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/joro/iommu
    
    Pull IOMMU updates from Joerg Roedel:
    
     - code optimizations for the Intel VT-d driver
    
     - ability to switch off a previously enabled Intel IOMMU
    
     - support for 'struct iommu_device' for OMAP, Rockchip and Mediatek
       IOMMUs
    
     - header optimizations for IOMMU core code headers and a few fixes that
       became necessary in other parts of the kernel because of that
    
     - ACPI/IORT updates and fixes
    
     - Exynos IOMMU optimizations
    
     - updates for the IOMMU dma-api code to bring it closer to use per-cpu
       iova caches
    
     - new command-line option to set default domain type allocated by the
       iommu core code
    
     - another command line option to allow the Intel IOMMU switched off in
       a tboot environment
    
     - ARM/SMMU: TLB sync optimisations for SMMUv2, Support for using an
       IDENTITY domain in conjunction with DMA ops, Support for SMR masking,
       Support for 16-bit ASIDs (was previously broken)
    
     - various other small fixes and improvements
    
    * tag 'iommu-updates-v4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/joro/iommu: (63 commits)
      soc/qbman: Move dma-mapping.h include to qman_priv.h
      soc/qbman: Fix implicit header dependency now causing build fails
      iommu: Remove trace-events include from iommu.h
      iommu: Remove pci.h include from trace/events/iommu.h
      arm: dma-mapping: Don't override dma_ops in arch_setup_dma_ops()
      ACPI/IORT: Fix CONFIG_IOMMU_API dependency
      iommu/vt-d: Don't print the failure message when booting non-kdump kernel
      iommu: Move report_iommu_fault() to iommu.c
      iommu: Include device.h in iommu.h
      x86, iommu/vt-d: Add an option to disable Intel IOMMU force on
      iommu/arm-smmu: Return IOVA in iova_to_phys when SMMU is bypassed
      iommu/arm-smmu: Correct sid to mask
      iommu/amd: Fix incorrect error handling in amd_iommu_bind_pasid()
      iommu: Make iommu_bus_notifier return NOTIFY_DONE rather than error code
      omap3isp: Remove iommu_group related code
      iommu/omap: Add iommu-group support
      iommu/omap: Make use of 'struct iommu_device'
      iommu/omap: Store iommu_dev pointer in arch_data
      iommu/omap: Move data structures to omap-iommu.h
      iommu/omap: Drop legacy-style device support
      ...

commit 2c0248d68880fc0e783af1048b3367ee5d4412f0
Merge: d5bf739dc762 fd8e2d4b3932 c9d9f2394c6a 6f66ea099fc2 bdf95923086f 26b37b946a5c 8e1218840066 73dbd4a42302 290d638e04e7
Author: Joerg Roedel <jroedel@suse.de>
Date:   Thu May 4 18:06:17 2017 +0200

    Merge branches 'arm/exynos', 'arm/omap', 'arm/rockchip', 'arm/mediatek', 'arm/smmu', 'arm/core', 'x86/vt-d', 'x86/amd' and 'core' into next

commit 3fb9268e43e7ab62adb5c6ddec58d3cb4767bd9a
Merge: 12ca7c8db30d 262fa734a002
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 22:07:51 2017 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 asm updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - unwinder fixes and enhancements
    
       - improve ftrace interaction with the unwinder
    
       - optimize the code footprint of WARN() and related debugging
         constructs
    
       - ... plus misc updates, cleanups and fixes"
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/unwind: Dump all stacks in unwind_dump()
      x86/unwind: Silence more entry-code related warnings
      x86/ftrace: Fix ebp in ftrace_regs_caller that screws up unwinder
      x86/unwind: Remove unused 'sp' parameter in unwind_dump()
      x86/unwind: Prepend hex mask value with '0x' in unwind_dump()
      x86/unwind: Properly zero-pad 32-bit values in unwind_dump()
      x86/unwind: Ensure stack pointer is aligned
      debug: Avoid setting BUGFLAG_WARNING twice
      x86/unwind: Silence entry-related warnings
      x86/unwind: Read stack return address in update_stack_state()
      x86/unwind: Move common code into update_stack_state()
      debug: Fix __bug_table[] in arch linker scripts
      debug: Add _ONCE() logic to report_bug()
      x86/debug: Define BUG() again for !CONFIG_BUG
      x86/debug: Implement __WARN() using UD0
      x86/ftrace: Use Makefile logic instead of #ifdef for compiling ftrace_*.o
      x86/ftrace: Add -mfentry support to x86_32 with DYNAMIC_FTRACE set
      x86/ftrace: Clean up ftrace_regs_caller
      x86/ftrace: Add stack frame pointer to ftrace_caller
      x86/ftrace: Move the ftrace specific code out of entry_32.S
      ...

commit 316ca8804ea84a782d5ba2163711ebb22116ff5a
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Apr 10 16:51:06 2017 +0530

    ACPI/IORT: Remove linker section for IORT entries probing
    
    The IORT linker section introduced by commit 34ceea275f62
    ("ACPI/IORT: Introduce linker section for IORT entries probing")
    was needed to make sure SMMU drivers are registered (and therefore
    probed) in the kernel before devices using the SMMU have a chance
    to probe in turn.
    
    Through the introduction of deferred IOMMU configuration the linker
    section based IORT probing infrastructure is not needed any longer, in
    that device/SMMU probe dependencies are managed through the probe
    deferral mechanism, making the IORT linker section infrastructure
    unused, so that it can be removed.
    
    Remove the unused IORT linker section probing infrastructure
    from the kernel to complete the ACPI IORT IOMMU configure probe
    deferral mechanism implementation.
    
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Sricharan R <sricharan@codeaurora.org>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0968d13b3885..9faa26c41c14 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -566,7 +566,6 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(clksrc)					\
-	ACPI_PROBE_TABLE(iort)						\
 	EARLYCON_TABLE()
 
 #define INIT_TEXT							\

commit d79bf21e0e02f8ad24219f0587cc599a7e67f6c6
Author: Jessica Yu <jeyu@redhat.com>
Date:   Fri Apr 7 16:04:48 2017 -0700

    vmlinux.lds: add missing VMLINUX_SYMBOL macros
    
    When __{start,end}_ro_after_init is referenced from C code, we run into
    the following build errors on blackfin:
    
      kernel/extable.c:169: undefined reference to `__start_ro_after_init'
      kernel/extable.c:169: undefined reference to `__end_ro_after_init'
    
    The build error is due to the fact that blackfin is one of the few
    arches that prepends an underscore '_' to all symbols defined in C.
    
    Fix this by wrapping __{start,end}_ro_after_init in vmlinux.lds.h with
    VMLINUX_SYMBOL(), which adds the necessary prefix for arches that have
    HAVE_UNDERSCORE_SYMBOL_PREFIX.
    
    Link: http://lkml.kernel.org/r/1491259387-15869-1-git-send-email-jeyu@redhat.com
    Signed-off-by: Jessica Yu <jeyu@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Eddie Kovsky <ewk@edkovsky.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7cdfe167074f..143db9c523e2 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -261,9 +261,9 @@
  */
 #ifndef RO_AFTER_INIT_DATA
 #define RO_AFTER_INIT_DATA						\
-	__start_ro_after_init = .;					\
+	VMLINUX_SYMBOL(__start_ro_after_init) = .;			\
 	*(.data..ro_after_init)						\
-	__end_ro_after_init = .;
+	VMLINUX_SYMBOL(__end_ro_after_init) = .;
 #endif
 
 /*

commit 4a6808f347a3f969e56e8e4274e70849ecdc33de
Merge: 907977b2a22c 8d09617b076f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 2 09:22:03 2017 -0700

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer fixes from Thomas Gleixner:
     "Two small fixes for the new CLKEVT_OF infrastructure"
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      vmlinux.lds: Add __clkevt_of_table to kernel
      clockevents: Fix syntax error in clkevt-of macro

commit 906f2a51c941e251ca196d5128953d9899a608ef
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Mar 31 15:11:58 2017 -0700

    mm: fix section name for .data..ro_after_init
    
    A section name for .data..ro_after_init was added by both:
    
        commit d07a980c1b8d ("s390: add proper __ro_after_init support")
    
    and
    
        commit d7c19b066dcf ("mm: kmemleak: scan .data.ro_after_init")
    
    The latter adds incorrect wrapping around the existing s390 section, and
    came later.  I'd prefer the s390 naming, so this moves the s390-specific
    name up to the asm-generic/sections.h and renames the section as used by
    kmemleak (and in the future, kernel/extable.c).
    
    Link: http://lkml.kernel.org/r/20170327192213.GA129375@beast
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>    [s390 parts]
    Acked-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Cc: Eddie Kovsky <ewk@edkovsky.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0968d13b3885..f9f21e2c59f3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -260,9 +260,9 @@
  */
 #ifndef RO_AFTER_INIT_DATA
 #define RO_AFTER_INIT_DATA						\
-	__start_data_ro_after_init = .;					\
+	__start_ro_after_init = .;					\
 	*(.data..ro_after_init)						\
-	__end_data_ro_after_init = .;
+	__end_ro_after_init = .;
 #endif
 
 /*

commit 19d436268dde95389c616bb3819da73f0a8b28a8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Feb 25 08:56:53 2017 +0100

    debug: Add _ONCE() logic to report_bug()
    
    Josh suggested moving the _ONCE logic inside the trap handler, using a
    bit in the bug_entry::flags field, avoiding the need for the extra
    variable.
    
    Sadly this only works for WARN_ON_ONCE(), since the others have
    printk() statements prior to triggering the trap.
    
    Still, this saves a fair amount of text and some data:
    
      text         data       filename
      10682460     4530992    defconfig-build/vmlinux.orig
      10665111     4530096    defconfig-build/vmlinux.patched
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0968d13b3885..51c8dbe8e8d9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -286,8 +286,6 @@
 		*(.rodata1)						\
 	}								\
 									\
-	BUG_TABLE							\
-									\
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
@@ -855,7 +853,8 @@
 		READ_MOSTLY_DATA(cacheline)				\
 		DATA_DATA						\
 		CONSTRUCTORS						\
-	}
+	}								\
+	BUG_TABLE
 
 #define INIT_TEXT_SECTION(inittext_align)				\
 	. = ALIGN(inittext_align);					\

commit 8d09617b076fd03ee9ae124abce94dda17bf3723
Author: Alexander Kochetkov <al.kochet@gmail.com>
Date:   Wed Mar 22 17:29:06 2017 +0300

    vmlinux.lds: Add __clkevt_of_table to kernel
    
    The code introduced by commit 0c8893c9095d ("clockevents: Add a
    clkevt-of mechanism like clksrc-of") refer to __clkevt_of_table
    what doesn't exist in the vmlinux. As a result kernel build
    failed with error: "clkevt-probe.c:63: undefined reference to
    `__clkevt_of_table"
    
    Fixes: 0c8893c9095d ("clockevents: Add a clkevt-of mechanism like clksrc-of")
    Signed-off-by: Alexander Kochetkov <al.kochet@gmail.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0968d13b3885..8c6b525eb0fa 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -173,6 +173,7 @@
 	KEEP(*(__##name##_of_table_end))
 
 #define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
+#define CLKEVT_OF_TABLES()	OF_TABLE(CONFIG_CLKEVT_OF, clkevt)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
 #define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
@@ -559,6 +560,7 @@
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\
 	CLKSRC_OF_TABLES()						\
+	CLKEVT_OF_TABLES()						\
 	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	CPUIDLE_METHOD_OF_TABLES()					\

commit 41e0e24b450fadc079dfb659d81f3076afcfbd8a
Merge: 0aaf2146ecf0 334bb7738764
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 17 16:24:13 2016 -0800

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild
    
    Pull kbuild updates from Michal Marek:
    
     - prototypes for x86 asm-exported symbols (Adam Borowski) and a warning
       about missing CRCs (Nick Piggin)
    
     - asm-exports fix for LTO (Nicolas Pitre)
    
     - thin archives improvements (Nick Piggin)
    
     - linker script fix for CONFIG_LD_DEAD_CODE_DATA_ELIMINATION (Nick
       Piggin)
    
     - genksyms support for __builtin_va_list keyword
    
     - misc minor fixes
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild:
      x86/kbuild: enable modversions for symbols exported from asm
      kbuild: fix scripts/adjust_autoksyms.sh* for the no modules case
      scripts/kallsyms: remove last remnants of --page-offset option
      make use of make variable CURDIR instead of calling pwd
      kbuild: cmd_export_list: tighten the sed script
      kbuild: minor improvement for thin archives build
      kbuild: modpost warn if export version crc is missing
      kbuild: keep data tables through dead code elimination
      kbuild: improve linker compatibility with lib-ksyms.o build
      genksyms: Regenerate parser
      kbuild/genksyms: handle va_list type
      kbuild: thin archives for multi-y targets
      kbuild: kallsyms allow 3-pass generation if symbols size has changed

commit 34ceea275f626ae624b55f2b388a07f806988a55
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Nov 21 10:01:34 2016 +0000

    ACPI/IORT: Introduce linker section for IORT entries probing
    
    Since commit e647b532275b ("ACPI: Add early device probing
    infrastructure") the kernel has gained the infrastructure that allows
    adding linker script section entries to execute ACPI driver callbacks
    (ie probe routines) for all subsystems that register a table entry
    in the respective kernel section (eg clocksource, irqchip).
    
    Since ARM IOMMU devices data is described through IORT tables when
    booting with ACPI, the ARM IOMMU drivers must be made able to hook ACPI
    callback routines that are called to probe IORT entries and initialize
    the respective IOMMU devices.
    
    To avoid adding driver specific hooks into IORT table initialization
    code (breaking therefore code modularity - ie ACPI IORT code must be made
    aware of ARM SMMU drivers ACPI init callbacks), this patch adds code
    that allows ARM SMMU drivers to take advantage of the ACPI early probing
    infrastructure, so that they can add linker script section entries
    containing drivers callback to be executed on IORT tables detection.
    
    Since IORT nodes are differentiated by a type, the callback routines
    can easily parse the IORT table entries, check the IORT nodes and
    carry out some actions whenever the IORT node type associated with
    the driver specific callback is matched.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Reviewed-by: Tomasz Nowicki <tn@semihalf.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Tomasz Nowicki <tn@semihalf.com>
    Cc: Tomasz Nowicki <tn@semihalf.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 31e1d639abed..9e3aa34341f4 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -566,6 +566,7 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(clksrc)					\
+	ACPI_PROBE_TABLE(iort)						\
 	EARLYCON_TABLE()
 
 #define INIT_TEXT							\

commit 4b89b7f7aad5742523db801b7107598e5b5a42c8
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Nov 24 03:41:41 2016 +1100

    kbuild: keep data tables through dead code elimination
    
    When CONFIG_LD_DEAD_CODE_DATA_ELIMINATION is enabled we must ensure
    that we still keep various programatically-accessed tables.
    
    [npiggin: Fold Paul's patches into one, and add a few more tables.
     diff symbol tables of allyesconfig with/without -gc-sections shows up
     lost tables quite easily.]
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 30747960bc54..5372775161d0 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -114,7 +114,7 @@
 #ifdef CONFIG_KPROBES
 #define KPROBE_BLACKLIST()	. = ALIGN(8);				      \
 				VMLINUX_SYMBOL(__start_kprobe_blacklist) = .; \
-				*(_kprobe_blacklist)			      \
+				KEEP(*(_kprobe_blacklist))		      \
 				VMLINUX_SYMBOL(__stop_kprobe_blacklist) = .;
 #else
 #define KPROBE_BLACKLIST()
@@ -123,10 +123,10 @@
 #ifdef CONFIG_EVENT_TRACING
 #define FTRACE_EVENTS()	. = ALIGN(8);					\
 			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
-			*(_ftrace_events)				\
+			KEEP(*(_ftrace_events))				\
 			VMLINUX_SYMBOL(__stop_ftrace_events) = .;	\
 			VMLINUX_SYMBOL(__start_ftrace_enum_maps) = .;	\
-			*(_ftrace_enum_map)				\
+			KEEP(*(_ftrace_enum_map))			\
 			VMLINUX_SYMBOL(__stop_ftrace_enum_maps) = .;
 #else
 #define FTRACE_EVENTS()
@@ -134,10 +134,10 @@
 
 #ifdef CONFIG_TRACING
 #define TRACE_PRINTKS() VMLINUX_SYMBOL(__start___trace_bprintk_fmt) = .;      \
-			 *(__trace_printk_fmt) /* Trace_printk fmt' pointer */ \
+			 KEEP(*(__trace_printk_fmt)) /* Trace_printk fmt' pointer */ \
 			 VMLINUX_SYMBOL(__stop___trace_bprintk_fmt) = .;
 #define TRACEPOINT_STR() VMLINUX_SYMBOL(__start___tracepoint_str) = .;	\
-			 *(__tracepoint_str) /* Trace_printk fmt' pointer */ \
+			 KEEP(*(__tracepoint_str)) /* Trace_printk fmt' pointer */ \
 			 VMLINUX_SYMBOL(__stop___tracepoint_str) = .;
 #else
 #define TRACE_PRINTKS()
@@ -147,7 +147,7 @@
 #ifdef CONFIG_FTRACE_SYSCALLS
 #define TRACE_SYSCALLS() . = ALIGN(8);					\
 			 VMLINUX_SYMBOL(__start_syscalls_metadata) = .;	\
-			 *(__syscalls_metadata)				\
+			 KEEP(*(__syscalls_metadata))			\
 			 VMLINUX_SYMBOL(__stop_syscalls_metadata) = .;
 #else
 #define TRACE_SYSCALLS()
@@ -156,7 +156,7 @@
 #ifdef CONFIG_SERIAL_EARLYCON
 #define EARLYCON_TABLE() STRUCT_ALIGN();			\
 			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
-			 *(__earlycon_table)			\
+			 KEEP(*(__earlycon_table))		\
 			 VMLINUX_SYMBOL(__earlycon_table_end) = .;
 #else
 #define EARLYCON_TABLE()
@@ -169,8 +169,8 @@
 #define _OF_TABLE_1(name)						\
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__##name##_of_table) = .;			\
-	*(__##name##_of_table)						\
-	*(__##name##_of_table_end)
+	KEEP(*(__##name##_of_table))					\
+	KEEP(*(__##name##_of_table_end))
 
 #define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
@@ -184,7 +184,7 @@
 #define ACPI_PROBE_TABLE(name)						\
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__##name##_acpi_probe_table) = .;		\
-	*(__##name##_acpi_probe_table)					\
+	KEEP(*(__##name##_acpi_probe_table))				\
 	VMLINUX_SYMBOL(__##name##_acpi_probe_table_end) = .;
 #else
 #define ACPI_PROBE_TABLE(name)
@@ -193,7 +193,7 @@
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	VMLINUX_SYMBOL(__dtb_start) = .;				\
-	*(.dtb.init.rodata)						\
+	KEEP(*(.dtb.init.rodata))					\
 	VMLINUX_SYMBOL(__dtb_end) = .;
 
 /*
@@ -214,11 +214,11 @@
 	/* implement dynamic printk debug */				\
 	. = ALIGN(8);                                                   \
 	VMLINUX_SYMBOL(__start___jump_table) = .;                       \
-	*(__jump_table)                                                 \
+	KEEP(*(__jump_table))                                           \
 	VMLINUX_SYMBOL(__stop___jump_table) = .;                        \
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___verbose) = .;                          \
-	*(__verbose)                                                    \
+	KEEP(*(__verbose))                                              \
 	VMLINUX_SYMBOL(__stop___verbose) = .;				\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
@@ -271,10 +271,10 @@
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		RO_AFTER_INIT_DATA	/* Read only after init */	\
-		*(__vermagic)		/* Kernel version magic */	\
+		KEEP(*(__vermagic))	/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\
-		*(__tracepoints_ptrs)	/* Tracepoints: pointer array */\
+		KEEP(*(__tracepoints_ptrs)) /* Tracepoints: pointer array */ \
 		VMLINUX_SYMBOL(__stop___tracepoints_ptrs) = .;		\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
 	}								\
@@ -288,35 +288,35 @@
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
-		*(.pci_fixup_early)					\
+		KEEP(*(.pci_fixup_early))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_early) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_header) = .;		\
-		*(.pci_fixup_header)					\
+		KEEP(*(.pci_fixup_header))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_header) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_final) = .;		\
-		*(.pci_fixup_final)					\
+		KEEP(*(.pci_fixup_final))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_final) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_enable) = .;		\
-		*(.pci_fixup_enable)					\
+		KEEP(*(.pci_fixup_enable))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_enable) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_resume) = .;		\
-		*(.pci_fixup_resume)					\
+		KEEP(*(.pci_fixup_resume))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_resume) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_resume_early) = .;	\
-		*(.pci_fixup_resume_early)				\
+		KEEP(*(.pci_fixup_resume_early))			\
 		VMLINUX_SYMBOL(__end_pci_fixups_resume_early) = .;	\
 		VMLINUX_SYMBOL(__start_pci_fixups_suspend) = .;		\
-		*(.pci_fixup_suspend)					\
+		KEEP(*(.pci_fixup_suspend))				\
 		VMLINUX_SYMBOL(__end_pci_fixups_suspend) = .;		\
 		VMLINUX_SYMBOL(__start_pci_fixups_suspend_late) = .;	\
-		*(.pci_fixup_suspend_late)				\
+		KEEP(*(.pci_fixup_suspend_late))			\
 		VMLINUX_SYMBOL(__end_pci_fixups_suspend_late) = .;	\
 	}								\
 									\
 	/* Built-in firmware blobs */					\
 	.builtin_fw        : AT(ADDR(.builtin_fw) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_builtin_fw) = .;			\
-		*(.builtin_fw)						\
+		KEEP(*(.builtin_fw))					\
 		VMLINUX_SYMBOL(__end_builtin_fw) = .;			\
 	}								\
 									\
@@ -394,7 +394,7 @@
 									\
 	/* Kernel symbol table: strings */				\
         __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
-		KEEP(*(__ksymtab_strings))				\
+		*(__ksymtab_strings)					\
 	}								\
 									\
 	/* __*init sections */						\
@@ -407,14 +407,14 @@
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___param) = .;			\
-		*(__param)						\
+		KEEP(*(__param))					\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
 	}								\
 									\
 	/* Built-in module versions. */					\
 	__modver : AT(ADDR(__modver) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___modver) = .;			\
-		*(__modver)						\
+		KEEP(*(__modver))					\
 		VMLINUX_SYMBOL(__stop___modver) = .;			\
 		. = ALIGN((align));					\
 		VMLINUX_SYMBOL(__end_rodata) = .;			\
@@ -517,7 +517,7 @@
 	. = ALIGN(align);						\
 	__ex_table : AT(ADDR(__ex_table) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___ex_table) = .;			\
-		*(__ex_table)						\
+		KEEP(*(__ex_table))					\
 		VMLINUX_SYMBOL(__stop___ex_table) = .;			\
 	}
 
@@ -533,9 +533,9 @@
 #ifdef CONFIG_CONSTRUCTORS
 #define KERNEL_CTORS()	. = ALIGN(8);			   \
 			VMLINUX_SYMBOL(__ctors_start) = .; \
-			*(.ctors)			   \
-			*(SORT(.init_array.*))		   \
-			*(.init_array)			   \
+			KEEP(*(.ctors))			   \
+			KEEP(*(SORT(.init_array.*)))	   \
+			KEEP(*(.init_array))		   \
 			VMLINUX_SYMBOL(__ctors_end) = .;
 #else
 #define KERNEL_CTORS()
@@ -659,7 +659,7 @@
 	. = ALIGN(8);							\
 	__bug_table : AT(ADDR(__bug_table) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___bug_table) = .;		\
-		*(__bug_table)						\
+		KEEP(*(__bug_table))					\
 		VMLINUX_SYMBOL(__stop___bug_table) = .;			\
 	}
 #else
@@ -671,7 +671,7 @@
 	. = ALIGN(4);							\
 	.tracedata : AT(ADDR(.tracedata) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__tracedata_start) = .;			\
-		*(.tracedata)						\
+		KEEP(*(.tracedata))					\
 		VMLINUX_SYMBOL(__tracedata_end) = .;			\
 	}
 #else
@@ -688,7 +688,7 @@
 #define INIT_SETUP(initsetup_align)					\
 		. = ALIGN(initsetup_align);				\
 		VMLINUX_SYMBOL(__setup_start) = .;			\
-		*(.init.setup)						\
+		KEEP(*(.init.setup))					\
 		VMLINUX_SYMBOL(__setup_end) = .;
 
 #define INIT_CALLS_LEVEL(level)						\

commit d7c19b066dcf4bd19c4385e8065558d4e74f9e73
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Nov 10 10:46:44 2016 -0800

    mm: kmemleak: scan .data.ro_after_init
    
    Limit the number of kmemleak false positives by including
    .data.ro_after_init in memory scanning.  To achieve this we need to add
    symbols for start and end of the section to the linker scripts.
    
    The problem was been uncovered by commit 56989f6d8568 ("genetlink: mark
    families as __ro_after_init").
    
    Link: http://lkml.kernel.org/r/1478274173-15218-1-git-send-email-jakub.kicinski@netronome.com
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Johannes Berg <johannes@sipsolutions.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 30747960bc54..31e1d639abed 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -259,7 +259,10 @@
  * own by defining an empty RO_AFTER_INIT_DATA.
  */
 #ifndef RO_AFTER_INIT_DATA
-#define RO_AFTER_INIT_DATA *(.data..ro_after_init)
+#define RO_AFTER_INIT_DATA						\
+	__start_data_ro_after_init = .;					\
+	*(.data..ro_after_init)						\
+	__end_data_ro_after_init = .;
 #endif
 
 /*

commit 84d69848c97faab0c25aa2667b273404d2e2a64a
Merge: d4d24d2d0a7e 590abbdd2733
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 14 14:26:58 2016 -0700

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild
    
    Pull kbuild updates from Michal Marek:
    
     - EXPORT_SYMBOL for asm source by Al Viro.
    
       This does bring a regression, because genksyms no longer generates
       checksums for these symbols (CONFIG_MODVERSIONS). Nick Piggin is
       working on a patch to fix this.
    
       Plus, we are talking about functions like strcpy(), which rarely
       change prototypes.
    
     - Fixes for PPC fallout of the above by Stephen Rothwell and Nick
       Piggin
    
     - fixdep speedup by Alexey Dobriyan.
    
     - preparatory work by Nick Piggin to allow architectures to build with
       -ffunction-sections, -fdata-sections and --gc-sections
    
     - CONFIG_THIN_ARCHIVES support by Stephen Rothwell
    
     - fix for filenames with colons in the initramfs source by me.
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild: (22 commits)
      initramfs: Escape colons in depfile
      ppc: there is no clear_pages to export
      powerpc/64: whitelist unresolved modversions CRCs
      kbuild: -ffunction-sections fix for archs with conflicting sections
      kbuild: add arch specific post-link Makefile
      kbuild: allow archs to select link dead code/data elimination
      kbuild: allow architectures to use thin archives instead of ld -r
      kbuild: Regenerate genksyms lexer
      kbuild: genksyms fix for typeof handling
      fixdep: faster CONFIG_ search
      ia64: move exports to definitions
      sparc32: debride memcpy.S a bit
      [sparc] unify 32bit and 64bit string.h
      sparc: move exports to definitions
      ppc: move exports to definitions
      arm: move exports to definitions
      s390: move exports to definitions
      m68k: move exports to definitions
      alpha: move exports to actual definitions
      x86: move exports to actual definitions
      ...

commit 6727ad9e206cc08b80d8000a4d67f8417e53539d
Author: Chris Metcalf <cmetcalf@mellanox.com>
Date:   Fri Oct 7 17:02:55 2016 -0700

    nmi_backtrace: generate one-line reports for idle cpus
    
    When doing an nmi backtrace of many cores, most of which are idle, the
    output is a little overwhelming and very uninformative.  Suppress
    messages for cpus that are idling when they are interrupted and just
    emit one line, "NMI backtrace for N skipped: idling at pc 0xNNN".
    
    We do this by grouping all the cpuidle code together into a new
    .cpuidle.text section, and then checking the address of the interrupted
    PC to see if it lies within that section.
    
    This commit suitably tags x86 and tile idle routines, and only adds in
    the minimal framework for other architectures.
    
    Link: http://lkml.kernel.org/r/1472487169-14923-5-git-send-email-cmetcalf@mellanox.com
    Signed-off-by: Chris Metcalf <cmetcalf@mellanox.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Thompson <daniel.thompson@linaro.org> [arm]
    Tested-by: Petr Mladek <pmladek@suse.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 24563970ff7b..3e42bcdd014b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -454,6 +454,12 @@
 		*(.spinlock.text)					\
 		VMLINUX_SYMBOL(__lock_text_end) = .;
 
+#define CPUIDLE_TEXT							\
+		ALIGN_FUNCTION();					\
+		VMLINUX_SYMBOL(__cpuidle_text_start) = .;		\
+		*(.cpuidle.text)					\
+		VMLINUX_SYMBOL(__cpuidle_text_end) = .;
+
 #define KPROBES_TEXT							\
 		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__kprobes_text_start) = .;		\

commit 0f4c4af06eec5717041d6fe568b80ee4cf3c1475
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Sep 14 12:24:03 2016 +1000

    kbuild: -ffunction-sections fix for archs with conflicting sections
    
    Enabling -ffunction-sections modified the generic linker script to
    pull .text.* sections into regular TEXT_TEXT section, conflicting
    with some architectures. Revert that change and require archs that
    enable the option to ensure they have no conflicting section names,
    and do the appropriate merging.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Fixes: b67067f1176d ("kbuild: allow archs to select link dead code/data elimination")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ad9d8f94dc7a..48dd44f3f24b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -198,9 +198,9 @@
 
 /*
  * .data section
- * -fdata-sections generates .data.identifier which needs to be pulled in
- * with .data, but don't want to pull in .data..stuff which has its own
- * requirements. Same for bss.
+ * LD_DEAD_CODE_DATA_ELIMINATION option enables -fdata-sections generates
+ * .data.identifier which needs to be pulled in with .data, but don't want to
+ * pull in .data..stuff which has its own requirements. Same for bss.
  */
 #define DATA_DATA							\
 	*(.data .data.[0-9a-zA-Z_]*)					\
@@ -434,10 +434,15 @@
 	}
 
 /* .text section. Map to function alignment to avoid address changes
- * during second ld run in second ld pass when generating System.map */
+ * during second ld run in second ld pass when generating System.map
+ * LD_DEAD_CODE_DATA_ELIMINATION option enables -ffunction-sections generates
+ * .text.identifier which needs to be pulled in with .text , but some
+ * architectures define .text.foo which is not intended to be pulled in here.
+ * Those enabling LD_DEAD_CODE_DATA_ELIMINATION must ensure they don't have
+ * conflicting section names, and must pull in .text.[0-9a-zA-Z_]* */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
-		*(.text.hot .text .text.fixup .text.unlikely .text.*)	\
+		*(.text.hot .text .text.fixup .text.unlikely)		\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\

commit b67067f1176df6ee727450546b58704e4b588563
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Aug 24 22:29:20 2016 +1000

    kbuild: allow archs to select link dead code/data elimination
    
    Introduce LD_DEAD_CODE_DATA_ELIMINATION option for architectures to
    select to build with -ffunction-sections, -fdata-sections, and link
    with --gc-sections. It requires some work (documented) to ensure all
    unreferenced entrypoints are live, and requires toolchain and build
    verification, so it is made a per-arch option for now.
    
    On a random powerpc64le build, this yelds a significant size saving,
    it boots and runs fine, but there is a lot I haven't tested as yet, so
    these savings may be reduced if there are bugs in the link.
    
        text      data        bss        dec   filename
    11169741   1180744    1923176   14273661   vmlinux
    10445269   1004127    1919707   13369103   vmlinux.dce
    
    ~700K text, ~170K data, 6% removed from kernel image size.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 24563970ff7b..ad9d8f94dc7a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -196,9 +196,14 @@
 	*(.dtb.init.rodata)						\
 	VMLINUX_SYMBOL(__dtb_end) = .;
 
-/* .data section */
+/*
+ * .data section
+ * -fdata-sections generates .data.identifier which needs to be pulled in
+ * with .data, but don't want to pull in .data..stuff which has its own
+ * requirements. Same for bss.
+ */
 #define DATA_DATA							\
-	*(.data)							\
+	*(.data .data.[0-9a-zA-Z_]*)					\
 	*(.ref.data)							\
 	*(.data..shared_aligned) /* percpu related */			\
 	MEM_KEEP(init.data)						\
@@ -320,76 +325,76 @@
 	/* Kernel symbol table: Normal symbols */			\
 	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___ksymtab) = .;			\
-		*(SORT(___ksymtab+*))					\
+		KEEP(*(SORT(___ksymtab+*)))				\
 		VMLINUX_SYMBOL(__stop___ksymtab) = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__ksymtab_gpl     : AT(ADDR(__ksymtab_gpl) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___ksymtab_gpl) = .;		\
-		*(SORT(___ksymtab_gpl+*))				\
+		KEEP(*(SORT(___ksymtab_gpl+*)))				\
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__ksymtab_unused  : AT(ADDR(__ksymtab_unused) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___ksymtab_unused) = .;		\
-		*(SORT(___ksymtab_unused+*))				\
+		KEEP(*(SORT(___ksymtab_unused+*)))			\
 		VMLINUX_SYMBOL(__stop___ksymtab_unused) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__ksymtab_unused_gpl : AT(ADDR(__ksymtab_unused_gpl) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___ksymtab_unused_gpl) = .;	\
-		*(SORT(___ksymtab_unused_gpl+*))			\
+		KEEP(*(SORT(___ksymtab_unused_gpl+*)))			\
 		VMLINUX_SYMBOL(__stop___ksymtab_unused_gpl) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__ksymtab_gpl_future : AT(ADDR(__ksymtab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___ksymtab_gpl_future) = .;	\
-		*(SORT(___ksymtab_gpl_future+*))			\
+		KEEP(*(SORT(___ksymtab_gpl_future+*)))			\
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl_future) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: Normal symbols */			\
 	__kcrctab         : AT(ADDR(__kcrctab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___kcrctab) = .;			\
-		*(SORT(___kcrctab+*))					\
+		KEEP(*(SORT(___kcrctab+*)))				\
 		VMLINUX_SYMBOL(__stop___kcrctab) = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__kcrctab_gpl     : AT(ADDR(__kcrctab_gpl) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___kcrctab_gpl) = .;		\
-		*(SORT(___kcrctab_gpl+*))				\
+		KEEP(*(SORT(___kcrctab_gpl+*)))				\
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__kcrctab_unused  : AT(ADDR(__kcrctab_unused) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___kcrctab_unused) = .;		\
-		*(SORT(___kcrctab_unused+*))				\
+		KEEP(*(SORT(___kcrctab_unused+*)))			\
 		VMLINUX_SYMBOL(__stop___kcrctab_unused) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__kcrctab_unused_gpl : AT(ADDR(__kcrctab_unused_gpl) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___kcrctab_unused_gpl) = .;	\
-		*(SORT(___kcrctab_unused_gpl+*))			\
+		KEEP(*(SORT(___kcrctab_unused_gpl+*)))			\
 		VMLINUX_SYMBOL(__stop___kcrctab_unused_gpl) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__kcrctab_gpl_future : AT(ADDR(__kcrctab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___kcrctab_gpl_future) = .;	\
-		*(SORT(___kcrctab_gpl_future+*))			\
+		KEEP(*(SORT(___kcrctab_gpl_future+*)))			\
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl_future) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: strings */				\
         __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
-		*(__ksymtab_strings)					\
+		KEEP(*(__ksymtab_strings))				\
 	}								\
 									\
 	/* __*init sections */						\
@@ -424,7 +429,7 @@
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
-		*(.security_initcall.init) 				\
+		KEEP(*(.security_initcall.init))			\
 		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
 	}
 
@@ -432,7 +437,7 @@
  * during second ld run in second ld pass when generating System.map */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
-		*(.text.hot .text .text.fixup .text.unlikely)		\
+		*(.text.hot .text .text.fixup .text.unlikely .text.*)	\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\
@@ -527,6 +532,7 @@
 
 /* init and exit section handling */
 #define INIT_DATA							\
+	KEEP(*(SORT(___kentry+*)))					\
 	*(.init.data)							\
 	MEM_DISCARD(init.data)						\
 	KERNEL_CTORS()							\
@@ -593,7 +599,7 @@
 		BSS_FIRST_SECTIONS					\
 		*(.bss..page_aligned)					\
 		*(.dynbss)						\
-		*(.bss)							\
+		*(.bss .bss.[0-9a-zA-Z_]*)				\
 		*(COMMON)						\
 	}
 
@@ -676,12 +682,12 @@
 
 #define INIT_CALLS_LEVEL(level)						\
 		VMLINUX_SYMBOL(__initcall##level##_start) = .;		\
-		*(.initcall##level##.init)				\
-		*(.initcall##level##s.init)				\
+		KEEP(*(.initcall##level##.init))			\
+		KEEP(*(.initcall##level##s.init))			\
 
 #define INIT_CALLS							\
 		VMLINUX_SYMBOL(__initcall_start) = .;			\
-		*(.initcallearly.init)					\
+		KEEP(*(.initcallearly.init))				\
 		INIT_CALLS_LEVEL(0)					\
 		INIT_CALLS_LEVEL(1)					\
 		INIT_CALLS_LEVEL(2)					\
@@ -695,21 +701,21 @@
 
 #define CON_INITCALL							\
 		VMLINUX_SYMBOL(__con_initcall_start) = .;		\
-		*(.con_initcall.init)					\
+		KEEP(*(.con_initcall.init))				\
 		VMLINUX_SYMBOL(__con_initcall_end) = .;
 
 #define SECURITY_INITCALL						\
 		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
-		*(.security_initcall.init)				\
+		KEEP(*(.security_initcall.init))			\
 		VMLINUX_SYMBOL(__security_initcall_end) = .;
 
 #ifdef CONFIG_BLK_DEV_INITRD
 #define INIT_RAM_FS							\
 	. = ALIGN(4);							\
 	VMLINUX_SYMBOL(__initramfs_start) = .;				\
-	*(.init.ramfs)							\
+	KEEP(*(.init.ramfs))						\
 	. = ALIGN(8);							\
-	*(.init.ramfs.info)
+	KEEP(*(.init.ramfs.info))
 #else
 #define INIT_RAM_FS
 #endif

commit f716a85cd6045c994011268223706642cff7e485
Merge: 221bb8a46e23 a519167e753e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:37:12 2016 -0400

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild
    
    Pull kbuild updates from Michal Marek:
    
     - GCC plugin support by Emese Revfy from grsecurity, with a fixup from
       Kees Cook.  The plugins are meant to be used for static analysis of
       the kernel code.  Two plugins are provided already.
    
     - reduction of the gcc commandline by Arnd Bergmann.
    
     - IS_ENABLED / IS_REACHABLE macro enhancements by Masahiro Yamada
    
     - bin2c fix by Michael Tautschnig
    
     - setlocalversion fix by Wolfram Sang
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild:
      gcc-plugins: disable under COMPILE_TEST
      kbuild: Abort build on bad stack protector flag
      scripts: Fix size mismatch of kexec_purgatory_size
      kbuild: make samples depend on headers_install
      Kbuild: don't add obj tree in additional includes
      Kbuild: arch: look for generated headers in obtree
      Kbuild: always prefix objtree in LINUXINCLUDE
      Kbuild: avoid duplicate include path
      Kbuild: don't add ../../ to include path
      vmlinux.lds.h: replace config_enabled() with IS_ENABLED()
      kconfig.h: allow to use IS_{ENABLE,REACHABLE} in macro expansion
      kconfig.h: use already defined macros for IS_REACHABLE() define
      export.h: use __is_defined() to check if __KSYM_* is defined
      kconfig.h: use __is_defined() to check if MODULE is defined
      kbuild: setlocalversion: print error to STDERR
      Add sancov plugin
      Add Cyclomatic complexity GCC plugin
      GCC plugin infrastructure
      Shared library support

commit 015cd867e566e3a27b5e8062eb24eeaa4d77297f
Merge: 85802a49a85c 64a40c84001e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 26 12:22:51 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "There are a couple of new things for s390 with this merge request:
    
       - a new scheduling domain "drawer" is added to reflect the unusual
         topology found on z13 machines.  Performance tests showed up to 8
         percent gain with the additional domain.
    
       - the new crc-32 checksum crypto module uses the vector-galois-field
         multiply and sum SIMD instruction to speed up crc-32 and crc-32c.
    
       - proper __ro_after_init support, this requires RO_AFTER_INIT_DATA in
         the generic vmlinux.lds linker script definitions.
    
       - kcov instrumentation support.  A prerequisite for that is the
         inline assembly basic block cleanup, which is the reason for the
         net/iucv/iucv.c change.
    
       - support for 2GB pages is added to the hugetlbfs backend.
    
      Then there are two removals:
    
       - the oprofile hardware sampling support is dead code and is removed.
         The oprofile user space uses the perf interface nowadays.
    
       - the ETR clock synchronization is removed, this has been superseeded
         be the STP clock synchronization.  And it always has been
         "interesting" code..
    
      And the usual bug fixes and cleanups"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (82 commits)
      s390/pci: Delete an unnecessary check before the function call "pci_dev_put"
      s390/smp: clean up a condition
      s390/cio/chp : Remove deprecated create_singlethread_workqueue
      s390/chsc: improve channel path descriptor determination
      s390/chsc: sanitize fmt check for chp_desc determination
      s390/cio: make fmt1 channel path descriptor optional
      s390/chsc: fix ioctl CHSC_INFO_CU command
      s390/cio/device_ops: fix kernel doc
      s390/cio: allow to reset channel measurement block
      s390/console: Make preferred console handling more consistent
      s390/mm: fix gmap tlb flush issues
      s390/mm: add support for 2GB hugepages
      s390: have unique symbol for __switch_to address
      s390/cpuinfo: show maximum thread id
      s390/ptrace: clarify bits in the per_struct
      s390: stack address vs thread_info
      s390: remove pointless load within __switch_to
      s390: enable kcov support
      s390/cpumf: use basic block for ecctr inline assembly
      s390/hypfs: use basic block for diag inline assembly
      ...

commit e41f501d391265ff568f3e49d6128cc30856a36f
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Thu Jul 14 12:07:29 2016 -0700

    vmlinux.lds: account for destructor sections
    
    If CONFIG_KASAN is enabled and gcc is configured with
    --disable-initfini-array and/or gold linker is used, gcc emits
    .ctors/.dtors and .text.startup/.text.exit sections instead of
    .init_array/.fini_array.  .dtors section is not explicitly accounted in
    the linker script and messes vvar/percpu layout.
    
    We want:
      ffffffff822bfd80 D _edata
      ffffffff822c0000 D __vvar_beginning_hack
      ffffffff822c0000 A __vvar_page
      ffffffff822c0080 0000000000000098 D vsyscall_gtod_data
      ffffffff822c1000 A __init_begin
      ffffffff822c1000 D init_per_cpu__irq_stack_union
      ffffffff822c1000 A __per_cpu_load
      ffffffff822d3000 D init_per_cpu__gdt_page
    
    We got:
      ffffffff8279a600 D _edata
      ffffffff8279b000 A __vvar_page
      ffffffff8279c000 A __init_begin
      ffffffff8279c000 D init_per_cpu__irq_stack_union
      ffffffff8279c000 A __per_cpu_load
      ffffffff8279e000 D __vvar_beginning_hack
      ffffffff8279e080 0000000000000098 D vsyscall_gtod_data
      ffffffff827ae000 D init_per_cpu__gdt_page
    
    This happens because __vvar_page and .vvar get different addresses in
    arch/x86/kernel/vmlinux.lds.S:
    
            . = ALIGN(PAGE_SIZE);
            __vvar_page = .;
    
            .vvar : AT(ADDR(.vvar) - LOAD_OFFSET) {
                    /* work around gold bug 13023 */
                    __vvar_beginning_hack = .;
    
    Discard .dtors/.fini_array/.text.exit, since we don't call dtors.
    Merge .text.startup into init text.
    
    Link: http://lkml.kernel.org/r/1467386363-120030-1-git-send-email-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: <stable@vger.kernel.org>    [4.0+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6a67ab94b553..081d0f258d4c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -542,15 +542,19 @@
 
 #define INIT_TEXT							\
 	*(.init.text)							\
+	*(.text.startup)						\
 	MEM_DISCARD(init.text)
 
 #define EXIT_DATA							\
 	*(.exit.data)							\
+	*(.fini_array)							\
+	*(.dtors)							\
 	MEM_DISCARD(exit.data)						\
 	MEM_DISCARD(exit.rodata)
 
 #define EXIT_TEXT							\
 	*(.exit.text)							\
+	*(.text.exit)							\
 	MEM_DISCARD(exit.text)
 
 #define EXIT_CALL							\

commit 5ee02af153661ed98b5ccdfb984d78e7a8881b56
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Jun 14 14:58:58 2016 +0900

    vmlinux.lds.h: replace config_enabled() with IS_ENABLED()
    
    The use of config_enabled() against config options is ambiguous.
    
    Now, IS_ENABLED() is implemented purely with macro expansion, so
    let's replace config_enabled() with IS_ENABLED().
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6a67ab94b553..faa4d2bf92f5 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -164,7 +164,7 @@
 
 #define ___OF_TABLE(cfg, name)	_OF_TABLE_##cfg(name)
 #define __OF_TABLE(cfg, name)	___OF_TABLE(cfg, name)
-#define OF_TABLE(cfg, name)	__OF_TABLE(config_enabled(cfg), name)
+#define OF_TABLE(cfg, name)	__OF_TABLE(IS_ENABLED(cfg), name)
 #define _OF_TABLE_0(name)
 #define _OF_TABLE_1(name)						\
 	. = ALIGN(8);							\

commit 32fb2fc5c357fb99616bbe100dbcb27bc7f5d045
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Jun 7 12:20:51 2016 +0200

    vmlinux.lds.h: allow arch specific handling of ro_after_init data section
    
    commit c74ba8b3480d ("arch: Introduce post-init read-only memory")
    introduced the __ro_after_init attribute which allows to add variables
    to the ro_after_init data section.
    
    This new section was added to rodata, even though it contains writable
    data. This in turn causes problems on architectures which mark the
    page table entries read-only that point to rodata very early.
    
    This patch allows architectures to implement an own handling of the
    .data..ro_after_init section.
    Usually that would be:
    - mark the rodata section read-only very early
    - mark the ro_after_init section read-only within mark_rodata_ro
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6a67ab94b553..c4436d072d37 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -249,6 +249,14 @@
 	*(.data..init_task)						\
 	VMLINUX_SYMBOL(__end_init_task) = .;
 
+/*
+ * Allow architectures to handle ro_after_init data on their
+ * own by defining an empty RO_AFTER_INIT_DATA.
+ */
+#ifndef RO_AFTER_INIT_DATA
+#define RO_AFTER_INIT_DATA *(.data..ro_after_init)
+#endif
+
 /*
  * Read only Data
  */
@@ -257,7 +265,7 @@
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
-		*(.data..ro_after_init)	/* Read only after init */	\
+		RO_AFTER_INIT_DATA	/* Read only after init */	\
 		*(__vermagic)		/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\

commit 91ed140d6c1e168b11bbbddac4f6066f40a0c6b5
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Mar 31 16:21:02 2016 +0200

    x86/asm: Make sure verify_cpu() has a good stack
    
    04633df0c43d ("x86/cpu: Call verify_cpu() after having entered long mode too")
    added the call to verify_cpu() for sanitizing CPU configuration.
    
    The latter uses the stack minimally and it can happen that we land in
    startup_64() directly from a 64-bit bootloader. Then we want to use our
    own, known good stack.
    
    Do that.
    
    APs don't need this as the trampoline sets up a stack for them.
    
    Reported-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mika Penttil <mika.penttila@nextfour.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1459434062-31055-1-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 339125bb4d2c..6a67ab94b553 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -245,7 +245,9 @@
 
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
-	*(.data..init_task)
+	VMLINUX_SYMBOL(__start_init_task) = .;				\
+	*(.data..init_task)						\
+	VMLINUX_SYMBOL(__end_init_task) = .;
 
 /*
  * Read only Data

commit be7635e7287e0e8013af3c89a6354a9e0182594c
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:22:05 2016 -0700

    arch, ftrace: for KASAN put hard/soft IRQ entries into separate sections
    
    KASAN needs to know whether the allocation happens in an IRQ handler.
    This lets us strip everything below the IRQ entry point to reduce the
    number of unique stack traces needed to be stored.
    
    Move the definition of __irq_entry to <linux/interrupt.h> so that the
    users don't need to pull in <linux/ftrace.h>.  Also introduce the
    __softirq_entry macro which is similar to __irq_entry, but puts the
    corresponding functions to the .softirqentry.text section.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8f5a12ab2f2b..339125bb4d2c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -456,7 +456,7 @@
 		*(.entry.text)						\
 		VMLINUX_SYMBOL(__entry_text_end) = .;
 
-#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)
 #define IRQENTRY_TEXT							\
 		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__irqentry_text_start) = .;		\
@@ -466,6 +466,16 @@
 #define IRQENTRY_TEXT
 #endif
 
+#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)
+#define SOFTIRQENTRY_TEXT						\
+		ALIGN_FUNCTION();					\
+		VMLINUX_SYMBOL(__softirqentry_text_start) = .;		\
+		*(.softirqentry.text)					\
+		VMLINUX_SYMBOL(__softirqentry_text_end) = .;
+#else
+#define SOFTIRQENTRY_TEXT
+#endif
+
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  *(.head.text)
 

commit 96b9b1c95660d4bc5510c5d798d3817ae9f0b391
Merge: 8eee93e2576c a95fc9c8e576
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 13:53:25 2016 -0700

    Merge tag 'tty-4.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty
    
    Pull tty/serial updates from Greg KH:
     "Here's the big tty/serial driver pull request for 4.6-rc1.
    
      Lots of changes in here, Peter has been on a tear again, with lots of
      refactoring and bugs fixes, many thanks to the great work he has been
      doing.  Lots of driver updates and fixes as well, full details in the
      shortlog.
    
      All have been in linux-next for a while with no reported issues"
    
    * tag 'tty-4.6-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty: (220 commits)
      serial: 8250: describe CONFIG_SERIAL_8250_RSA
      serial: samsung: optimize UART rx fifo access routine
      serial: pl011: add mark/space parity support
      serial: sa1100: make sa1100_register_uart_fns a function
      tty: serial: 8250: add MOXA Smartio MUE boards support
      serial: 8250: convert drivers to use up_to_u8250p()
      serial: 8250/mediatek: fix building with SERIAL_8250=m
      serial: 8250/ingenic: fix building with SERIAL_8250=m
      serial: 8250/uniphier: fix modular build
      Revert "drivers/tty/serial: make 8250/8250_ingenic.c explicitly non-modular"
      Revert "drivers/tty/serial: make 8250/8250_mtk.c explicitly non-modular"
      serial: mvebu-uart: initial support for Armada-3700 serial port
      serial: mctrl_gpio: Add missing module license
      serial: ifx6x60: avoid uninitialized variable use
      tty/serial: at91: fix bad offset for UART timeout register
      tty/serial: at91: restore dynamic driver binding
      serial: 8250: Add hardware dependency to RT288X option
      TTY, devpts: document pty count limiting
      tty: goldfish: support platform_device with id -1
      drivers: tty: goldfish: Add device tree bindings
      ...

commit c74ba8b3480da6ddaea17df2263ec09b869ac496
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 17 14:41:15 2016 -0800

    arch: Introduce post-init read-only memory
    
    One of the easiest ways to protect the kernel from attack is to reduce
    the internal attack surface exposed when a "write" flaw is available. By
    making as much of the kernel read-only as possible, we reduce the
    attack surface.
    
    Many things are written to only during __init, and never changed
    again. These cannot be made "const" since the compiler will do the wrong
    thing (we do actually need to write to them). Instead, move these items
    into a memory region that will be made read-only during mark_rodata_ro()
    which happens after all kernel __init code has finished.
    
    This introduces __ro_after_init as a way to mark such memory, and adds
    some documentation about the existing __read_mostly marking.
    
    This improves the security of the Linux kernel by marking formerly
    read-write memory regions as read-only on a fully booted up system.
    
    Based on work by PaX Team and Brad Spengler.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1455748879-21872-5-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c4bd0e2c173c..772c784ba763 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -256,6 +256,7 @@
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
+		*(.data..ro_after_init)	/* Read only after init */	\
 		*(__vermagic)		/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\

commit 2eaa790989e03900298ad24f77f1086dbbc1aebd
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat Jan 16 15:23:39 2016 -0800

    earlycon: Use common framework for earlycon declarations
    
    Use a single common table of struct earlycon_id for both command line
    and devicetree. Re-define OF_EARLYCON_DECLARE() macro to instance a
    unique earlycon declaration (the declaration is only guaranteed to be
    unique within a compilation unit; separate compilation units must still
    use unique earlycon names).
    
    The semantics of OF_EARLYCON_DECLARE() is different; it declares an
    earlycon which can matched either on the command line or by devicetree.
    EARLYCON_DECLARE() is semantically unchanged; it declares an earlycon
    which is matched by command line only. Remove redundant instances of
    EARLYCON_DECLARE().
    
    This enables all earlycons to properly initialize struct console
    with the appropriate name and index, which improves diagnostics and
    enables direct earlycon-to-console handoff.
    
    Acked-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c4bd0e2c173c..e9a81a6a109f 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -157,7 +157,7 @@
 #define EARLYCON_TABLE() STRUCT_ALIGN();			\
 			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
 			 *(__earlycon_table)			\
-			 *(__earlycon_table_end)
+			 VMLINUX_SYMBOL(__earlycon_table_end) = .;
 #else
 #define EARLYCON_TABLE()
 #endif
@@ -179,7 +179,6 @@
 #define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
 #define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
 #define CPUIDLE_METHOD_OF_TABLES() OF_TABLE(CONFIG_CPU_IDLE, cpuidle_method)
-#define EARLYCON_OF_TABLES()	OF_TABLE(CONFIG_SERIAL_EARLYCON, earlycon)
 
 #ifdef CONFIG_ACPI
 #define ACPI_PROBE_TABLE(name)						\
@@ -526,8 +525,7 @@
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
 	ACPI_PROBE_TABLE(clksrc)					\
-	EARLYCON_TABLE()						\
-	EARLYCON_OF_TABLES()
+	EARLYCON_TABLE()
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit c625f76a9910b9d51df5d6ca40a8da0684326996
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Mon Sep 28 15:49:15 2015 +0100

    clocksource / ACPI: Add probing infrastructure for ACPI-based clocksources
    
    DT enjoys a rather nice probing infrastructure for clocksources,
    while ACPI is so far stuck into a very distant past.
    
    This patch introduces a declarative API, allowing clocksources
    to be self-contained and be called when parsing the GTDT table.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index cd836cd1ea24..c4bd0e2c173c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -525,6 +525,7 @@
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\
 	ACPI_PROBE_TABLE(irqchip)					\
+	ACPI_PROBE_TABLE(clksrc)					\
 	EARLYCON_TABLE()						\
 	EARLYCON_OF_TABLES()
 

commit 46e589a391809627144e6bee93d71d73fe915db2
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Mon Sep 28 15:49:13 2015 +0100

    irqchip / ACPI: Add probing infrastructure for ACPI-based irqchips
    
    DT enjoys a rather nice probing infrastructure for irqchips, while
    ACPI is so far stuck into a very distant past.
    
    This patch introduces a declarative API, allowing irqchips to be
    self-contained and be called when a particular entry is matched
    in the MADT table.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index efd7ed1eafae..cd836cd1ea24 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -524,6 +524,7 @@
 	CPUIDLE_METHOD_OF_TABLES()					\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\
+	ACPI_PROBE_TABLE(irqchip)					\
 	EARLYCON_TABLE()						\
 	EARLYCON_OF_TABLES()
 

commit e647b532275bb357e87272e052fccf5fcdb36a17
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Mon Sep 28 15:49:12 2015 +0100

    ACPI: Add early device probing infrastructure
    
    IRQ controllers and timers are the two types of device the kernel
    requires before being able to use the device driver model.
    
    ACPI so far lacks a proper probing infrastructure similar to the one
    we have with DT, where we're able to declare IRQ chips and
    clocksources inside the driver code, and let the core code pick it up
    and call us back on a match. This leads to all kind of really ugly
    hacks all over the arm64 code and even in the ACPI layer.
    
    In order to allow some basic probing based on the ACPI tables,
    introduce "struct acpi_probe_entry" which contains just enough
    data and callbacks to match a table, an optional subtable, and
    call a probe function. A driver can, at build time, register itself
    and expect being called if the right entry exists in the ACPI
    table.
    
    A acpi_probe_device_table() is provided, taking an identifier for
    a set of acpi_prove_entries, and iterating over the registered
    entries.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 1781e54ea6d3..efd7ed1eafae 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -181,6 +181,16 @@
 #define CPUIDLE_METHOD_OF_TABLES() OF_TABLE(CONFIG_CPU_IDLE, cpuidle_method)
 #define EARLYCON_OF_TABLES()	OF_TABLE(CONFIG_SERIAL_EARLYCON, earlycon)
 
+#ifdef CONFIG_ACPI
+#define ACPI_PROBE_TABLE(name)						\
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__##name##_acpi_probe_table) = .;		\
+	*(__##name##_acpi_probe_table)					\
+	VMLINUX_SYMBOL(__##name##_acpi_probe_table_end) = .;
+#else
+#define ACPI_PROBE_TABLE(name)
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	VMLINUX_SYMBOL(__dtb_start) = .;				\

commit 9bebe9e5b0f3109a14000df25308c2971f872605
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun Jul 19 18:01:19 2015 -0700

    kbuild: Fix .text.unlikely placement
    
    When building a kernel with .text.unlikely text the unlikely text for
    each translation unit was put next to the main .text code in the
    final vmlinux.
    
    The problem is that the linker doesn't allow more specific submatches
    of a section name in a different linker script statement after the
    main match.
    
    So we need to move them all into one line. With that change
    .text.unlikely is at the end of everything again.
    
    I also moved .text.hot into the same statement though, even though
    that's not strictly needed.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Michal Marek <mmarek@suse.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8bd374d3cf21..1781e54ea6d3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -412,12 +412,10 @@
  * during second ld run in second ld pass when generating System.map */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
-		*(.text.hot)						\
-		*(.text .text.fixup)					\
+		*(.text.hot .text .text.fixup .text.unlikely)		\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\
-		*(.text.unlikely)
 
 
 /* sched.text is aling to function alignment to secure we have same

commit 41d5e08ea86af3359239d5a6f7021cdc61beaa49
Merge: 8d582b94291b 5dbc32a88f1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 21 09:33:10 2015 -0700

    Merge tag 'tty-4.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty
    
    Pull tty/serial updates from Greg KH:
     "Here's the big tty/serial driver update for 4.1-rc1.
    
      It was delayed for a bit due to some questions surrounding some of the
      console command line parsing changes that are in here.  There's still
      one tiny regression for people who were previously putting multiple
      console command lines and expecting them all to be ignored for some
      odd reason, but Peter is working on fixing that.  If not, I'll send a
      revert for the offending patch, but I have faith that Peter can
      address it.
    
      Other than the console work here, there's the usual serial driver
      updates and changes, and a buch of 8250 reworks to try to make that
      driver easier to maintain over time, and have it support more devices
      in the future.
    
      All of these have been in linux-next for a while"
    
    * tag 'tty-4.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty: (119 commits)
      n_gsm: Drop unneeded cast on netdev_priv
      sc16is7xx: expose RTS inversion in RS-485 mode
      serial: 8250_pci: port failed after wakeup from S3
      earlycon: 8250: Document kernel command line options
      earlycon: 8250: Fix command line regression
      earlycon: Fix __earlycon_table stride
      tty: clean up the tty time logic a bit
      serial: 8250_dw: only get the clock rate in one place
      serial: 8250_dw: remove useless ACPI ID check
      dmaengine: hsu: move memory allocation to GFP_NOWAIT
      dmaengine: hsu: remove redundant pieces of code
      serial: 8250_pci: add Intel Tangier support
      dmaengine: hsu: add Intel Tangier PCI ID
      serial: 8250_pci: replace switch-case by formula for Intel MID
      serial: 8250_pci: replace switch-case by formula
      tty: cpm_uart: replace CONFIG_8xx by CONFIG_CPM1
      serial: jsm: some off by one bugs
      serial: xuartps: Fix check in console_setup().
      serial: xuartps: Get rid of register access macros.
      serial: xuartps: Fix iobase use.
      ...

commit bb0fd7ab0986105765d11baa82e619c618a235aa
Merge: bdfa54dfd9ee 4b2f8838479e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 21:03:26 2015 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Included in this update are both some long term fixes and some new
      features.
    
      Fixes:
    
       - An integer overflow in the calculation of ELF_ET_DYN_BASE.
    
       - Avoiding OOMs for high-order IOMMU allocations
    
       - SMP requires the data cache to be enabled for synchronisation
         primitives to work, so prevent the CPU_DCACHE_DISABLE option being
         visible on SMP builds.
    
       - A bug going back 10+ years in the noMMU ARM94* CPU support code,
         where it corrupts registers.  Found by folk getting Linux running
         on their cameras.
    
       - Versatile Express needs an errata workaround enabled for CPU
         hot-unplug to work.
    
      Features:
    
       - Clean up module linker by handling out of range relocations
         separately from relocation cases we don't handle.
    
       - Fix a long term bug in the pci_mmap_page_range() code, which we
         hope won't impact userspace (we hope there's no users of the
         existing broken interface.)
    
       - Don't map DMA coherent allocations when we don't have a MMU.
    
       - Drop experimental status for SMP_ON_UP.
    
       - Warn when DT doesn't specify ePAPR mandatory cache properties.
    
       - Add documentation concerning how we find the start of physical
         memory for AUTO_ZRELADDR kernels, detailing why we have chosen the
         mask and the implications of changing it.
    
       - Updates from Ard Biesheuvel to address some issues with large
         kernels (such as allyesconfig) failing to link.
    
       - Allow hibernation to work on modern (ARMv7) CPUs - this appears to
         have never worked in the past on these CPUs.
    
       - Enable IRQ_SHOW_LEVEL, which changes the /proc/interrupts output
         format (hopefully without userspace breaking...  let's hope that if
         it causes someone a problem, they tell us.)
    
       - Fix tegra-ahb DT offsets.
    
       - Rework ARM errata 643719 code (and ARMv7 flush_cache_louis()/
         flush_dcache_all()) code to be more efficient, and enable this
         errata workaround by default for ARMv7+SMP CPUs.  This complements
         the Versatile Express fix above.
    
       - Rework ARMv7 context code for errata 430973, so that only Cortex A8
         CPUs are impacted by the branch target buffer flush when this
         errata is enabled.  Also update the help text to indicate that all
         r1p* A8 CPUs are impacted.
    
       - Switch ARM to the generic show_mem() implementation, it conveys all
         the information which we were already reporting.
    
       - Prevent slow timer sources being used for udelay() - timers running
         at less than 1MHz are not useful for this, and can cause udelay()
         to return immediately, without any wait.  Using such a slow timer
         is silly.
    
       - VDSO support for 32-bit ARM, mainly for gettimeofday() using the
         ARM architected timer.
    
       - Perf support for Scorpion performance monitoring units"
    
    vdso semantic conflict fixed up as per linux-next.
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (52 commits)
      ARM: update errata 430973 documentation to cover Cortex A8 r1p*
      ARM: ensure delay timer has sufficient accuracy for delays
      ARM: switch to use the generic show_mem() implementation
      ARM: proc-v7: avoid errata 430973 workaround for non-Cortex A8 CPUs
      ARM: enable ARM errata 643719 workaround by default
      ARM: cache-v7: optimise test for Cortex A9 r0pX devices
      ARM: cache-v7: optimise branches in v7_flush_cache_louis
      ARM: cache-v7: consolidate initialisation of cache level index
      ARM: cache-v7: shift CLIDR to extract appropriate field before masking
      ARM: cache-v7: use movw/movt instructions
      ARM: allow 16-bit instructions in ALT_UP()
      ARM: proc-arm94*.S: fix setup function
      ARM: vexpress: fix CPU hotplug with CT9x4 tile.
      ARM: 8276/1: Make CPU_DCACHE_DISABLE depend on !SMP
      ARM: 8335/1: Documentation: DT bindings: Tegra AHB: document the legacy base address
      ARM: 8334/1: amba: tegra-ahb: detect and correct bogus base address
      ARM: 8333/1: amba: tegra-ahb: fix register offsets in the macros
      ARM: 8339/1: Enable CONFIG_GENERIC_IRQ_SHOW_LEVEL
      ARM: 8338/1: kexec: Relax SMP validation to improve DT compatibility
      ARM: 8337/1: mm: Do not invoke OOM for higher order IOMMU DMA allocations
      ...

commit 2481bc75283ea10e75d5fb1a8b42af363fc4b45c
Merge: 8691c130fae1 518b4e272d99
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 20:21:54 2015 -0700

    Merge tag 'pm+acpi-4.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "These are mostly fixes and cleanups all over, although there are a few
      items that sort of fall into the new feature category.
    
      First off, we have new callbacks for PM domains that should help us to
      handle some issues related to device initialization in a better way.
    
      There also is some consolidation in the unified device properties API
      area allowing us to use that inferface for accessing data coming from
      platform initialization code in addition to firmware-provided data.
    
      We have some new device/CPU IDs in a few drivers, support for new
      chips and a new cpufreq driver too.
    
      Specifics:
    
       - Generic PM domains support update including new PM domain callbacks
         to handle device initialization better (Russell King, Rafael J
         Wysocki, Kevin Hilman)
    
       - Unified device properties API update including a new mechanism for
         accessing data provided by platform initialization code (Rafael J
         Wysocki, Adrian Hunter)
    
       - ARM cpuidle update including ARM32/ARM64 handling consolidation
         (Daniel Lezcano)
    
       - intel_idle update including support for the Silvermont Core in the
         Baytrail SOC and for the Airmont Core in the Cherrytrail and
         Braswell SOCs (Len Brown, Mathias Krause)
    
       - New cpufreq driver for Hisilicon ACPU (Leo Yan)
    
       - intel_pstate update including support for the Knights Landing chip
         (Dasaratharaman Chandramouli, Kristen Carlson Accardi)
    
       - QorIQ cpufreq driver update (Tang Yuantian, Arnd Bergmann)
    
       - powernv cpufreq driver update (Shilpasri G Bhat)
    
       - devfreq update including Tegra support changes (Tomeu Vizoso,
         MyungJoo Ham, Chanwoo Choi)
    
       - powercap RAPL (Running-Average Power Limit) driver update including
         support for Intel Broadwell server chips (Jacob Pan, Mathias Krause)
    
       - ACPI device enumeration update related to the handling of the
         special PRP0001 device ID allowing DT-style 'compatible' property
         to be used for ACPI device identification (Rafael J Wysocki)
    
       - ACPI EC driver update including limited _DEP support (Lan Tianyu,
         Lv Zheng)
    
       - ACPI backlight driver update including a new mechanism to allow
         native backlight handling to be forced on non-Windows 8 systems and
         a new quirk for Lenovo Ideapad Z570 (Aaron Lu, Hans de Goede)
    
       - New Windows Vista compatibility quirk for Sony VGN-SR19XN (Chen Yu)
    
       - Assorted ACPI fixes and cleanups (Aaron Lu, Martin Kepplinger,
         Masanari Iida, Mika Westerberg, Nan Li, Rafael J Wysocki)
    
       - Fixes related to suspend-to-idle for the iTCO watchdog driver and
         the ACPI core system suspend/resume code (Rafael J Wysocki, Chen Yu)
    
       - PM tracing support for the suspend phase of system suspend/resume
         transitions (Zhonghui Fu)
    
       - Configurable delay for the system suspend/resume testing facility
         (Brian Norris)
    
       - PNP subsystem cleanups (Peter Huewe, Rafael J Wysocki)"
    
    * tag 'pm+acpi-4.1-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (74 commits)
      ACPI / scan: Fix NULL pointer dereference in acpi_companion_match()
      ACPI / scan: Rework modalias creation when "compatible" is present
      intel_idle: mark cpu id array as __initconst
      powercap / RAPL: mark rapl_ids array as __initconst
      powercap / RAPL: add ID for Broadwell server
      intel_pstate: Knights Landing support
      intel_pstate: remove MSR test
      cpufreq: fix qoriq uniprocessor build
      ACPI / scan: Take the PRP0001 position in the list of IDs into account
      ACPI / scan: Simplify acpi_match_device()
      ACPI / scan: Generalize of_compatible matching
      device property: Introduce firmware node type for platform data
      device property: Make it possible to use secondary firmware nodes
      PM / watchdog: iTCO: stop watchdog during system suspend
      cpufreq: hisilicon: add acpu driver
      ACPI / EC: Call acpi_walk_dep_device_list() after installing EC opregion handler
      cpufreq: powernv: Report cpu frequency throttling
      intel_idle: Add support for the Airmont Core in the Cherrytrail and Braswell SOCs
      intel_idle: Update support for Silvermont Core in Baytrail SOC
      PM / devfreq: tegra: Register governor on module init
      ...

commit 99492c39f39fc2d8c4ae36ecfb88d7de5d8106b5
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Fri Apr 3 08:57:51 2015 -0400

    earlycon: Fix __earlycon_table stride
    
    The compiler and the linker must agree on the alignment of
    struct earlycon_id; empirical testing and commit 07fca0e57fca92
    ("tracing: Properly align linker defined symbols") suggests
    32-byte alignment is the LCD.
    
    Reported-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 87e5b6f8f4fc..561daf49e52f 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -151,7 +151,7 @@
 #endif
 
 #ifdef CONFIG_SERIAL_EARLYCON
-#define EARLYCON_TABLE() . = ALIGN(8);				\
+#define EARLYCON_TABLE() STRUCT_ALIGN();			\
 			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
 			 *(__earlycon_table)			\
 			 *(__earlycon_table_end)

commit 0c564a538aa934ad15b2145aaf8b64f3feb0be63
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 24 17:58:09 2015 -0400

    tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values
    
    Several tracepoints use the helper functions __print_symbolic() or
    __print_flags() and pass in enums that do the mapping between the
    binary data stored and the value to print. This works well for reading
    the ASCII trace files, but when the data is read via userspace tools
    such as perf and trace-cmd, the conversion of the binary value to a
    human string format is lost if an enum is used, as userspace does not
    have access to what the ENUM is.
    
    For example, the tracepoint trace_tlb_flush() has:
    
     __print_symbolic(REC->reason,
        { TLB_FLUSH_ON_TASK_SWITCH, "flush on task switch" },
        { TLB_REMOTE_SHOOTDOWN, "remote shootdown" },
        { TLB_LOCAL_SHOOTDOWN, "local shootdown" },
        { TLB_LOCAL_MM_SHOOTDOWN, "local mm shootdown" })
    
    Which maps the enum values to the strings they represent. But perf and
    trace-cmd do no know what value TLB_LOCAL_MM_SHOOTDOWN is, and would
    not be able to map it.
    
    With TRACE_DEFINE_ENUM(), developers can place these in the event header
    files and ftrace will convert the enums to their values:
    
    By adding:
    
     TRACE_DEFINE_ENUM(TLB_FLUSH_ON_TASK_SWITCH);
     TRACE_DEFINE_ENUM(TLB_REMOTE_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_MM_SHOOTDOWN);
    
     $ cat /sys/kernel/debug/tracing/events/tlb/tlb_flush/format
    [...]
     __print_symbolic(REC->reason,
        { 0, "flush on task switch" },
        { 1, "remote shootdown" },
        { 2, "local shootdown" },
        { 3, "local mm shootdown" })
    
    The above is what userspace expects to see, and tools do not need to
    be modified to parse them.
    
    Link: http://lkml.kernel.org/r/20150403013802.220157513@goodmis.org
    
    Cc: Guilherme Cox <cox@computer.org>
    Cc: Tony Luck <tony.luck@gmail.com>
    Cc: Xie XiuQi <xiexiuqi@huawei.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ac78910d7416..f8e8b34dc427 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -124,7 +124,10 @@
 #define FTRACE_EVENTS()	. = ALIGN(8);					\
 			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
 			*(_ftrace_events)				\
-			VMLINUX_SYMBOL(__stop_ftrace_events) = .;
+			VMLINUX_SYMBOL(__stop_ftrace_events) = .;	\
+			VMLINUX_SYMBOL(__start_ftrace_enum_maps) = .;	\
+			*(_ftrace_enum_map)				\
+			VMLINUX_SYMBOL(__stop_ftrace_enum_maps) = .;
 #else
 #define FTRACE_EVENTS()
 #endif

commit 779c88c94c34bd3b521da97b456a1aa51d870dec
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 24 10:39:10 2015 +0100

    ARM: 8321/1: asm-generic: introduce .text.fixup input section
    
    This introduces a new .text.fixup input section that gets emitted
    together with the .text section for each input object file.
    
    Note that
    
      *(.text)
      *(.text.fixup)
    
    is not the same as
    
      *(.text .text.fixup)
    
    and we are looking for the latter, to ensure that fixup snippets that
    are assembled into a separate section in the object file do not end
    up out of range for the relative branch instructions it contains if
    the .text section itself grows very large.
    
    This helps prevent linker failures on large ARM kernels.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ac78910d7416..463231d5bfc7 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -401,7 +401,7 @@
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
 		*(.text.hot)						\
-		*(.text)						\
+		*(.text .text.fixup)					\
 		*(.ref.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\

commit 470ca0de69feaba5df215ad804cec1859883a5ed
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Mon Mar 9 16:27:21 2015 -0400

    serial: earlycon: Enable earlycon without command line param
    
    Earlycon matching can only be triggered if 'earlycon=...' has been
    specified on the kernel command line. To workaround this limitation
    requires tight coupling between arches and specific serial drivers
    in order to start an earlycon. Devicetree avoids this limitation
    with a link table that contains the required data to match earlycons.
    
    Mirror this approach for earlycon match by name. Re-purpose
    EARLYCON_DECLARE to generate a table entry which associates name with
    setup() function. Re-purpose setup_earlycon() to scan this table for
    an earlycon match, which is registered if found.
    
    Declare one "earlycon" early_param, which calls setup_earlycon().
    
    This design allows setup_earlycon() to be called directly with a
    param string (as if 'earlycon=...' had been set on the command line).
    Re-registration (either directly or by early_param) is prevented.
    
    Acked-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ac78910d7416..87e5b6f8f4fc 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -150,6 +150,14 @@
 #define TRACE_SYSCALLS()
 #endif
 
+#ifdef CONFIG_SERIAL_EARLYCON
+#define EARLYCON_TABLE() . = ALIGN(8);				\
+			 VMLINUX_SYMBOL(__earlycon_table) = .;	\
+			 *(__earlycon_table)			\
+			 *(__earlycon_table_end)
+#else
+#define EARLYCON_TABLE()
+#endif
 
 #define ___OF_TABLE(cfg, name)	_OF_TABLE_##cfg(name)
 #define __OF_TABLE(cfg, name)	___OF_TABLE(cfg, name)
@@ -503,6 +511,7 @@
 	CPU_METHOD_OF_TABLES()						\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\
+	EARLYCON_TABLE()						\
 	EARLYCON_OF_TABLES()
 
 #define INIT_TEXT							\

commit 449e056c76cc8c777f3f5c3fb51c197ba2300c0c
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Feb 2 16:32:45 2015 +0100

    ARM: cpuidle: Add a cpuidle ops structure to be used for DT
    
    The current state of the different cpuidle drivers is the different PM
    operations are passed via the platform_data using the platform driver
    paradigm.
    
    This approach allowed to split the low level PM code from the arch specific
    and the generic cpuidle code.
    
    Unfortunately there are complaints about this approach as, in the context of the
    single kernel image, we have multiple drivers loaded in memory for nothing and
    the platform driver is not adequate for cpuidle.
    
    This patch provides a common interface via cpuidle ops for all new cpuidle
    driver and a definition for the device tree.
    
    It will allow with the next patches to a have a common definition with ARM64
    and share the same cpuidle driver.
    
    The code is optimized to use the __init section intensively in order to reduce
    the memory footprint after the driver is initialized and unify the function
    names with ARM64.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Kevin Hilman <khilman@linaro.org>
    Acked-by: Rob Herring <robherring2@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ac78910d7416..91c09305106d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -167,6 +167,7 @@
 #define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
 #define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
 #define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
+#define CPUIDLE_METHOD_OF_TABLES() OF_TABLE(CONFIG_CPU_IDLE, cpuidle_method)
 #define EARLYCON_OF_TABLES()	OF_TABLE(CONFIG_SERIAL_EARLYCON, earlycon)
 
 #define KERNEL_DTB()							\
@@ -501,6 +502,7 @@
 	CLKSRC_OF_TABLES()						\
 	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
+	CPUIDLE_METHOD_OF_TABLES()					\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\
 	EARLYCON_OF_TABLES()

commit 9ddf82521c86ae07af79dbe5a93c52890f2bab23
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Fri Feb 13 14:40:10 2015 -0800

    kernel: add support for .init_array.* constructors
    
    KASan uses constructors for initializing redzones for global variables.
    Globals instrumentation in GCC 4.9.2 produces constructors with priority
    (.init_array.00099)
    
    Currently kernel ignores such constructors.  Only constructors with
    default priority supported (.init_array)
    
    This patch adds support for constructors with priorities.  For kernel
    image we put pointers to constructors between __ctors_start/__ctors_end
    and do_ctors() will call them on start up.  For modules we merge
    .init_array.* sections into resulting .init_array.  Module code properly
    handles constructors in .init_array section.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Yuri Gribov <tetra2005@gmail.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bee5d683074d..ac78910d7416 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -478,6 +478,7 @@
 #define KERNEL_CTORS()	. = ALIGN(8);			   \
 			VMLINUX_SYMBOL(__ctors_start) = .; \
 			*(.ctors)			   \
+			*(SORT(.init_array.*))		   \
 			*(.init_array)			   \
 			VMLINUX_SYMBOL(__ctors_end) = .;
 #else

commit 1cd076bf67793942ed921b766f7d461de2ebc0a2
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 14:40:58 2014 +0100

    iommu: provide early initialisation hook for IOMMU drivers
    
    IOMMU drivers must be initialised before any of their upstream devices,
    otherwise the relevant iommu_ops won't be configured for the bus in
    question. To solve this, a number of IOMMU drivers use initcalls to
    initialise the driver before anything has a chance to be probed.
    
    Whilst this solves the immediate problem, it leaves the job of probing
    the IOMMU completely separate from the iommu_ops to configure the IOMMU,
    which are called on a per-bus basis and require the driver to figure out
    exactly which instance of the IOMMU is being requested. In particular,
    the add_device callback simply passes a struct device to the driver,
    which then has to parse firmware tables or probe buses to identify the
    relevant IOMMU instance.
    
    This patch takes the first step in addressing this problem by adding an
    early initialisation pass for IOMMU drivers, giving them the ability to
    store some per-instance data in their iommu_ops structure and store that
    in their of_node. This can later be used when parsing OF masters to
    identify the IOMMU instance in question.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Joerg Roedel <jroedel@suse.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index aa70cbda327c..bee5d683074d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -164,6 +164,7 @@
 #define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
 #define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
+#define IOMMU_OF_TABLES()	OF_TABLE(CONFIG_OF_IOMMU, iommu)
 #define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
 #define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
 #define EARLYCON_OF_TABLES()	OF_TABLE(CONFIG_SERIAL_EARLYCON, earlycon)
@@ -497,6 +498,7 @@
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\
 	CLKSRC_OF_TABLES()						\
+	IOMMU_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()					\

commit 562c85cadb065e33ec9f651b8d41cdfd3054a5d0
Author: Yalin Wang <Yalin.Wang@sonymobile.com>
Date:   Fri Sep 26 03:30:59 2014 +0100

    ARM: 8168/1: extend __init_end to a page align address
    
    This patch changes the __init_end address to a
    page align address, so that free_initmem() can
    free the whole .init section, because if the end
    address is not page aligned, it will round down to
    a page align address, then the tail unligned page
    will not be freed.
    
    Signed-off-by: wang <yalin.wang2010@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 5ba0360663a7..aa70cbda327c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -40,6 +40,8 @@
  * }
  *
  * [__init_begin, __init_end] is the init section that may be freed after init
+ * 	// __init_begin and __init_end should be page aligned, so that we can
+ *	// free the whole .init memory
  * [_stext, _etext] is the text section
  * [_sdata, _edata] is the data section
  *

commit 9f48c89862e39b7f33b44123fc425cf901c89428
Merge: 2a211f320ee3 1795cd9b3a91
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sun Jul 13 15:26:47 2014 -0700

    Merge 3.16-rc5 into char-misc-next
    
    This resolves a number of merge issues with changes in this tree and
    Linus's tree at the same time.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 330d282216d6e4d845a21b72572dc4df4122e8fa
Author: Zhengyu He <hzy@google.com>
Date:   Tue Jul 1 12:11:47 2014 -0700

    core: fix typo in percpu read_mostly section
    
    This fixes a typo that named the read_mostly section of percpu as
    readmostly. It works fine with SMP because the linker script specifies
    .data..percpu..readmostly. However, UP kernel builds don't have percpu
    sections defined and the non-percpu version of the section is called
    data..read_mostly, so .data..readmostly will float around and may break
    things unexpectedly.
    
    Looking at the original change that introduced data..percpu..readmostly
    (commit c957ef2c59e952803766ddc22e89981ab534606f), it looks like this
    was the original intention.
    
    Tested: Built UP kernel and confirmed the sections got merged.
    
    - Before the patch:
    $ objdump -h vmlinux.o  | grep '\.data\.\.read.*mostly'
    38 .data..read_mostly 00004418  0000000000000000  0000000000000000  00431ac0  2**6
    50 .data..readmostly 00000014  0000000000000000  0000000000000000  00444000  2**3
    
    - After the patch:
    $ objdump -h vmlinux.o  | grep '\.data\.\.read.*mostly'
    38 .data..read_mostly 00004438  0000000000000000  0000000000000000  00431ac0  2**6
    
    Signed-off-by: Zhengyu He <hzy@google.com>
    Signed-off-by: Filipe Brandenburger <filbranden@google.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 471ba48c7ae4..c1c0b0cf39b4 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -693,7 +693,7 @@
 	. = ALIGN(PAGE_SIZE);						\
 	*(.data..percpu..page_aligned)					\
 	. = ALIGN(cacheline);						\
-	*(.data..percpu..readmostly)					\
+	*(.data..percpu..read_mostly)					\
 	. = ALIGN(cacheline);						\
 	*(.data..percpu)						\
 	*(.data..percpu..shared_aligned)				\

commit 7d2a01b87f1682fde87461864e6682031bfaa0a9
Author: Andreas Noever <andreas.noever@gmail.com>
Date:   Tue Jun 3 22:04:09 2014 +0200

    PCI: Add pci_fixup_suspend_late quirk pass
    
    Add pci_fixup_suspend_late as a new pci_fixup_pass. The pass is called
    from suspend_noirq and poweroff_noirq. Using the same pass for suspend
    and hibernate is consistent with resume_early which is called by
    resume_noirq and restore_noirq.
    
    The new quirk pass is required for Thunderbolt support on Apple
    hardware.
    
    Signed-off-by: Andreas Noever <andreas.noever@gmail.com>
    Acked-by: Bjorn Helgaas <bhelgaas@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 471ba48c7ae4..47cd98656f9d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -268,6 +268,9 @@
 		VMLINUX_SYMBOL(__start_pci_fixups_suspend) = .;		\
 		*(.pci_fixup_suspend)					\
 		VMLINUX_SYMBOL(__end_pci_fixups_suspend) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_suspend_late) = .;	\
+		*(.pci_fixup_suspend_late)				\
+		VMLINUX_SYMBOL(__end_pci_fixups_suspend_late) = .;	\
 	}								\
 									\
 	/* Built-in firmware blobs */					\

commit 3737a12761636ebde0f09ef49daebb8eed18cc8a
Merge: c29deef32e36 82b897782d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:18:49 2014 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more perf updates from Ingo Molnar:
     "A second round of perf updates:
    
       - wide reaching kprobes sanitization and robustization, with the hope
         of fixing all 'probe this function crashes the kernel' bugs, by
         Masami Hiramatsu.
    
       - uprobes updates from Oleg Nesterov: tmpfs support, corner case
         fixes and robustization work.
    
       - perf tooling updates and fixes from Jiri Olsa, Namhyung Ki, Arnaldo
         et al:
            * Add support to accumulate hist periods (Namhyung Kim)
            * various fixes, refactorings and enhancements"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      perf: Differentiate exec() and non-exec() comm events
      perf: Fix perf_event_comm() vs. exec() assumption
      uprobes/x86: Rename arch_uprobe->def to ->defparam, minor comment updates
      perf/documentation: Add description for conditional branch filter
      perf/x86: Add conditional branch filtering support
      perf/tool: Add conditional branch filter 'cond' to perf record
      perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
      uprobes: Teach copy_insn() to support tmpfs
      uprobes: Shift ->readpage check from __copy_insn() to uprobe_register()
      perf/x86: Use common PMU interrupt disabled code
      perf/ARM: Use common PMU interrupt disabled code
      perf: Disable sampled events if no PMU interrupt
      perf: Fix use after free in perf_remove_from_context()
      perf tools: Fix 'make help' message error
      perf record: Fix poll return value propagation
      perf tools: Move elide bool into perf_hpp_fmt struct
      perf tools: Remove elide setup for SORT_MODE__MEMORY mode
      perf tools: Fix "==" into "=" in ui_browser__warning assignment
      perf tools: Allow overriding sysfs and proc finding with env var
      perf tools: Consider header files outside perf directory in tags target
      ...

commit b0b6abd34c1b508d4ac95dbc614f36c49d29e65a
Author: Rob Herring <robh@kernel.org>
Date:   Thu Mar 27 08:06:16 2014 -0500

    serial: earlycon: add DT support
    
    This adds the infrastructure to generic earlycon for earlycon setup
    using DT. The actual setup is not enabled until a following commit to
    add the FDT parsing.
    
    Signed-off-by: Rob Herring <robh@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Grant Likely <grant.likely@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b9404f6590f1..d647637cd699 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -155,6 +155,7 @@
 #define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
 #define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
 #define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
+#define EARLYCON_OF_TABLES()	OF_TABLE(CONFIG_SERIAL_EARLYCON, earlycon)
 
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
@@ -483,7 +484,8 @@
 	CLKSRC_OF_TABLES()						\
 	CPU_METHOD_OF_TABLES()						\
 	KERNEL_DTB()							\
-	IRQCHIP_OF_MATCH_TABLE()
+	IRQCHIP_OF_MATCH_TABLE()					\
+	EARLYCON_OF_TABLES()
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit 063092883039e49e5edc1f63133dc5ad437361a2
Author: Rob Herring <robh@kernel.org>
Date:   Mon Mar 24 16:59:20 2014 -0500

    vmlinuz.lds: define OF table sections with macros
    
    OF table sections all have the same pattern, so create a macro to define
    them and insure consistency.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index fe57c5f1bd1a..b9404f6590f1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -139,52 +139,22 @@
 #define TRACE_SYSCALLS()
 #endif
 
-#ifdef CONFIG_CLKSRC_OF
-#define CLKSRC_OF_TABLES() . = ALIGN(8);				\
-			   VMLINUX_SYMBOL(__clksrc_of_table) = .;	\
-			   *(__clksrc_of_table)				\
-			   *(__clksrc_of_table_end)
-#else
-#define CLKSRC_OF_TABLES()
-#endif
 
-#ifdef CONFIG_IRQCHIP
-#define IRQCHIP_OF_MATCH_TABLE()					\
+#define ___OF_TABLE(cfg, name)	_OF_TABLE_##cfg(name)
+#define __OF_TABLE(cfg, name)	___OF_TABLE(cfg, name)
+#define OF_TABLE(cfg, name)	__OF_TABLE(config_enabled(cfg), name)
+#define _OF_TABLE_0(name)
+#define _OF_TABLE_1(name)						\
 	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__irqchip_of_table) = .;				\
-	*(__irqchip_of_table)		  				\
-	*(__irqchip_of_table_end)
-#else
-#define IRQCHIP_OF_MATCH_TABLE()
-#endif
-
-#ifdef CONFIG_COMMON_CLK
-#define CLK_OF_TABLES() . = ALIGN(8);				\
-			VMLINUX_SYMBOL(__clk_of_table) = .;	\
-			*(__clk_of_table)			\
-			*(__clk_of_table_end)
-#else
-#define CLK_OF_TABLES()
-#endif
-
-#ifdef CONFIG_OF_RESERVED_MEM
-#define RESERVEDMEM_OF_TABLES()				\
-	. = ALIGN(8);					\
-	VMLINUX_SYMBOL(__reservedmem_of_table) = .;	\
-	*(__reservedmem_of_table)			\
-	*(__reservedmem_of_table_end)
-#else
-#define RESERVEDMEM_OF_TABLES()
-#endif
-
-#ifdef CONFIG_SMP
-#define CPU_METHOD_OF_TABLES() . = ALIGN(8);				    \
-			   VMLINUX_SYMBOL(__cpu_method_of_table) = .; \
-			   *(__cpu_method_of_table)			    \
-			   *(__cpu_method_of_table_end)
-#else
-#define CPU_METHOD_OF_TABLES()
-#endif
+	VMLINUX_SYMBOL(__##name##_of_table) = .;			\
+	*(__##name##_of_table)						\
+	*(__##name##_of_table_end)
+
+#define CLKSRC_OF_TABLES()	OF_TABLE(CONFIG_CLKSRC_OF, clksrc)
+#define IRQCHIP_OF_MATCH_TABLE() OF_TABLE(CONFIG_IRQCHIP, irqchip)
+#define CLK_OF_TABLES()		OF_TABLE(CONFIG_COMMON_CLK, clk)
+#define RESERVEDMEM_OF_TABLES()	OF_TABLE(CONFIG_OF_RESERVED_MEM, reservedmem)
+#define CPU_METHOD_OF_TABLES()	OF_TABLE(CONFIG_SMP, cpu_method)
 
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\

commit 9a721c41113a50ccbe184d67a5e551feb99e36a9
Author: Rob Herring <robh@kernel.org>
Date:   Mon Mar 24 16:11:54 2014 -0500

    ARM: align cpu_method_of_table naming
    
    The cpu_method_of_table is the oddball of the various OF linker sections.
    In preparation to have common linker section definitions, align the
    cpu_method_of_table with the other definitions for the naming and ending
    with a blank struct.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Cc: Russell King <linux@arm.linux.org.uk>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b1c6f9d0c4ff..fe57c5f1bd1a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -179,9 +179,9 @@
 
 #ifdef CONFIG_SMP
 #define CPU_METHOD_OF_TABLES() . = ALIGN(8);				    \
-			   VMLINUX_SYMBOL(__cpu_method_of_table_begin) = .; \
+			   VMLINUX_SYMBOL(__cpu_method_of_table) = .; \
 			   *(__cpu_method_of_table)			    \
-			   VMLINUX_SYMBOL(__cpu_method_of_table_end) = .;
+			   *(__cpu_method_of_table_end)
 #else
 #define CPU_METHOD_OF_TABLES()
 #endif

commit 735e0da7fc55a0456476f6b40f85024f68f87092
Author: Rob Herring <robh@kernel.org>
Date:   Mon Mar 24 16:06:42 2014 -0500

    irqchip: align irqchip OF match table section naming
    
    Make the irqchip OF match table section naming aligned with other
    OF match table sections in preparation to have a common definition.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 146e4fffd710..b1c6f9d0c4ff 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -151,9 +151,9 @@
 #ifdef CONFIG_IRQCHIP
 #define IRQCHIP_OF_MATCH_TABLE()					\
 	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__irqchip_begin) = .;				\
+	VMLINUX_SYMBOL(__irqchip_of_table) = .;				\
 	*(__irqchip_of_table)		  				\
-	*(__irqchip_of_end)
+	*(__irqchip_of_table_end)
 #else
 #define IRQCHIP_OF_MATCH_TABLE()
 #endif

commit 69902c718c0b476e94ed7fccd3cf29ca39fe433a
Author: Vineet Gupta <Vineet.Gupta1@synopsys.com>
Date:   Thu May 1 10:56:44 2014 +0530

    kprobes: Ensure blacklist data is aligned
    
    ARC Linux (not supporting native unaligned access) was failing
    to boot because __start_kprobe_blacklist was not aligned.
    
    This was because per generated vmlinux.lds it was emitted right
    next to .rodata with strings etc hence could be randomly
    unaligned.
    
    Fix that by ensuring a word alignment. While 4 would suffice for
    32bit arches and problem at hand, it is probably better to put 8.
    
    | Path: (null) CPU: 0 PID: 1 Comm: swapper Not tainted
    | 3.15.0-rc3-next-20140430 #2
    | task: 8f044000 ti: 8f01e000 task.ti: 8f01e000
    |
    | [ECR   ]: 0x00230400 => Misaligned r/w from 0x800fb0d3
    | [EFA   ]: 0x800fb0d3
    | [BLINK ]: do_one_initcall+0x86/0x1bc
    | [ERET  ]: init_kprobes+0x52/0x120
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Cc: <torvalds@linux-foundation.org>
    Cc: <rusty@rustcorp.com.au>
    Cc: <rdunlap@infradead.org>
    Cc: <jeremy@goop.org>
    Cc: <arnd@arndb.de>
    Cc: <dl9pf@gmx.de>
    Cc: <sparse@chrisli.org>
    Cc: <anil.s.keshavamurthy@intel.com>
    Cc: <davem@davemloft.net>
    Cc: <ananth@in.ibm.com>
    Cc: <masami.hiramatsu.pt@hitachi.com>
    Cc: <chrisw@sous-sol.org>
    Cc: <akataria@vmware.com>
    Cc: anton Kolesov <Anton.Kolesov@synopsys.com>
    Link: http://lkml.kernel.org/r/5361DB14.7010406@synopsys.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 40ceb3ceba79..8e0204a68c74 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -110,7 +110,8 @@
 #endif
 
 #ifdef CONFIG_KPROBES
-#define KPROBE_BLACKLIST()	VMLINUX_SYMBOL(__start_kprobe_blacklist) = .; \
+#define KPROBE_BLACKLIST()	. = ALIGN(8);				      \
+				VMLINUX_SYMBOL(__start_kprobe_blacklist) = .; \
 				*(_kprobe_blacklist)			      \
 				VMLINUX_SYMBOL(__stop_kprobe_blacklist) = .;
 #else

commit 376e242429bf8539ef39a080ac113c8799840b13
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:17:05 2014 +0900

    kprobes: Introduce NOKPROBE_SYMBOL() macro to maintain kprobes blacklist
    
    Introduce NOKPROBE_SYMBOL() macro which builds a kprobes
    blacklist at kernel build time.
    
    The usage of this macro is similar to EXPORT_SYMBOL(),
    placed after the function definition:
    
      NOKPROBE_SYMBOL(function);
    
    Since this macro will inhibit inlining of static/inline
    functions, this patch also introduces a nokprobe_inline macro
    for static/inline functions. In this case, we must use
    NOKPROBE_SYMBOL() for the inline function caller.
    
    When CONFIG_KPROBES=y, the macro stores the given function
    address in the "_kprobe_blacklist" section.
    
    Since the data structures are not fully initialized by the
    macro (because there is no "size" information),  those
    are re-initialized at boot time by using kallsyms.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Link: http://lkml.kernel.org/r/20140417081705.26341.96719.stgit@ltc230.yrl.intra.hitachi.co.jp
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christopher Li <sparse@chrisli.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jan-Simon Mller <dl9pf@gmx.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-sparse@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 146e4fffd710..40ceb3ceba79 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -109,6 +109,14 @@
 #define BRANCH_PROFILE()
 #endif
 
+#ifdef CONFIG_KPROBES
+#define KPROBE_BLACKLIST()	VMLINUX_SYMBOL(__start_kprobe_blacklist) = .; \
+				*(_kprobe_blacklist)			      \
+				VMLINUX_SYMBOL(__stop_kprobe_blacklist) = .;
+#else
+#define KPROBE_BLACKLIST()
+#endif
+
 #ifdef CONFIG_EVENT_TRACING
 #define FTRACE_EVENTS()	. = ALIGN(8);					\
 			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
@@ -507,6 +515,7 @@
 	*(.init.rodata)							\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
+	KPROBE_BLACKLIST()						\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	RESERVEDMEM_OF_TABLES()						\

commit ff050ad12c551233e546506409c89eb2f640d9f3
Merge: dfc25e4503ae 9233087dc468
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 5 14:19:54 2014 -0700

    Merge tag 'soc-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC specific changes from Arnd Bergmann:
     "Lots of changes specific to one of the SoC families.  Some that stick
      out are:
    
       - mach-qcom gains new features, most importantly SMP support for the
         newer chips (Stephen Boyd, Rohit Vaswani)
       - mvebu gains support for three new SoCs: Armada 375, 380 and 385
         (Thomas Petazzoni and Free-electrons team)
       - SMP support for Rockchips (Heiko Stbner)
       - Lots of i.MX changes (Shawn Guo)
       - Added support for BCM5301x SoC (Hauke Mehrtens)
       - Multiplatform support for Marvell Kirkwood and Dove (Andrew Lunn
         and Sebastian Hesselbarth doing the final part of a long journey)
       - Unify davinci platforms and remove obsolete ones (Sekhar Nori, Arnd
         Bergmann)"
    
    * tag 'soc-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (126 commits)
      ARM: sunxi: Select HAVE_ARM_ARCH_TIMER
      ARM: cache-tauros2: remove ARMv6 code
      ARM: mvebu: don't select CONFIG_NEON
      ARM: davinci: fix DT booting with default defconfig
      ARM: configs: bcm_defconfig: enable bcm590xx regulator support
      ARM: davinci: remove tnetv107x support
      MAINTAINERS: Update ARM STi maintainers
      ARM: restrict BCM_KONA_UART to ARCH_BCM_MOBILE
      ARM: bcm21664: Add board support.
      ARM: sunxi: Add the new watchog compatibles to the reboot code
      ARM: enable ARM_HAS_SG_CHAIN for multiplatform
      ARM: davinci: remove da8xx_omapl_defconfig
      ARM: davinci: da8xx: fix multiple watchdog device registration
      ARM: davinci: add da8xx specific configs to davinci_all_defconfig
      ARM: davinci: enable da8xx build concurrently with older devices
      ARM: BCM5301X: workaround suppress fault
      ARM: BCM5301X: add early debugging support
      ARM: BCM5301X: initial support for the BCM5301X/BCM470X SoCs with ARM CPU
      ARM: mach-bcm: Remove GENERIC_TIME
      ARM: shmobile: APMU: Fix warnings due to improper printk formats
      ...

commit f618c4703a14672d27bc2ca5d132a844363d6f5f
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 28 14:42:49 2014 +0100

    drivers: of: add support for custom reserved memory drivers
    
    Add support for custom reserved memory drivers. Call their init() function
    for each reserved region and prepare for using operations provided by them
    with by the reserved_mem->ops array.
    
    Based on previous code provided by Josh Cartwright <joshc@codeaurora.org>
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Grant Likely <grant.likely@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bc2121fa9132..f10f64fcc815 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -167,6 +167,16 @@
 #define CLK_OF_TABLES()
 #endif
 
+#ifdef CONFIG_OF_RESERVED_MEM
+#define RESERVEDMEM_OF_TABLES()				\
+	. = ALIGN(8);					\
+	VMLINUX_SYMBOL(__reservedmem_of_table) = .;	\
+	*(__reservedmem_of_table)			\
+	*(__reservedmem_of_table_end)
+#else
+#define RESERVEDMEM_OF_TABLES()
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	VMLINUX_SYMBOL(__dtb_start) = .;				\
@@ -490,6 +500,7 @@
 	TRACE_SYSCALLS()						\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
+	RESERVEDMEM_OF_TABLES()						\
 	CLKSRC_OF_TABLES()						\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()

commit 6c3ff8b11a16ec69199ab85b74a5fae6d9c59db7
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Oct 30 18:21:09 2013 -0700

    ARM: Introduce CPU_METHOD_OF_DECLARE() for cpu hotplug/smp
    
    The goal of multi-platform kernels is to remove the need for mach
    directories and machine descriptors. To further that goal,
    introduce CPU_METHOD_OF_DECLARE() to allow cpu hotplug/smp
    support to be separated from the machine descriptors.
    Implementers should specify an enable-method property in their
    cpus node and then implement a matching set of smp_ops in their
    hotplug/smp code, wiring it up with the CPU_METHOD_OF_DECLARE()
    macro. When the kernel is compiled we'll collect all the
    enable-method smp_ops into one section for use at boot.
    
    At boot time we'll look for an enable-method in each cpu node and
    try to match that against all known CPU enable methods in the
    kernel. If there are no enable-methods in the cpu nodes we
    fallback to the cpus node and try to use any enable-method found
    there. If that doesn't work we fall back to the old way of using
    the machine descriptor.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: <devicetree@vger.kernel.org>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Kumar Gala <galak@codeaurora.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bc2121fa9132..bd02ca7a1d55 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -167,6 +167,15 @@
 #define CLK_OF_TABLES()
 #endif
 
+#ifdef CONFIG_SMP
+#define CPU_METHOD_OF_TABLES() . = ALIGN(8);				    \
+			   VMLINUX_SYMBOL(__cpu_method_of_table_begin) = .; \
+			   *(__cpu_method_of_table)			    \
+			   VMLINUX_SYMBOL(__cpu_method_of_table_end) = .;
+#else
+#define CPU_METHOD_OF_TABLES()
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	VMLINUX_SYMBOL(__dtb_start) = .;				\
@@ -491,6 +500,7 @@
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	CLKSRC_OF_TABLES()						\
+	CPU_METHOD_OF_TABLES()						\
 	KERNEL_DTB()							\
 	IRQCHIP_OF_MATCH_TABLE()
 

commit eb3057df732c304622aee77c450761746939a2dc
Author: Frantisek Hrbata <fhrbata@redhat.com>
Date:   Mon Oct 14 18:08:46 2013 +1030

    kernel: add support for init_array constructors
    
    This adds the .init_array section as yet another section with constructors. This
    is needed because gcc could add __gcov_init calls to .init_array or .ctors
    section, depending on gcc (and binutils) version .
    
    v2: - reuse mod->ctors for .init_array section for modules, because gcc uses
          .ctors or .init_array, but not both at the same time
    v3: - fail to load if that does happen somehow.
    
    Signed-off-by: Frantisek Hrbata <fhrbata@redhat.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 83e2c31e8b00..bc2121fa9132 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -473,6 +473,7 @@
 #define KERNEL_CTORS()	. = ALIGN(8);			   \
 			VMLINUX_SYMBOL(__ctors_start) = .; \
 			*(.ctors)			   \
+			*(.init_array)			   \
 			VMLINUX_SYMBOL(__ctors_end) = .;
 #else
 #define KERNEL_CTORS()

commit 102c9323c35a83789ad5ebd3c45fa8fb389add88
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jul 12 17:07:27 2013 -0400

    tracing: Add __tracepoint_string() to export string pointers
    
    There are several tracepoints (mostly in RCU), that reference a string
    pointer and uses the print format of "%s" to display the string that
    exists in the kernel, instead of copying the actual string to the
    ring buffer (saves time and ring buffer space).
    
    But this has an issue with userspace tools that read the binary buffers
    that has the address of the string but has no access to what the string
    itself is. The end result is just output that looks like:
    
     rcu_dyntick:          ffffffff818adeaa 1 0
     rcu_dyntick:          ffffffff818adeb5 0 140000000000000
     rcu_dyntick:          ffffffff818adeb5 0 140000000000000
     rcu_utilization:      ffffffff8184333b
     rcu_utilization:      ffffffff8184333b
    
    The above is pretty useless when read by the userspace tools. Ideally
    we would want something that looks like this:
    
     rcu_dyntick:          Start 1 0
     rcu_dyntick:          End 0 140000000000000
     rcu_dyntick:          Start 140000000000000 0
     rcu_callback:         rcu_preempt rhp=0xffff880037aff710 func=put_cred_rcu 0/4
     rcu_callback:         rcu_preempt rhp=0xffff880078961980 func=file_free_rcu 0/5
     rcu_dyntick:          End 0 1
    
    The trace_printk() which also only stores the address of the string
    format instead of recording the string into the buffer itself, exports
    the mapping of kernel addresses to format strings via the printk_format
    file in the debugfs tracing directory.
    
    The tracepoint strings can use this same method and output the format
    to the same file and the userspace tools will be able to decipher
    the address without any modification.
    
    The tracepoint strings need its own section to save the strings because
    the trace_printk section will cause the trace_printk() buffers to be
    allocated if anything exists within the section. trace_printk() is only
    used for debugging and should never exist in the kernel, we can not use
    the trace_printk sections.
    
    Add a new tracepoint_str section that will also be examined by the output
    of the printk_format file.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 69732d279e8b..83e2c31e8b00 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -122,8 +122,12 @@
 #define TRACE_PRINTKS() VMLINUX_SYMBOL(__start___trace_bprintk_fmt) = .;      \
 			 *(__trace_printk_fmt) /* Trace_printk fmt' pointer */ \
 			 VMLINUX_SYMBOL(__stop___trace_bprintk_fmt) = .;
+#define TRACEPOINT_STR() VMLINUX_SYMBOL(__start___tracepoint_str) = .;	\
+			 *(__tracepoint_str) /* Trace_printk fmt' pointer */ \
+			 VMLINUX_SYMBOL(__stop___tracepoint_str) = .;
 #else
 #define TRACE_PRINTKS()
+#define TRACEPOINT_STR()
 #endif
 
 #ifdef CONFIG_FTRACE_SYSCALLS
@@ -190,7 +194,8 @@
 	VMLINUX_SYMBOL(__stop___verbose) = .;				\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
-	TRACE_PRINTKS()
+	TRACE_PRINTKS()							\
+	TRACEPOINT_STR()
 
 /*
  * Data section helpers

commit 8dce5f3dee21bf976193ddb06426b9727cf5d1a2
Merge: 21884a83b219 e24f6628811e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 7 11:01:19 2013 -0700

    Merge branch 'cpuinit-delete' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull first stage of __cpuinit removal from Paul Gortmaker:
     "The two commits here 1) dummy out all the __cpuinit macros so that we
      no longer generate such sections, and then 2) remove all the section
      processing that we used to do for those sections.
    
      This makes all the __cpuinit and friends no-ops, so that we can remove
      the use cases of it at our leisure.  Expect stage 2, which does the
      tree wide removal sweep at the end of the merge window."
    
    * 'cpuinit-delete' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      modpost: remove all traces of cpuinit/cpuexit sections
      init.h: remove __cpuinit sections from the kernel

commit 2ec3ba69faf301fb599e3651515e808e8efa533e
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Wed Jul 3 15:08:50 2013 -0700

    rapidio: convert switch drivers to modules
    
    Rework RapidIO switch drivers to add an option to build them as loadable
    kernel modules.
    
    This patch removes RapidIO-specific vmlinux section and converts switch
    drivers to be compatible with LDM driver registration method.  To simplify
    registration of device-specific callback routines this patch introduces
    rio_switch_ops data structure.  The sw_sysfs() callback is removed from
    the list of device-specific operations because under the new structure its
    functions can be handled by switch driver's probe() and remove() routines.
    
    If a specific switch device driver is not loaded the RapidIO subsystem
    core will use default standard-based operations to configure a switch.
    Because the current implementation of RapidIO enumeration/discovery method
    relies on availability of device-specific operations for error management,
    switch device drivers must be loaded before the RapidIO
    enumeration/discovery starts.
    
    This patch also moves several common routines from enumeration/discovery
    module into the RapidIO core code to make switch-specific operations
    accessible to all components of RapidIO subsystem.
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Andre van Herk <andre.van.herk@Prodrive.nl>
    Cc: Micha Nelissen <micha.nelissen@Prodrive.nl>
    Cc: Stef van Os <stef.van.os@Prodrive.nl>
    Cc: Jean Delvare <jdelvare@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 4f2737208c42..c74d88baea60 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -275,13 +275,6 @@
 		VMLINUX_SYMBOL(__end_builtin_fw) = .;			\
 	}								\
 									\
-	/* RapidIO route ops */						\
-	.rio_ops        : AT(ADDR(.rio_ops) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start_rio_switch_ops) = .;		\
-		*(.rio_switch_ops)					\
-		VMLINUX_SYMBOL(__end_rio_switch_ops) = .;		\
-	}								\
-									\
 	TRACEDATA							\
 									\
 	/* Kernel symbol table: Normal symbols */			\

commit e24f6628811e2d4531b443684b598f7050932012
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 19:30:48 2013 -0400

    modpost: remove all traces of cpuinit/cpuexit sections
    
    Delete all audit rules that were checking how the .cpuXYZ
    related sections were inter-operating with other __init
    like sections, now that __cpuinit is gone.  Update the linker
    script to not have any knowledge of .cpuinit sections.
    
    [lds.h update courtesy of Ralf Baechle <ralf@linux-mips.org>]
    
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index eb58d2d7d971..5e01bee9d1d3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -184,8 +184,6 @@
 	*(.data..shared_aligned) /* percpu related */			\
 	DEV_KEEP(init.data)						\
 	DEV_KEEP(exit.data)						\
-	CPU_KEEP(init.data)						\
-	CPU_KEEP(exit.data)						\
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
 	*(.data.unlikely)						\
@@ -374,8 +372,6 @@
 		*(.ref.rodata)						\
 		DEV_KEEP(init.rodata)					\
 		DEV_KEEP(exit.rodata)					\
-		CPU_KEEP(init.rodata)					\
-		CPU_KEEP(exit.rodata)					\
 		MEM_KEEP(init.rodata)					\
 		MEM_KEEP(exit.rodata)					\
 	}								\
@@ -418,8 +414,6 @@
 		*(.ref.text)						\
 	DEV_KEEP(init.text)						\
 	DEV_KEEP(exit.text)						\
-	CPU_KEEP(init.text)						\
-	CPU_KEEP(exit.text)						\
 	MEM_KEEP(init.text)						\
 	MEM_KEEP(exit.text)						\
 		*(.text.unlikely)
@@ -504,7 +498,6 @@
 #define INIT_DATA							\
 	*(.init.data)							\
 	DEV_DISCARD(init.data)						\
-	CPU_DISCARD(init.data)						\
 	MEM_DISCARD(init.data)						\
 	KERNEL_CTORS()							\
 	MCOUNT_REC()							\
@@ -512,7 +505,6 @@
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
 	DEV_DISCARD(init.rodata)					\
-	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
 	CLKSRC_OF_TABLES()						\
@@ -522,22 +514,18 @@
 #define INIT_TEXT							\
 	*(.init.text)							\
 	DEV_DISCARD(init.text)						\
-	CPU_DISCARD(init.text)						\
 	MEM_DISCARD(init.text)
 
 #define EXIT_DATA							\
 	*(.exit.data)							\
 	DEV_DISCARD(exit.data)						\
 	DEV_DISCARD(exit.rodata)					\
-	CPU_DISCARD(exit.data)						\
-	CPU_DISCARD(exit.rodata)					\
 	MEM_DISCARD(exit.data)						\
 	MEM_DISCARD(exit.rodata)
 
 #define EXIT_TEXT							\
 	*(.exit.text)							\
 	DEV_DISCARD(exit.text)						\
-	CPU_DISCARD(exit.text)						\
 	MEM_DISCARD(exit.text)
 
 #define EXIT_CALL							\

commit 40b313608ad4ea655addd2ec6cdd106477ae8e15
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue May 21 13:49:35 2013 +1000

    Finally eradicate CONFIG_HOTPLUG
    
    Ever since commit 45f035ab9b8f ("CONFIG_HOTPLUG should be always on"),
    it has been basically impossible to build a kernel with CONFIG_HOTPLUG
    turned off.  Remove all the remaining references to it.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Doug Thompson <dougthompson@xmission.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Mauro Carvalho Chehab <mchehab@redhat.com>
    Acked-by: Hans Verkuil <hans.verkuil@cisco.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index eb58d2d7d971..4f2737208c42 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -68,14 +68,6 @@
  * are handled as text/data or they can be discarded (which
  * often happens at runtime)
  */
-#ifdef CONFIG_HOTPLUG
-#define DEV_KEEP(sec)    *(.dev##sec)
-#define DEV_DISCARD(sec)
-#else
-#define DEV_KEEP(sec)
-#define DEV_DISCARD(sec) *(.dev##sec)
-#endif
-
 #ifdef CONFIG_HOTPLUG_CPU
 #define CPU_KEEP(sec)    *(.cpu##sec)
 #define CPU_DISCARD(sec)
@@ -182,8 +174,6 @@
 	*(.data)							\
 	*(.ref.data)							\
 	*(.data..shared_aligned) /* percpu related */			\
-	DEV_KEEP(init.data)						\
-	DEV_KEEP(exit.data)						\
 	CPU_KEEP(init.data)						\
 	CPU_KEEP(exit.data)						\
 	MEM_KEEP(init.data)						\
@@ -372,8 +362,6 @@
 	/* __*init sections */						\
 	__init_rodata : AT(ADDR(__init_rodata) - LOAD_OFFSET) {		\
 		*(.ref.rodata)						\
-		DEV_KEEP(init.rodata)					\
-		DEV_KEEP(exit.rodata)					\
 		CPU_KEEP(init.rodata)					\
 		CPU_KEEP(exit.rodata)					\
 		MEM_KEEP(init.rodata)					\
@@ -416,8 +404,6 @@
 		*(.text.hot)						\
 		*(.text)						\
 		*(.ref.text)						\
-	DEV_KEEP(init.text)						\
-	DEV_KEEP(exit.text)						\
 	CPU_KEEP(init.text)						\
 	CPU_KEEP(exit.text)						\
 	MEM_KEEP(init.text)						\
@@ -503,7 +489,6 @@
 /* init and exit section handling */
 #define INIT_DATA							\
 	*(.init.data)							\
-	DEV_DISCARD(init.data)						\
 	CPU_DISCARD(init.data)						\
 	MEM_DISCARD(init.data)						\
 	KERNEL_CTORS()							\
@@ -511,7 +496,6 @@
 	*(.init.rodata)							\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
-	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\
 	CLK_OF_TABLES()							\
@@ -521,14 +505,11 @@
 
 #define INIT_TEXT							\
 	*(.init.text)							\
-	DEV_DISCARD(init.text)						\
 	CPU_DISCARD(init.text)						\
 	MEM_DISCARD(init.text)
 
 #define EXIT_DATA							\
 	*(.exit.data)							\
-	DEV_DISCARD(exit.data)						\
-	DEV_DISCARD(exit.rodata)					\
 	CPU_DISCARD(exit.data)						\
 	CPU_DISCARD(exit.rodata)					\
 	MEM_DISCARD(exit.data)						\
@@ -536,7 +517,6 @@
 
 #define EXIT_TEXT							\
 	*(.exit.text)							\
-	DEV_DISCARD(exit.text)						\
 	CPU_DISCARD(exit.text)						\
 	MEM_DISCARD(exit.text)
 

commit b92021b09df70c1609e3547f3d6128dd560be97f
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Fri Mar 15 15:04:17 2013 +1030

    CONFIG_SYMBOL_PREFIX: cleanup.
    
    We have CONFIG_SYMBOL_PREFIX, which three archs define to the string
    "_".  But Al Viro broke this in "consolidate cond_syscall and
    SYSCALL_ALIAS declarations" (in linux-next), and he's not the first to
    do so.
    
    Using CONFIG_SYMBOL_PREFIX is awkward, since we usually just want to
    prefix it so something.  So various places define helpers which are
    defined to nothing if CONFIG_SYMBOL_PREFIX isn't set:
    
    1) include/asm-generic/unistd.h defines __SYMBOL_PREFIX.
    2) include/asm-generic/vmlinux.lds.h defines VMLINUX_SYMBOL(sym)
    3) include/linux/export.h defines MODULE_SYMBOL_PREFIX.
    4) include/linux/kernel.h defines SYMBOL_PREFIX (which differs from #7)
    5) kernel/modsign_certificate.S defines ASM_SYMBOL(sym)
    6) scripts/modpost.c defines MODULE_SYMBOL_PREFIX
    7) scripts/Makefile.lib defines SYMBOL_PREFIX on the commandline if
       CONFIG_SYMBOL_PREFIX is set, so that we have a non-string version
       for pasting.
    
    (arch/h8300/include/asm/linkage.h defines SYMBOL_NAME(), too).
    
    Let's solve this properly:
    1) No more generic prefix, just CONFIG_HAVE_UNDERSCORE_SYMBOL_PREFIX.
    2) Make linux/export.h usable from asm.
    3) Define VMLINUX_SYMBOL() and VMLINUX_SYMBOL_STR().
    4) Make everyone use them.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: James Hogan <james.hogan@imgtec.com>
    Tested-by: James Hogan <james.hogan@imgtec.com> (metag)

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index afa12c7a025c..eb58d2d7d971 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -52,13 +52,7 @@
 #define LOAD_OFFSET 0
 #endif
 
-#ifndef SYMBOL_PREFIX
-#define VMLINUX_SYMBOL(sym) sym
-#else
-#define PASTE2(x,y) x##y
-#define PASTE(x,y) PASTE2(x,y)
-#define VMLINUX_SYMBOL(sym) PASTE(SYMBOL_PREFIX, sym)
-#endif
+#include <linux/export.h>
 
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)

commit b274776c54c320763bc12eb035c0e244f76ccb43
Merge: b24174b0cbbe 3b1209e7994c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 21 14:58:40 2013 -0800

    Merge tag 'cleanup' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC cleanups from Arnd Bergmann:
     "A large number of cleanups, all over the platforms.  This is dominated
      largely by the Samsung platforms (s3c, s5p, exynos) and a few of the
      others moving code out of arch/arm into more appropriate subsystems.
    
      The clocksource and irqchip drivers are now abstracted to the point
      where platforms that are already cleaned up do not need to even
      specify the driver they use, it can all get configured from the device
      tree as we do for normal device drivers.  The clocksource changes
      basically touch every single platform in the process.
    
      We further clean up the use of platform specific header files here,
      with the goal of turning more of the platforms over to being
      "multiplatform" enabled, which implies that they cannot expose their
      headers to architecture independent code any more.
    
      It is expected that no functional changes are part of the cleanup.
      The overall reduction in total code lines is mostly the result of
      removing broken and obsolete code."
    
    * tag 'cleanup' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (133 commits)
      ARM: mvebu: correct gated clock documentation
      ARM: kirkwood: add missing include for nsa310
      ARM: exynos: move exynos4210-combiner to drivers/irqchip
      mfd: db8500-prcmu: update resource passing
      drivers/db8500-cpufreq: delete dangling include
      ARM: at91: remove NEOCORE 926 board
      sunxi: Cleanup the reset code and add meaningful registers defines
      ARM: S3C24XX: header mach/regs-mem.h local
      ARM: S3C24XX: header mach/regs-power.h local
      ARM: S3C24XX: header mach/regs-s3c2412-mem.h local
      ARM: S3C24XX: Remove plat-s3c24xx directory in arch/arm/
      ARM: S3C24XX: transform s3c2443 subirqs into new structure
      ARM: S3C24XX: modify s3c2443 irq init to initialize all irqs
      ARM: S3C24XX: move s3c2443 irq code to irq.c
      ARM: S3C24XX: transform s3c2416 irqs into new structure
      ARM: S3C24XX: modify s3c2416 irq init to initialize all irqs
      ARM: S3C24XX: move s3c2416 irq init to common irq code
      ARM: S3C24XX: Modify s3c_irq_wake to use the hwirq property
      ARM: S3C24XX: Move irq syscore-ops to irq-pm
      clocksource: always define CLOCKSOURCE_OF_DECLARE
      ...

commit f2f6c2556dcc432e50003bc8fa4d62d95906f149
Author: Prashant Gaikwad <pgaikwad@nvidia.com>
Date:   Fri Jan 4 12:30:52 2013 +0530

    clk: add common of_clk_init() function
    
    Modify of_clk_init function so that it will determine which
    driver to initialize based on device tree instead of each driver
    registering to it.
    
    Based on a similar patch for drivers/irqchip by Thomas Petazzoni and
    drivers/clocksource by Stephen Warren.
    
    Signed-off-by: Prashant Gaikwad <pgaikwad@nvidia.com>
    Tested-by: Tony Prisk <linux@prisktech.co.nz>
    Tested-by: Pawel Moll <pawel.moll@arm.com>
    Tested-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Josh Cartwright <josh.cartwright@ni.com>
    Reviewed-by: Josh Cartwright <josh.cartwright@ni.com>
    Acked-by: Maxime Ripard <maxime.ripard@anandra.org>
    Signed-off-by: Mike Turquette <mturquette@linaro.org>
    [mturquette@linaro.org: merge conflict from missing CLKSRC_OF_TABLES()]
    
    Signed-off-by: Mike Turquette <mturquette@linaro.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index d1ea7ce0b4cb..c1fe60ad1540 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -150,6 +150,15 @@
 #endif
 
 
+#ifdef CONFIG_COMMON_CLK
+#define CLK_OF_TABLES() . = ALIGN(8);				\
+			VMLINUX_SYMBOL(__clk_of_table) = .;	\
+			*(__clk_of_table)			\
+			*(__clk_of_table_end)
+#else
+#define CLK_OF_TABLES()
+#endif
+
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
 	VMLINUX_SYMBOL(__dtb_start) = .;				\
@@ -493,6 +502,7 @@
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\
+	CLK_OF_TABLES()							\
 	KERNEL_DTB()
 
 #define INIT_TEXT							\

commit f8060f5446b1f2782f0a8ca9be2d870ea4198aee
Merge: 175dbc1eeadb 9e47b8bf9815
Author: Olof Johansson <olof@lixom.net>
Date:   Mon Jan 14 17:22:00 2013 -0800

    Merge tag 'gic-vic-to-irqchip' of git://sources.calxeda.com/kernel/linux into next/cleanup
    
    From Rob Herring:
    
    Initial irqchip init infrastructure and GIC and VIC clean-ups
    
    This creates irqchip initialization infrastructure from Thomas
    Petazzoni. The VIC and GIC irqchip code is moved to drivers/irqchips
    and adapted to use the new infrastructure. All DT enabled platforms
    using GIC and VIC are converted over to use the new irqchip_init.
    
    * tag 'gic-vic-to-irqchip' of git://sources.calxeda.com/kernel/linux:
      irqchip: Move ARM vic.h to include/linux/irqchip/arm-vic.h
      ARM: picoxcell: use common irqchip_init function
      ARM: spear: use common irqchip_init function
      irqchip: Move ARM VIC to drivers/irqchip
      ARM: samsung: remove unused tick.h
      ARM: remove unneeded vic.h includes
      ARM: remove mach .handle_irq for VIC users
      ARM: VIC: set handle_arch_irq in VIC initialization
      ARM: VIC: shrink down vic.h
      irqchip: Move ARM gic.h to include/linux/irqchip/arm-gic.h
      ARM: use common irqchip_init for GIC init
      irqchip: Move ARM GIC to drivers/irqchip
      ARM: remove mach .handle_irq for GIC users
      ARM: GIC: set handle_arch_irq in GIC initialization
      ARM: GIC: remove direct use of gic_raise_softirq
      ARM: GIC: remove assembly ifdefs from gic.h
      ARM: mach-ux500: use SGI0 to wake up the other core
      arm: add set_handle_irq() to register the parent IRQ controller handler function
      irqchip: add basic infrastructure
      irqchip: add to the directories part of the IRQ subsystem in MAINTAINERS
    
    Fixed up massive merge conflicts with the timer cleanup due to adjacent changes:
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    
    Conflicts:
            arch/arm/mach-bcm/board_bcm.c
            arch/arm/mach-cns3xxx/cns3420vb.c
            arch/arm/mach-ep93xx/adssphere.c
            arch/arm/mach-ep93xx/edb93xx.c
            arch/arm/mach-ep93xx/gesbc9312.c
            arch/arm/mach-ep93xx/micro9.c
            arch/arm/mach-ep93xx/simone.c
            arch/arm/mach-ep93xx/snappercl15.c
            arch/arm/mach-ep93xx/ts72xx.c
            arch/arm/mach-ep93xx/vision_ep9307.c
            arch/arm/mach-highbank/highbank.c
            arch/arm/mach-imx/mach-imx6q.c
            arch/arm/mach-msm/board-dt-8960.c
            arch/arm/mach-netx/nxdb500.c
            arch/arm/mach-netx/nxdkn.c
            arch/arm/mach-netx/nxeb500hmi.c
            arch/arm/mach-nomadik/board-nhk8815.c
            arch/arm/mach-picoxcell/common.c
            arch/arm/mach-realview/realview_eb.c
            arch/arm/mach-realview/realview_pb1176.c
            arch/arm/mach-realview/realview_pb11mp.c
            arch/arm/mach-realview/realview_pba8.c
            arch/arm/mach-realview/realview_pbx.c
            arch/arm/mach-socfpga/socfpga.c
            arch/arm/mach-spear13xx/spear1310.c
            arch/arm/mach-spear13xx/spear1340.c
            arch/arm/mach-spear13xx/spear13xx.c
            arch/arm/mach-spear3xx/spear300.c
            arch/arm/mach-spear3xx/spear310.c
            arch/arm/mach-spear3xx/spear320.c
            arch/arm/mach-spear3xx/spear3xx.c
            arch/arm/mach-spear6xx/spear6xx.c
            arch/arm/mach-tegra/board-dt-tegra20.c
            arch/arm/mach-tegra/board-dt-tegra30.c
            arch/arm/mach-u300/core.c
            arch/arm/mach-ux500/board-mop500.c
            arch/arm/mach-ux500/cpu-db8500.c
            arch/arm/mach-versatile/versatile_ab.c
            arch/arm/mach-versatile/versatile_dt.c
            arch/arm/mach-versatile/versatile_pb.c
            arch/arm/mach-vexpress/v2m.c
            include/asm-generic/vmlinux.lds.h

commit f6e916b82022cba67bdd0ec7df84e2bce2ef3f73
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Tue Nov 20 23:00:52 2012 +0100

    irqchip: add basic infrastructure
    
    With the recent creation of the drivers/irqchip/ directory, it is
    desirable to move irq controller drivers here. At the moment, the only
    driver here is irq-bcm2835, the driver for the irq controller found in
    the ARM BCM2835 SoC, present in Rasberry Pi systems. This irq
    controller driver was exporting its initialization function and its
    irq handling function through a header file in
    <linux/irqchip/bcm2835.h>.
    
    When proposing to also move another irq controller driver in
    drivers/irqchip, Rob Herring raised the very valid point that moving
    things to drivers/irqchip was good in order to remove more stuff from
    arch/arm, but if it means adding gazillions of headers files in
    include/linux/irqchip/, it would not be very nice.
    
    So, upon the suggestion of Rob Herring and Arnd Bergmann, this commit
    introduces a small infrastructure that defines a central
    irqchip_init() function in drivers/irqchip/irqchip.c, which is meant
    to be called as the ->init_irq() callback of ARM platforms. This
    function calls of_irq_init() with an array of match strings and init
    functions generated from a special linker section.
    
    Note that the irq controller driver initialization function is
    responsible for setting the global handle_arch_irq() variable, so that
    ARM platforms no longer have to define the ->handle_irq field in their
    DT_MACHINE structure.
    
    A global header, <linux/irqchip.h> is also added to expose the single
    irqchip_init() function to the reset of the kernel.
    
    A further commit moves the BCM2835 irq controller driver to this new
    small infrastructure, therefore removing the include/linux/irqchip/
    directory.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Stephen Warren <swarren@wwwdotorg.org>
    Reviewed-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    [rob.herring: reword commit message to reflect use of linker sections.]
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index d1ea7ce0b4cb..c80c599897b9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -149,6 +149,15 @@
 #define TRACE_SYSCALLS()
 #endif
 
+#ifdef CONFIG_IRQCHIP
+#define IRQCHIP_OF_MATCH_TABLE()					\
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__irqchip_begin) = .;				\
+	*(__irqchip_of_table)		  				\
+	*(__irqchip_of_end)
+#else
+#define IRQCHIP_OF_MATCH_TABLE()
+#endif
 
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
@@ -493,7 +502,8 @@
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\
-	KERNEL_DTB()
+	KERNEL_DTB()							\
+	IRQCHIP_OF_MATCH_TABLE()
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit ae278a935f086775e8ae31a8ec9f7224ea25ea3c
Author: Stephen Warren <swarren@nvidia.com>
Date:   Mon Nov 19 16:41:20 2012 -0700

    clocksource: add common of_clksrc_init() function
    
    It is desirable to move all clocksource drivers to drivers/clocksource,
    yet each requires its own initialization function. We'd rather not
    pollute <linux/> with a header for each function. Instead, create a
    single of_clksrc_init() function which will determine which clocksource
    driver to initialize based on device tree.
    
    Based on a similar patch for drivers/irqchip by Thomas Petazzoni.
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index d1ea7ce0b4cb..1e744c5a0ffe 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -149,6 +149,14 @@
 #define TRACE_SYSCALLS()
 #endif
 
+#ifdef CONFIG_CLKSRC_OF
+#define CLKSRC_OF_TABLES() . = ALIGN(8);				\
+			   VMLINUX_SYMBOL(__clksrc_of_table) = .;	\
+			   *(__clksrc_of_table)				\
+			   *(__clksrc_of_table_end)
+#else
+#define CLKSRC_OF_TABLES()
+#endif
 
 #define KERNEL_DTB()							\
 	STRUCT_ALIGN();							\
@@ -493,6 +501,7 @@
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\
+	CLKSRC_OF_TABLES()						\
 	KERNEL_DTB()
 
 #define INIT_TEXT							\

commit c87728ca82a1057eb8f84e139bf416ca5488b6fb
Author: David Daney <david.daney@cavium.com>
Date:   Tue Aug 14 11:08:00 2012 -0700

    vmlinux.lds.h: Allow architectures to add sections to the front of .bss
    
    Follow-on MIPS patch will put an object here that needs 64K alignment
    to minimize padding.
    
    For those architectures that don't define BSS_FIRST_SECTIONS, there is
    no change.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-arch@vger.kernel.org,
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Patchwork: https://patchwork.linux-mips.org/patch/4221/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 4e2e1cc505ab..d1ea7ce0b4cb 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -530,9 +530,18 @@
 		*(.scommon)						\
 	}
 
+/*
+ * Allow archectures to redefine BSS_FIRST_SECTIONS to add extra
+ * sections to the front of bss.
+ */
+#ifndef BSS_FIRST_SECTIONS
+#define BSS_FIRST_SECTIONS
+#endif
+
 #define BSS(bss_align)							\
 	. = ALIGN(bss_align);						\
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {				\
+		BSS_FIRST_SECTIONS					\
 		*(.bss..page_aligned)					\
 		*(.dynbss)						\
 		*(.bss)							\

commit 9fd49328fc2a1cbfea542bcbcf004b5c81dc495b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 24 22:32:06 2012 -0400

    ftrace: Sort all function addresses, not just per page
    
    Instead of just sorting the ip's of the functions per ftrace page,
    sort the entire list before adding them to the ftrace pages.
    
    This will allow the bsearch algorithm to be sped up as it can
    also sort by pages, not just records within a page.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8aeadf6b553a..4e2e1cc505ab 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -486,8 +486,8 @@
 	CPU_DISCARD(init.data)						\
 	MEM_DISCARD(init.data)						\
 	KERNEL_CTORS()							\
-	*(.init.rodata)							\
 	MCOUNT_REC()							\
+	*(.init.rodata)							\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
 	DEV_DISCARD(init.rodata)					\

commit 026cee0086fe1df4cf74691cf273062cc769617d
Author: Pawel Moll <pawel.moll@arm.com>
Date:   Mon Mar 26 12:50:51 2012 +1030

    params: <level>_initcall-like kernel parameters
    
    This patch adds a set of macros that can be used to declare
    kernel parameters to be parsed _before_ initcalls at a chosen
    level are executed.  We rename the now-unused "flags" field of
    struct kernel_param as the level.  It's signed, for when we
    use this for early params as well, in future.
    
    Linker macro collating init calls had to be modified in order
    to add additional symbols between levels that are later used
    by the init code to split the calls into blocks.
    
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 798603e8ec38..8aeadf6b553a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -616,30 +616,23 @@
 		*(.init.setup)						\
 		VMLINUX_SYMBOL(__setup_end) = .;
 
-#define INITCALLS							\
-	*(.initcallearly.init)						\
-	VMLINUX_SYMBOL(__early_initcall_end) = .;			\
-  	*(.initcall0.init)						\
-  	*(.initcall0s.init)						\
-  	*(.initcall1.init)						\
-  	*(.initcall1s.init)						\
-  	*(.initcall2.init)						\
-  	*(.initcall2s.init)						\
-  	*(.initcall3.init)						\
-  	*(.initcall3s.init)						\
-  	*(.initcall4.init)						\
-  	*(.initcall4s.init)						\
-  	*(.initcall5.init)						\
-  	*(.initcall5s.init)						\
-	*(.initcallrootfs.init)						\
-  	*(.initcall6.init)						\
-  	*(.initcall6s.init)						\
-  	*(.initcall7.init)						\
-  	*(.initcall7s.init)
+#define INIT_CALLS_LEVEL(level)						\
+		VMLINUX_SYMBOL(__initcall##level##_start) = .;		\
+		*(.initcall##level##.init)				\
+		*(.initcall##level##s.init)				\
 
 #define INIT_CALLS							\
 		VMLINUX_SYMBOL(__initcall_start) = .;			\
-		INITCALLS						\
+		*(.initcallearly.init)					\
+		INIT_CALLS_LEVEL(0)					\
+		INIT_CALLS_LEVEL(1)					\
+		INIT_CALLS_LEVEL(2)					\
+		INIT_CALLS_LEVEL(3)					\
+		INIT_CALLS_LEVEL(4)					\
+		INIT_CALLS_LEVEL(5)					\
+		INIT_CALLS_LEVEL(rootfs)				\
+		INIT_CALLS_LEVEL(6)					\
+		INIT_CALLS_LEVEL(7)					\
 		VMLINUX_SYMBOL(__initcall_end) = .;
 
 #define CON_INITCALL							\

commit 7ccaba5314caf3a2b1052edb3146ccc969b4d466
Author: Jan Beulich <JBeulich@suse.com>
Date:   Fri Mar 23 15:01:52 2012 -0700

    consolidate WARN_...ONCE() static variables
    
    Due to the alignment of following variables, these typically consume
    more than just the single byte that 'bool' requires, and as there are a
    few hundred instances, the cache pollution (not so much the waste of
    memory) sums up.  Put these variables into their own section, outside of
    any half way frequently used memory range.
    
    Do the same also to the __warned variable of rcu_lockdep_assert().
    (Don't, however, include the ones used by printk_once() and alike, as
    they can potentially be hot.)
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b5e2e4c6b017..798603e8ec38 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -167,6 +167,7 @@
 	CPU_KEEP(exit.data)						\
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
+	*(.data.unlikely)						\
 	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
 	/* implement dynamic printk debug */				\

commit bc74ee976959616e3c1cc1341383bf2316dd4096
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Sun Sep 4 03:18:37 2011 +0400

    m68k: Finally remove leftover markers sections
    
    Markers have removed already twice:
    
    1: fc5377668c3d808e1d53c4aee152c836f55c3490
    2: eb878b3bc0349344dbf70c51bf01fc734d5cf2d3
    
    But a little bit is still here.
    
    Signed-off-by: Tkhai Kirill <tkhai@yandex.ru>
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index db22d136ad08..b5e2e4c6b017 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -222,7 +222,6 @@
 		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\
 		*(__tracepoints_ptrs)	/* Tracepoints: pointer array */\
 		VMLINUX_SYMBOL(__stop___tracepoints_ptrs) = .;		\
-		*(__markers_strings)	/* Markers: strings */		\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
 	}								\
 									\

commit 5129df03d0c44b2d5a5f9d7d52f3b079706b9a8f
Merge: 4d429480352c 6988f20fe04e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 24 11:53:42 2011 -0700

    Merge branch 'for-2.6.40' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.40' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu:
      percpu: Unify input section names
      percpu: Avoid extra NOP in percpu_cmpxchg16b_double
      percpu: Cast away printk format warning
      percpu: Always align percpu output section to PAGE_SIZE
    
    Fix up fairly trivial conflict in arch/x86/include/asm/percpu.h as per Tejun

commit 6988f20fe04e9ef3aea488cb8ab57fbeb78e12f0
Merge: 0415b00d175e 6ea0c34dac89
Author: Tejun Heo <tj@kernel.org>
Date:   Tue May 24 09:59:36 2011 +0200

    Merge branch 'fixes-2.6.39' into for-2.6.40

commit df48d8716eab9608fe93924e4ae06ff110e8674f
Merge: acd30250d7d0 29510ec3b626
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 17:36:08 2011 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (107 commits)
      perf stat: Add more cache-miss percentage printouts
      perf stat: Add -d -d and -d -d -d options to show more CPU events
      ftrace/kbuild: Add recordmcount files to force full build
      ftrace: Add self-tests for multiple function trace users
      ftrace: Modify ftrace_set_filter/notrace to take ops
      ftrace: Allow dynamically allocated function tracers
      ftrace: Implement separate user function filtering
      ftrace: Free hash with call_rcu_sched()
      ftrace: Have global_ops store the functions that are to be traced
      ftrace: Add ops parameter to ftrace_startup/shutdown functions
      ftrace: Add enabled_functions file
      ftrace: Use counters to enable functions to trace
      ftrace: Separate hash allocation and assignment
      ftrace: Create a global_ops to hold the filter and notrace hashes
      ftrace: Use hash instead for FTRACE_FL_FILTER
      ftrace: Replace FTRACE_FL_NOTRACE flag with a hash of ignored functions
      perf bench, x86: Add alternatives-asm.h wrapper
      x86, 64-bit: Fix copy_[to/from]_user() checks for the userspace address limit
      x86, mem: memset_64.S: Optimize memset by enhanced REP MOVSB/STOSB
      x86, mem: memmove_64.S: Optimize memmove by enhanced REP MOVSB/STOSB
      ...

commit f02e8a6596b7dc9b2171f7ff5654039ef0950cdc
Author: Alessio Igor Bogani <abogani@kernel.org>
Date:   Thu Apr 14 14:59:39 2011 +0200

    module: Sort exported symbols
    
    This patch places every exported symbol in its own section
    (i.e. "___ksymtab+printk").  Thus the linker will use its SORT() directive
    to sort and finally merge all symbol in the right and final section
    (i.e. "__ksymtab").
    
    The symbol prefixed archs use an underscore as prefix for symbols.
    To avoid collision we use a different character to create the temporary
    section names.
    
    This work was supported by a hardware donation from the CE Linux Forum.
    
    Signed-off-by: Alessio Igor Bogani <abogani@kernel.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au> (folded in '+' fixup)
    Tested-by: Dirk Behme <dirk.behme@googlemail.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bd297a20ab98..b27445e00b6c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -274,70 +274,70 @@
 	/* Kernel symbol table: Normal symbols */			\
 	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___ksymtab) = .;			\
-		*(__ksymtab)						\
+		*(SORT(___ksymtab+*))					\
 		VMLINUX_SYMBOL(__stop___ksymtab) = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__ksymtab_gpl     : AT(ADDR(__ksymtab_gpl) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___ksymtab_gpl) = .;		\
-		*(__ksymtab_gpl)					\
+		*(SORT(___ksymtab_gpl+*))				\
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__ksymtab_unused  : AT(ADDR(__ksymtab_unused) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___ksymtab_unused) = .;		\
-		*(__ksymtab_unused)					\
+		*(SORT(___ksymtab_unused+*))				\
 		VMLINUX_SYMBOL(__stop___ksymtab_unused) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__ksymtab_unused_gpl : AT(ADDR(__ksymtab_unused_gpl) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___ksymtab_unused_gpl) = .;	\
-		*(__ksymtab_unused_gpl)					\
+		*(SORT(___ksymtab_unused_gpl+*))			\
 		VMLINUX_SYMBOL(__stop___ksymtab_unused_gpl) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__ksymtab_gpl_future : AT(ADDR(__ksymtab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___ksymtab_gpl_future) = .;	\
-		*(__ksymtab_gpl_future)					\
+		*(SORT(___ksymtab_gpl_future+*))			\
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl_future) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: Normal symbols */			\
 	__kcrctab         : AT(ADDR(__kcrctab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___kcrctab) = .;			\
-		*(__kcrctab)						\
+		*(SORT(___kcrctab+*))					\
 		VMLINUX_SYMBOL(__stop___kcrctab) = .;			\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only symbols */			\
 	__kcrctab_gpl     : AT(ADDR(__kcrctab_gpl) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___kcrctab_gpl) = .;		\
-		*(__kcrctab_gpl)					\
+		*(SORT(___kcrctab_gpl+*))				\
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: Normal unused symbols */		\
 	__kcrctab_unused  : AT(ADDR(__kcrctab_unused) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start___kcrctab_unused) = .;		\
-		*(__kcrctab_unused)					\
+		*(SORT(___kcrctab_unused+*))				\
 		VMLINUX_SYMBOL(__stop___kcrctab_unused) = .;		\
 	}								\
 									\
 	/* Kernel symbol table: GPL-only unused symbols */		\
 	__kcrctab_unused_gpl : AT(ADDR(__kcrctab_unused_gpl) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___kcrctab_unused_gpl) = .;	\
-		*(__kcrctab_unused_gpl)					\
+		*(SORT(___kcrctab_unused_gpl+*))			\
 		VMLINUX_SYMBOL(__stop___kcrctab_unused_gpl) = .;	\
 	}								\
 									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__kcrctab_gpl_future : AT(ADDR(__kcrctab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___kcrctab_gpl_future) = .;	\
-		*(__kcrctab_gpl_future)					\
+		*(SORT(___kcrctab_gpl_future+*))			\
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl_future) = .;	\
 	}								\
 									\

commit 32673822e440eb92eb334631eb0a199d0c532d13
Merge: fa7b69475a6c 5373db886b79
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 27 10:38:30 2011 +0200

    Merge branch 'tip/perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/core
    
    Conflicts:
            include/linux/perf_event.h
    
    Merge reason: pick up the latest jump-label enhancements, they are cooked ready.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d430d3d7e646eb1eac2bb4aa244a644312e67c76
Author: Jason Baron <jbaron@redhat.com>
Date:   Wed Mar 16 17:29:47 2011 -0400

    jump label: Introduce static_branch() interface
    
    Introduce:
    
    static __always_inline bool static_branch(struct jump_label_key *key);
    
    instead of the old JUMP_LABEL(key, label) macro.
    
    In this way, jump labels become really easy to use:
    
    Define:
    
            struct jump_label_key jump_key;
    
    Can be used as:
    
            if (static_branch(&jump_key))
                    do unlikely code
    
    enable/disale via:
    
            jump_label_inc(&jump_key);
            jump_label_dec(&jump_key);
    
    that's it!
    
    For the jump labels disabled case, the static_branch() becomes an
    atomic_read(), and jump_label_inc()/dec() are simply atomic_inc(),
    atomic_dec() operations. We show testing results for this change below.
    
    Thanks to H. Peter Anvin for suggesting the 'static_branch()' construct.
    
    Since we now require a 'struct jump_label_key *key', we can store a pointer into
    the jump table addresses. In this way, we can enable/disable jump labels, in
    basically constant time. This change allows us to completely remove the previous
    hashtable scheme. Thanks to Peter Zijlstra for this re-write.
    
    Testing:
    
    I ran a series of 'tbench 20' runs 5 times (with reboots) for 3
    configurations, where tracepoints were disabled.
    
    jump label configured in
    avg: 815.6
    
    jump label *not* configured in (using atomic reads)
    avg: 800.1
    
    jump label *not* configured in (regular reads)
    avg: 803.4
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20110316212947.GA8792@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Suggested-by: H. Peter Anvin <hpa@linux.intel.com>
    Tested-by: David Daney <ddaney@caviumnetworks.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32c45e5fe0ab..79522166d7f1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -170,6 +170,10 @@
 	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
 	/* implement dynamic printk debug */				\
+	. = ALIGN(8);                                                   \
+	VMLINUX_SYMBOL(__start___jump_table) = .;                       \
+	*(__jump_table)                                                 \
+	VMLINUX_SYMBOL(__stop___jump_table) = .;                        \
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___verbose) = .;                          \
 	*(__verbose)                                                    \
@@ -228,8 +232,6 @@
 									\
 	BUG_TABLE							\
 									\
-	JUMP_TABLE							\
-									\
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
@@ -589,14 +591,6 @@
 #define BUG_TABLE
 #endif
 
-#define JUMP_TABLE							\
-	. = ALIGN(8);							\
-	__jump_table : AT(ADDR(__jump_table) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start___jump_table) = .;		\
-		*(__jump_table)						\
-		VMLINUX_SYMBOL(__stop___jump_table) = .;		\
-	}
-
 #ifdef CONFIG_PM_TRACE
 #define TRACEDATA							\
 	. = ALIGN(4);							\

commit 6ea0c34dac89611126455537552cffe6c7e832ad
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Mon Apr 4 01:41:32 2011 +0200

    percpu: Unify input section names
    
    The two percpu helper macros have the section names duplicated.  So create
    a new define to merge the two.  This also allows arches who need to link
    things more directly themselves to avoid duplicating the input sections in
    their linker script.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32c45e5fe0ab..bf90fbc6688b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -687,6 +687,28 @@
 	*(.discard.*)							\
 	}
 
+/**
+ * PERCPU_INPUT - the percpu input sections
+ * @cacheline: cacheline size
+ *
+ * The core percpu section names and core symbols which do not rely
+ * directly upon load addresses.
+ *
+ * @cacheline is used to align subsections to avoid false cacheline
+ * sharing between subsections for different purposes.
+ */
+#define PERCPU_INPUT(cacheline)						\
+	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
+	*(.data..percpu..first)						\
+	. = ALIGN(PAGE_SIZE);						\
+	*(.data..percpu..page_aligned)					\
+	. = ALIGN(cacheline);						\
+	*(.data..percpu..readmostly)					\
+	. = ALIGN(cacheline);						\
+	*(.data..percpu)						\
+	*(.data..percpu..shared_aligned)				\
+	VMLINUX_SYMBOL(__per_cpu_end) = .;
+
 /**
  * PERCPU_VADDR - define output section for percpu area
  * @cacheline: cacheline size
@@ -715,16 +737,7 @@
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
 	.data..percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
 				- LOAD_OFFSET) {			\
-		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
-		*(.data..percpu..first)					\
-		. = ALIGN(PAGE_SIZE);					\
-		*(.data..percpu..page_aligned)				\
-		. = ALIGN(cacheline);					\
-		*(.data..percpu..readmostly)				\
-		. = ALIGN(cacheline);					\
-		*(.data..percpu)					\
-		*(.data..percpu..shared_aligned)			\
-		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
+		PERCPU_INPUT(cacheline)					\
 	} phdr								\
 	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data..percpu);
 
@@ -745,16 +758,7 @@
 	. = ALIGN(align);						\
 	.data..percpu	: AT(ADDR(.data..percpu) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
-		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
-		*(.data..percpu..first)					\
-		. = ALIGN(PAGE_SIZE);					\
-		*(.data..percpu..page_aligned)				\
-		. = ALIGN(cacheline);					\
-		*(.data..percpu..readmostly)				\
-		. = ALIGN(cacheline);					\
-		*(.data..percpu)					\
-		*(.data..percpu..shared_aligned)			\
-		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
+		PERCPU_INPUT(cacheline)					\
 	}
 
 

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32c45e5fe0ab..bd297a20ab98 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -773,7 +773,7 @@
  * the sections that has this restriction (or similar)
  * is located before the ones requiring PAGE_SIZE alignment.
  * NOSAVE_DATA starts and ends with a PAGE_SIZE alignment which
- * matches the requirment of PAGE_ALIGNED_DATA.
+ * matches the requirement of PAGE_ALIGNED_DATA.
  *
  * use 0 as page_align if page_aligned data is not used */
 #define RW_DATA_SECTION(cacheline, pagealigned, inittask)		\

commit 0415b00d175e0d8945e6785aad21b5f157976ce0
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 24 18:50:09 2011 +0100

    percpu: Always align percpu output section to PAGE_SIZE
    
    Percpu allocator honors alignment request upto PAGE_SIZE and both the
    percpu addresses in the percpu address space and the translated kernel
    addresses should be aligned accordingly.  The calculation of the
    former depends on the alignment of percpu output section in the kernel
    image.
    
    The linker script macros PERCPU_VADDR() and PERCPU() are used to
    define this output section and the latter takes @align parameter.
    Several architectures are using @align smaller than PAGE_SIZE breaking
    percpu memory alignment.
    
    This patch removes @align parameter from PERCPU(), renames it to
    PERCPU_SECTION() and makes it always align to PAGE_SIZE.  While at it,
    add PCPU_SETUP_BUG_ON() checks such that alignment problems are
    reliably detected and remove percpu alignment comment recently added
    in workqueue.c as the condition would trigger BUG way before reaching
    there.
    
    For um, this patch raises the alignment of percpu area.  As the area
    is in .init, there shouldn't be any noticeable difference.
    
    This problem was discovered by David Howells while debugging boot
    failure on mn10300.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Cc: uclinux-dist-devel@blackfin.uclinux.org
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: user-mode-linux-devel@lists.sourceforge.net

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32c45e5fe0ab..f301cea5ca2d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -15,7 +15,7 @@
  *	HEAD_TEXT_SECTION
  *	INIT_TEXT_SECTION(PAGE_SIZE)
  *	INIT_DATA_SECTION(...)
- *	PERCPU(CACHELINE_SIZE, PAGE_SIZE)
+ *	PERCPU_SECTION(CACHELINE_SIZE)
  *	__init_end = .;
  *
  *	_stext = .;
@@ -709,7 +709,7 @@
  *
  * Note that this macros defines __per_cpu_load as an absolute symbol.
  * If there is no need to put the percpu section at a predetermined
- * address, use PERCPU().
+ * address, use PERCPU_SECTION.
  */
 #define PERCPU_VADDR(cacheline, vaddr, phdr)				\
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
@@ -729,20 +729,19 @@
 	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data..percpu);
 
 /**
- * PERCPU - define output section for percpu area, simple version
+ * PERCPU_SECTION - define output section for percpu area, simple version
  * @cacheline: cacheline size
- * @align: required alignment
  *
- * Align to @align and outputs output section for percpu area.  This macro
- * doesn't manipulate @vaddr or @phdr and __per_cpu_load and
+ * Align to PAGE_SIZE and outputs output section for percpu area.  This
+ * macro doesn't manipulate @vaddr or @phdr and __per_cpu_load and
  * __per_cpu_start will be identical.
  *
- * This macro is equivalent to ALIGN(@align); PERCPU_VADDR(@cacheline,,)
+ * This macro is equivalent to ALIGN(PAGE_SIZE); PERCPU_VADDR(@cacheline,,)
  * except that __per_cpu_load is defined as a relative symbol against
  * .data..percpu which is required for relocatable x86_32 configuration.
  */
-#define PERCPU(cacheline, align)					\
-	. = ALIGN(align);						\
+#define PERCPU_SECTION(cacheline)					\
+	. = ALIGN(PAGE_SIZE);						\
 	.data..percpu	: AT(ADDR(.data..percpu) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\

commit 79d8a8f736151b12129984b1250fd708440e742c
Merge: bd2895eeade5 b9ec40af0e18
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 08:22:41 2011 -0700

    Merge branch 'for-2.6.39' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.39' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu:
      percpu, x86: Add arch-specific this_cpu_cmpxchg_double() support
      percpu: Generic support for this_cpu_cmpxchg_double()
      alpha: use L1_CACHE_BYTES for cacheline size in the linker script
      percpu: align percpu readmostly subsection to cacheline
    
    Fix up trivial conflict in arch/x86/kernel/vmlinux.lds.S due to the
    percpu alignment having changed ("x86: Reduce back the alignment of the
    per-CPU data section")

commit ea7145477a461e09d8d194cac4b996dc4f449107
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Mar 7 19:10:39 2011 +0100

    x86: Separate out entry text section
    
    Put x86 entry code into a separate link section: .entry.text.
    
    Separating the entry text section seems to have performance
    benefits - caused by more efficient instruction cache usage.
    
    Running hackbench with perf stat --repeat showed that the change
    compresses the icache footprint. The icache load miss rate went
    down by about 15%:
    
     before patch:
             19417627  L1-icache-load-misses      ( +-   0.147% )
    
     after patch:
             16490788  L1-icache-load-misses      ( +-   0.180% )
    
    The motivation of the patch was to fix a particular kprobes
    bug that relates to the entry text section, the performance
    advantage was discovered accidentally.
    
    Whole perf output follows:
    
     - results for current tip tree:
    
      Performance counter stats for './hackbench/hackbench 10' (500 runs):
    
             19417627  L1-icache-load-misses      ( +-   0.147% )
           2676914223  instructions             #      0.497 IPC     ( +- 0.079% )
           5389516026  cycles                     ( +-   0.144% )
    
          0.206267711  seconds time elapsed   ( +-   0.138% )
    
     - results for current tip tree with the patch applied:
    
      Performance counter stats for './hackbench/hackbench 10' (500 runs):
    
             16490788  L1-icache-load-misses      ( +-   0.180% )
           2717734941  instructions             #      0.502 IPC     ( +- 0.079% )
           5414756975  cycles                     ( +-   0.148% )
    
          0.206747566  seconds time elapsed   ( +-   0.137% )
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: masami.hiramatsu.pt@hitachi.com
    Cc: ananth@in.ibm.com
    Cc: davem@davemloft.net
    Cc: 2nddept-manager@sdl.hitachi.co.jp
    LKML-Reference: <20110307181039.GB15197@jolsa.redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index fe77e3395b40..906c3ceca9a2 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -424,6 +424,12 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
+#define ENTRY_TEXT							\
+		ALIGN_FUNCTION();					\
+		VMLINUX_SYMBOL(__entry_text_start) = .;			\
+		*(.entry.text)						\
+		VMLINUX_SYMBOL(__entry_text_end) = .;
+
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 #define IRQENTRY_TEXT							\
 		ALIGN_FUNCTION();					\

commit 3d56e331b6537671c66f1b510bed0f1e0331dfc8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 2 17:06:09 2011 -0500

    tracing: Replace syscall_meta_data struct array with pointer array
    
    Currently the syscall_meta structures for the syscall tracepoints are
    placed in the __syscall_metadata section, and at link time, the linker
    makes one large array of all these syscall metadata structures. On boot
    up, this array is read (much like the initcall sections) and the syscall
    data is processed.
    
    The problem is that there is no guarantee that gcc will place complex
    structures nicely together in an array format. Two structures in the
    same file may be placed awkwardly, because gcc has no clue that they
    are suppose to be in an array.
    
    A hack was used previous to force the alignment to 4, to pack the
    structures together. But this caused alignment issues with other
    architectures (sparc).
    
    Instead of packing the structures into an array, the structures' addresses
    are now put into the __syscall_metadata section. As pointers are always the
    natural alignment, gcc should always pack them tightly together
    (otherwise initcall, extable, etc would also fail).
    
    By having the pointers to the structures in the section, we can still
    iterate the trace_events without causing unnecessary alignment problems
    with other architectures, or depending on the current behaviour of
    gcc that will likely change in the future just to tick us kernel developers
    off a little more.
    
    The __syscall_metadata section is also moved into the .init.data section
    as it is now only needed at boot up.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 57b1b6811b61..fe77e3395b40 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -141,7 +141,8 @@
 #endif
 
 #ifdef CONFIG_FTRACE_SYSCALLS
-#define TRACE_SYSCALLS() VMLINUX_SYMBOL(__start_syscalls_metadata) = .;	\
+#define TRACE_SYSCALLS() . = ALIGN(8);					\
+			 VMLINUX_SYMBOL(__start_syscalls_metadata) = .;	\
 			 *(__syscalls_metadata)				\
 			 VMLINUX_SYMBOL(__stop_syscalls_metadata) = .;
 #else
@@ -175,10 +176,7 @@
 	VMLINUX_SYMBOL(__stop___verbose) = .;				\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
-	TRACE_PRINTKS()							\
-									\
-	STRUCT_ALIGN();							\
-	TRACE_SYSCALLS()
+	TRACE_PRINTKS()
 
 /*
  * Data section helpers
@@ -483,6 +481,7 @@
 	*(.init.rodata)							\
 	MCOUNT_REC()							\
 	FTRACE_EVENTS()							\
+	TRACE_SYSCALLS()						\
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\

commit 654986462939cd7ec18f276c6379a334dac106a7
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Wed Jan 26 17:26:22 2011 -0500

    tracepoints: Fix section alignment using pointer array
    
    Make the tracepoints more robust, making them solid enough to handle compiler
    changes by not relying on anything based on compiler-specific behavior with
    respect to structure alignment. Implement an approach proposed by David Miller:
    use an array of const pointers to refer to the individual structures, and export
    this pointer array through the linker script rather than the structures per se.
    It will consume 32 extra bytes per tracepoint (24 for structure padding and 8
    for the pointers), but are less likely to break due to compiler changes.
    
    History:
    
    commit 7e066fb8 tracepoints: add DECLARE_TRACE() and DEFINE_TRACE()
    added the aligned(32) type and variable attribute to the tracepoint structures
    to deal with gcc happily aligning statically defined structures on 32-byte
    multiples.
    
    One attempt was to use a 8-byte alignment for tracepoint structures by applying
    both the variable and type attribute to tracepoint structures definitions and
    declarations. It worked fine with gcc 4.5.1, but broke with gcc 4.4.4 and 4.4.5.
    
    The reason is that the "aligned" attribute only specify the _minimum_ alignment
    for a structure, leaving both the compiler and the linker free to align on
    larger multiples. Because tracepoint.c expects the structures to be placed as an
    array within each section, up-alignment cause NULL-pointer exceptions due to the
    extra unexpected padding.
    
    (this patch applies on top of -tip)
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    LKML-Reference: <20110126222622.GA10794@Krystal>
    CC: Frederic Weisbecker <fweisbec@gmail.com>
    CC: Ingo Molnar <mingo@elte.hu>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f53708be95eb..57b1b6811b61 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -166,10 +166,8 @@
 	CPU_KEEP(exit.data)						\
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
-	. = ALIGN(32);							\
-	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
+	STRUCT_ALIGN();							\
 	*(__tracepoints)						\
-	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
 	/* implement dynamic printk debug */				\
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___verbose) = .;                          \
@@ -218,6 +216,10 @@
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
+		. = ALIGN(8);						\
+		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\
+		*(__tracepoints_ptrs)	/* Tracepoints: pointer array */\
+		VMLINUX_SYMBOL(__stop___tracepoints_ptrs) = .;		\
 		*(__markers_strings)	/* Markers: strings */		\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
 	}								\

commit e4a9ea5ee7c8812a7bf0c3fb725ceeaa3d4c2fcc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 09:15:30 2011 -0500

    tracing: Replace trace_event struct array with pointer array
    
    Currently the trace_event structures are placed in the _ftrace_events
    section, and at link time, the linker makes one large array of all
    the trace_event structures. On boot up, this array is read (much like
    the initcall sections) and the events are processed.
    
    The problem is that there is no guarantee that gcc will place complex
    structures nicely together in an array format. Two structures in the
    same file may be placed awkwardly, because gcc has no clue that they
    are suppose to be in an array.
    
    A hack was used previous to force the alignment to 4, to pack the
    structures together. But this caused alignment issues with other
    architectures (sparc).
    
    Instead of packing the structures into an array, the structures' addresses
    are now put into the _ftrace_event section. As pointers are always the
    natural alignment, gcc should always pack them tightly together
    (otherwise initcall, extable, etc would also fail).
    
    By having the pointers to the structures in the section, we can still
    iterate the trace_events without causing unnecessary alignment problems
    with other architectures, or depending on the current behaviour of
    gcc that will likely change in the future just to tick us kernel developers
    off a little more.
    
    The _ftrace_event section is also moved into the .init.data section
    as it is now only needed at boot up.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6ebb81030d2d..f53708be95eb 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -124,7 +124,8 @@
 #endif
 
 #ifdef CONFIG_EVENT_TRACING
-#define FTRACE_EVENTS()	VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
+#define FTRACE_EVENTS()	. = ALIGN(8);					\
+			VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
 			*(_ftrace_events)				\
 			VMLINUX_SYMBOL(__stop_ftrace_events) = .;
 #else
@@ -179,9 +180,6 @@
 	TRACE_PRINTKS()							\
 									\
 	STRUCT_ALIGN();							\
-	FTRACE_EVENTS()							\
-									\
-	STRUCT_ALIGN();							\
 	TRACE_SYSCALLS()
 
 /*
@@ -482,6 +480,7 @@
 	KERNEL_CTORS()							\
 	*(.init.rodata)							\
 	MCOUNT_REC()							\
+	FTRACE_EVENTS()							\
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)					\

commit 19df0c2fef010e94e90df514aaf4e73f6b80145c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 25 14:26:50 2011 +0100

    percpu: align percpu readmostly subsection to cacheline
    
    Currently percpu readmostly subsection may share cachelines with other
    percpu subsections which may result in unnecessary cacheline bounce
    and performance degradation.
    
    This patch adds @cacheline parameter to PERCPU() and PERCPU_VADDR()
    linker macros, makes each arch linker scripts specify its cacheline
    size and use it to align percpu subsections.
    
    This is based on Shaohua's x86 only patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Shaohua Li <shaohua.li@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6ebb81030d2d..439df587c12c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -15,7 +15,7 @@
  *	HEAD_TEXT_SECTION
  *	INIT_TEXT_SECTION(PAGE_SIZE)
  *	INIT_DATA_SECTION(...)
- *	PERCPU(PAGE_SIZE)
+ *	PERCPU(CACHELINE_SIZE, PAGE_SIZE)
  *	__init_end = .;
  *
  *	_stext = .;
@@ -683,13 +683,18 @@
 
 /**
  * PERCPU_VADDR - define output section for percpu area
+ * @cacheline: cacheline size
  * @vaddr: explicit base address (optional)
  * @phdr: destination PHDR (optional)
  *
- * Macro which expands to output section for percpu area.  If @vaddr
- * is not blank, it specifies explicit base address and all percpu
- * symbols will be offset from the given address.  If blank, @vaddr
- * always equals @laddr + LOAD_OFFSET.
+ * Macro which expands to output section for percpu area.
+ *
+ * @cacheline is used to align subsections to avoid false cacheline
+ * sharing between subsections for different purposes.
+ *
+ * If @vaddr is not blank, it specifies explicit base address and all
+ * percpu symbols will be offset from the given address.  If blank,
+ * @vaddr always equals @laddr + LOAD_OFFSET.
  *
  * @phdr defines the output PHDR to use if not blank.  Be warned that
  * output PHDR is sticky.  If @phdr is specified, the next output
@@ -700,7 +705,7 @@
  * If there is no need to put the percpu section at a predetermined
  * address, use PERCPU().
  */
-#define PERCPU_VADDR(vaddr, phdr)					\
+#define PERCPU_VADDR(cacheline, vaddr, phdr)				\
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
 	.data..percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
 				- LOAD_OFFSET) {			\
@@ -708,7 +713,9 @@
 		*(.data..percpu..first)					\
 		. = ALIGN(PAGE_SIZE);					\
 		*(.data..percpu..page_aligned)				\
+		. = ALIGN(cacheline);					\
 		*(.data..percpu..readmostly)				\
+		. = ALIGN(cacheline);					\
 		*(.data..percpu)					\
 		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
@@ -717,18 +724,18 @@
 
 /**
  * PERCPU - define output section for percpu area, simple version
+ * @cacheline: cacheline size
  * @align: required alignment
  *
- * Align to @align and outputs output section for percpu area.  This
- * macro doesn't maniuplate @vaddr or @phdr and __per_cpu_load and
+ * Align to @align and outputs output section for percpu area.  This macro
+ * doesn't manipulate @vaddr or @phdr and __per_cpu_load and
  * __per_cpu_start will be identical.
  *
- * This macro is equivalent to ALIGN(align); PERCPU_VADDR( , ) except
- * that __per_cpu_load is defined as a relative symbol against
- * .data..percpu which is required for relocatable x86_32
- * configuration.
+ * This macro is equivalent to ALIGN(@align); PERCPU_VADDR(@cacheline,,)
+ * except that __per_cpu_load is defined as a relative symbol against
+ * .data..percpu which is required for relocatable x86_32 configuration.
  */
-#define PERCPU(align)							\
+#define PERCPU(cacheline, align)					\
 	. = ALIGN(align);						\
 	.data..percpu	: AT(ADDR(.data..percpu) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
@@ -736,7 +743,9 @@
 		*(.data..percpu..first)					\
 		. = ALIGN(PAGE_SIZE);					\
 		*(.data..percpu..page_aligned)				\
+		. = ALIGN(cacheline);					\
 		*(.data..percpu..readmostly)				\
+		. = ALIGN(cacheline);					\
 		*(.data..percpu)					\
 		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\

commit e94965ed5beb23c6fabf7ed31f625e66d7ff28de
Author: Dmitry Torokhov <dtor@vmware.com>
Date:   Wed Dec 15 14:00:19 2010 -0800

    module: show version information for built-in modules in sysfs
    
    Currently only drivers that are built as modules have their versions
    shown in /sys/module/<module_name>/version, but this information might
    also be useful for built-in drivers as well. This especially important
    for drivers that do not define any parameters - such drivers, if
    built-in, are completely invisible from userspace.
    
    This patch changes MODULE_VERSION() macro so that in case when we are
    compiling built-in module, version information is stored in a separate
    section. Kernel then uses this data to create 'version' sysfs attribute
    in the same fashion it creates attributes for module parameters.
    
    Signed-off-by: Dmitry Torokhov <dtor@vmware.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 68649336c4ad..6ebb81030d2d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -364,6 +364,13 @@
 		VMLINUX_SYMBOL(__start___param) = .;			\
 		*(__param)						\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
+	}								\
+									\
+	/* Built-in module versions. */					\
+	__modver : AT(ADDR(__modver) - LOAD_OFFSET) {			\
+		VMLINUX_SYMBOL(__start___modver) = .;			\
+		*(__modver)						\
+		VMLINUX_SYMBOL(__stop___modver) = .;			\
 		. = ALIGN((align));					\
 		VMLINUX_SYMBOL(__end_rodata) = .;			\
 	}								\

commit 8369744fc4418743d3d84a8490d576e3dbf01594
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Wed Jan 12 16:59:38 2011 -0800

    include/asm-generic/vmlinux.lds.h: make readmostly section correctly align
    
    The readmostly section should end at a cacheline aligned address,
    otherwise the last several data might share cachline with other data and
    make the readmostly data still have cache bounce.
    
    For example, in ia64, secpath_cachep is the last readmostly data, and it
    shares cacheline with init_uts_ns.
    
    a000000100e80480 d secpath_cachep
    a000000100e80488 D init_uts_ns
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 05cbad03c5ab..68649336c4ad 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -200,7 +200,8 @@
 
 #define READ_MOSTLY_DATA(align)						\
 	. = ALIGN(align);						\
-	*(.data..read_mostly)
+	*(.data..read_mostly)						\
+	. = ALIGN(align);
 
 #define CACHELINE_ALIGNED_DATA(align)					\
 	. = ALIGN(align);						\

commit aab94339cd85d726abeae78fc02351fc1910e6a4
Author: Dirk Brandewie <dirk.brandewie@gmail.com>
Date:   Wed Dec 22 11:57:26 2010 -0800

    of: Add support for linking device tree blobs into vmlinux
    
    This patch adds support for linking device tree blob(s) into
    vmlinux. Modifies asm-generic/vmlinux.lds.h to add linking
    .dtb sections into vmlinux. To maintain compatiblity with the of/fdt
    driver code platforms MUST copy the blob to a non-init memory location
    before the kernel frees the .init.* sections in the image.
    
    Modifies scripts/Makefile.lib to add a kbuild command to
    compile DTS files to device tree blobs and a rule to create objects to
    wrap the blobs for linking.
    
    STRUCT_ALIGNMENT is defined in vmlinux.lds.h for use in the rule to
    create wrapper objects for the dtb in Makefile.lib.  The
    STRUCT_ALIGN() macro in vmlinux.lds.h is modified to use the
    STRUCT_ALIGNMENT definition.
    
    The DTB's are placed on 32 byte boundries to allow parsing the blob
    with driver/of/fdt.c during early boot without having to copy the blob
    to get the structure alignment GCC expects.
    
    A DTB is linked in by adding the DTB object to the list of objects to
    be linked into vmlinux in the archtecture specific Makefile using
       obj-y += foo.dtb.o
    
    Signed-off-by: Dirk Brandewie <dirk.brandewie@gmail.com>
    Acked-by: Michal Marek <mmarek@suse.cz>
    [grant.likely@secretlab.ca: cleaned up whitespace inconsistencies]
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index bd69d79208de..05cbad03c5ab 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -67,7 +67,8 @@
  * Align to a 32 byte boundary equal to the
  * alignment gcc 4.5 uses for a struct
  */
-#define STRUCT_ALIGN() . = ALIGN(32)
+#define STRUCT_ALIGNMENT 32
+#define STRUCT_ALIGN() . = ALIGN(STRUCT_ALIGNMENT)
 
 /* The actual configuration determine if the init/exit sections
  * are handled as text/data or they can be discarded (which
@@ -146,6 +147,13 @@
 #define TRACE_SYSCALLS()
 #endif
 
+
+#define KERNEL_DTB()							\
+	STRUCT_ALIGN();							\
+	VMLINUX_SYMBOL(__dtb_start) = .;				\
+	*(.dtb.init.rodata)						\
+	VMLINUX_SYMBOL(__dtb_end) = .;
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -468,7 +476,8 @@
 	MCOUNT_REC()							\
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
-	MEM_DISCARD(init.rodata)
+	MEM_DISCARD(init.rodata)					\
+	KERNEL_DTB()
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit c9e2a72ff1acfdffdecb338b3d997f90c507e665
Merge: 9aca0e7c8c3a d63f6d1b4d3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 28 15:13:55 2010 -0700

    Merge branch 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild-2.6
    
    * 'kbuild' of git://git.kernel.org/pub/scm/linux/kernel/git/mmarek/kbuild-2.6:
      initramfs: Fix build break on symbol-prefixed archs
      initramfs: fix initramfs size calculation
      initramfs: generalize initramfs_data.xxx.S variants
      scripts/kallsyms: Enable error messages while hush up unnecessary warnings
      scripts/setlocalversion: update comment
      kbuild: Use a single clean rule for kernel and external modules
      kbuild: Do not run make clean in $(srctree)
      scripts/mod/modpost.c: fix commentary accordingly to last changes
      kbuild: Really don't clean bounds.h and asm-offsets.h

commit d88262623fb58853ed60fb110141c435de26e3de
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Tue Oct 26 14:22:30 2010 -0700

    vmlinux.lds.h: lower init ramfs alignment to 4
    
    The new init ramfs format (cpio based) requires an alignment of 4 (per the
    documentation and per the source files themselves).  As for compressed
    sources, the decompressors can all deal with unaligned buffers.
    
    The cpio source is also found in the __init sections of the kernel, so
    once they are read and expanded into a tmpfs, the source is freed.  That
    means there is no need to force page alignment here either.
    
    This has been used on Blackfin systems for many releases without issue.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index aaf1105d9d91..2c0fc10956ba 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -637,7 +637,7 @@
 
 #ifdef CONFIG_BLK_DEV_INITRD
 #define INIT_RAM_FS							\
-	. = ALIGN(PAGE_SIZE);						\
+	. = ALIGN(4);							\
 	VMLINUX_SYMBOL(__initramfs_start) = .;				\
 	*(.init.ramfs)							\
 	VMLINUX_SYMBOL(__initramfs_end) = .;

commit d356c0b680d15e0187de48aa303e541b461ea794
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Tue Oct 26 14:22:29 2010 -0700

    vmlinux.lds.h: gather .data..shared_aligned sections in DATA_DATA
    
    With the recent change "net: remove time limit in process_backlog()", the
    softnet_data variable changed from "DEFINE_PER_CPU()" to
    "DEFINE_PER_CPU_ALIGNED()" which moved it from the .data section to the
    .data.shared_align section.  I'm not saying this patch is wrong, just that
    is what caused me to notice this larger problem.  No one else in the
    kernel is using this aligned macro variant, so I imagine that's why no one
    has noticed yet.
    
    Since .data..shared_align isn't declared in any vmlinux files that I can
    see, the linker just places it last.  This "just works" for most people,
    but when building a ROM kernel on Blackfin systems, it causes section
    overlap errors:
    
    bfin-uclinux-ld.real:
            section .init.data [00000000202e06b8 -> 00000000202e48b7] overlaps
            section .data.shared_aligned [00000000202e06b8 -> 00000000202e0723]
    
    I imagine other arches which support the ROM config option and thus do
    funky placement would see similar issues ...
    
    On x86, it is stuck in a dedicated section at the end:
     [8] .data             PROGBITS ffffffff810ec000 2ec0000303a8 00 WA 0 0 4096
     [9] .data.shared_alig PROGBITS ffffffff8111c3c0 31c3c00000c8 00 WA 0 0 64
    
    So make sure we include this section in the DATA_DATA macro so that it is
    placed in the right location.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Alan Jenkins <alan-jenkins@tuffmail.co.uk>
    Cc: Greg Ungerer <gerg@snapgear.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f4229fb315e1..aaf1105d9d91 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -150,6 +150,7 @@
 #define DATA_DATA							\
 	*(.data)							\
 	*(.ref.data)							\
+	*(.data..shared_aligned) /* percpu related */			\
 	DEV_KEEP(init.data)						\
 	DEV_KEEP(exit.data)						\
 	CPU_KEEP(init.data)						\

commit c3b86a29429dac1033e3f602f51fa8d00006a8eb
Merge: 8d8d2e9ccd33 2aeb66d3036d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 13:47:29 2010 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86-32, percpu: Correct the ordering of the percpu readmostly section
      x86, mm: Enable ARCH_DMA_ADDR_T_64BIT with X86_64 || HIGHMEM64G
      x86: Spread tlb flush vector between nodes
      percpu: Introduce a read-mostly percpu API
      x86, mm: Fix incorrect data type in vmalloc_sync_all()
      x86, mm: Hold mm->page_table_lock while doing vmalloc_sync
      x86, mm: Fix bogus whitespace in sync_global_pgds()
      x86-32: Fix sparse warning for the __PHYSICAL_MASK calculation
      x86, mm: Add RESERVE_BRK_ARRAY() helper
      mm, x86: Saving vmcore with non-lazy freeing of vmas
      x86, kdump: Change copy_oldmem_page() to use cached addressing
      x86, mm: fix uninitialized addr in kernel_physical_mapping_init()
      x86, kmemcheck: Remove double test
      x86, mm: Make spurious_fault check explicitly check the PRESENT bit
      x86-64, mem: Update all PGDs for direct mapping and vmemmap mapping changes
      x86, mm: Separate x86_64 vmalloc_sync_all() into separate functions
      x86, mm: Avoid unnecessary TLB flush

commit 2aeb66d3036dbafc297ac553a257a40283dadb3e
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Thu Oct 21 00:15:00 2010 -0700

    x86-32, percpu: Correct the ordering of the percpu readmostly section
    
    Checkin c957ef2c59e952803766ddc22e89981ab534606f had inconsistent
    ordering of .data..percpu..page_aligned and .data..percpu..readmostly;
    the still-broken version affected x86-32 at least.
    
    The page aligned version really must be page aligned...
    
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    LKML-Reference: <1287544022.4571.7.camel@sli10-conroe.sh.intel.com>
    Cc: Shaohua Li <shaohua.li@intel.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index d7e7b21511b1..1457b81357af 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -706,8 +706,8 @@
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
 		*(.data..percpu..first)					\
 		. = ALIGN(PAGE_SIZE);					\
-		*(.data..percpu..readmostly)				\
 		*(.data..percpu..page_aligned)				\
+		*(.data..percpu..readmostly)				\
 		*(.data..percpu)					\
 		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\

commit c957ef2c59e952803766ddc22e89981ab534606f
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Wed Oct 20 11:07:02 2010 +0800

    percpu: Introduce a read-mostly percpu API
    
    Add a new readmostly percpu section and API.  This can be used to
    avoid dirtying data lines which are generally not written to, which is
    especially important for data which may be accessed by processors
    other than the one for which the percpu area belongs to.
    
    [ hpa: moved it *after* the page-aligned section, for obvious
      reasons. ]
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    LKML-Reference: <1287544022.4571.7.camel@sli10-conroe.sh.intel.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8a92a170fb7d..d7e7b21511b1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -677,7 +677,9 @@
 				- LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
 		*(.data..percpu..first)					\
+		. = ALIGN(PAGE_SIZE);					\
 		*(.data..percpu..page_aligned)				\
+		*(.data..percpu..readmostly)				\
 		*(.data..percpu)					\
 		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
@@ -703,6 +705,8 @@
 		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
 		*(.data..percpu..first)					\
+		. = ALIGN(PAGE_SIZE);					\
+		*(.data..percpu..readmostly)				\
 		*(.data..percpu..page_aligned)				\
 		*(.data..percpu)					\
 		*(.data..percpu..shared_aligned)			\

commit ffe8018c3424892c9590048fc36caa6c3e0c8a76
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Fri Sep 17 15:24:11 2010 -0700

    initramfs: fix initramfs size calculation
    
    The size of a built-in initramfs is calculated in init/initramfs.c by
    "__initramfs_end - __initramfs_start".  Those symbols are defined in the
    linker script include/asm-generic/vmlinux.lds.h:
    
    #define INIT_RAM_FS                                                     \
            . = ALIGN(PAGE_SIZE);                                           \
            VMLINUX_SYMBOL(__initramfs_start) = .;                          \
            *(.init.ramfs)                                                  \
            VMLINUX_SYMBOL(__initramfs_end) = .;
    
    If the initramfs file has an odd number of bytes, the "__initramfs_end"
    symbol points to an odd address, for example, the symbols in the
    System.map might look like:
    
        0000000000572000 T __initramfs_start
        00000000005bcd05 T __initramfs_end    <-- odd address
    
    At least on s390 this causes a problem:
    
    Certain s390 instructions, especially instructions for loading addresses
    (larl) or branch addresses must be on even addresses.  The compiler loads
    the symbol addresses with the "larl" instruction.  This instruction sets
    the last bit to 0 and, therefore, for odd size files, the calculated size
    is one byte less than it should be:
    
        0000000000540a9c <populate_rootfs>:
          540a9c:     eb cf f0 78 00 24       stmg    %r12,%r15,120(%r15),
          540aa2:     c0 10 00 01 8a af       larl    %r1,572000 <__initramfs_start>
          540aa8:     c0 c0 00 03 e1 2e       larl    %r12,5bcd04 <initramfs_end>
                                                      (Instead of  5bcd05)
          ...
          540abe:     1b c1                   sr      %r12,%r1
    
    To fix the problem, this patch introduces the global variable
    __initramfs_size, which is calculated in the "usr/initramfs_data.S" file.
    The populate_rootfs() function can then use the start marker of the
    .init.ramfs section and the value of __initramfs_size for loading the
    initramfs.  Because the start marker and size is sufficient, the
    __initramfs_end symbol is no longer needed and is removed.
    
    Signed-off-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Reviewed-by: WANG Cong <xiyou.wangcong@gmail.com>
    Acked-by: Michal Marek <mmarek@suse.cz>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 030a954ed292..0c6387d6a6ae 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -633,7 +633,8 @@
 	. = ALIGN(PAGE_SIZE);						\
 	VMLINUX_SYMBOL(__initramfs_start) = .;				\
 	*(.init.ramfs)							\
-	VMLINUX_SYMBOL(__initramfs_end) = .;
+	. = ALIGN(8);							\
+	*(.init.ramfs.info)
 #else
 #define INIT_RAM_FS
 #endif

commit bf5438fca2950b03c21ad868090cc1a8fcd49536
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Sep 17 11:09:00 2010 -0400

    jump label: Base patch for jump label
    
    base patch to implement 'jump labeling'. Based on a new 'asm goto' inline
    assembly gcc mechanism, we can now branch to labels from an 'asm goto'
    statment. This allows us to create a 'no-op' fastpath, which can subsequently
    be patched with a jump to the slowpath code. This is useful for code which
    might be rarely used, but which we'd like to be able to call, if needed.
    Tracepoints are the current usecase that these are being implemented for.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <ee8b3595967989fdaf84e698dc7447d315ce972a.1284733808.git.jbaron@redhat.com>
    
    [ cleaned up some formating ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8a92a170fb7d..ef2af9948eac 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -220,6 +220,8 @@
 									\
 	BUG_TABLE							\
 									\
+	JUMP_TABLE							\
+									\
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
@@ -563,6 +565,14 @@
 #define BUG_TABLE
 #endif
 
+#define JUMP_TABLE							\
+	. = ALIGN(8);							\
+	__jump_table : AT(ADDR(__jump_table) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start___jump_table) = .;		\
+		*(__jump_table)						\
+		VMLINUX_SYMBOL(__stop___jump_table) = .;		\
+	}
+
 #ifdef CONFIG_PM_TRACE
 #define TRACEDATA							\
 	. = ALIGN(4);							\

commit 4aed2fd8e3181fea7c09ba79cf64e7e3f4413bf9
Merge: 3a3527b6461b fc9ea5a1e53e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:30:52 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (162 commits)
      tracing/kprobes: unregister_trace_probe needs to be called under mutex
      perf: expose event__process function
      perf events: Fix mmap offset determination
      perf, powerpc: fsl_emb: Restore setting perf_sample_data.period
      perf, powerpc: Convert the FSL driver to use local64_t
      perf tools: Don't keep unreferenced maps when unmaps are detected
      perf session: Invalidate last_match when removing threads from rb_tree
      perf session: Free the ref_reloc_sym memory at the right place
      x86,mmiotrace: Add support for tracing STOS instruction
      perf, sched migration: Librarize task states and event headers helpers
      perf, sched migration: Librarize the GUI class
      perf, sched migration: Make the GUI class client agnostic
      perf, sched migration: Make it vertically scrollable
      perf, sched migration: Parameterize cpu height and spacing
      perf, sched migration: Fix key bindings
      perf, sched migration: Ignore unhandled task states
      perf, sched migration: Handle ignored migrate out events
      perf: New migration tool overview
      tracing: Drop cpparg() macro
      perf: Use tracepoint_synchronize_unregister() to flush any pending tracepoint call
      ...
    
    Fix up trivial conflicts in Makefile and drivers/cpufreq/cpufreq.c

commit ca50a5f39041497253c6362f2ba4da1b56d3e6cb
Merge: a70ce4b6064b ca65f9fc0c44
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Wed Aug 4 14:49:16 2010 -0700

    Merge branch 'upstream/pvhvm' into upstream/xen
    
    * upstream/pvhvm:
      Introduce CONFIG_XEN_PVHVM compile option
      blkfront: do not create a PV cdrom device if xen_hvm_guest
      support multiple .discard.* sections to avoid section type conflicts
      xen/pvhvm: fix build problem when !CONFIG_XEN
      xenfs: enable for HVM domains too
      x86: Call HVMOP_pagetable_dying on exit_mmap.
      x86: Unplug emulated disks and nics.
      x86: Use xen_vcpuop_clockevent, xen_clocksource and xen wallclock.
      xen: Fix find_unbound_irq in presence of ioapic irqs.
      xen: Add suspend/resume support for PV on HVM guests.
      xen: Xen PCI platform device driver.
      x86/xen: event channels delivery on HVM.
      x86: early PV on HVM features initialization.
      xen: Add support for HVM hypercalls.
    
    Conflicts:
            arch/x86/xen/enlighten.c
            arch/x86/xen/time.c

commit 3772b734720e1a3f2dc1d95cfdfaa5332f4ccf01
Merge: 9fc3af467d07 9fe6206f4006
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Aug 2 08:29:56 2010 +0200

    Merge commit 'v2.6.35' into perf/core
    
    Conflicts:
            tools/perf/Makefile
            tools/perf/util/hist.c
    
    Merge reason: Resolve the conflicts and update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c7f52cdc2f3e1733d3864e439ac2e92edd99ef31
Author: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
Date:   Thu Jul 22 22:58:01 2010 -0700

    support multiple .discard.* sections to avoid section type conflicts
    
    gcc 4.4.4 will complain if you use a .discard section for both text and
    data ("causes a section type conflict").  Add support for ".discard.*"
    sections, and use .discard.text for a dummy function in the x86
    RESERVE_BRK() macro.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 48c5299cbf26..ae6b88eb1de1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -643,6 +643,7 @@
 	EXIT_DATA							\
 	EXIT_CALL							\
 	*(.discard)							\
+	*(.discard.*)							\
 	}
 
 /**

commit 86c65a7857896b1de99628ad392556965c4841e6
Merge: 20a52d4f5998 7ffb65f84bd3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 23 13:26:16 2010 -0700

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      vmlinux.lds: fix .data..init_task output section (fix popwerpc boot)
      powerpc: Fix erroneous lmb->memblock conversions
      powerpc/mm: Add some debug output when hash insertion fails
      powerpc/mm: Fix bugs in huge page hashing
      powerpc/mm: Move around testing of _PAGE_PRESENT in hash code
      powerpc/mm: Handle hypervisor pte insert failure in __hash_page_huge
      powerpc/kexec: Fix boundary case for book-e kexec memory limits

commit da5e37efe8704fc2b354626467f80f73c5e3c020
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Tue Jul 13 11:39:42 2010 +0200

    vmlinux.lds: fix .data..init_task output section (fix popwerpc boot)
    
    The .data..init_task output section was missing
    a load offset causing a popwerpc target to fail to boot.
    
    Sean MacLennan tracked it down to the definition of
    INIT_TASK_DATA_SECTION().
    
    There are only two users of INIT_TASK_DATA_SECTION()
    in the kernel today: cris and popwerpc.
    cris do not support relocatable kernels and is thus not
    impacted by this change.
    
    Fix INIT_TASK_DATA_SECTION() to specify load offset like
    all other output sections.
    
    Reported-by: Sean MacLennan <smaclennan@pikatech.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 48c5299cbf26..cdfff74e9739 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -435,7 +435,7 @@
  */
 #define INIT_TASK_DATA_SECTION(align)					\
 	. = ALIGN(align);						\
-	.data..init_task : {						\
+	.data..init_task :  AT(ADDR(.data..init_task) - LOAD_OFFSET) {	\
 		INIT_TASK_DATA(align)					\
 	}
 

commit 07fca0e57fca925032526349f4370f97ed580cc9
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sat Jul 10 08:35:00 2010 +0200

    tracing: Properly align linker defined symbols
    
    We define a number of symbols in the linker scipt like this:
    
        __start_syscalls_metadata = .;
        *(__syscalls_metadata)
    
    But we do not know the alignment of "." when we assign
    the __start_syscalls_metadata symbol.
    gcc started to uses bigger alignment for structs (32 bytes),
    so we saw situations where the linker due to alignment
    constraints increased the value of "." after the symbol assignment.
    
    This resulted in boot fails.
    
    Fix this by forcing a 32 byte alignment of "." before the
    assignment.
    
    This patch introduces the forced alignment for
    ftrace_events and syscalls_metadata.
    It may be required in more places.
    
    Reported-by: Zeev Tarantov <zeev.tarantov@gmail.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    LKML-Reference: <20100710063459.GA14596@merkur.ravnborg.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 48c5299cbf26..4b5902ad0d5d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -63,6 +63,12 @@
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
+/*
+ * Align to a 32 byte boundary equal to the
+ * alignment gcc 4.5 uses for a struct
+ */
+#define STRUCT_ALIGN() . = ALIGN(32)
+
 /* The actual configuration determine if the init/exit sections
  * are handled as text/data or they can be discarded (which
  * often happens at runtime)
@@ -166,7 +172,11 @@
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
 	TRACE_PRINTKS()							\
+									\
+	STRUCT_ALIGN();							\
 	FTRACE_EVENTS()							\
+									\
+	STRUCT_ALIGN();							\
 	TRACE_SYSCALLS()
 
 /*

commit eb878b3bc0349344dbf70c51bf01fc734d5cf2d3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 15 23:52:45 2010 +0200

    tracing: Remove letfover markers section
    
    Markers have been removed, but we forgot to remove their
    section.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 48c5299cbf26..415b1a9118e7 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -150,10 +150,6 @@
 	CPU_KEEP(exit.data)						\
 	MEM_KEEP(init.data)						\
 	MEM_KEEP(exit.data)						\
-	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__start___markers) = .;				\
-	*(__markers)							\
-	VMLINUX_SYMBOL(__stop___markers) = .;				\
 	. = ALIGN(32);							\
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\

commit 1f73897861b8ef0be64ff4b801f8d6f830f683b5
Merge: b904d7131d11 64ffc9ff424c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 1 08:55:52 2010 -0700

    Merge branch 'for-35' of git://repo.or.cz/linux-kbuild
    
    * 'for-35' of git://repo.or.cz/linux-kbuild: (81 commits)
      kbuild: Revert part of e8d400a to resolve a conflict
      kbuild: Fix checking of scm-identifier variable
      gconfig: add support to show hidden options that have prompts
      menuconfig: add support to show hidden options which have prompts
      gconfig: remove show_debug option
      gconfig: remove dbg_print_ptype() and dbg_print_stype()
      kconfig: fix zconfdump()
      kconfig: some small fixes
      add random binaries to .gitignore
      kbuild: Include gen_initramfs_list.sh and the file list in the .d file
      kconfig: recalc symbol value before showing search results
      .gitignore: ignore *.lzo files
      headerdep: perlcritic warning
      scripts/Makefile.lib: Align the output of LZO
      kbuild: Generate modules.builtin in make modules_install
      Revert "kbuild: specify absolute paths for cscope"
      kbuild: Do not unnecessarily regenerate modules.builtin
      headers_install: use local file handles
      headers_check: fix perl warnings
      export_report: fix perl warnings
      ...

commit 058f88d672b3161fe511ebe2996c3faef63c1c8e
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Wed May 26 14:44:03 2010 -0700

    rapidio: modify initialization of switch operations
    
    Modify the way how RapidIO switch operations are declared.  Multiple
    assignments through the linker script replaced by single initialization
    call.
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Thomas Moll <thomas.moll@sysgo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8f7c89b20639..ef779c6fc3d7 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -248,12 +248,9 @@
 									\
 	/* RapidIO route ops */						\
 	.rio_ops        : AT(ADDR(.rio_ops) - LOAD_OFFSET) {		\
-		VMLINUX_SYMBOL(__start_rio_route_ops) = .;		\
-		*(.rio_route_ops)					\
-		VMLINUX_SYMBOL(__end_rio_route_ops) = .;		\
-		VMLINUX_SYMBOL(__start_rio_em_ops) = .;			\
-		*(.rio_em_ops)						\
-		VMLINUX_SYMBOL(__end_rio_em_ops) = .;			\
+		VMLINUX_SYMBOL(__start_rio_switch_ops) = .;		\
+		*(.rio_switch_ops)					\
+		VMLINUX_SYMBOL(__end_rio_switch_ops) = .;		\
 	}								\
 									\
 	TRACEDATA							\

commit e5cabeb3d60f9cd3e3950aff071319ae0e2d08d8
Author: Alexandre Bounine <alexandre.bounine@idt.com>
Date:   Wed May 26 14:43:59 2010 -0700

    rapidio: add Port-Write handling for EM
    
    Add RapidIO Port-Write message handling in the context of Error
       Management Extensions Specification Rev.1.3.
    
    Signed-off-by: Alexandre Bounine <alexandre.bounine@idt.com>
    Tested-by: Thomas Moll <thomas.moll@sysgo.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 67e652068e0e..8f7c89b20639 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -247,10 +247,13 @@
 	}								\
 									\
 	/* RapidIO route ops */						\
-	.rio_route        : AT(ADDR(.rio_route) - LOAD_OFFSET) {	\
+	.rio_ops        : AT(ADDR(.rio_ops) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start_rio_route_ops) = .;		\
 		*(.rio_route_ops)					\
 		VMLINUX_SYMBOL(__end_rio_route_ops) = .;		\
+		VMLINUX_SYMBOL(__start_rio_em_ops) = .;			\
+		*(.rio_em_ops)						\
+		VMLINUX_SYMBOL(__end_rio_em_ops) = .;			\
 	}								\
 									\
 	TRACEDATA							\

commit 07b3bb1ef211fdf20eddcae902d1098788ea2f6e
Author: Denys Vlasenko <vda.linux@googlemail.com>
Date:   Sat Feb 20 01:03:52 2010 +0100

    Rename .data.nosave to .data..nosave.
    
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6f6da4f080f2..ea3660526e91 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -175,7 +175,7 @@
 #define NOSAVE_DATA							\
 	. = ALIGN(PAGE_SIZE);						\
 	VMLINUX_SYMBOL(__nosave_begin) = .;				\
-	*(.data.nosave)							\
+	*(.data..nosave)						\
 	. = ALIGN(PAGE_SIZE);						\
 	VMLINUX_SYMBOL(__nosave_end) = .;
 

commit 54cb27a71f51d304342c79e62fd7667f2171062b
Author: Denys Vlasenko <vda.linux@googlemail.com>
Date:   Sat Feb 20 01:03:44 2010 +0100

    Rename .data.read_mostly to .data..read_mostly.
    
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e304fcd10bc7..6f6da4f080f2 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -185,7 +185,7 @@
 
 #define READ_MOSTLY_DATA(align)						\
 	. = ALIGN(align);						\
-	*(.data.read_mostly)
+	*(.data..read_mostly)
 
 #define CACHELINE_ALIGNED_DATA(align)					\
 	. = ALIGN(align);						\

commit 3d9a854c2dac3e888393b23ba7adafcce4d6d4b9
Author: Denys Vlasenko <vda.linux@googlemail.com>
Date:   Sat Feb 20 01:03:43 2010 +0100

    Rename .data[.percpu][.XXX] to .data[..percpu][..XXX].
    
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32cddc155940..e304fcd10bc7 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -666,16 +666,16 @@
  */
 #define PERCPU_VADDR(vaddr, phdr)					\
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
-	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
+	.data..percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
 				- LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
-		*(.data.percpu.first)					\
-		*(.data.percpu.page_aligned)				\
-		*(.data.percpu)						\
-		*(.data.percpu.shared_aligned)				\
+		*(.data..percpu..first)					\
+		*(.data..percpu..page_aligned)				\
+		*(.data..percpu)					\
+		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	} phdr								\
-	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
+	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data..percpu);
 
 /**
  * PERCPU - define output section for percpu area, simple version
@@ -687,18 +687,18 @@
  *
  * This macro is equivalent to ALIGN(align); PERCPU_VADDR( , ) except
  * that __per_cpu_load is defined as a relative symbol against
- * .data.percpu which is required for relocatable x86_32
+ * .data..percpu which is required for relocatable x86_32
  * configuration.
  */
 #define PERCPU(align)							\
 	. = ALIGN(align);						\
-	.data.percpu	: AT(ADDR(.data.percpu) - LOAD_OFFSET) {	\
+	.data..percpu	: AT(ADDR(.data..percpu) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
-		*(.data.percpu.first)					\
-		*(.data.percpu.page_aligned)				\
-		*(.data.percpu)						\
-		*(.data.percpu.shared_aligned)				\
+		*(.data..percpu..first)					\
+		*(.data..percpu..page_aligned)				\
+		*(.data..percpu)					\
+		*(.data..percpu..shared_aligned)			\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	}
 

commit 7c74df07f90cabe61d700727bca04682b4e477f3
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sat Feb 20 01:03:38 2010 +0100

    Rename .bss.page_aligned to .bss..page_aligned.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 569c25a85558..32cddc155940 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -499,7 +499,7 @@
 #define BSS(bss_align)							\
 	. = ALIGN(bss_align);						\
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {				\
-		*(.bss.page_aligned)					\
+		*(.bss..page_aligned)					\
 		*(.dynbss)						\
 		*(.bss)							\
 		*(COMMON)						\

commit 75b134837263eb919d91678f7fcf3d54cd088c8d
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sat Feb 20 01:03:37 2010 +0100

    Rename .data.page_aligned to .data..page_aligned.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9cb9a9021e6e..569c25a85558 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -181,7 +181,7 @@
 
 #define PAGE_ALIGNED_DATA(page_align)					\
 	. = ALIGN(page_align);						\
-	*(.data.page_aligned)
+	*(.data..page_aligned)
 
 #define READ_MOSTLY_DATA(align)						\
 	. = ALIGN(align);						\

commit 2af7687f1ad2c4571b9835f9bb2e3db9a738d258
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sat Feb 20 01:03:35 2010 +0100

    Rename .data.init_task to .data..init_task.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 78450aaab9ef..9cb9a9021e6e 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -193,7 +193,7 @@
 
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
-	*(.data.init_task)
+	*(.data..init_task)
 
 /*
  * Read only Data
@@ -435,7 +435,7 @@
  */
 #define INIT_TASK_DATA_SECTION(align)					\
 	. = ALIGN(align);						\
-	.data.init_task : {						\
+	.data..init_task : {						\
 		INIT_TASK_DATA(align)					\
 	}
 

commit 4af57b787b4be09419a2bb48aa705fa87ef41cca
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sat Feb 20 01:03:34 2010 +0100

    Rename .data.cacheline_aligned to .data..cacheline_aligned.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Signed-off-by: Michal Marek <mmarek@suse.cz>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 67e652068e0e..78450aaab9ef 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -189,7 +189,7 @@
 
 #define CACHELINE_ALIGNED_DATA(align)					\
 	. = ALIGN(align);						\
-	*(.data.cacheline_aligned)
+	*(.data..cacheline_aligned)
 
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\

commit 9e1b9b80721661bd63b3662453767b22cd614fe7
Author: Alan Jenkins <alan-jenkins@tuffmail.co.uk>
Date:   Sat Nov 7 21:03:54 2009 +0000

    module: make MODULE_SYMBOL_PREFIX into a CONFIG option
    
    The next commit will require the use of MODULE_SYMBOL_PREFIX in
    .tmp_exports-asm.S.  Currently it is mixed in with C structure
    definitions in "asm/module.h".  Move the definition of this arch option
    into Kconfig, so it can be easily accessed by any code.
    
    This also lets modpost.c use the same definition.  Previously modpost
    relied on a hardcoded list of architectures in mk_elfconfig.c.
    
    A build test for blackfin, one of the two MODULE_SYMBOL_PREFIX archs,
    showed the generated code was unchanged.  vmlinux was identical save
    for build ids, and an apparently randomized suffix on a single "__key"
    symbol in the kallsyms data).
    
    Signed-off-by: Alan Jenkins <alan-jenkins@tuffmail.co.uk>
    Acked-by: Mike Frysinger <vapier@gentoo.org> (blackfin)
    CC: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b6e818f4b247..67e652068e0e 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -52,8 +52,12 @@
 #define LOAD_OFFSET 0
 #endif
 
-#ifndef VMLINUX_SYMBOL
-#define VMLINUX_SYMBOL(_sym_) _sym_
+#ifndef SYMBOL_PREFIX
+#define VMLINUX_SYMBOL(sym) sym
+#else
+#define PASTE2(x,y) x##y
+#define PASTE(x,y) PASTE2(x,y)
+#define VMLINUX_SYMBOL(sym) PASTE(SYMBOL_PREFIX, sym)
 #endif
 
 /* Align . to a 8 byte boundary equals to maximum function alignment. */

commit 1b2086227cd1a24f748398c22ea9652c383499cf
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Thu Sep 24 10:36:16 2009 -0400

    Optimize the ordering of sections in RW_DATA_SECTION.
    
    The old RW_DATA_SECTION had INIT_TASK_DATA (which was
    more-than-PAGE_SIZE-aligned), followed by a bunch of small alignment
    stuff, followed by more PAGE_SIZE-aligned stuff, so you wasted memory
    in the middle of .data re-aligning back up to PAGE_SIZE.
    
    This patch sorts the sections by alignment requirements, which should
    pack them essentially optimally.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Reviewed-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 29ca8f53ffbe..b6e818f4b247 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -721,12 +721,12 @@
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
 		INIT_TASK_DATA(inittask)				\
+		NOSAVE_DATA						\
+		PAGE_ALIGNED_DATA(pagealigned)				\
 		CACHELINE_ALIGNED_DATA(cacheline)			\
 		READ_MOSTLY_DATA(cacheline)				\
 		DATA_DATA						\
 		CONSTRUCTORS						\
-		NOSAVE_DATA						\
-		PAGE_ALIGNED_DATA(pagealigned)				\
 	}
 
 #define INIT_TEXT_SECTION(inittext_align)				\

commit 45bd00d31de886f8425b4dd33204b911b0a466a9
Merge: 40d9d82c8ab8 ab86e5765d41
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Sep 17 20:52:23 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: Pick up kernel/softirq.c update for dependent fix.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4b3b4c5e64ce26612646867ee354373620063534
Author: John Reiser <jreiser@bitwagon.com>
Date:   Mon Jul 27 11:23:50 2009 -0700

    ftrace: __start_mcount_loc should be .init.rodata
    
    __start_mcount_loc[] is unused after init, yet occupies RAM forever
    as part of .rodata.  152kiB is typical on a 64-bit architecture.  Instead,
    __start_mcount_loc should be in the interval [__init_begin, __init_end)
    so that the space is reclaimed after init.
    
    __start_mcount_loc[] is generated during the load portion
    of kernel build, and is used only by ftrace_init().  ftrace_init is declared
    '__init' and is in .init.text, which is freed after init.
    __start_mcount_loc is placed into .rodata by a call to MCOUNT_REC inside
    the RO_DATA macro of include/asm-generic/vmlinux.lds.h.  The array *is*
    read-only, but more importantly it is not used after init.  So the call to
    MCOUNT_REC should be moved from RO_DATA to INIT_DATA.
    
    This patch has been tested on x86_64 with CONFIG_DEBUG_PAGEALLOC=y
    which verifies that the address range never is accessed after init.
    
    Signed-off-by: John Reiser <jreiser@BitWagon.com>
    LKML-Reference: <4A6DF0B6.7080402@bitwagon.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6ad76bf5fb40..98b37cf3ac6d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -91,7 +91,8 @@
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
-#define MCOUNT_REC()	VMLINUX_SYMBOL(__start_mcount_loc) = .; \
+#define MCOUNT_REC()	. = ALIGN(8);				\
+			VMLINUX_SYMBOL(__start_mcount_loc) = .; \
 			*(__mcount_loc)				\
 			VMLINUX_SYMBOL(__stop_mcount_loc) = .;
 #else
@@ -331,7 +332,6 @@
 	/* __*init sections */						\
 	__init_rodata : AT(ADDR(__init_rodata) - LOAD_OFFSET) {		\
 		*(.ref.rodata)						\
-		MCOUNT_REC()						\
 		DEV_KEEP(init.rodata)					\
 		DEV_KEEP(exit.rodata)					\
 		CPU_KEEP(init.rodata)					\
@@ -455,6 +455,7 @@
 	MEM_DISCARD(init.data)						\
 	KERNEL_CTORS()							\
 	*(.init.rodata)							\
+	MCOUNT_REC()							\
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)

commit 384be2b18a5f9475eab9ca2bdfa95cc1a04ef59c
Merge: a76761b621bc 142d44b0dd67
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 14 14:41:02 2009 +0900

    Merge branch 'percpu-for-linus' into percpu-for-next
    
    Conflicts:
            arch/sparc/kernel/smp_64.c
            arch/x86/kernel/cpu/perf_counter.c
            arch/x86/kernel/setup_percpu.c
            drivers/cpufreq/cpufreq_ondemand.c
            mm/percpu.c
    
    Conflicts in core and arch percpu codes are mostly from commit
    ed78e1e078dd44249f88b1dd8c76dafb39567161 which substituted many
    num_possible_cpus() with nr_cpu_ids.  As for-next branch has moved all
    the first chunk allocators into mm/percpu.c, the changes are moved
    from arch code to mm/percpu.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 04e448d9a386640a79a4aa71251aa1cdd314f662
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Sun Jul 12 18:23:33 2009 -0400

    vmlinux.lds.h: restructure BSS linker script macros
    
    The BSS section macros in vmlinux.lds.h currently place the .sbss
    input section outside the bounds of [__bss_start, __bss_end].  On all
    architectures except for microblaze that handle both .sbss and
    __bss_start/__bss_end, this is wrong: the .sbss input section is
    within the range [__bss_start, __bss_end].  Relatedly, the example
    code at the top of the file actually has __bss_start/__bss_end defined
    twice; I believe the right fix here is to define them in the
    BSS_SECTION macro but not in the BSS macro.
    
    Another problem with the current macros is that several
    architectures have an ALIGN(4) or some other small number just before
    __bss_stop in their linker scripts.  The BSS_SECTION macro currently
    hardcodes this to 4; while it should really be an argument.  It also
    ignores its sbss_align argument; fix that.
    
    mn10300 is the only user at present of any of the macros touched by
    this patch.  It looks like mn10300 actually was incorrectly converted
    to use the new BSS() macro (the alignment of 4 prior to conversion was
    a __bss_stop alignment, but the argument to the BSS macro is a start
    alignment).  So fix this as well.
    
    I'd like acks from Sam and David on this one.  Also CCing Paul, since
    he has a patch from me which will need to be updated to use
    BSS_SECTION(0, PAGE_SIZE, 4) once this gets merged.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a553f1041cf1..6ad76bf5fb40 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -30,9 +30,7 @@
  *	EXCEPTION_TABLE(...)
  *	NOTES
  *
- *	__bss_start = .;
- *	BSS_SECTION(0, 0)
- *	__bss_stop = .;
+ *	BSS_SECTION(0, 0, 0)
  *	_end = .;
  *
  *	/DISCARD/ : {
@@ -489,7 +487,8 @@
  * bss (Block Started by Symbol) - uninitialized data
  * zeroed during startup
  */
-#define SBSS								\
+#define SBSS(sbss_align)						\
+	. = ALIGN(sbss_align);						\
 	.sbss : AT(ADDR(.sbss) - LOAD_OFFSET) {				\
 		*(.sbss)						\
 		*(.scommon)						\
@@ -498,12 +497,10 @@
 #define BSS(bss_align)							\
 	. = ALIGN(bss_align);						\
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {				\
-		VMLINUX_SYMBOL(__bss_start) = .;			\
 		*(.bss.page_aligned)					\
 		*(.dynbss)						\
 		*(.bss)							\
 		*(COMMON)						\
-		VMLINUX_SYMBOL(__bss_stop) = .;				\
 	}
 
 /*
@@ -735,8 +732,10 @@
 		INIT_RAM_FS						\
 	}
 
-#define BSS_SECTION(sbss_align, bss_align)				\
-	SBSS								\
+#define BSS_SECTION(sbss_align, bss_align, stop_align)			\
+	. = ALIGN(sbss_align);						\
+	VMLINUX_SYMBOL(__bss_start) = .;				\
+	SBSS(sbss_align)						\
 	BSS(bss_align)							\
-	. = ALIGN(4);
-
+	. = ALIGN(stop_align);						\
+	VMLINUX_SYMBOL(__bss_stop) = .;

commit 023bf6f1b8bf58dc4da7f0dc1cf4787b0d5297c1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 9 11:27:40 2009 +0900

    linker script: unify usage of discard definition
    
    Discarded sections in different archs share some commonality but have
    considerable differences.  This led to linker script for each arch
    implementing its own /DISCARD/ definition, which makes maintaining
    tedious and adding new entries error-prone.
    
    This patch makes all linker scripts to move discard definitions to the
    end of the linker script and use the common DISCARDS macro.  As ld
    uses the first matching section definition, archs can include default
    discarded sections by including them earlier in the linker script.
    
    ia64 is notable because it first throws away some ia64 specific
    subsections and then include the rest of the sections into the final
    image, so those sections must be discarded before the inclusion.
    
    defconfig compile tested for x86, x86-64, powerpc, powerpc64, ia64,
    alpha, sparc, sparc64 and s390.  Michal Simek tested microblaze.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Tested-by: Michal Simek <monstr@monstr.eu>
    Cc: linux-arch@vger.kernel.org
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: microblaze-uclinux@itee.uq.edu.au
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Tony Luck <tony.luck@intel.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c5c18ac878ab..ab8ea9b7741e 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -35,13 +35,10 @@
  *	__bss_stop = .;
  *	_end = .;
  *
- *	/DISCARD/ : {
- *		EXIT_TEXT
- *		EXIT_DATA
- *		EXIT_CALL
- *	}
  *	STABS_DEBUG
  *	DWARF_DEBUG
+ *
+ *	DISCARDS		// must be the last
  * }
  *
  * [__init_begin, __init_end] is the init section that may be freed after init
@@ -629,11 +626,20 @@
 #define INIT_RAM_FS
 #endif
 
+/*
+ * Default discarded sections.
+ *
+ * Some archs want to discard exit text/data at runtime rather than
+ * link time due to cross-section references such as alt instructions,
+ * bug table, eh_frame, etc.  DISCARDS must be the last of output
+ * section definitions so that such archs put those in earlier section
+ * definitions.
+ */
 #define DISCARDS							\
 	/DISCARD/ : {							\
 	EXIT_TEXT							\
 	EXIT_DATA							\
-	*(.exitcall.exit)						\
+	EXIT_CALL							\
 	*(.discard)							\
 	}
 

commit 29f31773e07772e73e3177a4af147244cd080554
Merge: 59107c6525c0 112942353992
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 4 09:46:01 2009 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-fixes
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-fixes:
      kbuild: finally remove the obsolete variable $TOPDIR
      gitignore: ignore scripts/ihex2fw
      Kbuild: Disable the -Wformat-security gcc flag
      gitignore: ignore gcov output files
      kbuild: deb-pkg ship changelog
      Add new __init_task_data macro to be used in arch init_task.c files.
      asm-generic/vmlinux.lds.h: shuffle INIT_TASK* macro names in vmlinux.lds.h
      Add new macros for page-aligned data and bss sections.
      asm-generic/vmlinux.lds.h: Fix up RW_DATA_SECTION definition.

commit c43768cbb7655ea5ff782ae250f6e2ef4297cf98
Merge: 1a8dd307cc0a 746a99a5af60
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Jul 4 07:13:18 2009 +0900

    Merge branch 'master' into for-next
    
    Pull linus#master to merge PER_CPU_DEF_ATTRIBUTES and alpha build fix
    changes.  As alpha in percpu tree uses 'weak' attribute instead of
    inline assembly, there's no need for __used attribute.
    
    Conflicts:
            arch/alpha/include/asm/percpu.h
            arch/mn10300/kernel/vmlinux.lds.S
            include/linux/percpu-defs.h

commit 2a2325e6e8a3782795fb520220c36fd805775972
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Jun 30 11:41:13 2009 -0700

    gcov: fix __ctors_start alignment
    
    The ctors section for each object file is eight byte aligned (on 64 bit).
    However the __ctors_start symbol starts at an arbitrary address dependent
    on the size of the previous sections.
    
    Therefore the linker may add some zeroes after __ctors_start to make sure
    the ctors contents are properly aligned.  However the extra zeroes at the
    beginning aren't expected by the code.  When walking the functions
    pointers contained in there and extra zeroes are added this may result in
    random jumps.  So make sure that the __ctors_start symbol is always
    aligned as well.
    
    Fixes this crash on an allyesconfig on s390:
    
    [    0.582482] Kernel BUG at 0000000000000012 [verbose debug info unavailable]
    [    0.582489] illegal operation: 0001 [#1] SMP DEBUG_PAGEALLOC
    [    0.582496] Modules linked in:
    [    0.582501] CPU: 0 Tainted: G        W  2.6.31-rc1-dirty #273
    [    0.582506] Process swapper (pid: 1, task: 000000003f218000, ksp: 000000003f2238e8)
    [    0.582510] Krnl PSW : 0704200180000000 0000000000000012 (0x12)
    [    0.582518]            R:0 T:1 IO:1 EX:1 Key:0 M:1 W:0 P:0 AS:0 CC:2 PM:0 EA:3
    [    0.582524] Krnl GPRS: 0000000000036727 0000000000000010 0000000000000001 0000000000000001
    [    0.582529]            00000000001dfefa 0000000000000000 0000000000000000 0000000000000040
    [    0.582534]            0000000001fff0f0 0000000001790628 0000000002296048 0000000002296048
    [    0.582540]            00000000020c438e 0000000001786000 0000000002014a66 000000003f223e60
    [    0.582553] Krnl Code:>0000000000000012: 0000                unknown
    [    0.582559]            0000000000000014: 0000                unknown
    [    0.582564]            0000000000000016: 0000                unknown
    [    0.582570]            0000000000000018: 0000                unknown
    [    0.582575]            000000000000001a: 0000                unknown
    [    0.582580]            000000000000001c: 0000                unknown
    [    0.582585]            000000000000001e: 0000                unknown
    [    0.582591]            0000000000000020: 0000                unknown
    [    0.582596] Call Trace:
    [    0.582599] ([<0000000002014a46>] kernel_init+0x622/0x7a0)
    [    0.582607]  [<0000000000113e22>] kernel_thread_starter+0x6/0xc
    [    0.582615]  [<0000000000113e1c>] kernel_thread_starter+0x0/0xc
    [    0.582621] INFO: lockdep is turned off.
    [    0.582624] Last Breaking-Event-Address:
    [    0.582627]  [<0000000002014a64>] kernel_init+0x640/0x7a0
    
    Cc: Peter Oberparleiter <oberpar@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 92b73b6140ff..dccdbed05848 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -441,7 +441,8 @@
 	}
 
 #ifdef CONFIG_CONSTRUCTORS
-#define KERNEL_CTORS()	VMLINUX_SYMBOL(__ctors_start) = .; \
+#define KERNEL_CTORS()	. = ALIGN(8);			   \
+			VMLINUX_SYMBOL(__ctors_start) = .; \
 			*(.ctors)			   \
 			VMLINUX_SYMBOL(__ctors_end) = .;
 #else

commit 39a449d96ac3db9b6d498b6ffbf4c763746d5e8b
Author: Tim Abbott <tabbott@ksplice.com>
Date:   Tue Jun 23 18:53:15 2009 -0400

    asm-generic/vmlinux.lds.h: shuffle INIT_TASK* macro names in vmlinux.lds.h
    
    We recently added a INIT_TASK(align) in include/asm-generic/vmlinux.lds.h,
    but there is already a macro INIT_TASK in include/linux/init_task.h, which
    is quite confusing.  We should switch the macro in the linker script to
    INIT_TASK_DATA. (Sorry that I missed this in reviewing the patch).  Since
    the macros are new, there is only one user of the INIT_TASK in
    vmlinux.lds.h, arch/mn10300/kernel/vmlinux.lds.S.
    
    However, we are currently using INIT_TASK_DATA for laying down an entire
    .data.init_task section.  So rename that to INIT_TASK_DATA_SECTION.
    
    I would be worried about changing the meaning of INIT_TASK_DATA, but the
    old INIT_TASK_DATA implementation had no users, and in fact if anyone had
    tried to use it, it would have failed to compile because it didn't pass
    the alignment to the old INIT_TASK.
    
    Signed-off-by: Tim Abbott <tabbott@ksplice.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jesper Nilsson <Jesper.Nilsson@axis.com
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f92e730695c8..720af4c72206 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -191,7 +191,7 @@
 	. = ALIGN(align);						\
 	*(.data.cacheline_aligned)
 
-#define INIT_TASK(align)						\
+#define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
 	*(.data.init_task)
 
@@ -434,10 +434,10 @@
 /*
  * Init task
  */
-#define INIT_TASK_DATA(align)						\
+#define INIT_TASK_DATA_SECTION(align)					\
 	. = ALIGN(align);						\
 	.data.init_task : {						\
-		INIT_TASK						\
+		INIT_TASK_DATA(align)					\
 	}
 
 #ifdef CONFIG_CONSTRUCTORS
@@ -707,7 +707,7 @@
 #define RW_DATA_SECTION(cacheline, pagealigned, inittask)		\
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
-		INIT_TASK(inittask)					\
+		INIT_TASK_DATA(inittask)				\
 		CACHELINE_ALIGNED_DATA(cacheline)			\
 		READ_MOSTLY_DATA(cacheline)				\
 		DATA_DATA						\

commit 73f1d9391a6aa72efdcea2f302ee7bfcd313c631
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Jun 24 01:04:36 2009 +0900

    asm-generic/vmlinux.lds.h: Fix up RW_DATA_SECTION definition.
    
    RW_DATA_SECTION is defined to take 4 different alignment parameters,
    while NOSAVE_DATA currently uses a fixed PAGE_SIZE alignment as noted
    in the comments.
    
    There are presently no in-tree users of this at present, and I just
    stumbled across this while implementing the simplified script on a new
    architecture port, which subsequently resulted in a syntax error.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 92b73b6140ff..f92e730695c8 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -704,7 +704,7 @@
  * matches the requirment of PAGE_ALIGNED_DATA.
  *
  * use 0 as page_align if page_aligned data is not used */
-#define RW_DATA_SECTION(cacheline, nosave, pagealigned, inittask)	\
+#define RW_DATA_SECTION(cacheline, pagealigned, inittask)		\
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
 		INIT_TASK(inittask)					\
@@ -712,7 +712,7 @@
 		READ_MOSTLY_DATA(cacheline)				\
 		DATA_DATA						\
 		CONSTRUCTORS						\
-		NOSAVE_DATA(nosave)					\
+		NOSAVE_DATA						\
 		PAGE_ALIGNED_DATA(pagealigned)				\
 	}
 

commit 405d967dc70002991f8fc35c20e0d3cbc7614f63
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jun 24 15:13:38 2009 +0900

    linker script: throw away .discard section
    
    x86 throws away .discard section but no other archs do.  Also,
    .discard is not thrown away while linking modules.  Make every arch
    and module linking throw it away.  This will be used to define dummy
    variables for percpu declarations and definitions.
    
    This patch is based on Ivan Kokshaysky's alpha percpu patch.
    
    [ Impact: always throw away everything in .discard ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Bryan Wu <cooloney@kernel.org>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 55413e568f07..a19120c4e109 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -628,6 +628,14 @@
 #define INITRAMFS
 #endif
 
+#define DISCARDS							\
+	/DISCARD/ : {							\
+	EXIT_TEXT							\
+	EXIT_DATA							\
+	*(.exitcall.exit)						\
+	*(.discard)							\
+	}
+
 /**
  * PERCPU_VADDR - define output section for percpu area
  * @vaddr: explicit base address (optional)

commit eadfe21989d728b5af936487627b4e288bd805f8
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 22 15:32:31 2009 +0100

    LDSCRIPT: Name INIT_RAM_FS consistently
    
    In asm-generic/vmlinux.lds.h, name INIT_RAM_FS consistently, no matter the
    setting of CONFIG_BLK_DEV_INITRD.  This corrects:
    
            commit ef53dae8658cf0e93d380983824a661067948d87
            Author: Sam Ravnborg <sam@ravnborg.org>
            Date:   Sun Jun 7 20:46:37 2009 +0200
            Subject: Improve vmlinux.lds.h support for arch specific linker scripts
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 55413e568f07..92b73b6140ff 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -625,7 +625,7 @@
 	*(.init.ramfs)							\
 	VMLINUX_SYMBOL(__initramfs_end) = .;
 #else
-#define INITRAMFS
+#define INIT_RAM_FS
 #endif
 
 /**

commit b99b87f70c7785ab1e253c6220f4b0b57ce3a7f7
Author: Peter Oberparleiter <oberpar@linux.vnet.ibm.com>
Date:   Wed Jun 17 16:28:03 2009 -0700

    kernel: constructor support
    
    Call constructors (gcc-generated initcall-like functions) during kernel
    start and module load.  Constructors are e.g.  used for gcov data
    initialization.
    
    Disable constructor support for usermode Linux to prevent conflicts with
    host glibc.
    
    Signed-off-by: Peter Oberparleiter <oberpar@linux.vnet.ibm.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Li Wei <W.Li@Sun.COM>
    Cc: Michael Ellerman <michaele@au1.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Heiko Carstens <heicars2@linux.vnet.ibm.com>
    Cc: Martin Schwidefsky <mschwid2@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6bdba10fef4a..55413e568f07 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -440,12 +440,21 @@
 		INIT_TASK						\
 	}
 
+#ifdef CONFIG_CONSTRUCTORS
+#define KERNEL_CTORS()	VMLINUX_SYMBOL(__ctors_start) = .; \
+			*(.ctors)			   \
+			VMLINUX_SYMBOL(__ctors_end) = .;
+#else
+#define KERNEL_CTORS()
+#endif
+
 /* init and exit section handling */
 #define INIT_DATA							\
 	*(.init.data)							\
 	DEV_DISCARD(init.data)						\
 	CPU_DISCARD(init.data)						\
 	MEM_DISCARD(init.data)						\
+	KERNEL_CTORS()							\
 	*(.init.rodata)							\
 	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.rodata)					\

commit 45e3e1935e2857c54783291107d33323b3ef33c8
Merge: cf5046323ea2 3f8d9ced7746
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 14 14:12:18 2009 -0700

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-next
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-next: (53 commits)
      .gitignore: ignore *.lzma files
      kbuild: add generic --set-str option to scripts/config
      kbuild: simplify argument loop in scripts/config
      kbuild: handle non-existing options in scripts/config
      kallsyms: generalize text region handling
      kallsyms: support kernel symbols in Blackfin on-chip memory
      documentation: make version fix
      kbuild: fix a compile warning
      gitignore: Add GNU GLOBAL files to top .gitignore
      kbuild: fix delay in setlocalversion on readonly source
      README: fix misleading pointer to the defconf directory
      vmlinux.lds.h update
      kernel-doc: cleanup perl script
      Improve vmlinux.lds.h support for arch specific linker scripts
      kbuild: fix headers_exports with boolean expression
      kbuild/headers_check: refine extern check
      kbuild: fix "Argument list too long" error for "make headers_check",
      ignore *.patch files
      Remove bashisms from scripts
      menu: fix embedded menu presentation
      ...

commit 7923f90fffa8746f6457d4eea2109fd3d6414189
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jun 14 22:10:41 2009 +0200

    vmlinux.lds.h update
    
    Updated after review by Tim Abbott.
    - Use HEAD_TEXT_SECTION
    - Drop use of section-names.h and delete file
    - Introduce EXIT_CALL
    
    Deleting section-names.h required a few simple
    updates of init.h
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Tim Abbott <tabbott@ksplice.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index fba42236e942..7381f701f3f3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -12,7 +12,7 @@
  * {
  *	. = START;
  *	__init_begin = .;
- *	HEAD_SECTION
+ *	HEAD_TEXT_SECTION
  *	INIT_TEXT_SECTION(PAGE_SIZE)
  *	INIT_DATA_SECTION(...)
  *	PERCPU(PAGE_SIZE)
@@ -38,7 +38,7 @@
  *	/DISCARD/ : {
  *		EXIT_TEXT
  *		EXIT_DATA
- *		*(.exitcall.exit)
+ *		EXIT_CALL
  *	}
  *	STABS_DEBUG
  *	DWARF_DEBUG
@@ -52,7 +52,6 @@
  * Examples are: [__initramfs_start, __initramfs_end] for initramfs and
  *               [__nosave_begin, __nosave_end] for the nosave data
  */
- #include <linux/section-names.h>
 
 #ifndef LOAD_OFFSET
 #define LOAD_OFFSET 0
@@ -414,9 +413,9 @@
 #endif
 
 /* Section used for early init (in .S files) */
-#define HEAD_TEXT  *(HEAD_TEXT_SECTION)
+#define HEAD_TEXT  *(.head.text)
 
-#define HEAD_SECTION							\
+#define HEAD_TEXT_SECTION							\
 	.head.text : AT(ADDR(.head.text) - LOAD_OFFSET) {		\
 		HEAD_TEXT						\
 	}
@@ -473,6 +472,9 @@
 	CPU_DISCARD(exit.text)						\
 	MEM_DISCARD(exit.text)
 
+#define EXIT_CALL							\
+	*(.exitcall.exit)
+
 /*
  * bss (Block Started by Symbol) - uninitialized data
  * zeroed during startup
@@ -692,7 +694,7 @@
  * NOSAVE_DATA starts and ends with a PAGE_SIZE alignment which
  * matches the requirment of PAGE_ALIGNED_DATA.
  *
-/* use 0 as page_align if page_aligned data is not used */
+ * use 0 as page_align if page_aligned data is not used */
 #define RW_DATA_SECTION(cacheline, nosave, pagealigned, inittask)	\
 	. = ALIGN(PAGE_SIZE);						\
 	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
@@ -726,4 +728,5 @@
 #define BSS_SECTION(sbss_align, bss_align)				\
 	SBSS								\
 	BSS(bss_align)							\
-	. = ALIGN(4);							\
+	. = ALIGN(4);
+

commit ef53dae8658cf0e93d380983824a661067948d87
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jun 7 20:46:37 2009 +0200

    Improve vmlinux.lds.h support for arch specific linker scripts
    
    To support alingment of the individual architecture specific linker scripts
    provide a set of general definitions in vmlinux.lds.h
    
    With these definitions applied the diverse linekr scripts can be reduced
    in line count and their readability are improved - IMO.
    
    A sample linker script is included to give the preferred
    order of the sections for the architectures that do not
    have any special requirments.
    
    These definitions are also a first step towards eventual
    support for -ffunction-sections.
    The definitions makes it much easier to do a global
    renaming of section names - but the main purpose is
    to clean up the linker scripts.
    
    Tim Aboot has provided a lot of inputs to improve
    the definitions - all faults are mine.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Tim Abbott <tabbott@mit.edu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 3edb11499743..fba42236e942 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -1,4 +1,58 @@
-#include <linux/section-names.h>
+/*
+ * Helper macros to support writing architecture specific
+ * linker scripts.
+ *
+ * A minimal linker scripts has following content:
+ * [This is a sample, architectures may have special requiriements]
+ *
+ * OUTPUT_FORMAT(...)
+ * OUTPUT_ARCH(...)
+ * ENTRY(...)
+ * SECTIONS
+ * {
+ *	. = START;
+ *	__init_begin = .;
+ *	HEAD_SECTION
+ *	INIT_TEXT_SECTION(PAGE_SIZE)
+ *	INIT_DATA_SECTION(...)
+ *	PERCPU(PAGE_SIZE)
+ *	__init_end = .;
+ *
+ *	_stext = .;
+ *	TEXT_SECTION = 0
+ *	_etext = .;
+ *
+ *      _sdata = .;
+ *	RO_DATA_SECTION(PAGE_SIZE)
+ *	RW_DATA_SECTION(...)
+ *	_edata = .;
+ *
+ *	EXCEPTION_TABLE(...)
+ *	NOTES
+ *
+ *	__bss_start = .;
+ *	BSS_SECTION(0, 0)
+ *	__bss_stop = .;
+ *	_end = .;
+ *
+ *	/DISCARD/ : {
+ *		EXIT_TEXT
+ *		EXIT_DATA
+ *		*(.exitcall.exit)
+ *	}
+ *	STABS_DEBUG
+ *	DWARF_DEBUG
+ * }
+ *
+ * [__init_begin, __init_end] is the init section that may be freed after init
+ * [_stext, _etext] is the text section
+ * [_sdata, _edata] is the data section
+ *
+ * Some of the included output section have their own set of constants.
+ * Examples are: [__initramfs_start, __initramfs_end] for initramfs and
+ *               [__nosave_begin, __nosave_end] for the nosave data
+ */
+ #include <linux/section-names.h>
 
 #ifndef LOAD_OFFSET
 #define LOAD_OFFSET 0
@@ -116,7 +170,36 @@
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()
 
-#define RO_DATA(align)							\
+/*
+ * Data section helpers
+ */
+#define NOSAVE_DATA							\
+	. = ALIGN(PAGE_SIZE);						\
+	VMLINUX_SYMBOL(__nosave_begin) = .;				\
+	*(.data.nosave)							\
+	. = ALIGN(PAGE_SIZE);						\
+	VMLINUX_SYMBOL(__nosave_end) = .;
+
+#define PAGE_ALIGNED_DATA(page_align)					\
+	. = ALIGN(page_align);						\
+	*(.data.page_aligned)
+
+#define READ_MOSTLY_DATA(align)						\
+	. = ALIGN(align);						\
+	*(.data.read_mostly)
+
+#define CACHELINE_ALIGNED_DATA(align)					\
+	. = ALIGN(align);						\
+	*(.data.cacheline_aligned)
+
+#define INIT_TASK(align)						\
+	. = ALIGN(align);						\
+	*(.data.init_task)
+
+/*
+ * Read only Data
+ */
+#define RO_DATA_SECTION(align)						\
 	. = ALIGN((align));						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
@@ -270,9 +353,10 @@
 	}								\
 	. = ALIGN((align));
 
-/* RODATA provided for backward compatibility.
+/* RODATA & RO_DATA provided for backward compatibility.
  * All archs are supposed to use RO_DATA() */
-#define RODATA RO_DATA(4096)
+#define RODATA          RO_DATA_SECTION(4096)
+#define RO_DATA(align)  RO_DATA_SECTION(align)
 
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
@@ -332,6 +416,31 @@
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  *(HEAD_TEXT_SECTION)
 
+#define HEAD_SECTION							\
+	.head.text : AT(ADDR(.head.text) - LOAD_OFFSET) {		\
+		HEAD_TEXT						\
+	}
+
+/*
+ * Exception table
+ */
+#define EXCEPTION_TABLE(align)						\
+	. = ALIGN(align);						\
+	__ex_table : AT(ADDR(__ex_table) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start___ex_table) = .;			\
+		*(__ex_table)						\
+		VMLINUX_SYMBOL(__stop___ex_table) = .;			\
+	}
+
+/*
+ * Init task
+ */
+#define INIT_TASK_DATA(align)						\
+	. = ALIGN(align);						\
+	.data.init_task : {						\
+		INIT_TASK						\
+	}
+
 /* init and exit section handling */
 #define INIT_DATA							\
 	*(.init.data)							\
@@ -364,9 +473,32 @@
 	CPU_DISCARD(exit.text)						\
 	MEM_DISCARD(exit.text)
 
-		/* DWARF debug sections.
-		Symbols in the DWARF debugging sections are relative to
-		the beginning of the section so we begin them at 0.  */
+/*
+ * bss (Block Started by Symbol) - uninitialized data
+ * zeroed during startup
+ */
+#define SBSS								\
+	.sbss : AT(ADDR(.sbss) - LOAD_OFFSET) {				\
+		*(.sbss)						\
+		*(.scommon)						\
+	}
+
+#define BSS(bss_align)							\
+	. = ALIGN(bss_align);						\
+	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {				\
+		VMLINUX_SYMBOL(__bss_start) = .;			\
+		*(.bss.page_aligned)					\
+		*(.dynbss)						\
+		*(.bss)							\
+		*(COMMON)						\
+		VMLINUX_SYMBOL(__bss_stop) = .;				\
+	}
+
+/*
+ * DWARF debug sections.
+ * Symbols in the DWARF debugging sections are relative to
+ * the beginning of the section so we begin them at 0.
+ */
 #define DWARF_DEBUG							\
 		/* DWARF 1 */						\
 		.debug          0 : { *(.debug) }			\
@@ -433,6 +565,12 @@
 		VMLINUX_SYMBOL(__stop_notes) = .;			\
 	}
 
+#define INIT_SETUP(initsetup_align)					\
+		. = ALIGN(initsetup_align);				\
+		VMLINUX_SYMBOL(__setup_start) = .;			\
+		*(.init.setup)						\
+		VMLINUX_SYMBOL(__setup_end) = .;
+
 #define INITCALLS							\
 	*(.initcallearly.init)						\
 	VMLINUX_SYMBOL(__early_initcall_end) = .;			\
@@ -454,6 +592,31 @@
   	*(.initcall7.init)						\
   	*(.initcall7s.init)
 
+#define INIT_CALLS							\
+		VMLINUX_SYMBOL(__initcall_start) = .;			\
+		INITCALLS						\
+		VMLINUX_SYMBOL(__initcall_end) = .;
+
+#define CON_INITCALL							\
+		VMLINUX_SYMBOL(__con_initcall_start) = .;		\
+		*(.con_initcall.init)					\
+		VMLINUX_SYMBOL(__con_initcall_end) = .;
+
+#define SECURITY_INITCALL						\
+		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
+		*(.security_initcall.init)				\
+		VMLINUX_SYMBOL(__security_initcall_end) = .;
+
+#ifdef CONFIG_BLK_DEV_INITRD
+#define INIT_RAM_FS							\
+	. = ALIGN(PAGE_SIZE);						\
+	VMLINUX_SYMBOL(__initramfs_start) = .;				\
+	*(.init.ramfs)							\
+	VMLINUX_SYMBOL(__initramfs_end) = .;
+#else
+#define INITRAMFS
+#endif
+
 /**
  * PERCPU_VADDR - define output section for percpu area
  * @vaddr: explicit base address (optional)
@@ -510,3 +673,57 @@
 		*(.data.percpu.shared_aligned)				\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	}
+
+
+/*
+ * Definition of the high level *_SECTION macros
+ * They will fit only a subset of the architectures
+ */
+
+
+/*
+ * Writeable data.
+ * All sections are combined in a single .data section.
+ * The sections following CONSTRUCTORS are arranged so their
+ * typical alignment matches.
+ * A cacheline is typical/always less than a PAGE_SIZE so
+ * the sections that has this restriction (or similar)
+ * is located before the ones requiring PAGE_SIZE alignment.
+ * NOSAVE_DATA starts and ends with a PAGE_SIZE alignment which
+ * matches the requirment of PAGE_ALIGNED_DATA.
+ *
+/* use 0 as page_align if page_aligned data is not used */
+#define RW_DATA_SECTION(cacheline, nosave, pagealigned, inittask)	\
+	. = ALIGN(PAGE_SIZE);						\
+	.data : AT(ADDR(.data) - LOAD_OFFSET) {				\
+		INIT_TASK(inittask)					\
+		CACHELINE_ALIGNED_DATA(cacheline)			\
+		READ_MOSTLY_DATA(cacheline)				\
+		DATA_DATA						\
+		CONSTRUCTORS						\
+		NOSAVE_DATA(nosave)					\
+		PAGE_ALIGNED_DATA(pagealigned)				\
+	}
+
+#define INIT_TEXT_SECTION(inittext_align)				\
+	. = ALIGN(inittext_align);					\
+	.init.text : AT(ADDR(.init.text) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(_sinittext) = .;				\
+		INIT_TEXT						\
+		VMLINUX_SYMBOL(_einittext) = .;				\
+	}
+
+#define INIT_DATA_SECTION(initsetup_align)				\
+	.init.data : AT(ADDR(.init.data) - LOAD_OFFSET) {		\
+		INIT_DATA						\
+		INIT_SETUP(initsetup_align)				\
+		INIT_CALLS						\
+		CON_INITCALL						\
+		SECURITY_INITCALL					\
+		INIT_RAM_FS						\
+	}
+
+#define BSS_SECTION(sbss_align, bss_align)				\
+	SBSS								\
+	BSS(bss_align)							\
+	. = ALIGN(4);							\

commit fd6c3a8dc44329d3aff9a578b5120982f63711ee
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Mar 12 10:58:33 2009 +0000

    initconst adjustments
    
    - add .init.rodata to INIT_DATA, and group all initconst flavors
      together
    - move strings generated from __setup_param() into .init.rodata
    - add .*init.rodata to modpost's sets of init sections
    - make modpost warn about references between meminit and cpuinit
      as well as memexit and cpuexit sections (as CPU and memory
      hotplug are independently selectable features)
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 89853bcd27a6..3edb11499743 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -336,10 +336,11 @@
 #define INIT_DATA							\
 	*(.init.data)							\
 	DEV_DISCARD(init.data)						\
-	DEV_DISCARD(init.rodata)					\
 	CPU_DISCARD(init.data)						\
-	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.data)						\
+	*(.init.rodata)							\
+	DEV_DISCARD(init.rodata)					\
+	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.rodata)
 
 #define INIT_TEXT							\

commit 44347d947f628060b92449702071bfe1d31dfb75
Merge: d94fc523f3c3 413f81eba35d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:17:13 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: tracing/core was on a .30-rc1 base and was missing out on
                  on a handful of tracing fixes present in .30-rc5-almost.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 27b1833279995e7c290a40cac4ef36ccea7e9283
Author: Tim Abbott <tabbott@MIT.EDU>
Date:   Mon Apr 27 14:02:27 2009 -0400

    Remove unused support code for refok sections.
    
    The old refok sections
    
      .text.init.refok
      .data.init.refok
      .exit.text.refok
    
    have been deprecated since commit
    312b1485fb509c9bc32eda28ad29537896658cb8.  After the other patches in
    this patch series nothing is put in these sections, so clean things up
    by eliminating all the remaining references to them.
    
    Signed-off-by: Tim Abbott <tabbott@mit.edu>
    Acked-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index eaa06ef6f7d9..89853bcd27a6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -90,7 +90,6 @@
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
-	*(.data.init.refok)						\
 	*(.ref.data)							\
 	DEV_KEEP(init.data)						\
 	DEV_KEEP(exit.data)						\
@@ -289,8 +288,6 @@
 		*(.text.hot)						\
 		*(.text)						\
 		*(.ref.text)						\
-		*(.text.init.refok)					\
-		*(.exit.text.refok)					\
 	DEV_KEEP(init.text)						\
 	DEV_KEEP(exit.text)						\
 	CPU_KEEP(init.text)						\

commit c80d471a476b6d6fe0bc1fd25293c24c66b7aaaf
Author: Tim Abbott <tabbott@MIT.EDU>
Date:   Sat Apr 25 22:10:56 2009 -0400

    Add new HEAD_TEXT_SECTION macro.
    
    This patch is preparation for replacing all uses of ".head.text" or
    ".text.head" in the kernel with macros, so that the section name can
    later be changed without having to touch a lot of the kernel.
    
    Since some linker scripts do more complex things than referencing
    HEAD_TEXT, we add a HEAD_TEXT_SECTION macro that just contains the
    actual name.
    
    I've defined HEAD_TEXT_SECTION in a new header,
    include/linux/section-names.h, so that this section name only needs to
    appear in one place.  I anticipate creating similar macro structures
    for a number of other section names.
    
    The long-term goal here is to be able to change the kernel's magic
    section names to those that are compatible with -ffunction-sections
    -fdata-sections.  This requires renaming all magic sections with names
    of the form ".text.foo".
    
    Signed-off-by: Tim Abbott <tabbott@mit.edu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7fa660fd449c..eaa06ef6f7d9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -1,3 +1,5 @@
+#include <linux/section-names.h>
+
 #ifndef LOAD_OFFSET
 #define LOAD_OFFSET 0
 #endif
@@ -331,7 +333,7 @@
 #endif
 
 /* Section used for early init (in .S files) */
-#define HEAD_TEXT  *(.head.text)
+#define HEAD_TEXT  *(HEAD_TEXT_SECTION)
 
 /* init and exit section handling */
 #define INIT_DATA							\

commit 5f77a88b3f8268b11940b51d2e03d26a663ceb90
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Wed Apr 8 03:14:01 2009 -0500

    tracing/infrastructure: separate event tracer from event support
    
    Add a new config option, CONFIG_EVENT_TRACING that gets selected
    when CONFIG_TRACING is selected and adds everything needed by the stuff
    in trace_export - basically all the event tracing support needed by e.g.
    bprint, minus the actual events, which are only included if
    CONFIG_EVENT_TRACER is selected.
    
    So CONFIG_EVENT_TRACER can be used to turn on or off the generated events
    (what I think of as the 'event tracer'), while CONFIG_EVENT_TRACING turns
    on or off the base event tracing support used by both the event tracer and
    the other things such as bprint that can't be configured out.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    LKML-Reference: <1239178441.10295.34.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7fa660fd449c..7e9b1e9f711c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -61,7 +61,7 @@
 #define BRANCH_PROFILE()
 #endif
 
-#ifdef CONFIG_EVENT_TRACER
+#ifdef CONFIG_EVENT_TRACING
 #define FTRACE_EVENTS()	VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
 			*(_ftrace_events)				\
 			VMLINUX_SYMBOL(__stop_ftrace_events) = .;

commit 8302294f43250dc337108c51882a6007f2b1e2e0
Merge: 4fe70410d9a2 2e572895bf32
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 1 21:54:19 2009 +0200

    Merge branch 'tracing/core-v2' into tracing-for-linus
    
    Conflicts:
            include/linux/slub_def.h
            lib/Kconfig.debug
            mm/slob.c
            mm/slub.c

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e9d376f0fa66bd630fe27403669c6ae6c22a868f
Author: Jason Baron <jbaron@redhat.com>
Date:   Thu Feb 5 11:51:38 2009 -0500

    dynamic debug: combine dprintk and dynamic printk
    
    This patch combines Greg Bank's dprintk() work with the existing dynamic
    printk patchset, we are now calling it 'dynamic debug'.
    
    The new feature of this patchset is a richer /debugfs control file interface,
    (an example output from my system is at the bottom), which allows fined grained
    control over the the debug output. The output can be controlled by function,
    file, module, format string, and line number.
    
    for example, enabled all debug messages in module 'nf_conntrack':
    
    echo -n 'module nf_conntrack +p' > /mnt/debugfs/dynamic_debug/control
    
    to disable them:
    
    echo -n 'module nf_conntrack -p' > /mnt/debugfs/dynamic_debug/control
    
    A further explanation can be found in the documentation patch.
    
    Signed-off-by: Greg Banks <gnb@sgi.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c61fab1dd2f8..aca40b93bd28 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -80,6 +80,11 @@
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
+	/* implement dynamic printk debug */				\
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__start___verbose) = .;                          \
+	*(__verbose)                                                    \
+	VMLINUX_SYMBOL(__stop___verbose) = .;				\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()
 
@@ -309,15 +314,7 @@
 	CPU_DISCARD(init.data)						\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.data)						\
-	MEM_DISCARD(init.rodata)					\
-	/* implement dynamic printk debug */				\
-	VMLINUX_SYMBOL(__start___verbose_strings) = .;                  \
-	*(__verbose_strings)                                            \
-	VMLINUX_SYMBOL(__stop___verbose_strings) = .;                   \
-	. = ALIGN(8);							\
-	VMLINUX_SYMBOL(__start___verbose) = .;                          \
-	*(__verbose)                                                    \
-	VMLINUX_SYMBOL(__stop___verbose) = .;
+	MEM_DISCARD(init.rodata)
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit bed1ffca022cc876fb83161d26670e9b5d3cf36b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 13 15:42:11 2009 +0100

    tracing/syscalls: core infrastructure for syscalls tracing, enhancements
    
    Impact: new feature
    
    This adds the generic support for syscalls tracing. This is
    currently exploited through a devoted tracer but other tracing
    engines can use it. (They just have to play with
    {start,stop}_ftrace_syscalls() and use the display callbacks
    unless they want to override them.)
    
    The syscalls prototypes definitions are abused here to steal
    some metadata informations:
    
    - syscall name, param types, param names, number of params
    
    The syscall addr is not directly saved during this definition
    because we don't know if its prototype is available in the
    namespace. But we don't really need it. The arch has just to
    build a function able to resolve the syscall number to its
    metadata struct.
    
    The current tracer prints the syscall names, parameters names
    and values (and their types optionally). Currently the value is
    a raw hex but higher level values diplaying is on my TODO list.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1236955332-10133-2-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0e0f39be6c8b..d3bc3c86df6a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -77,6 +77,14 @@
 #define TRACE_PRINTKS()
 #endif
 
+#ifdef CONFIG_FTRACE_SYSCALLS
+#define TRACE_SYSCALLS() VMLINUX_SYMBOL(__start_syscalls_metadata) = .;	\
+			 *(__syscalls_metadata)				\
+			 VMLINUX_SYMBOL(__stop_syscalls_metadata) = .;
+#else
+#define TRACE_SYSCALLS()
+#endif
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -99,7 +107,8 @@
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
 	TRACE_PRINTKS()							\
-	FTRACE_EVENTS()
+	FTRACE_EVENTS()							\
+	TRACE_SYSCALLS()
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\

commit 12e87e36e0141c08dbc8b2177c93c75fb18ad7e5
Merge: 42b40b3d55f5 c3ffc7a40b7e 7bffc23e56e9 7a203f3b089b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 10 09:56:25 2009 +0100

    Merge branches 'tracing/doc', 'tracing/ftrace', 'tracing/printk' and 'linus' into tracing/core

commit 8a20d84d09ab5d121f989cd99e4fc5f4b49f98ba
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 9 10:09:06 2009 +0100

    tracing: trace_printk() fix, move format array to data section
    
    Impact: fix kernel crash when using trace_printk()
    
    trace_printk_fmt section is defined into the readonly section.
    But we do:
    
            trace_printk_fmt = fmt;
    
    to fill in that table of format strings - which is not read-only.
    Under CONFIG_DEBUG_RODATA=y this crashes ...
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <1236356510-8381-5-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 48ade3168b13..d656b4624024 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -98,6 +98,7 @@
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
 	LIKELY_PROFILE()		       				\
 	BRANCH_PROFILE()						\
+	TRACE_PRINTKS()							\
 	FTRACE_EVENTS()
 
 #define RO_DATA(align)							\
@@ -108,7 +109,6 @@
 		*(__vermagic)		/* Kernel version magic */	\
 		*(__markers_strings)	/* Markers: strings */		\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
-		TRACE_PRINTKS()					\
 	}								\
 									\
 	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\

commit dba58e39ced7af63f2748d12bbb2b4ac83c72391
Merge: 9de36825b321 78ff7fae0455
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Mar 8 16:48:51 2009 +0100

    Merge branches 'tracing/doc', 'tracing/ftrace', 'tracing/printk' and 'tracing/textedit' into tracing/core

commit 1ba28e02a18cbdbea123836f6c98efb09cbf59ec
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 6 17:21:48 2009 +0100

    tracing: add trace_bprintk()
    
    Impact: add a generic printk() for tracing, like trace_printk()
    
    trace_bprintk() uses the infrastructure to record events on ring_buffer.
    
    [ fweisbec@gmail.com: ported to latest -tip, made it work if
      !CONFIG_MODULES, never free the format strings from modules
      because we can't keep track of them and conditionnaly create
      the ftrace format strings section (reported by Steven Rostedt) ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-4-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0add6b28c366..48ade3168b13 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -69,6 +69,14 @@
 #define FTRACE_EVENTS()
 #endif
 
+#ifdef CONFIG_TRACING
+#define TRACE_PRINTKS() VMLINUX_SYMBOL(__start___trace_bprintk_fmt) = .;      \
+			 *(__trace_printk_fmt) /* Trace_printk fmt' pointer */ \
+			 VMLINUX_SYMBOL(__stop___trace_bprintk_fmt) = .;
+#else
+#define TRACE_PRINTKS()
+#endif
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -100,6 +108,7 @@
 		*(__vermagic)		/* Kernel version magic */	\
 		*(__markers_strings)	/* Markers: strings */		\
 		*(__tracepoints_strings)/* Tracepoints: strings */	\
+		TRACE_PRINTKS()					\
 	}								\
 									\
 	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\

commit f0ef03985130287c6c84ebe69416cf790e6cc00e
Merge: 16097439703b 31bbed527e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 16:44:14 2009 +0100

    Merge branch 'x86/core' into tracing/textedit
    
    Conflicts:
            arch/x86/Kconfig
            block/blktrace.c
            kernel/irq/handle.c
    
    Semantic conflict:
            kernel/trace/blktrace.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b77e38aa240c3bd9c55c98b9f7c81541e042eae5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 24 10:21:36 2009 -0500

    tracing: add event trace infrastructure
    
    This patch creates the event tracing infrastructure of ftrace.
    It will create the files:
    
     /debug/tracing/available_events
     /debug/tracing/set_event
    
    The available_events will list the trace points that have been
    registered with the event tracer.
    
    set_events will allow the user to enable or disable an event hook.
    
    example:
    
     # echo sched_wakeup > /debug/tracing/set_event
    
    Will enable the sched_wakeup event (if it is registered).
    
     # echo "!sched_wakeup" >> /debug/tracing/set_event
    
    Will disable the sched_wakeup event (and only that event).
    
     # echo > /debug/tracing/set_event
    
    Will disable all events (notice the '>')
    
     # cat /debug/tracing/available_events > /debug/tracing/set_event
    
    Will enable all registered event hooks.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c61fab1dd2f8..0add6b28c366 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -61,6 +61,14 @@
 #define BRANCH_PROFILE()
 #endif
 
+#ifdef CONFIG_EVENT_TRACER
+#define FTRACE_EVENTS()	VMLINUX_SYMBOL(__start_ftrace_events) = .;	\
+			*(_ftrace_events)				\
+			VMLINUX_SYMBOL(__stop_ftrace_events) = .;
+#else
+#define FTRACE_EVENTS()
+#endif
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -81,7 +89,8 @@
 	*(__tracepoints)						\
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
 	LIKELY_PROFILE()		       				\
-	BRANCH_PROFILE()
+	BRANCH_PROFILE()						\
+	FTRACE_EVENTS()
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\

commit 3ac6cffea4aa18007a454a7442da2855882f403d
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jan 30 16:32:22 2009 +0900

    linker script: use separate simpler definition for PERCPU()
    
    Impact: fix linker screwup on x86_32
    
    Recent x86_64 zerobased patches introduced PERCPU_VADDR() to put
    .data.percpu to a predefined address and re-defined PERCPU() in terms
    of it.  The new macro defined one extra symbol, __per_cpu_load, for
    LMA of the section so that the init data could be accessed.  This new
    symbol introduced the following problems to x86_32.
    
    1. If __per_cpu_load is defined outside of .data.percpu as an absolute
       symbol, relocation generation for relocatable kernel fails due to
       absolute relocation.
    
    2. If __per_cpu_load is put inside .data.percpu with absolute address
       assignment to work around #1, linker gets confused and under
       certain configurations ends up relocating the symbol against
       .data.percpu such that the load address gets added on top of
       already set load address.
    
    As x86_32 doesn't use predefined address for .data.percpu, there's no
    need for it to care about the possibility of __per_cpu_load being
    different from __per_cpu_start.
    
    This patch defines PERCPU() separately so that __per_cpu_load is
    defined inside .data.percpu so that everything is ordinary
    linking-wise.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 53e21f36a802..5406e70aba86 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -445,10 +445,9 @@
  * section in the linker script will go there too.  @phdr should have
  * a leading colon.
  *
- * This macro defines three symbols, __per_cpu_load, __per_cpu_start
- * and __per_cpu_end.  The first one is the vaddr of loaded percpu
- * init data.  __per_cpu_start equals @vaddr and __per_cpu_end is the
- * end offset.
+ * Note that this macros defines __per_cpu_load as an absolute symbol.
+ * If there is no need to put the percpu section at a predetermined
+ * address, use PERCPU().
  */
 #define PERCPU_VADDR(vaddr, phdr)					\
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
@@ -470,7 +469,20 @@
  * Align to @align and outputs output section for percpu area.  This
  * macro doesn't maniuplate @vaddr or @phdr and __per_cpu_load and
  * __per_cpu_start will be identical.
+ *
+ * This macro is equivalent to ALIGN(align); PERCPU_VADDR( , ) except
+ * that __per_cpu_load is defined as a relative symbol against
+ * .data.percpu which is required for relocatable x86_32
+ * configuration.
  */
 #define PERCPU(align)							\
 	. = ALIGN(align);						\
-	PERCPU_VADDR( , )
+	.data.percpu	: AT(ADDR(.data.percpu) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__per_cpu_load) = .;			\
+		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
+		*(.data.percpu.first)					\
+		*(.data.percpu.page_aligned)				\
+		*(.data.percpu)						\
+		*(.data.percpu.shared_aligned)				\
+		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
+	}

commit dba3d36b2f0842ed7f25c33cd3a2ccdb3d0df9db
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jan 29 17:10:12 2009 +0100

    Revert "generic, x86: fix __per_cpu_load relocation"
    
    This reverts commit 5a611268b69f05262936dd177205acbce4471358.
    
    It is causing occasional boot crashes, caused by certain
    linker versions (GNU ld version 2.18.50.0.6-2 20080403) messing up:
    
     82dcc000 D __per_cpu_load
     c16e6000 A __per_cpu_load_abs
    
    The __per_cpu_load value is out of whack. Hpa noticed the following
    detail:
    
      * (gdb) p/x -(0xc16e6000-0x82dcc000)
      * $2 = 0xc16e6000
      * I.e. one is the other << 1
    
    The two symbols should be equal.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f3180a85c66a..53e21f36a802 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -451,18 +451,17 @@
  * end offset.
  */
 #define PERCPU_VADDR(vaddr, phdr)					\
-	VMLINUX_SYMBOL(__per_cpu_load_abs) = .;				\
-	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load_abs)	\
+	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
+	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
 				- LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
-		VMLINUX_SYMBOL(__per_cpu_load) = LOADADDR(.data.percpu) + LOAD_OFFSET;\
 		*(.data.percpu.first)					\
 		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	} phdr								\
-	. = VMLINUX_SYMBOL(__per_cpu_load_abs) + SIZEOF(.data.percpu);
+	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
 
 /**
  * PERCPU - define output section for percpu area, simple version

commit 5a611268b69f05262936dd177205acbce4471358
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 26 08:44:05 2009 -0500

    generic, x86: fix __per_cpu_load relocation
    
    This patch fixes this linker error:
    
     WARNING: Absolute relocations present
     Offset     Info     Type     Sym.Value Sym.Name
     c0a4e07d 00e78001   R_386_32 c0ab0000  __per_cpu_load
    
    Now, __per_cpu_load is a section-relative symbol:
    
     c0aa4000 D __per_cpu_load
     c0aa4000 A __per_cpu_load_abs
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 53e21f36a802..f3180a85c66a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -451,17 +451,18 @@
  * end offset.
  */
 #define PERCPU_VADDR(vaddr, phdr)					\
-	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
-	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
+	VMLINUX_SYMBOL(__per_cpu_load_abs) = .;				\
+	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load_abs)	\
 				- LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
+		VMLINUX_SYMBOL(__per_cpu_load) = LOADADDR(.data.percpu) + LOAD_OFFSET;\
 		*(.data.percpu.first)					\
 		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	} phdr								\
-	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
+	. = VMLINUX_SYMBOL(__per_cpu_load_abs) + SIZEOF(.data.percpu);
 
 /**
  * PERCPU - define output section for percpu area, simple version

commit 6b7c38d55587f43bcd2cbce3a98b1c0826982090
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Jan 19 12:21:28 2009 +0900

    linker script: kill PERCPU_VADDR_PREALLOC()
    
    Impact: cleanup
    
    With .data.percpu.first in place, PERCPU_VADDR_PREALLOC() is no longer
    necessary.  Kill it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 32bbf50d3055..53e21f36a802 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -430,22 +430,10 @@
   	*(.initcall7.init)						\
   	*(.initcall7s.init)
 
-#define PERCPU_PROLOG(vaddr)						\
-	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
-	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
-				- LOAD_OFFSET) {			\
-		VMLINUX_SYMBOL(__per_cpu_start) = .;
-
-#define PERCPU_EPILOG(phdr)						\
-		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
-	} phdr								\
-	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
-
 /**
- * PERCPU_VADDR_PREALLOC - define output section for percpu area with prealloc
+ * PERCPU_VADDR - define output section for percpu area
  * @vaddr: explicit base address (optional)
  * @phdr: destination PHDR (optional)
- * @prealloc: the size of prealloc area
  *
  * Macro which expands to output section for percpu area.  If @vaddr
  * is not blank, it specifies explicit base address and all percpu
@@ -457,40 +445,23 @@
  * section in the linker script will go there too.  @phdr should have
  * a leading colon.
  *
- * If @prealloc is non-zero, the specified number of bytes will be
- * reserved at the start of percpu area.  As the prealloc area is
- * likely to break alignment, this macro puts areas in increasing
- * alignment order.
- *
  * This macro defines three symbols, __per_cpu_load, __per_cpu_start
  * and __per_cpu_end.  The first one is the vaddr of loaded percpu
  * init data.  __per_cpu_start equals @vaddr and __per_cpu_end is the
  * end offset.
  */
-#define PERCPU_VADDR_PREALLOC(vaddr, segment, prealloc)			\
-	PERCPU_PROLOG(vaddr)						\
-		. += prealloc;						\
-		*(.data.percpu)						\
-		*(.data.percpu.shared_aligned)				\
-		*(.data.percpu.page_aligned)				\
-	PERCPU_EPILOG(segment)
-
-/**
- * PERCPU_VADDR - define output section for percpu area
- * @vaddr: explicit base address (optional)
- * @phdr: destination PHDR (optional)
- *
- * Macro which expands to output section for percpu area.  Mostly
- * identical to PERCPU_VADDR_PREALLOC(@vaddr, @phdr, 0) other than
- * using slighly different layout.
- */
 #define PERCPU_VADDR(vaddr, phdr)					\
-	PERCPU_PROLOG(vaddr)						\
+	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
+	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
+				- LOAD_OFFSET) {			\
+		VMLINUX_SYMBOL(__per_cpu_start) = .;			\
 		*(.data.percpu.first)					\
 		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
-	PERCPU_EPILOG(phdr)
+		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
+	} phdr								\
+	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
 
 /**
  * PERCPU - define output section for percpu area, simple version

commit 0bd74fa8e29dcad98f7e8ffe01ec05fb3326abaf
Author: Brian Gerst <brgerst@gmail.com>
Date:   Mon Jan 19 12:21:27 2009 +0900

    percpu: refactor percpu.h
    
    Impact: cleanup
    
    Refactor the DEFINE_PER_CPU_* macros and add .data.percpu.first
    section.
    
    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index aa6b9b1b30b5..32bbf50d3055 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -486,6 +486,7 @@
  */
 #define PERCPU_VADDR(vaddr, phdr)					\
 	PERCPU_PROLOG(vaddr)						\
+		*(.data.percpu.first)					\
 		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\

commit 145cd30bac885dffad9db9d487baad07b68a3d04
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Jan 17 14:42:50 2009 +0900

    linker script: add missing VMLINUX_SYMBOL
    
    The newly added PERCPU_*() macros define and use __per_cpu_load but
    VMLINUX_SYMBOL() was missing from usages causing build failures on
    archs where linker visible symbol is different from C symbols
    (e.g. blackfin).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e53319cf29cb..aa6b9b1b30b5 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -432,13 +432,14 @@
 
 #define PERCPU_PROLOG(vaddr)						\
 	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
-	.data.percpu vaddr : AT(__per_cpu_load - LOAD_OFFSET) {		\
+	.data.percpu vaddr : AT(VMLINUX_SYMBOL(__per_cpu_load)		\
+				- LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__per_cpu_start) = .;
 
 #define PERCPU_EPILOG(phdr)						\
 		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
 	} phdr								\
-	. = __per_cpu_load + SIZEOF(.data.percpu);
+	. = VMLINUX_SYMBOL(__per_cpu_load) + SIZEOF(.data.percpu);
 
 /**
  * PERCPU_VADDR_PREALLOC - define output section for percpu area with prealloc

commit 1a51e3a0aed18767cf2762e95456ecfeb0bca5e6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: fold pda into percpu area on SMP
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    Currently pdas and percpu areas are allocated separately.  %gs points
    to local pda and percpu area can be reached using pda->data_offset.
    This patch folds pda into percpu area.
    
    Due to strange gcc requirement, pda needs to be at the beginning of
    the percpu area so that pda->stack_canary is at %gs:40.  To achieve
    this, a new percpu output section macro - PERCPU_VADDR_PREALLOC() - is
    added and used to reserve pda sized chunk at the start of the percpu
    area.
    
    After this change, for boot cpu, %gs first points to pda in the
    data.init area and later during setup_per_cpu_areas() gets updated to
    point to the actual pda.  This means that setup_per_cpu_areas() need
    to reload %gs for CPU0 while clearing pda area for other cpus as cpu0
    already has modified it when control reaches setup_per_cpu_areas().
    
    This patch also removes now unnecessary get_local_pda() and its call
    sites.
    
    A lot of this patch is taken from Mike Travis' "x86_64: Fold pda into
    per cpu area" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index fc2f55f2dcd6..e53319cf29cb 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -441,9 +441,10 @@
 	. = __per_cpu_load + SIZEOF(.data.percpu);
 
 /**
- * PERCPU_VADDR - define output section for percpu area
+ * PERCPU_VADDR_PREALLOC - define output section for percpu area with prealloc
  * @vaddr: explicit base address (optional)
  * @phdr: destination PHDR (optional)
+ * @prealloc: the size of prealloc area
  *
  * Macro which expands to output section for percpu area.  If @vaddr
  * is not blank, it specifies explicit base address and all percpu
@@ -455,11 +456,33 @@
  * section in the linker script will go there too.  @phdr should have
  * a leading colon.
  *
+ * If @prealloc is non-zero, the specified number of bytes will be
+ * reserved at the start of percpu area.  As the prealloc area is
+ * likely to break alignment, this macro puts areas in increasing
+ * alignment order.
+ *
  * This macro defines three symbols, __per_cpu_load, __per_cpu_start
  * and __per_cpu_end.  The first one is the vaddr of loaded percpu
  * init data.  __per_cpu_start equals @vaddr and __per_cpu_end is the
  * end offset.
  */
+#define PERCPU_VADDR_PREALLOC(vaddr, segment, prealloc)			\
+	PERCPU_PROLOG(vaddr)						\
+		. += prealloc;						\
+		*(.data.percpu)						\
+		*(.data.percpu.shared_aligned)				\
+		*(.data.percpu.page_aligned)				\
+	PERCPU_EPILOG(segment)
+
+/**
+ * PERCPU_VADDR - define output section for percpu area
+ * @vaddr: explicit base address (optional)
+ * @phdr: destination PHDR (optional)
+ *
+ * Macro which expands to output section for percpu area.  Mostly
+ * identical to PERCPU_VADDR_PREALLOC(@vaddr, @phdr, 0) other than
+ * using slighly different layout.
+ */
 #define PERCPU_VADDR(vaddr, phdr)					\
 	PERCPU_PROLOG(vaddr)						\
 		*(.data.percpu.page_aligned)				\

commit 3e5d8f978435bb9ba4dfe3f4514e65e7885db1a9
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 13 20:41:35 2009 +0900

    x86: make percpu symbols zerobased on SMP
    
    [ Based on original patch from Christoph Lameter and Mike Travis. ]
    
    This patch makes percpu symbols zerobased on x86_64 SMP by adding
    PERCPU_VADDR() to vmlinux.lds.h which helps setting explicit vaddr on
    the percpu output section and using it in vmlinux_64.lds.S.  A new
    PHDR is added as existing ones cannot contain sections near address
    zero.  PERCPU_VADDR() also adds a new symbol __per_cpu_load which
    always points to the vaddr of the loaded percpu data.init region.
    
    The following adjustments have been made to accomodate the address
    change.
    
    * code to locate percpu gdt_page in head_64.S is updated to add the
      load address to the gdt_page offset.
    
    * __per_cpu_load is used in places where access to the init data area
      is necessary.
    
    * pda->data_offset is initialized soon after C code is entered as zero
      value doesn't work anymore.
    
    This patch is mostly taken from Mike Travis' "x86_64: Base percpu
    variables at zero" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index c61fab1dd2f8..fc2f55f2dcd6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -430,12 +430,51 @@
   	*(.initcall7.init)						\
   	*(.initcall7s.init)
 
-#define PERCPU(align)							\
-	. = ALIGN(align);						\
-	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
-	.data.percpu  : AT(ADDR(.data.percpu) - LOAD_OFFSET) {		\
+#define PERCPU_PROLOG(vaddr)						\
+	VMLINUX_SYMBOL(__per_cpu_load) = .;				\
+	.data.percpu vaddr : AT(__per_cpu_load - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__per_cpu_start) = .;
+
+#define PERCPU_EPILOG(phdr)						\
+		VMLINUX_SYMBOL(__per_cpu_end) = .;			\
+	} phdr								\
+	. = __per_cpu_load + SIZEOF(.data.percpu);
+
+/**
+ * PERCPU_VADDR - define output section for percpu area
+ * @vaddr: explicit base address (optional)
+ * @phdr: destination PHDR (optional)
+ *
+ * Macro which expands to output section for percpu area.  If @vaddr
+ * is not blank, it specifies explicit base address and all percpu
+ * symbols will be offset from the given address.  If blank, @vaddr
+ * always equals @laddr + LOAD_OFFSET.
+ *
+ * @phdr defines the output PHDR to use if not blank.  Be warned that
+ * output PHDR is sticky.  If @phdr is specified, the next output
+ * section in the linker script will go there too.  @phdr should have
+ * a leading colon.
+ *
+ * This macro defines three symbols, __per_cpu_load, __per_cpu_start
+ * and __per_cpu_end.  The first one is the vaddr of loaded percpu
+ * init data.  __per_cpu_start equals @vaddr and __per_cpu_end is the
+ * end offset.
+ */
+#define PERCPU_VADDR(vaddr, phdr)					\
+	PERCPU_PROLOG(vaddr)						\
 		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
-	}								\
-	VMLINUX_SYMBOL(__per_cpu_end) = .;
+	PERCPU_EPILOG(phdr)
+
+/**
+ * PERCPU - define output section for percpu area, simple version
+ * @align: required alignment
+ *
+ * Align to @align and outputs output section for percpu area.  This
+ * macro doesn't maniuplate @vaddr or @phdr and __per_cpu_load and
+ * __per_cpu_start will be identical.
+ */
+#define PERCPU(align)							\
+	. = ALIGN(align);						\
+	PERCPU_VADDR( , )

commit a0343e823184070f55364d8359f832dcb33c57c7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Dec 9 23:53:16 2008 +0100

    tracing/function-graph-tracer: add a new .irqentry.text section
    
    Impact: let the function-graph-tracer be aware of the irq entrypoints
    
    Add a new .irqentry.text section to store the irq entrypoints functions
    inside the same section. This way, the tracer will be able to signal
    an interrupts triggering on output by recognizing these entrypoints.
    
    Also, make this section recordable for dynamic tracing.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index eba835a2c2cd..c61fab1dd2f8 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -288,6 +288,16 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+#define IRQENTRY_TEXT							\
+		ALIGN_FUNCTION();					\
+		VMLINUX_SYMBOL(__irqentry_text_start) = .;		\
+		*(.irqentry.text)					\
+		VMLINUX_SYMBOL(__irqentry_text_end) = .;
+#else
+#define IRQENTRY_TEXT
+#endif
+
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  *(.head.text)
 

commit 2bcd521a684cc94befbe2ce7d5b613c841b0d304
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 21 01:30:54 2008 -0500

    trace: profile all if conditionals
    
    Impact: feature to profile if statements
    
    This patch adds a branch profiler for all if () statements.
    The results will be found in:
    
      /debugfs/tracing/profile_branch
    
    For example:
    
       miss      hit    %        Function                  File              Line
     ------- ---------  -        --------                  ----              ----
           0        1 100 x86_64_start_reservations      head64.c             127
           0        1 100 copy_bootdata                  head64.c             69
           1        0   0 x86_64_start_kernel            head64.c             111
          32        0   0 set_intr_gate                  desc.h               319
           1        0   0 reserve_ebda_region            head.c               51
           1        0   0 reserve_ebda_region            head.c               47
           0        1 100 reserve_ebda_region            head.c               42
           0        0   X maxcpus                        main.c               165
    
    Miss means the branch was not taken. Hit means the branch was taken.
    The percent is the percentage the branch was taken.
    
    This adds a significant amount of overhead and should only be used
    by those analyzing their system.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8bccb49981e5..eba835a2c2cd 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -53,6 +53,14 @@
 #define LIKELY_PROFILE()
 #endif
 
+#ifdef CONFIG_PROFILE_ALL_BRANCHES
+#define BRANCH_PROFILE()	VMLINUX_SYMBOL(__start_branch_profile) = .;   \
+				*(_ftrace_branch)			      \
+				VMLINUX_SYMBOL(__stop_branch_profile) = .;
+#else
+#define BRANCH_PROFILE()
+#endif
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -72,7 +80,8 @@
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
-	LIKELY_PROFILE()
+	LIKELY_PROFILE()		       				\
+	BRANCH_PROFILE()
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\

commit 45b797492a0758e64dff74e9db70e1f65e0603a5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 21 00:40:40 2008 -0500

    trace: consolidate unlikely and likely profiler
    
    Impact: clean up to make one profiler of like and unlikely tracer
    
    The likely and unlikely profiler prints out the file and line numbers
    of the annotated branches that it is profiling. It shows the number
    of times it was correct or incorrect in its guess. Having two
    different files or sections for that matter to tell us if it was a
    likely or unlikely is pretty pointless. We really only care if
    it was correct or not.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 3b46ae464933..8bccb49981e5 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -46,12 +46,9 @@
 #endif
 
 #ifdef CONFIG_TRACE_BRANCH_PROFILING
-#define LIKELY_PROFILE()	VMLINUX_SYMBOL(__start_likely_profile) = .;   \
-				*(_ftrace_likely)			      \
-				VMLINUX_SYMBOL(__stop_likely_profile) = .;    \
-				VMLINUX_SYMBOL(__start_unlikely_profile) = .; \
-				*(_ftrace_unlikely)			      \
-				VMLINUX_SYMBOL(__stop_unlikely_profile) = .;
+#define LIKELY_PROFILE()	VMLINUX_SYMBOL(__start_annotated_branch_profile) = .; \
+				*(_ftrace_annotated_branch)			      \
+				VMLINUX_SYMBOL(__stop_annotated_branch_profile) = .;
 #else
 #define LIKELY_PROFILE()
 #endif

commit 7e066fb870fcd1025ec3ba7bbde5d541094f4ce1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Nov 14 17:47:47 2008 -0500

    tracepoints: add DECLARE_TRACE() and DEFINE_TRACE()
    
    Impact: API *CHANGE*. Must update all tracepoint users.
    
    Add DEFINE_TRACE() to tracepoints to let them declare the tracepoint
    structure in a single spot for all the kernel. It helps reducing memory
    consumption, especially when declaring a lot of tracepoints, e.g. for
    kmalloc tracing.
    
    *API CHANGE WARNING*: now, DECLARE_TRACE() must be used in headers for
    tracepoint declarations rather than DEFINE_TRACE(). This is the sane way
    to do it. The name previously used was misleading.
    
    Updates scheduler instrumentation to follow this API change.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a5e4ed9baec8..3b46ae464933 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -71,6 +71,7 @@
 	VMLINUX_SYMBOL(__start___markers) = .;				\
 	*(__markers)							\
 	VMLINUX_SYMBOL(__stop___markers) = .;				\
+	. = ALIGN(32);							\
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\

commit 2ed84eeb8808cf3c9f039213ca137ffd7d753f0e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 12 15:24:24 2008 -0500

    trace: rename unlikely profiler to branch profiler
    
    Impact: name change of unlikely tracer and profiler
    
    Ingo Molnar suggested changing the config from UNLIKELY_PROFILE
    to BRANCH_PROFILING. I never did like the "unlikely" name so I
    went one step farther, and renamed all the unlikely configurations
    to a "BRANCH" variant.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e10beb5335c9..a5e4ed9baec8 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -45,7 +45,7 @@
 #define MCOUNT_REC()
 #endif
 
-#ifdef CONFIG_TRACE_UNLIKELY_PROFILE
+#ifdef CONFIG_TRACE_BRANCH_PROFILING
 #define LIKELY_PROFILE()	VMLINUX_SYMBOL(__start_likely_profile) = .;   \
 				*(_ftrace_likely)			      \
 				VMLINUX_SYMBOL(__stop_likely_profile) = .;    \

commit 1f0d69a9fc815db82f15722bf05227190b1d714d
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 12 00:14:39 2008 -0500

    tracing: profile likely and unlikely annotations
    
    Impact: new unlikely/likely profiler
    
    Andrew Morton recently suggested having an in-kernel way to profile
    likely and unlikely macros. This patch achieves that goal.
    
    When configured, every(*) likely and unlikely macro gets a counter attached
    to it. When the condition is hit, the hit and misses of that condition
    are recorded. These numbers can later be retrieved by:
    
      /debugfs/tracing/profile_likely    - All likely markers
      /debugfs/tracing/profile_unlikely  - All unlikely markers.
    
    # cat /debug/tracing/profile_unlikely | head
     correct incorrect  %        Function                  File              Line
     ------- ---------  -        --------                  ----              ----
        2167        0   0 do_arch_prctl                  process_64.c         832
           0        0   0 do_arch_prctl                  process_64.c         804
        2670        0   0 IS_ERR                         err.h                34
       71230     5693   7 __switch_to                    process_64.c         673
       76919        0   0 __switch_to                    process_64.c         639
       43184    33743  43 __switch_to                    process_64.c         624
       12740    64181  83 __switch_to                    process_64.c         594
       12740    64174  83 __switch_to                    process_64.c         590
    
    # cat /debug/tracing/profile_unlikely | \
      awk '{ if ($3 > 25) print $0; }' |head -20
       44963    35259  43 __switch_to                    process_64.c         624
       12762    67454  84 __switch_to                    process_64.c         594
       12762    67447  84 __switch_to                    process_64.c         590
        1478      595  28 syscall_get_error              syscall.h            51
           0     2821 100 syscall_trace_leave            ptrace.c             1567
           0        1 100 native_smp_prepare_cpus        smpboot.c            1237
       86338   265881  75 calc_delta_fair                sched_fair.c         408
      210410   108540  34 calc_delta_mine                sched.c              1267
           0    54550 100 sched_info_queued              sched_stats.h        222
       51899    66435  56 pick_next_task_fair            sched_fair.c         1422
           6       10  62 yield_task_fair                sched_fair.c         982
        7325     2692  26 rt_policy                      sched.c              144
           0     1270 100 pre_schedule_rt                sched_rt.c           1261
        1268    48073  97 pick_next_task_rt              sched_rt.c           884
           0    45181 100 sched_info_dequeued            sched_stats.h        177
           0       15 100 sched_move_task                sched.c              8700
           0       15 100 sched_move_task                sched.c              8690
       53167    33217  38 schedule                       sched.c              4457
           0    80208 100 sched_info_switch              sched_stats.h        270
       30585    49631  61 context_switch                 sched.c              2619
    
    # cat /debug/tracing/profile_likely | awk '{ if ($3 > 25) print $0; }'
       39900    36577  47 pick_next_task                 sched.c              4397
       20824    15233  42 switch_mm                      mmu_context_64.h     18
           0        7 100 __cancel_work_timer            workqueue.c          560
         617    66484  99 clocksource_adjust             timekeeping.c        456
           0   346340 100 audit_syscall_exit             auditsc.c            1570
          38   347350  99 audit_get_context              auditsc.c            732
           0   345244 100 audit_syscall_entry            auditsc.c            1541
          38     1017  96 audit_free                     auditsc.c            1446
           0     1090 100 audit_alloc                    auditsc.c            862
        2618     1090  29 audit_alloc                    auditsc.c            858
           0        6 100 move_masked_irq                migration.c          9
           1      198  99 probe_sched_wakeup             trace_sched_switch.c 58
           2        2  50 probe_wakeup                   trace_sched_wakeup.c 227
           0        2 100 probe_wakeup_sched_switch      trace_sched_wakeup.c 144
        4514     2090  31 __grab_cache_page              filemap.c            2149
       12882   228786  94 mapping_unevictable            pagemap.h            50
           4       11  73 __flush_cpu_slab               slub.c               1466
      627757   330451  34 slab_free                      slub.c               1731
        2959    61245  95 dentry_lru_del_init            dcache.c             153
         946     1217  56 load_elf_binary                binfmt_elf.c         904
         102       82  44 disk_put_part                  genhd.h              206
           1        1  50 dst_gc_task                    dst.c                82
           0       19 100 tcp_mss_split_point            tcp_output.c         1126
    
    As you can see by the above, there's a bit of work to do in rethinking
    the use of some unlikelys and likelys. Note: the unlikely case had 71 hits
    that were more than 25%.
    
    Note:  After submitting my first version of this patch, Andrew Morton
      showed me a version written by Daniel Walker, where I picked up
      the following ideas from:
    
      1)  Using __builtin_constant_p to avoid profiling fixed values.
      2)  Using __FILE__ instead of instruction pointers.
      3)  Using the preprocessor to stop all profiling of likely
           annotations from vsyscall_64.c.
    
    Thanks to Andrew Morton, Arjan van de Ven, Theodore Tso and Ingo Molnar
    for their feed back on this patch.
    
    (*) Not ever unlikely is recorded, those that are used by vsyscalls
     (a few of them) had to have profiling disabled.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Theodore Tso <tytso@mit.edu>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 80744606bad1..e10beb5335c9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -45,6 +45,17 @@
 #define MCOUNT_REC()
 #endif
 
+#ifdef CONFIG_TRACE_UNLIKELY_PROFILE
+#define LIKELY_PROFILE()	VMLINUX_SYMBOL(__start_likely_profile) = .;   \
+				*(_ftrace_likely)			      \
+				VMLINUX_SYMBOL(__stop_likely_profile) = .;    \
+				VMLINUX_SYMBOL(__start_unlikely_profile) = .; \
+				*(_ftrace_unlikely)			      \
+				VMLINUX_SYMBOL(__stop_unlikely_profile) = .;
+#else
+#define LIKELY_PROFILE()
+#endif
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
@@ -62,7 +73,8 @@
 	VMLINUX_SYMBOL(__stop___markers) = .;				\
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\
-	VMLINUX_SYMBOL(__stop___tracepoints) = .;
+	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
+	LIKELY_PROFILE()
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\

commit 92b29b86fe2e183d44eb467e5e74a5f718ef2e43
Merge: b9d7ccf56be1 98d9c66ab074
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:35:07 2008 -0700

    Merge branch 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (131 commits)
      tracing/fastboot: improve help text
      tracing/stacktrace: improve help text
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix bootgraph.pl initcall name regexp
      tracing/fastboot: fix issues and improve output of bootgraph.pl
      tracepoints: synchronize unregister static inline
      tracepoints: tracepoint_synchronize_unregister()
      ftrace: make ftrace_test_p6nop disassembler-friendly
      markers: fix synchronize marker unregister static inline
      tracing/fastboot: add better resolution to initcall debug/tracing
      trace: add build-time check to avoid overrunning hex buffer
      ftrace: fix hex output mode of ftrace
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix printk format typo in boot tracer
      ftrace: return an error when setting a nonexistent tracer
      ftrace: make some tracers reentrant
      ring-buffer: make reentrant
      ring-buffer: move page indexes into page headers
      tracing/fastboot: only trace non-module initcalls
      ftrace: move pc counter in irqtrace
      ...
    
    Manually fix conflicts:
     - init/main.c: initcall tracing
     - kernel/module.c: verbose level vs tracepoints
     - scripts/bootgraph.pl: fallout from cherry-picking commits.

commit 346e15beb5343c2eb8216d820f2ed8f150822b08
Author: Jason Baron <jbaron@redhat.com>
Date:   Tue Aug 12 16:46:19 2008 -0400

    driver core: basic infrastructure for per-module dynamic debug messages
    
    Base infrastructure to enable per-module debug messages.
    
    I've introduced CONFIG_DYNAMIC_PRINTK_DEBUG, which when enabled centralizes
    control of debugging statements on a per-module basis in one /proc file,
    currently, <debugfs>/dynamic_printk/modules. When, CONFIG_DYNAMIC_PRINTK_DEBUG,
    is not set, debugging statements can still be enabled as before, often by
    defining 'DEBUG' for the proper compilation unit. Thus, this patch set has no
    affect when CONFIG_DYNAMIC_PRINTK_DEBUG is not set.
    
    The infrastructure currently ties into all pr_debug() and dev_dbg() calls. That
    is, if CONFIG_DYNAMIC_PRINTK_DEBUG is set, all pr_debug() and dev_dbg() calls
    can be dynamically enabled/disabled on a per-module basis.
    
    Future plans include extending this functionality to subsystems, that define
    their own debug levels and flags.
    
    Usage:
    
    Dynamic debugging is controlled by the debugfs file,
    <debugfs>/dynamic_printk/modules. This file contains a list of the modules that
    can be enabled. The format of the file is as follows:
    
            <module_name> <enabled=0/1>
                    .
                    .
                    .
    
            <module_name> : Name of the module in which the debug call resides
            <enabled=0/1> : whether the messages are enabled or not
    
    For example:
    
            snd_hda_intel enabled=0
            fixup enabled=1
            driver enabled=0
    
    Enable a module:
    
            $echo "set enabled=1 <module_name>" > dynamic_printk/modules
    
    Disable a module:
    
            $echo "set enabled=0 <module_name>" > dynamic_printk/modules
    
    Enable all modules:
    
            $echo "set enabled=1 all" > dynamic_printk/modules
    
    Disable all modules:
    
            $echo "set enabled=0 all" > dynamic_printk/modules
    
    Finally, passing "dynamic_printk" at the command line enables
    debugging for all modules. This mode can be turned off via the above
    disable command.
    
    [gkh: minor cleanups and tweaks to make the build work quietly]
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7440a0dceddb..74c5faf26c05 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -268,7 +268,15 @@
 	CPU_DISCARD(init.data)						\
 	CPU_DISCARD(init.rodata)					\
 	MEM_DISCARD(init.data)						\
-	MEM_DISCARD(init.rodata)
+	MEM_DISCARD(init.rodata)					\
+	/* implement dynamic printk debug */				\
+	VMLINUX_SYMBOL(__start___verbose_strings) = .;                  \
+	*(__verbose_strings)                                            \
+	VMLINUX_SYMBOL(__stop___verbose_strings) = .;                   \
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__start___verbose) = .;                          \
+	*(__verbose)                                                    \
+	VMLINUX_SYMBOL(__stop___verbose) = .;
 
 #define INIT_TEXT							\
 	*(.init.text)							\

commit 8da3821ba5634497da63d58a69e24a97697c4a2b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Aug 14 15:45:07 2008 -0400

    ftrace: create __mcount_loc section
    
    This patch creates a section in the kernel called "__mcount_loc".
    This will hold a list of pointers to the mcount relocation for
    each call site of mcount.
    
    For example:
    
    objdump -dr init/main.o
    [...]
    Disassembly of section .text:
    
    0000000000000000 <do_one_initcall>:
       0:   55                      push   %rbp
    [...]
    000000000000017b <init_post>:
     17b:   55                      push   %rbp
     17c:   48 89 e5                mov    %rsp,%rbp
     17f:   53                      push   %rbx
     180:   48 83 ec 08             sub    $0x8,%rsp
     184:   e8 00 00 00 00          callq  189 <init_post+0xe>
                            185: R_X86_64_PC32      mcount+0xfffffffffffffffc
    [...]
    
    We will add a section to point to each function call.
    
       .section __mcount_loc,"a",@progbits
    [...]
       .quad .text + 0x185
    [...]
    
    The offset to of the mcount call site in init_post is an offset from
    the start of the section, and not the start of the function init_post.
    The mcount relocation is at the call site 0x185 from the start of the
    .text section.
    
      .text + 0x185  == init_post + 0xa
    
    We need a way to add this __mcount_loc section in a way that we do not
    lose the relocations after final link.  The .text section here will
    be attached to all other .text sections after final link and the
    offsets will be meaningless.  We need to keep track of where these
    .text sections are.
    
    To do this, we use the start of the first function in the section.
    do_one_initcall.  We can make a tmp.s file with this function as a reference
    to the start of the .text section.
    
       .section __mcount_loc,"a",@progbits
    [...]
       .quad do_one_initcall + 0x185
    [...]
    
    Then we can compile the tmp.s into a tmp.o
    
      gcc -c tmp.s -o tmp.o
    
    And link it into back into main.o.
    
      ld -r main.o tmp.o -o tmp_main.o
      mv tmp_main.o main.o
    
    But we have a problem.  What happens if the first function in a section
    is not exported, and is a static function. The linker will not let
    the tmp.o use it.  This case exists in main.o as well.
    
    Disassembly of section .init.text:
    
    0000000000000000 <set_reset_devices>:
       0:   55                      push   %rbp
       1:   48 89 e5                mov    %rsp,%rbp
       4:   e8 00 00 00 00          callq  9 <set_reset_devices+0x9>
                            5: R_X86_64_PC32        mcount+0xfffffffffffffffc
    
    The first function in .init.text is a static function.
    
    00000000000000a8 t __setup_set_reset_devices
    000000000000105f t __setup_str_set_reset_devices
    0000000000000000 t set_reset_devices
    
    The lowercase 't' means that set_reset_devices is local and is not exported.
    If we simply try to link the tmp.o with the set_reset_devices we end
    up with two symbols: one local and one global.
    
     .section __mcount_loc,"a",@progbits
     .quad set_reset_devices + 0x10
    
    00000000000000a8 t __setup_set_reset_devices
    000000000000105f t __setup_str_set_reset_devices
    0000000000000000 t set_reset_devices
                     U set_reset_devices
    
    We still have an undefined reference to set_reset_devices, and if we try
    to compile the kernel, we will end up with an undefined reference to
    set_reset_devices, or even worst, it could be exported someplace else,
    and then we will have a reference to the wrong location.
    
    To handle this case, we make an intermediate step using objcopy.
    We convert set_reset_devices into a global exported symbol before linking
    it with tmp.o and set it back afterwards.
    
    00000000000000a8 t __setup_set_reset_devices
    000000000000105f t __setup_str_set_reset_devices
    0000000000000000 T set_reset_devices
    
    00000000000000a8 t __setup_set_reset_devices
    000000000000105f t __setup_str_set_reset_devices
    0000000000000000 T set_reset_devices
    
    00000000000000a8 t __setup_set_reset_devices
    000000000000105f t __setup_str_set_reset_devices
    0000000000000000 t set_reset_devices
    
    Now we have a section in main.o called __mcount_loc that we can place
    somewhere in the kernel using vmlinux.ld.S and access it to convert
    all these locations that call mcount into nops before starting SMP
    and thus, eliminating the need to do this with kstop_machine.
    
    Note, A well documented perl script (scripts/recordmcount.pl) is used
    to do all this in one location.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 3d8e472a09c8..838d9b2a0da1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -37,6 +37,13 @@
 #define MEM_DISCARD(sec) *(.mem##sec)
 #endif
 
+#ifdef CONFIG_FTRACE_MCOUNT_RECORD
+#define MCOUNT_REC()	VMLINUX_SYMBOL(__start_mcount_loc) = .; \
+			*(__mcount_loc)				\
+			VMLINUX_SYMBOL(__stop_mcount_loc) = .;
+#else
+#define MCOUNT_REC()
+#endif
 
 /* .data section */
 #define DATA_DATA							\
@@ -192,6 +199,7 @@
 	/* __*init sections */						\
 	__init_rodata : AT(ADDR(__init_rodata) - LOAD_OFFSET) {		\
 		*(.ref.rodata)						\
+		MCOUNT_REC()						\
 		DEV_KEEP(init.rodata)					\
 		DEV_KEEP(exit.rodata)					\
 		CPU_KEEP(init.rodata)					\

commit 97e1c18e8d17bd87e1e383b2e9d9fc740332c8e2
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Jul 18 12:16:16 2008 -0400

    tracing: Kernel Tracepoints
    
    Implementation of kernel tracepoints. Inspired from the Linux Kernel
    Markers. Allows complete typing verification by declaring both tracing
    statement inline functions and probe registration/unregistration static
    inline functions within the same macro "DEFINE_TRACE". No format string
    is required. See the tracepoint Documentation and Samples patches for
    usage examples.
    
    Taken from the documentation patch :
    
    "A tracepoint placed in code provides a hook to call a function (probe)
    that you can provide at runtime. A tracepoint can be "on" (a probe is
    connected to it) or "off" (no probe is attached). When a tracepoint is
    "off" it has no effect, except for adding a tiny time penalty (checking
    a condition for a branch) and space penalty (adding a few bytes for the
    function call at the end of the instrumented function and adds a data
    structure in a separate section).  When a tracepoint is "on", the
    function you provide is called each time the tracepoint is executed, in
    the execution context of the caller. When the function provided ends its
    execution, it returns to the caller (continuing from the tracepoint
    site).
    
    You can put tracepoints at important locations in the code. They are
    lightweight hooks that can pass an arbitrary number of parameters, which
    prototypes are described in a tracepoint declaration placed in a header
    file."
    
    Addition and removal of tracepoints is synchronized by RCU using the
    scheduler (and preempt_disable) as guarantees to find a quiescent state
    (this is really RCU "classic"). The update side uses rcu_barrier_sched()
    with call_rcu_sched() and the read/execute side uses
    "preempt_disable()/preempt_enable()".
    
    We make sure the previous array containing probes, which has been
    scheduled for deletion by the rcu callback, is indeed freed before we
    proceed to the next update. It therefore limits the rate of modification
    of a single tracepoint to one update per RCU period. The objective here
    is to permit fast batch add/removal of probes on _different_
    tracepoints.
    
    Changelog :
    - Use #name ":" #proto as string to identify the tracepoint in the
      tracepoint table. This will make sure not type mismatch happens due to
      connexion of a probe with the wrong type to a tracepoint declared with
      the same name in a different header.
    - Add tracepoint_entry_free_old.
    - Change __TO_TRACE to get rid of the 'i' iterator.
    
    Masami Hiramatsu <mhiramat@redhat.com> :
    Tested on x86-64.
    
    Performance impact of a tracepoint : same as markers, except that it
    adds about 70 bytes of instructions in an unlikely branch of each
    instrumented function (the for loop, the stack setup and the function
    call). It currently adds a memory read, a test and a conditional branch
    at the instrumentation site (in the hot path). Immediate values will
    eventually change this into a load immediate, test and branch, which
    removes the memory read which will make the i-cache impact smaller
    (changing the memory read for a load immediate removes 3-4 bytes per
    site on x86_32 (depending on mov prefixes), or 7-8 bytes on x86_64, it
    also saves the d-cache hit).
    
    About the performance impact of tracepoints (which is comparable to
    markers), even without immediate values optimizations, tests done by
    Hideo Aoki on ia64 show no regression. His test case was using hackbench
    on a kernel where scheduler instrumentation (about 5 events in code
    scheduler code) was added.
    
    Quoting Hideo Aoki about Markers :
    
    I evaluated overhead of kernel marker using linux-2.6-sched-fixes git
    tree, which includes several markers for LTTng, using an ia64 server.
    
    While the immediate trace mark feature isn't implemented on ia64, there
    is no major performance regression. So, I think that we don't have any
    issues to propose merging marker point patches into Linus's tree from
    the viewpoint of performance impact.
    
    I prepared two kernels to evaluate. The first one was compiled without
    CONFIG_MARKERS. The second one was enabled CONFIG_MARKERS.
    
    I downloaded the original hackbench from the following URL:
    http://devresources.linux-foundation.org/craiger/hackbench/src/hackbench.c
    
    I ran hackbench 5 times in each condition and calculated the average and
    difference between the kernels.
    
        The parameter of hackbench: every 50 from 50 to 800
        The number of CPUs of the server: 2, 4, and 8
    
    Below is the results. As you can see, major performance regression
    wasn't found in any case. Even if number of processes increases,
    differences between marker-enabled kernel and marker- disabled kernel
    doesn't increase. Moreover, if number of CPUs increases, the differences
    doesn't increase either.
    
    Curiously, marker-enabled kernel is better than marker-disabled kernel
    in more than half cases, although I guess it comes from the difference
    of memory access pattern.
    
    * 2 CPUs
    
    Number of | without      | with         | diff     | diff    |
    processes | Marker [Sec] | Marker [Sec] |   [Sec]  |   [%]   |
    --------------------------------------------------------------
           50 |      4.811   |       4.872  |  +0.061  |  +1.27  |
          100 |      9.854   |      10.309  |  +0.454  |  +4.61  |
          150 |     15.602   |      15.040  |  -0.562  |  -3.6   |
          200 |     20.489   |      20.380  |  -0.109  |  -0.53  |
          250 |     25.798   |      25.652  |  -0.146  |  -0.56  |
          300 |     31.260   |      30.797  |  -0.463  |  -1.48  |
          350 |     36.121   |      35.770  |  -0.351  |  -0.97  |
          400 |     42.288   |      42.102  |  -0.186  |  -0.44  |
          450 |     47.778   |      47.253  |  -0.526  |  -1.1   |
          500 |     51.953   |      52.278  |  +0.325  |  +0.63  |
          550 |     58.401   |      57.700  |  -0.701  |  -1.2   |
          600 |     63.334   |      63.222  |  -0.112  |  -0.18  |
          650 |     68.816   |      68.511  |  -0.306  |  -0.44  |
          700 |     74.667   |      74.088  |  -0.579  |  -0.78  |
          750 |     78.612   |      79.582  |  +0.970  |  +1.23  |
          800 |     85.431   |      85.263  |  -0.168  |  -0.2   |
    --------------------------------------------------------------
    
    * 4 CPUs
    
    Number of | without      | with         | diff     | diff    |
    processes | Marker [Sec] | Marker [Sec] |   [Sec]  |   [%]   |
    --------------------------------------------------------------
           50 |      2.586   |       2.584  |  -0.003  |  -0.1   |
          100 |      5.254   |       5.283  |  +0.030  |  +0.56  |
          150 |      8.012   |       8.074  |  +0.061  |  +0.76  |
          200 |     11.172   |      11.000  |  -0.172  |  -1.54  |
          250 |     13.917   |      14.036  |  +0.119  |  +0.86  |
          300 |     16.905   |      16.543  |  -0.362  |  -2.14  |
          350 |     19.901   |      20.036  |  +0.135  |  +0.68  |
          400 |     22.908   |      23.094  |  +0.186  |  +0.81  |
          450 |     26.273   |      26.101  |  -0.172  |  -0.66  |
          500 |     29.554   |      29.092  |  -0.461  |  -1.56  |
          550 |     32.377   |      32.274  |  -0.103  |  -0.32  |
          600 |     35.855   |      35.322  |  -0.533  |  -1.49  |
          650 |     39.192   |      38.388  |  -0.804  |  -2.05  |
          700 |     41.744   |      41.719  |  -0.025  |  -0.06  |
          750 |     45.016   |      44.496  |  -0.520  |  -1.16  |
          800 |     48.212   |      47.603  |  -0.609  |  -1.26  |
    --------------------------------------------------------------
    
    * 8 CPUs
    
    Number of | without      | with         | diff     | diff    |
    processes | Marker [Sec] | Marker [Sec] |   [Sec]  |   [%]   |
    --------------------------------------------------------------
           50 |      2.094   |       2.072  |  -0.022  |  -1.07  |
          100 |      4.162   |       4.273  |  +0.111  |  +2.66  |
          150 |      6.485   |       6.540  |  +0.055  |  +0.84  |
          200 |      8.556   |       8.478  |  -0.078  |  -0.91  |
          250 |     10.458   |      10.258  |  -0.200  |  -1.91  |
          300 |     12.425   |      12.750  |  +0.325  |  +2.62  |
          350 |     14.807   |      14.839  |  +0.032  |  +0.22  |
          400 |     16.801   |      16.959  |  +0.158  |  +0.94  |
          450 |     19.478   |      19.009  |  -0.470  |  -2.41  |
          500 |     21.296   |      21.504  |  +0.208  |  +0.98  |
          550 |     23.842   |      23.979  |  +0.137  |  +0.57  |
          600 |     26.309   |      26.111  |  -0.198  |  -0.75  |
          650 |     28.705   |      28.446  |  -0.259  |  -0.9   |
          700 |     31.233   |      31.394  |  +0.161  |  +0.52  |
          750 |     34.064   |      33.720  |  -0.344  |  -1.01  |
          800 |     36.320   |      36.114  |  -0.206  |  -0.57  |
    --------------------------------------------------------------
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: 'Peter Zijlstra' <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7440a0dceddb..3d8e472a09c8 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -52,7 +52,10 @@
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___markers) = .;				\
 	*(__markers)							\
-	VMLINUX_SYMBOL(__stop___markers) = .;
+	VMLINUX_SYMBOL(__stop___markers) = .;				\
+	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
+	*(__tracepoints)						\
+	VMLINUX_SYMBOL(__stop___tracepoints) = .;
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\
@@ -61,6 +64,7 @@
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
 		*(__markers_strings)	/* Markers: strings */		\
+		*(__tracepoints_strings)/* Tracepoints: strings */	\
 	}								\
 									\
 	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\

commit d3d0ba7b8fb8f57c33207adcb41f40c176148c03
Merge: 9042763808c5 63cc8c751564
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 5 09:24:30 2008 +0200

    Merge commit '63cc8c75156462d4b42cbdd76c293b7eee7ddbfe':
    
      "percpu: introduce DEFINE_PER_CPU_PAGE_ALIGNED() macro"
    
    into x86/core
    
    Conflicts:
            arch/x86/kernel/cpu/common.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c6de002617c199f80f9a2a713dffc263bdc69b81
Author: Yoshinori Sato <ysato@users.sourceforge.jp>
Date:   Thu Jul 31 00:07:29 2008 -0700

    Missing symbol prefix on vmlinux.lds.h
    
    ARCH=h8300:
    
    init/main.c:781: undefined reference to `___early_initcall_end'
    
    Same problem have
    __start___bug_table
    __stop___bug_table
    __tracedata_start
    __tracedata_end
    __per_cpu_start
    __per_cpu_end
    
    When defining a symbol in vmlinux.lds, use the VMLINUX_SYMBOL macro.
    VMLINUX_SYMBOL adds a prefix charactor.
    
    You can't just use straight symbol names in common header files as they
    dont take into consideration weird arch-specific ABI conventions.  in the
    case of Blackfin/h8300, the ABI dictates that any C-visible symbols have
    an underscore prefixed to them.  Thus all symbols in vmlinux.lds.h need to
    be wrapped in VMLINUX_SYMBOL() so that each arch can put hide this magic
    in their own files.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: "Mike Frysinger" <vapier.adi@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6d88a923c945..cb752ba72466 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -333,9 +333,9 @@
 #define BUG_TABLE							\
 	. = ALIGN(8);							\
 	__bug_table : AT(ADDR(__bug_table) - LOAD_OFFSET) {		\
-		__start___bug_table = .;				\
+		VMLINUX_SYMBOL(__start___bug_table) = .;		\
 		*(__bug_table)						\
-		__stop___bug_table = .;					\
+		VMLINUX_SYMBOL(__stop___bug_table) = .;			\
 	}
 #else
 #define BUG_TABLE
@@ -345,9 +345,9 @@
 #define TRACEDATA							\
 	. = ALIGN(4);							\
 	.tracedata : AT(ADDR(.tracedata) - LOAD_OFFSET) {		\
-	  	__tracedata_start = .;					\
+		VMLINUX_SYMBOL(__tracedata_start) = .;			\
 		*(.tracedata)						\
-	  	__tracedata_end = .;					\
+		VMLINUX_SYMBOL(__tracedata_end) = .;			\
 	}
 #else
 #define TRACEDATA
@@ -362,7 +362,7 @@
 
 #define INITCALLS							\
 	*(.initcallearly.init)						\
-	__early_initcall_end = .;					\
+	VMLINUX_SYMBOL(__early_initcall_end) = .;			\
   	*(.initcall0.init)						\
   	*(.initcall0s.init)						\
   	*(.initcall1.init)						\
@@ -383,9 +383,9 @@
 
 #define PERCPU(align)							\
 	. = ALIGN(align);						\
-	__per_cpu_start = .;						\
+	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
 	.data.percpu  : AT(ADDR(.data.percpu) - LOAD_OFFSET) {		\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
 	}								\
-	__per_cpu_end = .;
+	VMLINUX_SYMBOL(__per_cpu_end) = .;

commit 6948385cbd83201fb933125c1a578b29b456605d
Merge: 7a76d89232f2 56b2f0706d82
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 27 09:59:59 2008 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-next
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/sam/kbuild-next: (25 commits)
      setlocalversion: do not describe if there is nothing to describe
      kconfig: fix typos: "Suport" -> "Support"
      kconfig: make defconfig is no longer chatty
      kconfig: make oldconfig is now less chatty
      kconfig: speed up all*config + randconfig
      kconfig: set all new symbols automatically
      kconfig: add diffconfig utility
      kbuild: remove Module.markers during mrproper
      kbuild: sparse needs CF not CHECKFLAGS
      kernel-doc: handle/strip __init
      vmlinux.lds: move __attribute__((__cold__)) functions back into final .text section
      init: fix URL of "The GNU Accounting Utilities"
      kbuild: add arch/$ARCH/include to search path
      kbuild: asm symlink support for arch/$ARCH/include
      kbuild: support arch/$ARCH/include for tags, cscope
      kbuild: prepare headers_* for arch/$ARCH/include
      kbuild: install all headers when arch is changed
      kbuild: make clean removes *.o.* as well
      kbuild: optimize headers_* targets
      kbuild: only one call for include/ in make headers_*
      ...

commit c2147a5092cfe13dbf3210e54e8a622015edeecc
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Fri Jul 25 19:45:11 2008 -0700

    Better interface for hooking early initcalls
    
    Added early initcall (pre-SMP) support, using an identical interface to
    that of regular initcalls.  Functions called from do_pre_smp_initcalls()
    could be converted to use this cleaner interface.
    
    This is required by CPU hotplug, because early users have to register
    notifiers before going SMP.  One such CPU hotplug user is the relay
    interface with buffer-only channels, which needs to register such a
    notifier, to be usable in early code.  This in turn is used by kmemtrace.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 729f6b0a60e9..9cd44b162ba1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -359,6 +359,8 @@
 	}
 
 #define INITCALLS							\
+	*(.initcallearly.init)						\
+	__early_initcall_end = .;					\
   	*(.initcall0.init)						\
   	*(.initcall0s.init)						\
   	*(.initcall1.init)						\

commit fb5e2b379732e1a6ea32392980bb42e0212db842
Author: Jan Beulich <jbeulich@novell.com>
Date:   Wed Jun 18 12:36:01 2008 +0100

    vmlinux.lds: move __attribute__((__cold__)) functions back into final .text section
    
    Due to the addition of __attribute__((__cold__)) to a few symbols
    without adjusting the linker scripts, those symbols currently may end
    up outside the [_stext,_etext) range, as they get placed in
    .text.unlikely by (at least) gcc 4.3.0. This may confuse code not only
    outside of the kernel, symbol_put_addr()'s BUG() could also trigger.
    Hence we need to add .text.unlikely (and for future uses of
    __attribute__((__hot__)) also .text.hot) to the TEXT_TEXT() macro.
    
    Issue observed by Lukas Lipavsky.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Tested-by: Lukas Lipavsky <llipavsky@suse.cz>
    Cc: <stable@kernel.org>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 729f6b0a60e9..bd2be5fd1276 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -221,6 +221,7 @@
  * during second ld run in second ld pass when generating System.map */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
+		*(.text.hot)						\
 		*(.text)						\
 		*(.ref.text)						\
 		*(.text.init.refok)					\
@@ -230,7 +231,8 @@
 	CPU_KEEP(init.text)						\
 	CPU_KEEP(exit.text)						\
 	MEM_KEEP(init.text)						\
-	MEM_KEEP(exit.text)
+	MEM_KEEP(exit.text)						\
+		*(.text.unlikely)
 
 
 /* sched.text is aling to function alignment to secure we have same

commit dc7c65db2845a8d17432d89252c4227a9a7cb15f
Merge: 8a0ca91e1db5 58b6e5538460
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 16 17:25:46 2008 -0700

    Merge branch 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6
    
    * 'linux-next' of git://git.kernel.org/pub/scm/linux/kernel/git/jbarnes/pci-2.6: (72 commits)
      Revert "x86/PCI: ACPI based PCI gap calculation"
      PCI: remove unnecessary volatile in PCIe hotplug struct controller
      x86/PCI: ACPI based PCI gap calculation
      PCI: include linux/pm_wakeup.h for device_set_wakeup_capable
      PCI PM: Fix pci_prepare_to_sleep
      x86/PCI: Fix PCI config space for domains > 0
      Fix acpi_pm_device_sleep_wake() by providing a stub for CONFIG_PM_SLEEP=n
      PCI: Simplify PCI device PM code
      PCI PM: Introduce pci_prepare_to_sleep and pci_back_from_sleep
      PCI ACPI: Rework PCI handling of wake-up
      ACPI: Introduce new device wakeup flag 'prepared'
      ACPI: Introduce acpi_device_sleep_wake function
      PCI: rework pci_set_power_state function to call platform first
      PCI: Introduce platform_pci_power_manageable function
      ACPI: Introduce acpi_bus_power_manageable function
      PCI: make pci_name use dev_name
      PCI: handle pci_name() being const
      PCI: add stub for pci_set_consistent_dma_mask()
      PCI: remove unused arch pcibios_update_resource() functions
      PCI: fix pci_setup_device()'s sprinting into a const buffer
      ...
    
    Fixed up conflicts in various files (arch/x86/kernel/setup_64.c,
    arch/x86/pci/irq.c, arch/x86/pci/pci.h, drivers/acpi/sleep/main.c,
    drivers/pci/pci.c, drivers/pci/pci.h, include/acpi/acpi_bus.h) from x86
    and ACPI updates manually.

commit 5a86102248592e178a9023359ccf7f0e489d8e35
Merge: 85082fd7cbe3 751851af7aae
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 16:54:07 2008 -0700

    Merge branch 'for-2.6.27' of git://git.infradead.org/users/dwmw2/firmware-2.6
    
    * 'for-2.6.27' of git://git.infradead.org/users/dwmw2/firmware-2.6: (64 commits)
      firmware: convert sb16_csp driver to use firmware loader exclusively
      dsp56k: use request_firmware
      edgeport-ti: use request_firmware()
      edgeport: use request_firmware()
      vicam: use request_firmware()
      dabusb: use request_firmware()
      cpia2: use request_firmware()
      ip2: use request_firmware()
      firmware: convert Ambassador ATM driver to request_firmware()
      whiteheat: use request_firmware()
      ti_usb_3410_5052: use request_firmware()
      emi62: use request_firmware()
      emi26: use request_firmware()
      keyspan_pda: use request_firmware()
      keyspan: use request_firmware()
      ttusb-budget: use request_firmware()
      kaweth: use request_firmware()
      smctr: use request_firmware()
      firmware: convert ymfpci driver to use firmware loader exclusively
      firmware: convert maestro3 driver to use firmware loader exclusively
      ...
    
    Fix up trivial conflicts with BKL removal in drivers/char/dsp56k.c and
    drivers/char/ip2/ip2main.c manually.

commit 751851af7aae9b8bd5a60b3897209081fbc18b2b
Merge: a41eebab7537 d71792ac3d48
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Mon Jul 14 15:49:04 2008 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git
    
    Conflicts:
    
            sound/pci/Kconfig

commit d18bb9a548e550f3ced57618e75085fb3f173133
Merge: 4bb0057f996b 6d72b7952fa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 15:28:10 2008 -0700

    Merge branch 'core/rodata' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core/rodata' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      move BUG_TABLE into RODATA

commit 5658c769443d543728b6c5c673dffc2df8676317
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Fri May 23 13:52:42 2008 +0100

    firmware: allow firmware files to be built into kernel image
    
    Some drivers have their own hacks to bypass the kernel's firmware loader
    and build their firmware into the kernel; this renders those unnecessary.
    
    Other drivers don't use the firmware loader at all, because they always
    want the firmware to be available. This allows them to start using the
    firmware loader.
    
    A third set of drivers already use the firmware loader, but can't be
    used without help from userspace, which sometimes requires an initrd.
    This allows them to work in a static kernel.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f054778e916c..8d71a40625f3 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -86,6 +86,13 @@
 		VMLINUX_SYMBOL(__end_pci_fixups_resume) = .;		\
 	}								\
 									\
+	/* Built-in firmware blobs */					\
+	.builtin_fw        : AT(ADDR(.builtin_fw) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start_builtin_fw) = .;			\
+		*(.builtin_fw)						\
+		VMLINUX_SYMBOL(__end_builtin_fw) = .;			\
+	}								\
+									\
 	/* RapidIO route ops */						\
 	.rio_route        : AT(ADDR(.rio_route) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_rio_route_ops) = .;		\

commit e1a2a51e684bfe9d6165992d4a065439617a3107
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu May 15 21:51:31 2008 +0200

    Suspend/Resume bug in PCI layer wrt quirks
    
    Some quirks should be called with interrupt disabled, we can't directly
    call them in .resume_early. Also the patch introduces
    pci_fixup_resume_early and pci_fixup_suspend, which matches current
    device core callbacks (.suspend/.resume_early).
    
    TBD: Somebody knows why we need quirk resume should double check if a
    quirk should be called in resume or resume_early. I changed some per my
    understanding, but can't make sure I fixed all.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Jesse Barnes <jbarnes@virtuousgeek.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f054778e916c..cf108a3c7f59 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -84,6 +84,12 @@
 		VMLINUX_SYMBOL(__start_pci_fixups_resume) = .;		\
 		*(.pci_fixup_resume)					\
 		VMLINUX_SYMBOL(__end_pci_fixups_resume) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_resume_early) = .;	\
+		*(.pci_fixup_resume_early)				\
+		VMLINUX_SYMBOL(__end_pci_fixups_resume_early) = .;	\
+		VMLINUX_SYMBOL(__start_pci_fixups_suspend) = .;		\
+		*(.pci_fixup_suspend)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_suspend) = .;		\
 	}								\
 									\
 	/* RapidIO route ops */						\

commit 63687a528c39a67c1a213cdffa09feb0e6af9dbe
Author: Jan Beulich <jbeulich@novell.com>
Date:   Mon May 12 15:44:41 2008 +0200

    x86: move tracedata to RODATA
    
    .. allowing it to be write-protected just as other read-only data
    under CONFIG_DEBUG_RODATA.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f054778e916c..f1992dc5c424 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -93,6 +93,8 @@
 		VMLINUX_SYMBOL(__end_rio_route_ops) = .;		\
 	}								\
 									\
+	TRACEDATA							\
+									\
 	/* Kernel symbol table: Normal symbols */			\
 	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___ksymtab) = .;			\
@@ -318,6 +320,18 @@
 		__stop___bug_table = .;					\
 	}
 
+#ifdef CONFIG_PM_TRACE
+#define TRACEDATA							\
+	. = ALIGN(4);							\
+	.tracedata : AT(ADDR(.tracedata) - LOAD_OFFSET) {		\
+	  	__tracedata_start = .;					\
+		*(.tracedata)						\
+	  	__tracedata_end = .;					\
+	}
+#else
+#define TRACEDATA
+#endif
+
 #define NOTES								\
 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start_notes) = .;			\

commit 6360b1fbb4a939efd34fc770c2ebd927c55506e0
Author: Jan Beulich <jbeulich@novell.com>
Date:   Mon May 12 15:44:41 2008 +0200

    move BUG_TABLE into RODATA
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f054778e916c..dd2cc8122ad8 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -67,6 +67,8 @@
 		*(.rodata1)						\
 	}								\
 									\
+	BUG_TABLE							\
+									\
 	/* PCI quirks */						\
 	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
 		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
@@ -310,6 +312,7 @@
 		.stab.indexstr 0 : { *(.stab.indexstr) }		\
 		.comment 0 : { *(.comment) }
 
+#ifdef CONFIG_GENERIC_BUG
 #define BUG_TABLE							\
 	. = ALIGN(8);							\
 	__bug_table : AT(ADDR(__bug_table) - LOAD_OFFSET) {		\
@@ -317,6 +320,9 @@
 		*(__bug_table)						\
 		__stop___bug_table = .;					\
 	}
+#else
+#define BUG_TABLE
+#endif
 
 #define NOTES								\
 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\

commit 63cc8c75156462d4b42cbdd76c293b7eee7ddbfe
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Mon May 12 15:44:40 2008 +0200

    percpu: introduce DEFINE_PER_CPU_PAGE_ALIGNED() macro
    
    While examining holes in percpu section I found this :
    
    c05f5000 D per_cpu__current_task
    c05f5000 D __per_cpu_start
    c05f5004 D per_cpu__cpu_number
    c05f5008 D per_cpu__irq_regs
    c05f500c d per_cpu__cpu_devices
    c05f5040 D per_cpu__cyc2ns
    
    <Big Hole of about 4000 bytes>
    
    c05f6000 d per_cpu__cpuid4_info
    c05f6004 d per_cpu__cache_kobject
    c05f6008 d per_cpu__index_kobject
    
    <Big Hole of about 4000 bytes>
    
    c05f7000 D per_cpu__gdt_page
    
    This is because gdt_page is a percpu variable, defined with
    a page alignement, and linker is doing its job, two times because of .o
    nesting in the build process.
    
    I introduced a new macro DEFINE_PER_CPU_PAGE_ALIGNED() to avoid
    wasting this space. All page aligned variables (only one at this time)
    are put in a separate
    subsection .data.percpu.page_aligned, at the very begining of percpu zone.
    
    Before patch , on a x86_32 machine :
    
    .data.percpu                30232   3227471872
    .data.percpu                22168   3227471872
    
    Thats 8064 bytes saved for each CPU.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f054778e916c..69e5c1182fde 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -348,6 +348,7 @@
 	. = ALIGN(align);						\
 	__per_cpu_start = .;						\
 	.data.percpu  : AT(ADDR(.data.percpu) - LOAD_OFFSET) {		\
+		*(.data.percpu.page_aligned)				\
 		*(.data.percpu)						\
 		*(.data.percpu.shared_aligned)				\
 	}								\

commit 37c514e3dfc8f55145d9c6895e2838ac31859aa4
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Tue Feb 19 21:00:18 2008 +0100

    Add missing init section definitions
    
    When adding __devinitconst etc. the __initconst variant
    were missed.
    Add this one and proper definitions for .head.text for use
    in .S files.
    The naming .head.text is preferred over .text.head as the
    latter will conflict for a function named head when introducing
    -ffunctions-sections.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f784d2f34149..f054778e916c 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -238,6 +238,9 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
+/* Section used for early init (in .S files) */
+#define HEAD_TEXT  *(.head.text)
+
 /* init and exit section handling */
 #define INIT_DATA							\
 	*(.init.data)							\

commit edeed30589f5defe63ce6aaae56f2b7c855e4520
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Wed Jan 30 13:34:08 2008 +0100

    x86: add testcases for RODATA and NX protections/attributes
    
    Latest update; I now have 4 NX tests, but 2 fail so they're #if 0'd.
    I also cleaned up the NX test code quite a bit, and got rid of the ugly
    exception table sorting stuff.
    
    From: Arjan van de Ven <arjan@linux.intel.com>
    
    This patch adds testcases for the CONFIG_DEBUG_RODATA configuration option
    as well as the NX CPU feature/mappings. Both testcases can move to tests/
    once that patch gets merged into mainline.
    (I'm half considering moving the rodata test into mm/init.c but I'll
    wait with that until init.c is unified)
    
    As part of this I had to fix a not-quite-right alignment in the vmlinux.lds.h
    for the RODATA sections, which lead to 1 page less being marked read only.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 76df771be585..f784d2f34149 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -184,6 +184,7 @@
 		VMLINUX_SYMBOL(__start___param) = .;			\
 		*(__param)						\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
+		. = ALIGN((align));					\
 		VMLINUX_SYMBOL(__end_rodata) = .;			\
 	}								\
 	. = ALIGN((align));

commit 312b1485fb509c9bc32eda28ad29537896658cb8
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Mon Jan 28 20:21:15 2008 +0100

    Introduce new section reference annotations tags: __ref, __refdata, __refconst
    
    Today we have the following annotations for functions/data
    referencing __init/__exit functions / data:
    
    __init_refok     => for init functions
    __initdata_refok => for init data
    __exit_refok     => for exit functions
    
    There is really no difference between the __init and __exit
    versions and simplify it and to introduce a shorter annotation
    the following new annotations are introduced:
    
    __ref      => for functions (code) that
                  references __*init / __*exit
    __refdata  => for variables
    __refconst => for const variables
    
    Whit this annotation is it more obvious what the annotation
    is for and there is no longer the arbitary division
    between __init and __exit code.
    
    The mechanishm is the same as before - a special section
    is created which is made part of the usual sections
    in the linker script.
    
    We will start to see annotations like this:
    
    -static struct pci_serial_quirk pci_serial_quirks[] = {
    +static const struct pci_serial_quirk pci_serial_quirks[] __refconst = {
    -----------------
    -static struct notifier_block __cpuinitdata cpuid_class_cpu_notifier =
    +static struct notifier_block cpuid_class_cpu_notifier __refdata =
    ----------------
    -static int threshold_cpu_callback(struct notifier_block *nfb,
    +static int __ref threshold_cpu_callback(struct notifier_block *nfb,
    
    [The above is just random samples].
    
    Note: No modifications were needed in modpost
    to support the new sections due to the newly introduced
    blacklisting.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 294853053707..76df771be585 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -42,6 +42,7 @@
 #define DATA_DATA							\
 	*(.data)							\
 	*(.data.init.refok)						\
+	*(.ref.data)							\
 	DEV_KEEP(init.data)						\
 	DEV_KEEP(exit.data)						\
 	CPU_KEEP(init.data)						\
@@ -169,6 +170,7 @@
 									\
 	/* __*init sections */						\
 	__init_rodata : AT(ADDR(__init_rodata) - LOAD_OFFSET) {		\
+		*(.ref.rodata)						\
 		DEV_KEEP(init.rodata)					\
 		DEV_KEEP(exit.rodata)					\
 		CPU_KEEP(init.rodata)					\
@@ -202,6 +204,7 @@
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
 		*(.text)						\
+		*(.ref.text)						\
 		*(.text.init.refok)					\
 		*(.exit.text.refok)					\
 	DEV_KEEP(init.text)						\

commit 1a3fb6d481689d0482eacadcbe3205b49b423c11
Author: Adrian Bunk <bunk@kernel.org>
Date:   Thu Jan 24 22:20:18 2008 +0100

    asm-generic/vmlix.lds.h: simplify __mem{init,exit}* dependencies
    
    Simplify the dependencies on __mem{init,exit}* (ACPI_HOTPLUG_MEMORY requires
    MEMORY_HOTPLUG).
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e0a56fb8f813..294853053707 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -29,8 +29,7 @@
 #define CPU_DISCARD(sec) *(.cpu##sec)
 #endif
 
-#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_ACPI_HOTPLUG_MEMORY) \
-	|| defined(CONFIG_ACPI_HOTPLUG_MEMORY_MODULE)
+#if defined(CONFIG_MEMORY_HOTPLUG)
 #define MEM_KEEP(sec)    *(.mem##sec)
 #define MEM_DISCARD(sec)
 #else

commit eb8f689046b857874e964463619f09df06d59fad
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jan 20 20:07:28 2008 +0100

    Use separate sections for __dev/__cpu/__mem code/data
    
    Introducing separate sections for __dev* (HOTPLUG),
    __cpu* (HOTPLUG_CPU) and __mem* (MEMORY_HOTPLUG)
    allows us to do a much more reliable Section mismatch
    check in modpost. We are no longer dependent on the actual
    configuration of for example HOTPLUG.
    
    This has the effect that all users see much more
    Section mismatch warnings than before because they
    were almost all hidden when HOTPLUG was enabled.
    The advantage of this is that when building a piece
    of code then it is much more likely that the Section
    mismatch errors are spotted and the warnings will be
    felt less random of nature.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Greg KH <greg@kroah.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Adrian Bunk <bunk@kernel.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index ae0166e83490..e0a56fb8f813 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -9,10 +9,46 @@
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
+/* The actual configuration determine if the init/exit sections
+ * are handled as text/data or they can be discarded (which
+ * often happens at runtime)
+ */
+#ifdef CONFIG_HOTPLUG
+#define DEV_KEEP(sec)    *(.dev##sec)
+#define DEV_DISCARD(sec)
+#else
+#define DEV_KEEP(sec)
+#define DEV_DISCARD(sec) *(.dev##sec)
+#endif
+
+#ifdef CONFIG_HOTPLUG_CPU
+#define CPU_KEEP(sec)    *(.cpu##sec)
+#define CPU_DISCARD(sec)
+#else
+#define CPU_KEEP(sec)
+#define CPU_DISCARD(sec) *(.cpu##sec)
+#endif
+
+#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_ACPI_HOTPLUG_MEMORY) \
+	|| defined(CONFIG_ACPI_HOTPLUG_MEMORY_MODULE)
+#define MEM_KEEP(sec)    *(.mem##sec)
+#define MEM_DISCARD(sec)
+#else
+#define MEM_KEEP(sec)
+#define MEM_DISCARD(sec) *(.mem##sec)
+#endif
+
+
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
 	*(.data.init.refok)						\
+	DEV_KEEP(init.data)						\
+	DEV_KEEP(exit.data)						\
+	CPU_KEEP(init.data)						\
+	CPU_KEEP(exit.data)						\
+	MEM_KEEP(init.data)						\
+	MEM_KEEP(exit.data)						\
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___markers) = .;				\
 	*(__markers)							\
@@ -132,6 +168,16 @@
 		*(__ksymtab_strings)					\
 	}								\
 									\
+	/* __*init sections */						\
+	__init_rodata : AT(ADDR(__init_rodata) - LOAD_OFFSET) {		\
+		DEV_KEEP(init.rodata)					\
+		DEV_KEEP(exit.rodata)					\
+		CPU_KEEP(init.rodata)					\
+		CPU_KEEP(exit.rodata)					\
+		MEM_KEEP(init.rodata)					\
+		MEM_KEEP(exit.rodata)					\
+	}								\
+									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___param) = .;			\
@@ -139,7 +185,6 @@
 		VMLINUX_SYMBOL(__stop___param) = .;			\
 		VMLINUX_SYMBOL(__end_rodata) = .;			\
 	}								\
-									\
 	. = ALIGN((align));
 
 /* RODATA provided for backward compatibility.
@@ -159,7 +204,14 @@
 		ALIGN_FUNCTION();					\
 		*(.text)						\
 		*(.text.init.refok)					\
-		*(.exit.text.refok)
+		*(.exit.text.refok)					\
+	DEV_KEEP(init.text)						\
+	DEV_KEEP(exit.text)						\
+	CPU_KEEP(init.text)						\
+	CPU_KEEP(exit.text)						\
+	MEM_KEEP(init.text)						\
+	MEM_KEEP(exit.text)
+
 
 /* sched.text is aling to function alignment to secure we have same
  * address even at second ld pass when generating System.map */
@@ -184,11 +236,35 @@
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
 /* init and exit section handling */
-#define INIT_TEXT *(.init.text)
-#define INIT_DATA *(.init.data)
-#define EXIT_TEXT *(.exit.text)
-#define EXIT_DATA *(.exit.data)
+#define INIT_DATA							\
+	*(.init.data)							\
+	DEV_DISCARD(init.data)						\
+	DEV_DISCARD(init.rodata)					\
+	CPU_DISCARD(init.data)						\
+	CPU_DISCARD(init.rodata)					\
+	MEM_DISCARD(init.data)						\
+	MEM_DISCARD(init.rodata)
+
+#define INIT_TEXT							\
+	*(.init.text)							\
+	DEV_DISCARD(init.text)						\
+	CPU_DISCARD(init.text)						\
+	MEM_DISCARD(init.text)
+
+#define EXIT_DATA							\
+	*(.exit.data)							\
+	DEV_DISCARD(exit.data)						\
+	DEV_DISCARD(exit.rodata)					\
+	CPU_DISCARD(exit.data)						\
+	CPU_DISCARD(exit.rodata)					\
+	MEM_DISCARD(exit.data)						\
+	MEM_DISCARD(exit.rodata)
 
+#define EXIT_TEXT							\
+	*(.exit.text)							\
+	DEV_DISCARD(exit.text)						\
+	CPU_DISCARD(exit.text)						\
+	MEM_DISCARD(exit.text)
 
 		/* DWARF debug sections.
 		Symbols in the DWARF debugging sections are relative to

commit 01ba2bdc6b639764745ff678caf3fb9e5bcd745a
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jan 20 14:15:03 2008 +0100

    all archs: consolidate init and exit sections in vmlinux.lds.h
    
    This patch consolidate all definitions of .init.text, .init.data
    and .exit.text, .exit.data section definitions in
    the generic vmlinux.lds.h.
    
    This is a preparational patch - alone it does not buy
    us much good.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9f584cc5c5fb..ae0166e83490 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -183,6 +183,13 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
+/* init and exit section handling */
+#define INIT_TEXT *(.init.text)
+#define INIT_DATA *(.init.data)
+#define EXIT_TEXT *(.exit.text)
+#define EXIT_DATA *(.exit.data)
+
+
 		/* DWARF debug sections.
 		Symbols in the DWARF debugging sections are relative to
 		the beginning of the section so we begin them at 0.  */

commit 8256e47cdc8923e9959eb1d7f95d80da538add80
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Thu Oct 18 23:41:06 2007 -0700

    Linux Kernel Markers
    
    The marker activation functions sits in kernel/marker.c.  A hash table is used
    to keep track of the registered probes and armed markers, so the markers
    within a newly loaded module that should be active can be activated at module
    load time.
    
    marker_query has been removed. marker_get_first, marker_get_next and
    marker_release should be used as iterators on the markers.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Mike Mason <mmlnx@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 5615440027ec..9f584cc5c5fb 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -12,7 +12,11 @@
 /* .data section */
 #define DATA_DATA							\
 	*(.data)							\
-	*(.data.init.refok)
+	*(.data.init.refok)						\
+	. = ALIGN(8);							\
+	VMLINUX_SYMBOL(__start___markers) = .;				\
+	*(__markers)							\
+	VMLINUX_SYMBOL(__stop___markers) = .;
 
 #define RO_DATA(align)							\
 	. = ALIGN((align));						\
@@ -20,6 +24,7 @@
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
+		*(__markers_strings)	/* Markers: strings */		\
 	}								\
 									\
 	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\

commit 23ec23c2d3496d1cdf74f73d60ab1051405ca774
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Oct 13 08:40:24 2007 +0100

    fix sparc32 breakage (result of vmlinux.lds.S bug)
    
    In commit 4665079cbb2a3e17de82f2ab2940b9f97f37d65e ("[NETNS]: Move some
    code into __init section when CONFIG_NET_NS=n") we got a new section -
    .exit.text.refok (more of 'let's tell modpost that some bogus calls are
    not bogus', a-la text.init.refok).
    
    Unfortunately, the commit in question forgot to add it to TEXT_TEXT,
    with rather amusing results.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 0240e0506a07..5615440027ec 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -153,7 +153,8 @@
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
 		*(.text)						\
-		*(.text.init.refok)
+		*(.text.init.refok)					\
+		*(.exit.text.refok)
 
 /* sched.text is aling to function alignment to secure we have same
  * address even at second ld pass when generating System.map */

commit cbe87121f1545bb3e98ae114519bf0c4db27d6ab
Author: Roland McGrath <roland@redhat.com>
Date:   Thu Jul 19 01:48:36 2007 -0700

    i386: Put allocated ELF notes in read-only data segment
    
    This changes the i386 linker script and the asm-generic macro it uses so that
    ELF note sections with SHF_ALLOC set are linked into the kernel image along
    with other read-only data.  The PT_NOTE also points to their location.
    
    This paves the way for putting useful build-time information into ELF notes
    that can be found easily later in a kernel memory dump.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a2b09ed852ad..0240e0506a07 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -224,7 +224,11 @@
 	}
 
 #define NOTES								\
-	.notes : { *(.note.*) } :note
+	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
+		VMLINUX_SYMBOL(__start_notes) = .;			\
+		*(.note.*)						\
+		VMLINUX_SYMBOL(__stop_notes) = .;			\
+	}
 
 #define INITCALLS							\
   	*(.initcall0.init)						\

commit 5fb7dc37dc16fbc8b80d81318a582201ef7e280d
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Thu Jul 19 01:48:12 2007 -0700

    define new percpu interface for shared data
    
    per cpu data section contains two types of data.  One set which is
    exclusively accessed by the local cpu and the other set which is per cpu,
    but also shared by remote cpus.  In the current kernel, these two sets are
    not clearely separated out.  This can potentially cause the same data
    cacheline shared between the two sets of data, which will result in
    unnecessary bouncing of the cacheline between cpus.
    
    One way to fix the problem is to cacheline align the remotely accessed per
    cpu data, both at the beginning and at the end.  Because of the padding at
    both ends, this will likely cause some memory wastage and also the
    interface to achieve this is not clean.
    
    This patch:
    
    Moves the remotely accessed per cpu data (which is currently marked
    as ____cacheline_aligned_in_smp) into a different section, where all the data
    elements are cacheline aligned. And as such, this differentiates the local
    only data and remotely accessed data cleanly.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: <linux-arch@vger.kernel.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 84155eb67f1d..a2b09ed852ad 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -245,3 +245,11 @@
   	*(.initcall7.init)						\
   	*(.initcall7s.init)
 
+#define PERCPU(align)							\
+	. = ALIGN(align);						\
+	__per_cpu_start = .;						\
+	.data.percpu  : AT(ADDR(.data.percpu) - LOAD_OFFSET) {		\
+		*(.data.percpu)						\
+		*(.data.percpu.shared_aligned)				\
+	}								\
+	__per_cpu_end = .;

commit 4096b46f01a362fe2cc83f6be25cc7be6bce2ab7
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Tue May 29 21:29:00 2007 +0200

    sparc64: fix alignment bug in linker definition script
    
    The RO_DATA section were hardcoded to a specific
    alignment in include/asm-generic/vmlinux.h.
    But for sparc64 this did not match the PAGE_SIZE.
    
    Introduce a new section definition named:
    RO_DATA that takes actual alignment as parameter.
    RODATA are provided for backward compatibility.
    
    On top of this avoid hardcoding alignment for
    sparc64 in reset of the script
    Fix is build-tested on sparc64 + x86_64.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 8307b1bb337a..84155eb67f1d 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -14,8 +14,8 @@
 	*(.data)							\
 	*(.data.init.refok)
 
-#define RODATA								\
-	. = ALIGN(4096);						\
+#define RO_DATA(align)							\
+	. = ALIGN((align));						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
@@ -135,7 +135,11 @@
 		VMLINUX_SYMBOL(__end_rodata) = .;			\
 	}								\
 									\
-	. = ALIGN(4096);
+	. = ALIGN((align));
+
+/* RODATA provided for backward compatibility.
+ * All archs are supposed to use RO_DATA() */
+#define RODATA RO_DATA(4096)
 
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \

commit 0e0d314e6a01bb14d303e35e6f7ba24b17020044
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Thu May 17 20:14:48 2007 +0200

    kbuild: introduce __init_refok/__initdata_refok to supress section mismatch warnings
    
    Throughout the kernel there are a few legitimite references
    to init or exit sections. Most of these are covered by the
    patterns included in modpost but a few nees special attention.
    To avoid hardcoding a lot of function names in modpost introduce
    a marker so relevant function/data can be marked.
    When modpost see a reference to a init/exit function from
    a function/data marked no warning will be issued.
    
    Idea from: Andrew Morton <akpm@linux-foundation.org>
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 52e2d69ee535..8307b1bb337a 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -11,7 +11,8 @@
 
 /* .data section */
 #define DATA_DATA							\
-	*(.data)
+	*(.data)							\
+	*(.data.init.refok)
 
 #define RODATA								\
 	. = ALIGN(4096);						\
@@ -147,7 +148,8 @@
  * during second ld run in second ld pass when generating System.map */
 #define TEXT_TEXT							\
 		ALIGN_FUNCTION();					\
-		*(.text)
+		*(.text)						\
+		*(.text.init.refok)
 
 /* sched.text is aling to function alignment to secure we have same
  * address even at second ld pass when generating System.map */

commit ca967258b69eb65dcb07bbab90fdf964c6d2ec45
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Thu May 17 13:38:44 2007 +0200

    all-archs: consolidate .data section definition in asm-generic
    
    With this consolidation we can now modify the .data
    section definition in one spot for all archs.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a464227a66b1..52e2d69ee535 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -9,6 +9,10 @@
 /* Align . to a 8 byte boundary equals to maximum function alignment. */
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
+/* .data section */
+#define DATA_DATA							\
+	*(.data)
+
 #define RODATA								\
 	. = ALIGN(4096);						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\

commit 7664709b44a13e2e0b545e2dd8e7b8797a1748dc
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun May 13 00:31:33 2007 +0200

    all-archs: consolidate .text section definition in asm-generic
    
    Move definition of .text section to asm-generic.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index f3806a74c478..a464227a66b1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -139,6 +139,12 @@
 		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
 	}
 
+/* .text section. Map to function alignment to avoid address changes
+ * during second ld run in second ld pass when generating System.map */
+#define TEXT_TEXT							\
+		ALIGN_FUNCTION();					\
+		*(.text)
+
 /* sched.text is aling to function alignment to secure we have same
  * address even at second ld pass when generating System.map */
 #define SCHED_TEXT							\

commit 03df4f6ee997589a84d5f9492c6419183724c710
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed May 2 19:27:17 2007 +0200

    [PATCH] i386: Clean up ELF note generation
    
    Three cleanups:
    
    1: ELF notes are never mapped, so there's no need to have any access
    flags in their phdr.
    
    2: When generating them from asm, tell the assembler to use a SHT_NOTE
    section type.  There doesn't seem to be a way to do this from C.
    
    3: Use ANSI rather than traditional cpp behaviour to stringify the
    macro argument.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9fcc8d9fbb14..f3806a74c478 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -208,7 +208,7 @@
 	}
 
 #define NOTES								\
-		.notes : { *(.note.*) } :note
+	.notes : { *(.note.*) } :note
 
 #define INITCALLS							\
   	*(.initcall0.init)						\

commit 1597cacbe39802d86656d1f2e6329895bd2ef531
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Mon Dec 4 15:14:45 2006 -0800

    PCI: Fix multiple problems with VIA hardware
    
    This patch is designed to fix:
    - Disk eating corruptor on KT7 after resume from RAM
    - VIA IRQ handling
    - VIA fixups for bus lockups after resume from RAM
    
    The core of this is to add a table of resume fixups run at resume time.
    We need to do this for a variety of boards and features, but particularly
    we need to do this to get various critical VIA fixups done on resume.
    
    The second part of the problem is to handle VIA IRQ number rules which
    are a bit odd and need special handling for PIC interrupts. Various
    patches broke various boxes and while this one may not be perfect
    (hopefully it is) it ensures the workaround is applied to the right
    devices only.
    
    From: Jean Delvare <khali@linux-fr.org>
    
    Now that PCI quirks are replayed on software resume, we can safely
    re-enable the Asus SMBus unhiding quirk even when software suspend support
    is enabled.
    
    [akpm@osdl.org: fix const warning]
    Signed-off-by: Alan Cox <alan@redhat.com>
    Cc: Jean Delvare <khali@linux-fr.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 1587121730c5..9fcc8d9fbb14 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -35,6 +35,9 @@
 		VMLINUX_SYMBOL(__start_pci_fixups_enable) = .;		\
 		*(.pci_fixup_enable)					\
 		VMLINUX_SYMBOL(__end_pci_fixups_enable) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_resume) = .;		\
+		*(.pci_fixup_resume)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_resume) = .;		\
 	}								\
 									\
 	/* RapidIO route ops */						\

commit d1526e2cda64d5a1de56aef50bad9e5df14245c2
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Fri Dec 15 08:43:13 2006 -0800

    Remove stack unwinder for now
    
    It has caused more problems than it ever really solved, and is
    apparently not getting cleaned up and fixed.  We can put it back when
    it's stable and isn't likely to make warning or bug events worse.
    
    In the meantime, enable frame pointers for more readable stack traces.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 7437ccaada77..1587121730c5 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -119,8 +119,6 @@
 		*(__ksymtab_strings)					\
 	}								\
 									\
-	EH_FRAME							\
-									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___param) = .;			\
@@ -160,26 +158,6 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
-#ifdef CONFIG_STACK_UNWIND
-#define EH_FRAME							\
-		/* Unwind data binary search table */			\
-		. = ALIGN(8);						\
-        	.eh_frame_hdr : AT(ADDR(.eh_frame_hdr) - LOAD_OFFSET) {	\
-			VMLINUX_SYMBOL(__start_unwind_hdr) = .;		\
-			*(.eh_frame_hdr)				\
-			VMLINUX_SYMBOL(__end_unwind_hdr) = .;		\
-		}							\
-		/* Unwind data */					\
-		. = ALIGN(8);						\
-		.eh_frame : AT(ADDR(.eh_frame) - LOAD_OFFSET) {		\
-			VMLINUX_SYMBOL(__start_unwind) = .;		\
-		  	*(.eh_frame)					\
-			VMLINUX_SYMBOL(__end_unwind) = .;		\
-		}
-#else
-#define EH_FRAME
-#endif
-
 		/* DWARF debug sections.
 		Symbols in the DWARF debugging sections are relative to
 		the beginning of the section so we begin them at 0.  */

commit 8d610dd52dd1da696e199e4b4545f33a2a5de5c6
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Mon Dec 11 12:12:04 2006 -0800

    Make sure we populate the initroot filesystem late enough
    
    We should not initialize rootfs before all the core initializers have
    run.  So do it as a separate stage just before starting the regular
    driver initializers.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6e9fcebbf89f..7437ccaada77 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -242,6 +242,7 @@
   	*(.initcall4s.init)						\
   	*(.initcall5.init)						\
   	*(.initcall5s.init)						\
+	*(.initcallrootfs.init)						\
   	*(.initcall6.init)						\
   	*(.initcall6s.init)						\
   	*(.initcall7.init)						\

commit 7664c5a1da4711bb6383117f51b94c8dc8f3f1cd
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Dec 8 02:36:19 2006 -0800

    [PATCH] Generic BUG implementation
    
    This patch adds common handling for kernel BUGs, for use by architectures as
    they wish.  The code is derived from arch/powerpc.
    
    The advantages of having common BUG handling are:
     - consistent BUG reporting across architectures
     - shared implementation of out-of-line file/line data
     - implement CONFIG_DEBUG_BUGVERBOSE consistently
    
    This means that in inline impact of BUG is just the illegal instruction
    itself, which is an improvement for i386 and x86-64.
    
    A BUG is represented in the instruction stream as an illegal instruction,
    which has file/line information associated with it.  This extra information is
    stored in the __bug_table section in the ELF file.
    
    When the kernel gets an illegal instruction, it first confirms it might
    possibly be from a BUG (ie, in kernel mode, the right illegal instruction).
    It then calls report_bug().  This searches __bug_table for a matching
    instruction pointer, and if found, prints the corresponding file/line
    information.  If report_bug() determines that it wasn't a BUG which caused the
    trap, it returns BUG_TRAP_TYPE_NONE.
    
    Some architectures (powerpc) implement WARN using the same mechanism; if the
    illegal instruction was the result of a WARN, then report_bug(Q) returns
    CONFIG_DEBUG_BUGVERBOSE; otherwise it returns BUG_TRAP_TYPE_BUG.
    
    lib/bug.c keeps a list of loaded modules which can be searched for __bug_table
    entries.  The architecture must call
    module_bug_finalize()/module_bug_cleanup() from its corresponding
    module_finalize/cleanup functions.
    
    Unsetting CONFIG_DEBUG_BUGVERBOSE will reduce the kernel size by some amount.
    At the very least, filename and line information will not be recorded for each
    but, but architectures may decide to store no extra information per BUG at
    all.
    
    Unfortunately, gcc doesn't have a general way to mark an asm() as noreturn, so
    architectures will generally have to include an infinite loop (or similar) in
    the BUG code, so that gcc knows execution won't continue beyond that point.
    gcc does have a __builtin_trap() operator which may be useful to achieve the
    same effect, unfortunately it cannot be used to actually implement the BUG
    itself, because there's no way to get the instruction's address for use in
    generating the __bug_table entry.
    
    [randy.dunlap@oracle.com: Handle BUG=n, GENERIC_BUG=n to prevent build errors]
    [bunk@stusta.de: include/linux/bug.h must always #include <linux/module.h]
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Hugh Dickens <hugh@veritas.com>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 4d4c62d11059..6e9fcebbf89f 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -218,6 +218,14 @@
 		.stab.indexstr 0 : { *(.stab.indexstr) }		\
 		.comment 0 : { *(.comment) }
 
+#define BUG_TABLE							\
+	. = ALIGN(8);							\
+	__bug_table : AT(ADDR(__bug_table) - LOAD_OFFSET) {		\
+		__start___bug_table = .;				\
+		*(__bug_table)						\
+		__stop___bug_table = .;					\
+	}
+
 #define NOTES								\
 		.notes : { *(.note.*) } :note
 

commit b65780e123ba9b762276482bbfb52836e4d41fd9
Author: Jan Beulich <jbeulich@novell.com>
Date:   Thu Dec 7 02:14:19 2006 +0100

    [PATCH] unwinder: move .eh_frame to RODATA
    
    The .eh_frame section contents is never written to, so it can as well
    benefit from CONFIG_DEBUG_RODATA.
    
    Diff-ed against firstfloor tree.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Andi Kleen <ak@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9f4747780dac..4d4c62d11059 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -119,8 +119,7 @@
 		*(__ksymtab_strings)					\
 	}								\
 									\
-	/* Unwind data binary search table */				\
-	EH_FRAME_HDR							\
+	EH_FRAME							\
 									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
@@ -162,15 +161,23 @@
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
 #ifdef CONFIG_STACK_UNWIND
-		/* Unwind data binary search table */
-#define EH_FRAME_HDR							\
+#define EH_FRAME							\
+		/* Unwind data binary search table */			\
+		. = ALIGN(8);						\
         	.eh_frame_hdr : AT(ADDR(.eh_frame_hdr) - LOAD_OFFSET) {	\
 			VMLINUX_SYMBOL(__start_unwind_hdr) = .;		\
 			*(.eh_frame_hdr)				\
 			VMLINUX_SYMBOL(__end_unwind_hdr) = .;		\
+		}							\
+		/* Unwind data */					\
+		. = ALIGN(8);						\
+		.eh_frame : AT(ADDR(.eh_frame) - LOAD_OFFSET) {		\
+			VMLINUX_SYMBOL(__start_unwind) = .;		\
+		  	*(.eh_frame)					\
+			VMLINUX_SYMBOL(__end_unwind) = .;		\
 		}
 #else
-#define EH_FRAME_HDR
+#define EH_FRAME
 #endif
 
 		/* DWARF debug sections.

commit 6569580de7ae367def89b7671029cb97c1965574
Author: Vivek Goyal <vgoyal@in.ibm.com>
Date:   Thu Dec 7 02:14:03 2006 +0100

    [PATCH] i386: Distinguish absolute symbols
    
    Ld knows about 2 kinds of symbols,  absolute and section
    relative.  Section relative symbols symbols change value
    when a section is moved and absolute symbols do not.
    
    Currently in the linker script we have several labels
    marking the beginning and ending of sections that
    are outside of sections, making them absolute symbols.
    Having a mixture of absolute and section relative
    symbols refereing to the same data is currently harmless
    but it is confusing.
    
    This must be done carefully as newer revs of ld do not place
    symbols that appear in sections without data and instead
    ld makes those symbols global :(
    
    My ultimate goal is to build a relocatable kernel.  The
    safest and least intrusive technique is to generate
    relocation entries so the kernel can be relocated at load
    time.  The only penalty would be an increase in the size
    of the kernel binary.  The problem is that if absolute and
    relocatable symbols are not properly specified absolute symbols
    will be relocated or section relative symbols won't be, which
    is fatal.
    
    The practical motivation is that when generating kernels that
    will run from a reserved area for analyzing what caused
    a kernel panic, it is simpler if you don't need to hard code
    the physical memory location they will run at, especially
    for the distributions.
    
    [AK: and merged:]
    
    o Also put a message so that in future people can be aware of it and
      avoid introducing absolute symbols.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andi Kleen <ak@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e60d6f21fa62..9f4747780dac 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -11,8 +11,8 @@
 
 #define RODATA								\
 	. = ALIGN(4096);						\
-	__start_rodata = .;						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
 	}								\
@@ -119,17 +119,17 @@
 		*(__ksymtab_strings)					\
 	}								\
 									\
+	/* Unwind data binary search table */				\
+	EH_FRAME_HDR							\
+									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___param) = .;			\
 		*(__param)						\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
+		VMLINUX_SYMBOL(__end_rodata) = .;			\
 	}								\
 									\
-	/* Unwind data binary search table */				\
-	EH_FRAME_HDR							\
-									\
-	__end_rodata = .;						\
 	. = ALIGN(4096);
 
 #define SECURITY_INIT							\

commit b3438f8266cb1f5010085ac47d7ad6a36a212164
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Mon Nov 20 11:47:18 2006 -0800

    Add "pure_initcall" for static variable initialization
    
    This is a quick hack to overcome the fact that SRCU currently does not
    allow static initializers, and we need to sometimes initialize those
    things before any other initializers (even "core" ones) can do so.
    
    Currently we don't allow this at all for modules, and the only user that
    needs is right now is cpufreq. As reported by Thomas Gleixner:
    
       "Commit b4dfdbb3c707474a2254c5b4d7e62be31a4b7da9 ("[PATCH] cpufreq:
        make the transition_notifier chain use SRCU breaks cpu frequency
        notification users, which register the callback > on core_init
        level."
    
    Cc: Thomas Gleixner <tglx@timesys.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Andrew Morton <akpm@osdl.org>,
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9d873163a7ab..e60d6f21fa62 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -215,6 +215,8 @@
 		.notes : { *(.note.*) } :note
 
 #define INITCALLS							\
+  	*(.initcall0.init)						\
+  	*(.initcall0s.init)						\
   	*(.initcall1.init)						\
   	*(.initcall1s.init)						\
   	*(.initcall2.init)						\

commit 735a7ffb739b6efeaeb1e720306ba308eaaeb20e
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Oct 27 11:42:37 2006 -0700

    [PATCH] drivers: wait for threaded probes between initcall levels
    
    The multithreaded-probing code has a problem: after one initcall level (eg,
    core_initcall) has been processed, we will then start processing the next
    level (postcore_initcall) while the kernel threads which are handling
    core_initcall are still executing.  This breaks the guarantees which the
    layered initcalls previously gave us.
    
    IOW, we want to be multithreaded _within_ an initcall level, but not between
    different levels.
    
    Fix that up by causing the probing code to wait for all outstanding probes at
    one level to complete before we start processing the next level.
    
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e3e83bcaf710..9d873163a7ab 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -216,10 +216,17 @@
 
 #define INITCALLS							\
   	*(.initcall1.init)						\
+  	*(.initcall1s.init)						\
   	*(.initcall2.init)						\
+  	*(.initcall2s.init)						\
   	*(.initcall3.init)						\
+  	*(.initcall3s.init)						\
   	*(.initcall4.init)						\
+  	*(.initcall4s.init)						\
   	*(.initcall5.init)						\
+  	*(.initcall5s.init)						\
   	*(.initcall6.init)						\
-  	*(.initcall7.init)
+  	*(.initcall6s.init)						\
+  	*(.initcall7.init)						\
+  	*(.initcall7s.init)
 

commit 61ce1efe6e40233663d27ab8ac9ba9710eebcaad
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Oct 27 11:41:44 2006 -0700

    [PATCH] vmlinux.lds: consolidate initcall sections
    
    Add a vmlinux.lds.h helper macro for defining the eight-level initcall table,
    teach all the architectures to use it.
    
    This is a prerequisite for a patch which performs initcall synchronisation for
    multithreaded-probing.
    
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    [ Added AVR32 as well ]
    Signed-off-by: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9d0d11c180d9..e3e83bcaf710 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -213,3 +213,13 @@
 
 #define NOTES								\
 		.notes : { *(.note.*) } :note
+
+#define INITCALLS							\
+  	*(.initcall1.init)						\
+  	*(.initcall2.init)						\
+  	*(.initcall3.init)						\
+  	*(.initcall4.init)						\
+  	*(.initcall5.init)						\
+  	*(.initcall6.init)						\
+  	*(.initcall7.init)
+

commit 690a973f48b6ba2954465992c08e65059c8374fe
Author: Jan Beulich <jbeulich@novell.com>
Date:   Sat Oct 21 18:37:01 2006 +0200

    [PATCH] x86-64: Speed up dwarf2 unwinder
    
    This changes the dwarf2 unwinder to do a binary search for CIEs
    instead of a linear work. The linker is unfortunately not
    able to build a proper lookup table at link time, instead it creates
    one at runtime as soon as the bootmem allocator is usable (so you'll continue
    using the linear lookup for the first [hopefully] few calls).
    The code should be ready to utilize a build-time created table once
    a fixed linker becomes available.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Andi Kleen <ak@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 69240b52f8e1..9d0d11c180d9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -125,6 +125,10 @@
 		*(__param)						\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
 	}								\
+									\
+	/* Unwind data binary search table */				\
+	EH_FRAME_HDR							\
+									\
 	__end_rodata = .;						\
 	. = ALIGN(4096);
 
@@ -157,6 +161,18 @@
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
 
+#ifdef CONFIG_STACK_UNWIND
+		/* Unwind data binary search table */
+#define EH_FRAME_HDR							\
+        	.eh_frame_hdr : AT(ADDR(.eh_frame_hdr) - LOAD_OFFSET) {	\
+			VMLINUX_SYMBOL(__start_unwind_hdr) = .;		\
+			*(.eh_frame_hdr)				\
+			VMLINUX_SYMBOL(__end_unwind_hdr) = .;		\
+		}
+#else
+#define EH_FRAME_HDR
+#endif
+
 		/* DWARF debug sections.
 		Symbols in the DWARF debugging sections are relative to
 		the beginning of the section so we begin them at 0.  */

commit 7583ddfd3aae1007bc4fc67ea4c07d573d376e9e
Author: Marcelo Tosatti <marcelo@kvack.org>
Date:   Wed Sep 27 01:51:02 2006 -0700

    [PATCH] Include __param section in read-only data range
    
    The param section is an array of "kernel_param" structures, storing only
    constant data: pointer to name, permission of the variable pointed to by
    (void *)arg and pointers to set/get methods.
    
    Move end_rodata down to include __param section in the read-only range used
    by CONFIG_DEBUG_RODATA.
    
    Signed-off-by: Marcelo Tosatti <marcelo@kvack.org>
    Acked-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 253ae1328271..69240b52f8e1 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -118,15 +118,15 @@
         __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
 		*(__ksymtab_strings)					\
 	}								\
-	__end_rodata = .;						\
-	. = ALIGN(4096);						\
 									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
 		VMLINUX_SYMBOL(__start___param) = .;			\
 		*(__param)						\
 		VMLINUX_SYMBOL(__stop___param) = .;			\
-	}
+	}								\
+	__end_rodata = .;						\
+	. = ALIGN(4096);
 
 #define SECURITY_INIT							\
 	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \

commit 9c9b8b388296ad5a306ab238dc677cfe6ff4cb12
Author: Jeremy Fitzhardinge <jeremy@xensource.com>
Date:   Mon Sep 25 23:32:26 2006 -0700

    [PATCH] x86: put .note.* sections into a PT_NOTE segment in vmlinux
    
    This patch will pack any .note.* section into a PT_NOTE segment in the output
    file.
    
    To do this, we tell ld that we need a PT_NOTE segment.  This requires us to
    start explicitly mapping sections to segments, so we also need to explicitly
    create PT_LOAD segments for text and data, and map the sections to them
    appropriately.  Fortunately, each section will default to its previous
    section's segment, so it doesn't take many changes to vmlinux.lds.S.
    
    This only changes i386 for now, but I presume the corresponding changes for
    other architectures will be as simple.
    
    This change also adds <linux/elfnote.h>, which defines C and Assembler macros
    for actually creating ELF notes.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index db5a3732f106..253ae1328271 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -194,3 +194,6 @@
 		.stab.index 0 : { *(.stab.index) }			\
 		.stab.indexstr 0 : { *(.stab.indexstr) }		\
 		.comment 0 : { *(.comment) }
+
+#define NOTES								\
+		.notes : { *(.note.*) } :note

commit f71d20e961474dde77e6558396efb93d6ac80a4b
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Wed Jun 28 04:26:45 2006 -0700

    [PATCH] Add EXPORT_UNUSED_SYMBOL and EXPORT_UNUSED_SYMBOL_GPL
    
    Temporarily add EXPORT_UNUSED_SYMBOL and EXPORT_UNUSED_SYMBOL_GPL.  These
    will be used as a transition measure for symbols that aren't used in the
    kernel and are on the way out.  When a module uses such a symbol, a warning
    is printk'd at modprobe time.
    
    The main reason for removing unused exports is size: eacho export takes
    roughly between 100 and 150 bytes of kernel space in the binary.  This
    patch gives users the option to immediately get this size gain via a config
    option.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 9d11550b4818..db5a3732f106 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -58,6 +58,20 @@
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
 	}								\
 									\
+	/* Kernel symbol table: Normal unused symbols */		\
+	__ksymtab_unused  : AT(ADDR(__ksymtab_unused) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start___ksymtab_unused) = .;		\
+		*(__ksymtab_unused)					\
+		VMLINUX_SYMBOL(__stop___ksymtab_unused) = .;		\
+	}								\
+									\
+	/* Kernel symbol table: GPL-only unused symbols */		\
+	__ksymtab_unused_gpl : AT(ADDR(__ksymtab_unused_gpl) - LOAD_OFFSET) { \
+		VMLINUX_SYMBOL(__start___ksymtab_unused_gpl) = .;	\
+		*(__ksymtab_unused_gpl)					\
+		VMLINUX_SYMBOL(__stop___ksymtab_unused_gpl) = .;	\
+	}								\
+									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__ksymtab_gpl_future : AT(ADDR(__ksymtab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___ksymtab_gpl_future) = .;	\
@@ -79,6 +93,20 @@
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
 	}								\
 									\
+	/* Kernel symbol table: Normal unused symbols */		\
+	__kcrctab_unused  : AT(ADDR(__kcrctab_unused) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start___kcrctab_unused) = .;		\
+		*(__kcrctab_unused)					\
+		VMLINUX_SYMBOL(__stop___kcrctab_unused) = .;		\
+	}								\
+									\
+	/* Kernel symbol table: GPL-only unused symbols */		\
+	__kcrctab_unused_gpl : AT(ADDR(__kcrctab_unused_gpl) - LOAD_OFFSET) { \
+		VMLINUX_SYMBOL(__start___kcrctab_unused_gpl) = .;	\
+		*(__kcrctab_unused_gpl)					\
+		VMLINUX_SYMBOL(__stop___kcrctab_unused_gpl) = .;	\
+	}								\
+									\
 	/* Kernel symbol table: GPL-future-only symbols */		\
 	__kcrctab_gpl_future : AT(ADDR(__kcrctab_gpl_future) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__start___kcrctab_gpl_future) = .;	\

commit 9f28bb7e1d0188a993403ab39b774785892805e1
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Mar 20 13:17:13 2006 -0800

    [PATCH] add EXPORT_SYMBOL_GPL_FUTURE()
    
    This patch adds the ability to mark symbols that will be changed in the
    future, so that kernel modules that don't include MODULE_LICENSE("GPL")
    and use the symbols, will be flagged and printed out to the system log.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 35de20cf8fac..9d11550b4818 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -58,6 +58,13 @@
 		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
 	}								\
 									\
+	/* Kernel symbol table: GPL-future-only symbols */		\
+	__ksymtab_gpl_future : AT(ADDR(__ksymtab_gpl_future) - LOAD_OFFSET) { \
+		VMLINUX_SYMBOL(__start___ksymtab_gpl_future) = .;	\
+		*(__ksymtab_gpl_future)					\
+		VMLINUX_SYMBOL(__stop___ksymtab_gpl_future) = .;	\
+	}								\
+									\
 	/* Kernel symbol table: Normal symbols */			\
 	__kcrctab         : AT(ADDR(__kcrctab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___kcrctab) = .;			\
@@ -72,6 +79,13 @@
 		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
 	}								\
 									\
+	/* Kernel symbol table: GPL-future-only symbols */		\
+	__kcrctab_gpl_future : AT(ADDR(__kcrctab_gpl_future) - LOAD_OFFSET) { \
+		VMLINUX_SYMBOL(__start___kcrctab_gpl_future) = .;	\
+		*(__kcrctab_gpl_future)					\
+		VMLINUX_SYMBOL(__stop___kcrctab_gpl_future) = .;	\
+	}								\
+									\
 	/* Kernel symbol table: strings */				\
         __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
 		*(__ksymtab_strings)					\

commit 37b73c828185731f6236a6387c02d7b08c150810
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Fri Jan 6 00:12:01 2006 -0800

    [PATCH] x86/x86_64: mark rodata section read only: generic infrastructure
    
    Generic prep-work for marking the .rodata section readonly:
    * Align the rodata section at 4Kb boundary
    * call the mark_rodata_ro() function when available
    
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 094d4917c1a9..35de20cf8fac 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -10,6 +10,8 @@
 #define ALIGN_FUNCTION()  . = ALIGN(8)
 
 #define RODATA								\
+	. = ALIGN(4096);						\
+	__start_rodata = .;						\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		*(.rodata) *(.rodata.*)					\
 		*(__vermagic)		/* Kernel version magic */	\
@@ -74,6 +76,8 @@
         __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
 		*(__ksymtab_strings)					\
 	}								\
+	__end_rodata = .;						\
+	. = ALIGN(4096);						\
 									\
 	/* Built-in module parameters. */				\
 	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\

commit 394b701ce4fbfde919a9bcbf84cb4820a7c6d47c
Author: Matt Porter <mporter@kernel.crashing.org>
Date:   Mon Nov 7 01:00:15 2005 -0800

    [PATCH] RapidIO support: core base
    
    Adds a RapidIO subsystem to the kernel.  RIO is a switched fabric interconnect
    used in higher-end embedded applications.  The curious can look at the specs
    over at http://www.rapidio.org
    
    The core code implements enumeration/discovery, management of
    devices/resources, and interfaces for RIO drivers.
    
    There's a lot more to do to take advantages of all the hardware features.
    However, this should provide a good base for folks with RIO hardware to start
    contributing.
    
    Signed-off-by: Matt Porter <mporter@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index a9c55490fb82..094d4917c1a9 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -35,6 +35,13 @@
 		VMLINUX_SYMBOL(__end_pci_fixups_enable) = .;		\
 	}								\
 									\
+	/* RapidIO route ops */						\
+	.rio_route        : AT(ADDR(.rio_route) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start_rio_route_ops) = .;		\
+		*(.rio_route_ops)					\
+		VMLINUX_SYMBOL(__end_rio_route_ops) = .;		\
+	}								\
+									\
 	/* Kernel symbol table: Normal symbols */			\
 	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
 		VMLINUX_SYMBOL(__start___ksymtab) = .;			\

commit a7d0c210337246fa9c25b73cf76dfdbb159f642b
Author: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
Date:   Sat Sep 10 19:44:54 2005 +0200

    [PATCH] i386 / uml: add dwarf sections to static link script
    
    Inside the linker script, insert the code for DWARF debug info sections. This
    may help GDB'ing a Uml binary. Actually, it seems that ld is able to guess
    what I added correctly, but normal linker scripts include this section so it
    should be correct anyway adding it.
    
    On request by Sam Ravnborg <sam@ravnborg.org>, I've added it to
    asm-generic/vmlinux.lds.s. I've also moved there the stabs debug section,
    used the new macro in i386 linker script and added DWARF debug section to
    that.
    
    In the truth, I've not been able to verify the difference in GDB behaviour
    after this change (I've seen large improvements with another patch). This
    may depend on my binutils version, older one may have worse defaults.
    
    However, this section is present in normal linker script, so add it at
    least for the sake of cleanness.
    
    Signed-off-by: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Acked-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 6f857be2b644..a9c55490fb82 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -103,3 +103,41 @@
 		VMLINUX_SYMBOL(__kprobes_text_start) = .;		\
 		*(.kprobes.text)					\
 		VMLINUX_SYMBOL(__kprobes_text_end) = .;
+
+		/* DWARF debug sections.
+		Symbols in the DWARF debugging sections are relative to
+		the beginning of the section so we begin them at 0.  */
+#define DWARF_DEBUG							\
+		/* DWARF 1 */						\
+		.debug          0 : { *(.debug) }			\
+		.line           0 : { *(.line) }			\
+		/* GNU DWARF 1 extensions */				\
+		.debug_srcinfo  0 : { *(.debug_srcinfo) }		\
+		.debug_sfnames  0 : { *(.debug_sfnames) }		\
+		/* DWARF 1.1 and DWARF 2 */				\
+		.debug_aranges  0 : { *(.debug_aranges) }		\
+		.debug_pubnames 0 : { *(.debug_pubnames) }		\
+		/* DWARF 2 */						\
+		.debug_info     0 : { *(.debug_info			\
+				.gnu.linkonce.wi.*) }			\
+		.debug_abbrev   0 : { *(.debug_abbrev) }		\
+		.debug_line     0 : { *(.debug_line) }			\
+		.debug_frame    0 : { *(.debug_frame) }			\
+		.debug_str      0 : { *(.debug_str) }			\
+		.debug_loc      0 : { *(.debug_loc) }			\
+		.debug_macinfo  0 : { *(.debug_macinfo) }		\
+		/* SGI/MIPS DWARF 2 extensions */			\
+		.debug_weaknames 0 : { *(.debug_weaknames) }		\
+		.debug_funcnames 0 : { *(.debug_funcnames) }		\
+		.debug_typenames 0 : { *(.debug_typenames) }		\
+		.debug_varnames  0 : { *(.debug_varnames) }		\
+
+		/* Stabs debugging sections.  */
+#define STABS_DEBUG							\
+		.stab 0 : { *(.stab) }					\
+		.stabstr 0 : { *(.stabstr) }				\
+		.stab.excl 0 : { *(.stab.excl) }			\
+		.stab.exclstr 0 : { *(.stab.exclstr) }			\
+		.stab.index 0 : { *(.stab.index) }			\
+		.stab.indexstr 0 : { *(.stab.indexstr) }		\
+		.comment 0 : { *(.comment) }

commit d0aaff9796c3310326d10da44fc0faed352a1d29
Author: Prasanna S Panchamukhi <prasanna@in.ibm.com>
Date:   Tue Sep 6 15:19:26 2005 -0700

    [PATCH] Kprobes: prevent possible race conditions generic
    
    There are possible race conditions if probes are placed on routines within the
    kprobes files and routines used by the kprobes.  For example if you put probe
    on get_kprobe() routines, the system can hang while inserting probes on any
    routine such as do_fork().  Because while inserting probes on do_fork(),
    register_kprobes() routine grabs the kprobes spin lock and executes
    get_kprobe() routine and to handle probe of get_kprobe(), kprobes_handler()
    gets executed and tries to grab kprobes spin lock, and spins forever.  This
    patch avoids such possible race conditions by preventing probes on routines
    within the kprobes file and routines used by kprobes.
    
    I have modified the patches as per Andi Kleen's suggestion to move kprobes
    routines and other routines used by kprobes to a seperate section
    .kprobes.text.
    
    Also moved page fault and exception handlers, general protection fault to
    .kprobes.text section.
    
    These patches have been tested on i386, x86_64 and ppc64 architectures, also
    compiled on ia64 and sparc64 architectures.
    
    Signed-off-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 3fa94288aa93..6f857be2b644 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -97,3 +97,9 @@
 		VMLINUX_SYMBOL(__lock_text_start) = .;			\
 		*(.spinlock.text)					\
 		VMLINUX_SYMBOL(__lock_text_end) = .;
+
+#define KPROBES_TEXT							\
+		ALIGN_FUNCTION();					\
+		VMLINUX_SYMBOL(__kprobes_text_start) = .;		\
+		*(.kprobes.text)					\
+		VMLINUX_SYMBOL(__kprobes_text_end) = .;

commit 6d30e3a8995c9fa9e8471bb1dff8e070638df5ea
Author: Sam Ravnborg <sam@mars.(none)>
Date:   Thu Jul 14 20:15:44 2005 +0000

    kbuild: Avoid inconsistent kallsyms data
    
    Several reports on inconsistent kallsyms data has been caused by the aliased symbols
    __sched_text_start and __down to shift places in the output of nm.
    The root cause was that on second pass ld aligned __sched_text_start to a 4 byte boundary
    which is the function alignment on i386.
    sched.text and spinlock.text is now aligned to an 8 byte boundary to make sure they
    are aligned to a function alignemnt on most (all?) archs.
    
    Tested by: Paulo Marques <pmarques@grupopie.com>
    Tested by: Alexander Stohr <Alexander.Stohr@gmx.de>
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index b3bb326ae5b6..3fa94288aa93 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -6,6 +6,9 @@
 #define VMLINUX_SYMBOL(_sym_) _sym_
 #endif
 
+/* Align . to a 8 byte boundary equals to maximum function alignment. */
+#define ALIGN_FUNCTION()  . = ALIGN(8)
+
 #define RODATA								\
 	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
 		*(.rodata) *(.rodata.*)					\
@@ -79,12 +82,18 @@
 		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
 	}
 
+/* sched.text is aling to function alignment to secure we have same
+ * address even at second ld pass when generating System.map */
 #define SCHED_TEXT							\
+		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__sched_text_start) = .;			\
 		*(.sched.text)						\
 		VMLINUX_SYMBOL(__sched_text_end) = .;
 
+/* spinlock.text is aling to function alignment to secure we have same
+ * address even at second ld pass when generating System.map */
 #define LOCK_TEXT							\
+		ALIGN_FUNCTION();					\
 		VMLINUX_SYMBOL(__lock_text_start) = .;			\
 		*(.spinlock.text)					\
 		VMLINUX_SYMBOL(__lock_text_end) = .;

commit 60bad7fadf59313a6359f8828bb0087884ad001a
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jun 25 14:57:46 2005 -0700

    [PATCH] kexec: vmlinux: fix physical addresses
    
    In vmlinux.lds.h the code is carefull to define every section so vmlinux
    properly reports the correct physical load address of code, as well as
    it's virtual address.
    
    The new SECURITY_INIT definition fails to follow that convention and
    and causes incorrect physical address to appear in the vmlinux if
    there are any security initcalls.
    
    This patch updates the SECURITY_INIT to follow the convention in the rest of
    the file.
    
    Signed-off-by: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 99cef06a364a..b3bb326ae5b6 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -73,7 +73,7 @@
 	}
 
 #define SECURITY_INIT							\
-	.security_initcall.init : {					\
+	.security_initcall.init : AT(ADDR(.security_initcall.init) - LOAD_OFFSET) { \
 		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
 		*(.security_initcall.init) 				\
 		VMLINUX_SYMBOL(__security_initcall_end) = .;		\

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
new file mode 100644
index 000000000000..99cef06a364a
--- /dev/null
+++ b/include/asm-generic/vmlinux.lds.h
@@ -0,0 +1,90 @@
+#ifndef LOAD_OFFSET
+#define LOAD_OFFSET 0
+#endif
+
+#ifndef VMLINUX_SYMBOL
+#define VMLINUX_SYMBOL(_sym_) _sym_
+#endif
+
+#define RODATA								\
+	.rodata           : AT(ADDR(.rodata) - LOAD_OFFSET) {		\
+		*(.rodata) *(.rodata.*)					\
+		*(__vermagic)		/* Kernel version magic */	\
+	}								\
+									\
+	.rodata1          : AT(ADDR(.rodata1) - LOAD_OFFSET) {		\
+		*(.rodata1)						\
+	}								\
+									\
+	/* PCI quirks */						\
+	.pci_fixup        : AT(ADDR(.pci_fixup) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start_pci_fixups_early) = .;		\
+		*(.pci_fixup_early)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_early) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_header) = .;		\
+		*(.pci_fixup_header)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_header) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_final) = .;		\
+		*(.pci_fixup_final)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_final) = .;		\
+		VMLINUX_SYMBOL(__start_pci_fixups_enable) = .;		\
+		*(.pci_fixup_enable)					\
+		VMLINUX_SYMBOL(__end_pci_fixups_enable) = .;		\
+	}								\
+									\
+	/* Kernel symbol table: Normal symbols */			\
+	__ksymtab         : AT(ADDR(__ksymtab) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start___ksymtab) = .;			\
+		*(__ksymtab)						\
+		VMLINUX_SYMBOL(__stop___ksymtab) = .;			\
+	}								\
+									\
+	/* Kernel symbol table: GPL-only symbols */			\
+	__ksymtab_gpl     : AT(ADDR(__ksymtab_gpl) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start___ksymtab_gpl) = .;		\
+		*(__ksymtab_gpl)					\
+		VMLINUX_SYMBOL(__stop___ksymtab_gpl) = .;		\
+	}								\
+									\
+	/* Kernel symbol table: Normal symbols */			\
+	__kcrctab         : AT(ADDR(__kcrctab) - LOAD_OFFSET) {		\
+		VMLINUX_SYMBOL(__start___kcrctab) = .;			\
+		*(__kcrctab)						\
+		VMLINUX_SYMBOL(__stop___kcrctab) = .;			\
+	}								\
+									\
+	/* Kernel symbol table: GPL-only symbols */			\
+	__kcrctab_gpl     : AT(ADDR(__kcrctab_gpl) - LOAD_OFFSET) {	\
+		VMLINUX_SYMBOL(__start___kcrctab_gpl) = .;		\
+		*(__kcrctab_gpl)					\
+		VMLINUX_SYMBOL(__stop___kcrctab_gpl) = .;		\
+	}								\
+									\
+	/* Kernel symbol table: strings */				\
+        __ksymtab_strings : AT(ADDR(__ksymtab_strings) - LOAD_OFFSET) {	\
+		*(__ksymtab_strings)					\
+	}								\
+									\
+	/* Built-in module parameters. */				\
+	__param : AT(ADDR(__param) - LOAD_OFFSET) {			\
+		VMLINUX_SYMBOL(__start___param) = .;			\
+		*(__param)						\
+		VMLINUX_SYMBOL(__stop___param) = .;			\
+	}
+
+#define SECURITY_INIT							\
+	.security_initcall.init : {					\
+		VMLINUX_SYMBOL(__security_initcall_start) = .;		\
+		*(.security_initcall.init) 				\
+		VMLINUX_SYMBOL(__security_initcall_end) = .;		\
+	}
+
+#define SCHED_TEXT							\
+		VMLINUX_SYMBOL(__sched_text_start) = .;			\
+		*(.sched.text)						\
+		VMLINUX_SYMBOL(__sched_text_end) = .;
+
+#define LOCK_TEXT							\
+		VMLINUX_SYMBOL(__lock_text_start) = .;			\
+		*(.spinlock.text)					\
+		VMLINUX_SYMBOL(__lock_text_end) = .;
