commit 549887271a961a79375b2a55bf675515b9107778
Author: Will Deacon <will@kernel.org>
Date:   Thu Dec 19 16:22:56 2019 +0000

    locking/barriers: Use '__unqual_scalar_typeof' for load-acquire macros
    
    Passing volatile-qualified pointers to the asm-generic implementations of
    the load-acquire macros results in a re-load from the stack due to the
    temporary result variable inheriting the volatile semantics thanks to the
    use of 'typeof()'.
    
    Define these temporary variables using 'unqual_scalar_typeof' to drop
    the volatile qualifier in the case that they are scalar types.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 85b28eb80b11..2eacaf7d62f6 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -128,10 +128,10 @@ do {									\
 #ifndef __smp_load_acquire
 #define __smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = READ_ONCE(*p);				\
+	__unqual_scalar_typeof(*p) ___p1 = READ_ONCE(*p);		\
 	compiletime_assert_atomic_type(*p);				\
 	__smp_mb();							\
-	___p1;								\
+	(typeof(*p))___p1;						\
 })
 #endif
 
@@ -183,10 +183,10 @@ do {									\
 #ifndef smp_load_acquire
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = READ_ONCE(*p);				\
+	__unqual_scalar_typeof(*p) ___p1 = READ_ONCE(*p);		\
 	compiletime_assert_atomic_type(*p);				\
 	barrier();							\
-	___p1;								\
+	(typeof(*p))___p1;						\
 })
 #endif
 
@@ -229,14 +229,14 @@ do {									\
 #ifndef smp_cond_load_relaxed
 #define smp_cond_load_relaxed(ptr, cond_expr) ({		\
 	typeof(ptr) __PTR = (ptr);				\
-	typeof(*ptr) VAL;					\
+	__unqual_scalar_typeof(*ptr) VAL;			\
 	for (;;) {						\
 		VAL = READ_ONCE(*__PTR);			\
 		if (cond_expr)					\
 			break;					\
 		cpu_relax();					\
 	}							\
-	VAL;							\
+	(typeof(*ptr))VAL;					\
 })
 #endif
 
@@ -250,10 +250,10 @@ do {									\
  */
 #ifndef smp_cond_load_acquire
 #define smp_cond_load_acquire(ptr, cond_expr) ({		\
-	typeof(*ptr) _val;					\
+	__unqual_scalar_typeof(*ptr) _val;			\
 	_val = smp_cond_load_relaxed(ptr, cond_expr);		\
 	smp_acquire__after_ctrl_dep();				\
-	_val;							\
+	(typeof(*ptr))_val;					\
 })
 #endif
 

commit b4d0d230ccfb5d1a9ea85da64aa584df7c148ee9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 20 19:08:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 36
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public licence as published by
      the free software foundation either version 2 of the licence or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 114 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190520170857.552531963@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 2cafdbb9ae4c..85b28eb80b11 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Generic barrier definitions.
  *
@@ -6,11 +7,6 @@
  *
  * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public Licence
- * as published by the Free Software Foundation; either version
- * 2 of the Licence, or (at your option) any later version.
  */
 #ifndef __ASM_GENERIC_BARRIER_H
 #define __ASM_GENERIC_BARRIER_H

commit fcfdfe30e324725007e9dc5814b62a4c430ea909
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:15 2018 +0100

    locking/barriers: Introduce smp_cond_load_relaxed() and atomic_cond_read_relaxed()
    
    Whilst we currently provide smp_cond_load_acquire() and
    atomic_cond_read_acquire(), there are cases where the ACQUIRE semantics are
    not required because of a subsequent fence or release operation once the
    conditional loop has exited.
    
    This patch adds relaxed versions of the conditional spinning primitives
    to avoid unnecessary barrier overhead on architectures such as arm64.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 29458bbb2fa0..2cafdbb9ae4c 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -221,18 +221,17 @@ do {									\
 #endif
 
 /**
- * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
+ * smp_cond_load_relaxed() - (Spin) wait for cond with no ordering guarantees
  * @ptr: pointer to the variable to wait on
  * @cond: boolean expression to wait for
  *
- * Equivalent to using smp_load_acquire() on the condition variable but employs
- * the control dependency of the wait to reduce the barrier on many platforms.
+ * Equivalent to using READ_ONCE() on the condition variable.
  *
  * Due to C lacking lambda expressions we load the value of *ptr into a
  * pre-named variable @VAL to be used in @cond.
  */
-#ifndef smp_cond_load_acquire
-#define smp_cond_load_acquire(ptr, cond_expr) ({		\
+#ifndef smp_cond_load_relaxed
+#define smp_cond_load_relaxed(ptr, cond_expr) ({		\
 	typeof(ptr) __PTR = (ptr);				\
 	typeof(*ptr) VAL;					\
 	for (;;) {						\
@@ -241,10 +240,26 @@ do {									\
 			break;					\
 		cpu_relax();					\
 	}							\
-	smp_acquire__after_ctrl_dep();				\
 	VAL;							\
 })
 #endif
 
+/**
+ * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
+ * @ptr: pointer to the variable to wait on
+ * @cond: boolean expression to wait for
+ *
+ * Equivalent to using smp_load_acquire() on the condition variable but employs
+ * the control dependency of the wait to reduce the barrier on many platforms.
+ */
+#ifndef smp_cond_load_acquire
+#define smp_cond_load_acquire(ptr, cond_expr) ({		\
+	typeof(*ptr) _val;					\
+	_val = smp_cond_load_relaxed(ptr, cond_expr);		\
+	smp_acquire__after_ctrl_dep();				\
+	_val;							\
+})
+#endif
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 739d875dd6982618020d30f58f8acf10f6076e6d
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 8 09:48:46 2018 +0000

    mn10300: Remove the architecture
    
    Remove the MN10300 arch as the hardware is defunct.
    
    Suggested-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    cc: linux-am33-list@redhat.com
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index fe297b599b0a..29458bbb2fa0 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -1,5 +1,5 @@
 /*
- * Generic barrier definitions, originally based on MN10300 definitions.
+ * Generic barrier definitions.
  *
  * It should be possible to use these on really simple architectures,
  * but it serves more as a starting point for new ports.

commit 726328d92a42b6d4b76078e2659f43067f82c4e8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 26 10:35:03 2016 +0200

    locking/spinlock, arch: Update and fix spin_unlock_wait() implementations
    
    This patch updates/fixes all spin_unlock_wait() implementations.
    
    The update is in semantics; where it previously was only a control
    dependency, we now upgrade to a full load-acquire to match the
    store-release from the spin_unlock() we waited on. This ensures that
    when spin_unlock_wait() returns, we're guaranteed to observe the full
    critical section we waited on.
    
    This fixes a number of spin_unlock_wait() users that (not
    unreasonably) rely on this.
    
    I also fixed a number of ticket lock versions to only wait on the
    current lock holder, instead of for a full unlock, as this is
    sufficient.
    
    Furthermore; again for ticket locks; I added an smp_rmb() in between
    the initial ticket load and the spin loop testing the current value
    because I could not convince myself the address dependency is
    sufficient, esp. if the loads are of different sizes.
    
    I'm more than happy to remove this smp_rmb() again if people are
    certain the address dependency does indeed work as expected.
    
    Note: PPC32 will be fixed independently
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: chris@zankel.net
    Cc: cmetcalf@mellanox.com
    Cc: davem@davemloft.net
    Cc: dhowells@redhat.com
    Cc: james.hogan@imgtec.com
    Cc: jejb@parisc-linux.org
    Cc: linux@armlinux.org.uk
    Cc: mpe@ellerman.id.au
    Cc: ralf@linux-mips.org
    Cc: realmz6@gmail.com
    Cc: rkuo@codeaurora.org
    Cc: rth@twiddle.net
    Cc: schwidefsky@de.ibm.com
    Cc: tony.luck@intel.com
    Cc: vgupta@synopsys.com
    Cc: ysato@users.sourceforge.jp
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index ab7b0bd7d4dd..fe297b599b0a 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -194,7 +194,7 @@ do {									\
 })
 #endif
 
-#endif
+#endif	/* CONFIG_SMP */
 
 /* Barriers for virtual machine guests when talking to an SMP host */
 #define virt_mb() __smp_mb()

commit 7cb45c0fe9858f92cc264f6bf9d2f6a0e7d3b249
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 1 19:23:54 2016 +0200

    locking/barriers: Move smp_cond_load_acquire() to asm-generic/barrier.h
    
    Since all asm/barrier.h should/must include asm-generic/barrier.h the
    latter is a good place for generic infrastructure like this.
    
    This also allows archs to override the new smp_acquire__after_ctrl_dep().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 1cceca146905..ab7b0bd7d4dd 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -207,5 +207,44 @@ do {									\
 #define virt_store_release(p, v) __smp_store_release(p, v)
 #define virt_load_acquire(p) __smp_load_acquire(p)
 
+/**
+ * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
+ *
+ * A control dependency provides a LOAD->STORE order, the additional RMB
+ * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
+ * aka. (load)-ACQUIRE.
+ *
+ * Architectures that do not do load speculation can have this be barrier().
+ */
+#ifndef smp_acquire__after_ctrl_dep
+#define smp_acquire__after_ctrl_dep()		smp_rmb()
+#endif
+
+/**
+ * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
+ * @ptr: pointer to the variable to wait on
+ * @cond: boolean expression to wait for
+ *
+ * Equivalent to using smp_load_acquire() on the condition variable but employs
+ * the control dependency of the wait to reduce the barrier on many platforms.
+ *
+ * Due to C lacking lambda expressions we load the value of *ptr into a
+ * pre-named variable @VAL to be used in @cond.
+ */
+#ifndef smp_cond_load_acquire
+#define smp_cond_load_acquire(ptr, cond_expr) ({		\
+	typeof(ptr) __PTR = (ptr);				\
+	typeof(*ptr) VAL;					\
+	for (;;) {						\
+		VAL = READ_ONCE(*__PTR);			\
+		if (cond_expr)					\
+			break;					\
+		cpu_relax();					\
+	}							\
+	smp_acquire__after_ctrl_dep();				\
+	VAL;							\
+})
+#endif
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 6a65d26385bf487926a0616650927303058551e3
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Dec 27 18:23:01 2015 +0200

    asm-generic: implement virt_xxx memory barriers
    
    Guests running within virtual machines might be affected by SMP effects even if
    the guest itself is compiled without SMP support.  This is an artifact of
    interfacing with an SMP host while running an UP kernel.  Using mandatory
    barriers for this use-case would be possible but is often suboptimal.
    
    In particular, virtio uses a bunch of confusing ifdefs to work around
    this, while xen just uses the mandatory barriers.
    
    To better handle this case, low-level virt_mb() etc macros are made available.
    These are implemented trivially using the low-level __smp_xxx macros,
    the purpose of these wrappers is to annotate those specific cases.
    
    These have the same effect as smp_mb() etc when SMP is enabled, but generate
    identical code for SMP and non-SMP systems. For example, virtual machine guests
    should use virt_mb() rather than smp_mb() when synchronizing against a
    (possibly SMP) host.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 8752964f8ff7..1cceca146905 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -196,5 +196,16 @@ do {									\
 
 #endif
 
+/* Barriers for virtual machine guests when talking to an SMP host */
+#define virt_mb() __smp_mb()
+#define virt_rmb() __smp_rmb()
+#define virt_wmb() __smp_wmb()
+#define virt_read_barrier_depends() __smp_read_barrier_depends()
+#define virt_store_mb(var, value) __smp_store_mb(var, value)
+#define virt_mb__before_atomic() __smp_mb__before_atomic()
+#define virt_mb__after_atomic()	__smp_mb__after_atomic()
+#define virt_store_release(p, v) __smp_store_release(p, v)
+#define virt_load_acquire(p) __smp_load_acquire(p)
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit a9e4252a9b147043142282ebb65da94dcb951e2a
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Dec 27 13:50:07 2015 +0200

    asm-generic: add __smp_xxx wrappers
    
    On !SMP, most architectures define their
    barriers as compiler barriers.
    On SMP, most need an actual barrier.
    
    Make it possible to remove the code duplication for
    !SMP by defining low-level __smp_xxx barriers
    which do not depend on the value of SMP, then
    use them from asm-generic conditionally.
    
    Besides reducing code duplication, these low level APIs will also be
    useful for virtualization, where a barrier is sometimes needed even if
    !SMP since we might be talking to another kernel on the same SMP system.
    
    Both virtio and Xen drivers will benefit.
    
    The smp_xxx variants should use __smp_XXX ones or barrier() depending on
    SMP, identically for all architectures.
    
    We keep ifndef guards around them for now - once/if all
    architectures are converted to use the generic
    code, we'll be able to remove these.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 987b2e04ad5a..8752964f8ff7 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -54,22 +54,38 @@
 #define read_barrier_depends()		do { } while (0)
 #endif
 
+#ifndef __smp_mb
+#define __smp_mb()	mb()
+#endif
+
+#ifndef __smp_rmb
+#define __smp_rmb()	rmb()
+#endif
+
+#ifndef __smp_wmb
+#define __smp_wmb()	wmb()
+#endif
+
+#ifndef __smp_read_barrier_depends
+#define __smp_read_barrier_depends()	read_barrier_depends()
+#endif
+
 #ifdef CONFIG_SMP
 
 #ifndef smp_mb
-#define smp_mb()	mb()
+#define smp_mb()	__smp_mb()
 #endif
 
 #ifndef smp_rmb
-#define smp_rmb()	rmb()
+#define smp_rmb()	__smp_rmb()
 #endif
 
 #ifndef smp_wmb
-#define smp_wmb()	wmb()
+#define smp_wmb()	__smp_wmb()
 #endif
 
 #ifndef smp_read_barrier_depends
-#define smp_read_barrier_depends()	read_barrier_depends()
+#define smp_read_barrier_depends()	__smp_read_barrier_depends()
 #endif
 
 #else	/* !CONFIG_SMP */
@@ -92,23 +108,78 @@
 
 #endif	/* CONFIG_SMP */
 
+#ifndef __smp_store_mb
+#define __smp_store_mb(var, value)  do { WRITE_ONCE(var, value); __smp_mb(); } while (0)
+#endif
+
+#ifndef __smp_mb__before_atomic
+#define __smp_mb__before_atomic()	__smp_mb()
+#endif
+
+#ifndef __smp_mb__after_atomic
+#define __smp_mb__after_atomic()	__smp_mb()
+#endif
+
+#ifndef __smp_store_release
+#define __smp_store_release(p, v)					\
+do {									\
+	compiletime_assert_atomic_type(*p);				\
+	__smp_mb();							\
+	WRITE_ONCE(*p, v);						\
+} while (0)
+#endif
+
+#ifndef __smp_load_acquire
+#define __smp_load_acquire(p)						\
+({									\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
+	compiletime_assert_atomic_type(*p);				\
+	__smp_mb();							\
+	___p1;								\
+})
+#endif
+
+#ifdef CONFIG_SMP
+
+#ifndef smp_store_mb
+#define smp_store_mb(var, value)  __smp_store_mb(var, value)
+#endif
+
+#ifndef smp_mb__before_atomic
+#define smp_mb__before_atomic()	__smp_mb__before_atomic()
+#endif
+
+#ifndef smp_mb__after_atomic
+#define smp_mb__after_atomic()	__smp_mb__after_atomic()
+#endif
+
+#ifndef smp_store_release
+#define smp_store_release(p, v) __smp_store_release(p, v)
+#endif
+
+#ifndef smp_load_acquire
+#define smp_load_acquire(p) __smp_load_acquire(p)
+#endif
+
+#else	/* !CONFIG_SMP */
+
 #ifndef smp_store_mb
-#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); smp_mb(); } while (0)
+#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); barrier(); } while (0)
 #endif
 
 #ifndef smp_mb__before_atomic
-#define smp_mb__before_atomic()	smp_mb()
+#define smp_mb__before_atomic()	barrier()
 #endif
 
 #ifndef smp_mb__after_atomic
-#define smp_mb__after_atomic()	smp_mb()
+#define smp_mb__after_atomic()	barrier()
 #endif
 
 #ifndef smp_store_release
 #define smp_store_release(p, v)						\
 do {									\
 	compiletime_assert_atomic_type(*p);				\
-	smp_mb();							\
+	barrier();							\
 	WRITE_ONCE(*p, v);						\
 } while (0)
 #endif
@@ -118,10 +189,12 @@ do {									\
 ({									\
 	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
-	smp_mb();							\
+	barrier();							\
 	___p1;								\
 })
 #endif
 
+#endif
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 57f7c0370f386d5e0960e25d2c3ceb0b8e8c489d
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Dec 27 14:10:52 2015 +0200

    asm-generic: guard smp_store_release/load_acquire
    
    Allow architectures to override smp_store_release
    and smp_load_acquire by guarding the defines
    in asm-generic/barrier.h with ifndef directives.
    
    This is in preparation to reusing asm-generic/barrier.h
    on architectures which have their own definition
    of these macros.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 0f45f93ef692..987b2e04ad5a 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -104,13 +104,16 @@
 #define smp_mb__after_atomic()	smp_mb()
 #endif
 
+#ifndef smp_store_release
 #define smp_store_release(p, v)						\
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
 	WRITE_ONCE(*p, v);						\
 } while (0)
+#endif
 
+#ifndef smp_load_acquire
 #define smp_load_acquire(p)						\
 ({									\
 	typeof(*p) ___p1 = READ_ONCE(*p);				\
@@ -118,6 +121,7 @@ do {									\
 	smp_mb();							\
 	___p1;								\
 })
+#endif
 
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 5a1b26d7c629915446222ebe77d16567c98426ff
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Dec 31 12:09:13 2015 +0200

    lcoking/barriers, arch: Use smp barriers in smp_store_release()
    
    With commit b92b8b35a2e ("locking/arch: Rename set_mb() to smp_store_mb()")
    it was made clear that the context of this call (and thus set_mb)
    is strictly for CPU ordering, as opposed to IO. As such all archs
    should use the smp variant of mb(), respecting the semantics and
    saving a mandatory barrier on UP.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: dave@stgolabs.net
    Link: http://lkml.kernel.org/r/1445975631-17047-3-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index b42afada1280..0f45f93ef692 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -93,7 +93,7 @@
 #endif	/* CONFIG_SMP */
 
 #ifndef smp_store_mb
-#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)
+#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); smp_mb(); } while (0)
 #endif
 
 #ifndef smp_mb__before_atomic

commit 76695af20c015206cffb84b15912be6797d0cca2
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Sun Aug 2 17:11:04 2015 +0200

    locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire()
    
    Replace ACCESS_ONCE() macro in smp_store_release() and smp_load_acquire()
    with WRITE_ONCE() and READ_ONCE() on x86, arm, arm64, ia64, metag, mips,
    powerpc, s390, sparc and asm-generic since ACCESS_ONCE() does not work
    reliably on non-scalar types.
    
    WRITE_ONCE() and READ_ONCE() were introduced in the following commits:
    
      230fa253df63 ("kernel: Provide READ_ONCE and ASSIGN_ONCE")
      43239cbe79fc ("kernel: Change ASSIGN_ONCE(val, x) to WRITE_ONCE(x, val)")
    
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Alexander Duyck <alexander.h.duyck@redhat.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arch@vger.kernel.org
    Link: http://lkml.kernel.org/r/1438528264-714-1-git-send-email-andreyknvl@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 55e3abc2d027..b42afada1280 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -108,12 +108,12 @@
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
 	___p1;								\

commit 0890a264794f33df540fbaf274699146903b4e6b
Merge: 05a8256c586a 40b8ad8f762c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 1 09:24:26 2015 -0700

    Merge tag 'arc-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/vgupta/arc
    
    Pull ARC architecture updates from Vineet Gupta:
    
     - support for HS38 cores based on ARCv2 ISA
    
         ARCv2 is the next generation ISA from Synopsys and basis for the
         HS3{4,6,8} families of processors which retain the traditional ARC mantra of
         low power and configurability and are now more performant and feature rich.
    
         HS38x is a 10 stage pipeline core which supports MMU (with huge pages) and
         SMP (upto 4 cores) among other features.
    
         + www.synopsys.com/dw/ipdir.php?ds=arc-hs38-processor
         + http://news.synopsys.com/2014-10-14-New-DesignWare-ARC-HS38-Processor-Doubles-Performance-for-Embedded-Linux-Applications
         + http://www.embedded.com/electronics-news/4435975/Synopsys-ARC-HS38-core-gives-2X-boost-to-Linux-based-apps
    
     - support for ARC SDP (Software Development platform): Main Board + CPU Cards
        = AXS101: CPU Card with ARC700 in silicon @ 700 MHz
        = AXS103: CPU Card with HS38x in FPGA
    
     - refactoring of ARCompact port to accomodate new ARCv2 ISA
    
     - misc updates/cleanups
    
    * tag 'arc-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/vgupta/arc: (72 commits)
      ARC: Fix build failures for ARCompact in linux-next after ARCv2 support
      ARCv2: Allow older gcc to cope with new regime of ARCv2/ARCompact support
      ARCv2: [vdk] dts files and defconfig for HS38 VDK
      ARCv2: [axs103] Support ARC SDP FPGA platform for HS38x cores
      ARC: [axs101] Prepare for AXS103
      ARCv2: [nsim*hs*] Support simulation platforms for HS38x cores
      ARCv2: All bits in place, allow ARCv2 builds
      ARCv2: SLC: Handle explcit flush for DMA ops (w/o IO-coherency)
      ARCv2: STAR 9000837815 workaround hardware exclusive transactions livelock
      ARC: Reduce bitops lines of code using macros
      ARCv2: barriers
      arch: conditionally define smp_{mb,rmb,wmb}
      ARC: add smp barriers around atomics per Documentation/atomic_ops.txt
      ARC: add compiler barrier to LLSC based cmpxchg
      ARCv2: SMP: intc: IDU 2nd level intc for dynamic IRQ distribution
      ARCv2: SMP: clocksource: Enable Global Real Time counter
      ARCv2: SMP: ARConnect debug/robustness
      ARCv2: SMP: Support ARConnect (MCIP) for Inter-Core-Interrupts et al
      ARC: make plat_smp_ops weak to allow over-rides
      ARCv2: clocksource: Introduce 64bit local RTC counter
      ...

commit 470c27e4695add2a4e167467775081870d83185a
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Wed Mar 11 21:42:02 2015 +0530

    arch: conditionally define smp_{mb,rmb,wmb}
    
    That way arches can define the minimal versions and still #include
    asm-generic for defaults (vs. defining defaults in arch code)
    
    See new barrier.h in arc for usage !
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index f5c40b0fadc2..270b7c989ea1 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -55,17 +55,42 @@
 #endif
 
 #ifdef CONFIG_SMP
+
+#ifndef smp_mb
 #define smp_mb()	mb()
+#endif
+
+#ifndef smp_rmb
 #define smp_rmb()	rmb()
+#endif
+
+#ifndef smp_wmb
 #define smp_wmb()	wmb()
+#endif
+
+#ifndef smp_read_barrier_depends
 #define smp_read_barrier_depends()	read_barrier_depends()
+#endif
+
 #else
+#ifndef smp_mb
 #define smp_mb()	barrier()
+#endif
+
+#ifndef smp_rmb
 #define smp_rmb()	barrier()
+#endif
+
+#ifndef smp_wmb
 #define smp_wmb()	barrier()
+#endif
+
+#ifndef smp_read_barrier_depends
 #define smp_read_barrier_depends()	do { } while (0)
 #endif
 
+#endif
+
 #ifndef set_mb
 #define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
 #endif

commit b92b8b35a2e38bde319fd1d68ec84628c1f1b0fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:51:55 2015 +0200

    locking/arch: Rename set_mb() to smp_store_mb()
    
    Since set_mb() is really about an smp_mb() -- not a IO/DMA barrier
    like mb() rename it to match the recent smp_load_acquire() and
    smp_store_release().
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 3938716b44d7..e6a83d712ef6 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -66,8 +66,8 @@
 #define smp_read_barrier_depends()	do { } while (0)
 #endif
 
-#ifndef set_mb
-#define set_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)
+#ifndef smp_store_mb
+#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)
 #endif
 
 #ifndef smp_mb__before_atomic

commit ab3f02fc237211f0583c1e7ba3bf504747be9b8d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:52:27 2015 +0200

    locking/arch: Add WRITE_ONCE() to set_mb()
    
    Since we assume set_mb() to result in a single store followed by a
    full memory barrier, employ WRITE_ONCE().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index f5c40b0fadc2..3938716b44d7 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -67,7 +67,7 @@
 #endif
 
 #ifndef set_mb
-#define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
+#define set_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)
 #endif
 
 #ifndef smp_mb__before_atomic

commit 1077fa36f23e259858caf6f269a47393a5aff523
Author: Alexander Duyck <alexander.h.duyck@redhat.com>
Date:   Thu Dec 11 15:02:06 2014 -0800

    arch: Add lightweight memory barriers dma_rmb() and dma_wmb()
    
    There are a number of situations where the mandatory barriers rmb() and
    wmb() are used to order memory/memory operations in the device drivers
    and those barriers are much heavier than they actually need to be.  For
    example in the case of PowerPC wmb() calls the heavy-weight sync
    instruction when for coherent memory operations all that is really needed
    is an lsync or eieio instruction.
    
    This commit adds a coherent only version of the mandatory memory barriers
    rmb() and wmb().  In most cases this should result in the barrier being the
    same as the SMP barriers for the SMP case, however in some cases we use a
    barrier that is somewhere in between rmb() and smp_rmb().  For example on
    ARM the rmb barriers break down as follows:
    
      Barrier   Call     Explanation
      --------- -------- ----------------------------------
      rmb()     dsb()    Data synchronization barrier - system
      dma_rmb() dmb(osh) data memory barrier - outer sharable
      smp_rmb() dmb(ish) data memory barrier - inner sharable
    
    These new barriers are not as safe as the standard rmb() and wmb().
    Specifically they do not guarantee ordering between coherent and incoherent
    memories.  The primary use case for these would be to enforce ordering of
    reads and writes when accessing coherent memory that is shared between the
    CPU and a device.
    
    It may also be noted that there is no dma_mb().  Most architectures don't
    provide a good mechanism for performing a coherent only full barrier without
    resorting to the same mechanism used in mb().  As such there isn't much to
    be gained in trying to define such a function.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 1402fa855388..f5c40b0fadc2 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -42,6 +42,14 @@
 #define wmb()	mb()
 #endif
 
+#ifndef dma_rmb
+#define dma_rmb()	rmb()
+#endif
+
+#ifndef dma_wmb
+#define dma_wmb()	wmb()
+#endif
+
 #ifndef read_barrier_depends
 #define read_barrier_depends()		do { } while (0)
 #endif

commit febdbfe8a91ce0d11939d4940b592eb0dba8d663
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Feb 6 18:16:07 2014 +0100

    arch: Prepare for smp_mb__{before,after}_atomic()
    
    Since the smp_mb__{before,after}*() ops are fundamentally dependent on
    how an arch can implement atomics it doesn't make sense to have 3
    variants of them. They must all be the same.
    
    Furthermore, the 3 variants suggest they're only valid for those 3
    atomic ops, while we have many more where they could be applied.
    
    So move away from
    smp_mb__{before,after}_{atomic,clear}_{dec,inc,bit}() and reduce the
    interface to just the two: smp_mb__{before,after}_atomic().
    
    This patch prepares the way by introducing default implementations in
    asm-generic/barrier.h that default to a full barrier and providing
    __deprecated inlines for the previous 6 barriers if they're not
    provided by the arch.
    
    This should allow for a mostly painless transition (lots of deprecated
    warns in the interim).
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-wr59327qdyi9mbzn6x937s4e@git.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Chen, Gong" <gong.chen@linux.intel.com>
    Cc: John Sullivan <jsrhbz@kanargh.force9.co.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mauro Carvalho Chehab <m.chehab@samsung.com>
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 6f692f8ac664..1402fa855388 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -62,6 +62,14 @@
 #define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
 #endif
 
+#ifndef smp_mb__before_atomic
+#define smp_mb__before_atomic()	smp_mb()
+#endif
+
+#ifndef smp_mb__after_atomic
+#define smp_mb__after_atomic()	smp_mb()
+#endif
+
 #define smp_store_release(p, v)						\
 do {									\
 	compiletime_assert_atomic_type(*p);				\

commit 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()
    
    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.
    
    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.
    
    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.
    
    [Changelog by PaulMck]
    
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index d12a90f93689..6f692f8ac664 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -62,5 +62,20 @@
 #define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
 #endif
 
+#define smp_store_release(p, v)						\
+do {									\
+	compiletime_assert_atomic_type(*p);				\
+	smp_mb();							\
+	ACCESS_ONCE(*p) = (v);						\
+} while (0)
+
+#define smp_load_acquire(p)						\
+({									\
+	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	compiletime_assert_atomic_type(*p);				\
+	smp_mb();							\
+	___p1;								\
+})
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 93ea02bb84354370e51de803a9405f171f3edf88
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Clean up asm/barrier.h implementations using asm-generic/barrier.h
    
    We're going to be adding a few new barrier primitives, and in order to
    avoid endless duplication make more agressive use of
    asm-generic/barrier.h.
    
    Change the asm-generic/barrier.h such that it allows partial barrier
    definitions and fills out the rest with defaults.
    
    There are a few architectures (m32r, m68k) that could probably
    do away with their barrier.h file entirely but are kept for now due to
    their unconventional nop() implementation.
    
    Suggested-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20131213150640.846368594@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 639d7a4d033b..d12a90f93689 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -1,4 +1,5 @@
-/* Generic barrier definitions, based on MN10300 definitions.
+/*
+ * Generic barrier definitions, originally based on MN10300 definitions.
  *
  * It should be possible to use these on really simple architectures,
  * but it serves more as a starting point for new ports.
@@ -16,35 +17,50 @@
 
 #ifndef __ASSEMBLY__
 
-#define nop() asm volatile ("nop")
+#include <linux/compiler.h>
+
+#ifndef nop
+#define nop()	asm volatile ("nop")
+#endif
 
 /*
- * Force strict CPU ordering.
- * And yes, this is required on UP too when we're talking
- * to devices.
+ * Force strict CPU ordering. And yes, this is required on UP too when we're
+ * talking to devices.
  *
- * This implementation only contains a compiler barrier.
+ * Fall back to compiler barriers if nothing better is provided.
  */
 
-#define mb()	asm volatile ("": : :"memory")
+#ifndef mb
+#define mb()	barrier()
+#endif
+
+#ifndef rmb
 #define rmb()	mb()
-#define wmb()	asm volatile ("": : :"memory")
+#endif
+
+#ifndef wmb
+#define wmb()	mb()
+#endif
+
+#ifndef read_barrier_depends
+#define read_barrier_depends()		do { } while (0)
+#endif
 
 #ifdef CONFIG_SMP
 #define smp_mb()	mb()
 #define smp_rmb()	rmb()
 #define smp_wmb()	wmb()
+#define smp_read_barrier_depends()	read_barrier_depends()
 #else
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
 #define smp_wmb()	barrier()
+#define smp_read_barrier_depends()	do { } while (0)
 #endif
 
-#define set_mb(var, value)  do { var = value;  mb(); } while (0)
-#define set_wmb(var, value) do { var = value; wmb(); } while (0)
-
-#define read_barrier_depends()		do {} while (0)
-#define smp_read_barrier_depends()	do {} while (0)
+#ifndef set_mb
+#define set_mb(var, value)  do { (var) = (value); mb(); } while (0)
+#endif
 
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_GENERIC_BARRIER_H */

commit 885df91ca3571afd4b3f50f3391329bbf3c9e262
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Create asm-generic/barrier.h
    
    Create asm-generic/barrier.h and move the barrier definitions from
    asm-generic/system.h to it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
new file mode 100644
index 000000000000..639d7a4d033b
--- /dev/null
+++ b/include/asm-generic/barrier.h
@@ -0,0 +1,50 @@
+/* Generic barrier definitions, based on MN10300 definitions.
+ *
+ * It should be possible to use these on really simple architectures,
+ * but it serves more as a starting point for new ports.
+ *
+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public Licence
+ * as published by the Free Software Foundation; either version
+ * 2 of the Licence, or (at your option) any later version.
+ */
+#ifndef __ASM_GENERIC_BARRIER_H
+#define __ASM_GENERIC_BARRIER_H
+
+#ifndef __ASSEMBLY__
+
+#define nop() asm volatile ("nop")
+
+/*
+ * Force strict CPU ordering.
+ * And yes, this is required on UP too when we're talking
+ * to devices.
+ *
+ * This implementation only contains a compiler barrier.
+ */
+
+#define mb()	asm volatile ("": : :"memory")
+#define rmb()	mb()
+#define wmb()	asm volatile ("": : :"memory")
+
+#ifdef CONFIG_SMP
+#define smp_mb()	mb()
+#define smp_rmb()	rmb()
+#define smp_wmb()	wmb()
+#else
+#define smp_mb()	barrier()
+#define smp_rmb()	barrier()
+#define smp_wmb()	barrier()
+#endif
+
+#define set_mb(var, value)  do { var = value;  mb(); } while (0)
+#define set_wmb(var, value) do { var = value; wmb(); } while (0)
+
+#define read_barrier_depends()		do {} while (0)
+#define smp_read_barrier_depends()	do {} while (0)
+
+#endif /* !__ASSEMBLY__ */
+#endif /* __ASM_GENERIC_BARRIER_H */
