commit 6546b19f95acc986807de981402bbac6b3a94b0f
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Mar 25 21:45:29 2020 +0900

    perf/core: Add PERF_SAMPLE_CGROUP feature
    
    The PERF_SAMPLE_CGROUP bit is to save (perf_event) cgroup information in
    the sample.  It will add a 64-bit id to identify current cgroup and it's
    the file handle in the cgroup file system.  Userspace should use this
    information with PERF_RECORD_CGROUP event to match which cgroup it
    belongs.
    
    I put it before PERF_SAMPLE_AUX for simplicity since it just needs a
    64-bit word.  But if we want bigger samples, I can work on that
    direction too.
    
    Committer testing:
    
      $ pahole perf_sample_data | grep -w cgroup -B5 -A5
            /* --- cacheline 4 boundary (256 bytes) was 56 bytes ago --- */
            struct perf_regs           regs_intr;            /*   312    16 */
            /* --- cacheline 5 boundary (320 bytes) was 8 bytes ago --- */
            u64                        stack_user_size;      /*   328     8 */
            u64                        phys_addr;            /*   336     8 */
            u64                        cgroup;               /*   344     8 */
    
            /* size: 384, cachelines: 6, members: 22 */
            /* padding: 32 */
      };
      $
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lore.kernel.org/lkml/20200325124536.2800725-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index de95f6c7b273..7b2d6fc9e6ed 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -142,8 +142,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_REGS_INTR			= 1U << 18,
 	PERF_SAMPLE_PHYS_ADDR			= 1U << 19,
 	PERF_SAMPLE_AUX				= 1U << 20,
+	PERF_SAMPLE_CGROUP			= 1U << 21,
 
-	PERF_SAMPLE_MAX = 1U << 21,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 22,		/* non-ABI */
 
 	__PERF_SAMPLE_CALLCHAIN_EARLY		= 1ULL << 63, /* non-ABI; internal use */
 };

commit 96aaab686505c449e24d76e76507290dcc30e008
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Mar 25 21:45:28 2020 +0900

    perf/core: Add PERF_RECORD_CGROUP event
    
    To support cgroup tracking, add CGROUP event to save a link between
    cgroup path and id number.  This is needed since cgroups can go away
    when userspace tries to read the cgroup info (from the id) later.
    
    The attr.cgroup bit was also added to enable cgroup tracking from
    userspace.
    
    This event will be generated when a new cgroup becomes active.
    Userspace might need to synthesize those events for existing cgroups.
    
    Committer testing:
    
    From the resulting kernel, using /sys/kernel/btf/vmlinux:
    
      $ pahole perf_event_attr | grep -w cgroup -B5 -A1
            __u64                      write_backward:1;     /*    40:27  8 */
            __u64                      namespaces:1;         /*    40:28  8 */
            __u64                      ksymbol:1;            /*    40:29  8 */
            __u64                      bpf_event:1;          /*    40:30  8 */
            __u64                      aux_output:1;         /*    40:31  8 */
            __u64                      cgroup:1;             /*    40:32  8 */
            __u64                      __reserved_1:31;      /*    40:33  8 */
      $
    
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    [staticize perf_event_cgroup function]
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lore.kernel.org/lkml/20200325124536.2800725-2-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 397cfd65b3fe..de95f6c7b273 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -381,7 +381,8 @@ struct perf_event_attr {
 				ksymbol        :  1, /* include ksymbol events */
 				bpf_event      :  1, /* include bpf events */
 				aux_output     :  1, /* generate AUX records instead of events */
-				__reserved_1   : 32;
+				cgroup         :  1, /* include cgroup events */
+				__reserved_1   : 31;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -1012,6 +1013,16 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_BPF_EVENT			= 18,
 
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u64				id;
+	 *	char				path[];
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_CGROUP			= 19,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit bbfd5e4fab63703375eafaf241a0c696024a59e1
Author: Kan Liang <kan.liang@linux.intel.com>
Date:   Mon Jan 27 08:53:54 2020 -0800

    perf/core: Add new branch sample type for HW index of raw branch records
    
    The low level index is the index in the underlying hardware buffer of
    the most recently captured taken branch which is always saved in
    branch_entries[0]. It is very useful for reconstructing the call stack.
    For example, in Intel LBR call stack mode, the depth of reconstructed
    LBR call stack limits to the number of LBR registers. With the low level
    index information, perf tool may stitch the stacks of two samples. The
    reconstructed LBR call stack can break the HW limitation.
    
    Add a new branch sample type to retrieve low level index of raw branch
    records. The low level index is between -1 (unknown) and max depth which
    can be retrieved in /sys/devices/cpu/caps/branches.
    
    Only when the new branch sample type is set, the low level index
    information is dumped into the PERF_SAMPLE_BRANCH_STACK output.
    Perf tool should check the attr.branch_sample_type, and apply the
    corresponding format for PERF_SAMPLE_BRANCH_STACK samples.
    Otherwise, some user case may be broken. For example, users may parse a
    perf.data, which include the new branch sample type, with an old version
    perf tool (without the check). Users probably get incorrect information
    without any warning.
    
    Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200127165355.27495-2-kan.liang@linux.intel.com

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 377d794d3105..397cfd65b3fe 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -181,6 +181,8 @@ enum perf_branch_sample_type_shift {
 
 	PERF_SAMPLE_BRANCH_TYPE_SAVE_SHIFT	= 16, /* save branch type */
 
+	PERF_SAMPLE_BRANCH_HW_INDEX_SHIFT	= 17, /* save low level index of raw branch records */
+
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
 
@@ -208,6 +210,8 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_TYPE_SAVE	=
 		1U << PERF_SAMPLE_BRANCH_TYPE_SAVE_SHIFT,
 
+	PERF_SAMPLE_BRANCH_HW_INDEX	= 1U << PERF_SAMPLE_BRANCH_HW_INDEX_SHIFT,
+
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };
 
@@ -853,7 +857,9 @@ enum perf_event_type {
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
 	 *
 	 *	{ u64                   nr;
-	 *        { u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
+	 *	  { u64	hw_idx; } && PERF_SAMPLE_BRANCH_HW_INDEX
+	 *        { u64 from, to, flags } lbr[nr];
+	 *      } && PERF_SAMPLE_BRANCH_STACK
 	 *
 	 * 	{ u64			abi; # enum perf_sample_regs_abi
 	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER

commit a4faf00d994c40e64f656805ac375c65e324eefb
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Fri Oct 25 17:08:33 2019 +0300

    perf/aux: Allow using AUX data in perf samples
    
    AUX data can be used to annotate perf events such as performance counters
    or tracepoints/breakpoints by including it in sample records when
    PERF_SAMPLE_AUX flag is set. Such samples would be instrumental in debugging
    and profiling by providing, for example, a history of instruction flow
    leading up to the event's overflow.
    
    The implementation makes use of grouping an AUX event with all the events
    that wish to take samples of the AUX data, such that the former is the
    group leader. The samplees should also specify the desired size of the AUX
    sample via attr.aux_sample_size.
    
    AUX capable PMUs need to explicitly add support for sampling, because it
    relies on a new callback to take a snapshot of the buffer without touching
    the event states.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: adrian.hunter@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: https://lkml.kernel.org/r/20191025140835.53665-2-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index bb7b271397a6..377d794d3105 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -141,8 +141,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_TRANSACTION			= 1U << 17,
 	PERF_SAMPLE_REGS_INTR			= 1U << 18,
 	PERF_SAMPLE_PHYS_ADDR			= 1U << 19,
+	PERF_SAMPLE_AUX				= 1U << 20,
 
-	PERF_SAMPLE_MAX = 1U << 20,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 21,		/* non-ABI */
 
 	__PERF_SAMPLE_CALLCHAIN_EARLY		= 1ULL << 63, /* non-ABI; internal use */
 };
@@ -300,6 +301,7 @@ enum perf_event_read_format {
 					/* add: sample_stack_user */
 #define PERF_ATTR_SIZE_VER4	104	/* add: sample_regs_intr */
 #define PERF_ATTR_SIZE_VER5	112	/* add: aux_watermark */
+#define PERF_ATTR_SIZE_VER6	120	/* add: aux_sample_size */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
@@ -424,7 +426,9 @@ struct perf_event_attr {
 	 */
 	__u32	aux_watermark;
 	__u16	sample_max_stack;
-	__u16	__reserved_2;	/* align to __u64 */
+	__u16	__reserved_2;
+	__u32	aux_sample_size;
+	__u32	__reserved_3;
 };
 
 /*
@@ -864,6 +868,8 @@ enum perf_event_type {
 	 *	{ u64			abi; # enum perf_sample_regs_abi
 	 *	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_INTR
 	 *	{ u64			phys_addr;} && PERF_SAMPLE_PHYS_ADDR
+	 *	{ u64			size;
+	 *	  char			data[size]; } && PERF_SAMPLE_AUX
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit ab43762ef010967e4ccd53627f70a2eecbeafefb
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Tue Aug 6 11:46:00 2019 +0300

    perf: Allow normal events to output AUX data
    
    In some cases, ordinary (non-AUX) events can generate data for AUX events.
    For example, PEBS events can come out as records in the Intel PT stream
    instead of their usual DS records, if configured to do so.
    
    One requirement for such events is to consistently schedule together, to
    ensure that the data from the "AUX output" events isn't lost while their
    corresponding AUX event is not scheduled. We use grouping to provide this
    guarantee: an "AUX output" event can be added to a group where an AUX event
    is a group leader, and provided that the former supports writing to the
    latter.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: kan.liang@linux.intel.com
    Link: https://lkml.kernel.org/r/20190806084606.4021-2-alexander.shishkin@linux.intel.com

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 7198ddd0c6b1..bb7b271397a6 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -374,7 +374,8 @@ struct perf_event_attr {
 				namespaces     :  1, /* include namespaces data */
 				ksymbol        :  1, /* include ksymbol events */
 				bpf_event      :  1, /* include bpf events */
-				__reserved_1   : 33;
+				aux_output     :  1, /* generate AUX records instead of events */
+				__reserved_1   : 32;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */

commit 6ee52e2a3fe4ea35520720736e6791df1fb67106
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:15 2019 -0800

    perf, bpf: Introduce PERF_RECORD_BPF_EVENT
    
    For better performance analysis of BPF programs, this patch introduces
    PERF_RECORD_BPF_EVENT, a new perf_event_type that exposes BPF program
    load/unload information to user space.
    
    Each BPF program may contain up to BPF_MAX_SUBPROGS (256) sub programs.
    The following example shows kernel symbols for a BPF program with 7 sub
    programs:
    
        ffffffffa0257cf9 t bpf_prog_b07ccb89267cf242_F
        ffffffffa02592e1 t bpf_prog_2dcecc18072623fc_F
        ffffffffa025b0e9 t bpf_prog_bb7a405ebaec5d5c_F
        ffffffffa025dd2c t bpf_prog_a7540d4a39ec1fc7_F
        ffffffffa025fcca t bpf_prog_05762d4ade0e3737_F
        ffffffffa026108f t bpf_prog_db4bd11e35df90d4_F
        ffffffffa0263f00 t bpf_prog_89d64e4abf0f0126_F
        ffffffffa0257cf9 t bpf_prog_ae31629322c4b018__dummy_tracepoi
    
    When a bpf program is loaded, PERF_RECORD_KSYMBOL is generated for each
    of these sub programs. Therefore, PERF_RECORD_BPF_EVENT is not needed
    for simple profiling.
    
    For annotation, user space need to listen to PERF_RECORD_BPF_EVENT and
    gather more information about these (sub) programs via sys_bpf.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradeaed.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-4-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 1dee5c8f166b..7198ddd0c6b1 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -373,7 +373,8 @@ struct perf_event_attr {
 				write_backward :  1, /* Write ring buffer from end to beginning */
 				namespaces     :  1, /* include namespaces data */
 				ksymbol        :  1, /* include ksymbol events */
-				__reserved_1   : 34;
+				bpf_event      :  1, /* include bpf events */
+				__reserved_1   : 33;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -979,6 +980,25 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_KSYMBOL			= 17,
 
+	/*
+	 * Record bpf events:
+	 *  enum perf_bpf_event_type {
+	 *	PERF_BPF_EVENT_UNKNOWN		= 0,
+	 *	PERF_BPF_EVENT_PROG_LOAD	= 1,
+	 *	PERF_BPF_EVENT_PROG_UNLOAD	= 2,
+	 *  };
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u16				type;
+	 *	u16				flags;
+	 *	u32				id;
+	 *	u8				tag[BPF_TAG_SIZE];
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_BPF_EVENT			= 18,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 
@@ -990,6 +1010,13 @@ enum perf_record_ksymbol_type {
 
 #define PERF_RECORD_KSYMBOL_FLAGS_UNREGISTER	(1 << 0)
 
+enum perf_bpf_event_type {
+	PERF_BPF_EVENT_UNKNOWN		= 0,
+	PERF_BPF_EVENT_PROG_LOAD	= 1,
+	PERF_BPF_EVENT_PROG_UNLOAD	= 2,
+	PERF_BPF_EVENT_MAX,		/* non-ABI */
+};
+
 #define PERF_MAX_STACK_DEPTH		127
 #define PERF_MAX_CONTEXTS_PER_STACK	  8
 

commit 76193a94522f1d4edf2447a536f3f796ce56343b
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:13 2019 -0800

    perf, bpf: Introduce PERF_RECORD_KSYMBOL
    
    For better performance analysis of dynamically JITed and loaded kernel
    functions, such as BPF programs, this patch introduces
    PERF_RECORD_KSYMBOL, a new perf_event_type that exposes kernel symbol
    register/unregister information to user space.
    
    The following data structure is used for PERF_RECORD_KSYMBOL.
    
        /*
         * struct {
         *      struct perf_event_header        header;
         *      u64                             addr;
         *      u32                             len;
         *      u16                             ksym_type;
         *      u16                             flags;
         *      char                            name[];
         *      struct sample_id                sample_id;
         * };
         */
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-2-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index ea19b5d491bf..1dee5c8f166b 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -372,7 +372,8 @@ struct perf_event_attr {
 				context_switch :  1, /* context switch data */
 				write_backward :  1, /* Write ring buffer from end to beginning */
 				namespaces     :  1, /* include namespaces data */
-				__reserved_1   : 35;
+				ksymbol        :  1, /* include ksymbol events */
+				__reserved_1   : 34;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -963,9 +964,32 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_NAMESPACES			= 16,
 
+	/*
+	 * Record ksymbol register/unregister events:
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u64				addr;
+	 *	u32				len;
+	 *	u16				ksym_type;
+	 *	u16				flags;
+	 *	char				name[];
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_KSYMBOL			= 17,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 
+enum perf_record_ksymbol_type {
+	PERF_RECORD_KSYMBOL_TYPE_UNKNOWN	= 0,
+	PERF_RECORD_KSYMBOL_TYPE_BPF		= 1,
+	PERF_RECORD_KSYMBOL_TYPE_MAX		/* non-ABI */
+};
+
+#define PERF_RECORD_KSYMBOL_FLAGS_UNREGISTER	(1 << 0)
+
 #define PERF_MAX_STACK_DEPTH		127
 #define PERF_MAX_CONTEXTS_PER_STACK	  8
 

commit ad07c8ceb6631a83b62d405a61448bba92adac68
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Thu Jan 10 13:53:34 2019 +0000

    perf/core: Remove unused perf_flags
    
    Now that perf_flags is not used we remove it.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Sascha Hauer <s.hauer@pengutronix.de>
    Cc: Shawn Guo <shawnguo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: robin.murphy@arm.com
    Cc: suzuki.poulose@arm.com
    Link: https://lkml.kernel.org/r/1547128414-50693-13-git-send-email-andrew.murray@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 9de8780ac8d9..ea19b5d491bf 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -445,8 +445,6 @@ struct perf_event_query_bpf {
 	__u32	ids[0];
 };
 
-#define perf_flags(attr)	(*(&(attr)->read_format + 1))
-
 /*
  * Ioctls that can be done on a perf event fd:
  */

commit 4f8f382e635707ddaddf8269a116e4f8cc8835c0
Author: David Miller <davem@davemloft.net>
Date:   Tue Oct 30 22:24:04 2018 -0700

    perf tools: Don't clone maps from parent when synthesizing forks
    
    When synthesizing FORK events, we are trying to create thread objects
    for the already running tasks on the machine.
    
    Normally, for a kernel FORK event, we want to clone the parent's maps
    because that is what the kernel just did.
    
    But when synthesizing, this should not be done.  If we do, we end up
    with overlapping maps as we process the sythesized MMAP2 events that
    get delivered shortly thereafter.
    
    Use the FORK event misc flags in an internal way to signal this
    situation, so we can elide the map clone when appropriate.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Joe Mario <jmario@redhat.com>
    Link: http://lkml.kernel.org/r/20181030.222404.2085088822877051075.davem@davemloft.net
    [ Added comment about flag use in machine__process_fork_event(),
      use ternary op in thread__clone_map_groups() as suggested by Jiri ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index f35eb72739c0..9de8780ac8d9 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -646,10 +646,12 @@ struct perf_event_mmap_page {
  *
  *   PERF_RECORD_MISC_MMAP_DATA  - PERF_RECORD_MMAP* events
  *   PERF_RECORD_MISC_COMM_EXEC  - PERF_RECORD_COMM event
+ *   PERF_RECORD_MISC_FORK_EXEC  - PERF_RECORD_FORK event (perf internal)
  *   PERF_RECORD_MISC_SWITCH_OUT - PERF_RECORD_SWITCH* events
  */
 #define PERF_RECORD_MISC_MMAP_DATA		(1 << 13)
 #define PERF_RECORD_MISC_COMM_EXEC		(1 << 13)
+#define PERF_RECORD_MISC_FORK_EXEC		(1 << 13)
 #define PERF_RECORD_MISC_SWITCH_OUT		(1 << 13)
 /*
  * These PERF_RECORD_MISC_* flags below are safely reused

commit 09121255c784fd36ad6237a4e239c634b0209de0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 29 14:13:13 2018 +0200

    perf/UAPI: Clearly mark __PERF_SAMPLE_CALLCHAIN_EARLY as internal use
    
    Vince noted that commit:
    
      6cbc304f2f36 ("perf/x86/intel: Fix unwind errors from PEBS entries (mk-II)")
    
    'leaked' __PERF_SAMPLE_CALLCHAIN_EARLY into the UAPI namespace. And
    while sys_perf_event_open() will error out if you try to use it, it is
    exposed.
    
    Clearly mark it for internal use only to avoid any confusion.
    
    Requested-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index eeb787b1c53c..f35eb72739c0 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -144,7 +144,7 @@ enum perf_event_sample_format {
 
 	PERF_SAMPLE_MAX = 1U << 20,		/* non-ABI */
 
-	__PERF_SAMPLE_CALLCHAIN_EARLY		= 1ULL << 63,
+	__PERF_SAMPLE_CALLCHAIN_EARLY		= 1ULL << 63, /* non-ABI; internal use */
 };
 
 /*

commit 6cbc304f2f360f25cc8607817239d6f4a2fd3dc5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 10 15:48:41 2018 +0200

    perf/x86/intel: Fix unwind errors from PEBS entries (mk-II)
    
    Vince reported the perf_fuzzer giving various unwinder warnings and
    Josh reported:
    
    > Deja vu.  Most of these are related to perf PEBS, similar to the
    > following issue:
    >
    >   b8000586c90b ("perf/x86/intel: Cure bogus unwind from PEBS entries")
    >
    > This is basically the ORC version of that.  setup_pebs_sample_data() is
    > assembling a franken-pt_regs which ORC isn't happy about.  RIP is
    > inconsistent with some of the other registers (like RSP and RBP).
    
    And where the previous unwinder only needed BP,SP ORC also requires
    IP. But we cannot spoof IP because then the sample will get displaced,
    entirely negating the point of PEBS.
    
    So cure the whole thing differently by doing the unwind early; this
    does however require a means to communicate we did the unwind early.
    We (ab)use an unused sample_type bit for this, which we set on events
    that fill out the data->callchain before the normal
    perf_prepare_sample().
    
    Debugged-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index b8e288a1f740..eeb787b1c53c 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -143,6 +143,8 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_PHYS_ADDR			= 1U << 19,
 
 	PERF_SAMPLE_MAX = 1U << 20,		/* non-ABI */
+
+	__PERF_SAMPLE_CALLCHAIN_EARLY		= 1ULL << 63,
 };
 
 /*

commit 101592b4904ecf6b8ed2a4784d41d180319d95a1
Author: Alexey Budankov <alexey.budankov@linux.intel.com>
Date:   Mon Apr 9 10:25:32 2018 +0300

    perf/core: Store context switch out type in PERF_RECORD_SWITCH[_CPU_WIDE]
    
    Store preempting context switch out event into Perf trace as a part of
    PERF_RECORD_SWITCH[_CPU_WIDE] record.
    
    Percentage of preempting and non-preempting context switches help
    understanding the nature of workloads (CPU or IO bound) that are running
    on a machine;
    
    The event is treated as preemption one when task->state value of the
    thread being switched out is TASK_RUNNING. Event type encoding is
    implemented using PERF_RECORD_MISC_SWITCH_OUT_PREEMPT bit;
    
    Signed-off-by: Alexey Budankov <alexey.budankov@linux.intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/9ff84e83-a0ca-dd82-a6d0-cb951689be74@linux.intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 912b85b52344..b8e288a1f740 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -650,11 +650,23 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_COMM_EXEC		(1 << 13)
 #define PERF_RECORD_MISC_SWITCH_OUT		(1 << 13)
 /*
- * Indicates that the content of PERF_SAMPLE_IP points to
- * the actual instruction that triggered the event. See also
- * perf_event_attr::precise_ip.
+ * These PERF_RECORD_MISC_* flags below are safely reused
+ * for the following events:
+ *
+ *   PERF_RECORD_MISC_EXACT_IP           - PERF_RECORD_SAMPLE of precise events
+ *   PERF_RECORD_MISC_SWITCH_OUT_PREEMPT - PERF_RECORD_SWITCH* events
+ *
+ *
+ * PERF_RECORD_MISC_EXACT_IP:
+ *   Indicates that the content of PERF_SAMPLE_IP points to
+ *   the actual instruction that triggered the event. See also
+ *   perf_event_attr::precise_ip.
+ *
+ * PERF_RECORD_MISC_SWITCH_OUT_PREEMPT:
+ *   Indicates that thread was preempted in TASK_RUNNING state.
  */
 #define PERF_RECORD_MISC_EXACT_IP		(1 << 14)
+#define PERF_RECORD_MISC_SWITCH_OUT_PREEMPT	(1 << 14)
 /*
  * Reserve the last bit to indicate some extended misc field
  */

commit 32ff77e8cc9e66cc4fb38098f64fd54cc8f54573
Author: Milind Chabbi <chabbi.milind@gmail.com>
Date:   Mon Mar 12 14:45:47 2018 +0100

    perf/core: Implement fast breakpoint modification via _IOC_MODIFY_ATTRIBUTES
    
    Problem and motivation: Once a breakpoint perf event (PERF_TYPE_BREAKPOINT)
    is created, there is no flexibility to change the breakpoint type
    (bp_type), breakpoint address (bp_addr), or breakpoint length (bp_len). The
    only option is to close the perf event and configure a new breakpoint
    event. This inflexibility has a significant performance overhead. For
    example, sampling-based, lightweight performance profilers (and also
    concurrency bug detection tools),  monitor different addresses for a short
    duration using PERF_TYPE_BREAKPOINT and change the address (bp_addr) to
    another address or change the kind of breakpoint (bp_type) from  "write" to
    a "read" or vice-versa or change the length (bp_len) of the address being
    monitored. The cost of these modifications is prohibitive since it involves
    unmapping the circular buffer associated with the perf event, closing the
    perf event, opening another perf event and mmaping another circular buffer.
    
    Solution: The new ioctl flag for perf events,
    PERF_EVENT_IOC_MODIFY_ATTRIBUTES, introduced in this patch takes a pointer
    to a struct perf_event_attr as an argument to update an old breakpoint
    event with new address, type, and size. This facility allows retaining a
    previous mmaped perf events ring buffer and avoids having to close and
    reopen another perf event.
    
    This patch supports only changing PERF_TYPE_BREAKPOINT event type; future
    implementations can extend this feature. The patch replicates some of its
    functionality of modify_user_hw_breakpoint() in
    kernel/events/hw_breakpoint.c. modify_user_hw_breakpoint cannot be called
    directly since perf_event_ctx_lock() is already held in _perf_ioctl().
    
    Evidence: Experiments show that the baseline (not able to modify an already
    created breakpoint) costs an order of magnitude (~10x) more than the
    suggested optimization (having the ability to dynamically modifying a
    configured breakpoint via ioctl). When the breakpoints typically do not
    trap, the speedup due to the suggested optimization is ~10x; even when the
    breakpoints always trap, the speedup is ~4x due to the suggested
    optimization.
    
    Testing: tests posted at
    https://github.com/linux-contrib/perf_event_modify_bp demonstrate the
    performance significance of this patch. Tests also check the functional
    correctness of the patch.
    
    Signed-off-by: Milind Chabbi <chabbi.milind@gmail.com>
    [ Using modify_user_hw_breakpoint_check function. ]
    [ Reformated PERF_EVENT_IOC_*, so the values are all in one column. ]
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Cc: Jin Yao <yao.jin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Oleg Nesterov <onestero@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20180312134548.31532-8-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 6f873503552d..912b85b52344 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -448,17 +448,18 @@ struct perf_event_query_bpf {
 /*
  * Ioctls that can be done on a perf event fd:
  */
-#define PERF_EVENT_IOC_ENABLE		_IO ('$', 0)
-#define PERF_EVENT_IOC_DISABLE		_IO ('$', 1)
-#define PERF_EVENT_IOC_REFRESH		_IO ('$', 2)
-#define PERF_EVENT_IOC_RESET		_IO ('$', 3)
-#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
-#define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
-#define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
-#define PERF_EVENT_IOC_ID		_IOR('$', 7, __u64 *)
-#define PERF_EVENT_IOC_SET_BPF		_IOW('$', 8, __u32)
-#define PERF_EVENT_IOC_PAUSE_OUTPUT	_IOW('$', 9, __u32)
-#define PERF_EVENT_IOC_QUERY_BPF	_IOWR('$', 10, struct perf_event_query_bpf *)
+#define PERF_EVENT_IOC_ENABLE			_IO ('$', 0)
+#define PERF_EVENT_IOC_DISABLE			_IO ('$', 1)
+#define PERF_EVENT_IOC_REFRESH			_IO ('$', 2)
+#define PERF_EVENT_IOC_RESET			_IO ('$', 3)
+#define PERF_EVENT_IOC_PERIOD			_IOW('$', 4, __u64)
+#define PERF_EVENT_IOC_SET_OUTPUT		_IO ('$', 5)
+#define PERF_EVENT_IOC_SET_FILTER		_IOW('$', 6, char *)
+#define PERF_EVENT_IOC_ID			_IOR('$', 7, __u64 *)
+#define PERF_EVENT_IOC_SET_BPF			_IOW('$', 8, __u32)
+#define PERF_EVENT_IOC_PAUSE_OUTPUT		_IOW('$', 9, __u32)
+#define PERF_EVENT_IOC_QUERY_BPF		_IOWR('$', 10, struct perf_event_query_bpf *)
+#define PERF_EVENT_IOC_MODIFY_ATTRIBUTES	_IOW('$', 11, struct perf_event_attr *)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit 7057bb975dab827997e0ca9dd92cafef0856b0cc
Merge: 33ea4b24277b 297f9233b53a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 17 11:39:28 2018 +0100

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 65074d43fc77bcae32776724b7fa2696923c78e4
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 6 14:45:13 2017 -0800

    perf/core: Prepare perf_event.h for new types: 'perf_kprobe' and 'perf_uprobe'
    
    Two new perf types, perf_kprobe and perf_uprobe, will be added to allow
    creating [k,u]probe with perf_event_open. These [k,u]probe are associated
    with the file decriptor created by perf_event_open(), thus are easy to
    clean when the file descriptor is destroyed.
    
    kprobe_func and uprobe_path are added to union config1 for pointers to
    function name for kprobe or binary path for uprobe.
    
    kprobe_addr and probe_offset are added to union config2 for kernel
    address (when kprobe_func is NULL), or [k,u]probe offset.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: <daniel@iogearbox.net>
    Cc: <davem@davemloft.net>
    Cc: <kernel-team@fb.com>
    Cc: <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171206224518.3598254-4-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index c77c9a2ebbbb..5d49cfc509e7 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -380,10 +380,14 @@ struct perf_event_attr {
 	__u32			bp_type;
 	union {
 		__u64		bp_addr;
+		__u64		kprobe_func; /* for perf_kprobe */
+		__u64		uprobe_path; /* for perf_uprobe */
 		__u64		config1; /* extension of config */
 	};
 	union {
 		__u64		bp_len;
+		__u64		kprobe_addr; /* when kprobe_func == NULL */
+		__u64		probe_offset; /* for perf_[k,u]probe */
 		__u64		config2; /* extension of config1 */
 	};
 	__u64	branch_sample_type; /* enum perf_branch_sample_type */

commit b2fe5fa68642860e7de76167c3111623aa0d5de1
Merge: a103950e0dd2 a54667f6728c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 31 14:31:10 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Significantly shrink the core networking routing structures. Result
        of http://vger.kernel.org/~davem/seoul2017_netdev_keynote.pdf
    
     2) Add netdevsim driver for testing various offloads, from Jakub
        Kicinski.
    
     3) Support cross-chip FDB operations in DSA, from Vivien Didelot.
    
     4) Add a 2nd listener hash table for TCP, similar to what was done for
        UDP. From Martin KaFai Lau.
    
     5) Add eBPF based queue selection to tun, from Jason Wang.
    
     6) Lockless qdisc support, from John Fastabend.
    
     7) SCTP stream interleave support, from Xin Long.
    
     8) Smoother TCP receive autotuning, from Eric Dumazet.
    
     9) Lots of erspan tunneling enhancements, from William Tu.
    
    10) Add true function call support to BPF, from Alexei Starovoitov.
    
    11) Add explicit support for GRO HW offloading, from Michael Chan.
    
    12) Support extack generation in more netlink subsystems. From Alexander
        Aring, Quentin Monnet, and Jakub Kicinski.
    
    13) Add 1000BaseX, flow control, and EEE support to mvneta driver. From
        Russell King.
    
    14) Add flow table abstraction to netfilter, from Pablo Neira Ayuso.
    
    15) Many improvements and simplifications to the NFP driver bpf JIT,
        from Jakub Kicinski.
    
    16) Support for ipv6 non-equal cost multipath routing, from Ido
        Schimmel.
    
    17) Add resource abstration to devlink, from Arkadi Sharshevsky.
    
    18) Packet scheduler classifier shared filter block support, from Jiri
        Pirko.
    
    19) Avoid locking in act_csum, from Davide Caratti.
    
    20) devinet_ioctl() simplifications from Al viro.
    
    21) More TCP bpf improvements from Lawrence Brakmo.
    
    22) Add support for onlink ipv6 route flag, similar to ipv4, from David
        Ahern.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1925 commits)
      tls: Add support for encryption using async offload accelerator
      ip6mr: fix stale iterator
      net/sched: kconfig: Remove blank help texts
      openvswitch: meter: Use 64-bit arithmetic instead of 32-bit
      tcp_nv: fix potential integer overflow in tcpnv_acked
      r8169: fix RTL8168EP take too long to complete driver initialization.
      qmi_wwan: Add support for Quectel EP06
      rtnetlink: enable IFLA_IF_NETNSID for RTM_NEWLINK
      ipmr: Fix ptrdiff_t print formatting
      ibmvnic: Wait for device response when changing MAC
      qlcnic: fix deadlock bug
      tcp: release sk_frag.page in tcp_disconnect
      ipv4: Get the address of interface correctly.
      net_sched: gen_estimator: fix lockdep splat
      net: macb: Handle HRESP error
      net/mlx5e: IPoIB, Fix copy-paste bug in flow steering refactoring
      ipv6: addrconf: break critical section in addrconf_verify_rtnl()
      ipv6: change route cache aging logic
      i40e/i40evf: Update DESC_NEEDED value to reflect larger value
      bnxt_en: cleanup DIM work on device shutdown
      ...

commit 972c14884728bf5f69ec69cfb1beeec1a9cd29ee
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jan 7 17:03:51 2018 +0100

    perf: Update PERF_RECORD_MISC_* comment for perf_event_header::misc bit 13
    
    The perf_event_header::misc bit 13 is shared on different events and
    next patch is adding yet another bit 13 user.  Updating the comment to
    make it more structured and clear which events use bit 13.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20180107160356.28203-8-jolsa@kernel.org
    [ Update the tools/include/uapi/linux copy ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 8bb66e8da945..c77c9a2ebbbb 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -612,9 +612,12 @@ struct perf_event_mmap_page {
  */
 #define PERF_RECORD_MISC_PROC_MAP_PARSE_TIMEOUT	(1 << 12)
 /*
- * PERF_RECORD_MISC_MMAP_DATA and PERF_RECORD_MISC_COMM_EXEC are used on
- * different events so can reuse the same bit position.
- * Ditto PERF_RECORD_MISC_SWITCH_OUT.
+ * Following PERF_RECORD_MISC_* are used on different
+ * events, so can reuse the same bit position:
+ *
+ *   PERF_RECORD_MISC_MMAP_DATA  - PERF_RECORD_MMAP* events
+ *   PERF_RECORD_MISC_COMM_EXEC  - PERF_RECORD_COMM event
+ *   PERF_RECORD_MISC_SWITCH_OUT - PERF_RECORD_SWITCH* events
  */
 #define PERF_RECORD_MISC_MMAP_DATA		(1 << 13)
 #define PERF_RECORD_MISC_COMM_EXEC		(1 << 13)

commit 81df978c49379481716aef591de77313c286d747
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Sun Jan 7 17:03:48 2018 +0100

    perf: Add sample_id to PERF_RECORD_ITRACE_START event comment
    
    Adding missing sample_id line into PERF_RECORD_ITRACE_START
    event comment.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20180107160356.28203-5-jolsa@kernel.org
    [ Update the tools/include/uapi/linux copy ]
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index b9a4953018ed..8bb66e8da945 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -864,6 +864,7 @@ enum perf_event_type {
 	 *	struct perf_event_header	header;
 	 *	u32				pid;
 	 *	u32				tid;
+	 *	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_ITRACE_START		= 12,

commit f371b304f12e31fe30207c41ca7754564e0ea4dc
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Dec 11 11:39:02 2017 -0800

    bpf/tracing: allow user space to query prog array on the same tp
    
    Commit e87c6bc3852b ("bpf: permit multiple bpf attachments
    for a single perf event") added support to attach multiple
    bpf programs to a single perf event.
    Although this provides flexibility, users may want to know
    what other bpf programs attached to the same tp interface.
    Besides getting visibility for the underlying bpf system,
    such information may also help consolidate multiple bpf programs,
    understand potential performance issues due to a large array,
    and debug (e.g., one bpf program which overwrites return code
    may impact subsequent program results).
    
    Commit 2541517c32be ("tracing, perf: Implement BPF programs
    attached to kprobes") utilized the existing perf ioctl
    interface and added the command PERF_EVENT_IOC_SET_BPF
    to attach a bpf program to a tracepoint. This patch adds a new
    ioctl command, given a perf event fd, to query the bpf program
    array attached to the same perf tracepoint event.
    
    The new uapi ioctl command:
      PERF_EVENT_IOC_QUERY_BPF
    
    The new uapi/linux/perf_event.h structure:
      struct perf_event_query_bpf {
           __u32    ids_len;
           __u32    prog_cnt;
           __u32    ids[0];
      };
    
    User space provides buffer "ids" for kernel to copy to.
    When returning from the kernel, the number of available
    programs in the array is set in "prog_cnt".
    
    The usage:
      struct perf_event_query_bpf *query =
        malloc(sizeof(*query) + sizeof(u32) * ids_len);
      query.ids_len = ids_len;
      err = ioctl(pmu_efd, PERF_EVENT_IOC_QUERY_BPF, query);
      if (err == 0) {
        /* query.prog_cnt is the number of available progs,
         * number of progs in ids: (ids_len == 0) ? 0 : query.prog_cnt
         */
      } else if (errno == ENOSPC) {
        /* query.ids_len number of progs copied,
         * query.prog_cnt is the number of available progs
         */
      } else {
          /* other errors */
      }
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index b9a4953018ed..769533696483 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -418,6 +418,27 @@ struct perf_event_attr {
 	__u16	__reserved_2;	/* align to __u64 */
 };
 
+/*
+ * Structure used by below PERF_EVENT_IOC_QUERY_BPF command
+ * to query bpf programs attached to the same perf tracepoint
+ * as the given perf event.
+ */
+struct perf_event_query_bpf {
+	/*
+	 * The below ids array length
+	 */
+	__u32	ids_len;
+	/*
+	 * Set by the kernel to indicate the number of
+	 * available programs
+	 */
+	__u32	prog_cnt;
+	/*
+	 * User provided buffer to store program ids
+	 */
+	__u32	ids[0];
+};
+
 #define perf_flags(attr)	(*(&(attr)->read_format + 1))
 
 /*
@@ -433,6 +454,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_ID		_IOR('$', 7, __u64 *)
 #define PERF_EVENT_IOC_SET_BPF		_IOW('$', 8, __u32)
 #define PERF_EVENT_IOC_PAUSE_OUTPUT	_IOW('$', 9, __u32)
+#define PERF_EVENT_IOC_QUERY_BPF	_IOWR('$', 10, struct perf_event_query_bpf *)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit c9b012e5f4a1d01dfa8abc6318211a67ba7d5db2
Merge: b293fca43be5 6cfa7cc46b1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 10:56:56 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The big highlight is support for the Scalable Vector Extension (SVE)
      which required extensive ABI work to ensure we don't break existing
      applications by blowing away their signal stack with the rather large
      new vector context (<= 2 kbit per vector register). There's further
      work to be done optimising things like exception return, but the ABI
      is solid now.
    
      Much of the line count comes from some new PMU drivers we have, but
      they're pretty self-contained and I suspect we'll have more of them in
      future.
    
      Plenty of acronym soup here:
    
       - initial support for the Scalable Vector Extension (SVE)
    
       - improved handling for SError interrupts (required to handle RAS
         events)
    
       - enable GCC support for 128-bit integer types
    
       - remove kernel text addresses from backtraces and register dumps
    
       - use of WFE to implement long delay()s
    
       - ACPI IORT updates from Lorenzo Pieralisi
    
       - perf PMU driver for the Statistical Profiling Extension (SPE)
    
       - perf PMU driver for Hisilicon's system PMUs
    
       - misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (97 commits)
      arm64: Make ARMV8_DEPRECATED depend on SYSCTL
      arm64: Implement __lshrti3 library function
      arm64: support __int128 on gcc 5+
      arm64/sve: Add documentation
      arm64/sve: Detect SVE and activate runtime support
      arm64/sve: KVM: Hide SVE from CPU features exposed to guests
      arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
      arm64/sve: KVM: Prevent guests from using SVE
      arm64/sve: Add sysctl to set the default vector length for new processes
      arm64/sve: Add prctl controls for userspace vector length management
      arm64/sve: ptrace and ELF coredump support
      arm64/sve: Preserve SVE registers around EFI runtime service calls
      arm64/sve: Preserve SVE registers around kernel-mode NEON use
      arm64/sve: Probe SVE capabilities and usable vector lengths
      arm64: cpufeature: Move sys_caps_initialised declarations
      arm64/sve: Backend logic for setting the vector length
      arm64/sve: Signal handling support
      arm64/sve: Support vector length resetting for new processes
      arm64/sve: Core task context handling
      arm64/sve: Low-level CPU setup
      ...

commit e2be04c7f9958dde770eeb8b30e829ca969b37bb
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:09:13 2017 +0100

    License cleanup: add SPDX license identifier to uapi header files with a license
    
    Many user space API headers have licensing information, which is either
    incomplete, badly formatted or just a shorthand for referring to the
    license under which the file is supposed to be.  This makes it hard for
    compliance tools to determine the correct license.
    
    Update these files with an SPDX license identifier.  The identifier was
    chosen based on the license information in the file.
    
    GPL/LGPL licensed headers get the matching GPL/LGPL SPDX license
    identifier with the added 'WITH Linux-syscall-note' exception, which is
    the officially assigned exception identifier for the kernel syscall
    exception:
    
       NOTE! This copyright does *not* cover user programs that use kernel
       services by normal system calls - this is merely considered normal use
       of the kernel, and does *not* fall under the heading of "derived work".
    
    This exception makes it possible to include GPL headers into non GPL
    code, without confusing license compliance tools.
    
    Headers which have either explicit dual licensing or are just licensed
    under a non GPL license are updated with the corresponding SPDX
    identifier and the GPLv2 with syscall exception identifier.  The format
    is:
            ((GPL-2.0 WITH Linux-syscall-note) OR SPDX-ID-OF-OTHER-LICENSE)
    
    SPDX license identifiers are a legally binding shorthand, which can be
    used instead of the full boiler plate text.  The update does not remove
    existing license information as this has to be done on a case by case
    basis and the copyright holders might have to be consulted. This will
    happen in a separate step.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.  See the previous patch in this series for the
    methodology of how this patch was researched.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 140ae638cfd6..362493a2f950 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  * Performance events:
  *

commit 085b30625e39df67d7320f22269796276c6b0c11
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 23 14:05:12 2016 +0100

    perf/core: Add PERF_AUX_FLAG_COLLISION to report colliding samples
    
    The ARM SPE architecture permits an implementation to ignore a sample
    if the sample is due to be taken whilst another sample is already being
    produced. In this case, it is desirable to report the collision to
    userspace, as they may want to lower the sample period.
    
    This patch adds a PERF_AUX_FLAG_COLLISION flag, so that such events can
    be relayed to userspace.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 140ae638cfd6..7ca1b22ea417 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -941,6 +941,7 @@ enum perf_callchain_context {
 #define PERF_AUX_FLAG_TRUNCATED		0x01	/* record was truncated to fit */
 #define PERF_AUX_FLAG_OVERWRITE		0x02	/* snapshot from overwrite mode */
 #define PERF_AUX_FLAG_PARTIAL		0x04	/* record contains gaps */
+#define PERF_AUX_FLAG_COLLISION		0x08	/* sample collided with another */
 
 #define PERF_FLAG_FD_NO_GROUP		(1UL << 0)
 #define PERF_FLAG_FD_OUTPUT		(1UL << 1)

commit fc7ce9c74c3ad232b084d80148654f926d01ece7
Author: Kan Liang <kan.liang@intel.com>
Date:   Mon Aug 28 20:52:49 2017 -0400

    perf/core, x86: Add PERF_SAMPLE_PHYS_ADDR
    
    For understanding how the workload maps to memory channels and hardware
    behavior, it's very important to collect address maps with physical
    addresses. For example, 3D XPoint access can only be found by filtering
    the physical address.
    
    Add a new sample type for physical address.
    
    perf already has a facility to collect data virtual address. This patch
    introduces a function to convert the virtual address to physical address.
    The function is quite generic and can be extended to any architecture as
    long as a virtual address is provided.
    
     - For kernel direct mapping addresses, virt_to_phys is used to convert
       the virtual addresses to physical address.
    
     - For user virtual addresses, __get_user_pages_fast is used to walk the
       pages tables for user physical address.
    
     - This does not work for vmalloc addresses right now. These are not
       resolved, but code to do that could be added.
    
    The new sample type requires collecting the virtual address. The
    virtual address will not be output unless SAMPLE_ADDR is applied.
    
    For security, the physical address can only be exposed to root or
    privileged user.
    
    Tested-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: mpe@ellerman.id.au
    Link: http://lkml.kernel.org/r/1503967969-48278-1-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 2a37ae925d85..140ae638cfd6 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -139,8 +139,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_IDENTIFIER			= 1U << 16,
 	PERF_SAMPLE_TRANSACTION			= 1U << 17,
 	PERF_SAMPLE_REGS_INTR			= 1U << 18,
+	PERF_SAMPLE_PHYS_ADDR			= 1U << 19,
 
-	PERF_SAMPLE_MAX = 1U << 19,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 20,		/* non-ABI */
 };
 
 /*
@@ -814,6 +815,7 @@ enum perf_event_type {
 	 *	{ u64			transaction; } && PERF_SAMPLE_TRANSACTION
 	 *	{ u64			abi; # enum perf_sample_regs_abi
 	 *	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_INTR
+	 *	{ u64			phys_addr;} && PERF_SAMPLE_PHYS_ADDR
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit 6ae5fa61d27dcb055f4198bcf6c8dbbf1bb33f52
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed Aug 16 15:21:54 2017 -0700

    perf/x86: Fix data source decoding for Skylake
    
    Skylake changed the encoding of the PEBS data source field.
    Some combinations are not available anymore, but some new cases
    e.g. for L4 cache hit are added.
    
    Fix up the conversion table for Skylake, similar as had been done
    for Nehalem.
    
    On Skylake server the encoding for L4 actually means persistent
    memory. Handle this case too.
    
    To properly describe it in the abstracted perf format I had to add
    some new fields. Since a hit can have only one level add a new
    field that is an enumeration, not a bit field to describe
    the level. It can describe any level. Some numbers are also
    used to describe PMEM and LFB.
    
    Also add a new generic remote flag that can be combined with
    the generic level to signify a remote cache.
    
    And there is an extension field for the snoop indication to handle
    the Forward state.
    
    I didn't add a generic flag for hops because it's not needed
    for Skylake.
    
    I changed the existing encodings for older CPUs to also fill in the
    new level and remote fields.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: http://lkml.kernel.org/r/20170816222156.19953-3-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 642db5fa3286..2a37ae925d85 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -954,14 +954,20 @@ union perf_mem_data_src {
 			mem_snoop:5,	/* snoop mode */
 			mem_lock:2,	/* lock instr */
 			mem_dtlb:7,	/* tlb access */
-			mem_rsvd:31;
+			mem_lvl_num:4,	/* memory hierarchy level number */
+			mem_remote:1,   /* remote */
+			mem_snoopx:2,	/* snoop mode, ext */
+			mem_rsvd:24;
 	};
 };
 #elif defined(__BIG_ENDIAN_BITFIELD)
 union perf_mem_data_src {
 	__u64 val;
 	struct {
-		__u64	mem_rsvd:31,
+		__u64	mem_rsvd:24,
+			mem_snoopx:2,	/* snoop mode, ext */
+			mem_remote:1,   /* remote */
+			mem_lvl_num:4,	/* memory hierarchy level number */
 			mem_dtlb:7,	/* tlb access */
 			mem_lock:2,	/* lock instr */
 			mem_snoop:5,	/* snoop mode */
@@ -998,6 +1004,22 @@ union perf_mem_data_src {
 #define PERF_MEM_LVL_UNC	0x2000 /* Uncached memory */
 #define PERF_MEM_LVL_SHIFT	5
 
+#define PERF_MEM_REMOTE_REMOTE	0x01  /* Remote */
+#define PERF_MEM_REMOTE_SHIFT	37
+
+#define PERF_MEM_LVLNUM_L1	0x01 /* L1 */
+#define PERF_MEM_LVLNUM_L2	0x02 /* L2 */
+#define PERF_MEM_LVLNUM_L3	0x03 /* L3 */
+#define PERF_MEM_LVLNUM_L4	0x04 /* L4 */
+/* 5-0xa available */
+#define PERF_MEM_LVLNUM_ANY_CACHE 0x0b /* Any cache */
+#define PERF_MEM_LVLNUM_LFB	0x0c /* LFB */
+#define PERF_MEM_LVLNUM_RAM	0x0d /* RAM */
+#define PERF_MEM_LVLNUM_PMEM	0x0e /* PMEM */
+#define PERF_MEM_LVLNUM_NA	0x0f /* N/A */
+
+#define PERF_MEM_LVLNUM_SHIFT	33
+
 /* snoop mode */
 #define PERF_MEM_SNOOP_NA	0x01 /* not available */
 #define PERF_MEM_SNOOP_NONE	0x02 /* no snoop */
@@ -1006,6 +1028,10 @@ union perf_mem_data_src {
 #define PERF_MEM_SNOOP_HITM	0x10 /* snoop hit modified */
 #define PERF_MEM_SNOOP_SHIFT	19
 
+#define PERF_MEM_SNOOPX_FWD	0x01 /* forward */
+/* 1 free */
+#define PERF_MEM_SNOOPX_SHIFT	37
+
 /* locked instruction */
 #define PERF_MEM_LOCK_NA	0x01 /* not available */
 #define PERF_MEM_LOCK_LOCKED	0x02 /* locked transaction */

commit eb0baf8a0d9259d168523b8e7c436b55ade7c546
Author: Jin Yao <yao.jin@linux.intel.com>
Date:   Tue Jul 18 20:13:09 2017 +0800

    perf/core: Define the common branch type classification
    
    It is often useful to know the branch types while analyzing branch data.
    For example, a call is very different from a conditional branch.
    
    Currently we have to look it up in binary while the binary may later not
    be available and even the binary is available but user has to take some
    time. It is very useful for user to check it directly in perf report.
    
    Perf already has support for disassembling the branch instruction to get
    the x86 branch type.
    
    To keep consistent on kernel and userspace and make the classification
    more common, the patch adds the common branch type classification
    in perf_event.h.
    
    The patch only defines a minimum but most common set of branch types.
    
    PERF_BR_UNKNOWN         : unknown
    PERF_BR_COND            ：conditional
    PERF_BR_UNCOND          : unconditional
    PERF_BR_IND             : indirect
    PERF_BR_CALL            : function call
    PERF_BR_IND_CALL        : indirect function call
    PERF_BR_RET             : function return
    PERF_BR_SYSCALL         : syscall
    PERF_BR_SYSRET          : syscall return
    PERF_BR_COND_CALL       : conditional function call
    PERF_BR_COND_RET        : conditional function return
    
    The patch also adds a new field type (4 bits) in perf_branch_entry
    to record the branch type.
    
    Since the disassembling of branch instruction needs some overhead,
    a new PERF_SAMPLE_BRANCH_TYPE_SAVE is introduced to indicate if it
    needs to disassemble the branch instruction and record the branch
    type.
    
    Change log:
    
    v10: Not changed.
    
    v9: Not changed.
    
    v8: Change PERF_BR_NONE to PERF_BR_UNKNOWN.
        No other change.
    
    v7: Just keep the most common branch types.
        Others are removed.
    
    v6: Not changed.
    
    v5: Not changed. The v5 patch series just change the userspace.
    
    v4: Comparing to previous version, the major changes are:
    
    1. Remove the PERF_BR_JCC_FWD/PERF_BR_JCC_BWD, they will be
       computed later in userspace.
    
    2. Remove the "cross" field in perf_branch_entry. The cross page
       computing will be done later in userspace.
    
    Signed-off-by: Yao Jin <yao.jin@linux.intel.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Link: http://lkml.kernel.org/r/1500379995-6449-2-git-send-email-yao.jin@linux.intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index b1c0b187acfe..642db5fa3286 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -174,6 +174,8 @@ enum perf_branch_sample_type_shift {
 	PERF_SAMPLE_BRANCH_NO_FLAGS_SHIFT	= 14, /* no flags */
 	PERF_SAMPLE_BRANCH_NO_CYCLES_SHIFT	= 15, /* no cycles */
 
+	PERF_SAMPLE_BRANCH_TYPE_SAVE_SHIFT	= 16, /* save branch type */
+
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
 
@@ -198,9 +200,30 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_NO_FLAGS	= 1U << PERF_SAMPLE_BRANCH_NO_FLAGS_SHIFT,
 	PERF_SAMPLE_BRANCH_NO_CYCLES	= 1U << PERF_SAMPLE_BRANCH_NO_CYCLES_SHIFT,
 
+	PERF_SAMPLE_BRANCH_TYPE_SAVE	=
+		1U << PERF_SAMPLE_BRANCH_TYPE_SAVE_SHIFT,
+
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };
 
+/*
+ * Common flow change classification
+ */
+enum {
+	PERF_BR_UNKNOWN		= 0,	/* unknown */
+	PERF_BR_COND		= 1,	/* conditional */
+	PERF_BR_UNCOND		= 2,	/* unconditional  */
+	PERF_BR_IND		= 3,	/* indirect */
+	PERF_BR_CALL		= 4,	/* function call */
+	PERF_BR_IND_CALL	= 5,	/* indirect function call */
+	PERF_BR_RET		= 6,	/* function return */
+	PERF_BR_SYSCALL		= 7,	/* syscall */
+	PERF_BR_SYSRET		= 8,	/* syscall return */
+	PERF_BR_COND_CALL	= 9,	/* conditional function call */
+	PERF_BR_COND_RET	= 10,	/* conditional function return */
+	PERF_BR_MAX,
+};
+
 #define PERF_SAMPLE_BRANCH_PLM_ALL \
 	(PERF_SAMPLE_BRANCH_USER|\
 	 PERF_SAMPLE_BRANCH_KERNEL|\
@@ -1015,6 +1038,7 @@ union perf_mem_data_src {
  *     in_tx: running in a hardware transaction
  *     abort: aborting a hardware transaction
  *    cycles: cycles from last branch (or 0 if not supported)
+ *      type: branch type
  */
 struct perf_branch_entry {
 	__u64	from;
@@ -1024,7 +1048,8 @@ struct perf_branch_entry {
 		in_tx:1,    /* in transaction */
 		abort:1,    /* transaction abort */
 		cycles:16,  /* cycle count to last branch */
-		reserved:44;
+		type:4,     /* branch type */
+		reserved:40;
 };
 
 #endif /* _UAPI_LINUX_PERF_EVENT_H */

commit 7246f60068840847bdcf595be5f0b5ca632736e0
Merge: e579dde654fc 700b7eadd562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 11:36:44 2017 -0700

    Merge tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Larger virtual address space on 64-bit server CPUs. By default we
         use a 128TB virtual address space, but a process can request access
         to the full 512TB by passing a hint to mmap().
    
       - Support for the new Power9 "XIVE" interrupt controller.
    
       - TLB flushing optimisations for the radix MMU on Power9.
    
       - Support for CAPI cards on Power9, using the "Coherent Accelerator
         Interface Architecture 2.0".
    
       - The ability to configure the mmap randomisation limits at build and
         runtime.
    
       - Several small fixes and cleanups to the kprobes code, as well as
         support for KPROBES_ON_FTRACE.
    
       - Major improvements to handling of system reset interrupts,
         correctly treating them as NMIs, giving them a dedicated stack and
         using a new hypervisor call to trigger them, all of which should
         aid debugging and robustness.
    
       - Many fixes and other minor enhancements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Alistair Popple,
      Andrew Donnellan, Aneesh Kumar K.V, Anshuman Khandual, Anton
      Blanchard, Balbir Singh, Ben Hutchings, Benjamin Herrenschmidt,
      Bhupesh Sharma, Chris Packham, Christian Zigotzky, Christophe Leroy,
      Christophe Lombard, Daniel Axtens, David Gibson, Gautham R. Shenoy,
      Gavin Shan, Geert Uytterhoeven, Guilherme G. Piccoli, Hamish Martin,
      Hari Bathini, Kees Cook, Laurent Dufour, Madhavan Srinivasan, Mahesh J
      Salgaonkar, Mahesh Salgaonkar, Masami Hiramatsu, Matt Brown, Matthew
      R. Ochs, Michael Neuling, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Pan Xinhui, Paul Mackerras, Rashmica Gupta, Russell
      Currey, Sukadev Bhattiprolu, Thadeu Lima de Souza Cascardo, Tobin C.
      Harding, Tyrel Datwyler, Uma Krishnan, Vaibhav Jain, Vipin K Parashar,
      Yang Shi"
    
    * tag 'powerpc-4.12-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (214 commits)
      powerpc/64s: Power9 has no LPCR[VRMASD] field so don't set it
      powerpc/powernv: Fix TCE kill on NVLink2
      powerpc/mm/radix: Drop support for CPUs without lockless tlbie
      powerpc/book3s/mce: Move add_taint() later in virtual mode
      powerpc/sysfs: Move #ifdef CONFIG_HOTPLUG_CPU out of the function body
      powerpc/smp: Document irq enable/disable after migrating IRQs
      powerpc/mpc52xx: Don't select user-visible RTAS_PROC
      powerpc/powernv: Document cxl dependency on special case in pnv_eeh_reset()
      powerpc/eeh: Clean up and document event handling functions
      powerpc/eeh: Avoid use after free in eeh_handle_special_event()
      cxl: Mask slice error interrupts after first occurrence
      cxl: Route eeh events to all drivers in cxl_pci_error_detected()
      cxl: Force context lock during EEH flow
      powerpc/64: Allow CONFIG_RELOCATABLE if COMPILE_TEST
      powerpc/xmon: Teach xmon oops about radix vectors
      powerpc/mm/hash: Fix off-by-one in comment about kernel contexts ids
      powerpc/pseries: Enable VFIO
      powerpc/powernv: Fix iommu table size calculation hook for small tables
      powerpc/powernv: Check kzalloc() return value in pnv_pci_table_alloc
      powerpc: Add arch/powerpc/tools directory
      ...

commit 8c5073db0ee680c7e70e123918c9b260e49f757d
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Apr 11 07:21:05 2017 +0530

    powerpc/perf: Define big-endian version of perf_mem_data_src
    
    perf_mem_data_src is a union that is initialized in the kernel via the ->val
    field and accessed by userspace via the mem_xxx bitfields. For this to work
    correctly on big endian platforms, we need a big-endian definition for the
    bitfields.
    
    Currently on a big endian system, if a user requests PERF_SAMPLE_DATA_SRC (perf
    report -d), they will get the default value from perf_sample_data_init(), which
    is PERF_MEM_NA. The value for PERF_MEM_NA is constructed using shifts:
    
      /* TLB access */
      #define PERF_MEM_TLB_NA               0x01 /* not available */
      ...
      #define PERF_MEM_TLB_SHIFT    26
    
      #define PERF_MEM_S(a, s) \
            (((__u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
    
      #define PERF_MEM_NA (PERF_MEM_S(OP, NA)   |\
                        PERF_MEM_S(LVL, NA)   |\
                        PERF_MEM_S(SNOOP, NA) |\
                        PERF_MEM_S(LOCK, NA)  |\
                        PERF_MEM_S(TLB, NA))
    
    Which works out as:
    
      ((0x01 << 0) | (0x01 << 5) | (0x01 << 19) | (0x01 << 24) | (0x01 << 26))
    
    Which means the PERF_MEM_NA value comes out of the kernel as 0x5080021
    in CPU endian.
    
    But then in the perf tool, the code uses the bitfields to inspect the value, and
    currently the bitfields are defined using little endian ordering.
    
    So eg. in perf_mem__tlb_scnprintf() we see:
      data_src->val = 0x5080021
                 op = 0x0
                lvl = 0x0
              snoop = 0x0
               lock = 0x0
               dtlb = 0x0
               rsvd = 0x5080021
    
    Because of the way the perf tool code is written this is still displayed to the
    user as "N/A", so there is no bug visible at the UI level.
    
    Currently there are no big endian architectures which export a meaningful
    value (ie. other than PERF_MEM_NA), so the extent of the bug on big endian
    platforms is that the PERF_MEM_NA value is exported incorrectly as described
    above. Subsequent patches will add support on big endian powerpc for populating
    the data source value.
    
    This patch does a minimal fix of adding big endian definition of the bitfields
    to match the values that are already exported by the kernel on big endian. And
    it makes no change on little endian.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index c66a485a24ac..c4af1159a200 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -891,6 +891,7 @@ enum perf_callchain_context {
 #define PERF_FLAG_PID_CGROUP		(1UL << 2) /* pid=cgroup id, per-cpu mode only */
 #define PERF_FLAG_FD_CLOEXEC		(1UL << 3) /* O_CLOEXEC */
 
+#if defined(__LITTLE_ENDIAN_BITFIELD)
 union perf_mem_data_src {
 	__u64 val;
 	struct {
@@ -902,6 +903,21 @@ union perf_mem_data_src {
 			mem_rsvd:31;
 	};
 };
+#elif defined(__BIG_ENDIAN_BITFIELD)
+union perf_mem_data_src {
+	__u64 val;
+	struct {
+		__u64	mem_rsvd:31,
+			mem_dtlb:7,	/* tlb access */
+			mem_lock:2,	/* lock instr */
+			mem_snoop:5,	/* snoop mode */
+			mem_lvl:14,	/* memory hierarchy level */
+			mem_op:5;	/* type of opcode */
+	};
+};
+#else
+#error "Unknown endianness"
+#endif
 
 /* type of opcode (load/store/prefetch,code) */
 #define PERF_MEM_OP_NA		0x01 /* not available */

commit ae0c2d995d648d5165545d5e05e2869642009b38
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Mon Feb 20 15:33:51 2017 +0200

    perf/core: Add a flag for partial AUX records
    
    The Intel PT driver needs to be able to communicate partial AUX transactions,
    that is, transactions with gaps in data for reasons other than no room
    left in the buffer (i.e. truncated transactions). Therefore, this condition
    does not imply a wakeup for the consumer.
    
    To this end, add a new "partial" AUX flag.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: vince@deater.net
    Link: http://lkml.kernel.org/r/20170220133352.17995-4-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index bec0aad0e15c..d09a9cd021b1 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -915,6 +915,7 @@ enum perf_callchain_context {
  */
 #define PERF_AUX_FLAG_TRUNCATED		0x01	/* record was truncated to fit */
 #define PERF_AUX_FLAG_OVERWRITE		0x02	/* snapshot from overwrite mode */
+#define PERF_AUX_FLAG_PARTIAL		0x04	/* record contains gaps */
 
 #define PERF_FLAG_FD_NO_GROUP		(1UL << 0)
 #define PERF_FLAG_FD_OUTPUT		(1UL << 1)

commit e422267322cd319e2695a535e47c5b1feeac45eb
Author: Hari Bathini <hbathini@linux.vnet.ibm.com>
Date:   Wed Mar 8 02:11:36 2017 +0530

    perf: Add PERF_RECORD_NAMESPACES to include namespaces related info
    
    With the advert of container technologies like docker, that depend on
    namespaces for isolation, there is a need for tracing support for
    namespaces. This patch introduces new PERF_RECORD_NAMESPACES event for
    recording namespaces related info. By recording info for every
    namespace, it is left to userspace to take a call on the definition of a
    container and trace containers by updating perf tool accordingly.
    
    Each namespace has a combination of device and inode numbers. Though
    every namespace has the same device number currently, that may change in
    future to avoid the need for a namespace of namespaces. Considering such
    possibility, record both device and inode numbers separately for each
    namespace.
    
    Signed-off-by: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Sargun Dhillon <sargun@sargun.me>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/148891929686.25309.2827618988917007768.stgit@hbathini.in.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index c66a485a24ac..bec0aad0e15c 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -344,7 +344,8 @@ struct perf_event_attr {
 				use_clockid    :  1, /* use @clockid for time fields */
 				context_switch :  1, /* context switch data */
 				write_backward :  1, /* Write ring buffer from end to beginning */
-				__reserved_1   : 36;
+				namespaces     :  1, /* include namespaces data */
+				__reserved_1   : 35;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -610,6 +611,23 @@ struct perf_event_header {
 	__u16	size;
 };
 
+struct perf_ns_link_info {
+	__u64	dev;
+	__u64	ino;
+};
+
+enum {
+	NET_NS_INDEX		= 0,
+	UTS_NS_INDEX		= 1,
+	IPC_NS_INDEX		= 2,
+	PID_NS_INDEX		= 3,
+	USER_NS_INDEX		= 4,
+	MNT_NS_INDEX		= 5,
+	CGROUP_NS_INDEX		= 6,
+
+	NR_NAMESPACES,		/* number of available namespaces */
+};
+
 enum perf_event_type {
 
 	/*
@@ -862,6 +880,18 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_SWITCH_CPU_WIDE		= 15,
 
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid;
+	 *	u32				tid;
+	 *	u64				nr_namespaces;
+	 *	{ u64				dev, inode; } [nr_namespaces];
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_NAMESPACES			= 16,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit 97c79a38cd454602645f0470ffb444b3b75ce574
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Apr 28 13:16:33 2016 -0300

    perf core: Per event callchain limit
    
    Additionally to being able to control the system wide maximum depth via
    /proc/sys/kernel/perf_event_max_stack, now we are able to ask for
    different depths per event, using perf_event_attr.sample_max_stack for
    that.
    
    This uses an u16 hole at the end of perf_event_attr, that, when
    perf_event_attr.sample_type has the PERF_SAMPLE_CALLCHAIN, if
    sample_max_stack is zero, means use perf_event_max_stack, otherwise
    it'll be bounds checked under callchain_mutex.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/n/tip-kolmn1yo40p7jhswxwrc7rrd@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 36ce552cf6a9..c66a485a24ac 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -276,6 +276,9 @@ enum perf_event_read_format {
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
+ *
+ * @sample_max_stack: Max number of frame pointers in a callchain,
+ *		      should be < /proc/sys/kernel/perf_event_max_stack
  */
 struct perf_event_attr {
 
@@ -385,7 +388,8 @@ struct perf_event_attr {
 	 * Wakeup watermark for AUX area
 	 */
 	__u32	aux_watermark;
-	__u32	__reserved_2;	/* align to __u64 */
+	__u16	sample_max_stack;
+	__u16	__reserved_2;	/* align to __u64 */
 };
 
 #define perf_flags(attr)	(*(&(attr)->read_format + 1))

commit c85b03349640b34f3545503c8429fc43005e9a92
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu May 12 13:06:21 2016 -0300

    perf core: Separate accounting of contexts and real addresses in a stack trace
    
    The perf_sample->ip_callchain->nr value includes all the entries in the
    ip_callchain->ip[] array, real addresses and PERF_CONTEXT_{KERNEL,USER,etc},
    while what the user expects is that what is in the kernel.perf_event_max_stack
    sysctl or in the upcoming per event perf_event_attr.sample_max_stack knob be
    honoured in terms of IP addresses in the stack trace.
    
    So allocate a bunch of extra entries for contexts, and do the accounting
    via perf_callchain_entry_ctx struct members.
    
    A new sysctl, kernel.perf_event_max_contexts_per_stack is also
    introduced for investigating possible bugs in the callchain
    implementation by some arch.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Milian Wolff <milian.wolff@kdab.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Wang Nan <wangnan0@huawei.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/n/tip-3b4wnqk340c4sg4gwkfdi9yk@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 43fc8d213472..36ce552cf6a9 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -862,6 +862,7 @@ enum perf_event_type {
 };
 
 #define PERF_MAX_STACK_DEPTH		127
+#define PERF_MAX_CONTEXTS_PER_STACK	  8
 
 enum perf_callchain_context {
 	PERF_CONTEXT_HV			= (__u64)-32,

commit 9ecda41acb971ebd07c8fb35faf24005c0baea12
Author: Wang Nan <wangnan0@huawei.com>
Date:   Tue Apr 5 14:11:18 2016 +0000

    perf/core: Add ::write_backward attribute to perf event
    
    This patch introduces 'write_backward' bit to perf_event_attr, which
    controls the direction of a ring buffer. After set, the corresponding
    ring buffer is written from end to beginning. This feature is design to
    support reading from overwritable ring buffer.
    
    Ring buffer can be created by mapping a perf event fd. Kernel puts event
    records into ring buffer, user tooling like perf fetch them from
    address returned by mmap(). To prevent racing between kernel and tooling,
    they communicate to each other through 'head' and 'tail' pointers.
    Kernel maintains 'head' pointer, points it to the next free area (tail
    of the last record). Tooling maintains 'tail' pointer, points it to the
    tail of last consumed record (record has already been fetched). Kernel
    determines the available space in a ring buffer using these two
    pointers to avoid overwrite unfetched records.
    
    By mapping without 'PROT_WRITE', an overwritable ring buffer is created.
    Different from normal ring buffer, tooling is unable to maintain 'tail'
    pointer because writing is forbidden. Therefore, for this type of ring
    buffers, kernel overwrite old records unconditionally, works like flight
    recorder. This feature would be useful if reading from overwritable ring
    buffer were as easy as reading from normal ring buffer. However,
    there's an obscure problem.
    
    The following figure demonstrates a full overwritable ring buffer. In
    this figure, the 'head' pointer points to the end of last record, and a
    long record 'E' is pending. For a normal ring buffer, a 'tail' pointer
    would have pointed to position (X), so kernel knows there's no more
    space in the ring buffer. However, for an overwritable ring buffer,
    kernel ignore the 'tail' pointer.
    
       (X)                              head
        .                                |
        .                                V
        +------+-------+----------+------+---+
        |A....A|B.....B|C........C|D....D|   |
        +------+-------+----------+------+---+
    
    Record 'A' is overwritten by event 'E':
    
          head
           |
           V
        +--+---+-------+----------+------+---+
        |.E|..A|B.....B|C........C|D....D|E..|
        +--+---+-------+----------+------+---+
    
    Now tooling decides to read from this ring buffer. However, none of these
    two natural positions, 'head' and the start of this ring buffer, are
    pointing to the head of a record. Even the full ring buffer can be
    accessed by tooling, it is unable to find a position to start decoding.
    
    The first attempt tries to solve this problem AFAIK can be found from
    [1]. It makes kernel to maintain 'tail' pointer: updates it when ring
    buffer is half full. However, this approach introduces overhead to
    fast path. Test result shows a 1% overhead [2]. In addition, this method
    utilizes no more tham 50% records.
    
    Another attempt can be found from [3], which allows putting the size of
    an event at the end of each record. This approach allows tooling to find
    records in a backward manner from 'head' pointer by reading size of a
    record from its tail. However, because of alignment requirement, it
    needs 8 bytes to record the size of a record, which is a huge waste. Its
    performance is also not good, because more data need to be written.
    This approach also introduces some extra branch instructions to fast
    path.
    
    'write_backward' is a better solution to this problem.
    
    Following figure demonstrates the state of the overwritable ring buffer
    when 'write_backward' is set before overwriting:
    
           head
            |
            V
        +---+------+----------+-------+------+
        |   |D....D|C........C|B.....B|A....A|
        +---+------+----------+-------+------+
    
    and after overwriting:
                                         head
                                          |
                                          V
        +---+------+----------+-------+---+--+
        |..E|D....D|C........C|B.....B|A..|E.|
        +---+------+----------+-------+---+--+
    
    In each situation, 'head' points to the beginning of the newest record.
    From this record, tooling can iterate over the full ring buffer and fetch
    records one by one.
    
    The only limitation that needs to be considered is back-to-back reading.
    Due to the non-deterministic of user programs, it is impossible to ensure
    the ring buffer keeps stable during reading. Consider an extreme situation:
    tooling is scheduled out after reading record 'D', then a burst of events
    come, eat up the whole ring buffer (one or multiple rounds). When the
    tooling process comes back, reading after 'D' is incorrect now.
    
    To prevent this problem, we need to find a way to ensure the ring buffer
    is stable during reading. ioctl(PERF_EVENT_IOC_PAUSE_OUTPUT) is
    suggested because its overhead is lower than
    ioctl(PERF_EVENT_IOC_ENABLE).
    
    By carefully verifying 'header' pointer, reader can avoid pausing the
    ring-buffer. For example:
    
        /* A union of all possible events */
        union perf_event event;
    
        p = head = perf_mmap__read_head();
        while (true) {
            /* copy header of next event */
            fetch(&event.header, p, sizeof(event.header));
    
            /* read 'head' pointer */
            head = perf_mmap__read_head();
    
            /* check overwritten: is the header good? */
            if (!verify(sizeof(event.header), p, head))
                break;
    
            /* copy the whole event */
            fetch(&event, p, event.header.size);
    
            /* read 'head' pointer again */
            head = perf_mmap__read_head();
    
            /* is the whole event good? */
            if (!verify(event.header.size, p, head))
                break;
            p += event.header.size;
        }
    
    However, the overhead is high because:
    
     a) In-place decoding is not safe.
        Copying-verifying-decoding is required.
     b) Fetching 'head' pointer requires additional synchronization.
    
    (From Alexei Starovoitov:
    
    Even when this trick works, pause is needed for more than stability of
    reading. When we collect the events into overwrite buffer we're waiting
    for some other trigger (like all cpu utilization spike or just one cpu
    running and all others are idle) and when it happens the buffer has
    valuable info from the past. At this point new events are no longer
    interesting and buffer should be paused, events read and unpaused until
    next trigger comes.)
    
    This patch utilizes event's default overflow_handler introduced
    previously. perf_event_output_backward() is created as the default
    overflow handler for backward ring buffers. To avoid extra overhead to
    fast path, original perf_event_output() becomes __perf_event_output()
    and marked '__always_inline'. In theory, there's no extra overhead
    introduced to fast path.
    
    Performance testing:
    
    Calling 3000000 times of 'close(-1)', use gettimeofday() to check
    duration.  Use 'perf record -o /dev/null -e raw_syscalls:*' to capture
    system calls. In ns.
    
    Testing environment:
    
      CPU    : Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz
      Kernel : v4.5.0
                        MEAN         STDVAR
     BASE            800214.950    2853.083
     PRE1           2253846.700    9997.014
     PRE2           2257495.540    8516.293
     POST           2250896.100    8933.921
    
    Where 'BASE' is pure performance without capturing. 'PRE1' is test
    result of pure 'v4.5.0' kernel. 'PRE2' is test result before this
    patch. 'POST' is test result after this patch. See [4] for the detailed
    experimental setup.
    
    Considering the stdvar, this patch doesn't introduce performance
    overhead to the fast path.
    
     [1] http://lkml.iu.edu/hypermail/linux/kernel/1304.1/04584.html
     [2] http://lkml.iu.edu/hypermail/linux/kernel/1307.1/00535.html
     [3] http://lkml.iu.edu/hypermail/linux/kernel/1512.0/01265.html
     [4] http://lkml.kernel.org/g/56F89DCD.1040202@huawei.com
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: <acme@kernel.org>
    Cc: <pi3orama@163.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/r/1459865478-53413-1-git-send-email-wangnan0@huawei.com
    [ Fixed the changelog some more. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index a3c19034d5f8..43fc8d213472 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -340,7 +340,8 @@ struct perf_event_attr {
 				comm_exec      :  1, /* flag comm events that are due to an exec */
 				use_clockid    :  1, /* use @clockid for time fields */
 				context_switch :  1, /* context switch data */
-				__reserved_1   : 37;
+				write_backward :  1, /* Write ring buffer from end to beginning */
+				__reserved_1   : 36;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */

commit 86e7972f690c1017fd086cdfe53d8524e68c661c
Author: Wang Nan <wangnan0@huawei.com>
Date:   Mon Mar 28 06:41:29 2016 +0000

    perf/ring_buffer: Introduce new ioctl options to pause and resume the ring-buffer
    
    Add new ioctl() to pause/resume ring-buffer output.
    
    In some situations we want to read from the ring-buffer only when we
    ensure nothing can write to the ring-buffer during reading. Without
    this patch we have to turn off all events attached to this ring-buffer
    to achieve this.
    
    This patch is a prerequisite to enable overwrite support for the
    perf ring-buffer support. Following commits will introduce new methods
    support reading from overwrite ring buffer. Before reading, caller
    must ensure the ring buffer is frozen, or the reading is unreliable.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <pi3orama@163.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: He Kuang <hekuang@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Zefan Li <lizefan@huawei.com>
    Link: http://lkml.kernel.org/r/1459147292-239310-2-git-send-email-wangnan0@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 1afe9623c1a7..a3c19034d5f8 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -401,6 +401,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
 #define PERF_EVENT_IOC_ID		_IOR('$', 7, __u64 *)
 #define PERF_EVENT_IOC_SET_BPF		_IOW('$', 8, __u32)
+#define PERF_EVENT_IOC_PAUSE_OUTPUT	_IOW('$', 9, __u32)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit b16a5b52eb90d92b597257778e51e1fdc6423e64
Author: Andi Kleen <ak@linux.intel.com>
Date:   Tue Oct 20 11:46:34 2015 -0700

    perf/x86: Add option to disable reading branch flags/cycles
    
    With LBRv5 reading the extra LBR flags like mispredict, TSX, cycles is
    not free anymore, as it has moved to a separate MSR.
    
    For callstack mode we don't need any of this information; so we can
    avoid the unnecessary MSR read. Add flags to the perf interface where
    perf record can request not collecting this information.
    
    Add branch_sample_type flags for CYCLES and FLAGS. It's a bit unusual
    for branch_sample_types to be negative (disable), not positive (enable),
    but since the legacy ABI reported the flags we need some form of
    explicit disabling to avoid breaking the ABI.
    
    After we have the flags the x86 perf code can keep track if any users
    need the flags. If noone needs it the information is not collected.
    
    This cuts down the cost of LBR callstack on Skylake significantly.
    Profiling a kernel build with LBR call stack the average run time of
    the PMI handler drops by 43%.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: acme@kernel.org
    Cc: jolsa@kernel.org
    Link: http://lkml.kernel.org/r/1445366797-30894-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index d801bb0d9f6d..1afe9623c1a7 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -171,6 +171,9 @@ enum perf_branch_sample_type_shift {
 	PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT	= 12, /* indirect jumps */
 	PERF_SAMPLE_BRANCH_CALL_SHIFT		= 13, /* direct call */
 
+	PERF_SAMPLE_BRANCH_NO_FLAGS_SHIFT	= 14, /* no flags */
+	PERF_SAMPLE_BRANCH_NO_CYCLES_SHIFT	= 15, /* no cycles */
+
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
 
@@ -192,6 +195,9 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_IND_JUMP	= 1U << PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT,
 	PERF_SAMPLE_BRANCH_CALL		= 1U << PERF_SAMPLE_BRANCH_CALL_SHIFT,
 
+	PERF_SAMPLE_BRANCH_NO_FLAGS	= 1U << PERF_SAMPLE_BRANCH_NO_FLAGS_SHIFT,
+	PERF_SAMPLE_BRANCH_NO_CYCLES	= 1U << PERF_SAMPLE_BRANCH_NO_CYCLES_SHIFT,
+
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };
 

commit b0f85fa11aefc4f3e03306b4cd47f113bd57dcba
Merge: ccc9d4a6d640 f32bfb9a8ca0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 09:41:05 2015 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
    Changes of note:
    
     1) Allow to schedule ICMP packets in IPVS, from Alex Gartrell.
    
     2) Provide FIB table ID in ipv4 route dumps just as ipv6 does, from
        David Ahern.
    
     3) Allow the user to ask for the statistics to be filtered out of
        ipv4/ipv6 address netlink dumps.  From Sowmini Varadhan.
    
     4) More work to pass the network namespace context around deep into
        various packet path APIs, starting with the netfilter hooks.  From
        Eric W Biederman.
    
     5) Add layer 2 TX/RX checksum offloading to qeth driver, from Thomas
        Richter.
    
     6) Use usec resolution for SYN/ACK RTTs in TCP, from Yuchung Cheng.
    
     7) Support Very High Throughput in wireless MESH code, from Bob
        Copeland.
    
     8) Allow setting the ageing_time in switchdev/rocker.  From Scott
        Feldman.
    
     9) Properly autoload L2TP type modules, from Stephen Hemminger.
    
    10) Fix and enable offload features by default in 8139cp driver, from
        David Woodhouse.
    
    11) Support both ipv4 and ipv6 sockets in a single vxlan device, from
        Jiri Benc.
    
    12) Fix CWND limiting of thin streams in TCP, from Bendik Rønning
        Opstad.
    
    13) Fix IPSEC flowcache overflows on large systems, from Steffen
        Klassert.
    
    14) Convert bridging to track VLANs using rhashtable entries rather than
        a bitmap.  From Nikolay Aleksandrov.
    
    15) Make TCP listener handling completely lockless, this is a major
        accomplishment.  Incoming request sockets now live in the
        established hash table just like any other socket too.
    
        From Eric Dumazet.
    
    15) Provide more bridging attributes to netlink, from Nikolay
        Aleksandrov.
    
    16) Use hash based algorithm for ipv4 multipath routing, this was very
        long overdue.  From Peter Nørlund.
    
    17) Several y2038 cures, mostly avoiding timespec.  From Arnd Bergmann.
    
    18) Allow non-root execution of EBPF programs, from Alexei Starovoitov.
    
    19) Support SO_INCOMING_CPU as setsockopt, from Eric Dumazet.  This
        influences the port binding selection logic used by SO_REUSEPORT.
    
    20) Add ipv6 support to VRF, from David Ahern.
    
    21) Add support for Mellanox Spectrum switch ASIC, from Jiri Pirko.
    
    22) Add rtl8xxxu Realtek wireless driver, from Jes Sorensen.
    
    23) Implement RACK loss recovery in TCP, from Yuchung Cheng.
    
    24) Support multipath routes in MPLS, from Roopa Prabhu.
    
    25) Fix POLLOUT notification for listening sockets in AF_UNIX, from Eric
        Dumazet.
    
    26) Add new QED Qlogic river, from Yuval Mintz, Manish Chopra, and
        Sudarsana Kalluru.
    
    27) Don't fetch timestamps on AF_UNIX sockets, from Hannes Frederic
        Sowa.
    
    28) Support ipv6 geneve tunnels, from John W Linville.
    
    29) Add flood control support to switchdev layer, from Ido Schimmel.
    
    30) Fix CHECKSUM_PARTIAL handling of potentially fragmented frames, from
        Hannes Frederic Sowa.
    
    31) Support persistent maps and progs in bpf, from Daniel Borkmann.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1790 commits)
      sh_eth: use DMA barriers
      switchdev: respect SKIP_EOPNOTSUPP flag in case there is no recursion
      net: sched: kill dead code in sch_choke.c
      irda: Delete an unnecessary check before the function call "irlmp_unregister_service"
      net: dsa: mv88e6xxx: include DSA ports in VLANs
      net: dsa: mv88e6xxx: disable SA learning for DSA and CPU ports
      net/core: fix for_each_netdev_feature
      vlan: Invoke driver vlan hooks only if device is present
      arcnet/com20020: add LEDS_CLASS dependency
      bpf, verifier: annotate verbose printer with __printf
      dp83640: Only wait for timestamps for packets with timestamping enabled.
      ptp: Change ptp_class to a proper bitmask
      dp83640: Prune rx timestamp list before reading from it
      dp83640: Delay scheduled work.
      dp83640: Include hash in timestamp/packet matching
      ipv6: fix tunnel error handling
      net/mlx5e: Fix LSO vlan insertion
      net/mlx5e: Re-eanble client vlan TX acceleration
      net/mlx5e: Return error in case mlx5e_set_features() fails
      net/mlx5e: Don't allow more than max supported channels
      ...

commit a43eec304259a6c637f4014a6d4767159b6a3aa3
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Tue Oct 20 20:02:34 2015 -0700

    bpf: introduce bpf_perf_event_output() helper
    
    This helper is used to send raw data from eBPF program into
    special PERF_TYPE_SOFTWARE/PERF_COUNT_SW_BPF_OUTPUT perf_event.
    User space needs to perf_event_open() it (either for one or all cpus) and
    store FD into perf_event_array (similar to bpf_perf_event_read() helper)
    before eBPF program can send data into it.
    
    Today the programs triggered by kprobe collect the data and either store
    it into the maps or print it via bpf_trace_printk() where latter is the debug
    facility and not suitable to stream the data. This new helper replaces
    such bpf_trace_printk() usage and allows programs to have dedicated
    channel into user space for post-processing of the raw data collected.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 2881145cda86..d3c417615361 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -110,6 +110,7 @@ enum perf_sw_ids {
 	PERF_COUNT_SW_ALIGNMENT_FAULTS		= 7,
 	PERF_COUNT_SW_EMULATION_FAULTS		= 8,
 	PERF_COUNT_SW_DUMMY			= 9,
+	PERF_COUNT_SW_BPF_OUTPUT		= 10,
 
 	PERF_COUNT_SW_MAX,			/* non-ABI */
 };

commit c229bf9dc179d2023e185c0f705bdf68484c1e73
Author: Stephane Eranian <eranian@google.com>
Date:   Tue Oct 13 09:09:08 2015 +0200

    perf: Add PERF_SAMPLE_BRANCH_CALL
    
    Add a new branch sample type to cover only call branches (function calls).
    The current ANY_CALL included direct, indirect calls and far jumps.
    
    We want to be able to differentiate indirect from direct calls. Therefore
    we introduce PERF_SAMPLE_BRANCH_CALL. The implementation is up to each
    architecture.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: khandual@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1444720151-10275-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 6c72e72e975c..651221334f49 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -168,6 +168,7 @@ enum perf_branch_sample_type_shift {
 
 	PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT	= 11, /* call/ret stack */
 	PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT	= 12, /* indirect jumps */
+	PERF_SAMPLE_BRANCH_CALL_SHIFT		= 13, /* direct call */
 
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
@@ -188,6 +189,7 @@ enum perf_branch_sample_type {
 
 	PERF_SAMPLE_BRANCH_CALL_STACK	= 1U << PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT,
 	PERF_SAMPLE_BRANCH_IND_JUMP	= 1U << PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT,
+	PERF_SAMPLE_BRANCH_CALL		= 1U << PERF_SAMPLE_BRANCH_CALL_SHIFT,
 
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };

commit b9511cd761faafca7a1acc059e792c1399f9d7c6
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Oct 16 16:24:05 2015 +0300

    perf/x86: Fix time_shift in perf_event_mmap_page
    
    Commit:
    
      b20112edeadf ("perf/x86: Improve accuracy of perf/sched clock")
    
    allowed the time_shift value in perf_event_mmap_page to be as much
    as 32.  Unfortunately the documented algorithms for using time_shift
    have it shifting an integer, whereas to work correctly with the value
    32, the type must be u64.
    
    In the case of perf tools, Intel PT decodes correctly but the timestamps
    that are output (for example by perf script) have lost 32-bits of
    granularity so they look like they are not changing at all.
    
    Fix by limiting the shift to 31 and adjusting the multiplier accordingly.
    
    Also update the documentation of perf_event_mmap_page so that new code
    based on it will be more future-proof.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Fixes: b20112edeadf ("perf/x86: Improve accuracy of perf/sched clock")
    Link: http://lkml.kernel.org/r/1445001845-13688-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 2881145cda86..6c72e72e975c 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -476,7 +476,7 @@ struct perf_event_mmap_page {
 	 *   u64 delta;
 	 *
 	 *   quot = (cyc >> time_shift);
-	 *   rem = cyc & ((1 << time_shift) - 1);
+	 *   rem = cyc & (((u64)1 << time_shift) - 1);
 	 *   delta = time_offset + quot * time_mult +
 	 *              ((rem * time_mult) >> time_shift);
 	 *
@@ -507,7 +507,7 @@ struct perf_event_mmap_page {
 	 * And vice versa:
 	 *
 	 *   quot = cyc >> time_shift;
-	 *   rem  = cyc & ((1 << time_shift) - 1);
+	 *   rem  = cyc & (((u64)1 << time_shift) - 1);
 	 *   timestamp = time_zero + quot * time_mult +
 	 *               ((rem * time_mult) >> time_shift);
 	 */

commit 71ef3c6b9d4665ee7afbbe4c208a98917dcfc32f
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun May 10 12:22:42 2015 -0700

    perf: Add cycles to branch_info
    
    Intel Skylake supports reporting the time in cycles a branch in the LBR
    took, to give a rough indication of the basic block performance.
    
    Export the cycle information in the branch_info structure.
    This can be done by just reusing some currently zero padding.
    
    This is just the generic header change. The architecture
    still needs to fill it in.
    
    There's no attempt to convert to real time, as we really
    want cycles here.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285767-27027-5-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 022d0acf7df0..2881145cda86 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -951,6 +951,7 @@ union perf_mem_data_src {
  *
  *     in_tx: running in a hardware transaction
  *     abort: aborting a hardware transaction
+ *    cycles: cycles from last branch (or 0 if not supported)
  */
 struct perf_branch_entry {
 	__u64	from;
@@ -959,7 +960,8 @@ struct perf_branch_entry {
 		predicted:1,/* target predicted */
 		in_tx:1,    /* in transaction */
 		abort:1,    /* transaction abort */
-		reserved:60;
+		cycles:16,  /* cycle count to last branch */
+		reserved:44;
 };
 
 #endif /* _UAPI_LINUX_PERF_EVENT_H */

commit 45ac1403f564f411c6a383a2448688ba8dd705a4
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Jul 21 12:44:02 2015 +0300

    perf: Add PERF_RECORD_SWITCH to indicate context switches
    
    There are already two events for context switches, namely the tracepoint
    sched:sched_switch and the software event context_switches.
    Unfortunately neither are suitable for use by non-privileged users for
    the purpose of synchronizing hardware trace data (e.g. Intel PT) to the
    context switch.
    
    Tracepoints are no good at all for non-privileged users because they
    need either CAP_SYS_ADMIN or /proc/sys/kernel/perf_event_paranoid <= -1.
    
    On the other hand, kernel software events need either CAP_SYS_ADMIN or
    /proc/sys/kernel/perf_event_paranoid <= 1.
    
    Now many distributions do default perf_event_paranoid to 1 making
    context_switches a contender, except it has another problem (which is
    also shared with sched:sched_switch) which is that it happens before
    perf schedules events out instead of after perf schedules events in.
    Whereas a privileged user can see all the events anyway, a
    non-privileged user only sees events for their own processes, in other
    words they see when their process was scheduled out not when it was
    scheduled in. That presents two problems to use the event:
    
    1. the information comes too late, so tools have to look ahead in the
       event stream to find out what the current state is
    
    2. if they are unlucky tracing might have stopped before the
       context-switches event is recorded.
    
    This new PERF_RECORD_SWITCH event does not have those problems
    and it also has a couple of other small advantages.
    
    It is easier to use because it is an auxiliary event (like mmap, comm
    and task events) which can be enabled by setting a single bit. It is
    smaller than sched:sched_switch and easier to parse.
    
    To make the event useful for privileged users also, if the
    context is cpu-wide then the event record will be
    PERF_RECORD_SWITCH_CPU_WIDE which is the same as
    PERF_RECORD_SWITCH except it also provides the next or
    previous pid/tid.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Pawel Moll <pawel.moll@arm.com>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1437471846-26995-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index d97f84c080da..022d0acf7df0 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -330,7 +330,8 @@ struct perf_event_attr {
 				mmap2          :  1, /* include mmap with inode data     */
 				comm_exec      :  1, /* flag comm events that are due to an exec */
 				use_clockid    :  1, /* use @clockid for time fields */
-				__reserved_1   : 38;
+				context_switch :  1, /* context switch data */
+				__reserved_1   : 37;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -572,9 +573,11 @@ struct perf_event_mmap_page {
 /*
  * PERF_RECORD_MISC_MMAP_DATA and PERF_RECORD_MISC_COMM_EXEC are used on
  * different events so can reuse the same bit position.
+ * Ditto PERF_RECORD_MISC_SWITCH_OUT.
  */
 #define PERF_RECORD_MISC_MMAP_DATA		(1 << 13)
 #define PERF_RECORD_MISC_COMM_EXEC		(1 << 13)
+#define PERF_RECORD_MISC_SWITCH_OUT		(1 << 13)
 /*
  * Indicates that the content of PERF_SAMPLE_IP points to
  * the actual instruction that triggered the event. See also
@@ -818,6 +821,32 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_LOST_SAMPLES		= 13,
 
+	/*
+	 * Records a context switch in or out (flagged by
+	 * PERF_RECORD_MISC_SWITCH_OUT). See also
+	 * PERF_RECORD_SWITCH_CPU_WIDE.
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_SWITCH			= 14,
+
+	/*
+	 * CPU-wide version of PERF_RECORD_SWITCH with next_prev_pid and
+	 * next_prev_tid that are the next (switching out) or previous
+	 * (switching in) pid/tid.
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				next_prev_pid;
+	 *	u32				next_prev_tid;
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_SWITCH_CPU_WIDE		= 15,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit 930e6fcd2bcce9bcd9d4aa7e755678d33f3fe6f4
Author: Kan Liang <kan.liang@intel.com>
Date:   Wed Jun 17 09:51:10 2015 -0400

    perf tools: Add time out to force stop proc map processing
    
    System wide sampling like 'perf top' or 'perf record -a' read all
    threads /proc/xxx/maps before sampling. If there are any threads which
    generating a keeping growing huge maps, perf will do infinite loop
    during synthesizing. Nothing will be sampled.
    
    This patch fixes this issue by adding per-thread timeout to force stop
    this kind of endless proc map processing.
    
    PERF_RECORD_MISC_PROC_MAP_PARSE_TIME_OUT is introduced to indicate that
    the mmap record are truncated by time out. User will get warning
    notification when truncated mmap records are detected.
    
    Reported-by: Ying Huang <ying.huang@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Ying Huang <ying.huang@intel.com>
    Link: http://lkml.kernel.org/r/1434549071-25611-1-git-send-email-kan.liang@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 613ed9ad588f..d97f84c080da 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -565,6 +565,10 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
 #define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
 
+/*
+ * Indicates that /proc/PID/maps parsing are truncated by time out.
+ */
+#define PERF_RECORD_MISC_PROC_MAP_PARSE_TIMEOUT	(1 << 12)
 /*
  * PERF_RECORD_MISC_MMAP_DATA and PERF_RECORD_MISC_COMM_EXEC are used on
  * different events so can reuse the same bit position.

commit f38b0dbb491a6987e198aa6b428db8692a6480f8
Author: Kan Liang <kan.liang@intel.com>
Date:   Sun May 10 15:13:14 2015 -0400

    perf/x86/intel: Introduce PERF_RECORD_LOST_SAMPLES
    
    After enlarging the PEBS interrupt threshold, there may be some mixed up
    PEBS samples which are discarded by the kernel.
    
    This patch makes the kernel emit a PERF_RECORD_LOST_SAMPLES record with
    the number of possible discarded records when it is impossible to demux
    the samples.
    
    It makes sure the user is not left in the dark about such discards.
    
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: eranian@google.com
    Link: http://lkml.kernel.org/r/1431285195-14269-8-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index c4622f1ce046..613ed9ad588f 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -802,6 +802,18 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_ITRACE_START		= 12,
 
+	/*
+	 * Records the dropped/lost sample number.
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u64				lost;
+	 *	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_LOST_SAMPLES		= 13,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit c9fdfa14c3792c0160849c484e83aa57afd80ccc
Author: Stephane Eranian <eranian@google.com>
Date:   Thu May 14 23:09:58 2015 +0200

    perf: add new PERF_SAMPLE_BRANCH_IND_JUMP branch sample type
    
    This patch adds a new branch_sample_type flag to enable
    filtering branch sampling to indirect jumps. The support
    is subject to hardware or kernel software support on each
    architecture.
    
    Filtering on indirect jump is useful to study the targets
    of the jump.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@redhat.com
    Cc: dsahern@gmail.com
    Cc: jolsa@redhat.com
    Cc: kan.liang@intel.com
    Cc: namhyung@kernel.org
    Link: http://lkml.kernel.org/r/1431637800-31061-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 309211b3eb67..c4622f1ce046 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -167,6 +167,7 @@ enum perf_branch_sample_type_shift {
 	PERF_SAMPLE_BRANCH_COND_SHIFT		= 10, /* conditional branches */
 
 	PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT	= 11, /* call/ret stack */
+	PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT	= 12, /* indirect jumps */
 
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
@@ -186,6 +187,7 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_COND		= 1U << PERF_SAMPLE_BRANCH_COND_SHIFT,
 
 	PERF_SAMPLE_BRANCH_CALL_STACK	= 1U << PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT,
+	PERF_SAMPLE_BRANCH_IND_JUMP	= 1U << PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT,
 
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };

commit ec0d7729bbaed4b9d2d3fada693278e13a3d1368
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:23 2015 +0200

    perf: Add ITRACE_START record to indicate that tracing has started
    
    For counters that generate AUX data that is bound to the context of a
    running task, such as instruction tracing, the decoder needs to know
    exactly which task is running when the event is first scheduled in,
    before the first sched_switch. The decoder's need to know this stems
    from the fact that instruction flow trace decoding will almost always
    require program's object code in order to reconstruct said flow and
    for that we need at least its pid/tid in the perf stream.
    
    To single out such instruction tracing pmus, this patch introduces
    ITRACE PMU capability. The reason this is not part of RECORD_AUX
    record is that not all pmus capable of generating AUX data need this,
    and the opposite is *probably* also true.
    
    While sched_switch covers for most cases, there are two problems with it:
    the consumer will need to process events out of order (that is, having
    found RECORD_AUX, it will have to skip forward to the nearest sched_switch
    to figure out which task it was, then go back to the actual trace to
    decode it) and it completely misses the case when the tracing is enabled
    and disabled before sched_switch, for example, via PERF_EVENT_IOC_DISABLE.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-15-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 84819546c8ce..309211b3eb67 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -789,6 +789,17 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_AUX				= 11,
 
+	/*
+	 * Indicates that instruction trace has started
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid;
+	 *	u32				tid;
+	 * };
+	 */
+	PERF_RECORD_ITRACE_START		= 12,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit 1a5941312414c71dece6717da9a0fa1303127afa
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:18 2015 +0200

    perf: Add wakeup watermark control to the AUX area
    
    When AUX area gets a certain amount of new data, we want to wake up
    userspace to collect it. This adds a new control to specify how much
    data will cause a wakeup. This is then passed down to pmu drivers via
    output handle's "wakeup" field, so that the driver can find the nearest
    point where it can generate an interrupt.
    
    We repurpose __reserved_2 in the event attribute for this, even though
    it was never checked to be zero before, aux_watermark will only matter
    for new AUX-aware code, so the old code should still be fine.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-10-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 29ef2f73bb4a..84819546c8ce 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -261,6 +261,7 @@ enum perf_event_read_format {
 #define PERF_ATTR_SIZE_VER3	96	/* add: sample_regs_user */
 					/* add: sample_stack_user */
 #define PERF_ATTR_SIZE_VER4	104	/* add: sample_regs_intr */
+#define PERF_ATTR_SIZE_VER5	112	/* add: aux_watermark */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
@@ -366,6 +367,12 @@ struct perf_event_attr {
 	 * See asm/perf_regs.h for details.
 	 */
 	__u64	sample_regs_intr;
+
+	/*
+	 * Wakeup watermark for AUX area
+	 */
+	__u32	aux_watermark;
+	__u32	__reserved_2;	/* align to __u64 */
 };
 
 #define perf_flags(attr)	(*(&(attr)->read_format + 1))

commit 2023a0d2829e521fe6ad6b9907f3f90bfbf57142
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:17 2015 +0200

    perf: Support overwrite mode for the AUX area
    
    This adds support for overwrite mode in the AUX area, which means "keep
    collecting data till you're stopped", turning AUX area into a circular
    buffer, where new data overwrites old data. It does not depend on data
    buffer's overwrite mode, so that it doesn't lose sideband data that is
    instrumental for processing AUX data.
    
    Overwrite mode is enabled at mapping AUX area read only. Even though
    aux_tail in the buffer's user page might be user writable, it will be
    ignored in this mode.
    
    A PERF_RECORD_AUX with PERF_AUX_FLAG_OVERWRITE set is written to the perf
    data stream every time an event writes new data to the AUX area. The pmu
    driver might not be able to infer the exact beginning of the new data in
    each snapshot, some drivers will only provide the tail, which is
    aux_offset + aux_size in the AUX record. Consumer has to be able to tell
    the new data from the old one, for example, by means of time stamps if
    such are provided in the trace.
    
    Consumer is also responsible for disabling any events that might write
    to the AUX area (thus potentially racing with the consumer) before
    collecting the data.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-9-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 8904ad3a850b..29ef2f73bb4a 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -803,6 +803,7 @@ enum perf_callchain_context {
  * PERF_RECORD_AUX::flags bits
  */
 #define PERF_AUX_FLAG_TRUNCATED		0x01	/* record was truncated to fit */
+#define PERF_AUX_FLAG_OVERWRITE		0x02	/* snapshot from overwrite mode */
 
 #define PERF_FLAG_FD_NO_GROUP		(1UL << 0)
 #define PERF_FLAG_FD_OUTPUT		(1UL << 1)

commit 68db7e98c3a6ebe7284b6cf14906ed7c55f3f7f0
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:15 2015 +0200

    perf: Add AUX record
    
    When there's new data in the AUX space, output a record indicating its
    offset and size and a set of flags, such as PERF_AUX_FLAG_TRUNCATED, to
    mean the described data was truncated to fit in the ring buffer.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-7-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 6c5013a71714..8904ad3a850b 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -768,6 +768,20 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_MMAP2			= 10,
 
+	/*
+	 * Records that new data landed in the AUX buffer part.
+	 *
+	 * struct {
+	 * 	struct perf_event_header	header;
+	 *
+	 * 	u64				aux_offset;
+	 * 	u64				aux_size;
+	 *	u64				flags;
+	 * 	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_AUX				= 11,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 
@@ -785,6 +799,11 @@ enum perf_callchain_context {
 	PERF_CONTEXT_MAX		= (__u64)-4095,
 };
 
+/**
+ * PERF_RECORD_AUX::flags bits
+ */
+#define PERF_AUX_FLAG_TRUNCATED		0x01	/* record was truncated to fit */
+
 #define PERF_FLAG_FD_NO_GROUP		(1UL << 0)
 #define PERF_FLAG_FD_OUTPUT		(1UL << 1)
 #define PERF_FLAG_PID_CGROUP		(1UL << 2) /* pid=cgroup id, per-cpu mode only */

commit 45bfb2e50471abbbfd83d40d28c986078b0d24ff
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jan 14 14:18:11 2015 +0200

    perf: Add AUX area to ring buffer for raw data streams
    
    This patch introduces "AUX space" in the perf mmap buffer, intended for
    exporting high bandwidth data streams to userspace, such as instruction
    flow traces.
    
    AUX space is a ring buffer, defined by aux_{offset,size} fields in the
    user_page structure, and read/write pointers aux_{head,tail}, which abide
    by the same rules as data_* counterparts of the main perf buffer.
    
    In order to allocate/mmap AUX, userspace needs to set up aux_offset to
    such an offset that will be greater than data_offset+data_size and
    aux_size to be the desired buffer size. Both need to be page aligned.
    Then, same aux_offset and aux_size should be passed to mmap() call and
    if everything adds up, you should have an AUX buffer as a result.
    
    Pages that are mapped into this buffer also come out of user's mlock
    rlimit plus perf_event_mlock_kb allowance.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-3-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 86c44ae66d43..6c5013a71714 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -530,6 +530,22 @@ struct perf_event_mmap_page {
 	__u64	data_tail;		/* user-space written tail */
 	__u64	data_offset;		/* where the buffer starts */
 	__u64	data_size;		/* data buffer size */
+
+	/*
+	 * AUX area is defined by aux_{offset,size} fields that should be set
+	 * by the userspace, so that
+	 *
+	 *   aux_offset >= data_offset + data_size
+	 *
+	 * prior to mmap()ing it. Size of the mmap()ed area should be aux_size.
+	 *
+	 * Ring buffer pointers aux_{head,tail} have the same semantics as
+	 * data_{head,tail} and same ordering rules apply.
+	 */
+	__u64	aux_head;
+	__u64	aux_tail;
+	__u64	aux_offset;
+	__u64	aux_size;
 };
 
 #define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)

commit e8c6deac69629c0cb97c3d3272f8631ef17f8f0f
Author: Alexander Shishkin <alexander.shishkin@linux.intel.com>
Date:   Wed Jan 14 14:18:10 2015 +0200

    perf: Add data_{offset,size} to user_page
    
    Currently, the actual perf ring buffer is one page into the mmap area,
    following the user page and the userspace follows this convention. This
    patch adds data_{offset,size} fields to user_page that can be used by
    userspace instead for locating perf data in the mmap area. This is also
    helpful when mapping existing or shared buffers if their size is not
    known in advance.
    
    Right now, it is made to follow the existing convention that
    
            data_offset == PAGE_SIZE and
            data_offset + data_size == mmap_size.
    
    Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kaixu Xia <kaixu.xia@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Robert Richter <rric@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@infradead.org
    Cc: adrian.hunter@intel.com
    Cc: kan.liang@intel.com
    Cc: markus.t.metzger@intel.com
    Cc: mathieu.poirier@linaro.org
    Link: http://lkml.kernel.org/r/1421237903-181015-2-git-send-email-alexander.shishkin@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 91803e54ee73..86c44ae66d43 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -522,9 +522,14 @@ struct perf_event_mmap_page {
 	 * In this case the kernel will not over-write unread data.
 	 *
 	 * See perf_output_put_handle() for the data ordering.
+	 *
+	 * data_{offset,size} indicate the location and size of the perf record
+	 * buffer within the mmapped area.
 	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */
+	__u64	data_offset;		/* where the buffer starts */
+	__u64	data_size;		/* data buffer size */
 };
 
 #define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)

commit 2541517c32be2531e0da59dfd7efc1ce844644f5
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Mar 25 12:49:20 2015 -0700

    tracing, perf: Implement BPF programs attached to kprobes
    
    BPF programs, attached to kprobes, provide a safe way to execute
    user-defined BPF byte-code programs without being able to crash or
    hang the kernel in any way. The BPF engine makes sure that such
    programs have a finite execution time and that they cannot break
    out of their sandbox.
    
    The user interface is to attach to a kprobe via the perf syscall:
    
            struct perf_event_attr attr = {
                    .type   = PERF_TYPE_TRACEPOINT,
                    .config = event_id,
                    ...
            };
    
            event_fd = perf_event_open(&attr,...);
            ioctl(event_fd, PERF_EVENT_IOC_SET_BPF, prog_fd);
    
    'prog_fd' is a file descriptor associated with BPF program
    previously loaded.
    
    'event_id' is an ID of the kprobe created.
    
    Closing 'event_fd':
    
            close(event_fd);
    
    ... automatically detaches BPF program from it.
    
    BPF programs can call in-kernel helper functions to:
    
      - lookup/update/delete elements in maps
    
      - probe_read - wraper of probe_kernel_read() used to access any
        kernel data structures
    
    BPF programs receive 'struct pt_regs *' as an input ('struct pt_regs' is
    architecture dependent) and return 0 to ignore the event and 1 to store
    kprobe event into the ring buffer.
    
    Note, kprobes are a fundamentally _not_ a stable kernel ABI,
    so BPF programs attached to kprobes must be recompiled for
    every kernel version and user must supply correct LINUX_VERSION_CODE
    in attr.kern_version during bpf_prog_load() call.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1427312966-8434-4-git-send-email-ast@plumgrid.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 3bb40ddadbe5..91803e54ee73 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -381,6 +381,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
 #define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
 #define PERF_EVENT_IOC_ID		_IOR('$', 7, __u64 *)
+#define PERF_EVENT_IOC_SET_BPF		_IOW('$', 8, __u32)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit 34f439278cef7b1177f8ce24f9fc81dfc6221d3b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Feb 20 14:05:38 2015 +0100

    perf: Add per event clockid support
    
    While thinking on the whole clock discussion it occurred to me we have
    two distinct uses of time:
    
     1) the tracking of event/ctx/cgroup enabled/running/stopped times
        which includes the self-monitoring support in struct
        perf_event_mmap_page.
    
     2) the actual timestamps visible in the data records.
    
    And we've been conflating them.
    
    The first is all about tracking time deltas, nobody should really care
    in what time base that happens, its all relative information, as long
    as its internally consistent it works.
    
    The second however is what people are worried about when having to
    merge their data with external sources. And here we have the
    discussion on MONOTONIC vs MONOTONIC_RAW etc..
    
    Where MONOTONIC is good for correlating between machines (static
    offset), MONOTNIC_RAW is required for correlating against a fixed rate
    hardware clock.
    
    This means configurability; now 1) makes that hard because it needs to
    be internally consistent across groups of unrelated events; which is
    why we had to have a global perf_clock().
    
    However, for 2) it doesn't really matter, perf itself doesn't care
    what it writes into the buffer.
    
    The below patch makes the distinction between these two cases by
    adding perf_event_clock() which is used for the second case. It
    further makes this configurable on a per-event basis, but adds a few
    sanity checks such that we cannot combine events with different clocks
    in confusing ways.
    
    And since we then have per-event configurability we might as well
    retain the 'legacy' behaviour as a default.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 1e3cd07cf76e..3bb40ddadbe5 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -326,7 +326,8 @@ struct perf_event_attr {
 				exclude_callchain_user   : 1, /* exclude user callchains */
 				mmap2          :  1, /* include mmap with inode data     */
 				comm_exec      :  1, /* flag comm events that are due to an exec */
-				__reserved_1   : 39;
+				use_clockid    :  1, /* use @clockid for time fields */
+				__reserved_1   : 38;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -355,8 +356,7 @@ struct perf_event_attr {
 	 */
 	__u32	sample_stack_user;
 
-	/* Align to u64. */
-	__u32	__reserved_2;
+	__s32	clockid;
 	/*
 	 * Defines set of regs to dump for each sample
 	 * state captured on:

commit 2c44b1936bb3b135a3fac8b3493394d42e51cf70
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 5 10:36:45 2014 +0100

    perf/x86/intel: Expose LBR callstack to user space tooling
    
    With LBR call stack feature enable, there are three callchain options.
    Enable the 3rd callchain option (LBR callstack) to user space tooling.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/20141105093759.GQ10501@worktop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index e46b93279e3d..1e3cd07cf76e 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -166,6 +166,8 @@ enum perf_branch_sample_type_shift {
 	PERF_SAMPLE_BRANCH_NO_TX_SHIFT		= 9, /* not in transaction */
 	PERF_SAMPLE_BRANCH_COND_SHIFT		= 10, /* conditional branches */
 
+	PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT	= 11, /* call/ret stack */
+
 	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
 };
 
@@ -175,18 +177,16 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_HV		= 1U << PERF_SAMPLE_BRANCH_HV_SHIFT,
 
 	PERF_SAMPLE_BRANCH_ANY		= 1U << PERF_SAMPLE_BRANCH_ANY_SHIFT,
-	PERF_SAMPLE_BRANCH_ANY_CALL	=
-				1U << PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT,
-	PERF_SAMPLE_BRANCH_ANY_RETURN	=
-				1U << PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT,
-	PERF_SAMPLE_BRANCH_IND_CALL	=
-				1U << PERF_SAMPLE_BRANCH_IND_CALL_SHIFT,
-	PERF_SAMPLE_BRANCH_ABORT_TX	=
-				1U << PERF_SAMPLE_BRANCH_ABORT_TX_SHIFT,
+	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT,
+	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT,
+	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << PERF_SAMPLE_BRANCH_IND_CALL_SHIFT,
+	PERF_SAMPLE_BRANCH_ABORT_TX	= 1U << PERF_SAMPLE_BRANCH_ABORT_TX_SHIFT,
 	PERF_SAMPLE_BRANCH_IN_TX	= 1U << PERF_SAMPLE_BRANCH_IN_TX_SHIFT,
 	PERF_SAMPLE_BRANCH_NO_TX	= 1U << PERF_SAMPLE_BRANCH_NO_TX_SHIFT,
 	PERF_SAMPLE_BRANCH_COND		= 1U << PERF_SAMPLE_BRANCH_COND_SHIFT,
 
+	PERF_SAMPLE_BRANCH_CALL_STACK	= 1U << PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT,
+
 	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };
 

commit 27ac905b8f88d28779b0661809286b5ba2817d37
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Tue Nov 4 21:55:57 2014 -0500

    perf/x86/intel: Reduce lbr_sel_map[] size
    
    The index of lbr_sel_map is bit value of perf branch_sample_type.
    PERF_SAMPLE_BRANCH_MAX is 1024 at present, so each lbr_sel_map uses
    4096 bytes. By using bit shift as index, we can reduce lbr_sel_map
    size to 40 bytes. This patch defines 'bit shift' for branch types,
    and use 'bit shift' to define lbr_sel_maps.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Kan Liang <kan.liang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: jolsa@redhat.com
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/1415156173-10035-2-git-send-email-kan.liang@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 9b79abbd1ab8..e46b93279e3d 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -152,21 +152,42 @@ enum perf_event_sample_format {
  * The branch types can be combined, however BRANCH_ANY covers all types
  * of branches and therefore it supersedes all the other types.
  */
+enum perf_branch_sample_type_shift {
+	PERF_SAMPLE_BRANCH_USER_SHIFT		= 0, /* user branches */
+	PERF_SAMPLE_BRANCH_KERNEL_SHIFT		= 1, /* kernel branches */
+	PERF_SAMPLE_BRANCH_HV_SHIFT		= 2, /* hypervisor branches */
+
+	PERF_SAMPLE_BRANCH_ANY_SHIFT		= 3, /* any branch types */
+	PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT	= 4, /* any call branch */
+	PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT	= 5, /* any return branch */
+	PERF_SAMPLE_BRANCH_IND_CALL_SHIFT	= 6, /* indirect calls */
+	PERF_SAMPLE_BRANCH_ABORT_TX_SHIFT	= 7, /* transaction aborts */
+	PERF_SAMPLE_BRANCH_IN_TX_SHIFT		= 8, /* in transaction */
+	PERF_SAMPLE_BRANCH_NO_TX_SHIFT		= 9, /* not in transaction */
+	PERF_SAMPLE_BRANCH_COND_SHIFT		= 10, /* conditional branches */
+
+	PERF_SAMPLE_BRANCH_MAX_SHIFT		/* non-ABI */
+};
+
 enum perf_branch_sample_type {
-	PERF_SAMPLE_BRANCH_USER		= 1U << 0, /* user branches */
-	PERF_SAMPLE_BRANCH_KERNEL	= 1U << 1, /* kernel branches */
-	PERF_SAMPLE_BRANCH_HV		= 1U << 2, /* hypervisor branches */
-
-	PERF_SAMPLE_BRANCH_ANY		= 1U << 3, /* any branch types */
-	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << 4, /* any call branch */
-	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << 5, /* any return branch */
-	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << 6, /* indirect calls */
-	PERF_SAMPLE_BRANCH_ABORT_TX	= 1U << 7, /* transaction aborts */
-	PERF_SAMPLE_BRANCH_IN_TX	= 1U << 8, /* in transaction */
-	PERF_SAMPLE_BRANCH_NO_TX	= 1U << 9, /* not in transaction */
-	PERF_SAMPLE_BRANCH_COND		= 1U << 10, /* conditional branches */
-
-	PERF_SAMPLE_BRANCH_MAX		= 1U << 11, /* non-ABI */
+	PERF_SAMPLE_BRANCH_USER		= 1U << PERF_SAMPLE_BRANCH_USER_SHIFT,
+	PERF_SAMPLE_BRANCH_KERNEL	= 1U << PERF_SAMPLE_BRANCH_KERNEL_SHIFT,
+	PERF_SAMPLE_BRANCH_HV		= 1U << PERF_SAMPLE_BRANCH_HV_SHIFT,
+
+	PERF_SAMPLE_BRANCH_ANY		= 1U << PERF_SAMPLE_BRANCH_ANY_SHIFT,
+	PERF_SAMPLE_BRANCH_ANY_CALL	=
+				1U << PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT,
+	PERF_SAMPLE_BRANCH_ANY_RETURN	=
+				1U << PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT,
+	PERF_SAMPLE_BRANCH_IND_CALL	=
+				1U << PERF_SAMPLE_BRANCH_IND_CALL_SHIFT,
+	PERF_SAMPLE_BRANCH_ABORT_TX	=
+				1U << PERF_SAMPLE_BRANCH_ABORT_TX_SHIFT,
+	PERF_SAMPLE_BRANCH_IN_TX	= 1U << PERF_SAMPLE_BRANCH_IN_TX_SHIFT,
+	PERF_SAMPLE_BRANCH_NO_TX	= 1U << PERF_SAMPLE_BRANCH_NO_TX_SHIFT,
+	PERF_SAMPLE_BRANCH_COND		= 1U << PERF_SAMPLE_BRANCH_COND_SHIFT,
+
+	PERF_SAMPLE_BRANCH_MAX		= 1U << PERF_SAMPLE_BRANCH_MAX_SHIFT,
 };
 
 #define PERF_SAMPLE_BRANCH_PLM_ALL \

commit 60e2364e60e86e81bc6377f49779779e6120977f
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Sep 24 13:48:37 2014 +0200

    perf: Add ability to sample machine state on interrupt
    
    Enable capture of interrupted machine state for each sample.
    
    Registers to sample are passed per event in the sample_regs_intr bitmask.
    
    To sample interrupt machine state, the PERF_SAMPLE_INTR_REGS must be passed in
    sample_type.
    
    The list of available registers is arch dependent and provided by asm/perf_regs.h
    
    Registers are laid out as u64 in the order of the bit order of sample_intr_regs.
    
    This patch also adds a new ABI version PERF_ATTR_SIZE_VER4 because we extend
    the perf_event_attr struct with a new u64 field.
    
    Reviewed-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: cebbert.lkml@gmail.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/1411559322-16548-2-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 9d845404d875..9b79abbd1ab8 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -137,8 +137,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_DATA_SRC			= 1U << 15,
 	PERF_SAMPLE_IDENTIFIER			= 1U << 16,
 	PERF_SAMPLE_TRANSACTION			= 1U << 17,
+	PERF_SAMPLE_REGS_INTR			= 1U << 18,
 
-	PERF_SAMPLE_MAX = 1U << 18,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 19,		/* non-ABI */
 };
 
 /*
@@ -238,6 +239,7 @@ enum perf_event_read_format {
 #define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
 #define PERF_ATTR_SIZE_VER3	96	/* add: sample_regs_user */
 					/* add: sample_stack_user */
+#define PERF_ATTR_SIZE_VER4	104	/* add: sample_regs_intr */
 
 /*
  * Hardware event_id to monitor via a performance monitoring event:
@@ -334,6 +336,15 @@ struct perf_event_attr {
 
 	/* Align to u64. */
 	__u32	__reserved_2;
+	/*
+	 * Defines set of regs to dump for each sample
+	 * state captured on:
+	 *  - precise = 0: PMU interrupt
+	 *  - precise > 0: sampled instruction
+	 *
+	 * See asm/perf_regs.h for details.
+	 */
+	__u64	sample_regs_intr;
 };
 
 #define perf_flags(attr)	(*(&(attr)->read_format + 1))
@@ -686,6 +697,8 @@ enum perf_event_type {
 	 *	{ u64			weight;   } && PERF_SAMPLE_WEIGHT
 	 *	{ u64			data_src; } && PERF_SAMPLE_DATA_SRC
 	 *	{ u64			transaction; } && PERF_SAMPLE_TRANSACTION
+	 *	{ u64			abi; # enum perf_sample_regs_abi
+	 *	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_INTR
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit b438b1ab35507bbccc28d13f0b8286ffcf24019d
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Oct 2 22:16:36 2014 -0700

    perf: Fix typos in sample code in the perf_event.h header
    
    struct perf_event_mmap_page has members called "index" and
    "cap_user_rdpmc".  Spell them correctly in the examples.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-api@vger.kernel.org
    Link: http://lkml.kernel.org/r/320ba26391a8123cc16e5f02d24d34bd404332fd.1412313343.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 9269de254874..9d845404d875 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -364,7 +364,7 @@ struct perf_event_mmap_page {
 	/*
 	 * Bits needed to read the hw events in user-space.
 	 *
-	 *   u32 seq, time_mult, time_shift, idx, width;
+	 *   u32 seq, time_mult, time_shift, index, width;
 	 *   u64 count, enabled, running;
 	 *   u64 cyc, time_offset;
 	 *   s64 pmc = 0;
@@ -383,11 +383,11 @@ struct perf_event_mmap_page {
 	 *       time_shift  = pc->time_shift;
 	 *     }
 	 *
-	 *     idx = pc->index;
+	 *     index = pc->index;
 	 *     count = pc->offset;
-	 *     if (pc->cap_usr_rdpmc && idx) {
+	 *     if (pc->cap_user_rdpmc && index) {
 	 *       width = pc->pmc_width;
-	 *       pmc = rdpmc(idx - 1);
+	 *       pmc = rdpmc(index - 1);
 	 *     }
 	 *
 	 *     barrier();
@@ -415,7 +415,7 @@ struct perf_event_mmap_page {
 	};
 
 	/*
-	 * If cap_usr_rdpmc this field provides the bit-width of the value
+	 * If cap_user_rdpmc this field provides the bit-width of the value
 	 * read using the rdpmc() or equivalent instruction. This can be used
 	 * to sign extend the result like:
 	 *
@@ -439,10 +439,10 @@ struct perf_event_mmap_page {
 	 *
 	 * Where time_offset,time_mult,time_shift and cyc are read in the
 	 * seqcount loop described above. This delta can then be added to
-	 * enabled and possible running (if idx), improving the scaling:
+	 * enabled and possible running (if index), improving the scaling:
 	 *
 	 *   enabled += delta;
-	 *   if (idx)
+	 *   if (index)
 	 *     running += delta;
 	 *
 	 *   quot = count / running;

commit f972eb63b1003fae68d7b7e9b674d4ba5db681c2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon May 19 15:13:47 2014 -0400

    perf: Pass protection and flags bits through mmap2 interface
    
    The mmap2 interface was missing the protection and flags bits needed to
    accurately determine if a mmap memory area was shared or private and
    if it was readable or not.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [tweaked patch to compile and wrote changelog]
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Link: http://lkml.kernel.org/r/1400526833-141779-2-git-send-email-dzickus@redhat.com
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 5312fae47218..9269de254874 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -705,6 +705,7 @@ enum perf_event_type {
 	 *	u32				min;
 	 *	u64				ino;
 	 *	u64				ino_generation;
+	 *	u32				prot, flags;
 	 *	char				filename[];
 	 * 	struct sample_id		sample_id;
 	 * };

commit 82b897782d10fcc4930c9d4a15b175348fdd2871
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Wed May 28 11:45:04 2014 +0300

    perf: Differentiate exec() and non-exec() comm events
    
    perf tools like 'perf report' can aggregate samples by comm strings,
    which generally works.  However, there are other potential use-cases.
    For example, to pair up 'calls' with 'returns' accurately (from branch
    events like Intel BTS) it is necessary to identify whether the process
    has exec'd.  Although a comm event is generated when an 'exec' happens
    it is also generated whenever the comm string is changed on a whim
    (e.g. by prctl PR_SET_NAME).  This patch adds a flag to the comm event
    to differentiate one case from the other.
    
    In order to determine whether the kernel supports the new flag, a
    selection bit named 'exec' is added to struct perf_event_attr.  The
    bit does nothing but will cause perf_event_open() to fail if the bit
    is set on kernels that do not have it defined.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/537D9EBE.7030806@intel.com
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-fsdevel@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index d9cd853818ad..5312fae47218 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -302,8 +302,8 @@ struct perf_event_attr {
 				exclude_callchain_kernel : 1, /* exclude kernel callchains */
 				exclude_callchain_user   : 1, /* exclude user callchains */
 				mmap2          :  1, /* include mmap with inode data     */
-
-				__reserved_1   : 40;
+				comm_exec      :  1, /* flag comm events that are due to an exec */
+				__reserved_1   : 39;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -502,7 +502,12 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
 #define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
 
+/*
+ * PERF_RECORD_MISC_MMAP_DATA and PERF_RECORD_MISC_COMM_EXEC are used on
+ * different events so can reuse the same bit position.
+ */
 #define PERF_RECORD_MISC_MMAP_DATA		(1 << 13)
+#define PERF_RECORD_MISC_COMM_EXEC		(1 << 13)
 /*
  * Indicates that the content of PERF_SAMPLE_IP points to
  * the actual instruction that triggered the event. See also

commit bac52139f0b7ab31330e98fd87fc5a2664951050
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu May 22 12:50:07 2014 +0530

    perf: Add new conditional branch filter 'PERF_SAMPLE_BRANCH_COND'
    
    This patch introduces new branch filter PERF_SAMPLE_BRANCH_COND which
    will extend the existing perf ABI. This will filter branches which are
    conditional. Various architectures can provide this functionality either
    with HW filtering support (if present) or with SW filtering of captured
    branch instructions.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: mpe@ellerman.id.au
    Cc: benh@kernel.crashing.org
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1400743210-32289-1-git-send-email-khandual@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index e3fc8f09d110..d9cd853818ad 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -163,8 +163,9 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_ABORT_TX	= 1U << 7, /* transaction aborts */
 	PERF_SAMPLE_BRANCH_IN_TX	= 1U << 8, /* in transaction */
 	PERF_SAMPLE_BRANCH_NO_TX	= 1U << 9, /* not in transaction */
+	PERF_SAMPLE_BRANCH_COND		= 1U << 10, /* conditional branches */
 
-	PERF_SAMPLE_BRANCH_MAX		= 1U << 10, /* non-ABI */
+	PERF_SAMPLE_BRANCH_MAX		= 1U << 11, /* non-ABI */
 };
 
 #define PERF_SAMPLE_BRANCH_PLM_ALL \

commit 643fd0b9f5dc40fedbfbb908ebe6f1169284f7d8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 23 12:22:54 2014 +0200

    perf: Fix perf_event_open(.flags) test
    
    Vince noticed that we test the (unsigned long) flags field against an
    (unsigned int) constant. This would allow setting the high bits on 64bit
    platforms and not get an error.
    
    There is nothing that uses the high bits, so it should be entirely
    harmless, but we don't want userspace to accidentally set them anyway,
    so fix the constants.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140423102254.GL11096@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 853bc1ccb395..e3fc8f09d110 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -722,10 +722,10 @@ enum perf_callchain_context {
 	PERF_CONTEXT_MAX		= (__u64)-4095,
 };
 
-#define PERF_FLAG_FD_NO_GROUP		(1U << 0)
-#define PERF_FLAG_FD_OUTPUT		(1U << 1)
-#define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
-#define PERF_FLAG_FD_CLOEXEC		(1U << 3) /* O_CLOEXEC */
+#define PERF_FLAG_FD_NO_GROUP		(1UL << 0)
+#define PERF_FLAG_FD_OUTPUT		(1UL << 1)
+#define PERF_FLAG_PID_CGROUP		(1UL << 2) /* pid=cgroup id, per-cpu mode only */
+#define PERF_FLAG_FD_CLOEXEC		(1UL << 3) /* O_CLOEXEC */
 
 union perf_mem_data_src {
 	__u64 val;

commit 0d9dfc23f4d8c17365c84eb48ecca28b963ba192
Author: Mike Frysinger <vapier@gentoo.org>
Date:   Thu Jan 23 15:54:11 2014 -0800

    uapi: convert u64 to __u64 in exported headers
    
    The u64 type is not defined in any exported kernel headers, so trying to
    use it will lead to build failures.
    
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index e244ed412745..853bc1ccb395 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -788,7 +788,7 @@ union perf_mem_data_src {
 #define PERF_MEM_TLB_SHIFT	26
 
 #define PERF_MEM_S(a, s) \
-	(((u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
+	(((__u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
 
 /*
  * single taken branch record layout:

commit 860fc2f2640ec348b9520ca4649b1bfd23d91bc2
Merge: 197749981e53 bee09ed91cac
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Jan 16 09:33:30 2014 +0100

    Merge branch 'perf/urgent' into perf/core
    
    Pick up the latest fixes, refresh the development tree.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a21b0b354d4ac39be691f51c53562e2c24443d9e
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Sun Jan 5 21:36:33 2014 +0100

    perf: Introduce a flag to enable close-on-exec in perf_event_open()
    
    Unlike recent modern userspace API such as:
    
      epoll_create1 (EPOLL_CLOEXEC), eventfd (EFD_CLOEXEC),
      fanotify_init (FAN_CLOEXEC), inotify_init1 (IN_CLOEXEC),
      signalfd (SFD_CLOEXEC), timerfd_create (TFD_CLOEXEC),
      or the venerable general purpose open (O_CLOEXEC),
    
    perf_event_open() syscall lack a flag to atomically set FD_CLOEXEC
    (eg. close-on-exec) flag on file descriptor it returns to userspace.
    
    The present patch adds a PERF_FLAG_FD_CLOEXEC flag to allow
    perf_event_open() syscall to atomically set close-on-exec.
    
    Having this flag will enable userspace to remove the file descriptor
    from the list of file descriptors being inherited across exec,
    without the need to call fcntl(fd, F_SETFD, FD_CLOEXEC) and the
    associated race condition between the current thread and another
    thread calling fork(2) then execve(2).
    
    Links:
    
     - Secure File Descriptor Handling (Ulrich Drepper, 2008)
       http://udrepper.livejournal.com/20407.html
    
     - Excuse me son, but your code is leaking !!! (Dan Walsh, March 2012)
       http://danwalsh.livejournal.com/53603.html
    
     - Notes in DMA buffer sharing: leak and security hole
       http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/dma-buf-sharing.txt?id=v3.13-rc3#n428
    
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/8c03f54e1598b1727c19706f3af03f98685d9fe6.1388952061.git.ydroneaud@opteya.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index e1802d6153ae..ca018b4085c6 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -724,6 +724,7 @@ enum perf_callchain_context {
 #define PERF_FLAG_FD_NO_GROUP		(1U << 0)
 #define PERF_FLAG_FD_OUTPUT		(1U << 1)
 #define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
+#define PERF_FLAG_FD_CLOEXEC		(1U << 3) /* O_CLOEXEC */
 
 union perf_mem_data_src {
 	__u64 val;

commit 189b84fb54490ae24111124346a8e63f8e019385
Author: Vince Weaver <vince@deater.net>
Date:   Fri Dec 13 15:52:25 2013 -0500

    perf: Document the new transaction sample type
    
    Commit fdfbbd07e91f8fe3871 ("perf: Add generic transaction flags")
    added support for PERF_SAMPLE_TRANSACTION but forgot to add documentation
    for the sample type to include/uapi/linux/perf_event.h
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1312131548450.10372@pianoman.cluster.toy
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index e1802d6153ae..959d454f76a1 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -679,6 +679,7 @@ enum perf_event_type {
 	 *
 	 *	{ u64			weight;   } && PERF_SAMPLE_WEIGHT
 	 *	{ u64			data_src; } && PERF_SAMPLE_DATA_SRC
+	 *	{ u64			transaction; } && PERF_SAMPLE_TRANSACTION
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit 2a3ede8cb2ddee5885518e4232aca13056f9a6e0
Merge: 0e73453e172a 215432ed3039
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Nov 4 07:49:35 2013 +0100

    Merge branch 'perf/urgent' into perf/core to fix conflicts
    
    Conflicts:
            tools/perf/bench/numa.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit bf378d341e4873ed928dc3c636252e6895a21f50
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 28 13:55:29 2013 +0100

    perf: Fix perf ring buffer memory ordering
    
    The PPC64 people noticed a missing memory barrier and crufty old
    comments in the perf ring buffer code. So update all the comments and
    add the missing barrier.
    
    When the architecture implements local_t using atomic_long_t there
    will be double barriers issued; but short of introducing more
    conditional barrier primitives this is the best we can do.
    
    Reported-by: Victor Kaplansky <victork@il.ibm.com>
    Tested-by: Victor Kaplansky <victork@il.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: michael@ellerman.id.au
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: anton@samba.org
    Cc: benh@kernel.crashing.org
    Link: http://lkml.kernel.org/r/20131025173749.GG19466@laptop.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 009a655a5d35..2fc1602e23bb 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -456,13 +456,15 @@ struct perf_event_mmap_page {
 	/*
 	 * Control data for the mmap() data buffer.
 	 *
-	 * User-space reading the @data_head value should issue an rmb(), on
-	 * SMP capable platforms, after reading this value -- see
-	 * perf_event_wakeup().
+	 * User-space reading the @data_head value should issue an smp_rmb(),
+	 * after reading this value.
 	 *
 	 * When the mapping is PROT_WRITE the @data_tail value should be
-	 * written by userspace to reflect the last read data. In this case
-	 * the kernel will not over-write unread data.
+	 * written by userspace to reflect the last read data, after issueing
+	 * an smp_mb() to separate the data read from the ->data_tail store.
+	 * In this case the kernel will not over-write unread data.
+	 *
+	 * See perf_output_put_handle() for the data ordering.
 	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */

commit fdfbbd07e91f8fe387140776f3fd94605f0c89e5
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Sep 20 07:40:39 2013 -0700

    perf: Add generic transaction flags
    
    Add a generic qualifier for transaction events, as a new sample
    type that returns a flag word. This is particularly useful
    for qualifying aborts: to distinguish aborts which happen
    due to asynchronous events (like conflicts caused by another
    CPU) versus instructions that lead to an abort.
    
    The tuning strategies are very different for those cases,
    so it's important to distinguish them easily and early.
    
    Since it's inconvenient and inflexible to filter for this
    in the kernel we report all the events out and allow
    some post processing in user space.
    
    The flags are based on the Intel TSX events, but should be fairly
    generic and mostly applicable to other HTM architectures too. In addition
    to various flag words there's also reserved space to report an
    program supplied abort code. For TSX this is used to distinguish specific
    classes of aborts, like a lock busy abort when doing lock elision.
    
    Flags:
    
    Elision and generic transactions                   (ELISION vs TRANSACTION)
    (HLE vs RTM on TSX; IBM etc.  would likely only use TRANSACTION)
    Aborts caused by current thread vs aborts caused by others (SYNC vs ASYNC)
    Retryable transaction                              (RETRY)
    Conflicts with other threads                       (CONFLICT)
    Transaction write capacity overflow                (CAPACITY WRITE)
    Transaction read capacity overflow                 (CAPACITY READ)
    
    Transactions implicitely aborted can also return an abort code.
    This can be used to signal specific events to the profiler. A common
    case is abort on lock busy in a RTM eliding library (code 0xff)
    To handle this case we include the TSX abort code
    
    Common example aborts in TSX would be:
    
    - Data conflict with another thread on memory read.
                                          Flags: TRANSACTION|ASYNC|CONFLICT
    - executing a WRMSR in a transaction. Flags: TRANSACTION|SYNC
    - HLE transaction in user space is too large
                                          Flags: ELISION|SYNC|CAPACITY-WRITE
    
    The only flag that is somewhat TSX specific is ELISION.
    
    This adds the perf core glue needed for reporting the new flag word out.
    
    v2: Add MEM/MISC
    v3: Move transaction to the end
    v4: Separate capacity-read/write and remove misc
    v5: Remove _SAMPLE. Move abort flags to 32bit. Rename
        transaction to txn
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379688044-14173-2-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 009a655a5d35..da48837d617d 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -136,8 +136,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_WEIGHT			= 1U << 14,
 	PERF_SAMPLE_DATA_SRC			= 1U << 15,
 	PERF_SAMPLE_IDENTIFIER			= 1U << 16,
+	PERF_SAMPLE_TRANSACTION			= 1U << 17,
 
-	PERF_SAMPLE_MAX = 1U << 17,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 18,		/* non-ABI */
 };
 
 /*
@@ -180,6 +181,28 @@ enum perf_sample_regs_abi {
 	PERF_SAMPLE_REGS_ABI_64		= 2,
 };
 
+/*
+ * Values for the memory transaction event qualifier, mostly for
+ * abort events. Multiple bits can be set.
+ */
+enum {
+	PERF_TXN_ELISION        = (1 << 0), /* From elision */
+	PERF_TXN_TRANSACTION    = (1 << 1), /* From transaction */
+	PERF_TXN_SYNC           = (1 << 2), /* Instruction is related */
+	PERF_TXN_ASYNC          = (1 << 3), /* Instruction not related */
+	PERF_TXN_RETRY          = (1 << 4), /* Retry possible */
+	PERF_TXN_CONFLICT       = (1 << 5), /* Conflict abort */
+	PERF_TXN_CAPACITY_WRITE = (1 << 6), /* Capacity write abort */
+	PERF_TXN_CAPACITY_READ  = (1 << 7), /* Capacity read abort */
+
+	PERF_TXN_MAX	        = (1 << 8), /* non-ABI */
+
+	/* bits 32..63 are reserved for the abort code */
+
+	PERF_TXN_ABORT_MASK  = (0xffffffffULL << 32),
+	PERF_TXN_ABORT_SHIFT = 32,
+};
+
 /*
  * The format of the data returned by read() on a perf event fd,
  * as specified by attr.read_format:

commit fa7315871046b9a4c48627905691dbde57e51033
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 19 10:16:42 2013 +0200

    perf: Fix capabilities bitfield compatibility in 'struct perf_event_mmap_page'
    
    Solve the problems around the broken definition of perf_event_mmap_page::
    cap_usr_time and cap_usr_rdpmc fields which used to overlap, partially
    fixed by:
    
      860f085b74e9 ("perf: Fix broken union in 'struct perf_event_mmap_page'")
    
    The problem with the fix (merged in v3.12-rc1 and not yet released
    officially), noticed by Vince Weaver is that the new behavior is
    not detectable by new user-space, and that due to the reuse of the
    field names it's easy to mis-compile a binary if old headers are used
    on a new kernel or new headers are used on an old kernel.
    
    To solve all that make this change explicit, detectable and self-contained,
    by iterating the ABI the following way:
    
     - Always clear bit 0, and rename it to usrpage->cap_bit0, to at least not
       confuse old user-space binaries. RDPMC will be marked as unavailable
       to old binaries but that's within the ABI, this is a capability bit.
    
     - Rename bit 1 to ->cap_bit0_is_deprecated and always set it to 1, so new
       libraries can reliably detect that bit 0 is deprecated and perma-zero
       without having to check the kernel version.
    
     - Use bits 2, 3, 4 for the newly defined, correct functionality:
    
            cap_user_rdpmc          : 1, /* The RDPMC instruction can be used to read counts */
            cap_user_time           : 1, /* The time_* fields are used */
            cap_user_time_zero      : 1, /* The time_zero field is used */
    
     - Rename all the bitfield names in perf_event.h to be different from the
       old names, to make sure it's not possible to mis-compile it
       accidentally with old assumptions.
    
    The 'size' field can then be used in the future to add new fields and it
    will act as a natural ABI version indicator as well.
    
    Also adjust tools/perf/ userspace for the new definitions, noticed by
    Adrian Hunter.
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Also-Fixed-by: Adrian Hunter <adrian.hunter@intel.com>
    Link: http://lkml.kernel.org/n/tip-zr03yxjrpXesOzzupszqglbv@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 7f6d584c267b..009a655a5d35 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -380,10 +380,13 @@ struct perf_event_mmap_page {
 	union {
 		__u64	capabilities;
 		struct {
-			__u64	cap_usr_time		: 1,
-				cap_usr_rdpmc		: 1,
-				cap_usr_time_zero	: 1,
-				cap_____res		: 61;
+			__u64	cap_bit0		: 1, /* Always 0, deprecated, see commit 860f085b74e9 */
+				cap_bit0_is_deprecated	: 1, /* Always 1, signals that bit 0 is zero */
+
+				cap_user_rdpmc		: 1, /* The RDPMC instruction can be used to read counts */
+				cap_user_time		: 1, /* The time_* fields are used */
+				cap_user_time_zero	: 1, /* The time_zero field is used */
+				cap_____res		: 59;
 		};
 	};
 
@@ -442,12 +445,13 @@ struct perf_event_mmap_page {
 	 *               ((rem * time_mult) >> time_shift);
 	 */
 	__u64	time_zero;
+	__u32	size;			/* Header size up to __reserved[] fields. */
 
 		/*
 		 * Hole for extension of the self monitor capabilities
 		 */
 
-	__u64	__reserved[119];	/* align to 1k */
+	__u8	__reserved[118*8+4];	/* align to 1k. */
 
 	/*
 	 * Control data for the mmap() data buffer.

commit c5ecceefdb840af45db436adc58219ae97b6ef3c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 13 23:39:17 2013 +0200

    perf: Update ABI comment
    
    For some mysterious reason the sample_id field of PERF_RECORD_MMAP went AWOL.
    
    Reported-by: Vince Weaver <vince@deater.net>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 40a1fb807396..7f6d584c267b 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -528,6 +528,7 @@ enum perf_event_type {
 	 *	u64				len;
 	 *	u64				pgoff;
 	 *	char				filename[];
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_MMAP			= 1,

commit a8e0108cac181a7b141dacaa99ea52efaf9b5f07
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Tue Sep 17 14:53:41 2013 -0400

    perf: Fix UAPI export of PERF_EVENT_IOC_ID
    
    Without the following patch I have problems compiling code using
    the new PERF_EVENT_IOC_ID ioctl().  It looks like u64 was used
    instead of __u64
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1309171450380.11444@vincent-weaver-1.um.maine.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index ca1d90bcb74d..40a1fb807396 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -324,7 +324,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
 #define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
-#define PERF_EVENT_IOC_ID		_IOR('$', 7, u64 *)
+#define PERF_EVENT_IOC_ID		_IOR('$', 7, __u64 *)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit fa0097ee690693006ab1aea6c01ad3c851b65c77
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Sat Aug 31 21:50:51 2013 +0300

    perf: Add a dummy software event to keep tracking
    
    When an event is disabled the "tracking" events selected by the 'mmap',
    'comm' and 'task' bits of struct perf_event_attr, are also disabled.
    However, the information those events provide is necessary to resolve
    symbols for when the main event is re-enabled.
    
    The "tracking" events can be kept enabled by putting them on another
    event, but that requires an event that otherwise does nothing.  A new
    software event PERF_COUNT_SW_DUMMY is added for that purpose.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1377975053-3811-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 408b8c730731..ca1d90bcb74d 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -109,6 +109,7 @@ enum perf_sw_ids {
 	PERF_COUNT_SW_PAGE_FAULTS_MAJ		= 6,
 	PERF_COUNT_SW_ALIGNMENT_FAULTS		= 7,
 	PERF_COUNT_SW_EMULATION_FAULTS		= 8,
+	PERF_COUNT_SW_DUMMY			= 9,
 
 	PERF_COUNT_SW_MAX,			/* non-ABI */
 };

commit 274481de6cb69abdb49403ff32abb63c23743413
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Fri Aug 23 15:51:03 2013 -0400

    perf: Export struct perf_branch_entry to userspace
    
    If PERF_SAMPLE_BRANCH_STACK is enabled then samples are returned
    with the format { u64 from, to, flags } but the flags layout
    is not specified.
    
    This field has the type struct perf_branch_entry; move this
    definition into include/uapi/linux/perf_event.h so users can
    access these fields.
    
    This is similar to the existing inclusion of perf_mem_data_src in
    the include/uapi/linux/perf_event.h file.
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Acked-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1308231544420.1889@vincent-weaver-1.um.maine.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index a77f43af72b8..408b8c730731 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -757,4 +757,28 @@ union perf_mem_data_src {
 #define PERF_MEM_S(a, s) \
 	(((u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
 
+/*
+ * single taken branch record layout:
+ *
+ *      from: source instruction (may not always be a branch insn)
+ *        to: branch target
+ *   mispred: branch target was mispredicted
+ * predicted: branch target was predicted
+ *
+ * support for mispred, predicted is optional. In case it
+ * is not supported mispred = predicted = 0.
+ *
+ *     in_tx: running in a hardware transaction
+ *     abort: aborting a hardware transaction
+ */
+struct perf_branch_entry {
+	__u64	from;
+	__u64	to;
+	__u64	mispred:1,  /* target mispredicted */
+		predicted:1,/* target predicted */
+		in_tx:1,    /* in transaction */
+		abort:1,    /* transaction abort */
+		reserved:60;
+};
+
 #endif /* _UAPI_LINUX_PERF_EVENT_H */

commit 13d7a2410fa637f450a29ecb515ac318ee40c741
Author: Stephane Eranian <eranian@google.com>
Date:   Wed Aug 21 12:10:24 2013 +0200

    perf: Add attr->mmap2 attribute to an event
    
    Adds a new PERF_RECORD_MMAP2 record type which is essence
    an expanded version of PERF_RECORD_MMAP.
    
    Used to request mmap records with more information about
    the mapping, including device major, minor and the inode
    number and generation for mappings associated with files
    or shared memory segments. Works for code and data
    (with attr->mmap_data set).
    
    Existing PERF_RECORD_MMAP record is unmodified by this patch.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Link: http://lkml.kernel.org/r/1377079825-19057-2-git-send-email-eranian@google.com
    [ Added Al to the Cc:. Are the ino, maj/min exports of vma->vm_file OK? ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 42cb7b62ca59..a77f43af72b8 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -276,8 +276,9 @@ struct perf_event_attr {
 
 				exclude_callchain_kernel : 1, /* exclude kernel callchains */
 				exclude_callchain_user   : 1, /* exclude user callchains */
+				mmap2          :  1, /* include mmap with inode data     */
 
-				__reserved_1   : 41;
+				__reserved_1   : 40;
 
 	union {
 		__u32		wakeup_events;	  /* wakeup every n events */
@@ -651,6 +652,27 @@ enum perf_event_type {
 	 */
 	PERF_RECORD_SAMPLE			= 9,
 
+	/*
+	 * The MMAP2 records are an augmented version of MMAP, they add
+	 * maj, min, ino numbers to be used to uniquely identify each mapping
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u32				pid, tid;
+	 *	u64				addr;
+	 *	u64				len;
+	 *	u64				pgoff;
+	 *	u32				maj;
+	 *	u32				min;
+	 *	u64				ino;
+	 *	u64				ino_generation;
+	 *	char				filename[];
+	 * 	struct sample_id		sample_id;
+	 * };
+	 */
+	PERF_RECORD_MMAP2			= 10,
+
 	PERF_RECORD_MAX,			/* non-ABI */
 };
 

commit ff3d527cebc1fa3707c617bfe9e74f53fcfb0955
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Tue Aug 27 11:23:07 2013 +0300

    perf: make events stream always parsable
    
    The event stream is not always parsable because the format of a sample
    is dependent on the sample_type of the selected event.  When there is
    more than one selected event and the sample_types are not the same then
    parsing becomes problematic.  A sample can be matched to its selected
    event using the ID that is allocated when the event is opened.
    Unfortunately, to get the ID from the sample means first parsing it.
    
    This patch adds a new sample format bit PERF_SAMPLE_IDENTIFER that puts
    the ID at a fixed position so that the ID can be retrieved without
    parsing the sample.  For sample events, that is the first position
    immediately after the header.  For non-sample events, that is the last
    position.
    
    In this respect parsing samples requires that the sample_type and ID
    values are recorded.  For example, perf tools records struct
    perf_event_attr and the IDs within the perf.data file.  Those must be
    read first before it is possible to parse samples found later in the
    perf.data file.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Tested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/r/1377591794-30553-6-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 62c25a25291c..42cb7b62ca59 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -134,8 +134,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_STACK_USER			= 1U << 13,
 	PERF_SAMPLE_WEIGHT			= 1U << 14,
 	PERF_SAMPLE_DATA_SRC			= 1U << 15,
+	PERF_SAMPLE_IDENTIFIER			= 1U << 16,
 
-	PERF_SAMPLE_MAX = 1U << 16,		/* non-ABI */
+	PERF_SAMPLE_MAX = 1U << 17,		/* non-ABI */
 };
 
 /*
@@ -492,12 +493,12 @@ enum perf_event_type {
 	/*
 	 * If perf_event_attr.sample_id_all is set then all event types will
 	 * have the sample_type selected fields related to where/when
-	 * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)
-	 * described in PERF_RECORD_SAMPLE below, it will be stashed just after
-	 * the perf_event_header and the fields already present for the existing
-	 * fields, i.e. at the end of the payload. That way a newer perf.data
-	 * file will be supported by older perf tools, with these new optional
-	 * fields being ignored.
+	 * (identity) an event took place (TID, TIME, ID, STREAM_ID, CPU,
+	 * IDENTIFIER) described in PERF_RECORD_SAMPLE below, it will be stashed
+	 * just after the perf_event_header and the fields already present for
+	 * the existing fields, i.e. at the end of the payload. That way a newer
+	 * perf.data file will be supported by older perf tools, with these new
+	 * optional fields being ignored.
 	 *
 	 * struct sample_id {
 	 * 	{ u32			pid, tid; } && PERF_SAMPLE_TID
@@ -505,7 +506,12 @@ enum perf_event_type {
 	 * 	{ u64			id;       } && PERF_SAMPLE_ID
 	 * 	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
 	 * 	{ u32			cpu, res; } && PERF_SAMPLE_CPU
+	 *	{ u64			id;	  } && PERF_SAMPLE_IDENTIFIER
 	 * } && perf_event_attr::sample_id_all
+	 *
+	 * Note that PERF_SAMPLE_IDENTIFIER duplicates PERF_SAMPLE_ID.  The
+	 * advantage of PERF_SAMPLE_IDENTIFIER is that its position is fixed
+	 * relative to header.size.
 	 */
 
 	/*
@@ -594,6 +600,13 @@ enum perf_event_type {
 	 * struct {
 	 *	struct perf_event_header	header;
 	 *
+	 *	#
+	 *	# Note that PERF_SAMPLE_IDENTIFIER duplicates PERF_SAMPLE_ID.
+	 *	# The advantage of PERF_SAMPLE_IDENTIFIER is that its position
+	 *	# is fixed relative to header.
+	 *	#
+	 *
+	 *	{ u64			id;	  } && PERF_SAMPLE_IDENTIFIER
 	 *	{ u64			ip;	  } && PERF_SAMPLE_IP
 	 *	{ u32			pid, tid; } && PERF_SAMPLE_TID
 	 *	{ u64			time;     } && PERF_SAMPLE_TIME

commit cf4957f17f2a89984915ea808876d9c82225b862
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Oct 24 13:37:58 2012 +0200

    perf: Add PERF_EVENT_IOC_ID ioctl to return event ID
    
    The only way to get the event ID is by reading the event fd,
    followed by parsing the ID value out of the returned data.
    
    While this is ok for current read format used by perf tool,
    it is not ok when we use PERF_FORMAT_GROUP format.
    
    With this format the data are returned for the whole group
    and there's no way to find out what ID belongs to our fd
    (if we are not group leader event).
    
    Adding a simple ioctl that returns event primary ID for given fd.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-v1bn5cto707jn0bon34afqr1@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index efef1d37a371..62c25a25291c 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -321,6 +321,7 @@ struct perf_event_attr {
 #define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
 #define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
 #define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
+#define PERF_EVENT_IOC_ID		_IOR('$', 7, u64 *)
 
 enum perf_event_ioc_flags {
 	PERF_IOC_FLAG_GROUP		= 1U << 0,

commit c73deb6aecda2955716f31572516f09d930ef450
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Jun 28 16:22:18 2013 +0300

    perf/x86: Add ability to calculate TSC from perf sample timestamps
    
    For modern CPUs, perf clock is directly related to TSC.  TSC
    can be calculated from perf clock and vice versa using a simple
    calculation.  Two of the three componenets of that calculation
    are already exported in struct perf_event_mmap_page.  This patch
    exports the third.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Link: http://lkml.kernel.org/r/1372425741-1676-3-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 0041aedf2297..efef1d37a371 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -378,7 +378,8 @@ struct perf_event_mmap_page {
 		struct {
 			__u64	cap_usr_time		: 1,
 				cap_usr_rdpmc		: 1,
-				cap_____res		: 62;
+				cap_usr_time_zero	: 1,
+				cap_____res		: 61;
 		};
 	};
 
@@ -420,12 +421,29 @@ struct perf_event_mmap_page {
 	__u16	time_shift;
 	__u32	time_mult;
 	__u64	time_offset;
+	/*
+	 * If cap_usr_time_zero, the hardware clock (e.g. TSC) can be calculated
+	 * from sample timestamps.
+	 *
+	 *   time = timestamp - time_zero;
+	 *   quot = time / time_mult;
+	 *   rem  = time % time_mult;
+	 *   cyc = (quot << time_shift) + (rem << time_shift) / time_mult;
+	 *
+	 * And vice versa:
+	 *
+	 *   quot = cyc >> time_shift;
+	 *   rem  = cyc & ((1 << time_shift) - 1);
+	 *   timestamp = time_zero + quot * time_mult +
+	 *               ((rem * time_mult) >> time_shift);
+	 */
+	__u64	time_zero;
 
 		/*
 		 * Hole for extension of the self monitor capabilities
 		 */
 
-	__u64	__reserved[120];	/* align to 1k */
+	__u64	__reserved[119];	/* align to 1k */
 
 	/*
 	 * Control data for the mmap() data buffer.

commit 860f085b74e9f0075de8140ed3a1e5b5e3e39aa8
Author: Adrian Hunter <adrian.hunter@intel.com>
Date:   Fri Jun 28 16:22:17 2013 +0300

    perf: Fix broken union in 'struct perf_event_mmap_page'
    
    The capabilities bits must not be "union'ed" together.
    Put them in a separate struct.
    
    Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1372425741-1676-2-git-send-email-adrian.hunter@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 00d8274730b4..0041aedf2297 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -375,9 +375,11 @@ struct perf_event_mmap_page {
 	__u64	time_running;		/* time event on cpu */
 	union {
 		__u64	capabilities;
-		__u64	cap_usr_time  : 1,
-			cap_usr_rdpmc : 1,
-			cap_____res   : 62;
+		struct {
+			__u64	cap_usr_time		: 1,
+				cap_usr_rdpmc		: 1,
+				cap_____res		: 62;
+		};
 	};
 
 	/*

commit a5cdd40c9877e9aba704c020fd65d26b5cfecf18
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jul 16 17:09:07 2013 +0200

    perf: Update perf_event_type documentation
    
    Due to a discussion with Adrian I had a good look at the perf_event_type record
    layout and found the documentation to be somewhat unclear.
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130716150907.GL23818@dyad.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 0b1df41691e8..00d8274730b4 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -478,6 +478,16 @@ enum perf_event_type {
 	 * file will be supported by older perf tools, with these new optional
 	 * fields being ignored.
 	 *
+	 * struct sample_id {
+	 * 	{ u32			pid, tid; } && PERF_SAMPLE_TID
+	 * 	{ u64			time;     } && PERF_SAMPLE_TIME
+	 * 	{ u64			id;       } && PERF_SAMPLE_ID
+	 * 	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
+	 * 	{ u32			cpu, res; } && PERF_SAMPLE_CPU
+	 * } && perf_event_attr::sample_id_all
+	 */
+
+	/*
 	 * The MMAP events record the PROT_EXEC mappings so that we can
 	 * correlate userspace IPs to code. They have the following structure:
 	 *
@@ -498,6 +508,7 @@ enum perf_event_type {
 	 *	struct perf_event_header	header;
 	 *	u64				id;
 	 *	u64				lost;
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_LOST			= 2,
@@ -508,6 +519,7 @@ enum perf_event_type {
 	 *
 	 *	u32				pid, tid;
 	 *	char				comm[];
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_COMM			= 3,
@@ -518,6 +530,7 @@ enum perf_event_type {
 	 *	u32				pid, ppid;
 	 *	u32				tid, ptid;
 	 *	u64				time;
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_EXIT			= 4,
@@ -528,6 +541,7 @@ enum perf_event_type {
 	 *	u64				time;
 	 *	u64				id;
 	 *	u64				stream_id;
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_THROTTLE			= 5,
@@ -539,6 +553,7 @@ enum perf_event_type {
 	 *	u32				pid, ppid;
 	 *	u32				tid, ptid;
 	 *	u64				time;
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_FORK			= 7,
@@ -549,6 +564,7 @@ enum perf_event_type {
 	 *	u32				pid, tid;
 	 *
 	 *	struct read_format		values;
+	 * 	struct sample_id		sample_id;
 	 * };
 	 */
 	PERF_RECORD_READ			= 8,
@@ -596,7 +612,7 @@ enum perf_event_type {
 	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
 	 *
 	 *	{ u64			weight;   } && PERF_SAMPLE_WEIGHT
-	 *	{ u64			data_src;     } && PERF_SAMPLE_DATA_SRC
+	 *	{ u64			data_src; } && PERF_SAMPLE_DATA_SRC
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit 135c5612c460f89657c4698fe2ea753f6f667963
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Jun 17 17:36:51 2013 -0700

    perf/x86/intel: Support Haswell/v4 LBR format
    
    Haswell has two additional LBR from flags for TSX: in_tx and
    abort_tx, implemented as a new "v4" version of the LBR format.
    
    Handle those in and adjust the sign extension code to still
    correctly extend. The flags are exported similarly in the LBR
    record to the existing misprediction flag
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Andi Kleen <ak@linux.jf.intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Link: http://lkml.kernel.org/r/1371515812-9646-6-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index fb104e51496e..0b1df41691e8 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -157,8 +157,11 @@ enum perf_branch_sample_type {
 	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << 4, /* any call branch */
 	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << 5, /* any return branch */
 	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << 6, /* indirect calls */
+	PERF_SAMPLE_BRANCH_ABORT_TX	= 1U << 7, /* transaction aborts */
+	PERF_SAMPLE_BRANCH_IN_TX	= 1U << 8, /* in transaction */
+	PERF_SAMPLE_BRANCH_NO_TX	= 1U << 9, /* not in transaction */
 
-	PERF_SAMPLE_BRANCH_MAX		= 1U << 7, /* non-ABI */
+	PERF_SAMPLE_BRANCH_MAX		= 1U << 10, /* non-ABI */
 };
 
 #define PERF_SAMPLE_BRANCH_PLM_ALL \

commit cc2f5a8adbc7ab1fdb7d9bcf4ea9838c73e82dfe
Author: Stephane Eranian <eranian@google.com>
Date:   Fri Apr 5 16:49:41 2013 +0200

    perf: Fix comments in PERF_MEM_LVL bitmask
    
    This small patch fixes a mistake in the comments
    for the PERF_MEM_LVL_* events. The L2, L3 bits simply
    represent cache levels, not hits or misses. That is
    encoded in PERF_MEM_LVL_MISS/PERF_MEM_LVL_HIT.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: jolsa@redhat.com
    Link: http://lkml.kernel.org/r/20130405144941.GA30503@quad
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 964a450a6e2c..fb104e51496e 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -645,8 +645,8 @@ union perf_mem_data_src {
 #define PERF_MEM_LVL_MISS	0x04  /* miss level  */
 #define PERF_MEM_LVL_L1		0x08  /* L1 */
 #define PERF_MEM_LVL_LFB	0x10  /* Line Fill Buffer */
-#define PERF_MEM_LVL_L2		0x20  /* L2 hit */
-#define PERF_MEM_LVL_L3		0x40  /* L3 hit */
+#define PERF_MEM_LVL_L2		0x20  /* L2 */
+#define PERF_MEM_LVL_L3		0x40  /* L3 */
 #define PERF_MEM_LVL_LOC_RAM	0x80  /* Local DRAM */
 #define PERF_MEM_LVL_REM_RAM1	0x100 /* Remote DRAM (1 hop) */
 #define PERF_MEM_LVL_REM_RAM2	0x200 /* Remote DRAM (2 hops) */

commit 2fe85427e3bf65d791700d065132772fc26e4d75
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 24 16:10:39 2013 +0100

    perf: Add PERF_RECORD_MISC_MMAP_DATA to RECORD_MMAP
    
    Type of mapping was lost and made it hard for a tool
    to distinguish code vs. data mmaps. Perf has the ability
    to distinguish the two.
    
    Use a bit in the header->misc bitmask to keep track of
    the mmap type. If PERF_RECORD_MISC_MMAP_DATA is set then
    the mapping is not executable (!VM_EXEC). If not set, then
    the mapping is executable.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: ak@linux.intel.com
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-16-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 5b5762006855..964a450a6e2c 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -445,6 +445,7 @@ struct perf_event_mmap_page {
 #define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
 #define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
 
+#define PERF_RECORD_MISC_MMAP_DATA		(1 << 13)
 /*
  * Indicates that the content of PERF_SAMPLE_IP points to
  * the actual instruction that triggered the event. See also

commit d6be9ad6c960f43800a6f118932bc8a5a4eadcd1
Author: Stephane Eranian <eranian@google.com>
Date:   Thu Jan 24 16:10:31 2013 +0100

    perf: Add generic memory sampling interface
    
    This patch adds PERF_SAMPLE_DATA_SRC.
    
    PERF_SAMPLE_DATA_SRC collects the data source, i.e., where
    did the data associated with the sampled instruction
    come from. Information is stored in a perf_mem_data_src
    structure. It contains opcode, mem level, tlb, snoop,
    lock information, subject to availability in hardware.
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: ak@linux.intel.com
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-8-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index cdc255da02e2..5b5762006855 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -133,9 +133,9 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_REGS_USER			= 1U << 12,
 	PERF_SAMPLE_STACK_USER			= 1U << 13,
 	PERF_SAMPLE_WEIGHT			= 1U << 14,
+	PERF_SAMPLE_DATA_SRC			= 1U << 15,
 
-	PERF_SAMPLE_MAX = 1U << 15,		/* non-ABI */
-
+	PERF_SAMPLE_MAX = 1U << 16,		/* non-ABI */
 };
 
 /*
@@ -592,6 +592,7 @@ enum perf_event_type {
 	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
 	 *
 	 *	{ u64			weight;   } && PERF_SAMPLE_WEIGHT
+	 *	{ u64			data_src;     } && PERF_SAMPLE_DATA_SRC
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,
@@ -617,4 +618,67 @@ enum perf_callchain_context {
 #define PERF_FLAG_FD_OUTPUT		(1U << 1)
 #define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
 
+union perf_mem_data_src {
+	__u64 val;
+	struct {
+		__u64   mem_op:5,	/* type of opcode */
+			mem_lvl:14,	/* memory hierarchy level */
+			mem_snoop:5,	/* snoop mode */
+			mem_lock:2,	/* lock instr */
+			mem_dtlb:7,	/* tlb access */
+			mem_rsvd:31;
+	};
+};
+
+/* type of opcode (load/store/prefetch,code) */
+#define PERF_MEM_OP_NA		0x01 /* not available */
+#define PERF_MEM_OP_LOAD	0x02 /* load instruction */
+#define PERF_MEM_OP_STORE	0x04 /* store instruction */
+#define PERF_MEM_OP_PFETCH	0x08 /* prefetch */
+#define PERF_MEM_OP_EXEC	0x10 /* code (execution) */
+#define PERF_MEM_OP_SHIFT	0
+
+/* memory hierarchy (memory level, hit or miss) */
+#define PERF_MEM_LVL_NA		0x01  /* not available */
+#define PERF_MEM_LVL_HIT	0x02  /* hit level */
+#define PERF_MEM_LVL_MISS	0x04  /* miss level  */
+#define PERF_MEM_LVL_L1		0x08  /* L1 */
+#define PERF_MEM_LVL_LFB	0x10  /* Line Fill Buffer */
+#define PERF_MEM_LVL_L2		0x20  /* L2 hit */
+#define PERF_MEM_LVL_L3		0x40  /* L3 hit */
+#define PERF_MEM_LVL_LOC_RAM	0x80  /* Local DRAM */
+#define PERF_MEM_LVL_REM_RAM1	0x100 /* Remote DRAM (1 hop) */
+#define PERF_MEM_LVL_REM_RAM2	0x200 /* Remote DRAM (2 hops) */
+#define PERF_MEM_LVL_REM_CCE1	0x400 /* Remote Cache (1 hop) */
+#define PERF_MEM_LVL_REM_CCE2	0x800 /* Remote Cache (2 hops) */
+#define PERF_MEM_LVL_IO		0x1000 /* I/O memory */
+#define PERF_MEM_LVL_UNC	0x2000 /* Uncached memory */
+#define PERF_MEM_LVL_SHIFT	5
+
+/* snoop mode */
+#define PERF_MEM_SNOOP_NA	0x01 /* not available */
+#define PERF_MEM_SNOOP_NONE	0x02 /* no snoop */
+#define PERF_MEM_SNOOP_HIT	0x04 /* snoop hit */
+#define PERF_MEM_SNOOP_MISS	0x08 /* snoop miss */
+#define PERF_MEM_SNOOP_HITM	0x10 /* snoop hit modified */
+#define PERF_MEM_SNOOP_SHIFT	19
+
+/* locked instruction */
+#define PERF_MEM_LOCK_NA	0x01 /* not available */
+#define PERF_MEM_LOCK_LOCKED	0x02 /* locked transaction */
+#define PERF_MEM_LOCK_SHIFT	24
+
+/* TLB access */
+#define PERF_MEM_TLB_NA		0x01 /* not available */
+#define PERF_MEM_TLB_HIT	0x02 /* hit level */
+#define PERF_MEM_TLB_MISS	0x04 /* miss level */
+#define PERF_MEM_TLB_L1		0x08 /* L1 */
+#define PERF_MEM_TLB_L2		0x10 /* L2 */
+#define PERF_MEM_TLB_WK		0x20 /* Hardware Walker*/
+#define PERF_MEM_TLB_OS		0x40 /* OS fault handler */
+#define PERF_MEM_TLB_SHIFT	26
+
+#define PERF_MEM_S(a, s) \
+	(((u64)PERF_MEM_##a##_##s) << PERF_MEM_##a##_SHIFT)
+
 #endif /* _UAPI_LINUX_PERF_EVENT_H */

commit c3feedf2aaf9ac8bad6f19f5d21e4ee0b4b87e9c
Author: Andi Kleen <ak@linux.intel.com>
Date:   Thu Jan 24 16:10:28 2013 +0100

    perf/core: Add weighted samples
    
    For some events it's useful to weight sample with a hardware
    provided number. This expresses how expensive the action the
    sample represent was.  This allows the profiler to scale
    the samples to be more informative to the programmer.
    
    There is already the period which is used similarly, but it
    means something different, so I chose to not overload it.
    Instead a new sample type for WEIGHT is added.
    
    Can be used for multiple things. Initially it is used for TSX
    abort costs and profiling by memory latencies (so to make
    expensive load appear higher up in the histograms). The concept
    is quite generic and can be extended to many other kinds of
    events or architectures, as long as the hardware provides
    suitable auxillary values. In principle it could be also used
    for software tracepoints.
    
    This adds the generic glue. A new optional sample format for a
    64-bit weight value.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Cc: peterz@infradead.org
    Cc: acme@redhat.com
    Cc: jolsa@redhat.com
    Cc: namhyung.kim@lge.com
    Link: http://lkml.kernel.org/r/1359040242-8269-5-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 9fa9c622a7f4..cdc255da02e2 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -132,8 +132,10 @@ enum perf_event_sample_format {
 	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
 	PERF_SAMPLE_REGS_USER			= 1U << 12,
 	PERF_SAMPLE_STACK_USER			= 1U << 13,
+	PERF_SAMPLE_WEIGHT			= 1U << 14,
+
+	PERF_SAMPLE_MAX = 1U << 15,		/* non-ABI */
 
-	PERF_SAMPLE_MAX = 1U << 14,		/* non-ABI */
 };
 
 /*
@@ -588,6 +590,8 @@ enum perf_event_type {
 	 * 	{ u64			size;
 	 * 	  char			data[size];
 	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
+	 *
+	 *	{ u64			weight;   } && PERF_SAMPLE_WEIGHT
 	 * };
 	 */
 	PERF_RECORD_SAMPLE			= 9,

commit b878e7fb22ea48b0585bbbbef249f7efc6d42748
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Tue Jan 8 14:44:25 2013 -0500

    perf: Missing field in PERF_RECORD_SAMPLE documentation
    
    While trying to write a perf_event/mmap test for my perf_event
    test-suite I came across a missing field description in the
    PERF_RECORD_SAMPLE documentation in perf_event.h
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1301081439300.24507@vincent-weaver-1.um.maine.edu
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 4f63c05d27c9..9fa9c622a7f4 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -579,7 +579,8 @@ enum perf_event_type {
 	 *	{ u32			size;
 	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
 	 *
-	 *	{ u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
+	 *	{ u64                   nr;
+	 *        { u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
 	 *
 	 * 	{ u64			abi; # enum perf_sample_regs_abi
 	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
new file mode 100644
index 000000000000..4f63c05d27c9
--- /dev/null
+++ b/include/uapi/linux/perf_event.h
@@ -0,0 +1,615 @@
+/*
+ * Performance events:
+ *
+ *    Copyright (C) 2008-2009, Thomas Gleixner <tglx@linutronix.de>
+ *    Copyright (C) 2008-2011, Red Hat, Inc., Ingo Molnar
+ *    Copyright (C) 2008-2011, Red Hat, Inc., Peter Zijlstra
+ *
+ * Data type definitions, declarations, prototypes.
+ *
+ *    Started by: Thomas Gleixner and Ingo Molnar
+ *
+ * For licencing details see kernel-base/COPYING
+ */
+#ifndef _UAPI_LINUX_PERF_EVENT_H
+#define _UAPI_LINUX_PERF_EVENT_H
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+#include <asm/byteorder.h>
+
+/*
+ * User-space ABI bits:
+ */
+
+/*
+ * attr.type
+ */
+enum perf_type_id {
+	PERF_TYPE_HARDWARE			= 0,
+	PERF_TYPE_SOFTWARE			= 1,
+	PERF_TYPE_TRACEPOINT			= 2,
+	PERF_TYPE_HW_CACHE			= 3,
+	PERF_TYPE_RAW				= 4,
+	PERF_TYPE_BREAKPOINT			= 5,
+
+	PERF_TYPE_MAX,				/* non-ABI */
+};
+
+/*
+ * Generalized performance event event_id types, used by the
+ * attr.event_id parameter of the sys_perf_event_open()
+ * syscall:
+ */
+enum perf_hw_id {
+	/*
+	 * Common hardware events, generalized by the kernel:
+	 */
+	PERF_COUNT_HW_CPU_CYCLES		= 0,
+	PERF_COUNT_HW_INSTRUCTIONS		= 1,
+	PERF_COUNT_HW_CACHE_REFERENCES		= 2,
+	PERF_COUNT_HW_CACHE_MISSES		= 3,
+	PERF_COUNT_HW_BRANCH_INSTRUCTIONS	= 4,
+	PERF_COUNT_HW_BRANCH_MISSES		= 5,
+	PERF_COUNT_HW_BUS_CYCLES		= 6,
+	PERF_COUNT_HW_STALLED_CYCLES_FRONTEND	= 7,
+	PERF_COUNT_HW_STALLED_CYCLES_BACKEND	= 8,
+	PERF_COUNT_HW_REF_CPU_CYCLES		= 9,
+
+	PERF_COUNT_HW_MAX,			/* non-ABI */
+};
+
+/*
+ * Generalized hardware cache events:
+ *
+ *       { L1-D, L1-I, LLC, ITLB, DTLB, BPU, NODE } x
+ *       { read, write, prefetch } x
+ *       { accesses, misses }
+ */
+enum perf_hw_cache_id {
+	PERF_COUNT_HW_CACHE_L1D			= 0,
+	PERF_COUNT_HW_CACHE_L1I			= 1,
+	PERF_COUNT_HW_CACHE_LL			= 2,
+	PERF_COUNT_HW_CACHE_DTLB		= 3,
+	PERF_COUNT_HW_CACHE_ITLB		= 4,
+	PERF_COUNT_HW_CACHE_BPU			= 5,
+	PERF_COUNT_HW_CACHE_NODE		= 6,
+
+	PERF_COUNT_HW_CACHE_MAX,		/* non-ABI */
+};
+
+enum perf_hw_cache_op_id {
+	PERF_COUNT_HW_CACHE_OP_READ		= 0,
+	PERF_COUNT_HW_CACHE_OP_WRITE		= 1,
+	PERF_COUNT_HW_CACHE_OP_PREFETCH		= 2,
+
+	PERF_COUNT_HW_CACHE_OP_MAX,		/* non-ABI */
+};
+
+enum perf_hw_cache_op_result_id {
+	PERF_COUNT_HW_CACHE_RESULT_ACCESS	= 0,
+	PERF_COUNT_HW_CACHE_RESULT_MISS		= 1,
+
+	PERF_COUNT_HW_CACHE_RESULT_MAX,		/* non-ABI */
+};
+
+/*
+ * Special "software" events provided by the kernel, even if the hardware
+ * does not support performance events. These events measure various
+ * physical and sw events of the kernel (and allow the profiling of them as
+ * well):
+ */
+enum perf_sw_ids {
+	PERF_COUNT_SW_CPU_CLOCK			= 0,
+	PERF_COUNT_SW_TASK_CLOCK		= 1,
+	PERF_COUNT_SW_PAGE_FAULTS		= 2,
+	PERF_COUNT_SW_CONTEXT_SWITCHES		= 3,
+	PERF_COUNT_SW_CPU_MIGRATIONS		= 4,
+	PERF_COUNT_SW_PAGE_FAULTS_MIN		= 5,
+	PERF_COUNT_SW_PAGE_FAULTS_MAJ		= 6,
+	PERF_COUNT_SW_ALIGNMENT_FAULTS		= 7,
+	PERF_COUNT_SW_EMULATION_FAULTS		= 8,
+
+	PERF_COUNT_SW_MAX,			/* non-ABI */
+};
+
+/*
+ * Bits that can be set in attr.sample_type to request information
+ * in the overflow packets.
+ */
+enum perf_event_sample_format {
+	PERF_SAMPLE_IP				= 1U << 0,
+	PERF_SAMPLE_TID				= 1U << 1,
+	PERF_SAMPLE_TIME			= 1U << 2,
+	PERF_SAMPLE_ADDR			= 1U << 3,
+	PERF_SAMPLE_READ			= 1U << 4,
+	PERF_SAMPLE_CALLCHAIN			= 1U << 5,
+	PERF_SAMPLE_ID				= 1U << 6,
+	PERF_SAMPLE_CPU				= 1U << 7,
+	PERF_SAMPLE_PERIOD			= 1U << 8,
+	PERF_SAMPLE_STREAM_ID			= 1U << 9,
+	PERF_SAMPLE_RAW				= 1U << 10,
+	PERF_SAMPLE_BRANCH_STACK		= 1U << 11,
+	PERF_SAMPLE_REGS_USER			= 1U << 12,
+	PERF_SAMPLE_STACK_USER			= 1U << 13,
+
+	PERF_SAMPLE_MAX = 1U << 14,		/* non-ABI */
+};
+
+/*
+ * values to program into branch_sample_type when PERF_SAMPLE_BRANCH is set
+ *
+ * If the user does not pass priv level information via branch_sample_type,
+ * the kernel uses the event's priv level. Branch and event priv levels do
+ * not have to match. Branch priv level is checked for permissions.
+ *
+ * The branch types can be combined, however BRANCH_ANY covers all types
+ * of branches and therefore it supersedes all the other types.
+ */
+enum perf_branch_sample_type {
+	PERF_SAMPLE_BRANCH_USER		= 1U << 0, /* user branches */
+	PERF_SAMPLE_BRANCH_KERNEL	= 1U << 1, /* kernel branches */
+	PERF_SAMPLE_BRANCH_HV		= 1U << 2, /* hypervisor branches */
+
+	PERF_SAMPLE_BRANCH_ANY		= 1U << 3, /* any branch types */
+	PERF_SAMPLE_BRANCH_ANY_CALL	= 1U << 4, /* any call branch */
+	PERF_SAMPLE_BRANCH_ANY_RETURN	= 1U << 5, /* any return branch */
+	PERF_SAMPLE_BRANCH_IND_CALL	= 1U << 6, /* indirect calls */
+
+	PERF_SAMPLE_BRANCH_MAX		= 1U << 7, /* non-ABI */
+};
+
+#define PERF_SAMPLE_BRANCH_PLM_ALL \
+	(PERF_SAMPLE_BRANCH_USER|\
+	 PERF_SAMPLE_BRANCH_KERNEL|\
+	 PERF_SAMPLE_BRANCH_HV)
+
+/*
+ * Values to determine ABI of the registers dump.
+ */
+enum perf_sample_regs_abi {
+	PERF_SAMPLE_REGS_ABI_NONE	= 0,
+	PERF_SAMPLE_REGS_ABI_32		= 1,
+	PERF_SAMPLE_REGS_ABI_64		= 2,
+};
+
+/*
+ * The format of the data returned by read() on a perf event fd,
+ * as specified by attr.read_format:
+ *
+ * struct read_format {
+ *	{ u64		value;
+ *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
+ *	  { u64		id;           } && PERF_FORMAT_ID
+ *	} && !PERF_FORMAT_GROUP
+ *
+ *	{ u64		nr;
+ *	  { u64		time_enabled; } && PERF_FORMAT_TOTAL_TIME_ENABLED
+ *	  { u64		time_running; } && PERF_FORMAT_TOTAL_TIME_RUNNING
+ *	  { u64		value;
+ *	    { u64	id;           } && PERF_FORMAT_ID
+ *	  }		cntr[nr];
+ *	} && PERF_FORMAT_GROUP
+ * };
+ */
+enum perf_event_read_format {
+	PERF_FORMAT_TOTAL_TIME_ENABLED		= 1U << 0,
+	PERF_FORMAT_TOTAL_TIME_RUNNING		= 1U << 1,
+	PERF_FORMAT_ID				= 1U << 2,
+	PERF_FORMAT_GROUP			= 1U << 3,
+
+	PERF_FORMAT_MAX = 1U << 4,		/* non-ABI */
+};
+
+#define PERF_ATTR_SIZE_VER0	64	/* sizeof first published struct */
+#define PERF_ATTR_SIZE_VER1	72	/* add: config2 */
+#define PERF_ATTR_SIZE_VER2	80	/* add: branch_sample_type */
+#define PERF_ATTR_SIZE_VER3	96	/* add: sample_regs_user */
+					/* add: sample_stack_user */
+
+/*
+ * Hardware event_id to monitor via a performance monitoring event:
+ */
+struct perf_event_attr {
+
+	/*
+	 * Major type: hardware/software/tracepoint/etc.
+	 */
+	__u32			type;
+
+	/*
+	 * Size of the attr structure, for fwd/bwd compat.
+	 */
+	__u32			size;
+
+	/*
+	 * Type specific configuration information.
+	 */
+	__u64			config;
+
+	union {
+		__u64		sample_period;
+		__u64		sample_freq;
+	};
+
+	__u64			sample_type;
+	__u64			read_format;
+
+	__u64			disabled       :  1, /* off by default        */
+				inherit	       :  1, /* children inherit it   */
+				pinned	       :  1, /* must always be on PMU */
+				exclusive      :  1, /* only group on PMU     */
+				exclude_user   :  1, /* don't count user      */
+				exclude_kernel :  1, /* ditto kernel          */
+				exclude_hv     :  1, /* ditto hypervisor      */
+				exclude_idle   :  1, /* don't count when idle */
+				mmap           :  1, /* include mmap data     */
+				comm	       :  1, /* include comm data     */
+				freq           :  1, /* use freq, not period  */
+				inherit_stat   :  1, /* per task counts       */
+				enable_on_exec :  1, /* next exec enables     */
+				task           :  1, /* trace fork/exit       */
+				watermark      :  1, /* wakeup_watermark      */
+				/*
+				 * precise_ip:
+				 *
+				 *  0 - SAMPLE_IP can have arbitrary skid
+				 *  1 - SAMPLE_IP must have constant skid
+				 *  2 - SAMPLE_IP requested to have 0 skid
+				 *  3 - SAMPLE_IP must have 0 skid
+				 *
+				 *  See also PERF_RECORD_MISC_EXACT_IP
+				 */
+				precise_ip     :  2, /* skid constraint       */
+				mmap_data      :  1, /* non-exec mmap data    */
+				sample_id_all  :  1, /* sample_type all events */
+
+				exclude_host   :  1, /* don't count in host   */
+				exclude_guest  :  1, /* don't count in guest  */
+
+				exclude_callchain_kernel : 1, /* exclude kernel callchains */
+				exclude_callchain_user   : 1, /* exclude user callchains */
+
+				__reserved_1   : 41;
+
+	union {
+		__u32		wakeup_events;	  /* wakeup every n events */
+		__u32		wakeup_watermark; /* bytes before wakeup   */
+	};
+
+	__u32			bp_type;
+	union {
+		__u64		bp_addr;
+		__u64		config1; /* extension of config */
+	};
+	union {
+		__u64		bp_len;
+		__u64		config2; /* extension of config1 */
+	};
+	__u64	branch_sample_type; /* enum perf_branch_sample_type */
+
+	/*
+	 * Defines set of user regs to dump on samples.
+	 * See asm/perf_regs.h for details.
+	 */
+	__u64	sample_regs_user;
+
+	/*
+	 * Defines size of the user stack to dump on samples.
+	 */
+	__u32	sample_stack_user;
+
+	/* Align to u64. */
+	__u32	__reserved_2;
+};
+
+#define perf_flags(attr)	(*(&(attr)->read_format + 1))
+
+/*
+ * Ioctls that can be done on a perf event fd:
+ */
+#define PERF_EVENT_IOC_ENABLE		_IO ('$', 0)
+#define PERF_EVENT_IOC_DISABLE		_IO ('$', 1)
+#define PERF_EVENT_IOC_REFRESH		_IO ('$', 2)
+#define PERF_EVENT_IOC_RESET		_IO ('$', 3)
+#define PERF_EVENT_IOC_PERIOD		_IOW('$', 4, __u64)
+#define PERF_EVENT_IOC_SET_OUTPUT	_IO ('$', 5)
+#define PERF_EVENT_IOC_SET_FILTER	_IOW('$', 6, char *)
+
+enum perf_event_ioc_flags {
+	PERF_IOC_FLAG_GROUP		= 1U << 0,
+};
+
+/*
+ * Structure of the page that can be mapped via mmap
+ */
+struct perf_event_mmap_page {
+	__u32	version;		/* version number of this structure */
+	__u32	compat_version;		/* lowest version this is compat with */
+
+	/*
+	 * Bits needed to read the hw events in user-space.
+	 *
+	 *   u32 seq, time_mult, time_shift, idx, width;
+	 *   u64 count, enabled, running;
+	 *   u64 cyc, time_offset;
+	 *   s64 pmc = 0;
+	 *
+	 *   do {
+	 *     seq = pc->lock;
+	 *     barrier()
+	 *
+	 *     enabled = pc->time_enabled;
+	 *     running = pc->time_running;
+	 *
+	 *     if (pc->cap_usr_time && enabled != running) {
+	 *       cyc = rdtsc();
+	 *       time_offset = pc->time_offset;
+	 *       time_mult   = pc->time_mult;
+	 *       time_shift  = pc->time_shift;
+	 *     }
+	 *
+	 *     idx = pc->index;
+	 *     count = pc->offset;
+	 *     if (pc->cap_usr_rdpmc && idx) {
+	 *       width = pc->pmc_width;
+	 *       pmc = rdpmc(idx - 1);
+	 *     }
+	 *
+	 *     barrier();
+	 *   } while (pc->lock != seq);
+	 *
+	 * NOTE: for obvious reason this only works on self-monitoring
+	 *       processes.
+	 */
+	__u32	lock;			/* seqlock for synchronization */
+	__u32	index;			/* hardware event identifier */
+	__s64	offset;			/* add to hardware event value */
+	__u64	time_enabled;		/* time event active */
+	__u64	time_running;		/* time event on cpu */
+	union {
+		__u64	capabilities;
+		__u64	cap_usr_time  : 1,
+			cap_usr_rdpmc : 1,
+			cap_____res   : 62;
+	};
+
+	/*
+	 * If cap_usr_rdpmc this field provides the bit-width of the value
+	 * read using the rdpmc() or equivalent instruction. This can be used
+	 * to sign extend the result like:
+	 *
+	 *   pmc <<= 64 - width;
+	 *   pmc >>= 64 - width; // signed shift right
+	 *   count += pmc;
+	 */
+	__u16	pmc_width;
+
+	/*
+	 * If cap_usr_time the below fields can be used to compute the time
+	 * delta since time_enabled (in ns) using rdtsc or similar.
+	 *
+	 *   u64 quot, rem;
+	 *   u64 delta;
+	 *
+	 *   quot = (cyc >> time_shift);
+	 *   rem = cyc & ((1 << time_shift) - 1);
+	 *   delta = time_offset + quot * time_mult +
+	 *              ((rem * time_mult) >> time_shift);
+	 *
+	 * Where time_offset,time_mult,time_shift and cyc are read in the
+	 * seqcount loop described above. This delta can then be added to
+	 * enabled and possible running (if idx), improving the scaling:
+	 *
+	 *   enabled += delta;
+	 *   if (idx)
+	 *     running += delta;
+	 *
+	 *   quot = count / running;
+	 *   rem  = count % running;
+	 *   count = quot * enabled + (rem * enabled) / running;
+	 */
+	__u16	time_shift;
+	__u32	time_mult;
+	__u64	time_offset;
+
+		/*
+		 * Hole for extension of the self monitor capabilities
+		 */
+
+	__u64	__reserved[120];	/* align to 1k */
+
+	/*
+	 * Control data for the mmap() data buffer.
+	 *
+	 * User-space reading the @data_head value should issue an rmb(), on
+	 * SMP capable platforms, after reading this value -- see
+	 * perf_event_wakeup().
+	 *
+	 * When the mapping is PROT_WRITE the @data_tail value should be
+	 * written by userspace to reflect the last read data. In this case
+	 * the kernel will not over-write unread data.
+	 */
+	__u64   data_head;		/* head in the data section */
+	__u64	data_tail;		/* user-space written tail */
+};
+
+#define PERF_RECORD_MISC_CPUMODE_MASK		(7 << 0)
+#define PERF_RECORD_MISC_CPUMODE_UNKNOWN	(0 << 0)
+#define PERF_RECORD_MISC_KERNEL			(1 << 0)
+#define PERF_RECORD_MISC_USER			(2 << 0)
+#define PERF_RECORD_MISC_HYPERVISOR		(3 << 0)
+#define PERF_RECORD_MISC_GUEST_KERNEL		(4 << 0)
+#define PERF_RECORD_MISC_GUEST_USER		(5 << 0)
+
+/*
+ * Indicates that the content of PERF_SAMPLE_IP points to
+ * the actual instruction that triggered the event. See also
+ * perf_event_attr::precise_ip.
+ */
+#define PERF_RECORD_MISC_EXACT_IP		(1 << 14)
+/*
+ * Reserve the last bit to indicate some extended misc field
+ */
+#define PERF_RECORD_MISC_EXT_RESERVED		(1 << 15)
+
+struct perf_event_header {
+	__u32	type;
+	__u16	misc;
+	__u16	size;
+};
+
+enum perf_event_type {
+
+	/*
+	 * If perf_event_attr.sample_id_all is set then all event types will
+	 * have the sample_type selected fields related to where/when
+	 * (identity) an event took place (TID, TIME, ID, CPU, STREAM_ID)
+	 * described in PERF_RECORD_SAMPLE below, it will be stashed just after
+	 * the perf_event_header and the fields already present for the existing
+	 * fields, i.e. at the end of the payload. That way a newer perf.data
+	 * file will be supported by older perf tools, with these new optional
+	 * fields being ignored.
+	 *
+	 * The MMAP events record the PROT_EXEC mappings so that we can
+	 * correlate userspace IPs to code. They have the following structure:
+	 *
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u32				pid, tid;
+	 *	u64				addr;
+	 *	u64				len;
+	 *	u64				pgoff;
+	 *	char				filename[];
+	 * };
+	 */
+	PERF_RECORD_MMAP			= 1,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u64				id;
+	 *	u64				lost;
+	 * };
+	 */
+	PERF_RECORD_LOST			= 2,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	u32				pid, tid;
+	 *	char				comm[];
+	 * };
+	 */
+	PERF_RECORD_COMM			= 3,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid, ppid;
+	 *	u32				tid, ptid;
+	 *	u64				time;
+	 * };
+	 */
+	PERF_RECORD_EXIT			= 4,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u64				time;
+	 *	u64				id;
+	 *	u64				stream_id;
+	 * };
+	 */
+	PERF_RECORD_THROTTLE			= 5,
+	PERF_RECORD_UNTHROTTLE			= 6,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid, ppid;
+	 *	u32				tid, ptid;
+	 *	u64				time;
+	 * };
+	 */
+	PERF_RECORD_FORK			= 7,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *	u32				pid, tid;
+	 *
+	 *	struct read_format		values;
+	 * };
+	 */
+	PERF_RECORD_READ			= 8,
+
+	/*
+	 * struct {
+	 *	struct perf_event_header	header;
+	 *
+	 *	{ u64			ip;	  } && PERF_SAMPLE_IP
+	 *	{ u32			pid, tid; } && PERF_SAMPLE_TID
+	 *	{ u64			time;     } && PERF_SAMPLE_TIME
+	 *	{ u64			addr;     } && PERF_SAMPLE_ADDR
+	 *	{ u64			id;	  } && PERF_SAMPLE_ID
+	 *	{ u64			stream_id;} && PERF_SAMPLE_STREAM_ID
+	 *	{ u32			cpu, res; } && PERF_SAMPLE_CPU
+	 *	{ u64			period;   } && PERF_SAMPLE_PERIOD
+	 *
+	 *	{ struct read_format	values;	  } && PERF_SAMPLE_READ
+	 *
+	 *	{ u64			nr,
+	 *	  u64			ips[nr];  } && PERF_SAMPLE_CALLCHAIN
+	 *
+	 *	#
+	 *	# The RAW record below is opaque data wrt the ABI
+	 *	#
+	 *	# That is, the ABI doesn't make any promises wrt to
+	 *	# the stability of its content, it may vary depending
+	 *	# on event, hardware, kernel version and phase of
+	 *	# the moon.
+	 *	#
+	 *	# In other words, PERF_SAMPLE_RAW contents are not an ABI.
+	 *	#
+	 *
+	 *	{ u32			size;
+	 *	  char                  data[size];}&& PERF_SAMPLE_RAW
+	 *
+	 *	{ u64 from, to, flags } lbr[nr];} && PERF_SAMPLE_BRANCH_STACK
+	 *
+	 * 	{ u64			abi; # enum perf_sample_regs_abi
+	 * 	  u64			regs[weight(mask)]; } && PERF_SAMPLE_REGS_USER
+	 *
+	 * 	{ u64			size;
+	 * 	  char			data[size];
+	 * 	  u64			dyn_size; } && PERF_SAMPLE_STACK_USER
+	 * };
+	 */
+	PERF_RECORD_SAMPLE			= 9,
+
+	PERF_RECORD_MAX,			/* non-ABI */
+};
+
+#define PERF_MAX_STACK_DEPTH		127
+
+enum perf_callchain_context {
+	PERF_CONTEXT_HV			= (__u64)-32,
+	PERF_CONTEXT_KERNEL		= (__u64)-128,
+	PERF_CONTEXT_USER		= (__u64)-512,
+
+	PERF_CONTEXT_GUEST		= (__u64)-2048,
+	PERF_CONTEXT_GUEST_KERNEL	= (__u64)-2176,
+	PERF_CONTEXT_GUEST_USER		= (__u64)-2560,
+
+	PERF_CONTEXT_MAX		= (__u64)-4095,
+};
+
+#define PERF_FLAG_FD_NO_GROUP		(1U << 0)
+#define PERF_FLAG_FD_OUTPUT		(1U << 1)
+#define PERF_FLAG_PID_CGROUP		(1U << 2) /* pid=cgroup id, per-cpu mode only */
+
+#endif /* _UAPI_LINUX_PERF_EVENT_H */
