commit 39d010504e6b4485d7ceee167743620dd33f4417
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 1 07:07:41 2020 -0700

    net_sched: sch_fq: add horizon attribute
    
    QUIC servers would like to use SO_TXTIME, without having CAP_NET_ADMIN,
    to efficiently pace UDP packets.
    
    As far as sch_fq is concerned, we need to add safety checks, so
    that a buggy application does not fill the qdisc with packets
    having delivery time far in the future.
    
    This patch adds a configurable horizon (default: 10 seconds),
    and a configurable policy when a packet is beyond the horizon
    at enqueue() time:
    - either drop the packet (default policy)
    - or cap its delivery time to the horizon.
    
    $ tc -s -d qd sh dev eth0
    qdisc fq 8022: root refcnt 257 limit 10000p flow_limit 100p buckets 1024
     orphan_mask 1023 quantum 10Kb initial_quantum 51160b low_rate_threshold 550Kbit
     refill_delay 40.0ms timer_slack 10.000us horizon 10.000s
     Sent 1234215879 bytes 837099 pkt (dropped 21, overlimits 0 requeues 6)
     backlog 0b 0p requeues 6
      flows 1191 (inactive 1177 throttled 0)
      gc 0 highprio 0 throttled 692 latency 11.480us
      pkts_too_long 0 alloc_errors 0 horizon_drops 21 horizon_caps 0
    
    v2: fixed an overflow on 32bit kernels in fq_init(), reported
        by kbuild test robot <lkp@intel.com>
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 0c02737c8f47..a95f3ae7ab37 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -913,6 +913,10 @@ enum {
 
 	TCA_FQ_TIMER_SLACK,	/* timer slack */
 
+	TCA_FQ_HORIZON,		/* time horizon in us */
+
+	TCA_FQ_HORIZON_DROP,	/* drop packets beyond horizon, or cap their EDT */
+
 	__TCA_FQ_MAX
 };
 
@@ -932,6 +936,8 @@ struct tc_fq_qd_stats {
 	__u32	throttled_flows;
 	__u32	unthrottle_latency_ns;
 	__u64	ce_mark;		/* packets above ce_threshold */
+	__u64	horizon_drops;
+	__u64	horizon_caps;
 };
 
 /* Heavy-Hitter Filter */

commit 673040c3a82a7564423e09c791e242a846591e30
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Tue Mar 24 05:19:20 2020 +0100

    taprio: do not use BIT() in TCA_TAPRIO_ATTR_FLAG_* definitions
    
    BIT() macro definition is internal to the Linux kernel and is not
    to be used in UAPI headers; replace its usage with the _BITUL() macro
    that is already used elsewhere in the header.
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Acked-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 7307a29a103e..0c02737c8f47 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1216,8 +1216,8 @@ enum {
  *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_INTERVAL]
  */
 
-#define TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST	BIT(0)
-#define TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD	BIT(1)
+#define TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST	_BITUL(0)
+#define TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD	_BITUL(1)
 
 enum {
 	TCA_TAPRIO_ATTR_UNSPEC,

commit 583396f4ca4d6ee5ad314ba0f4cb5b89deb75e76
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Mar 16 19:12:51 2020 -0700

    net_sched: sch_fq: enable use of hrtimer slack
    
    Add a new attribute to control the fq qdisc hrtimer slack.
    
    Default is set to 10 usec.
    
    When/if packets are throttled, fq set up an hrtimer that can
    lead to one interrupt per packet in the throttled queue.
    
    By using a timer slack, we allow better use of timer interrupts,
    by giving them a chance to call multiple timer callbacks
    at each hardware interrupt.
    
    Also, giving a slack allows FQ to dequeue batches of packets
    instead of a single one, thus increasing xmit_more efficiency.
    
    This has no negative effect on the rate a TCP flow can sustain,
    since each TCP flow maintains its own precise vtime (tp->tcp_wstamp_ns)
    
    v2: added strict netlink checking (as feedback from Jakub Kicinski)
    
    Tested:
     1000 concurrent flows all using paced packets.
     1,000,000 packets sent per second.
    
    Before the patch :
    
    $ vmstat 2 10
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     0  0      0 60726784  23628 3485992    0    0   138     1  977  535  0 12 87  0  0
     0  0      0 60714700  23628 3485628    0    0     0     0 1568827 26462  0 22 78  0  0
     1  0      0 60716012  23628 3485656    0    0     0     0 1570034 26216  0 22 78  0  0
     0  0      0 60722420  23628 3485492    0    0     0     0 1567230 26424  0 22 78  0  0
     0  0      0 60727484  23628 3485556    0    0     0     0 1568220 26200  0 22 78  0  0
     2  0      0 60718900  23628 3485380    0    0     0    40 1564721 26630  0 22 78  0  0
     2  0      0 60718096  23628 3485332    0    0     0     0 1562593 26432  0 22 78  0  0
     0  0      0 60719608  23628 3485064    0    0     0     0 1563806 26238  0 22 78  0  0
     1  0      0 60722876  23628 3485236    0    0     0   130 1565874 26566  0 22 78  0  0
     1  0      0 60722752  23628 3484908    0    0     0     0 1567646 26247  0 22 78  0  0
    
    After the patch, slack of 10 usec, we can see a reduction of interrupts
    per second, and a small decrease of reported cpu usage.
    
    $ vmstat 2 10
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     1  0      0 60722564  23628 3484728    0    0   133     1  696  545  0 13 87  0  0
     1  0      0 60722568  23628 3484824    0    0     0     0 977278 25469  0 20 80  0  0
     0  0      0 60716396  23628 3484764    0    0     0     0 979997 25326  0 20 80  0  0
     0  0      0 60713844  23628 3484960    0    0     0     0 981394 25249  0 20 80  0  0
     2  0      0 60720468  23628 3484916    0    0     0     0 982860 25062  0 20 80  0  0
     1  0      0 60721236  23628 3484856    0    0     0     0 982867 25100  0 20 80  0  0
     1  0      0 60722400  23628 3484456    0    0     0     8 982698 25303  0 20 80  0  0
     0  0      0 60715396  23628 3484428    0    0     0     0 981777 25176  0 20 80  0  0
     0  0      0 60716520  23628 3486544    0    0     0    36 978965 27857  0 21 79  0  0
     0  0      0 60719592  23628 3486516    0    0     0    22 977318 25106  0 20 80  0  0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index ea39287d59c8..7307a29a103e 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -911,6 +911,8 @@ enum {
 
 	TCA_FQ_CE_THRESHOLD,	/* DCTCP-like CE-marking threshold */
 
+	TCA_FQ_TIMER_SLACK,	/* timer slack */
+
 	__TCA_FQ_MAX
 };
 

commit 0a7fad2376ba6b37c6b1a1072ed2a2381d82cd18
Author: Petr Machata <petrm@mellanox.com>
Date:   Fri Mar 13 01:10:57 2020 +0200

    net: sched: RED: Introduce an ECN nodrop mode
    
    When the RED Qdisc is currently configured to enable ECN, the RED algorithm
    is used to decide whether a certain SKB should be marked. If that SKB is
    not ECN-capable, it is early-dropped.
    
    It is also possible to keep all traffic in the queue, and just mark the
    ECN-capable subset of it, as appropriate under the RED algorithm. Some
    switches support this mode, and some installations make use of it.
    
    To that end, add a new RED flag, TC_RED_NODROP. When the Qdisc is
    configured with this flag, non-ECT traffic is enqueued instead of being
    early-dropped.
    
    Signed-off-by: Petr Machata <petrm@mellanox.com>
    Reviewed-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 6325507935ea..ea39287d59c8 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -286,6 +286,7 @@ struct tc_red_qopt {
 #define TC_RED_ECN		1
 #define TC_RED_HARDDROP		2
 #define TC_RED_ADAPTATIVE	4
+#define TC_RED_NODROP		8
 };
 
 #define TC_RED_HISTORIC_FLAGS (TC_RED_ECN | TC_RED_HARDDROP | TC_RED_ADAPTATIVE)

commit 14bc175d9c885c86239de3d730eea85ad67bfe7b
Author: Petr Machata <petrm@mellanox.com>
Date:   Fri Mar 13 01:10:56 2020 +0200

    net: sched: Allow extending set of supported RED flags
    
    The qdiscs RED, GRED, SFQ and CHOKE use different subsets of the same pool
    of global RED flags. These are passed in tc_red_qopt.flags. However none of
    these qdiscs validate the flag field, and just copy it over wholesale to
    internal structures, and later dump it back. (An exception is GRED, which
    does validate for VQs -- however not for the main setup.)
    
    A broken userspace can therefore configure a qdisc with arbitrary
    unsupported flags, and later expect to see the flags on qdisc dump. The
    current ABI therefore allows storage of several bits of custom data to
    qdisc instances of the types mentioned above. How many bits, depends on
    which flags are meaningful for the qdisc in question. E.g. SFQ recognizes
    flags ECN and HARDDROP, and the rest is not interpreted.
    
    If SFQ ever needs to support ADAPTATIVE, it needs another way of doing it,
    and at the same time it needs to retain the possibility to store 6 bits of
    uninterpreted data. Likewise RED, which adds a new flag later in this
    patchset.
    
    To that end, this patch adds a new function, red_get_flags(), to split the
    passed flags of RED-like qdiscs to flags and user bits, and
    red_validate_flags() to validate the resulting configuration. It further
    adds a new attribute, TCA_RED_FLAGS, to pass arbitrary flags.
    
    Signed-off-by: Petr Machata <petrm@mellanox.com>
    Reviewed-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index bbe791b24168..6325507935ea 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -256,6 +256,7 @@ enum {
 	TCA_RED_PARMS,
 	TCA_RED_STAB,
 	TCA_RED_MAX_P,
+	TCA_RED_FLAGS,		/* bitfield32 */
 	__TCA_RED_MAX,
 };
 
@@ -268,12 +269,27 @@ struct tc_red_qopt {
 	unsigned char   Wlog;		/* log(W)		*/
 	unsigned char   Plog;		/* log(P_max/(qth_max-qth_min))	*/
 	unsigned char   Scell_log;	/* cell size for idle damping */
+
+	/* This field can be used for flags that a RED-like qdisc has
+	 * historically supported. E.g. when configuring RED, it can be used for
+	 * ECN, HARDDROP and ADAPTATIVE. For SFQ it can be used for ECN,
+	 * HARDDROP. Etc. Because this field has not been validated, and is
+	 * copied back on dump, any bits besides those to which a given qdisc
+	 * has assigned a historical meaning need to be considered for free use
+	 * by userspace tools.
+	 *
+	 * Any further flags need to be passed differently, e.g. through an
+	 * attribute (such as TCA_RED_FLAGS above). Such attribute should allow
+	 * passing both recent and historic flags in one value.
+	 */
 	unsigned char	flags;
 #define TC_RED_ECN		1
 #define TC_RED_HARDDROP		2
 #define TC_RED_ADAPTATIVE	4
 };
 
+#define TC_RED_HISTORIC_FLAGS (TC_RED_ECN | TC_RED_HARDDROP | TC_RED_ADAPTATIVE)
+
 struct tc_red_xstats {
 	__u32           early;          /* Early drops */
 	__u32           pdrop;          /* Drops due to queue limits */

commit ec97ecf1ebe485a17cd8395a5f35e6b80b57665a
Author: Mohit P. Tahiliani <tahiliani@nitk.edu.in>
Date:   Wed Jan 22 23:52:33 2020 +0530

    net: sched: add Flow Queue PIE packet scheduler
    
    Principles:
      - Packets are classified on flows.
      - This is a Stochastic model (as we use a hash, several flows might
                                    be hashed to the same slot)
      - Each flow has a PIE managed queue.
      - Flows are linked onto two (Round Robin) lists,
        so that new flows have priority on old ones.
      - For a given flow, packets are not reordered.
      - Drops during enqueue only.
      - ECN capability is off by default.
      - ECN threshold (if ECN is enabled) is at 10% by default.
      - Uses timestamps to calculate queue delay by default.
    
    Usage:
    tc qdisc ... fq_pie [ limit PACKETS ] [ flows NUMBER ]
                        [ target TIME ] [ tupdate TIME ]
                        [ alpha NUMBER ] [ beta NUMBER ]
                        [ quantum BYTES ] [ memory_limit BYTES ]
                        [ ecnprob PERCENTAGE ] [ [no]ecn ]
                        [ [no]bytemode ] [ [no_]dq_rate_estimator ]
    
    defaults:
      limit: 10240 packets, flows: 1024
      target: 15 ms, tupdate: 15 ms (in jiffies)
      alpha: 1/8, beta : 5/4
      quantum: device MTU, memory_limit: 32 Mb
      ecnprob: 10%, ecn: off
      bytemode: off, dq_rate_estimator: off
    
    Signed-off-by: Mohit P. Tahiliani <tahiliani@nitk.edu.in>
    Signed-off-by: Sachin D. Patil <sdp.sachin@gmail.com>
    Signed-off-by: V. Saicharan <vsaicharan1998@gmail.com>
    Signed-off-by: Mohit Bhasi <mohitbhasi1998@gmail.com>
    Signed-off-by: Leslie Monis <lesliemonis@gmail.com>
    Signed-off-by: Gautam Ramakrishnan <gautamramk@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index bf5a5b1dfb0b..bbe791b24168 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -971,6 +971,37 @@ struct tc_pie_xstats {
 	__u32 ecn_mark;			/* packets marked with ecn*/
 };
 
+/* FQ PIE */
+enum {
+	TCA_FQ_PIE_UNSPEC,
+	TCA_FQ_PIE_LIMIT,
+	TCA_FQ_PIE_FLOWS,
+	TCA_FQ_PIE_TARGET,
+	TCA_FQ_PIE_TUPDATE,
+	TCA_FQ_PIE_ALPHA,
+	TCA_FQ_PIE_BETA,
+	TCA_FQ_PIE_QUANTUM,
+	TCA_FQ_PIE_MEMORY_LIMIT,
+	TCA_FQ_PIE_ECN_PROB,
+	TCA_FQ_PIE_ECN,
+	TCA_FQ_PIE_BYTEMODE,
+	TCA_FQ_PIE_DQ_RATE_ESTIMATOR,
+	__TCA_FQ_PIE_MAX
+};
+#define TCA_FQ_PIE_MAX   (__TCA_FQ_PIE_MAX - 1)
+
+struct tc_fq_pie_xstats {
+	__u32 packets_in;	/* total number of packets enqueued */
+	__u32 dropped;		/* packets dropped due to fq_pie_action */
+	__u32 overlimit;	/* dropped due to lack of space in queue */
+	__u32 overmemory;	/* dropped due to lack of memory in queue */
+	__u32 ecn_mark;		/* packets marked with ecn */
+	__u32 new_flow_count;	/* count of new flows created by packets */
+	__u32 new_flows_len;	/* count of flows in new list */
+	__u32 old_flows_len;	/* count of flows in old list */
+	__u32 memory_usage;	/* total memory across all queues */
+};
+
 /* CBS */
 struct tc_cbs_qopt {
 	__u8 offload;

commit dcc68b4d8084e1ac9af0d4022d6b1aff6a139a33
Author: Petr Machata <petrm@mellanox.com>
Date:   Wed Dec 18 14:55:13 2019 +0000

    net: sch_ets: Add a new Qdisc
    
    Introduces a new Qdisc, which is based on 802.1Q-2014 wording. It is
    PRIO-like in how it is configured, meaning one needs to specify how many
    bands there are, how many are strict and how many are dwrr, quanta for the
    latter, and priomap.
    
    The new Qdisc operates like the PRIO / DRR combo would when configured as
    per the standard. The strict classes, if any, are tried for traffic first.
    When there's no traffic in any of the strict queues, the ETS ones (if any)
    are treated in the same way as in DRR.
    
    Signed-off-by: Petr Machata <petrm@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 9f1a72876212..bf5a5b1dfb0b 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1187,4 +1187,21 @@ enum {
 
 #define TCA_TAPRIO_ATTR_MAX (__TCA_TAPRIO_ATTR_MAX - 1)
 
+/* ETS */
+
+#define TCQ_ETS_MAX_BANDS 16
+
+enum {
+	TCA_ETS_UNSPEC,
+	TCA_ETS_NBANDS,		/* u8 */
+	TCA_ETS_NSTRICT,	/* u8 */
+	TCA_ETS_QUANTA,		/* nested TCA_ETS_QUANTA_BAND */
+	TCA_ETS_QUANTA_BAND,	/* u32 */
+	TCA_ETS_PRIOMAP,	/* nested TCA_ETS_PRIOMAP_BAND */
+	TCA_ETS_PRIOMAP_BAND,	/* u8 */
+	__TCA_ETS_MAX,
+};
+
+#define TCA_ETS_MAX (__TCA_ETS_MAX - 1)
+
 #endif

commit cec2975f2b7058c42330a0f8164d94c6b7c8c446
Author: Gautam Ramakrishnan <gautamramk@gmail.com>
Date:   Wed Nov 20 19:43:54 2019 +0530

    net: sched: pie: enable timestamp based delay calculation
    
    RFC 8033 suggests an alternative approach to calculate the queue
    delay in PIE by using a timestamp on every enqueued packet. This
    patch adds an implementation of that approach and sets it as the
    default method to calculate queue delay. The previous method (based
    on Little's law) to calculate queue delay is set as optional.
    
    Signed-off-by: Gautam Ramakrishnan <gautamramk@gmail.com>
    Signed-off-by: Leslie Monis <lesliemonis@gmail.com>
    Signed-off-by: Mohit P. Tahiliani <tahiliani@nitk.edu.in>
    Acked-by: Dave Taht <dave.taht@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 5011259b8f67..9f1a72876212 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -950,19 +950,25 @@ enum {
 	TCA_PIE_BETA,
 	TCA_PIE_ECN,
 	TCA_PIE_BYTEMODE,
+	TCA_PIE_DQ_RATE_ESTIMATOR,
 	__TCA_PIE_MAX
 };
 #define TCA_PIE_MAX   (__TCA_PIE_MAX - 1)
 
 struct tc_pie_xstats {
-	__u64 prob;             /* current probability */
-	__u32 delay;            /* current delay in ms */
-	__u32 avg_dq_rate;      /* current average dq_rate in bits/pie_time */
-	__u32 packets_in;       /* total number of packets enqueued */
-	__u32 dropped;          /* packets dropped due to pie_action */
-	__u32 overlimit;        /* dropped due to lack of space in queue */
-	__u32 maxq;             /* maximum queue size */
-	__u32 ecn_mark;         /* packets marked with ecn*/
+	__u64 prob;			/* current probability */
+	__u32 delay;			/* current delay in ms */
+	__u32 avg_dq_rate;		/* current average dq_rate in
+					 * bits/pie_time
+					 */
+	__u32 dq_rate_estimating;	/* is avg_dq_rate being calculated? */
+	__u32 packets_in;		/* total number of packets enqueued */
+	__u32 dropped;			/* packets dropped due to pie_action */
+	__u32 overlimit;		/* dropped due to lack of space
+					 * in queue
+					 */
+	__u32 maxq;			/* maximum queue size */
+	__u32 ecn_mark;			/* packets marked with ecn*/
 };
 
 /* CBS */

commit 9c66d15646760eb8982242b4531c4d4fd36118fd
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Sun Sep 15 04:59:58 2019 +0300

    taprio: Add support for hardware offloading
    
    This allows taprio to offload the schedule enforcement to capable
    network cards, resulting in more precise windows and less CPU usage.
    
    The gate mask acts on traffic classes (groups of queues of same
    priority), as specified in IEEE 802.1Q-2018, and following the existing
    taprio and mqprio semantics.
    It is up to the driver to perform conversion between tc and individual
    netdev queues if for some reason it needs to make that distinction.
    
    Full offload is requested from the network interface by specifying
    "flags 2" in the tc qdisc creation command, which in turn corresponds to
    the TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD bit.
    
    The important detail here is the clockid which is implicitly /dev/ptpN
    for full offload, and hence not configurable.
    
    A reference counting API is added to support the use case where Ethernet
    drivers need to keep the taprio offload structure locally (i.e. they are
    a multi-port switch driver, and configuring a port depends on the
    settings of other ports as well). The refcount_t variable is kept in a
    private structure (__tc_taprio_qopt_offload) and not exposed to drivers.
    
    In the future, the private structure might also be expanded with a
    backpointer to taprio_sched *q, to implement the notification system
    described in the patch (of when admin became oper, or an error occurred,
    etc, so the offload can be monitored with 'tc qdisc show').
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Voon Weifeng <weifeng.voon@intel.com>
    Signed-off-by: Vladimir Oltean <olteanv@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 18f185299f47..5011259b8f67 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1160,7 +1160,8 @@ enum {
  *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_INTERVAL]
  */
 
-#define TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST 0x1
+#define TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST	BIT(0)
+#define TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD	BIT(1)
 
 enum {
 	TCA_TAPRIO_ATTR_UNSPEC,

commit a5b647007e9d794956dbed9339a3354a9fc4d5c3
Author: Vedang Patel <vedang.patel@intel.com>
Date:   Tue Jul 16 12:52:18 2019 -0700

    fix: taprio: Change type of txtime-delay parameter to u32
    
    During the review of the iproute2 patches for txtime-assist mode, it was
    pointed out that it does not make sense for the txtime-delay parameter to
    be negative. So, change the type of the parameter from s32 to u32.
    
    Fixes: 4cfd5779bd6e ("taprio: Add support for txtime-assist mode")
    Reported-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: Vedang Patel <vedang.patel@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 1f623252abe8..18f185299f47 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1174,7 +1174,7 @@ enum {
 	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME, /* s64 */
 	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION, /* s64 */
 	TCA_TAPRIO_ATTR_FLAGS, /* u32 */
-	TCA_TAPRIO_ATTR_TXTIME_DELAY, /* s32 */
+	TCA_TAPRIO_ATTR_TXTIME_DELAY, /* u32 */
 	__TCA_TAPRIO_ATTR_MAX,
 };
 

commit fbc697796e358d1ed8ed25758b19bdb3a1f8e9f9
Author: David Ahern <dsahern@gmail.com>
Date:   Tue Jul 9 14:45:17 2019 -0700

    pkt_sched: Include const.h
    
    Commit 9903c8dc7342 changed TC_ETF defines to use _BITUL instead of BIT
    but did not add the dependecy on linux/const.h. As a consequence,
    importing the uapi headers into iproute2 causes builds to fail. Add
    the dependency.
    
    Fixes: 9903c8dc7342 ("etf: Don't use BIT() in UAPI headers.")
    Cc: Vedang Patel <vedang.patel@intel.com>
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 390efb54b2e0..1f623252abe8 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -2,6 +2,7 @@
 #ifndef __LINUX_PKT_SCHED_H
 #define __LINUX_PKT_SCHED_H
 
+#include <linux/const.h>
 #include <linux/types.h>
 
 /* Logical priority bands not depending on specific packet scheduler.

commit 4cfd5779bd6efe8c76b4494aec63a063be0d2ff2
Author: Vedang Patel <vedang.patel@intel.com>
Date:   Tue Jun 25 15:07:17 2019 -0700

    taprio: Add support for txtime-assist mode
    
    Currently, we are seeing non-critical packets being transmitted outside of
    their timeslice. We can confirm that the packets are being dequeued at the
    right time. So, the delay is induced in the hardware side.  The most likely
    reason is the hardware queues are starving the lower priority queues.
    
    In order to improve the performance of taprio, we will be making use of the
    txtime feature provided by the ETF qdisc. For all the packets which do not
    have the SO_TXTIME option set, taprio will set the transmit timestamp (set
    in skb->tstamp) in this mode. TAPrio Qdisc will ensure that the transmit
    time for the packet is set to when the gate is open. If SO_TXTIME is set,
    the TAPrio qdisc will validate whether the timestamp (in skb->tstamp)
    occurs when the gate corresponding to skb's traffic class is open.
    
    Following two parameters added to support this mode:
    - flags: used to enable txtime-assist mode. Will also be used to enable
      other modes (like hardware offloading) later.
    - txtime-delay: This indicates the minimum time it will take for the packet
      to hit the wire. This is useful in determining whether we can transmit
    the packet in the remaining time if the gate corresponding to the packet is
    currently open.
    
    An example configuration for enabling txtime-assist:
    
    tc qdisc replace dev eth0 parent root handle 100 taprio \\
          num_tc 3 \\
          map 2 2 1 0 2 2 2 2 2 2 2 2 2 2 2 2 \\
          queues 1@0 1@0 1@0 \\
          base-time 1558653424279842568 \\
          sched-entry S 01 300000 \\
          sched-entry S 02 300000 \\
          sched-entry S 04 400000 \\
          flags 0x1 \\
          txtime-delay 40000 \\
          clockid CLOCK_TAI
    
    tc qdisc replace dev $IFACE parent 100:1 etf skip_sock_check \\
          offload delta 200000 clockid CLOCK_TAI
    
    Note that all the traffic classes are mapped to the same queue.  This is
    only possible in taprio when txtime-assist is enabled. Also, note that the
    ETF Qdisc is enabled with offload mode set.
    
    In this mode, if the packet's traffic class is open and the complete packet
    can be transmitted, taprio will try to transmit the packet immediately.
    This will be done by setting skb->tstamp to current_time + the time delta
    indicated in the txtime-delay parameter. This parameter indicates the time
    taken (in software) for packet to reach the network adapter.
    
    If the packet cannot be transmitted in the current interval or if the
    packet's traffic is not currently transmitting, the skb->tstamp is set to
    the next available timestamp value. This is tracked in the next_launchtime
    parameter in the struct sched_entry.
    
    The behaviour w.r.t admin and oper schedules is not changed from what is
    present in software mode.
    
    The transmit time is already known in advance. So, we do not need the HR
    timers to advance the schedule and wakeup the dequeue side of taprio.  So,
    HR timer won't be run when this mode is enabled.
    
    Signed-off-by: Vedang Patel <vedang.patel@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 127ac6d2888c..390efb54b2e0 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1159,6 +1159,8 @@ enum {
  *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_INTERVAL]
  */
 
+#define TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST 0x1
+
 enum {
 	TCA_TAPRIO_ATTR_UNSPEC,
 	TCA_TAPRIO_ATTR_PRIOMAP, /* struct tc_mqprio_qopt */
@@ -1170,6 +1172,8 @@ enum {
 	TCA_TAPRIO_ATTR_ADMIN_SCHED, /* The admin sched, only used in dump */
 	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME, /* s64 */
 	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION, /* s64 */
+	TCA_TAPRIO_ATTR_FLAGS, /* u32 */
+	TCA_TAPRIO_ATTR_TXTIME_DELAY, /* s32 */
 	__TCA_TAPRIO_ATTR_MAX,
 };
 

commit d14d2b20680f02fa739c2cbbb59e3629e487f359
Author: Vedang Patel <vedang.patel@intel.com>
Date:   Tue Jun 25 15:07:14 2019 -0700

    etf: Add skip_sock_check
    
    Currently, etf expects a socket with SO_TXTIME option set for each packet
    it encounters. So, it will drop all other packets. But, in the future
    commits we are planning to add functionality where tstamp value will be set
    by another qdisc. Also, some packets which are generated from within the
    kernel (e.g. ICMP packets) do not have any socket associated with them.
    
    So, this commit adds support for skip_sock_check. When this option is set,
    etf will skip checking for a socket and other associated options for all
    skbs.
    
    Signed-off-by: Vedang Patel <vedang.patel@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index f88c4e0bd9e5..127ac6d2888c 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -990,6 +990,7 @@ struct tc_etf_qopt {
 	__u32 flags;
 #define TC_ETF_DEADLINE_MODE_ON	_BITUL(0)
 #define TC_ETF_OFFLOAD_ON	_BITUL(1)
+#define TC_ETF_SKIP_SOCK_CHECK	_BITUL(2)
 };
 
 enum {

commit 9903c8dc734265689d5770ff28c84a7228fe5890
Author: Vedang Patel <vedang.patel@intel.com>
Date:   Tue Jun 25 15:07:13 2019 -0700

    etf: Don't use BIT() in UAPI headers.
    
    The BIT() macro isn't exported as part of the UAPI interface. So, the
    compile-test to ensure they are self contained fails. So, use _BITUL()
    instead.
    
    Signed-off-by: Vedang Patel <vedang.patel@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 8b2f993cbb77..f88c4e0bd9e5 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -988,8 +988,8 @@ struct tc_etf_qopt {
 	__s32 delta;
 	__s32 clockid;
 	__u32 flags;
-#define TC_ETF_DEADLINE_MODE_ON	BIT(0)
-#define TC_ETF_OFFLOAD_ON	BIT(1)
+#define TC_ETF_DEADLINE_MODE_ON	_BITUL(0)
+#define TC_ETF_OFFLOAD_ON	_BITUL(1)
 };
 
 enum {

commit c25031e993440debdd530278ce2171ce477df029
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Mon Apr 29 15:48:33 2019 -0700

    taprio: Add support for cycle-time-extension
    
    IEEE 802.1Q-2018 defines the concept of a cycle-time-extension, so the
    last entry of a schedule before the start of a new schedule can be
    extended, so "too-short" entries can be avoided.
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 7a32276838e1..8b2f993cbb77 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1168,6 +1168,7 @@ enum {
 	TCA_TAPRIO_PAD,
 	TCA_TAPRIO_ATTR_ADMIN_SCHED, /* The admin sched, only used in dump */
 	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME, /* s64 */
+	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION, /* s64 */
 	__TCA_TAPRIO_ATTR_MAX,
 };
 

commit 6ca6a6654225f3cd001304d33429c817e0c0b85f
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Mon Apr 29 15:48:32 2019 -0700

    taprio: Add support for setting the cycle-time manually
    
    IEEE 802.1Q-2018 defines that a the cycle-time of a schedule may be
    overridden, so the schedule is truncated to a determined "width".
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index d59770d0eb84..7a32276838e1 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1167,6 +1167,7 @@ enum {
 	TCA_TAPRIO_ATTR_SCHED_CLOCKID, /* s32 */
 	TCA_TAPRIO_PAD,
 	TCA_TAPRIO_ATTR_ADMIN_SCHED, /* The admin sched, only used in dump */
+	TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME, /* s64 */
 	__TCA_TAPRIO_ATTR_MAX,
 };
 

commit a3d43c0d56f1b94e74963a2fbadfb70126d92213
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Mon Apr 29 15:48:31 2019 -0700

    taprio: Add support adding an admin schedule
    
    The IEEE 802.1Q-2018 defines two "types" of schedules, the "Oper" (from
    operational?) and "Admin" ones. Up until now, 'taprio' only had
    support for the "Oper" one, added when the qdisc is created. This adds
    support for the "Admin" one, which allows the .change() operation to
    be supported.
    
    Just for clarification, some quick (and dirty) definitions, the "Oper"
    schedule is the currently (as in this instant) running one, and it's
    read-only. The "Admin" one is the one that the system configurator has
    installed, it can be changed, and it will be "promoted" to "Oper" when
    it's 'base-time' is reached.
    
    The idea behing this patch is that calling something like the below,
    (after taprio is already configured with an initial schedule):
    
    $ tc qdisc change taprio dev IFACE parent root       \
               base-time X                               \
               sched-entry <CMD> <GATES> <INTERVAL>      \
               ...
    
    Will cause a new admin schedule to be created and programmed to be
    "promoted" to "Oper" at instant X. If an "Admin" schedule already
    exists, it will be overwritten with the new parameters.
    
    Up until now, there was some code that was added to ease the support
    of changing a single entry of a schedule, but was ultimately unused.
    Now, that we have support for "change" with more well thought
    semantics, updating a single entry seems to be less useful.
    
    So we remove what is in practice dead code, and return a "not
    supported" error if the user tries to use it. If changing a single
    entry would make the user's life easier we may ressurrect this idea,
    but at this point, removing it simplifies the code.
    
    For now, only the schedule specific bits are allowed to be added for a
    new schedule, that means that 'clockid', 'num_tc', 'map' and 'queues'
    cannot be modified.
    
    Example:
    
    $ tc qdisc change dev IFACE parent root handle 100 taprio \
          base-time $BASE_TIME \
          sched-entry S 00 500000 \
          sched-entry S 0f 500000 \
          clockid CLOCK_TAI
    
    The only change in the netlink API introduced by this change is the
    introduction of an "admin" type in the response to a dump request,
    that type allows userspace to separate the "oper" schedule from the
    "admin" schedule. If userspace doesn't support the "admin" type, it
    will only display the "oper" schedule.
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 7ee74c3474bf..d59770d0eb84 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1148,6 +1148,16 @@ enum {
 
 #define TCA_TAPRIO_SCHED_MAX (__TCA_TAPRIO_SCHED_MAX - 1)
 
+/* The format for the admin sched (dump only):
+ * [TCA_TAPRIO_SCHED_ADMIN_SCHED]
+ *   [TCA_TAPRIO_ATTR_SCHED_BASE_TIME]
+ *   [TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST]
+ *     [TCA_TAPRIO_ATTR_SCHED_ENTRY]
+ *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_CMD]
+ *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_GATES]
+ *       [TCA_TAPRIO_ATTR_SCHED_ENTRY_INTERVAL]
+ */
+
 enum {
 	TCA_TAPRIO_ATTR_UNSPEC,
 	TCA_TAPRIO_ATTR_PRIOMAP, /* struct tc_mqprio_qopt */
@@ -1156,6 +1166,7 @@ enum {
 	TCA_TAPRIO_ATTR_SCHED_SINGLE_ENTRY, /* single entry */
 	TCA_TAPRIO_ATTR_SCHED_CLOCKID, /* s32 */
 	TCA_TAPRIO_PAD,
+	TCA_TAPRIO_ATTR_ADMIN_SCHED, /* The admin sched, only used in dump */
 	__TCA_TAPRIO_ATTR_MAX,
 };
 

commit 0b5c7efdfc6e389ec6840579fe90bdb6f42b08dc
Author: Kevin Darbyshire-Bryant <ldir@darbyshire-bryant.me.uk>
Date:   Fri Mar 1 16:04:05 2019 +0100

    sch_cake: Permit use of connmarks as tin classifiers
    
    Add flag 'FWMARK' to enable use of firewall connmarks as tin selector.
    The connmark (skbuff->mark) needs to be in the range 1->tin_cnt ie.
    for diffserv3 the mark needs to be 1->3.
    
    Background
    
    Typically CAKE uses DSCP as the basis for tin selection.  DSCP values
    are relatively easily changed as part of the egress path, usually with
    iptables & the mangle table, ingress is more challenging.  CAKE is often
    used on the WAN interface of a residential gateway where passthrough of
    DSCP from the ISP is either missing or set to unhelpful values thus use
    of ingress DSCP values for tin selection isn't helpful in that
    environment.
    
    An approach to solving the ingress tin selection problem is to use
    CAKE's understanding of tc filters.  Naive tc filters could match on
    source/destination port numbers and force tin selection that way, but
    multiple filters don't scale particularly well as each filter must be
    traversed whether it matches or not. e.g. a simple example to map 3
    firewall marks to tins:
    
    MAJOR=$( tc qdisc show dev $DEV | head -1 | awk '{print $3}' )
    tc filter add dev $DEV parent $MAJOR protocol all handle 0x01 fw action skbedit priority ${MAJOR}1
    tc filter add dev $DEV parent $MAJOR protocol all handle 0x02 fw action skbedit priority ${MAJOR}2
    tc filter add dev $DEV parent $MAJOR protocol all handle 0x03 fw action skbedit priority ${MAJOR}3
    
    Another option is to use eBPF cls_act with tc filters e.g.
    
    MAJOR=$( tc qdisc show dev $DEV | head -1 | awk '{print $3}' )
    tc filter add dev $DEV parent $MAJOR bpf da obj my-bpf-fwmark-to-class.o
    
    This has the disadvantages of a) needing someone to write & maintain
    the bpf program, b) a bpf toolchain to compile it and c) needing to
    hardcode the major number in the bpf program so it matches the cake
    instance (or forcing the cake instance to a particular major number)
    since the major number cannot be passed to the bpf program via tc
    command line.
    
    As already hinted at by the previous examples, it would be helpful
    to associate tins with something that survives the Internet path and
    ideally allows tin selection on both egress and ingress.  Netfilter's
    conntrack permits setting an identifying mark on a connection which
    can also be restored to an ingress packet with tc action connmark e.g.
    
    tc filter add dev eth0 parent ffff: protocol all prio 10 u32 \
            match u32 0 0 flowid 1:1 action connmark action mirred egress redirect dev ifb1
    
    Since tc's connmark action has restored any connmark into skb->mark,
    any of the previous solutions are based upon it and in one form or
    another copy that mark to the skb->priority field where again CAKE
    picks this up.
    
    This change cuts out at least one of the (less intuitive &
    non-scalable) middlemen and permit direct access to skb->mark.
    
    Signed-off-by: Kevin Darbyshire-Bryant <ldir@darbyshire-bryant.me.uk>
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 1eb572ef3f27..7ee74c3474bf 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1021,6 +1021,7 @@ enum {
 	TCA_CAKE_INGRESS,
 	TCA_CAKE_ACK_FILTER,
 	TCA_CAKE_SPLIT_GSO,
+	TCA_CAKE_FWMARK,
 	__TCA_CAKE_MAX
 };
 #define TCA_CAKE_MAX	(__TCA_CAKE_MAX - 1)

commit 3f7ae5f3dc5295ac17d6521130ed8a8f8a723fbf
Author: Mohit P. Tahiliani <tahiliani@nitk.edu.in>
Date:   Tue Feb 26 00:39:59 2019 +0530

    net: sched: pie: add more cases to auto-tune alpha and beta
    
    The current implementation scales the local alpha and beta
    variables in the calculate_probability function by the same
    amount for all values of drop probability below 1%.
    
    RFC 8033 suggests using additional cases for auto-tuning
    alpha and beta when the drop probability is less than 1%.
    
    In order to add more auto-tuning cases, MAX_PROB must be
    scaled by u64 instead of u32 to prevent underflow when
    scaling the local alpha and beta variables in the
    calculate_probability function.
    
    Signed-off-by: Mohit P. Tahiliani <tahiliani@nitk.edu.in>
    Signed-off-by: Dhaval Khandla <dhavaljkhandla26@gmail.com>
    Signed-off-by: Hrishikesh Hiraskar <hrishihiraskar@gmail.com>
    Signed-off-by: Manish Kumar B <bmanish15597@gmail.com>
    Signed-off-by: Sachin D. Patil <sdp.sachin@gmail.com>
    Signed-off-by: Leslie Monis <lesliemonis@gmail.com>
    Acked-by: Dave Taht <dave.taht@gmail.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 0d18b1d1fbbc..1eb572ef3f27 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -954,7 +954,7 @@ enum {
 #define TCA_PIE_MAX   (__TCA_PIE_MAX - 1)
 
 struct tc_pie_xstats {
-	__u32 prob;             /* current probability */
+	__u64 prob;             /* current probability */
 	__u32 delay;            /* current delay in ms */
 	__u32 avg_dq_rate;      /* current average dq_rate in bits/pie_time */
 	__u32 packets_in;       /* total number of packets enqueued */

commit 72111015024f4eddb5aac400ddbe38a4f8f0279a
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Nov 14 22:23:51 2018 -0800

    net: sched: gred: allow manipulating per-DP RED flags
    
    Allow users to set and dump RED flags (ECN enabled and harddrop)
    on per-virtual queue basis.  Validation of attributes is split
    from changes to make sure we won't have to undo previous operations
    when we find out configuration is invalid.
    
    The objective is to allow changing per-Qdisc parameters without
    overwriting the per-vq configured flags.
    
    Old user space will not pass the TCA_GRED_VQ_FLAGS attribute and
    per-Qdisc flags will always get propagated to the virtual queues.
    
    New user space which wants to make use of per-vq flags should set
    per-Qdisc flags to 0 and then configure per-vq flags as it
    sees fit.  Once per-vq flags are set per-Qdisc flags can't be
    changed to non-zero.  Vice versa - if the per-Qdisc flags are
    non-zero the TCA_GRED_VQ_FLAGS attribute has to either be omitted
    or set to the same value as per-Qdisc flags.
    
    Update per-Qdisc parameters:
    per-Qdisc | per-VQ | result
            0 |      0 | all vq flags updated
            0 |  non-0 | error (vq flags in use)
        non-0 |      0 | -- impossible --
        non-0 |  non-0 | all vq flags updated
    
    Update per-VQ state (flags parameter not specified):
       no change to flags
    
    Update per-VQ state (flags parameter set):
    per-Qdisc | per-VQ | result
            0 |   any  | per-vq flags updated
        non-0 |      0 | -- impossible --
        non-0 |  non-0 | error (per-Qdisc flags in use)
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index c8f717346b60..0d18b1d1fbbc 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -317,6 +317,7 @@ enum {
 	TCA_GRED_VQ_STAT_FORCED_MARK,	/* u32 */
 	TCA_GRED_VQ_STAT_PDROP,		/* u32 */
 	TCA_GRED_VQ_STAT_OTHER,		/* u32 */
+	TCA_GRED_VQ_FLAGS,		/* u32 */
 	__TCA_GRED_VQ_MAX
 };
 

commit 80e22e961dfd15530215f6f6dcd94cd8f65ba1ea
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Nov 14 22:23:49 2018 -0800

    net: sched: gred: provide a better structured dump and expose stats
    
    Currently all GRED's virtual queue data is dumped in a single
    array in a single attribute.  This makes it pretty much impossible
    to add new fields.  In order to expose more detailed stats add a
    new set of attributes.  We can now expose the 64 bit value of bytesin
    and all the mark stats which were not part of the original design.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index ee017bc057a3..c8f717346b60 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -291,11 +291,37 @@ enum {
        TCA_GRED_DPS,
        TCA_GRED_MAX_P,
        TCA_GRED_LIMIT,
+       TCA_GRED_VQ_LIST,	/* nested TCA_GRED_VQ_ENTRY */
        __TCA_GRED_MAX,
 };
 
 #define TCA_GRED_MAX (__TCA_GRED_MAX - 1)
 
+enum {
+	TCA_GRED_VQ_ENTRY_UNSPEC,
+	TCA_GRED_VQ_ENTRY,	/* nested TCA_GRED_VQ_* */
+	__TCA_GRED_VQ_ENTRY_MAX,
+};
+#define TCA_GRED_VQ_ENTRY_MAX (__TCA_GRED_VQ_ENTRY_MAX - 1)
+
+enum {
+	TCA_GRED_VQ_UNSPEC,
+	TCA_GRED_VQ_PAD,
+	TCA_GRED_VQ_DP,			/* u32 */
+	TCA_GRED_VQ_STAT_BYTES,		/* u64 */
+	TCA_GRED_VQ_STAT_PACKETS,	/* u32 */
+	TCA_GRED_VQ_STAT_BACKLOG,	/* u32 */
+	TCA_GRED_VQ_STAT_PROB_DROP,	/* u32 */
+	TCA_GRED_VQ_STAT_PROB_MARK,	/* u32 */
+	TCA_GRED_VQ_STAT_FORCED_DROP,	/* u32 */
+	TCA_GRED_VQ_STAT_FORCED_MARK,	/* u32 */
+	TCA_GRED_VQ_STAT_PDROP,		/* u32 */
+	TCA_GRED_VQ_STAT_OTHER,		/* u32 */
+	__TCA_GRED_VQ_MAX
+};
+
+#define TCA_GRED_VQ_MAX (__TCA_GRED_VQ_MAX - 1)
+
 struct tc_gred_qopt {
 	__u32		limit;        /* HARD maximal queue length (bytes)    */
 	__u32		qth_min;      /* Min average length threshold (bytes) */

commit 48872c11b77271ef9b070bdc50afe6655c4eb9aa
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Nov 11 09:11:31 2018 -0800

    net_sched: sch_fq: add dctcp-like marking
    
    Similar to 80ba92fa1a92 ("codel: add ce_threshold attribute")
    
    After EDT adoption, it became easier to implement DCTCP-like CE marking.
    
    In many cases, queues are not building in the network fabric but on
    the hosts themselves.
    
    If packets leaving fq missed their Earliest Departure Time by XXX usec,
    we mark them with ECN CE. This gives a feedback (after one RTT) to
    the sender to slow down and find better operating mode.
    
    Example :
    
    tc qd replace dev eth0 root fq ce_threshold 2.5ms
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 89ee47c2f17d..ee017bc057a3 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -864,6 +864,8 @@ enum {
 
 	TCA_FQ_LOW_RATE_THRESHOLD, /* per packet delay under this rate */
 
+	TCA_FQ_CE_THRESHOLD,	/* DCTCP-like CE-marking threshold */
+
 	__TCA_FQ_MAX
 };
 
@@ -882,6 +884,7 @@ struct tc_fq_qd_stats {
 	__u32	inactive_flows;
 	__u32	throttled_flows;
 	__u32	unthrottle_latency_ns;
+	__u64	ce_mark;		/* packets above ce_threshold */
 };
 
 /* Heavy-Hitter Filter */

commit 5a781ccbd19e4664babcbe4b4ead7aa2b9283d22
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Fri Sep 28 17:59:43 2018 -0700

    tc: Add support for configuring the taprio scheduler
    
    This traffic scheduler allows traffic classes states (transmission
    allowed/not allowed, in the simplest case) to be scheduled, according
    to a pre-generated time sequence. This is the basis of the IEEE
    802.1Qbv specification.
    
    Example configuration:
    
    tc qdisc replace dev enp3s0 parent root handle 100 taprio \
              num_tc 3 \
              map 2 2 1 0 2 2 2 2 2 2 2 2 2 2 2 2 \
              queues 1@0 1@1 2@2 \
              base-time 1528743495910289987 \
              sched-entry S 01 300000 \
              sched-entry S 02 300000 \
              sched-entry S 04 300000 \
              clockid CLOCK_TAI
    
    The configuration format is similar to mqprio. The main difference is
    the presence of a schedule, built by multiple "sched-entry"
    definitions, each entry has the following format:
    
         sched-entry <CMD> <GATE MASK> <INTERVAL>
    
    The only supported <CMD> is "S", which means "SetGateStates",
    following the IEEE 802.1Qbv-2015 definition (Table 8-6). <GATE MASK>
    is a bitmask where each bit is a associated with a traffic class, so
    bit 0 (the least significant bit) being "on" means that traffic class
    0 is "active" for that schedule entry. <INTERVAL> is a time duration
    in nanoseconds that specifies for how long that state defined by <CMD>
    and <GATE MASK> should be held before moving to the next entry.
    
    This schedule is circular, that is, after the last entry is executed
    it starts from the first one, indefinitely.
    
    The other parameters can be defined as follows:
    
     - base-time: specifies the instant when the schedule starts, if
      'base-time' is a time in the past, the schedule will start at
    
                  base-time + (N * cycle-time)
    
       where N is the smallest integer so the resulting time is greater
       than "now", and "cycle-time" is the sum of all the intervals of the
       entries in the schedule;
    
     - clockid: specifies the reference clock to be used;
    
    The parameters should be similar to what the IEEE 802.1Q family of
    specification defines.
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index e9b7244ac381..89ee47c2f17d 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1084,4 +1084,50 @@ enum {
 	CAKE_ATM_MAX
 };
 
+
+/* TAPRIO */
+enum {
+	TC_TAPRIO_CMD_SET_GATES = 0x00,
+	TC_TAPRIO_CMD_SET_AND_HOLD = 0x01,
+	TC_TAPRIO_CMD_SET_AND_RELEASE = 0x02,
+};
+
+enum {
+	TCA_TAPRIO_SCHED_ENTRY_UNSPEC,
+	TCA_TAPRIO_SCHED_ENTRY_INDEX, /* u32 */
+	TCA_TAPRIO_SCHED_ENTRY_CMD, /* u8 */
+	TCA_TAPRIO_SCHED_ENTRY_GATE_MASK, /* u32 */
+	TCA_TAPRIO_SCHED_ENTRY_INTERVAL, /* u32 */
+	__TCA_TAPRIO_SCHED_ENTRY_MAX,
+};
+#define TCA_TAPRIO_SCHED_ENTRY_MAX (__TCA_TAPRIO_SCHED_ENTRY_MAX - 1)
+
+/* The format for schedule entry list is:
+ * [TCA_TAPRIO_SCHED_ENTRY_LIST]
+ *   [TCA_TAPRIO_SCHED_ENTRY]
+ *     [TCA_TAPRIO_SCHED_ENTRY_CMD]
+ *     [TCA_TAPRIO_SCHED_ENTRY_GATES]
+ *     [TCA_TAPRIO_SCHED_ENTRY_INTERVAL]
+ */
+enum {
+	TCA_TAPRIO_SCHED_UNSPEC,
+	TCA_TAPRIO_SCHED_ENTRY,
+	__TCA_TAPRIO_SCHED_MAX,
+};
+
+#define TCA_TAPRIO_SCHED_MAX (__TCA_TAPRIO_SCHED_MAX - 1)
+
+enum {
+	TCA_TAPRIO_ATTR_UNSPEC,
+	TCA_TAPRIO_ATTR_PRIOMAP, /* struct tc_mqprio_qopt */
+	TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST, /* nested of entry */
+	TCA_TAPRIO_ATTR_SCHED_BASE_TIME, /* s64 */
+	TCA_TAPRIO_ATTR_SCHED_SINGLE_ENTRY, /* single entry */
+	TCA_TAPRIO_ATTR_SCHED_CLOCKID, /* s32 */
+	TCA_TAPRIO_PAD,
+	__TCA_TAPRIO_ATTR_MAX,
+};
+
+#define TCA_TAPRIO_ATTR_MAX (__TCA_TAPRIO_ATTR_MAX - 1)
+
 #endif

commit b9de3963cc2b373a655636335cb8c4ed12fc9d3b
Author: Florent Fourcot <florent.fourcot@wifirst.fr>
Date:   Thu Aug 30 16:39:23 2018 +0200

    net/sched: fix type of htb statistics
    
    tokens and ctokens are defined as s64 in htb_class structure,
    and clamped to 32bits value during netlink dumps:
    
    cl->xstats.tokens = clamp_t(s64, PSCHED_NS2TICKS(cl->tokens),
                                INT_MIN, INT_MAX);
    
    Defining it as u32 is working since userspace (tc) is printing it as
    signed int, but a correct definition from the beginning is probably
    better.
    
    In the same time, 'giants' structure member is unused since years, so
    update the comment to mark it unused.
    
    Signed-off-by: Florent Fourcot <florent.fourcot@wifirst.fr>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 8975fd1a1421..e9b7244ac381 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -395,9 +395,9 @@ enum {
 struct tc_htb_xstats {
 	__u32 lends;
 	__u32 borrows;
-	__u32 giants;	/* too big packets (rate will not be accurate) */
-	__u32 tokens;
-	__u32 ctokens;
+	__u32 giants;	/* unused since 'Make HTB scheduler work with TSO.' */
+	__s32 tokens;
+	__s32 ctokens;
 };
 
 /* HFSC section */

commit aea5f654e6b78a0c976f7a25950155932c77a53f
Author: Nishanth Devarajan <ndev2021@gmail.com>
Date:   Mon Jul 23 19:37:41 2018 +0530

    net/sched: add skbprio scheduler
    
    Skbprio (SKB Priority Queue) is a queueing discipline that prioritizes packets
    according to their skb->priority field. Under congestion, already-enqueued lower
    priority packets will be dropped to make space available for higher priority
    packets. Skbprio was conceived as a solution for denial-of-service defenses that
    need to route packets with different priorities as a means to overcome DoS
    attacks.
    
    v5
    *Do not reference qdisc_dev(sch)->tx_queue_len for setting limit. Instead set
    default sch->limit to 64.
    
    v4
    *Drop Documentation/networking/sch_skbprio.txt doc file to move it to tc man
    page for Skbprio, in iproute2.
    
    v3
    *Drop max_limit parameter in struct skbprio_sched_data and instead use
    sch->limit.
    
    *Reference qdisc_dev(sch)->tx_queue_len only once, during initialisation for
    qdisc (previously being referenced every time qdisc changes).
    
    *Move qdisc's detailed description from in-code to Documentation/networking.
    
    *When qdisc is saturated, enqueue incoming packet first before dequeueing
    lowest priority packet in queue - improves usage of call stack registers.
    
    *Introduce and use overlimit stat to keep track of number of dropped packets.
    
    v2
    *Use skb->priority field rather than DS field. Rename queueing discipline as
    SKB Priority Queue (previously Gatekeeper Priority Queue).
    
    *Queueing discipline is made classful to expose Skbprio's internal priority
    queues.
    
    Signed-off-by: Nishanth Devarajan <ndev2021@gmail.com>
    Reviewed-by: Sachin Paryani <sachin.paryani@gmail.com>
    Reviewed-by: Cody Doucette <doucette@bu.edu>
    Reviewed-by: Michel Machado <michel@digirati.com.br>
    Acked-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index d9cc9dc4f547..8975fd1a1421 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -124,6 +124,21 @@ struct tc_fifo_qopt {
 	__u32	limit;	/* Queue length: bytes for bfifo, packets for pfifo */
 };
 
+/* SKBPRIO section */
+
+/*
+ * Priorities go from zero to (SKBPRIO_MAX_PRIORITY - 1).
+ * SKBPRIO_MAX_PRIORITY should be at least 64 in order for skbprio to be able
+ * to map one to one the DS field of IPV4 and IPV6 headers.
+ * Memory allocation grows linearly with SKBPRIO_MAX_PRIORITY.
+ */
+
+#define SKBPRIO_MAX_PRIORITY 64
+
+struct tc_skbprio_qopt {
+	__u32	limit;		/* Queue length in packets. */
+};
+
 /* PRIO section */
 
 #define TCQ_PRIO_BANDS	16

commit 046f6fd5daefac7f5abdafb436b30f63bc7c602b
Author: Toke Høiland-Jørgensen <toke@toke.dk>
Date:   Fri Jul 6 17:37:19 2018 +0200

    sched: Add Common Applications Kept Enhanced (cake) qdisc
    
    sch_cake targets the home router use case and is intended to squeeze the
    most bandwidth and latency out of even the slowest ISP links and routers,
    while presenting an API simple enough that even an ISP can configure it.
    
    Example of use on a cable ISP uplink:
    
    tc qdisc add dev eth0 cake bandwidth 20Mbit nat docsis ack-filter
    
    To shape a cable download link (ifb and tc-mirred setup elided)
    
    tc qdisc add dev ifb0 cake bandwidth 200mbit nat docsis ingress wash
    
    CAKE is filled with:
    
    * A hybrid Codel/Blue AQM algorithm, "Cobalt", tied to an FQ_Codel
      derived Flow Queuing system, which autoconfigures based on the bandwidth.
    * A novel "triple-isolate" mode (the default) which balances per-host
      and per-flow FQ even through NAT.
    * An deficit based shaper, that can also be used in an unlimited mode.
    * 8 way set associative hashing to reduce flow collisions to a minimum.
    * A reasonable interpretation of various diffserv latency/loss tradeoffs.
    * Support for zeroing diffserv markings for entering and exiting traffic.
    * Support for interacting well with Docsis 3.0 shaper framing.
    * Extensive support for DSL framing types.
    * Support for ack filtering.
    * Extensive statistics for measuring, loss, ecn markings, latency
      variation.
    
    A paper describing the design of CAKE is available at
    https://arxiv.org/abs/1804.07617, and will be published at the 2018 IEEE
    International Symposium on Local and Metropolitan Area Networks (LANMAN).
    
    This patch adds the base shaper and packet scheduler, while subsequent
    commits add the optional (configurable) features. The full userspace API
    and most data structures are included in this commit, but options not
    understood in the base version will be ignored.
    
    Various versions baking have been available as an out of tree build for
    kernel versions going back to 3.10, as the embedded router world has been
    running a few years behind mainline Linux. A stable version has been
    generally available on lede-17.01 and later.
    
    sch_cake replaces a combination of iptables, tc filter, htb and fq_codel
    in the sqm-scripts, with sane defaults and vastly simpler configuration.
    
    CAKE's principal author is Jonathan Morton, with contributions from
    Kevin Darbyshire-Bryant, Toke Høiland-Jørgensen, Sebastian Moeller,
    Ryan Mounce, Tony Ambardar, Dean Scarff, Nils Andreas Svee, Dave Täht,
    and Loganaden Velvindron.
    
    Testing from Pete Heist, Georgios Amanakis, and the many other members of
    the cake@lists.bufferbloat.net mailing list.
    
    tc -s qdisc show dev eth2
     qdisc cake 8017: root refcnt 2 bandwidth 1Gbit diffserv3 triple-isolate split-gso rtt 100.0ms noatm overhead 38 mpu 84
     Sent 51504294511 bytes 37724591 pkt (dropped 6, overlimits 64958695 requeues 12)
      backlog 0b 0p requeues 12
      memory used: 1053008b of 15140Kb
      capacity estimate: 970Mbit
      min/max network layer size:           28 /    1500
      min/max overhead-adjusted size:       84 /    1538
      average network hdr offset:           14
                        Bulk  Best Effort        Voice
       thresh      62500Kbit        1Gbit      250Mbit
       target          5.0ms        5.0ms        5.0ms
       interval      100.0ms      100.0ms      100.0ms
       pk_delay          5us          5us          6us
       av_delay          3us          2us          2us
       sp_delay          2us          1us          1us
       backlog            0b           0b           0b
       pkts          3164050     25030267      9530280
       bytes      3227519915  35396974782  12879808898
       way_inds            0            8            0
       way_miss           21          366           25
       way_cols            0            0            0
       drops               5            0            1
       marks               0            0            0
       ack_drop            0            0            0
       sp_flows            1            3            0
       bk_flows            0            1            1
       un_flows            0            0            0
       max_len         68130        68130        68130
    
    Tested-by: Pete Heist <peteheist@gmail.com>
    Tested-by: Georgios Amanakis <gamanakis@gmail.com>
    Signed-off-by: Dave Taht <dave.taht@gmail.com>
    Signed-off-by: Toke Høiland-Jørgensen <toke@toke.dk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 949118461009..d9cc9dc4f547 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -955,4 +955,118 @@ enum {
 
 #define TCA_ETF_MAX (__TCA_ETF_MAX - 1)
 
+
+/* CAKE */
+enum {
+	TCA_CAKE_UNSPEC,
+	TCA_CAKE_PAD,
+	TCA_CAKE_BASE_RATE64,
+	TCA_CAKE_DIFFSERV_MODE,
+	TCA_CAKE_ATM,
+	TCA_CAKE_FLOW_MODE,
+	TCA_CAKE_OVERHEAD,
+	TCA_CAKE_RTT,
+	TCA_CAKE_TARGET,
+	TCA_CAKE_AUTORATE,
+	TCA_CAKE_MEMORY,
+	TCA_CAKE_NAT,
+	TCA_CAKE_RAW,
+	TCA_CAKE_WASH,
+	TCA_CAKE_MPU,
+	TCA_CAKE_INGRESS,
+	TCA_CAKE_ACK_FILTER,
+	TCA_CAKE_SPLIT_GSO,
+	__TCA_CAKE_MAX
+};
+#define TCA_CAKE_MAX	(__TCA_CAKE_MAX - 1)
+
+enum {
+	__TCA_CAKE_STATS_INVALID,
+	TCA_CAKE_STATS_PAD,
+	TCA_CAKE_STATS_CAPACITY_ESTIMATE64,
+	TCA_CAKE_STATS_MEMORY_LIMIT,
+	TCA_CAKE_STATS_MEMORY_USED,
+	TCA_CAKE_STATS_AVG_NETOFF,
+	TCA_CAKE_STATS_MIN_NETLEN,
+	TCA_CAKE_STATS_MAX_NETLEN,
+	TCA_CAKE_STATS_MIN_ADJLEN,
+	TCA_CAKE_STATS_MAX_ADJLEN,
+	TCA_CAKE_STATS_TIN_STATS,
+	TCA_CAKE_STATS_DEFICIT,
+	TCA_CAKE_STATS_COBALT_COUNT,
+	TCA_CAKE_STATS_DROPPING,
+	TCA_CAKE_STATS_DROP_NEXT_US,
+	TCA_CAKE_STATS_P_DROP,
+	TCA_CAKE_STATS_BLUE_TIMER_US,
+	__TCA_CAKE_STATS_MAX
+};
+#define TCA_CAKE_STATS_MAX (__TCA_CAKE_STATS_MAX - 1)
+
+enum {
+	__TCA_CAKE_TIN_STATS_INVALID,
+	TCA_CAKE_TIN_STATS_PAD,
+	TCA_CAKE_TIN_STATS_SENT_PACKETS,
+	TCA_CAKE_TIN_STATS_SENT_BYTES64,
+	TCA_CAKE_TIN_STATS_DROPPED_PACKETS,
+	TCA_CAKE_TIN_STATS_DROPPED_BYTES64,
+	TCA_CAKE_TIN_STATS_ACKS_DROPPED_PACKETS,
+	TCA_CAKE_TIN_STATS_ACKS_DROPPED_BYTES64,
+	TCA_CAKE_TIN_STATS_ECN_MARKED_PACKETS,
+	TCA_CAKE_TIN_STATS_ECN_MARKED_BYTES64,
+	TCA_CAKE_TIN_STATS_BACKLOG_PACKETS,
+	TCA_CAKE_TIN_STATS_BACKLOG_BYTES,
+	TCA_CAKE_TIN_STATS_THRESHOLD_RATE64,
+	TCA_CAKE_TIN_STATS_TARGET_US,
+	TCA_CAKE_TIN_STATS_INTERVAL_US,
+	TCA_CAKE_TIN_STATS_WAY_INDIRECT_HITS,
+	TCA_CAKE_TIN_STATS_WAY_MISSES,
+	TCA_CAKE_TIN_STATS_WAY_COLLISIONS,
+	TCA_CAKE_TIN_STATS_PEAK_DELAY_US,
+	TCA_CAKE_TIN_STATS_AVG_DELAY_US,
+	TCA_CAKE_TIN_STATS_BASE_DELAY_US,
+	TCA_CAKE_TIN_STATS_SPARSE_FLOWS,
+	TCA_CAKE_TIN_STATS_BULK_FLOWS,
+	TCA_CAKE_TIN_STATS_UNRESPONSIVE_FLOWS,
+	TCA_CAKE_TIN_STATS_MAX_SKBLEN,
+	TCA_CAKE_TIN_STATS_FLOW_QUANTUM,
+	__TCA_CAKE_TIN_STATS_MAX
+};
+#define TCA_CAKE_TIN_STATS_MAX (__TCA_CAKE_TIN_STATS_MAX - 1)
+#define TC_CAKE_MAX_TINS (8)
+
+enum {
+	CAKE_FLOW_NONE = 0,
+	CAKE_FLOW_SRC_IP,
+	CAKE_FLOW_DST_IP,
+	CAKE_FLOW_HOSTS,    /* = CAKE_FLOW_SRC_IP | CAKE_FLOW_DST_IP */
+	CAKE_FLOW_FLOWS,
+	CAKE_FLOW_DUAL_SRC, /* = CAKE_FLOW_SRC_IP | CAKE_FLOW_FLOWS */
+	CAKE_FLOW_DUAL_DST, /* = CAKE_FLOW_DST_IP | CAKE_FLOW_FLOWS */
+	CAKE_FLOW_TRIPLE,   /* = CAKE_FLOW_HOSTS  | CAKE_FLOW_FLOWS */
+	CAKE_FLOW_MAX,
+};
+
+enum {
+	CAKE_DIFFSERV_DIFFSERV3 = 0,
+	CAKE_DIFFSERV_DIFFSERV4,
+	CAKE_DIFFSERV_DIFFSERV8,
+	CAKE_DIFFSERV_BESTEFFORT,
+	CAKE_DIFFSERV_PRECEDENCE,
+	CAKE_DIFFSERV_MAX
+};
+
+enum {
+	CAKE_ACK_NONE = 0,
+	CAKE_ACK_FILTER,
+	CAKE_ACK_AGGRESSIVE,
+	CAKE_ACK_MAX
+};
+
+enum {
+	CAKE_ATM_NONE = 0,
+	CAKE_ATM_ATM,
+	CAKE_ATM_PTM,
+	CAKE_ATM_MAX
+};
+
 #endif

commit 88cab77162e86e0f6a2b7e4f859c1435c4e24feb
Author: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
Date:   Tue Jul 3 15:42:54 2018 -0700

    net/sched: Add HW offloading capability to ETF
    
    Add infra so etf qdisc supports HW offload of time-based transmission.
    
    For hw offload, the time sorted list is still used, so packets are
    dequeued always in order of txtime.
    
    Example:
    
    $ tc qdisc replace dev enp2s0 parent root handle 100 mqprio num_tc 3 \
               map 2 2 1 0 2 2 2 2 2 2 2 2 2 2 2 2 queues 1@0 1@1 2@2 hw 0
    
    $ tc qdisc add dev enp2s0 parent 100:1 etf offload delta 100000 \
               clockid CLOCK_REALTIME
    
    In this example, the Qdisc will use HW offload for the control of the
    transmission time through the network adapter. The hrtimer used for
    packets scheduling inside the qdisc will use the clockid CLOCK_REALTIME
    as reference and packets leave the Qdisc "delta" (100000) nanoseconds
    before their transmission time. Because this will be using HW offload and
    since dynamic clocks are not supported by the hrtimer, the system clock
    and the PHC clock must be synchronized for this mode to behave as
    expected.
    
    Signed-off-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index d5e933ce1447..949118461009 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -944,6 +944,7 @@ struct tc_etf_qopt {
 	__s32 clockid;
 	__u32 flags;
 #define TC_ETF_DEADLINE_MODE_ON	BIT(0)
+#define TC_ETF_OFFLOAD_ON	BIT(1)
 };
 
 enum {

commit 25db26a91364db00f5a30da2fea8e9afe14a163c
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Tue Jul 3 15:42:53 2018 -0700

    net/sched: Introduce the ETF Qdisc
    
    The ETF (Earliest TxTime First) qdisc uses the information added
    earlier in this series (the socket option SO_TXTIME and the new
    role of sk_buff->tstamp) to schedule packets transmission based
    on absolute time.
    
    For some workloads, just bandwidth enforcement is not enough, and
    precise control of the transmission of packets is necessary.
    
    Example:
    
    $ tc qdisc replace dev enp2s0 parent root handle 100 mqprio num_tc 3 \
               map 2 2 1 0 2 2 2 2 2 2 2 2 2 2 2 2 queues 1@0 1@1 2@2 hw 0
    
    $ tc qdisc add dev enp2s0 parent 100:1 etf delta 100000 \
               clockid CLOCK_TAI
    
    In this example, the Qdisc will provide SW best-effort for the control
    of the transmission time to the network adapter, the time stamp in the
    socket will be in reference to the clockid CLOCK_TAI and packets
    will leave the qdisc "delta" (100000) nanoseconds before its transmission
    time.
    
    The ETF qdisc will buffer packets sorted by their txtime. It will drop
    packets on enqueue() if their skbuff clockid does not match the clock
    reference of the Qdisc. Moreover, on dequeue(), a packet will be dropped
    if it expires while being enqueued.
    
    The qdisc also supports the SO_TXTIME deadline mode. For this mode, it
    will dequeue a packet as soon as possible and change the skb timestamp
    to 'now' during etf_dequeue().
    
    Note that both the qdisc's and the SO_TXTIME ABIs allow for a clockid
    to be configured, but it's been decided that usage of CLOCK_TAI should
    be enforced until we decide to allow for other clockids to be used.
    The rationale here is that PTP times are usually in the TAI scale, thus
    no other clocks should be necessary. For now, the qdisc will return
    EINVAL if any clocks other than CLOCK_TAI are used.
    
    Signed-off-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index bad3c03bcf43..d5e933ce1447 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -937,4 +937,21 @@ enum {
 
 #define TCA_CBS_MAX (__TCA_CBS_MAX - 1)
 
+
+/* ETF */
+struct tc_etf_qopt {
+	__s32 delta;
+	__s32 clockid;
+	__u32 flags;
+#define TC_ETF_DEADLINE_MODE_ON	BIT(0)
+};
+
+enum {
+	TCA_ETF_UNSPEC,
+	TCA_ETF_PARMS,
+	__TCA_ETF_MAX,
+};
+
+#define TCA_ETF_MAX (__TCA_ETF_MAX - 1)
+
 #endif

commit 0a9fe5c375b57fab6d18ed0a6a7f935eefb09db3
Author: Yousuk Seung <ysseung@google.com>
Date:   Wed Jun 27 10:32:19 2018 -0700

    netem: slotting with non-uniform distribution
    
    Extend slotting with support for non-uniform distributions. This is
    similar to netem's non-uniform distribution delay feature.
    
    Commit f043efeae2f1 ("netem: support delivering packets in delayed
    time slots") added the slotting feature to approximate the behaviors
    of media with packet aggregation but only supported a uniform
    distribution for delays between transmission attempts. Tests with TCP
    BBR with emulated wifi links with non-uniform distributions produced
    more useful results.
    
    Syntax:
       slot dist DISTRIBUTION DELAY JITTER [packets MAX_PACKETS] \
          [bytes MAX_BYTES]
    
    The syntax and use of the distribution table is the same as in the
    non-uniform distribution delay feature. A file DISTRIBUTION must be
    present in TC_LIB_DIR (e.g. /usr/lib/tc) containing numbers scaled by
    NETEM_DIST_SCALE. A random value x is selected from the table and it
    takes DELAY + ( x * JITTER ) as delay. Correlation between values is not
    supported.
    
    Examples:
      Normal distribution delay with mean = 800us and stdev = 100us.
      > tc qdisc add dev eth0 root netem slot dist normal 800us 100us
    
      Optionally set the max slot size in bytes and/or packets.
      > tc qdisc add dev eth0 root netem slot dist normal 800us 100us \
        bytes 64k packets 42
    
    Signed-off-by: Yousuk Seung <ysseung@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 37b5096ae97b..bad3c03bcf43 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -539,6 +539,7 @@ enum {
 	TCA_NETEM_LATENCY64,
 	TCA_NETEM_JITTER64,
 	TCA_NETEM_SLOT,
+	TCA_NETEM_SLOT_DIST,
 	__TCA_NETEM_MAX,
 };
 
@@ -581,6 +582,8 @@ struct tc_netem_slot {
 	__s64   max_delay;
 	__s32   max_packets;
 	__s32   max_bytes;
+	__s64	dist_delay; /* nsec */
+	__s64	dist_jitter; /* nsec */
 };
 
 enum {

commit 4a98795bc8ea148b1ebbbf001283e06430cffe36
Author: Yuval Mintz <yuvalm@mellanox.com>
Date:   Thu Dec 14 15:54:31 2017 +0200

    pkt_sched: Remove TC_RED_OFFLOADED from uapi
    
    Following the previous patch, RED is now using the new uniform uapi
    for indicating it's offloaded. As a result, TC_RED_OFFLOADED is no
    longer utilized by kernel and can be removed [as it's still not
    part of any stable release].
    
    Fixes: 602f3baf2218 ("net_sch: red: Add offload ability to RED qdisc")
    Signed-off-by: Yuval Mintz <yuvalm@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index af3cc2f4e1ad..37b5096ae97b 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -256,7 +256,6 @@ struct tc_red_qopt {
 #define TC_RED_ECN		1
 #define TC_RED_HARDDROP		2
 #define TC_RED_ADAPTATIVE	4
-#define TC_RED_OFFLOADED	8
 };
 
 struct tc_red_xstats {

commit 836af83b54e3e285c4a0cc06c24aeb737d3e0e18
Author: Dave Taht <dave.taht@gmail.com>
Date:   Wed Nov 8 15:12:28 2017 -0800

    netem: support delivering packets in delayed time slots
    
    Slotting is a crude approximation of the behaviors of shared media such
    as cable, wifi, and LTE, which gather up a bunch of packets within a
    varying delay window and deliver them, relative to that, nearly all at
    once.
    
    It works within the existing loss, duplication, jitter and delay
    parameters of netem. Some amount of inherent latency must be specified,
    regardless.
    
    The new "slot" parameter specifies a minimum and maximum delay between
    transmission attempts.
    
    The "bytes" and "packets" parameters can be used to limit the amount of
    information transferred per slot.
    
    Examples of use:
    
    tc qdisc add dev eth0 root netem delay 200us \
             slot 800us 10ms bytes 64k packets 42
    
    A more correct example, using stacked netem instances and a packet limit
    to emulate a tail drop wifi queue with slots and variable packet
    delivery, with a 200Mbit isochronous underlying rate, and 20ms path
    delay:
    
    tc qdisc add dev eth0 root handle 1: netem delay 20ms rate 200mbit \
             limit 10000
    tc qdisc add dev eth0 parent 1:1 handle 10:1 netem delay 200us \
             slot 800us 10ms bytes 64k packets 42 limit 512
    
    Signed-off-by: Dave Taht <dave.taht@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 8fe6d1842bee..af3cc2f4e1ad 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -539,6 +539,7 @@ enum {
 	TCA_NETEM_PAD,
 	TCA_NETEM_LATENCY64,
 	TCA_NETEM_JITTER64,
+	TCA_NETEM_SLOT,
 	__TCA_NETEM_MAX,
 };
 
@@ -576,6 +577,13 @@ struct tc_netem_rate {
 	__s32	cell_overhead;
 };
 
+struct tc_netem_slot {
+	__s64   min_delay; /* nsec */
+	__s64   max_delay;
+	__s32   max_packets;
+	__s32   max_bytes;
+};
+
 enum {
 	NETEM_LOSS_UNSPEC,
 	NETEM_LOSS_GI,		/* General Intuitive - 4 state model */

commit 99803171ef04037092bf5eb29ae801e8b4d49a75
Author: Dave Taht <dave.taht@gmail.com>
Date:   Wed Nov 8 15:12:27 2017 -0800

    netem: add uapi to express delay and jitter in nanoseconds
    
    netem userspace has long relied on a horrible /proc/net/psched hack
    to translate the current notion of "ticks" to nanoseconds.
    
    Expressing latency and jitter instead, in well defined nanoseconds,
    increases the dynamic range of emulated delays and jitter in netem.
    
    It will also ease a transition where reducing a tick to nsec
    equivalence would constrain the max delay in prior versions of
    netem to only 4.3 seconds.
    
    Signed-off-by: Dave Taht <dave.taht@gmail.com>
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 6a2c5ea7e9c4..8fe6d1842bee 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -537,6 +537,8 @@ enum {
 	TCA_NETEM_ECN,
 	TCA_NETEM_RATE64,
 	TCA_NETEM_PAD,
+	TCA_NETEM_LATENCY64,
+	TCA_NETEM_JITTER64,
 	__TCA_NETEM_MAX,
 };
 

commit 602f3baf22188aad24b9a58be3209ab774b97d74
Author: Nogah Frankel <nogahf@mellanox.com>
Date:   Mon Nov 6 07:23:41 2017 +0100

    net_sch: red: Add offload ability to RED qdisc
    
    Add the ability to offload RED qdisc by using ndo_setup_tc.
    There are four commands for RED offloading:
    * TC_RED_SET: handles set and change.
    * TC_RED_DESTROY: handle qdisc destroy.
    * TC_RED_STATS: update the qdiscs counters (given as reference)
    * TC_RED_XSTAT: returns red xstats.
    
    Whether RED is being offloaded is being determined every time dump action
    is being called because parent change of this qdisc could change its
    offload state but doesn't require any RED function to be called.
    
    Signed-off-by: Nogah Frankel <nogahf@mellanox.com>
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 5002562868cc..6a2c5ea7e9c4 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -256,6 +256,7 @@ struct tc_red_qopt {
 #define TC_RED_ECN		1
 #define TC_RED_HARDDROP		2
 #define TC_RED_ADAPTATIVE	4
+#define TC_RED_OFFLOADED	8
 };
 
 struct tc_red_xstats {

commit 2a171788ba7bb61995e98e8163204fc7880f63b2
Merge: bf5345882bd1 d4c2e9fca5b7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 4 09:26:51 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Files removed in 'net-next' had their license header updated
    in 'net'.  We take the remove from 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6f52b16c5b29b89d92c0e7236f4655dc8491ad70
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:08:43 2017 +0100

    License cleanup: add SPDX license identifier to uapi header files with no license
    
    Many user space API headers are missing licensing information, which
    makes it hard for compliance tools to determine the correct license.
    
    By default are files without license information under the default
    license of the kernel, which is GPLV2.  Marking them GPLV2 would exclude
    them from being included in non GPLV2 code, which is obviously not
    intended. The user space API headers fall under the syscall exception
    which is in the kernels COPYING file:
    
       NOTE! This copyright does *not* cover user programs that use kernel
       services by normal system calls - this is merely considered normal use
       of the kernel, and does *not* fall under the heading of "derived work".
    
    otherwise syscall usage would not be possible.
    
    Update the files which contain no license information with an SPDX
    license identifier.  The chosen identifier is 'GPL-2.0 WITH
    Linux-syscall-note' which is the officially assigned identifier for the
    Linux syscall exception.  SPDX license identifiers are a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.  See the previous patch in this series for the
    methodology of how this patch was researched.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 099bf5528fed..703cd9df6cef 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 #ifndef __LINUX_PKT_SCHED_H
 #define __LINUX_PKT_SCHED_H
 

commit 585d763af09cc21daf48ecc873604ccdb70f6014
Author: Vinicius Costa Gomes <vinicius.gomes@intel.com>
Date:   Mon Oct 16 18:01:26 2017 -0700

    net/sched: Introduce Credit Based Shaper (CBS) qdisc
    
    This queueing discipline implements the shaper algorithm defined by
    the 802.1Q-2014 Section 8.6.8.2 and detailed in Annex L.
    
    It's primary usage is to apply some bandwidth reservation to user
    defined traffic classes, which are mapped to different queues via the
    mqprio qdisc.
    
    Only a simple software implementation is added for now.
    
    Signed-off-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Tested-by: Henrik Austad <henrik@austad.us>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index e7cc3d3c7421..0e88cc262ca0 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -904,4 +904,23 @@ struct tc_pie_xstats {
 	__u32 maxq;             /* maximum queue size */
 	__u32 ecn_mark;         /* packets marked with ecn*/
 };
+
+/* CBS */
+struct tc_cbs_qopt {
+	__u8 offload;
+	__u8 _pad[3];
+	__s32 hicredit;
+	__s32 locredit;
+	__s32 idleslope;
+	__s32 sendslope;
+};
+
+enum {
+	TCA_CBS_UNSPEC,
+	TCA_CBS_PARMS,
+	__TCA_CBS_MAX,
+};
+
+#define TCA_CBS_MAX (__TCA_CBS_MAX - 1)
+
 #endif

commit 32302902ff093891d8e64439cbb8ceae83e21ef8
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Thu Oct 12 11:38:45 2017 -0700

    mqprio: Reserve last 32 classid values for HW traffic classes and misc IDs
    
    This patch makes a slight tweak to mqprio in order to bring the
    classid values used back in line with what is used for mq. The general idea
    is to reserve values :ffe0 - :ffef to identify hardware traffic classes
    normally reported via dev->num_tc. By doing this we can maintain a
    consistent behavior with mq for classid where :1 - :ffdf will represent a
    physical qdisc mapped onto a Tx queue represented by classid - 1, and the
    traffic classes will be mapped onto a known subset of classid values
    reserved for our virtual qdiscs.
    
    Note I reserved the range from :fff0 - :ffff since this way we might be
    able to reuse these classid values with clsact and ingress which would mean
    that for mq, mqprio, ingress, and clsact we should be able to maintain a
    similar classid layout.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index e95b5c9b9fad..e7cc3d3c7421 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -74,6 +74,7 @@ struct tc_estimator {
 #define TC_H_INGRESS    (0xFFFFFFF1U)
 #define TC_H_CLSACT	TC_H_INGRESS
 
+#define TC_H_MIN_PRIORITY	0xFFE0U
 #define TC_H_MIN_INGRESS	0xFFF2U
 #define TC_H_MIN_EGRESS		0xFFF3U
 

commit 4e8b86c062695454df0b76f3fee4fab8dc4bb716
Author: Amritha Nambiar <amritha.nambiar@intel.com>
Date:   Thu Sep 7 04:00:06 2017 -0700

    mqprio: Introduce new hardware offload mode and shaper in mqprio
    
    The offload types currently supported in mqprio are 0 (no offload) and
    1 (offload only TCs) by setting these values for the 'hw' option. If
    offloads are supported by setting the 'hw' option to 1, the default
    offload mode is 'dcb' where only the TC values are offloaded to the
    device. This patch introduces a new hardware offload mode called
    'channel' with 'hw' set to 1 in mqprio which makes full use of the
    mqprio options, the TCs, the queue configurations and the QoS parameters
    for the TCs. This is achieved through a new netlink attribute for the
    'mode' option which takes values such as 'dcb' (default) and 'channel'.
    The 'channel' mode also supports QoS attributes for traffic class such as
    minimum and maximum values for bandwidth rate limits.
    
    This patch enables configuring additional HW shaper attributes associated
    with a traffic class. Currently the shaper for bandwidth rate limiting is
    supported which takes options such as minimum and maximum bandwidth rates
    and are offloaded to the hardware in the 'channel' mode. The min and max
    limits for bandwidth rates are provided by the user along with the TCs
    and the queue configurations when creating the mqprio qdisc. The interface
    can be extended to support new HW shapers in future through the 'shaper'
    attribute.
    
    Introduces a new data structure 'tc_mqprio_qopt_offload' for offloading
    mqprio queue options and use this to be shared between the kernel and
    device driver. This contains a copy of the existing data structure
    for mqprio queue options. This new data structure can be extended when
    adding new attributes for traffic class such as mode, shaper, shaper
    parameters (bandwidth rate limits). The existing data structure for mqprio
    queue options will be shared between the kernel and userspace.
    
    Example:
      queues 4@0 4@4 hw 1 mode channel shaper bw_rlimit\
      min_rate 1Gbit 2Gbit max_rate 4Gbit 5Gbit
    
    To dump the bandwidth rates:
    
    qdisc mqprio 804a: root  tc 2 map 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0
                 queues:(0:3) (4:7)
                 mode:channel
                 shaper:bw_rlimit   min_rate:1Gbit 2Gbit   max_rate:4Gbit 5Gbit
    
    Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 099bf5528fed..e95b5c9b9fad 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -625,6 +625,22 @@ enum {
 
 #define TC_MQPRIO_HW_OFFLOAD_MAX (__TC_MQPRIO_HW_OFFLOAD_MAX - 1)
 
+enum {
+	TC_MQPRIO_MODE_DCB,
+	TC_MQPRIO_MODE_CHANNEL,
+	__TC_MQPRIO_MODE_MAX
+};
+
+#define __TC_MQPRIO_MODE_MAX (__TC_MQPRIO_MODE_MAX - 1)
+
+enum {
+	TC_MQPRIO_SHAPER_DCB,
+	TC_MQPRIO_SHAPER_BW_RATE,	/* Add new shapers below */
+	__TC_MQPRIO_SHAPER_MAX
+};
+
+#define __TC_MQPRIO_SHAPER_MAX (__TC_MQPRIO_SHAPER_MAX - 1)
+
 struct tc_mqprio_qopt {
 	__u8	num_tc;
 	__u8	prio_tc_map[TC_QOPT_BITMASK + 1];
@@ -633,6 +649,22 @@ struct tc_mqprio_qopt {
 	__u16	offset[TC_QOPT_MAX_QUEUE];
 };
 
+#define TC_MQPRIO_F_MODE		0x1
+#define TC_MQPRIO_F_SHAPER		0x2
+#define TC_MQPRIO_F_MIN_RATE		0x4
+#define TC_MQPRIO_F_MAX_RATE		0x8
+
+enum {
+	TCA_MQPRIO_UNSPEC,
+	TCA_MQPRIO_MODE,
+	TCA_MQPRIO_SHAPER,
+	TCA_MQPRIO_MIN_RATE64,
+	TCA_MQPRIO_MAX_RATE64,
+	__TCA_MQPRIO_MAX,
+};
+
+#define TCA_MQPRIO_MAX (__TCA_MQPRIO_MAX - 1)
+
 /* SFB */
 
 enum {

commit 2026fecf516bc04df20cb50874957cd8c364fb4e
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Wed Mar 15 10:39:18 2017 -0700

    mqprio: Change handling of hw u8 to allow for multiple hardware offload modes
    
    This patch is meant to allow for support of multiple hardware offload type
    for a single device. There is currently no bounds checking for the hw
    member of the mqprio_qopt structure.  This results in us being able to pass
    values from 1 to 255 with all being treated the same.  On retreiving the
    value it is returned as 1 for anything 1 or greater being set.
    
    With this change we are currently adding limited bounds checking by
    defining an enum and using those values to limit the reported hardware
    offloads.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index df7451d35131..099bf5528fed 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -617,6 +617,14 @@ struct tc_drr_stats {
 #define TC_QOPT_BITMASK 15
 #define TC_QOPT_MAX_QUEUE 16
 
+enum {
+	TC_MQPRIO_HW_OFFLOAD_NONE,	/* no offload requested */
+	TC_MQPRIO_HW_OFFLOAD_TCS,	/* offload TCs, no queue counts */
+	__TC_MQPRIO_HW_OFFLOAD_MAX
+};
+
+#define TC_MQPRIO_HW_OFFLOAD_MAX (__TC_MQPRIO_HW_OFFLOAD_MAX - 1)
+
 struct tc_mqprio_qopt {
 	__u8	num_tc;
 	__u8	prio_tc_map[TC_QOPT_BITMASK + 1];

commit fefa569a9d4bc4b7758c0fddd75bb0382c95da77
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 22 08:58:55 2016 -0700

    net_sched: sch_fq: account for schedule/timers drifts
    
    It looks like the following patch can make FQ very precise, even in VM
    or stressed hosts. It matters at high pacing rates.
    
    We take into account the difference between the time that was programmed
    when last packet was sent, and current time (a drift of tens of usecs is
    often observed)
    
    Add an EWMA of the unthrottle latency to help diagnostics.
    
    This latency is the difference between current time and oldest packet in
    delayed RB-tree. This accounts for the high resolution timer latency,
    but can be different under stress, as fq_check_throttled() can be
    opportunistically be called from a dequeue() called after an enqueue()
    for a different flow.
    
    Tested:
    // Start a 10Gbit flow
    $ netperf --google-pacing-rate 1250000000 -H lpaa24 -l 10000 -- -K bbr &
    
    Before patch :
    $ sar -n DEV 10 5 | grep eth0 | grep Average
    Average:         eth0  17106.04 756876.84   1102.75 1119049.02      0.00      0.00      0.52
    
    After patch :
    $ sar -n DEV 10 5 | grep eth0 | grep Average
    Average:         eth0  17867.00 800245.90   1151.77 1183172.12      0.00      0.00      0.52
    
    A new iproute2 tc can output the 'unthrottle latency' :
    
    $ tc -s qd sh dev eth0 | grep latency
      0 gc, 0 highprio, 32490767 throttled, 2382 ns latency
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index f8e39dbaa781..df7451d35131 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -811,7 +811,7 @@ struct tc_fq_qd_stats {
 	__u32	flows;
 	__u32	inactive_flows;
 	__u32	throttled_flows;
-	__u32	pad;
+	__u32	unthrottle_latency_ns;
 };
 
 /* Heavy-Hitter Filter */

commit 77879147a3481babffd7e368d977ab682545a6bd
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Sep 19 23:39:11 2016 -0400

    net_sched: sch_fq: add low_rate_threshold parameter
    
    This commit adds to the fq module a low_rate_threshold parameter to
    insert a delay after all packets if the socket requests a pacing rate
    below the threshold.
    
    This helps achieve more precise control of the sending rate with
    low-rate paths, especially policers. The basic issue is that if a
    congestion control module detects a policer at a certain rate, it may
    want fq to be able to shape to that policed rate. That way the sender
    can avoid policer drops by having the packets arrive at the policer at
    or just under the policed rate.
    
    The default threshold of 550Kbps was chosen analytically so that for
    policers or links at 500Kbps or 512Kbps fq would very likely invoke
    this mechanism, even if the pacing rate was briefly slightly above the
    available bandwidth. This value was then empirically validated with
    two years of production testing on YouTube video servers.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 2382eed50278..f8e39dbaa781 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -792,6 +792,8 @@ enum {
 
 	TCA_FQ_ORPHAN_MASK,	/* mask applied to orphaned skb hashes */
 
+	TCA_FQ_LOW_RATE_THRESHOLD, /* per packet delay under this rate */
+
 	__TCA_FQ_MAX
 };
 

commit 95b58430abe74f5e50970c57d27380bd5b8be324
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 6 08:55:12 2016 -0700

    fq_codel: add memory limitation per queue
    
    On small embedded routers, one wants to control maximal amount of
    memory used by fq_codel, instead of controlling number of packets or
    bytes, since GRO/TSO make these not practical.
    
    Assuming skb->truesize is accurate, we have to keep track of
    skb->truesize sum for skbs in queue.
    
    This patch adds a new TCA_FQ_CODEL_MEMORY_LIMIT attribute.
    
    I chose a default value of 32 MBytes, which looks reasonable even
    for heavy duty usages. (Prior fq_codel users should not be hurt
    when they upgrade their kernels)
    
    Two fields are added to tc_fq_codel_qd_stats to report :
     - Current memory usage
     - Number of drops caused by memory limits
    
    # tc qd replace dev eth1 root est 1sec 4sec fq_codel memory_limit 4M
    ..
    # tc -s -d qd sh dev eth1
    qdisc fq_codel 8008: root refcnt 257 limit 10240p flows 1024
     quantum 1514 target 5.0ms interval 100.0ms memory_limit 4Mb ecn
     Sent 2083566791363 bytes 1376214889 pkt (dropped 4994406, overlimits 0
    requeues 21705223)
     rate 9841Mbit 812549pps backlog 3906120b 376p requeues 21705223
      maxpacket 68130 drop_overlimit 4994406 new_flow_count 28855414
      ecn_mark 0 memory_used 4190048 drop_overmemory 4994406
      new_flows_len 1 old_flows_len 177
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Dave Täht <dave.taht@gmail.com>
    Cc: Sebastian Möller <moeller0@gmx.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index a11afecd4482..2382eed50278 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -719,6 +719,7 @@ enum {
 	TCA_FQ_CODEL_QUANTUM,
 	TCA_FQ_CODEL_CE_THRESHOLD,
 	TCA_FQ_CODEL_DROP_BATCH_SIZE,
+	TCA_FQ_CODEL_MEMORY_LIMIT,
 	__TCA_FQ_CODEL_MAX
 };
 
@@ -743,6 +744,8 @@ struct tc_fq_codel_qd_stats {
 	__u32	new_flows_len;	/* count of flows in new list */
 	__u32	old_flows_len;	/* count of flows in old list */
 	__u32	ce_mark;	/* packets above ce_threshold */
+	__u32	memory_usage;	/* in bytes */
+	__u32	drop_overmemory;
 };
 
 struct tc_fq_codel_cl_stats {

commit 9d18562a227874289fda8ca5d117d8f503f1dcca
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun May 1 16:47:26 2016 -0700

    fq_codel: add batch ability to fq_codel_drop()
    
    In presence of inelastic flows and stress, we can call
    fq_codel_drop() for every packet entering fq_codel qdisc.
    
    fq_codel_drop() is quite expensive, as it does a linear scan
    of 4 KB of memory to find a fat flow.
    Once found, it drops the oldest packet of this flow.
    
    Instead of dropping a single packet, try to drop 50% of the backlog
    of this fat flow, with a configurable limit of 64 packets per round.
    
    TCA_FQ_CODEL_DROP_BATCH_SIZE is the new attribute to make this
    limit configurable.
    
    With this strategy the 4 KB search is amortized to a single cache line
    per drop [1], so fq_codel_drop() no longer appears at the top of kernel
    profile in presence of few inelastic flows.
    
    [1] Assuming a 64byte cache line, and 1024 buckets
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Dave Taht <dave.taht@gmail.com>
    Cc: Jonathan Morton <chromatix99@gmail.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Dave Taht
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 1c78c7454c7c..a11afecd4482 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -718,6 +718,7 @@ enum {
 	TCA_FQ_CODEL_FLOWS,
 	TCA_FQ_CODEL_QUANTUM,
 	TCA_FQ_CODEL_CE_THRESHOLD,
+	TCA_FQ_CODEL_DROP_BATCH_SIZE,
 	__TCA_FQ_CODEL_MAX
 };
 

commit 2a51c1e8ecdcedfcb6f84efb3756822d0d0dfb36
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Mon Apr 25 10:25:15 2016 +0200

    sched: use nla_put_u64_64bit()
    
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 8cb18b44968e..1c78c7454c7c 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -179,6 +179,7 @@ enum {
 	TCA_TBF_PRATE64,
 	TCA_TBF_BURST,
 	TCA_TBF_PBURST,
+	TCA_TBF_PAD,
 	__TCA_TBF_MAX,
 };
 
@@ -368,6 +369,7 @@ enum {
 	TCA_HTB_DIRECT_QLEN,
 	TCA_HTB_RATE64,
 	TCA_HTB_CEIL64,
+	TCA_HTB_PAD,
 	__TCA_HTB_MAX,
 };
 
@@ -531,6 +533,7 @@ enum {
 	TCA_NETEM_RATE,
 	TCA_NETEM_ECN,
 	TCA_NETEM_RATE64,
+	TCA_NETEM_PAD,
 	__TCA_NETEM_MAX,
 };
 

commit 1f211a1b929c804100e138c5d3d656992cfd5622
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jan 7 22:29:47 2016 +0100

    net, sched: add clsact qdisc
    
    This work adds a generalization of the ingress qdisc as a qdisc holding
    only classifiers. The clsact qdisc works on ingress, but also on egress.
    In both cases, it's execution happens without taking the qdisc lock, and
    the main difference for the egress part compared to prior version of [1]
    is that this can be applied with _any_ underlying real egress qdisc (also
    classless ones).
    
    Besides solving the use-case of [1], that is, allowing for more programmability
    on assigning skb->priority for the mqprio case that is supported by most
    popular 10G+ NICs, it also opens up a lot more flexibility for other tc
    applications. The main work on classification can already be done at clsact
    egress time if the use-case allows and state stored for later retrieval
    f.e. again in skb->priority with major/minors (which is checked by most
    classful qdiscs before consulting tc_classify()) and/or in other skb fields
    like skb->tc_index for some light-weight post-processing to get to the
    eventual classid in case of a classful qdisc. Another use case is that
    the clsact egress part allows to have a central egress counterpart to
    the ingress classifiers, so that classifiers can easily share state (e.g.
    in cls_bpf via eBPF maps) for ingress and egress.
    
    Currently, default setups like mq + pfifo_fast would require for this to
    use, for example, prio qdisc instead (to get a tc_classify() run) and to
    duplicate the egress classifier for each queue. With clsact, it allows
    for leaving the setup as is, it can additionally assign skb->priority to
    put the skb in one of pfifo_fast's bands and it can share state with maps.
    Moreover, we can access the skb's dst entry (f.e. to retrieve tclassid)
    w/o the need to perform a skb_dst_force() to hold on to it any longer. In
    lwt case, we can also use this facility to setup dst metadata via cls_bpf
    (bpf_skb_set_tunnel_key()) without needing a real egress qdisc just for
    that (case of IFF_NO_QUEUE devices, for example).
    
    The realization can be done without any changes to the scheduler core
    framework. All it takes is that we have two a-priori defined minors/child
    classes, where we can mux between ingress and egress classifier list
    (dev->ingress_cl_list and dev->egress_cl_list, latter stored close to
    dev->_tx to avoid extra cacheline miss for moderate loads). The egress
    part is a bit similar modelled to handle_ing() and patched to a noop in
    case the functionality is not used. Both handlers are now called
    sch_handle_ingress() and sch_handle_egress(), code sharing among the two
    doesn't seem practical as there are various minor differences in both
    paths, so that making them conditional in a single handler would rather
    slow things down.
    
    Full compatibility to ingress qdisc is provided as well. Since both
    piggyback on TC_H_CLSACT, only one of them (ingress/clsact) can exist
    per netdevice, and thus ingress qdisc specific behaviour can be retained
    for user space. This means, either a user does 'tc qdisc add dev foo ingress'
    and configures ingress qdisc as usual, or the 'tc qdisc add dev foo clsact'
    alternative, where both, ingress and egress classifier can be configured
    as in the below example. ingress qdisc supports attaching classifier to any
    minor number whereas clsact has two fixed minors for muxing between the
    lists, therefore to not break user space setups, they are better done as
    two separate qdiscs.
    
    I decided to extend the sch_ingress module with clsact functionality so
    that commonly used code can be reused, the module is being aliased with
    sch_clsact so that it can be auto-loaded properly. Alternative would have been
    to add a flag when initializing ingress to alter its behaviour plus aliasing
    to a different name (as it's more than just ingress). However, the first would
    end up, based on the flag, choosing the new/old behaviour by calling different
    function implementations to handle each anyway, the latter would require to
    register ingress qdisc once again under different alias. So, this really begs
    to provide a minimal, cleaner approach to have Qdisc_ops and Qdisc_class_ops
    by its own that share callbacks used by both.
    
    Example, adding qdisc:
    
       # tc qdisc add dev foo clsact
       # tc qdisc show dev foo
       qdisc mq 0: root
       qdisc pfifo_fast 0: parent :1 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
       qdisc pfifo_fast 0: parent :2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
       qdisc pfifo_fast 0: parent :3 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
       qdisc pfifo_fast 0: parent :4 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
       qdisc clsact ffff: parent ffff:fff1
    
    Adding filters (deleting, etc works analogous by specifying ingress/egress):
    
       # tc filter add dev foo ingress bpf da obj bar.o sec ingress
       # tc filter add dev foo egress  bpf da obj bar.o sec egress
       # tc filter show dev foo ingress
       filter protocol all pref 49152 bpf
       filter protocol all pref 49152 bpf handle 0x1 bar.o:[ingress] direct-action
       # tc filter show dev foo egress
       filter protocol all pref 49152 bpf
       filter protocol all pref 49152 bpf handle 0x1 bar.o:[egress] direct-action
    
    A 'tc filter show dev foo' or 'tc filter show dev foo parent ffff:' will
    show an empty list for clsact. Either using the parent names (ingress/egress)
    or specifying the full major/minor will then show the related filter lists.
    
    Prior work on a mqprio prequeue() facility [1] was done mainly by John Fastabend.
    
      [1] http://patchwork.ozlabs.org/patch/512949/
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.r.fastabend@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 8d2530daca9f..8cb18b44968e 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -72,6 +72,10 @@ struct tc_estimator {
 #define TC_H_UNSPEC	(0U)
 #define TC_H_ROOT	(0xFFFFFFFFU)
 #define TC_H_INGRESS    (0xFFFFFFF1U)
+#define TC_H_CLSACT	TC_H_INGRESS
+
+#define TC_H_MIN_INGRESS	0xFFF2U
+#define TC_H_MIN_EGRESS		0xFFF3U
 
 /* Need to corrospond to iproute2 tc/tc_core.h "enum link_layer" */
 enum tc_link_layer {

commit a3eb95f891d6130b1fc03dd07a8b54cf0a5c8ab8
Author: David Ward <david.ward@ll.mit.edu>
Date:   Sat May 9 22:01:46 2015 -0400

    net_sched: gred: add TCA_GRED_LIMIT attribute
    
    In a GRED qdisc, if the default "virtual queue" (VQ) does not have drop
    parameters configured, then packets for the default VQ are not subjected
    to RED and are only dropped if the queue is larger than the net_device's
    tx_queue_len. This behavior is useful for WRED mode, since these packets
    will still influence the calculated average queue length and (therefore)
    the drop probability for all of the other VQs. However, for some drivers
    tx_queue_len is zero. In other cases the user may wish to make the limit
    the same for all VQs (including the default VQ with no drop parameters).
    
    This change adds a TCA_GRED_LIMIT attribute to set the GRED queue limit,
    in bytes, during qdisc setup. (This limit is in bytes to be consistent
    with the drop parameters.) The default limit is the same as for a bfifo
    queue (tx_queue_len * psched_mtu). If the drop parameters of any VQ are
    configured with a smaller limit than the GRED queue limit, that VQ will
    still observe the smaller limit instead.
    
    Signed-off-by: David Ward <david.ward@ll.mit.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 69d88b309cc7..8d2530daca9f 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -268,7 +268,8 @@ enum {
        TCA_GRED_STAB,
        TCA_GRED_DPS,
        TCA_GRED_MAX_P,
-	   __TCA_GRED_MAX,
+       TCA_GRED_LIMIT,
+       __TCA_GRED_MAX,
 };
 
 #define TCA_GRED_MAX (__TCA_GRED_MAX - 1)

commit 80ba92fa1a92dea128283f69f55b02242e213650
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 8 15:05:12 2015 -0700

    codel: add ce_threshold attribute
    
    For DCTCP or similar ECN based deployments on fabrics with shallow
    buffers, hosts are responsible for a good part of the buffering.
    
    This patch adds an optional ce_threshold to codel & fq_codel qdiscs,
    so that DCTCP can have feedback from queuing in the host.
    
    A DCTCP enabled egress port simply have a queue occupancy threshold
    above which ECT packets get CE mark.
    
    In codel language this translates to a sojourn time, so that one doesn't
    have to worry about bytes or bandwidth but delays.
    
    This makes the host an active participant in the health of the whole
    network.
    
    This also helps experimenting DCTCP in a setup without DCTCP compliant
    fabric.
    
    On following example, ce_threshold is set to 1ms, and we can see from
    'ldelay xxx us' that TCP is not trying to go around the 5ms codel
    target.
    
    Queue has more capacity to absorb inelastic bursts (say from UDP
    traffic), as queues are maintained to an optimal level.
    
    lpaa23:~# ./tc -s -d qd sh dev eth1
    qdisc mq 1: dev eth1 root
     Sent 87910654696 bytes 58065331 pkt (dropped 0, overlimits 0 requeues 42961)
     backlog 3108242b 364p requeues 42961
    qdisc codel 8063: dev eth1 parent 1:1 limit 1000p target 5.0ms ce_threshold 1.0ms interval 100.0ms
     Sent 7363778701 bytes 4863809 pkt (dropped 0, overlimits 0 requeues 5503)
     rate 2348Mbit 193919pps backlog 255866b 46p requeues 5503
      count 0 lastcount 0 ldelay 1.0ms drop_next 0us
      maxpacket 68130 ecn_mark 0 drop_overlimit 0 ce_mark 72384
    qdisc codel 8064: dev eth1 parent 1:2 limit 1000p target 5.0ms ce_threshold 1.0ms interval 100.0ms
     Sent 7636486190 bytes 5043942 pkt (dropped 0, overlimits 0 requeues 5186)
     rate 2319Mbit 191538pps backlog 207418b 64p requeues 5186
      count 0 lastcount 0 ldelay 694us drop_next 0us
      maxpacket 68130 ecn_mark 0 drop_overlimit 0 ce_mark 69873
    qdisc codel 8065: dev eth1 parent 1:3 limit 1000p target 5.0ms ce_threshold 1.0ms interval 100.0ms
     Sent 11569360142 bytes 7641602 pkt (dropped 0, overlimits 0 requeues 5554)
     rate 3041Mbit 251096pps backlog 210446b 59p requeues 5554
      count 0 lastcount 0 ldelay 889us drop_next 0us
      maxpacket 68130 ecn_mark 0 drop_overlimit 0 ce_mark 37780
    ...
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Florian Westphal <fw@strlen.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Glenn Judd <glenn.judd@morganstanley.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 534b84710745..69d88b309cc7 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -679,6 +679,7 @@ enum {
 	TCA_CODEL_LIMIT,
 	TCA_CODEL_INTERVAL,
 	TCA_CODEL_ECN,
+	TCA_CODEL_CE_THRESHOLD,
 	__TCA_CODEL_MAX
 };
 
@@ -695,6 +696,7 @@ struct tc_codel_xstats {
 	__u32	drop_overlimit; /* number of time max qdisc packet limit was hit */
 	__u32	ecn_mark;  /* number of packets we ECN marked instead of dropped */
 	__u32	dropping;  /* are we in dropping state ? */
+	__u32	ce_mark;   /* number of CE marked packets because of ce_threshold */
 };
 
 /* FQ_CODEL */
@@ -707,6 +709,7 @@ enum {
 	TCA_FQ_CODEL_ECN,
 	TCA_FQ_CODEL_FLOWS,
 	TCA_FQ_CODEL_QUANTUM,
+	TCA_FQ_CODEL_CE_THRESHOLD,
 	__TCA_FQ_CODEL_MAX
 };
 
@@ -730,6 +733,7 @@ struct tc_fq_codel_qd_stats {
 				 */
 	__u32	new_flows_len;	/* count of flows in new list */
 	__u32	old_flows_len;	/* count of flows in old list */
+	__u32	ce_mark;	/* packets above ce_threshold */
 };
 
 struct tc_fq_codel_cl_stats {

commit 06eb395fa9856b5a87cf7d80baee2a0ed3cdb9d7
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Feb 4 21:30:40 2015 -0800

    pkt_sched: fq: better control of DDOS traffic
    
    FQ has a fast path for skb attached to a socket, as it does not
    have to compute a flow hash. But for other packets, FQ being non
    stochastic means that hosts exposed to random Internet traffic
    can allocate million of flows structure (104 bytes each) pretty
    easily. Not only host can OOM, but lookup in RB trees can take
    too much cpu and memory resources.
    
    This patch adds a new attribute, orphan_mask, that is adding
    possibility of having a stochastic hash for orphaned skb.
    
    Its default value is 1024 slots, to mimic SFQ behavior.
    
    Note: This does not apply to locally generated TCP traffic,
    and no locally generated traffic will share a flow structure
    with another perfect or stochastic flow.
    
    This patch also handles the specific case of SYNACK messages:
    
    They are attached to the listener socket, and therefore all map
    to a single hash bucket. If listener have set SO_MAX_PACING_RATE,
    hoping to have new accepted socket inherit this rate, SYNACK
    might be paced and even dropped.
    
    This is very similar to an internal patch Google have used more
    than one year.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index d62316baae94..534b84710745 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -774,6 +774,8 @@ enum {
 
 	TCA_FQ_FLOW_REFILL_DELAY,	/* flow credit refill delay in usec */
 
+	TCA_FQ_ORPHAN_MASK,	/* mask applied to orphaned skb hashes */
+
 	__TCA_FQ_MAX
 };
 

commit d4b36210c2e6ecef0ce52fb6c18c51144f5c2d88
Author: Vijay Subramanian <vijaynsu@cisco.com>
Date:   Sat Jan 4 17:33:55 2014 -0800

    net: pkt_sched: PIE AQM scheme
    
    Proportional Integral controller Enhanced (PIE) is a scheduler to address the
    bufferbloat problem.
    
    >From the IETF draft below:
    " Bufferbloat is a phenomenon where excess buffers in the network cause high
    latency and jitter. As more and more interactive applications (e.g. voice over
    IP, real time video streaming and financial transactions) run in the Internet,
    high latency and jitter degrade application performance. There is a pressing
    need to design intelligent queue management schemes that can control latency and
    jitter; and hence provide desirable quality of service to users.
    
    We present here a lightweight design, PIE(Proportional Integral controller
    Enhanced) that can effectively control the average queueing latency to a target
    value. Simulation results, theoretical analysis and Linux testbed results have
    shown that PIE can ensure low latency and achieve high link utilization under
    various congestion situations. The design does not require per-packet
    timestamp, so it incurs very small overhead and is simple enough to implement
    in both hardware and software.  "
    
    Many thanks to Dave Taht for extensive feedback, reviews, testing and
    suggestions. Thanks also to Stephen Hemminger and Eric Dumazet for reviews and
    suggestions.  Naeem Khademi and Dave Taht independently contributed to ECN
    support.
    
    For more information, please see technical paper about PIE in the IEEE
    Conference on High Performance Switching and Routing 2013. A copy of the paper
    can be found at ftp://ftpeng.cisco.com/pie/.
    
    Please also refer to the IETF draft submission at
    http://tools.ietf.org/html/draft-pan-tsvwg-pie-00
    
    All relevant code, documents and test scripts and results can be found at
    ftp://ftpeng.cisco.com/pie/.
    
    For problems with the iproute2/tc or Linux kernel code, please contact Vijay
    Subramanian (vijaynsu@cisco.com or subramanian.vijay@gmail.com) Mythili Prabhu
    (mysuryan@cisco.com)
    
    Signed-off-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: Mythili Prabhu <mysuryan@cisco.com>
    CC: Dave Taht <dave.taht@bufferbloat.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 77eb331810b8..d62316baae94 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -818,4 +818,29 @@ struct tc_hhf_xstats {
 	__u32	hh_tot_count;   /* number of captured heavy-hitters so far */
 	__u32	hh_cur_count;   /* number of current heavy-hitters */
 };
+
+/* PIE */
+enum {
+	TCA_PIE_UNSPEC,
+	TCA_PIE_TARGET,
+	TCA_PIE_LIMIT,
+	TCA_PIE_TUPDATE,
+	TCA_PIE_ALPHA,
+	TCA_PIE_BETA,
+	TCA_PIE_ECN,
+	TCA_PIE_BYTEMODE,
+	__TCA_PIE_MAX
+};
+#define TCA_PIE_MAX   (__TCA_PIE_MAX - 1)
+
+struct tc_pie_xstats {
+	__u32 prob;             /* current probability */
+	__u32 delay;            /* current delay in ms */
+	__u32 avg_dq_rate;      /* current average dq_rate in bits/pie_time */
+	__u32 packets_in;       /* total number of packets enqueued */
+	__u32 dropped;          /* packets dropped due to pie_action */
+	__u32 overlimit;        /* dropped due to lack of space in queue */
+	__u32 maxq;             /* maximum queue size */
+	__u32 ecn_mark;         /* packets marked with ecn*/
+};
 #endif

commit 6a031f67c83aa175aedd10d4ae64750415ab57b0
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Wed Dec 25 17:35:15 2013 +0800

    sch_netem: support of 64bit rates
    
    Add a new attribute to support 64bit rates so that
    tc can use them to break the 32bit limit.
    
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index fe1192056fd1..77eb331810b8 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -525,6 +525,7 @@ enum {
 	TCA_NETEM_LOSS,
 	TCA_NETEM_RATE,
 	TCA_NETEM_ECN,
+	TCA_NETEM_RATE64,
 	__TCA_NETEM_MAX,
 };
 

commit 2e04ad424b03661ec8239acd52146497eb33be1c
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Fri Dec 20 09:24:47 2013 +0800

    sch_tbf: add TBF_BURST/TBF_PBURST attribute
    
    When we set burst to 1514 with low rate in userspace,
    the kernel get a value of burst that less than 1514,
    which doesn't work.
    
    Because it may make some loss when transform burst
    to buffer in userspace. This makes burst lose some
    bytes, when the kernel transform the buffer back to
    burst.
    
    This patch adds two new attributes to support sending
    burst/mtu to kernel directly to avoid the loss.
    
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 4566993b8385..fe1192056fd1 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -173,6 +173,8 @@ enum {
 	TCA_TBF_PTAB,
 	TCA_TBF_RATE64,
 	TCA_TBF_PRATE64,
+	TCA_TBF_BURST,
+	TCA_TBF_PBURST,
 	__TCA_TBF_MAX,
 };
 

commit 10239edf86f137ce4c39b62ea9575e8053c549a0
Author: Terry Lam <vtlam@google.com>
Date:   Sun Dec 15 00:30:21 2013 -0800

    net-qdisc-hhf: Heavy-Hitter Filter (HHF) qdisc
    
    This patch implements the first size-based qdisc that attempts to
    differentiate between small flows and heavy-hitters.  The goal is to
    catch the heavy-hitters and move them to a separate queue with less
    priority so that bulk traffic does not affect the latency of critical
    traffic.  Currently "less priority" means less weight (2:1 in
    particular) in a Weighted Deficit Round Robin (WDRR) scheduler.
    
    In essence, this patch addresses the "delay-bloat" problem due to
    bloated buffers. In some systems, large queues may be necessary for
    obtaining CPU efficiency, or due to the presence of unresponsive
    traffic like UDP, or just a large number of connections with each
    having a small amount of outstanding traffic. In these circumstances,
    HHF aims to reduce the HoL blocking for latency sensitive traffic,
    while not impacting the queues built up by bulk traffic.  HHF can also
    be used in conjunction with other AQM mechanisms such as CoDel.
    
    To capture heavy-hitters, we implement the "multi-stage filter" design
    in the following paper:
    C. Estan and G. Varghese, "New Directions in Traffic Measurement and
    Accounting", in ACM SIGCOMM, 2002.
    
    Some configurable qdisc settings through 'tc':
    - hhf_reset_timeout: period to reset counter values in the multi-stage
                         filter (default 40ms)
    - hhf_admit_bytes:   threshold to classify heavy-hitters
                         (default 128KB)
    - hhf_evict_timeout: threshold to evict idle heavy-hitters
                         (default 1s)
    - hhf_non_hh_weight: Weighted Deficit Round Robin (WDRR) weight for
                         non-heavy-hitters (default 2)
    - hh_flows_limit:    max number of heavy-hitter flow entries
                         (default 2048)
    
    Note that the ratio between hhf_admit_bytes and hhf_reset_timeout
    reflects the bandwidth of heavy-hitters that we attempt to capture
    (25Mbps with the above default settings).
    
    The false negative rate (heavy-hitter flows getting away unclassified)
    is zero by the design of the multi-stage filter algorithm.
    With 100 heavy-hitter flows, using four hashes and 4000 counters yields
    a false positive rate (non-heavy-hitters mistakenly classified as
    heavy-hitters) of less than 1e-4.
    
    Signed-off-by: Terry Lam <vtlam@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index a806687ad98f..4566993b8385 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -790,4 +790,29 @@ struct tc_fq_qd_stats {
 	__u32	throttled_flows;
 	__u32	pad;
 };
+
+/* Heavy-Hitter Filter */
+
+enum {
+	TCA_HHF_UNSPEC,
+	TCA_HHF_BACKLOG_LIMIT,
+	TCA_HHF_QUANTUM,
+	TCA_HHF_HH_FLOWS_LIMIT,
+	TCA_HHF_RESET_TIMEOUT,
+	TCA_HHF_ADMIT_BYTES,
+	TCA_HHF_EVICT_TIMEOUT,
+	TCA_HHF_NON_HH_WEIGHT,
+	__TCA_HHF_MAX
+};
+
+#define TCA_HHF_MAX	(__TCA_HHF_MAX - 1)
+
+struct tc_hhf_xstats {
+	__u32	drop_overlimit; /* number of times max qdisc packet limit
+				 * was hit
+				 */
+	__u32	hh_overlimit;   /* number of times max heavy-hitters was hit */
+	__u32	hh_tot_count;   /* number of captured heavy-hitters so far */
+	__u32	hh_cur_count;   /* number of current heavy-hitters */
+};
 #endif

commit f52ed89971adbe79b6438c459814034707b8ab91
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Nov 15 08:58:14 2013 -0800

    pkt_sched: fq: fix pacing for small frames
    
    For performance reasons, sch_fq tried hard to not setup timers for every
    sent packet, using a quantum based heuristic : A delay is setup only if
    the flow exhausted its credit.
    
    Problem is that application limited flows can refill their credit
    for every queued packet, and they can evade pacing.
    
    This problem can also be triggered when TCP flows use small MSS values,
    as TSO auto sizing builds packets that are smaller than the default fq
    quantum (3028 bytes)
    
    This patch adds a 40 ms delay to guard flow credit refill.
    
    Fixes: afe4fd062416 ("pkt_sched: fq: Fair Queue packet scheduler")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Maciej Żenczykowski <maze@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 885001b62c83..a806687ad98f 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -768,6 +768,9 @@ enum {
 	TCA_FQ_FLOW_MAX_RATE,	/* per flow max rate */
 
 	TCA_FQ_BUCKETS_LOG,	/* log2(number of buckets) */
+
+	TCA_FQ_FLOW_REFILL_DELAY,	/* flow credit refill delay in usec */
+
 	__TCA_FQ_MAX
 };
 

commit 65c5189a2b57b9aa1d89e4b79da39928257c9505
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Nov 15 08:57:26 2013 -0800

    pkt_sched: fq: warn users using defrate
    
    Commit 7eec4174ff29 ("pkt_sched: fq: fix non TCP flows pacing")
    obsoleted TCA_FQ_FLOW_DEFAULT_RATE without notice for the users.
    
    Suggested by David Miller
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 307f293477e8..885001b62c83 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -763,9 +763,7 @@ enum {
 
 	TCA_FQ_RATE_ENABLE,	/* enable/disable rate limiting */
 
-	TCA_FQ_FLOW_DEFAULT_RATE,/* for sockets with unspecified sk_rate,
-				  * use the following rate
-				  */
+	TCA_FQ_FLOW_DEFAULT_RATE,/* obsolete, do not use */
 
 	TCA_FQ_FLOW_MAX_RATE,	/* per flow max rate */
 

commit a33c4a2663c19ac01e557d6b78806271eec2a150
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Fri Nov 8 10:23:34 2013 +0800

    net_sched: tbf: support of 64bit rates
    
    With psched_ratecfg_precompute(), tbf can deal with 64bit rates.
    Add two new attributes so that tc can use them to break the 32bit
    limit.
    
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Suggested-by: Sergei Shtylyov <sergei.shtylyov@cogentembedded.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index f2624b549e61..307f293477e8 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -171,6 +171,8 @@ enum {
 	TCA_TBF_PARMS,
 	TCA_TBF_RTAB,
 	TCA_TBF_PTAB,
+	TCA_TBF_RATE64,
+	TCA_TBF_PRATE64,
 	__TCA_TBF_MAX,
 };
 

commit df62cdf348c91baac61b4cb19d19ea1ef87b271e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 19 09:10:20 2013 -0700

    net_sched: htb: support of 64bit rates
    
    HTB already can deal with 64bit rates, we only have to add two new
    attributes so that tc can use them to break the current 32bit ABI
    barrier.
    
    TCA_HTB_RATE64 : class rate  (in bytes per second)
    TCA_HTB_CEIL64 : class ceil  (in bytes per second)
    
    This allows us to setup HTB on 40Gbps links, as 32bit limit is
    actually ~34Gbps
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 9b829134d422..f2624b549e61 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -357,6 +357,8 @@ enum {
 	TCA_HTB_CTAB,
 	TCA_HTB_RTAB,
 	TCA_HTB_DIRECT_QLEN,
+	TCA_HTB_RATE64,
+	TCA_HTB_CEIL64,
 	__TCA_HTB_MAX,
 };
 

commit afe4fd062416b158a8a8538b23adc1930a9b88dc
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Aug 29 15:49:55 2013 -0700

    pkt_sched: fq: Fair Queue packet scheduler
    
    - Uses perfect flow match (not stochastic hash like SFQ/FQ_codel)
    - Uses the new_flow/old_flow separation from FQ_codel
    - New flows get an initial credit allowing IW10 without added delay.
    - Special FIFO queue for high prio packets (no need for PRIO + FQ)
    - Uses a hash table of RB trees to locate the flows at enqueue() time
    - Smart on demand gc (at enqueue() time, RB tree lookup evicts old
      unused flows)
    - Dynamic memory allocations.
    - Designed to allow millions of concurrent flows per Qdisc.
    - Small memory footprint : ~8K per Qdisc, and 104 bytes per flow.
    - Single high resolution timer for throttled flows (if any).
    - One RB tree to link throttled flows.
    - Ability to have a max rate per flow. We might add a socket option
      to add per socket limitation.
    
    Attempts have been made to add TCP pacing in TCP stack, but this
    seems to add complex code to an already complex stack.
    
    TCP pacing is welcomed for flows having idle times, as the cwnd
    permits TCP stack to queue a possibly large number of packets.
    
    This removes the 'slow start after idle' choice, hitting badly
    large BDP flows, and applications delivering chunks of data
    as video streams.
    
    Nicely spaced packets :
    Here interface is 10Gbit, but flow bottleneck is ~20Mbit
    
    cwin is big, yet FQ avoids the typical bursts generated by TCP
    (as in netperf TCP_RR -- -r 100000,100000)
    
    15:01:23.545279 IP A > B: . 78193:81089(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.545394 IP B > A: . ack 81089 win 3668 <nop,nop,timestamp 11597985 1115>
    15:01:23.546488 IP A > B: . 81089:83985(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.546565 IP B > A: . ack 83985 win 3668 <nop,nop,timestamp 11597986 1115>
    15:01:23.547713 IP A > B: . 83985:86881(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.547778 IP B > A: . ack 86881 win 3668 <nop,nop,timestamp 11597987 1115>
    15:01:23.548911 IP A > B: . 86881:89777(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.548949 IP B > A: . ack 89777 win 3668 <nop,nop,timestamp 11597988 1115>
    15:01:23.550116 IP A > B: . 89777:92673(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.550182 IP B > A: . ack 92673 win 3668 <nop,nop,timestamp 11597989 1115>
    15:01:23.551333 IP A > B: . 92673:95569(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.551406 IP B > A: . ack 95569 win 3668 <nop,nop,timestamp 11597991 1115>
    15:01:23.552539 IP A > B: . 95569:98465(2896) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.552576 IP B > A: . ack 98465 win 3668 <nop,nop,timestamp 11597992 1115>
    15:01:23.553756 IP A > B: . 98465:99913(1448) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.554138 IP A > B: P 99913:100001(88) ack 65248 win 3125 <nop,nop,timestamp 1115 11597805>
    15:01:23.554204 IP B > A: . ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.554234 IP B > A: . 65248:68144(2896) ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.555620 IP B > A: . 68144:71040(2896) ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.557005 IP B > A: . 71040:73936(2896) ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.558390 IP B > A: . 73936:76832(2896) ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.559773 IP B > A: . 76832:79728(2896) ack 100001 win 3668 <nop,nop,timestamp 11597993 1115>
    15:01:23.561158 IP B > A: . 79728:82624(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.562543 IP B > A: . 82624:85520(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.563928 IP B > A: . 85520:88416(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.565313 IP B > A: . 88416:91312(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.566698 IP B > A: . 91312:94208(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.568083 IP B > A: . 94208:97104(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.569467 IP B > A: . 97104:100000(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.570852 IP B > A: . 100000:102896(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.572237 IP B > A: . 102896:105792(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.573639 IP B > A: . 105792:108688(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.575024 IP B > A: . 108688:111584(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.576408 IP B > A: . 111584:114480(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    15:01:23.577793 IP B > A: . 114480:117376(2896) ack 100001 win 3668 <nop,nop,timestamp 11597994 1115>
    
    TCP timestamps show that most packets from B were queued in the same ms
    timeframe (TSval 1159799{3,4}), but FQ managed to send them right
    in time to avoid a big burst.
    
    In slow start or steady state, very few packets are throttled [1]
    
    FQ gets a bunch of tunables as :
    
      limit : max number of packets on whole Qdisc (default 10000)
    
      flow_limit : max number of packets per flow (default 100)
    
      quantum : the credit per RR round (default is 2 MTU)
    
      initial_quantum : initial credit for new flows (default is 10 MTU)
    
      maxrate : max per flow rate (default : unlimited)
    
      buckets : number of RB trees (default : 1024) in hash table.
                   (consumes 8 bytes per bucket)
    
      [no]pacing : disable/enable pacing (default is enable)
    
    All of them can be changed on a live qdisc.
    
    $ tc qd add dev eth0 root fq help
    Usage: ... fq [ limit PACKETS ] [ flow_limit PACKETS ]
                  [ quantum BYTES ] [ initial_quantum BYTES ]
                  [ maxrate RATE  ] [ buckets NUMBER ]
                  [ [no]pacing ]
    
    $ tc -s -d qd
    qdisc fq 8002: dev eth0 root refcnt 32 limit 10000p flow_limit 100p buckets 256 quantum 3028 initial_quantum 15140
     Sent 216532416 bytes 148395 pkt (dropped 0, overlimits 0 requeues 14)
     backlog 0b 0p requeues 14
      511 flows, 511 inactive, 0 throttled
      110 gc, 0 highprio, 0 retrans, 1143 throttled, 0 flows_plimit
    
    [1] Except if initial srtt is overestimated, as if using
    cached srtt in tcp metrics. We'll provide a fix for this issue.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 09d62b9228ff..9b829134d422 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -744,4 +744,45 @@ struct tc_fq_codel_xstats {
 	};
 };
 
+/* FQ */
+
+enum {
+	TCA_FQ_UNSPEC,
+
+	TCA_FQ_PLIMIT,		/* limit of total number of packets in queue */
+
+	TCA_FQ_FLOW_PLIMIT,	/* limit of packets per flow */
+
+	TCA_FQ_QUANTUM,		/* RR quantum */
+
+	TCA_FQ_INITIAL_QUANTUM,		/* RR quantum for new flow */
+
+	TCA_FQ_RATE_ENABLE,	/* enable/disable rate limiting */
+
+	TCA_FQ_FLOW_DEFAULT_RATE,/* for sockets with unspecified sk_rate,
+				  * use the following rate
+				  */
+
+	TCA_FQ_FLOW_MAX_RATE,	/* per flow max rate */
+
+	TCA_FQ_BUCKETS_LOG,	/* log2(number of buckets) */
+	__TCA_FQ_MAX
+};
+
+#define TCA_FQ_MAX	(__TCA_FQ_MAX - 1)
+
+struct tc_fq_qd_stats {
+	__u64	gc_flows;
+	__u64	highprio_packets;
+	__u64	tcp_retrans;
+	__u64	throttled;
+	__u64	flows_plimit;
+	__u64	pkts_too_long;
+	__u64	allocation_errors;
+	__s64	time_next_delayed_flow;
+	__u32	flows;
+	__u32	inactive_flows;
+	__u32	throttled_flows;
+	__u32	pad;
+};
 #endif

commit 8a8e3d84b1719a56f9151909e80ea6ebc5b8e318
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Wed Aug 14 23:47:11 2013 +0200

    net_sched: restore "linklayer atm" handling
    
    commit 56b765b79 ("htb: improved accuracy at high rates")
    broke the "linklayer atm" handling.
    
     tc class add ... htb rate X ceil Y linklayer atm
    
    The linklayer setting is implemented by modifying the rate table
    which is send to the kernel.  No direct parameter were
    transferred to the kernel indicating the linklayer setting.
    
    The commit 56b765b79 ("htb: improved accuracy at high rates")
    removed the use of the rate table system.
    
    To keep compatible with older iproute2 utils, this patch detects
    the linklayer by parsing the rate table.  It also supports future
    versions of iproute2 to send this linklayer parameter to the
    kernel directly. This is done by using the __reserved field in
    struct tc_ratespec, to convey the choosen linklayer option, but
    only using the lower 4 bits of this field.
    
    Linklayer detection is limited to speeds below 100Mbit/s, because
    at high rates the rtab is gets too inaccurate, so bad that
    several fields contain the same values, this resembling the ATM
    detect.  Fields even start to contain "0" time to send, e.g. at
    1000Mbit/s sending a 96 bytes packet cost "0", thus the rtab have
    been more broken than we first realized.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index dbd71b0c7d8c..09d62b9228ff 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -73,9 +73,17 @@ struct tc_estimator {
 #define TC_H_ROOT	(0xFFFFFFFFU)
 #define TC_H_INGRESS    (0xFFFFFFF1U)
 
+/* Need to corrospond to iproute2 tc/tc_core.h "enum link_layer" */
+enum tc_link_layer {
+	TC_LINKLAYER_UNAWARE, /* Indicate unaware old iproute2 util */
+	TC_LINKLAYER_ETHERNET,
+	TC_LINKLAYER_ATM,
+};
+#define TC_LINKLAYER_MASK 0x0F /* limit use to lower 4 bits */
+
 struct tc_ratespec {
 	unsigned char	cell_log;
-	unsigned char	__reserved;
+	__u8		linklayer; /* lower 4 bits */
 	unsigned short	overhead;
 	short		cell_align;
 	unsigned short	mpu;

commit 6906f4ed6f85b2d72fd944e15da6a905fdde8b40
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 6 06:49:21 2013 +0000

    htb: add HTB_DIRECT_QLEN attribute
    
    HTB uses an internal pfifo queue, which limit is not reported
    to userland tools (tc), and value inherited from device tx_queue_len
    at setup time.
    
    Introduce TCA_HTB_DIRECT_QLEN attribute to allow finer control.
    
    Remove two obsolete pr_err() calls as well.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Jamal Hadi Salim <jhs@mojatatu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
index 32aef0a439ef..dbd71b0c7d8c 100644
--- a/include/uapi/linux/pkt_sched.h
+++ b/include/uapi/linux/pkt_sched.h
@@ -348,6 +348,7 @@ enum {
 	TCA_HTB_INIT,
 	TCA_HTB_CTAB,
 	TCA_HTB_RTAB,
+	TCA_HTB_DIRECT_QLEN,
 	__TCA_HTB_MAX,
 };
 

commit 607ca46e97a1b6594b29647d98a32d545c24bdff
Author: David Howells <dhowells@redhat.com>
Date:   Sat Oct 13 10:46:48 2012 +0100

    UAPI: (Scripted) Disintegrate include/linux
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/uapi/linux/pkt_sched.h b/include/uapi/linux/pkt_sched.h
new file mode 100644
index 000000000000..32aef0a439ef
--- /dev/null
+++ b/include/uapi/linux/pkt_sched.h
@@ -0,0 +1,738 @@
+#ifndef __LINUX_PKT_SCHED_H
+#define __LINUX_PKT_SCHED_H
+
+#include <linux/types.h>
+
+/* Logical priority bands not depending on specific packet scheduler.
+   Every scheduler will map them to real traffic classes, if it has
+   no more precise mechanism to classify packets.
+
+   These numbers have no special meaning, though their coincidence
+   with obsolete IPv6 values is not occasional :-). New IPv6 drafts
+   preferred full anarchy inspired by diffserv group.
+
+   Note: TC_PRIO_BESTEFFORT does not mean that it is the most unhappy
+   class, actually, as rule it will be handled with more care than
+   filler or even bulk.
+ */
+
+#define TC_PRIO_BESTEFFORT		0
+#define TC_PRIO_FILLER			1
+#define TC_PRIO_BULK			2
+#define TC_PRIO_INTERACTIVE_BULK	4
+#define TC_PRIO_INTERACTIVE		6
+#define TC_PRIO_CONTROL			7
+
+#define TC_PRIO_MAX			15
+
+/* Generic queue statistics, available for all the elements.
+   Particular schedulers may have also their private records.
+ */
+
+struct tc_stats {
+	__u64	bytes;			/* Number of enqueued bytes */
+	__u32	packets;		/* Number of enqueued packets	*/
+	__u32	drops;			/* Packets dropped because of lack of resources */
+	__u32	overlimits;		/* Number of throttle events when this
+					 * flow goes out of allocated bandwidth */
+	__u32	bps;			/* Current flow byte rate */
+	__u32	pps;			/* Current flow packet rate */
+	__u32	qlen;
+	__u32	backlog;
+};
+
+struct tc_estimator {
+	signed char	interval;
+	unsigned char	ewma_log;
+};
+
+/* "Handles"
+   ---------
+
+    All the traffic control objects have 32bit identifiers, or "handles".
+
+    They can be considered as opaque numbers from user API viewpoint,
+    but actually they always consist of two fields: major and
+    minor numbers, which are interpreted by kernel specially,
+    that may be used by applications, though not recommended.
+
+    F.e. qdisc handles always have minor number equal to zero,
+    classes (or flows) have major equal to parent qdisc major, and
+    minor uniquely identifying class inside qdisc.
+
+    Macros to manipulate handles:
+ */
+
+#define TC_H_MAJ_MASK (0xFFFF0000U)
+#define TC_H_MIN_MASK (0x0000FFFFU)
+#define TC_H_MAJ(h) ((h)&TC_H_MAJ_MASK)
+#define TC_H_MIN(h) ((h)&TC_H_MIN_MASK)
+#define TC_H_MAKE(maj,min) (((maj)&TC_H_MAJ_MASK)|((min)&TC_H_MIN_MASK))
+
+#define TC_H_UNSPEC	(0U)
+#define TC_H_ROOT	(0xFFFFFFFFU)
+#define TC_H_INGRESS    (0xFFFFFFF1U)
+
+struct tc_ratespec {
+	unsigned char	cell_log;
+	unsigned char	__reserved;
+	unsigned short	overhead;
+	short		cell_align;
+	unsigned short	mpu;
+	__u32		rate;
+};
+
+#define TC_RTAB_SIZE	1024
+
+struct tc_sizespec {
+	unsigned char	cell_log;
+	unsigned char	size_log;
+	short		cell_align;
+	int		overhead;
+	unsigned int	linklayer;
+	unsigned int	mpu;
+	unsigned int	mtu;
+	unsigned int	tsize;
+};
+
+enum {
+	TCA_STAB_UNSPEC,
+	TCA_STAB_BASE,
+	TCA_STAB_DATA,
+	__TCA_STAB_MAX
+};
+
+#define TCA_STAB_MAX (__TCA_STAB_MAX - 1)
+
+/* FIFO section */
+
+struct tc_fifo_qopt {
+	__u32	limit;	/* Queue length: bytes for bfifo, packets for pfifo */
+};
+
+/* PRIO section */
+
+#define TCQ_PRIO_BANDS	16
+#define TCQ_MIN_PRIO_BANDS 2
+
+struct tc_prio_qopt {
+	int	bands;			/* Number of bands */
+	__u8	priomap[TC_PRIO_MAX+1];	/* Map: logical priority -> PRIO band */
+};
+
+/* MULTIQ section */
+
+struct tc_multiq_qopt {
+	__u16	bands;			/* Number of bands */
+	__u16	max_bands;		/* Maximum number of queues */
+};
+
+/* PLUG section */
+
+#define TCQ_PLUG_BUFFER                0
+#define TCQ_PLUG_RELEASE_ONE           1
+#define TCQ_PLUG_RELEASE_INDEFINITE    2
+#define TCQ_PLUG_LIMIT                 3
+
+struct tc_plug_qopt {
+	/* TCQ_PLUG_BUFFER: Inset a plug into the queue and
+	 *  buffer any incoming packets
+	 * TCQ_PLUG_RELEASE_ONE: Dequeue packets from queue head
+	 *   to beginning of the next plug.
+	 * TCQ_PLUG_RELEASE_INDEFINITE: Dequeue all packets from queue.
+	 *   Stop buffering packets until the next TCQ_PLUG_BUFFER
+	 *   command is received (just act as a pass-thru queue).
+	 * TCQ_PLUG_LIMIT: Increase/decrease queue size
+	 */
+	int             action;
+	__u32           limit;
+};
+
+/* TBF section */
+
+struct tc_tbf_qopt {
+	struct tc_ratespec rate;
+	struct tc_ratespec peakrate;
+	__u32		limit;
+	__u32		buffer;
+	__u32		mtu;
+};
+
+enum {
+	TCA_TBF_UNSPEC,
+	TCA_TBF_PARMS,
+	TCA_TBF_RTAB,
+	TCA_TBF_PTAB,
+	__TCA_TBF_MAX,
+};
+
+#define TCA_TBF_MAX (__TCA_TBF_MAX - 1)
+
+
+/* TEQL section */
+
+/* TEQL does not require any parameters */
+
+/* SFQ section */
+
+struct tc_sfq_qopt {
+	unsigned	quantum;	/* Bytes per round allocated to flow */
+	int		perturb_period;	/* Period of hash perturbation */
+	__u32		limit;		/* Maximal packets in queue */
+	unsigned	divisor;	/* Hash divisor  */
+	unsigned	flows;		/* Maximal number of flows  */
+};
+
+struct tc_sfqred_stats {
+	__u32           prob_drop;      /* Early drops, below max threshold */
+	__u32           forced_drop;	/* Early drops, after max threshold */
+	__u32           prob_mark;      /* Marked packets, below max threshold */
+	__u32           forced_mark;    /* Marked packets, after max threshold */
+	__u32           prob_mark_head; /* Marked packets, below max threshold */
+	__u32           forced_mark_head;/* Marked packets, after max threshold */
+};
+
+struct tc_sfq_qopt_v1 {
+	struct tc_sfq_qopt v0;
+	unsigned int	depth;		/* max number of packets per flow */
+	unsigned int	headdrop;
+/* SFQRED parameters */
+	__u32		limit;		/* HARD maximal flow queue length (bytes) */
+	__u32		qth_min;	/* Min average length threshold (bytes) */
+	__u32		qth_max;	/* Max average length threshold (bytes) */
+	unsigned char   Wlog;		/* log(W)		*/
+	unsigned char   Plog;		/* log(P_max/(qth_max-qth_min))	*/
+	unsigned char   Scell_log;	/* cell size for idle damping */
+	unsigned char	flags;
+	__u32		max_P;		/* probability, high resolution */
+/* SFQRED stats */
+	struct tc_sfqred_stats stats;
+};
+
+
+struct tc_sfq_xstats {
+	__s32		allot;
+};
+
+/* RED section */
+
+enum {
+	TCA_RED_UNSPEC,
+	TCA_RED_PARMS,
+	TCA_RED_STAB,
+	TCA_RED_MAX_P,
+	__TCA_RED_MAX,
+};
+
+#define TCA_RED_MAX (__TCA_RED_MAX - 1)
+
+struct tc_red_qopt {
+	__u32		limit;		/* HARD maximal queue length (bytes)	*/
+	__u32		qth_min;	/* Min average length threshold (bytes) */
+	__u32		qth_max;	/* Max average length threshold (bytes) */
+	unsigned char   Wlog;		/* log(W)		*/
+	unsigned char   Plog;		/* log(P_max/(qth_max-qth_min))	*/
+	unsigned char   Scell_log;	/* cell size for idle damping */
+	unsigned char	flags;
+#define TC_RED_ECN		1
+#define TC_RED_HARDDROP		2
+#define TC_RED_ADAPTATIVE	4
+};
+
+struct tc_red_xstats {
+	__u32           early;          /* Early drops */
+	__u32           pdrop;          /* Drops due to queue limits */
+	__u32           other;          /* Drops due to drop() calls */
+	__u32           marked;         /* Marked packets */
+};
+
+/* GRED section */
+
+#define MAX_DPs 16
+
+enum {
+       TCA_GRED_UNSPEC,
+       TCA_GRED_PARMS,
+       TCA_GRED_STAB,
+       TCA_GRED_DPS,
+       TCA_GRED_MAX_P,
+	   __TCA_GRED_MAX,
+};
+
+#define TCA_GRED_MAX (__TCA_GRED_MAX - 1)
+
+struct tc_gred_qopt {
+	__u32		limit;        /* HARD maximal queue length (bytes)    */
+	__u32		qth_min;      /* Min average length threshold (bytes) */
+	__u32		qth_max;      /* Max average length threshold (bytes) */
+	__u32		DP;           /* up to 2^32 DPs */
+	__u32		backlog;
+	__u32		qave;
+	__u32		forced;
+	__u32		early;
+	__u32		other;
+	__u32		pdrop;
+	__u8		Wlog;         /* log(W)               */
+	__u8		Plog;         /* log(P_max/(qth_max-qth_min)) */
+	__u8		Scell_log;    /* cell size for idle damping */
+	__u8		prio;         /* prio of this VQ */
+	__u32		packets;
+	__u32		bytesin;
+};
+
+/* gred setup */
+struct tc_gred_sopt {
+	__u32		DPs;
+	__u32		def_DP;
+	__u8		grio;
+	__u8		flags;
+	__u16		pad1;
+};
+
+/* CHOKe section */
+
+enum {
+	TCA_CHOKE_UNSPEC,
+	TCA_CHOKE_PARMS,
+	TCA_CHOKE_STAB,
+	TCA_CHOKE_MAX_P,
+	__TCA_CHOKE_MAX,
+};
+
+#define TCA_CHOKE_MAX (__TCA_CHOKE_MAX - 1)
+
+struct tc_choke_qopt {
+	__u32		limit;		/* Hard queue length (packets)	*/
+	__u32		qth_min;	/* Min average threshold (packets) */
+	__u32		qth_max;	/* Max average threshold (packets) */
+	unsigned char   Wlog;		/* log(W)		*/
+	unsigned char   Plog;		/* log(P_max/(qth_max-qth_min))	*/
+	unsigned char   Scell_log;	/* cell size for idle damping */
+	unsigned char	flags;		/* see RED flags */
+};
+
+struct tc_choke_xstats {
+	__u32		early;          /* Early drops */
+	__u32		pdrop;          /* Drops due to queue limits */
+	__u32		other;          /* Drops due to drop() calls */
+	__u32		marked;         /* Marked packets */
+	__u32		matched;	/* Drops due to flow match */
+};
+
+/* HTB section */
+#define TC_HTB_NUMPRIO		8
+#define TC_HTB_MAXDEPTH		8
+#define TC_HTB_PROTOVER		3 /* the same as HTB and TC's major */
+
+struct tc_htb_opt {
+	struct tc_ratespec 	rate;
+	struct tc_ratespec 	ceil;
+	__u32	buffer;
+	__u32	cbuffer;
+	__u32	quantum;
+	__u32	level;		/* out only */
+	__u32	prio;
+};
+struct tc_htb_glob {
+	__u32 version;		/* to match HTB/TC */
+    	__u32 rate2quantum;	/* bps->quantum divisor */
+    	__u32 defcls;		/* default class number */
+	__u32 debug;		/* debug flags */
+
+	/* stats */
+	__u32 direct_pkts; /* count of non shaped packets */
+};
+enum {
+	TCA_HTB_UNSPEC,
+	TCA_HTB_PARMS,
+	TCA_HTB_INIT,
+	TCA_HTB_CTAB,
+	TCA_HTB_RTAB,
+	__TCA_HTB_MAX,
+};
+
+#define TCA_HTB_MAX (__TCA_HTB_MAX - 1)
+
+struct tc_htb_xstats {
+	__u32 lends;
+	__u32 borrows;
+	__u32 giants;	/* too big packets (rate will not be accurate) */
+	__u32 tokens;
+	__u32 ctokens;
+};
+
+/* HFSC section */
+
+struct tc_hfsc_qopt {
+	__u16	defcls;		/* default class */
+};
+
+struct tc_service_curve {
+	__u32	m1;		/* slope of the first segment in bps */
+	__u32	d;		/* x-projection of the first segment in us */
+	__u32	m2;		/* slope of the second segment in bps */
+};
+
+struct tc_hfsc_stats {
+	__u64	work;		/* total work done */
+	__u64	rtwork;		/* work done by real-time criteria */
+	__u32	period;		/* current period */
+	__u32	level;		/* class level in hierarchy */
+};
+
+enum {
+	TCA_HFSC_UNSPEC,
+	TCA_HFSC_RSC,
+	TCA_HFSC_FSC,
+	TCA_HFSC_USC,
+	__TCA_HFSC_MAX,
+};
+
+#define TCA_HFSC_MAX (__TCA_HFSC_MAX - 1)
+
+
+/* CBQ section */
+
+#define TC_CBQ_MAXPRIO		8
+#define TC_CBQ_MAXLEVEL		8
+#define TC_CBQ_DEF_EWMA		5
+
+struct tc_cbq_lssopt {
+	unsigned char	change;
+	unsigned char	flags;
+#define TCF_CBQ_LSS_BOUNDED	1
+#define TCF_CBQ_LSS_ISOLATED	2
+	unsigned char  	ewma_log;
+	unsigned char  	level;
+#define TCF_CBQ_LSS_FLAGS	1
+#define TCF_CBQ_LSS_EWMA	2
+#define TCF_CBQ_LSS_MAXIDLE	4
+#define TCF_CBQ_LSS_MINIDLE	8
+#define TCF_CBQ_LSS_OFFTIME	0x10
+#define TCF_CBQ_LSS_AVPKT	0x20
+	__u32		maxidle;
+	__u32		minidle;
+	__u32		offtime;
+	__u32		avpkt;
+};
+
+struct tc_cbq_wrropt {
+	unsigned char	flags;
+	unsigned char	priority;
+	unsigned char	cpriority;
+	unsigned char	__reserved;
+	__u32		allot;
+	__u32		weight;
+};
+
+struct tc_cbq_ovl {
+	unsigned char	strategy;
+#define	TC_CBQ_OVL_CLASSIC	0
+#define	TC_CBQ_OVL_DELAY	1
+#define	TC_CBQ_OVL_LOWPRIO	2
+#define	TC_CBQ_OVL_DROP		3
+#define	TC_CBQ_OVL_RCLASSIC	4
+	unsigned char	priority2;
+	__u16		pad;
+	__u32		penalty;
+};
+
+struct tc_cbq_police {
+	unsigned char	police;
+	unsigned char	__res1;
+	unsigned short	__res2;
+};
+
+struct tc_cbq_fopt {
+	__u32		split;
+	__u32		defmap;
+	__u32		defchange;
+};
+
+struct tc_cbq_xstats {
+	__u32		borrows;
+	__u32		overactions;
+	__s32		avgidle;
+	__s32		undertime;
+};
+
+enum {
+	TCA_CBQ_UNSPEC,
+	TCA_CBQ_LSSOPT,
+	TCA_CBQ_WRROPT,
+	TCA_CBQ_FOPT,
+	TCA_CBQ_OVL_STRATEGY,
+	TCA_CBQ_RATE,
+	TCA_CBQ_RTAB,
+	TCA_CBQ_POLICE,
+	__TCA_CBQ_MAX,
+};
+
+#define TCA_CBQ_MAX	(__TCA_CBQ_MAX - 1)
+
+/* dsmark section */
+
+enum {
+	TCA_DSMARK_UNSPEC,
+	TCA_DSMARK_INDICES,
+	TCA_DSMARK_DEFAULT_INDEX,
+	TCA_DSMARK_SET_TC_INDEX,
+	TCA_DSMARK_MASK,
+	TCA_DSMARK_VALUE,
+	__TCA_DSMARK_MAX,
+};
+
+#define TCA_DSMARK_MAX (__TCA_DSMARK_MAX - 1)
+
+/* ATM  section */
+
+enum {
+	TCA_ATM_UNSPEC,
+	TCA_ATM_FD,		/* file/socket descriptor */
+	TCA_ATM_PTR,		/* pointer to descriptor - later */
+	TCA_ATM_HDR,		/* LL header */
+	TCA_ATM_EXCESS,		/* excess traffic class (0 for CLP)  */
+	TCA_ATM_ADDR,		/* PVC address (for output only) */
+	TCA_ATM_STATE,		/* VC state (ATM_VS_*; for output only) */
+	__TCA_ATM_MAX,
+};
+
+#define TCA_ATM_MAX	(__TCA_ATM_MAX - 1)
+
+/* Network emulator */
+
+enum {
+	TCA_NETEM_UNSPEC,
+	TCA_NETEM_CORR,
+	TCA_NETEM_DELAY_DIST,
+	TCA_NETEM_REORDER,
+	TCA_NETEM_CORRUPT,
+	TCA_NETEM_LOSS,
+	TCA_NETEM_RATE,
+	TCA_NETEM_ECN,
+	__TCA_NETEM_MAX,
+};
+
+#define TCA_NETEM_MAX (__TCA_NETEM_MAX - 1)
+
+struct tc_netem_qopt {
+	__u32	latency;	/* added delay (us) */
+	__u32   limit;		/* fifo limit (packets) */
+	__u32	loss;		/* random packet loss (0=none ~0=100%) */
+	__u32	gap;		/* re-ordering gap (0 for none) */
+	__u32   duplicate;	/* random packet dup  (0=none ~0=100%) */
+	__u32	jitter;		/* random jitter in latency (us) */
+};
+
+struct tc_netem_corr {
+	__u32	delay_corr;	/* delay correlation */
+	__u32	loss_corr;	/* packet loss correlation */
+	__u32	dup_corr;	/* duplicate correlation  */
+};
+
+struct tc_netem_reorder {
+	__u32	probability;
+	__u32	correlation;
+};
+
+struct tc_netem_corrupt {
+	__u32	probability;
+	__u32	correlation;
+};
+
+struct tc_netem_rate {
+	__u32	rate;	/* byte/s */
+	__s32	packet_overhead;
+	__u32	cell_size;
+	__s32	cell_overhead;
+};
+
+enum {
+	NETEM_LOSS_UNSPEC,
+	NETEM_LOSS_GI,		/* General Intuitive - 4 state model */
+	NETEM_LOSS_GE,		/* Gilbert Elliot models */
+	__NETEM_LOSS_MAX
+};
+#define NETEM_LOSS_MAX (__NETEM_LOSS_MAX - 1)
+
+/* State transition probabilities for 4 state model */
+struct tc_netem_gimodel {
+	__u32	p13;
+	__u32	p31;
+	__u32	p32;
+	__u32	p14;
+	__u32	p23;
+};
+
+/* Gilbert-Elliot models */
+struct tc_netem_gemodel {
+	__u32 p;
+	__u32 r;
+	__u32 h;
+	__u32 k1;
+};
+
+#define NETEM_DIST_SCALE	8192
+#define NETEM_DIST_MAX		16384
+
+/* DRR */
+
+enum {
+	TCA_DRR_UNSPEC,
+	TCA_DRR_QUANTUM,
+	__TCA_DRR_MAX
+};
+
+#define TCA_DRR_MAX	(__TCA_DRR_MAX - 1)
+
+struct tc_drr_stats {
+	__u32	deficit;
+};
+
+/* MQPRIO */
+#define TC_QOPT_BITMASK 15
+#define TC_QOPT_MAX_QUEUE 16
+
+struct tc_mqprio_qopt {
+	__u8	num_tc;
+	__u8	prio_tc_map[TC_QOPT_BITMASK + 1];
+	__u8	hw;
+	__u16	count[TC_QOPT_MAX_QUEUE];
+	__u16	offset[TC_QOPT_MAX_QUEUE];
+};
+
+/* SFB */
+
+enum {
+	TCA_SFB_UNSPEC,
+	TCA_SFB_PARMS,
+	__TCA_SFB_MAX,
+};
+
+#define TCA_SFB_MAX (__TCA_SFB_MAX - 1)
+
+/*
+ * Note: increment, decrement are Q0.16 fixed-point values.
+ */
+struct tc_sfb_qopt {
+	__u32 rehash_interval;	/* delay between hash move, in ms */
+	__u32 warmup_time;	/* double buffering warmup time in ms (warmup_time < rehash_interval) */
+	__u32 max;		/* max len of qlen_min */
+	__u32 bin_size;		/* maximum queue length per bin */
+	__u32 increment;	/* probability increment, (d1 in Blue) */
+	__u32 decrement;	/* probability decrement, (d2 in Blue) */
+	__u32 limit;		/* max SFB queue length */
+	__u32 penalty_rate;	/* inelastic flows are rate limited to 'rate' pps */
+	__u32 penalty_burst;
+};
+
+struct tc_sfb_xstats {
+	__u32 earlydrop;
+	__u32 penaltydrop;
+	__u32 bucketdrop;
+	__u32 queuedrop;
+	__u32 childdrop; /* drops in child qdisc */
+	__u32 marked;
+	__u32 maxqlen;
+	__u32 maxprob;
+	__u32 avgprob;
+};
+
+#define SFB_MAX_PROB 0xFFFF
+
+/* QFQ */
+enum {
+	TCA_QFQ_UNSPEC,
+	TCA_QFQ_WEIGHT,
+	TCA_QFQ_LMAX,
+	__TCA_QFQ_MAX
+};
+
+#define TCA_QFQ_MAX	(__TCA_QFQ_MAX - 1)
+
+struct tc_qfq_stats {
+	__u32 weight;
+	__u32 lmax;
+};
+
+/* CODEL */
+
+enum {
+	TCA_CODEL_UNSPEC,
+	TCA_CODEL_TARGET,
+	TCA_CODEL_LIMIT,
+	TCA_CODEL_INTERVAL,
+	TCA_CODEL_ECN,
+	__TCA_CODEL_MAX
+};
+
+#define TCA_CODEL_MAX	(__TCA_CODEL_MAX - 1)
+
+struct tc_codel_xstats {
+	__u32	maxpacket; /* largest packet we've seen so far */
+	__u32	count;	   /* how many drops we've done since the last time we
+			    * entered dropping state
+			    */
+	__u32	lastcount; /* count at entry to dropping state */
+	__u32	ldelay;    /* in-queue delay seen by most recently dequeued packet */
+	__s32	drop_next; /* time to drop next packet */
+	__u32	drop_overlimit; /* number of time max qdisc packet limit was hit */
+	__u32	ecn_mark;  /* number of packets we ECN marked instead of dropped */
+	__u32	dropping;  /* are we in dropping state ? */
+};
+
+/* FQ_CODEL */
+
+enum {
+	TCA_FQ_CODEL_UNSPEC,
+	TCA_FQ_CODEL_TARGET,
+	TCA_FQ_CODEL_LIMIT,
+	TCA_FQ_CODEL_INTERVAL,
+	TCA_FQ_CODEL_ECN,
+	TCA_FQ_CODEL_FLOWS,
+	TCA_FQ_CODEL_QUANTUM,
+	__TCA_FQ_CODEL_MAX
+};
+
+#define TCA_FQ_CODEL_MAX	(__TCA_FQ_CODEL_MAX - 1)
+
+enum {
+	TCA_FQ_CODEL_XSTATS_QDISC,
+	TCA_FQ_CODEL_XSTATS_CLASS,
+};
+
+struct tc_fq_codel_qd_stats {
+	__u32	maxpacket;	/* largest packet we've seen so far */
+	__u32	drop_overlimit; /* number of time max qdisc
+				 * packet limit was hit
+				 */
+	__u32	ecn_mark;	/* number of packets we ECN marked
+				 * instead of being dropped
+				 */
+	__u32	new_flow_count; /* number of time packets
+				 * created a 'new flow'
+				 */
+	__u32	new_flows_len;	/* count of flows in new list */
+	__u32	old_flows_len;	/* count of flows in old list */
+};
+
+struct tc_fq_codel_cl_stats {
+	__s32	deficit;
+	__u32	ldelay;		/* in-queue delay seen by most recently
+				 * dequeued packet
+				 */
+	__u32	count;
+	__u32	lastcount;
+	__u32	dropping;
+	__s32	drop_next;
+};
+
+struct tc_fq_codel_xstats {
+	__u32	type;
+	union {
+		struct tc_fq_codel_qd_stats qdisc_stats;
+		struct tc_fq_codel_cl_stats class_stats;
+	};
+};
+
+#endif
