commit 6d5f904904608a9cd32854d7d0a4dd65b27f9935
Author: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
Date:   Thu Jul 9 09:15:29 2020 +0800

    io_uring: export cq overflow status to userspace
    
    For those applications which are not willing to use io_uring_enter()
    to reap and handle cqes, they may completely rely on liburing's
    io_uring_peek_cqe(), but if cq ring has overflowed, currently because
    io_uring_peek_cqe() is not aware of this overflow, it won't enter
    kernel to flush cqes, below test program can reveal this bug:
    
    static void test_cq_overflow(struct io_uring *ring)
    {
            struct io_uring_cqe *cqe;
            struct io_uring_sqe *sqe;
            int issued = 0;
            int ret = 0;
    
            do {
                    sqe = io_uring_get_sqe(ring);
                    if (!sqe) {
                            fprintf(stderr, "get sqe failed\n");
                            break;;
                    }
                    ret = io_uring_submit(ring);
                    if (ret <= 0) {
                            if (ret != -EBUSY)
                                    fprintf(stderr, "sqe submit failed: %d\n", ret);
                            break;
                    }
                    issued++;
            } while (ret > 0);
            assert(ret == -EBUSY);
    
            printf("issued requests: %d\n", issued);
    
            while (issued) {
                    ret = io_uring_peek_cqe(ring, &cqe);
                    if (ret) {
                            if (ret != -EAGAIN) {
                                    fprintf(stderr, "peek completion failed: %s\n",
                                            strerror(ret));
                                    break;
                            }
                            printf("left requets: %d\n", issued);
                            continue;
                    }
                    io_uring_cqe_seen(ring, cqe);
                    issued--;
                    printf("left requets: %d\n", issued);
            }
    }
    
    int main(int argc, char *argv[])
    {
            int ret;
            struct io_uring ring;
    
            ret = io_uring_queue_init(16, &ring, 0);
            if (ret) {
                    fprintf(stderr, "ring setup failed: %d\n", ret);
                    return 1;
            }
    
            test_cq_overflow(&ring);
            return 0;
    }
    
    To fix this issue, export cq overflow status to userspace by adding new
    IORING_SQ_CQ_OVERFLOW flag, then helper functions() in liburing, such as
    io_uring_peek_cqe, can be aware of this cq overflow and do flush accordingly.
    
    Signed-off-by: Xiaoguang Wang <xiaoguang.wang@linux.alibaba.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 92c22699a5a7..7843742b8b74 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -197,6 +197,7 @@ struct io_sqring_offsets {
  * sq_ring->flags
  */
 #define IORING_SQ_NEED_WAKEUP	(1U << 0) /* needs io_uring_enter wakeup */
+#define IORING_SQ_CQ_OVERFLOW	(1U << 1) /* CQ ring is overflown */
 
 struct io_cqring_offsets {
 	__u32 head;

commit f2a8d5c7a218b9c24befb756c4eb30aa550ce822
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Sun May 17 14:18:06 2020 +0300

    io_uring: add tee(2) support
    
    Add IORING_OP_TEE implementing tee(2) support. Almost identical to
    splice bits, but without offsets.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 8c5775df08b8..92c22699a5a7 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -129,6 +129,7 @@ enum {
 	IORING_OP_SPLICE,
 	IORING_OP_PROVIDE_BUFFERS,
 	IORING_OP_REMOVE_BUFFERS,
+	IORING_OP_TEE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 7e55a19cf6e70ce08964b46dbbfbdb07fbc995fc
Author: Stefano Garzarella <sgarzare@redhat.com>
Date:   Fri May 15 18:38:05 2020 +0200

    io_uring: add IORING_CQ_EVENTFD_DISABLED to the CQ ring flags
    
    This new flag should be set/clear from the application to
    disable/enable eventfd notifications when a request is completed
    and queued to the CQ ring.
    
    Before this patch, notifications were always sent if an eventfd is
    registered, so IORING_CQ_EVENTFD_DISABLED is not set during the
    initialization.
    
    It will be up to the application to set the flag after initialization
    if no notifications are required at the beginning.
    
    Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 602bb0ece607..8c5775df08b8 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -209,6 +209,13 @@ struct io_cqring_offsets {
 	__u64 resv2;
 };
 
+/*
+ * cq_ring->flags
+ */
+
+/* disable eventfd notifications */
+#define IORING_CQ_EVENTFD_DISABLED	(1U << 0)
+
 /*
  * io_uring_enter(2) flags
  */

commit 0d9b5b3af134cddfdc1dd31d41946a0ad389bbf2
Author: Stefano Garzarella <sgarzare@redhat.com>
Date:   Fri May 15 18:38:04 2020 +0200

    io_uring: add 'cq_flags' field for the CQ ring
    
    This patch adds the new 'cq_flags' field that should be written by
    the application and read by the kernel.
    
    This new field is available to the userspace application through
    'cq_off.flags'.
    We are using 4-bytes previously reserved and set to zero. This means
    that if the application finds this field to zero, then the new
    functionality is not supported.
    
    In the next patch we will introduce the first flag available.
    
    Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e48d746b8e2a..602bb0ece607 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -204,7 +204,9 @@ struct io_cqring_offsets {
 	__u32 ring_entries;
 	__u32 overflow;
 	__u32 cqes;
-	__u64 resv[2];
+	__u32 flags;
+	__u32 resv1;
+	__u64 resv2;
 };
 
 /*

commit 9f5834c868e901b00f1bfe4d0052b5906b4a2b7f
Author: Lukas Bulwahn <lukas.bulwahn@gmail.com>
Date:   Sat Mar 21 12:19:07 2020 +0100

    io_uring: make spdxcheck.py happy
    
    Commit bbbdeb4720a0 ("io_uring: dual license io_uring.h uapi header")
    uses a nested SPDX-License-Identifier to dual license the header.
    
    Since then, ./scripts/spdxcheck.py complains:
    
      include/uapi/linux/io_uring.h: 1:60 Missing parentheses: OR
    
    Add parentheses to make spdxcheck.py happy.
    
    Signed-off-by: Lukas Bulwahn <lukas.bulwahn@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 6d9d2b1cc523..e48d746b8e2a 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -1,4 +1,4 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note OR MIT */
+/* SPDX-License-Identifier: (GPL-2.0 WITH Linux-syscall-note) OR MIT */
 /*
  * Header file for the io_uring interface.
  *

commit bbbdeb4720a0759ec90e3bcb20ad28d19e531346
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Mar 11 07:45:46 2020 -0600

    io_uring: dual license io_uring.h uapi header
    
    This just syncs the header it with the liburing version, so there's no
    confusion on the license of the header parts.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index cef4c0c0f26b..6d9d2b1cc523 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -1,4 +1,4 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note OR MIT */
 /*
  * Header file for the io_uring interface.
  *

commit 067524e914cb23e20d59480b318fe2625eaee7c8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Mar 2 16:32:28 2020 -0700

    io_uring: provide means of removing buffers
    
    We have IORING_OP_PROVIDE_BUFFERS, but the only way to remove buffers
    is to trigger IO on them. The usual case of shrinking a buffer pool
    would be to just not replenish the buffers when IO completes, and
    instead just free it. But it may be nice to have a way to manually
    remove a number of buffers from a given group, and
    IORING_OP_REMOVE_BUFFERS provides that functionality.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 9b263d9b24e6..cef4c0c0f26b 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -128,6 +128,7 @@ enum {
 	IORING_OP_EPOLL_CTL,
 	IORING_OP_SPLICE,
 	IORING_OP_PROVIDE_BUFFERS,
+	IORING_OP_REMOVE_BUFFERS,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit bcda7baaa3f15c7a95db3c024bb046d6e298f76b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Feb 23 16:42:51 2020 -0700

    io_uring: support buffer selection for OP_READ and OP_RECV
    
    If a server process has tons of pending socket connections, generally
    it uses epoll to wait for activity. When the socket is ready for reading
    (or writing), the task can select a buffer and issue a recv/send on the
    given fd.
    
    Now that we have fast (non-async thread) support, a task can have tons
    of pending reads or writes pending. But that means they need buffers to
    back that data, and if the number of connections is high enough, having
    them preallocated for all possible connections is unfeasible.
    
    With IORING_OP_PROVIDE_BUFFERS, an application can register buffers to
    use for any request. The request then sets IOSQE_BUFFER_SELECT in the
    sqe, and a given group ID in sqe->buf_group. When the fd becomes ready,
    a free buffer from the specified group is selected. If none are
    available, the request is terminated with -ENOBUFS. If successful, the
    CQE on completion will contain the buffer ID chosen in the cqe->flags
    member, encoded as:
    
            (buffer_id << IORING_CQE_BUFFER_SHIFT) | IORING_CQE_F_BUFFER;
    
    Once a buffer has been consumed by a request, it is no longer available
    and must be registered again with IORING_OP_PROVIDE_BUFFERS.
    
    Requests need to support this feature. For now, IORING_OP_READ and
    IORING_OP_RECV support it. This is checked on SQE submission, a CQE with
    res == -EOPNOTSUPP will be posted if attempted on unsupported requests.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index bc34a57a660b..9b263d9b24e6 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -66,6 +66,7 @@ enum {
 	IOSQE_IO_LINK_BIT,
 	IOSQE_IO_HARDLINK_BIT,
 	IOSQE_ASYNC_BIT,
+	IOSQE_BUFFER_SELECT_BIT,
 };
 
 /*
@@ -81,6 +82,8 @@ enum {
 #define IOSQE_IO_HARDLINK	(1U << IOSQE_IO_HARDLINK_BIT)
 /* always go async */
 #define IOSQE_ASYNC		(1U << IOSQE_ASYNC_BIT)
+/* select buffer from sqe->buf_group */
+#define IOSQE_BUFFER_SELECT	(1U << IOSQE_BUFFER_SELECT_BIT)
 
 /*
  * io_uring_setup() flags
@@ -155,6 +158,17 @@ struct io_uring_cqe {
 	__u32	flags;
 };
 
+/*
+ * cqe->flags
+ *
+ * IORING_CQE_F_BUFFER	If set, the upper 16 bits are the buffer ID
+ */
+#define IORING_CQE_F_BUFFER		(1U << 0)
+
+enum {
+	IORING_CQE_BUFFER_SHIFT		= 16,
+};
+
 /*
  * Magic offsets for the application to mmap the data it needs
  */

commit ddf0322db79c5984dc1a1db890f946dd19b7d6d9
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Feb 23 16:41:33 2020 -0700

    io_uring: add IORING_OP_PROVIDE_BUFFERS
    
    IORING_OP_PROVIDE_BUFFERS uses the buffer registration infrastructure to
    support passing in an addr/len that is associated with a buffer ID and
    buffer group ID. The group ID is used to index and lookup the buffers,
    while the buffer ID can be used to notify the application which buffer
    in the group was used. The addr passed in is the starting buffer address,
    and length is each buffer length. A number of buffers to add with can be
    specified, in which case addr is incremented by length for each addition,
    and each buffer increments the buffer ID specified.
    
    No validation is done of the buffer ID. If the application provides
    buffers within the same group with identical buffer IDs, then it'll have
    a hard time telling which buffer ID was used. The only restriction is
    that the buffer ID can be a max of 16-bits in size, so USHRT_MAX is the
    maximum ID that can be used.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 53b36311cdac..bc34a57a660b 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -45,8 +45,13 @@ struct io_uring_sqe {
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
 		struct {
-			/* index into fixed buffers, if used */
-			__u16	buf_index;
+			/* pack this to avoid bogus arm OABI complaints */
+			union {
+				/* index into fixed buffers, if used */
+				__u16	buf_index;
+				/* for grouped buffer selection */
+				__u16	buf_group;
+			} __attribute__((packed));
 			/* personality to use, if used */
 			__u16	personality;
 			__s32	splice_fd_in;
@@ -119,6 +124,7 @@ enum {
 	IORING_OP_OPENAT2,
 	IORING_OP_EPOLL_CTL,
 	IORING_OP_SPLICE,
+	IORING_OP_PROVIDE_BUFFERS,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit d7718a9d25a61442da8ee8aeeff6a0097f0ccfd6
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Feb 14 22:23:12 2020 -0700

    io_uring: use poll driven retry for files that support it
    
    Currently io_uring tries any request in a non-blocking manner, if it can,
    and then retries from a worker thread if we get -EAGAIN. Now that we have
    a new and fancy poll based retry backend, use that to retry requests if
    the file supports it.
    
    This means that, for example, an IORING_OP_RECVMSG on a socket no longer
    requires an async thread to complete the IO. If we get -EAGAIN reading
    from the socket in a non-blocking manner, we arm a poll handler for
    notification on when the socket becomes readable. When it does, the
    pending read is executed directly by the task again, through the io_uring
    task work handlers. Not only is this faster and more efficient, it also
    means we're not generating potentially tons of async threads that just
    sit and block, waiting for the IO to complete.
    
    The feature is marked with IORING_FEAT_FAST_POLL, meaning that async
    pollable IO is fast, and that poll<link>other_op is fast as well.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 08891cc1c1e7..53b36311cdac 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -216,6 +216,7 @@ struct io_uring_params {
 #define IORING_FEAT_SUBMIT_STABLE	(1U << 2)
 #define IORING_FEAT_RW_CUR_POS		(1U << 3)
 #define IORING_FEAT_CUR_PERSONALITY	(1U << 4)
+#define IORING_FEAT_FAST_POLL		(1U << 5)
 
 /*
  * io_uring_register(2) opcodes and arguments

commit 7d67af2c013402537385dae343a2d0f6a4cb3bfd
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Mon Feb 24 11:32:45 2020 +0300

    io_uring: add splice(2) support
    
    Add support for splice(2).
    
    - output file is specified as sqe->fd, so it's handled by generic code
    - hash_reg_file handled by generic code as well
    - len is 32bit, but should be fine
    - the fd_in is registered file, when SPLICE_F_FD_IN_FIXED is set, which
    is a splice flag (i.e. sqe->splice_flags).
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 3f7961c1c243..08891cc1c1e7 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -23,7 +23,10 @@ struct io_uring_sqe {
 		__u64	off;	/* offset into file */
 		__u64	addr2;
 	};
-	__u64	addr;		/* pointer to buffer or iovecs */
+	union {
+		__u64	addr;	/* pointer to buffer or iovecs */
+		__u64	splice_off_in;
+	};
 	__u32	len;		/* buffer size or number of iovecs */
 	union {
 		__kernel_rwf_t	rw_flags;
@@ -37,6 +40,7 @@ struct io_uring_sqe {
 		__u32		open_flags;
 		__u32		statx_flags;
 		__u32		fadvise_advice;
+		__u32		splice_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -45,6 +49,7 @@ struct io_uring_sqe {
 			__u16	buf_index;
 			/* personality to use, if used */
 			__u16	personality;
+			__s32	splice_fd_in;
 		};
 		__u64	__pad2[3];
 	};
@@ -113,6 +118,7 @@ enum {
 	IORING_OP_RECV,
 	IORING_OP_OPENAT2,
 	IORING_OP_EPOLL_CTL,
+	IORING_OP_SPLICE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,
@@ -128,6 +134,12 @@ enum {
  */
 #define IORING_TIMEOUT_ABS	(1U << 0)
 
+/*
+ * sqe->splice_flags
+ * extends splice(2) flags
+ */
+#define SPLICE_F_FD_IN_FIXED	(1U << 31) /* the last bit of __u32 */
+
 /*
  * IO completion data structure (Completion Queue Entry)
  */

commit 3e4827b05d2ac2d377ed136a52829ec46787bf4b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 8 15:18:09 2020 -0700

    io_uring: add support for epoll_ctl(2)
    
    This adds IORING_OP_EPOLL_CTL, which can perform the same work as the
    epoll_ctl(2) system call.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 98105ff8d3e6..3f7961c1c243 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -112,6 +112,7 @@ enum {
 	IORING_OP_SEND,
 	IORING_OP_RECV,
 	IORING_OP_OPENAT2,
+	IORING_OP_EPOLL_CTL,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 75c6a03904e0dd414a4d99a3072075cb5117e5bc
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jan 28 10:15:23 2020 -0700

    io_uring: support using a registered personality for commands
    
    For personalities previously registered via IORING_REGISTER_PERSONALITY,
    allow any command to select them. This is done through setting
    sqe->personality to the id returned from registration, and then flagging
    sqe->flags with IOSQE_PERSONALITY.
    
    Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index b4ccf31db2d1..98105ff8d3e6 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -40,7 +40,12 @@ struct io_uring_sqe {
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
-		__u16	buf_index;	/* index into fixed buffers, if used */
+		struct {
+			/* index into fixed buffers, if used */
+			__u16	buf_index;
+			/* personality to use, if used */
+			__u16	personality;
+		};
 		__u64	__pad2[3];
 	};
 };

commit 071698e13ac6ba786dfa22349a7b62deb5a9464d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Jan 28 10:04:42 2020 -0700

    io_uring: allow registering credentials
    
    If an application wants to use a ring with different kinds of
    credentials, it can register them upfront. We don't lookup credentials,
    the credentials of the task calling IORING_REGISTER_PERSONALITY is used.
    
    An 'id' is returned for the application to use in subsequent personality
    support.
    
    Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e067b92af5ad..b4ccf31db2d1 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -211,6 +211,8 @@ struct io_uring_params {
 #define IORING_REGISTER_FILES_UPDATE	6
 #define IORING_REGISTER_EVENTFD_ASYNC	7
 #define IORING_REGISTER_PROBE		8
+#define IORING_REGISTER_PERSONALITY	9
+#define IORING_UNREGISTER_PERSONALITY	10
 
 struct io_uring_files_update {
 	__u32 offset;

commit 24369c2e3bb06d8c4e71fd6ceaf4f8a01ae79b7c
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Tue Jan 28 03:15:48 2020 +0300

    io_uring: add io-wq workqueue sharing
    
    If IORING_SETUP_ATTACH_WQ is set, it expects wq_fd in io_uring_params to
    be a valid io_uring fd io-wq of which will be shared with the newly
    created io_uring instance. If the flag is set but it can't share io-wq,
    it fails.
    
    This allows creation of "sibling" io_urings, where we prefer to keep the
    SQ/CQ private, but want to share the async backend to minimize the amount
    of overhead associated with having multiple rings that belong to the same
    backend.
    
    Reported-by: Jens Axboe <axboe@kernel.dk>
    Reported-by: Daurnimator <quae@daurnimator.com>
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 9988e82f858b..e067b92af5ad 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -75,6 +75,7 @@ enum {
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
 #define IORING_SETUP_CLAMP	(1U << 4)	/* clamp SQ/CQ ring sizes */
+#define IORING_SETUP_ATTACH_WQ	(1U << 5)	/* attach to existing wq */
 
 enum {
 	IORING_OP_NOP,
@@ -183,7 +184,8 @@ struct io_uring_params {
 	__u32 sq_thread_cpu;
 	__u32 sq_thread_idle;
 	__u32 features;
-	__u32 resv[4];
+	__u32 wq_fd;
+	__u32 resv[3];
 	struct io_sqring_offsets sq_off;
 	struct io_cqring_offsets cq_off;
 };

commit cccf0ee834559ae0b327b40290e14f6a2a017177
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Jan 27 16:34:48 2020 -0700

    io_uring/io-wq: don't use static creds/mm assignments
    
    We currently setup the io_wq with a static set of mm and creds. Even for
    a single-use io-wq per io_uring, this is suboptimal as we have may have
    multiple enters of the ring. For sharing the io-wq backend, it doesn't
    work at all.
    
    Switch to passing in the creds and mm when the work item is setup. This
    means that async work is no longer deferred to the io_uring mm and creds,
    it is done with the current mm and creds.
    
    Flag this behavior with IORING_FEAT_CUR_PERSONALITY, so applications know
    they can rely on the current personality (mm and creds) being the same
    for direct issue and async issue.
    
    Reviewed-by: Stefan Metzmacher <metze@samba.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 57d05cc5e271..9988e82f858b 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -195,6 +195,7 @@ struct io_uring_params {
 #define IORING_FEAT_NODROP		(1U << 1)
 #define IORING_FEAT_SUBMIT_STABLE	(1U << 2)
 #define IORING_FEAT_RW_CUR_POS		(1U << 3)
+#define IORING_FEAT_CUR_PERSONALITY	(1U << 4)
 
 /*
  * io_uring_register(2) opcodes and arguments

commit 6b47ee6ecab142f938a40bf3b297abac74218ee2
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Sat Jan 18 20:22:41 2020 +0300

    io_uring: optimise sqe-to-req flags translation
    
    For each IOSQE_* flag there is a corresponding REQ_F_* flag. And there
    is a repetitive pattern of their translation:
    e.g. if (sqe->flags & SQE_FLAG*) req->flags |= REQ_F_FLAG*
    
    Use same numeric values/bits for them and copy instead of manual
    handling.
    
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 955fd477e530..57d05cc5e271 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -45,14 +45,27 @@ struct io_uring_sqe {
 	};
 };
 
+enum {
+	IOSQE_FIXED_FILE_BIT,
+	IOSQE_IO_DRAIN_BIT,
+	IOSQE_IO_LINK_BIT,
+	IOSQE_IO_HARDLINK_BIT,
+	IOSQE_ASYNC_BIT,
+};
+
 /*
  * sqe->flags
  */
-#define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
-#define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
-#define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
-#define IOSQE_IO_HARDLINK	(1U << 3)	/* like LINK, but stronger */
-#define IOSQE_ASYNC		(1U << 4)	/* always go async */
+/* use fixed fileset */
+#define IOSQE_FIXED_FILE	(1U << IOSQE_FIXED_FILE_BIT)
+/* issue after inflight IO */
+#define IOSQE_IO_DRAIN		(1U << IOSQE_IO_DRAIN_BIT)
+/* links next sqe */
+#define IOSQE_IO_LINK		(1U << IOSQE_IO_LINK_BIT)
+/* like LINK, but stronger */
+#define IOSQE_IO_HARDLINK	(1U << IOSQE_IO_HARDLINK_BIT)
+/* always go async */
+#define IOSQE_ASYNC		(1U << IOSQE_ASYNC_BIT)
 
 /*
  * io_uring_setup() flags

commit 66f4af93da5761d2fa05c0dc673a47003cdb9cfe
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Jan 16 15:36:52 2020 -0700

    io_uring: add support for probing opcodes
    
    The application currently has no way of knowing if a given opcode is
    supported or not without having to try and issue one and see if we get
    -EINVAL or not. And even this approach is fraught with peril, as maybe
    we're getting -EINVAL due to some fields being missing, or maybe it's
    just not that easy to issue that particular command without doing some
    other leg work in terms of setup first.
    
    This adds IORING_REGISTER_PROBE, which fills in a structure with info
    on what it supported or not. This will work even with sparse opcode
    fields, which may happen in the future or even today if someone
    backports specific features to older kernels.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index fea7da182851..955fd477e530 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -194,6 +194,7 @@ struct io_uring_params {
 #define IORING_UNREGISTER_EVENTFD	5
 #define IORING_REGISTER_FILES_UPDATE	6
 #define IORING_REGISTER_EVENTFD_ASYNC	7
+#define IORING_REGISTER_PROBE		8
 
 struct io_uring_files_update {
 	__u32 offset;
@@ -201,4 +202,21 @@ struct io_uring_files_update {
 	__aligned_u64 /* __s32 * */ fds;
 };
 
+#define IO_URING_OP_SUPPORTED	(1U << 0)
+
+struct io_uring_probe_op {
+	__u8 op;
+	__u8 resv;
+	__u16 flags;	/* IO_URING_OP_* flags */
+	__u32 resv2;
+};
+
+struct io_uring_probe {
+	__u8 last_op;	/* last opcode supported */
+	__u8 ops_len;	/* length of ops[] array below */
+	__u16 resv;
+	__u32 resv2[3];
+	struct io_uring_probe_op ops[0];
+};
+
 #endif

commit cebdb98617ae3e842c81c73758a185248b37cfd6
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 8 17:59:24 2020 -0700

    io_uring: add support for IORING_OP_OPENAT2
    
    Add support for the new openat2(2) system call. It's trivial to do, as
    we can have openat(2) just be wrapped around it.
    
    Suggested-by: Stefan Metzmacher <metze@samba.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 66772a90a7f2..fea7da182851 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -92,6 +92,7 @@ enum {
 	IORING_OP_MADVISE,
 	IORING_OP_SEND,
 	IORING_OP_RECV,
+	IORING_OP_OPENAT2,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit f2842ab5b72d7ee5f7f8385c2d4f32c133f5837b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 8 11:04:00 2020 -0700

    io_uring: enable option to only trigger eventfd for async completions
    
    If an application is using eventfd notifications with poll to know when
    new SQEs can be issued, it's expecting the following read/writes to
    complete inline. And with that, it knows that there are events available,
    and don't want spurious wakeups on the eventfd for those requests.
    
    This adds IORING_REGISTER_EVENTFD_ASYNC, which works just like
    IORING_REGISTER_EVENTFD, except it only triggers notifications for events
    that happen from async completions (IRQ, or io-wq worker completions).
    Any completions inline from the submission itself will not trigger
    notifications.
    
    Suggested-by: Mark Papadakis <markuspapadakis@icloud.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 0fe270ab191c..66772a90a7f2 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -192,6 +192,7 @@ struct io_uring_params {
 #define IORING_REGISTER_EVENTFD		4
 #define IORING_UNREGISTER_EVENTFD	5
 #define IORING_REGISTER_FILES_UPDATE	6
+#define IORING_REGISTER_EVENTFD_ASYNC	7
 
 struct io_uring_files_update {
 	__u32 offset;

commit fddafacee287b3140212c92464077e971401f860
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Jan 4 20:19:44 2020 -0700

    io_uring: add support for send(2) and recv(2)
    
    This adds IORING_OP_SEND for send(2) support, and IORING_OP_RECV for
    recv(2) support.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 29fae13395a8..0fe270ab191c 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -90,6 +90,8 @@ enum {
 	IORING_OP_WRITE,
 	IORING_OP_FADVISE,
 	IORING_OP_MADVISE,
+	IORING_OP_SEND,
+	IORING_OP_RECV,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 8110c1a6212e430a84edd2b83fe9043def8b743e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Dec 28 15:39:54 2019 -0700

    io_uring: add support for IORING_SETUP_CLAMP
    
    Some applications like to start small in terms of ring size, and then
    ramp up as needed. This is a bit tricky to do currently, since we don't
    advertise the max ring size.
    
    This adds IORING_SETUP_CLAMP. If set, and the values for SQ or CQ ring
    size exceed what we support, then clamp them at the max values instead
    of returning -EINVAL. Since we return the chosen ring sizes after setup,
    no further changes are needed on the application side. io_uring already
    changes the ring sizes if the application doesn't ask for power-of-two
    sizes, for example.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 8ad3cece5440..29fae13395a8 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -61,6 +61,7 @@ struct io_uring_sqe {
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
+#define IORING_SETUP_CLAMP	(1U << 4)	/* clamp SQ/CQ ring sizes */
 
 enum {
 	IORING_OP_NOP,

commit c1ca757bd6f4632c510714631ddcc2d13030fe1e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 25 22:18:28 2019 -0700

    io_uring: add IORING_OP_MADVISE
    
    This adds support for doing madvise(2) through io_uring. We assume that
    any operation can block, and hence punt everything async. This could be
    improved, but hard to make bullet proof. The async punt ensures it's
    safe.
    
    Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index f86d1c776078..8ad3cece5440 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -88,6 +88,7 @@ enum {
 	IORING_OP_READ,
 	IORING_OP_WRITE,
 	IORING_OP_FADVISE,
+	IORING_OP_MADVISE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 4840e418c2fc533d55ff6caa5b9313eed1d26cfd
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 25 22:03:45 2019 -0700

    io_uring: add IORING_OP_FADVISE
    
    This adds support for doing fadvise through io_uring. We assume that
    WILLNEED doesn't block, but that DONTNEED may block.
    
    Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 1f96136eb6ee..f86d1c776078 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -36,6 +36,7 @@ struct io_uring_sqe {
 		__u32		cancel_flags;
 		__u32		open_flags;
 		__u32		statx_flags;
+		__u32		fadvise_advice;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -86,6 +87,7 @@ enum {
 	IORING_OP_STATX,
 	IORING_OP_READ,
 	IORING_OP_WRITE,
+	IORING_OP_FADVISE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit ba04291eb66ed895f194ae5abd3748d72bf8aaea
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 25 16:33:42 2019 -0700

    io_uring: allow use of offset == -1 to mean file position
    
    This behaves like preadv2/pwritev2 with offset == -1, it'll use (and
    update) the current file position. This obviously comes with the caveat
    that if the application has multiple read/writes in flight, then the
    end result will not be as expected. This is similar to threads sharing
    a file descriptor and doing IO using the current file position.
    
    Since this feature isn't easily detectable by doing a read or write,
    add a feature flags, IORING_FEAT_RW_CUR_POS, to allow applications to
    detect presence of this feature.
    
    Reported-by: 李通洲 <carter.li@eoitek.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 7fdf994f3313..1f96136eb6ee 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -174,6 +174,7 @@ struct io_uring_params {
 #define IORING_FEAT_SINGLE_MMAP		(1U << 0)
 #define IORING_FEAT_NODROP		(1U << 1)
 #define IORING_FEAT_SUBMIT_STABLE	(1U << 2)
+#define IORING_FEAT_RW_CUR_POS		(1U << 3)
 
 /*
  * io_uring_register(2) opcodes and arguments

commit 3a6820f2bb8a079975109c25a5d1f29f46bce5d2
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Dec 22 15:19:35 2019 -0700

    io_uring: add non-vectored read/write commands
    
    For uses cases that don't already naturally have an iovec, it's easier
    (or more convenient) to just use a buffer address + length. This is
    particular true if the use case is from languages that want to create
    a memory safe abstraction on top of io_uring, and where introducing
    the need for the iovec may impose an ownership issue. For those cases,
    they currently need an indirection buffer, which means allocating data
    just for this purpose.
    
    Add basic read/write that don't require the iovec.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index d7ec50247a3a..7fdf994f3313 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -84,6 +84,8 @@ enum {
 	IORING_OP_CLOSE,
 	IORING_OP_FILES_UPDATE,
 	IORING_OP_STATX,
+	IORING_OP_READ,
+	IORING_OP_WRITE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit ce35a47a3a0208a77b4d31b7f2e8ed57d624093d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 17 08:04:44 2019 -0700

    io_uring: add IOSQE_ASYNC
    
    io_uring defaults to always doing inline submissions, if at all
    possible. But for larger copies, even if the data is fully cached, that
    can take a long time. Add an IOSQE_ASYNC flag that the application can
    set on the SQE - if set, it'll ensure that we always go async for those
    kinds of requests. Use the io-wq IO_WQ_WORK_CONCURRENT flag to ensure we
    get the concurrency we desire for this case.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 3f45f7c543de..d7ec50247a3a 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -51,6 +51,7 @@ struct io_uring_sqe {
 #define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
 #define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
 #define IOSQE_IO_HARDLINK	(1U << 3)	/* like LINK, but stronger */
+#define IOSQE_ASYNC		(1U << 4)	/* always go async */
 
 /*
  * io_uring_setup() flags

commit eddc7ef52a6b37b7ba3d1c8a8fbb63d5d9914f8a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Dec 13 21:18:10 2019 -0700

    io_uring: add support for IORING_OP_STATX
    
    This provides support for async statx(2) through io_uring.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index ca436b9d4921..3f45f7c543de 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -35,6 +35,7 @@ struct io_uring_sqe {
 		__u32		accept_flags;
 		__u32		cancel_flags;
 		__u32		open_flags;
+		__u32		statx_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -81,6 +82,7 @@ enum {
 	IORING_OP_OPENAT,
 	IORING_OP_CLOSE,
 	IORING_OP_FILES_UPDATE,
+	IORING_OP_STATX,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 05f3fb3c5397524feae2e73ee8e150a9090a7da2
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Dec 9 11:22:50 2019 -0700

    io_uring: avoid ring quiesce for fixed file set unregister and update
    
    We currently fully quiesce the ring before an unregister or update of
    the fixed fileset. This is very expensive, and we can be a bit smarter
    about this.
    
    Add a percpu refcount for the file tables as a whole. Grab a percpu ref
    when we use a registered file, and put it on completion. This is cheap
    to do. Upon removal of a file from a set, switch the ref count to atomic
    mode. When we hit zero ref on the completion side, then we know we can
    drop the previously registered files. When the old files have been
    dropped, switch the ref back to percpu mode for normal operation.
    
    Since there's a period between doing the update and the kernel being
    done with it, add a IORING_OP_FILES_UPDATE opcode that can perform the
    same action. The application knows the update has completed when it gets
    the CQE for it. Between doing the update and receiving this completion,
    the application must continue to use the unregistered fd if submitting
    IO on this particular file.
    
    This takes the runtime of test/file-register from liburing from 14s to
    about 0.7s.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 084dea85b838..ca436b9d4921 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -80,6 +80,7 @@ enum {
 	IORING_OP_FALLOCATE,
 	IORING_OP_OPENAT,
 	IORING_OP_CLOSE,
+	IORING_OP_FILES_UPDATE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 11 14:02:38 2019 -0700

    io_uring: add support for IORING_OP_CLOSE
    
    This works just like close(2), unsurprisingly. We remove the file
    descriptor and post the completion inline, then offload the actual
    (potential) last file put to async context.
    
    Mark the async part of this work as uncancellable, as we really must
    guarantee that the latter part of the close is run.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index c1a7c1c65eaf..084dea85b838 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -79,6 +79,7 @@ enum {
 	IORING_OP_CONNECT,
 	IORING_OP_FALLOCATE,
 	IORING_OP_OPENAT,
+	IORING_OP_CLOSE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 15b71abe7b52df214785dde0de9f581cc0216d17
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 11 11:20:36 2019 -0700

    io_uring: add support for IORING_OP_OPENAT
    
    This works just like openat(2), except it can be performed async. For
    the normal case of a non-blocking path lookup this will complete
    inline. If we have to do IO to perform the open, it'll be done from
    async context.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index ad1574f35eb3..c1a7c1c65eaf 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -34,6 +34,7 @@ struct io_uring_sqe {
 		__u32		timeout_flags;
 		__u32		accept_flags;
 		__u32		cancel_flags;
+		__u32		open_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -77,6 +78,7 @@ enum {
 	IORING_OP_LINK_TIMEOUT,
 	IORING_OP_CONNECT,
 	IORING_OP_FALLOCATE,
+	IORING_OP_OPENAT,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit d63d1b5edb7b832210bfde587ba9e7549fa064eb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 10 10:38:56 2019 -0700

    io_uring: add support for fallocate()
    
    This exposes fallocate(2) through io_uring.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 55cfcb71606d..ad1574f35eb3 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -76,6 +76,7 @@ enum {
 	IORING_OP_ASYNC_CANCEL,
 	IORING_OP_LINK_TIMEOUT,
 	IORING_OP_CONNECT,
+	IORING_OP_FALLOCATE,
 
 	/* this goes last, obviously */
 	IORING_OP_LAST,

commit 1292e972fff2b2d81e139e0c2fe5f50249e78c58
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Wed Jan 15 17:35:38 2020 +0100

    io_uring: fix compat for IORING_REGISTER_FILES_UPDATE
    
    fds field of struct io_uring_files_update is problematic with regards
    to compat user space, as pointer size is different in 32-bit, 32-on-64-bit,
    and 64-bit user space.  In order to avoid custom handling of compat in
    the syscall implementation, make fds __u64 and use u64_to_user_ptr in
    order to retrieve it.  Also, align the field naturally and check that
    no garbage is passed there.
    
    Fixes: c3a31e605620c279 ("io_uring: add support for IORING_REGISTER_FILES_UPDATE")
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a3300e1b9a01..55cfcb71606d 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -178,7 +178,8 @@ struct io_uring_params {
 
 struct io_uring_files_update {
 	__u32 offset;
-	__s32 *fds;
+	__u32 resv;
+	__aligned_u64 /* __s32 * */ fds;
 };
 
 #endif

commit 9e3aa61ae3e01ce1ce6361a41ef725e1f4d1d2bf
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Dec 11 15:55:43 2019 -0700

    io_uring: ensure we return -EINVAL on unknown opcode
    
    If we submit an unknown opcode and have fd == -1, io_op_needs_file()
    will return true as we default to needing a file. Then when we go and
    assign the file, we find the 'fd' invalid and return -EBADF. We really
    should be returning -EINVAL for that case, as we normally do for
    unsupported opcodes.
    
    Change io_op_needs_file() to have the following return values:
    
    0   - does not need a file
    1   - does need a file
    < 0 - error value
    
    and use this to pass back the right value for this invalid case.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index ea231366f5fd..a3300e1b9a01 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -58,23 +58,28 @@ struct io_uring_sqe {
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
 
-#define IORING_OP_NOP		0
-#define IORING_OP_READV		1
-#define IORING_OP_WRITEV	2
-#define IORING_OP_FSYNC		3
-#define IORING_OP_READ_FIXED	4
-#define IORING_OP_WRITE_FIXED	5
-#define IORING_OP_POLL_ADD	6
-#define IORING_OP_POLL_REMOVE	7
-#define IORING_OP_SYNC_FILE_RANGE	8
-#define IORING_OP_SENDMSG	9
-#define IORING_OP_RECVMSG	10
-#define IORING_OP_TIMEOUT	11
-#define IORING_OP_TIMEOUT_REMOVE	12
-#define IORING_OP_ACCEPT	13
-#define IORING_OP_ASYNC_CANCEL	14
-#define IORING_OP_LINK_TIMEOUT	15
-#define IORING_OP_CONNECT	16
+enum {
+	IORING_OP_NOP,
+	IORING_OP_READV,
+	IORING_OP_WRITEV,
+	IORING_OP_FSYNC,
+	IORING_OP_READ_FIXED,
+	IORING_OP_WRITE_FIXED,
+	IORING_OP_POLL_ADD,
+	IORING_OP_POLL_REMOVE,
+	IORING_OP_SYNC_FILE_RANGE,
+	IORING_OP_SENDMSG,
+	IORING_OP_RECVMSG,
+	IORING_OP_TIMEOUT,
+	IORING_OP_TIMEOUT_REMOVE,
+	IORING_OP_ACCEPT,
+	IORING_OP_ASYNC_CANCEL,
+	IORING_OP_LINK_TIMEOUT,
+	IORING_OP_CONNECT,
+
+	/* this goes last, obviously */
+	IORING_OP_LAST,
+};
 
 /*
  * sqe->fsync_flags

commit 4e88d6e7793f2f445f43bd608828541d7f43b608
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Dec 7 20:59:47 2019 -0700

    io_uring: allow unbreakable links
    
    Some commands will invariably end in a failure in the sense that the
    completion result will be less than zero. One such example is timeouts
    that don't have a completion count set, they will always complete with
    -ETIME unless cancelled.
    
    For linked commands, we sever links and fail the rest of the chain if
    the result is less than zero. Since we have commands where we know that
    will happen, add IOSQE_IO_HARDLINK as a stronger link that doesn't sever
    regardless of the completion result. Note that the link will still sever
    if we fail submitting the parent request, hard links are only resilient
    in the presence of completion results for requests that did submit
    correctly.
    
    Cc: stable@vger.kernel.org # v5.4
    Reviewed-by: Pavel Begunkov <asml.silence@gmail.com>
    Reported-by: 李通洲 <carter.li@eoitek.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index eabccb46edd1..ea231366f5fd 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -48,6 +48,7 @@ struct io_uring_sqe {
 #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 #define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
 #define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
+#define IOSQE_IO_HARDLINK	(1U << 3)	/* like LINK, but stronger */
 
 /*
  * io_uring_setup() flags

commit da8c96906990f1108cb626ee7865e69267a3263b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Dec 2 18:51:26 2019 -0700

    io_uring: mark us with IORING_FEAT_SUBMIT_STABLE
    
    If this flag is set, applications can be certain that any data for
    async offload has been consumed when the kernel has consumed the
    SQE.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 4637ed1d9949..eabccb46edd1 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -157,6 +157,7 @@ struct io_uring_params {
  */
 #define IORING_FEAT_SINGLE_MMAP		(1U << 0)
 #define IORING_FEAT_NODROP		(1U << 1)
+#define IORING_FEAT_SUBMIT_STABLE	(1U << 2)
 
 /*
  * io_uring_register(2) opcodes and arguments

commit f8e85cf255ad57d65eeb9a9d0e59e3dec55bdd9e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Nov 23 14:24:24 2019 -0700

    io_uring: add support for IORING_OP_CONNECT
    
    This allows an application to call connect() in an async fashion. Like
    other opcodes, we first try a non-blocking connect, then punt to async
    context if we have to.
    
    Note that we can still return -EINPROGRESS, and in that case the caller
    should use IORING_OP_POLL_ADD to do an async wait for completion of the
    connect request (just like for regular connect(2), except we can do it
    async here too).
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 2a1569211d87..4637ed1d9949 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -73,6 +73,7 @@ struct io_uring_sqe {
 #define IORING_OP_ACCEPT	13
 #define IORING_OP_ASYNC_CANCEL	14
 #define IORING_OP_LINK_TIMEOUT	15
+#define IORING_OP_CONNECT	16
 
 /*
  * sqe->fsync_flags

commit 1d7bb1d50fb4dc141c7431cc21fdd24ffcc83c76
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Nov 6 11:31:17 2019 -0700

    io_uring: add support for backlogged CQ ring
    
    Currently we drop completion events, if the CQ ring is full. That's fine
    for requests with bounded completion times, but it may make it harder or
    impossible to use io_uring with networked IO where request completion
    times are generally unbounded. Or with POLL, for example, which is also
    unbounded.
    
    After this patch, we never overflow the ring, we simply store requests
    in a backlog for later flushing. This flushing is done automatically by
    the kernel. To prevent the backlog from growing indefinitely, if the
    backlog is non-empty, we apply back pressure on IO submissions. Any
    attempt to submit new IO with a non-empty backlog will get an -EBUSY
    return from the kernel. This is a signal to the application that it has
    backlogged CQ events, and that it must reap those before being allowed
    to submit more IO.
    
    Note that if we do return -EBUSY, we will have filled whatever
    backlogged events into the CQ ring first, if there's room. This means
    the application can safely reap events WITHOUT entering the kernel and
    waiting for them, they are already available in the CQ ring.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index f1a118b01d18..2a1569211d87 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -155,6 +155,7 @@ struct io_uring_params {
  * io_uring_params->features flags
  */
 #define IORING_FEAT_SINGLE_MMAP		(1U << 0)
+#define IORING_FEAT_NODROP		(1U << 1)
 
 /*
  * io_uring_register(2) opcodes and arguments

commit 2665abfd757fb35a241c6f0b1ebf620e3ffb36fb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 5 12:40:47 2019 -0700

    io_uring: add support for linked SQE timeouts
    
    While we have support for generic timeouts, we don't have a way to tie
    a timeout to a specific SQE. The generic timeouts simply trigger wakeups
    on the CQ ring.
    
    This adds support for IORING_OP_LINK_TIMEOUT. This command is only valid
    as a link to a previous command. The timeout specific can be either
    relative or absolute, following the same rules as IORING_OP_TIMEOUT. If
    the timeout triggers before the dependent command completes, it will
    attempt to cancel that command. Likewise, if the dependent command
    completes before the timeout triggers, it will cancel the timeout.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 6877cf8894db..f1a118b01d18 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -72,6 +72,7 @@ struct io_uring_sqe {
 #define IORING_OP_TIMEOUT_REMOVE	12
 #define IORING_OP_ACCEPT	13
 #define IORING_OP_ASYNC_CANCEL	14
+#define IORING_OP_LINK_TIMEOUT	15
 
 /*
  * sqe->fsync_flags

commit 62755e35dfb2b113c52b81cd96d01c20971c8e02
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 28 21:49:21 2019 -0600

    io_uring: support for generic async request cancel
    
    This adds support for IORING_OP_ASYNC_CANCEL, which will attempt to
    cancel requests that have been punted to async context and are now
    in-flight. This works for regular read/write requests to files, as
    long as they haven't been started yet. For socket based IO (or things
    like accept4(2)), we can cancel work that is already running as well.
    
    To cancel a request, the sqe must have ->addr set to the user_data of
    the request it wishes to cancel. If the request is cancelled
    successfully, the original request is completed with -ECANCELED
    and the cancel request is completed with a result of 0. If the
    request was already running, the original may or may not complete
    in error. The cancel request will complete with -EALREADY for that
    case. And finally, if the request to cancel wasn't found, the cancel
    request is completed with -ENOENT.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index f82d90e617a6..6877cf8894db 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -33,6 +33,7 @@ struct io_uring_sqe {
 		__u32		msg_flags;
 		__u32		timeout_flags;
 		__u32		accept_flags;
+		__u32		cancel_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -70,6 +71,7 @@ struct io_uring_sqe {
 #define IORING_OP_TIMEOUT	11
 #define IORING_OP_TIMEOUT_REMOVE	12
 #define IORING_OP_ACCEPT	13
+#define IORING_OP_ASYNC_CANCEL	14
 
 /*
  * sqe->fsync_flags

commit 17f2fe35d080d8f64e86a60cdcd3a97edcbc213b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 17 14:42:58 2019 -0600

    io_uring: add support for IORING_OP_ACCEPT
    
    This allows an application to call accept4() in an async fashion. Like
    other opcodes, we first try a non-blocking accept, then punt to async
    context if we have to.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 6dc5ced1c37a..f82d90e617a6 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -19,7 +19,10 @@ struct io_uring_sqe {
 	__u8	flags;		/* IOSQE_ flags */
 	__u16	ioprio;		/* ioprio for the request */
 	__s32	fd;		/* file descriptor to do IO on */
-	__u64	off;		/* offset into file */
+	union {
+		__u64	off;	/* offset into file */
+		__u64	addr2;
+	};
 	__u64	addr;		/* pointer to buffer or iovecs */
 	__u32	len;		/* buffer size or number of iovecs */
 	union {
@@ -29,6 +32,7 @@ struct io_uring_sqe {
 		__u32		sync_range_flags;
 		__u32		msg_flags;
 		__u32		timeout_flags;
+		__u32		accept_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -65,6 +69,7 @@ struct io_uring_sqe {
 #define IORING_OP_RECVMSG	10
 #define IORING_OP_TIMEOUT	11
 #define IORING_OP_TIMEOUT_REMOVE	12
+#define IORING_OP_ACCEPT	13
 
 /*
  * sqe->fsync_flags

commit 11365043e5271fea4c92189a976833da477a3a44
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Oct 16 09:08:32 2019 -0600

    io_uring: add support for canceling timeout requests
    
    We might have cases where the need for a specific timeout is gone, add
    support for canceling an existing timeout operation. This works like the
    POLL_REMOVE command, where the application passes in the user_data of
    the timeout it wishes to cancel in the sqe->addr field.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index b402dfee5e15..6dc5ced1c37a 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -64,6 +64,7 @@ struct io_uring_sqe {
 #define IORING_OP_SENDMSG	9
 #define IORING_OP_RECVMSG	10
 #define IORING_OP_TIMEOUT	11
+#define IORING_OP_TIMEOUT_REMOVE	12
 
 /*
  * sqe->fsync_flags

commit a41525ab2e75987e809926352ebc6f1397da900e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Oct 15 16:48:15 2019 -0600

    io_uring: add support for absolute timeouts
    
    This is a pretty trivial addition on top of the relative timeouts
    we have now, but it's handy for ensuring tighter timing for those
    that are building scheduling primitives on top of io_uring.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e0137ea6ad79..b402dfee5e15 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -70,6 +70,11 @@ struct io_uring_sqe {
  */
 #define IORING_FSYNC_DATASYNC	(1U << 0)
 
+/*
+ * sqe->timeout_flags
+ */
+#define IORING_TIMEOUT_ABS	(1U << 0)
+
 /*
  * IO completion data structure (Completion Queue Entry)
  */

commit 33a107f0a1b8df0ad925e39d8afc97bb78e0cec1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Oct 4 12:10:03 2019 -0600

    io_uring: allow application controlled CQ ring size
    
    We currently size the CQ ring as twice the SQ ring, to allow some
    flexibility in not overflowing the CQ ring. This is done because the
    SQE life time is different than that of the IO request itself, the SQE
    is consumed as soon as the kernel has seen the entry.
    
    Certain application don't need a huge SQ ring size, since they just
    submit IO in batches. But they may have a lot of requests pending, and
    hence need a big CQ ring to hold them all. By allowing the application
    to control the CQ ring size multiplier, we can cater to those
    applications more efficiently.
    
    If an application wants to define its own CQ ring size, it must set
    IORING_SETUP_CQSIZE in the setup flags, and fill out
    io_uring_params->cq_entries. The value must be a power of two.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 4f532d9c0554..e0137ea6ad79 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -50,6 +50,7 @@ struct io_uring_sqe {
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
+#define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
 
 #define IORING_OP_NOP		0
 #define IORING_OP_READV		1

commit c3a31e605620c279163c14068a60869ea3fda203
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Oct 3 13:59:56 2019 -0600

    io_uring: add support for IORING_REGISTER_FILES_UPDATE
    
    Allows the application to remove/replace/add files to/from a file set.
    Passes in a struct:
    
    struct io_uring_files_update {
            __u32 offset;
            __s32 *fds;
    };
    
    that holds an array of fds, size of array passed in through the usual
    nr_args part of the io_uring_register() system call. The logic is as
    follows:
    
    1) If ->fds[i] is -1, the existing file at i + ->offset is removed from
       the set.
    2) If ->fds[i] is a valid fd, the existing file at i + ->offset is
       replaced with ->fds[i].
    
    For case #2, is the existing file is currently empty (fd == -1), the
    new fd is simply added to the array.
    
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index ea57526a5b89..4f532d9c0554 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -150,5 +150,11 @@ struct io_uring_params {
 #define IORING_UNREGISTER_FILES		3
 #define IORING_REGISTER_EVENTFD		4
 #define IORING_UNREGISTER_EVENTFD	5
+#define IORING_REGISTER_FILES_UPDATE	6
+
+struct io_uring_files_update {
+	__u32 offset;
+	__s32 *fds;
+};
 
 #endif

commit 5262f567987d3c30052b22e78c35c2313d07b230
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Sep 17 12:26:57 2019 -0600

    io_uring: IORING_OP_TIMEOUT support
    
    There's been a few requests for functionality similar to io_getevents()
    and epoll_wait(), where the user can specify a timeout for waiting on
    events. I deliberately did not add support for this through the system
    call initially to avoid overloading the args, but I can see that the use
    cases for this are valid.
    
    This adds support for IORING_OP_TIMEOUT. If a user wants to get woken
    when waiting for events, simply submit one of these timeout commands
    with your wait call (or before). This ensures that the application
    sleeping on the CQ ring waiting for events will get woken. The timeout
    command is passed in as a pointer to a struct timespec. Timeouts are
    relative. The timeout command also includes a way to auto-cancel after
    N events has passed.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 96ee9d94b73e..ea57526a5b89 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -28,6 +28,7 @@ struct io_uring_sqe {
 		__u16		poll_events;
 		__u32		sync_range_flags;
 		__u32		msg_flags;
+		__u32		timeout_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -61,6 +62,7 @@ struct io_uring_sqe {
 #define IORING_OP_SYNC_FILE_RANGE	8
 #define IORING_OP_SENDMSG	9
 #define IORING_OP_RECVMSG	10
+#define IORING_OP_TIMEOUT	11
 
 /*
  * sqe->fsync_flags

commit ac90f249e15cd2a850daa9e36e15f81ce1ff6550
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Sep 6 10:26:21 2019 -0600

    io_uring: expose single mmap capability
    
    After commit 75b28affdd6a we can get by with just a single mmap to
    map both the sq and cq ring. However, userspace doesn't know that.
    
    Add a features variable to io_uring_params, and notify userspace
    that the kernel has this ability. This can then be used in liburing
    (or in applications directly) to avoid the second mmap.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 1e1652f25cc1..96ee9d94b73e 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -128,11 +128,17 @@ struct io_uring_params {
 	__u32 flags;
 	__u32 sq_thread_cpu;
 	__u32 sq_thread_idle;
-	__u32 resv[5];
+	__u32 features;
+	__u32 resv[4];
 	struct io_sqring_offsets sq_off;
 	struct io_cqring_offsets cq_off;
 };
 
+/*
+ * io_uring_params->features flags
+ */
+#define IORING_FEAT_SINGLE_MMAP		(1U << 0)
+
 /*
  * io_uring_register(2) opcodes and arguments
  */

commit aa1fa28fc73ea6b740ee7b62bf3b07141883dbb8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Apr 19 13:38:09 2019 -0600

    io_uring: add support for recvmsg()
    
    This is done through IORING_OP_RECVMSG. This opcode uses the same
    sqe->msg_flags that IORING_OP_SENDMSG added, and we pass in the
    msghdr struct in the sqe->addr field as well.
    
    We use MSG_DONTWAIT to force an inline fast path if recvmsg() doesn't
    block, and punt to async execution if it would have.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index d74742d6269f..1e1652f25cc1 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -60,6 +60,7 @@ struct io_uring_sqe {
 #define IORING_OP_POLL_REMOVE	7
 #define IORING_OP_SYNC_FILE_RANGE	8
 #define IORING_OP_SENDMSG	9
+#define IORING_OP_RECVMSG	10
 
 /*
  * sqe->fsync_flags

commit 0fa03c624d8fc9932d0f27c39a9deca6a37e0e17
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Apr 19 13:34:07 2019 -0600

    io_uring: add support for sendmsg()
    
    This is done through IORING_OP_SENDMSG. There's a new sqe->msg_flags
    for the flags argument, and the msghdr struct is passed in the
    sqe->addr field.
    
    We use MSG_DONTWAIT to force an inline fast path if sendmsg() doesn't
    block, and punt to async execution if it would have.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 10b7c45f6d57..d74742d6269f 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -27,6 +27,7 @@ struct io_uring_sqe {
 		__u32		fsync_flags;
 		__u16		poll_events;
 		__u32		sync_range_flags;
+		__u32		msg_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -58,6 +59,7 @@ struct io_uring_sqe {
 #define IORING_OP_POLL_ADD	6
 #define IORING_OP_POLL_REMOVE	7
 #define IORING_OP_SYNC_FILE_RANGE	8
+#define IORING_OP_SENDMSG	9
 
 /*
  * sqe->fsync_flags

commit 9e645e1105ca60fbbc6bddf2fd5ef7e57ed3dca8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri May 10 16:07:28 2019 -0600

    io_uring: add support for sqe links
    
    With SQE links, we can create chains of dependent SQEs. One example
    would be queueing an SQE that's a read from one file descriptor, with
    the linked SQE being a write to another with the same set of buffers.
    
    An SQE link will not stall the pipeline, it'll just ensure that
    dependent SQEs aren't issued before the previous link has completed.
    
    Any error at submission or completion time will break the chain of SQEs.
    For completions, this also includes short reads or writes, as the next
    SQE could depend on the previous one being fully completed.
    
    Any SQE in a chain that gets canceled due to any of the above errors,
    will get an CQE fill with -ECANCELED as the error value.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a0c460025036..10b7c45f6d57 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -40,6 +40,7 @@ struct io_uring_sqe {
  */
 #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 #define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
+#define IOSQE_IO_LINK		(1U << 2)	/* links next sqe */
 
 /*
  * io_uring_setup() flags

commit 9b402849e80c85eee10bbd341aab3f1a0f942d4f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Apr 11 11:45:41 2019 -0600

    io_uring: add support for eventfd notifications
    
    Allow registration of an eventfd, which will trigger an event every
    time a completion event happens for this io_uring instance.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e707a17c6908..a0c460025036 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -136,5 +136,7 @@ struct io_uring_params {
 #define IORING_UNREGISTER_BUFFERS	1
 #define IORING_REGISTER_FILES		2
 #define IORING_UNREGISTER_FILES		3
+#define IORING_REGISTER_EVENTFD		4
+#define IORING_UNREGISTER_EVENTFD	5
 
 #endif

commit 5d17b4a4b7fa172b205be8a05051ae705d1dc3bb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Apr 9 14:56:44 2019 -0600

    io_uring: add support for IORING_OP_SYNC_FILE_RANGE
    
    This behaves just like sync_file_range(2) does.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a7a6384d0c70..e707a17c6908 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -26,6 +26,7 @@ struct io_uring_sqe {
 		__kernel_rwf_t	rw_flags;
 		__u32		fsync_flags;
 		__u16		poll_events;
+		__u32		sync_range_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -55,6 +56,7 @@ struct io_uring_sqe {
 #define IORING_OP_WRITE_FIXED	5
 #define IORING_OP_POLL_ADD	6
 #define IORING_OP_POLL_REMOVE	7
+#define IORING_OP_SYNC_FILE_RANGE	8
 
 /*
  * sqe->fsync_flags

commit de0617e467171ba44c73efd1ba63f101b164a035
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sat Apr 6 21:51:27 2019 -0600

    io_uring: add support for marking commands as draining
    
    There are no ordering constraints between the submission and completion
    side of io_uring. But sometimes that would be useful to have. One common
    example is doing an fsync, for instance, and have it ordered with
    previous writes. Without support for that, the application must do this
    tracking itself.
    
    This adds a general SQE flag, IOSQE_IO_DRAIN. If a command is marked
    with this flag, then it will not be issued before previous commands have
    completed, and subsequent commands submitted after the drain will not be
    issued before the drain is started.. If there are no pending commands,
    setting this flag will not change the behavior of the issue of the
    command.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e23408692118..a7a6384d0c70 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -38,6 +38,7 @@ struct io_uring_sqe {
  * sqe->flags
  */
 #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
+#define IOSQE_IO_DRAIN		(1U << 1)	/* issue after inflight IO */
 
 /*
  * io_uring_setup() flags

commit 221c5eb2338232f7340386de1c43decc32682e58
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Jan 17 09:41:58 2019 -0700

    io_uring: add support for IORING_OP_POLL
    
    This is basically a direct port of bfe4037e722e, which implements a
    one-shot poll command through aio. Description below is based on that
    commit as well. However, instead of adding a POLL command and relying
    on io_cancel(2) to remove it, we mimic the epoll(2) interface of
    having a command to add a poll notification, IORING_OP_POLL_ADD,
    and one to remove it again, IORING_OP_POLL_REMOVE.
    
    To poll for a file descriptor the application should submit an sqe of
    type IORING_OP_POLL. It will poll the fd for the events specified in the
    poll_events field.
    
    Unlike poll or epoll without EPOLLONESHOT this interface always works in
    one shot mode, that is once the sqe is completed, it will have to be
    resubmitted.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Based-on-code-from: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 0ec74bab8dbe..e23408692118 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -25,6 +25,7 @@ struct io_uring_sqe {
 	union {
 		__kernel_rwf_t	rw_flags;
 		__u32		fsync_flags;
+		__u16		poll_events;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	union {
@@ -51,6 +52,8 @@ struct io_uring_sqe {
 #define IORING_OP_FSYNC		3
 #define IORING_OP_READ_FIXED	4
 #define IORING_OP_WRITE_FIXED	5
+#define IORING_OP_POLL_ADD	6
+#define IORING_OP_POLL_REMOVE	7
 
 /*
  * sqe->fsync_flags

commit 6c271ce2f1d572f7fa225700a13cfe7ced492434
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Jan 10 11:22:30 2019 -0700

    io_uring: add submission polling
    
    This enables an application to do IO, without ever entering the kernel.
    By using the SQ ring to fill in new sqes and watching for completions
    on the CQ ring, we can submit and reap IOs without doing a single system
    call. The kernel side thread will poll for new submissions, and in case
    of HIPRI/polled IO, it'll also poll for completions.
    
    By default, we allow 1 second of active spinning. This can by changed
    by passing in a different grace period at io_uring_register(2) time.
    If the thread exceeds this idle time without having any work to do, it
    will set:
    
    sq_ring->flags |= IORING_SQ_NEED_WAKEUP.
    
    The application will have to call io_uring_enter() to start things back
    up again. If IO is kept busy, that will never be needed. Basically an
    application that has this feature enabled will guard it's
    io_uring_enter(2) call with:
    
    read_barrier();
    if (*sq_ring->flags & IORING_SQ_NEED_WAKEUP)
            io_uring_enter(fd, 0, 0, IORING_ENTER_SQ_WAKEUP);
    
    instead of calling it unconditionally.
    
    It's mandatory to use fixed files with this feature. Failure to do so
    will result in the application getting an -EBADF CQ entry when
    submitting IO.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 6257478d55e9..0ec74bab8dbe 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -42,6 +42,8 @@ struct io_uring_sqe {
  * io_uring_setup() flags
  */
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+#define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
+#define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 
 #define IORING_OP_NOP		0
 #define IORING_OP_READV		1
@@ -86,6 +88,11 @@ struct io_sqring_offsets {
 	__u64 resv2;
 };
 
+/*
+ * sq_ring->flags
+ */
+#define IORING_SQ_NEED_WAKEUP	(1U << 0) /* needs io_uring_enter wakeup */
+
 struct io_cqring_offsets {
 	__u32 head;
 	__u32 tail;
@@ -100,6 +107,7 @@ struct io_cqring_offsets {
  * io_uring_enter(2) flags
  */
 #define IORING_ENTER_GETEVENTS	(1U << 0)
+#define IORING_ENTER_SQ_WAKEUP	(1U << 1)
 
 /*
  * Passed in for io_uring_setup(2). Copied back with updated info on success
@@ -108,7 +116,9 @@ struct io_uring_params {
 	__u32 sq_entries;
 	__u32 cq_entries;
 	__u32 flags;
-	__u32 resv[7];
+	__u32 sq_thread_cpu;
+	__u32 sq_thread_idle;
+	__u32 resv[5];
 	struct io_sqring_offsets sq_off;
 	struct io_cqring_offsets cq_off;
 };

commit 6b06314c47e141031be043539900d80d2c7ba10f
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Jan 10 22:13:58 2019 -0700

    io_uring: add file set registration
    
    We normally have to fget/fput for each IO we do on a file. Even with
    the batching we do, the cost of the atomic inc/dec of the file usage
    count adds up.
    
    This adds IORING_REGISTER_FILES, and IORING_UNREGISTER_FILES opcodes
    for the io_uring_register(2) system call. The arguments passed in must
    be an array of __s32 holding file descriptors, and nr_args should hold
    the number of file descriptors the application wishes to pin for the
    duration of the io_uring instance (or until IORING_UNREGISTER_FILES is
    called).
    
    When used, the application must set IOSQE_FIXED_FILE in the sqe->flags
    member. Then, instead of setting sqe->fd to the real fd, it sets sqe->fd
    to the index in the array passed in to IORING_REGISTER_FILES.
    
    Files are automatically unregistered when the io_uring instance is torn
    down. An application need only unregister if it wishes to register a new
    set of fds.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index cf28f7a11f12..6257478d55e9 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -16,7 +16,7 @@
  */
 struct io_uring_sqe {
 	__u8	opcode;		/* type of operation for this sqe */
-	__u8	flags;		/* as of now unused */
+	__u8	flags;		/* IOSQE_ flags */
 	__u16	ioprio;		/* ioprio for the request */
 	__s32	fd;		/* file descriptor to do IO on */
 	__u64	off;		/* offset into file */
@@ -33,6 +33,11 @@ struct io_uring_sqe {
 	};
 };
 
+/*
+ * sqe->flags
+ */
+#define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
+
 /*
  * io_uring_setup() flags
  */
@@ -113,5 +118,7 @@ struct io_uring_params {
  */
 #define IORING_REGISTER_BUFFERS		0
 #define IORING_UNREGISTER_BUFFERS	1
+#define IORING_REGISTER_FILES		2
+#define IORING_UNREGISTER_FILES		3
 
 #endif

commit edafccee56ff31678a091ddb7219aba9b28bc3cb
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 9 09:16:05 2019 -0700

    io_uring: add support for pre-mapped user IO buffers
    
    If we have fixed user buffers, we can map them into the kernel when we
    setup the io_uring. That avoids the need to do get_user_pages() for
    each and every IO.
    
    To utilize this feature, the application must call io_uring_register()
    after having setup an io_uring instance, passing in
    IORING_REGISTER_BUFFERS as the opcode. The argument must be a pointer to
    an iovec array, and the nr_args should contain how many iovecs the
    application wishes to map.
    
    If successful, these buffers are now mapped into the kernel, eligible
    for IO. To use these fixed buffers, the application must use the
    IORING_OP_READ_FIXED and IORING_OP_WRITE_FIXED opcodes, and then
    set sqe->index to the desired buffer index. sqe->addr..sqe->addr+seq->len
    must point to somewhere inside the indexed buffer.
    
    The application may register buffers throughout the lifetime of the
    io_uring instance. It can call io_uring_register() with
    IORING_UNREGISTER_BUFFERS as the opcode to unregister the current set of
    buffers, and then register a new set. The application need not
    unregister buffers explicitly before shutting down the io_uring
    instance.
    
    It's perfectly valid to setup a larger buffer, and then sometimes only
    use parts of it for an IO. As long as the range is within the originally
    mapped region, it will work just fine.
    
    For now, buffers must not be file backed. If file backed buffers are
    passed in, the registration will fail with -1/EOPNOTSUPP. This
    restriction may be relaxed in the future.
    
    RLIMIT_MEMLOCK is used to check how much memory we can pin. A somewhat
    arbitrary 1G per buffer size is also imposed.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 5c457ea396e6..cf28f7a11f12 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -27,7 +27,10 @@ struct io_uring_sqe {
 		__u32		fsync_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
-	__u64	__pad2[3];
+	union {
+		__u16	buf_index;	/* index into fixed buffers, if used */
+		__u64	__pad2[3];
+	};
 };
 
 /*
@@ -39,6 +42,8 @@ struct io_uring_sqe {
 #define IORING_OP_READV		1
 #define IORING_OP_WRITEV	2
 #define IORING_OP_FSYNC		3
+#define IORING_OP_READ_FIXED	4
+#define IORING_OP_WRITE_FIXED	5
 
 /*
  * sqe->fsync_flags
@@ -103,4 +108,10 @@ struct io_uring_params {
 	struct io_cqring_offsets cq_off;
 };
 
+/*
+ * io_uring_register(2) opcodes and arguments
+ */
+#define IORING_REGISTER_BUFFERS		0
+#define IORING_UNREGISTER_BUFFERS	1
+
 #endif

commit def596e9557c91d9846fc4d84d26f2c564644416
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jan 9 08:59:42 2019 -0700

    io_uring: support for IO polling
    
    Add support for a polled io_uring instance. When a read or write is
    submitted to a polled io_uring, the application must poll for
    completions on the CQ ring through io_uring_enter(2). Polled IO may not
    generate IRQ completions, hence they need to be actively found by the
    application itself.
    
    To use polling, io_uring_setup() must be used with the
    IORING_SETUP_IOPOLL flag being set. It is illegal to mix and match
    polled and non-polled IO on an io_uring.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 4589d56d0b68..5c457ea396e6 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -30,6 +30,11 @@ struct io_uring_sqe {
 	__u64	__pad2[3];
 };
 
+/*
+ * io_uring_setup() flags
+ */
+#define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+
 #define IORING_OP_NOP		0
 #define IORING_OP_READV		1
 #define IORING_OP_WRITEV	2

commit c992fe2925d776be066d9f6cc13f9ea11d78b657
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 11 09:43:02 2019 -0700

    io_uring: add fsync support
    
    Add a new fsync opcode, which either syncs a range if one is passed,
    or the whole file if the offset and length fields are both cleared
    to zero.  A flag is provided to use fdatasync semantics, that is only
    force out metadata which is required to retrieve the file data, but
    not others like metadata.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index ac692823d6f4..4589d56d0b68 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -24,7 +24,7 @@ struct io_uring_sqe {
 	__u32	len;		/* buffer size or number of iovecs */
 	union {
 		__kernel_rwf_t	rw_flags;
-		__u32		__resv;
+		__u32		fsync_flags;
 	};
 	__u64	user_data;	/* data to be passed back at completion time */
 	__u64	__pad2[3];
@@ -33,6 +33,12 @@ struct io_uring_sqe {
 #define IORING_OP_NOP		0
 #define IORING_OP_READV		1
 #define IORING_OP_WRITEV	2
+#define IORING_OP_FSYNC		3
+
+/*
+ * sqe->fsync_flags
+ */
+#define IORING_FSYNC_DATASYNC	(1U << 0)
 
 /*
  * IO completion data structure (Completion Queue Entry)

commit 2b188cc1bb857a9d4701ae59aa7768b5124e262e
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Jan 7 10:46:33 2019 -0700

    Add io_uring IO interface
    
    The submission queue (SQ) and completion queue (CQ) rings are shared
    between the application and the kernel. This eliminates the need to
    copy data back and forth to submit and complete IO.
    
    IO submissions use the io_uring_sqe data structure, and completions
    are generated in the form of io_uring_cqe data structures. The SQ
    ring is an index into the io_uring_sqe array, which makes it possible
    to submit a batch of IOs without them being contiguous in the ring.
    The CQ ring is always contiguous, as completion events are inherently
    unordered, and hence any io_uring_cqe entry can point back to an
    arbitrary submission.
    
    Two new system calls are added for this:
    
    io_uring_setup(entries, params)
            Sets up an io_uring instance for doing async IO. On success,
            returns a file descriptor that the application can mmap to
            gain access to the SQ ring, CQ ring, and io_uring_sqes.
    
    io_uring_enter(fd, to_submit, min_complete, flags, sigset, sigsetsize)
            Initiates IO against the rings mapped to this fd, or waits for
            them to complete, or both. The behavior is controlled by the
            parameters passed in. If 'to_submit' is non-zero, then we'll
            try and submit new IO. If IORING_ENTER_GETEVENTS is set, the
            kernel will wait for 'min_complete' events, if they aren't
            already available. It's valid to set IORING_ENTER_GETEVENTS
            and 'min_complete' == 0 at the same time, this allows the
            kernel to return already completed events without waiting
            for them. This is useful only for polling, as for IRQ
            driven IO, the application can just check the CQ ring
            without entering the kernel.
    
    With this setup, it's possible to do async IO with a single system
    call. Future developments will enable polled IO with this interface,
    and polled submission as well. The latter will enable an application
    to do IO without doing ANY system calls at all.
    
    For IRQ driven IO, an application only needs to enter the kernel for
    completions if it wants to wait for them to occur.
    
    Each io_uring is backed by a workqueue, to support buffered async IO
    as well. We will only punt to an async context if the command would
    need to wait for IO on the device side. Any data that can be accessed
    directly in the page cache is done inline. This avoids the slowness
    issue of usual threadpools, since cached data is accessed as quickly
    as a sync interface.
    
    Sample application: http://git.kernel.dk/cgit/fio/plain/t/io_uring.c
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
new file mode 100644
index 000000000000..ac692823d6f4
--- /dev/null
+++ b/include/uapi/linux/io_uring.h
@@ -0,0 +1,95 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Header file for the io_uring interface.
+ *
+ * Copyright (C) 2019 Jens Axboe
+ * Copyright (C) 2019 Christoph Hellwig
+ */
+#ifndef LINUX_IO_URING_H
+#define LINUX_IO_URING_H
+
+#include <linux/fs.h>
+#include <linux/types.h>
+
+/*
+ * IO submission data structure (Submission Queue Entry)
+ */
+struct io_uring_sqe {
+	__u8	opcode;		/* type of operation for this sqe */
+	__u8	flags;		/* as of now unused */
+	__u16	ioprio;		/* ioprio for the request */
+	__s32	fd;		/* file descriptor to do IO on */
+	__u64	off;		/* offset into file */
+	__u64	addr;		/* pointer to buffer or iovecs */
+	__u32	len;		/* buffer size or number of iovecs */
+	union {
+		__kernel_rwf_t	rw_flags;
+		__u32		__resv;
+	};
+	__u64	user_data;	/* data to be passed back at completion time */
+	__u64	__pad2[3];
+};
+
+#define IORING_OP_NOP		0
+#define IORING_OP_READV		1
+#define IORING_OP_WRITEV	2
+
+/*
+ * IO completion data structure (Completion Queue Entry)
+ */
+struct io_uring_cqe {
+	__u64	user_data;	/* sqe->data submission passed back */
+	__s32	res;		/* result code for this event */
+	__u32	flags;
+};
+
+/*
+ * Magic offsets for the application to mmap the data it needs
+ */
+#define IORING_OFF_SQ_RING		0ULL
+#define IORING_OFF_CQ_RING		0x8000000ULL
+#define IORING_OFF_SQES			0x10000000ULL
+
+/*
+ * Filled with the offset for mmap(2)
+ */
+struct io_sqring_offsets {
+	__u32 head;
+	__u32 tail;
+	__u32 ring_mask;
+	__u32 ring_entries;
+	__u32 flags;
+	__u32 dropped;
+	__u32 array;
+	__u32 resv1;
+	__u64 resv2;
+};
+
+struct io_cqring_offsets {
+	__u32 head;
+	__u32 tail;
+	__u32 ring_mask;
+	__u32 ring_entries;
+	__u32 overflow;
+	__u32 cqes;
+	__u64 resv[2];
+};
+
+/*
+ * io_uring_enter(2) flags
+ */
+#define IORING_ENTER_GETEVENTS	(1U << 0)
+
+/*
+ * Passed in for io_uring_setup(2). Copied back with updated info on success
+ */
+struct io_uring_params {
+	__u32 sq_entries;
+	__u32 cq_entries;
+	__u32 flags;
+	__u32 resv[7];
+	struct io_sqring_offsets sq_off;
+	struct io_cqring_offsets cq_off;
+};
+
+#endif
