commit e2e966636aee99971965c0965a4c11b8ce077da3
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Wed Nov 13 19:02:42 2019 +0100

    drm/ttm, drm/vmwgfx: Use a configuration option for the TTM dma page pool
    
    Drivers like vmwgfx may want to test whether the dma page pool is present
    or not. Since it's activated by default by TTM if compiled-in, define a
    hidden configuration option that the driver can test for.
    
    Cc: Christian König <christian.koenig@amd.com>
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 4d9b019d253c..a6b6ef5f9bf4 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -74,7 +74,7 @@ void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt);
  */
 int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
 
-#if defined(CONFIG_SWIOTLB) || defined(CONFIG_INTEL_IOMMU)
+#if defined(CONFIG_DRM_TTM_DMA_PAGE_POOL)
 /**
  * Initialize pool allocator.
  */

commit d0cef9fa4411eb17dd350cced3336ca58f465ff1
Author: Roger He <Hongbo.He@amd.com>
Date:   Thu Dec 21 17:42:50 2017 +0800

    drm/ttm: use an operation ctx for ttm_tt_populate in ttm_bo_driver (v2)
    
    forward the operation context to ttm_tt_populate as well,
    and the ultimate goal is swapout enablement for reserved BOs.
    
    v2: squash in fix for vboxvideo
    
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Roger He <Hongbo.He@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 593811362a91..4d9b019d253c 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -47,7 +47,7 @@ void ttm_page_alloc_fini(void);
  *
  * Add backing pages to all of @ttm
  */
-int ttm_pool_populate(struct ttm_tt *ttm);
+int ttm_pool_populate(struct ttm_tt *ttm, struct ttm_operation_ctx *ctx);
 
 /**
  * ttm_pool_unpopulate:
@@ -61,7 +61,8 @@ void ttm_pool_unpopulate(struct ttm_tt *ttm);
 /**
  * Populates and DMA maps pages to fullfil a ttm_dma_populate() request
  */
-int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt);
+int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt,
+				struct ttm_operation_ctx *ctx);
 
 /**
  * Unpopulates and DMA unmaps pages as part of a
@@ -89,7 +90,8 @@ void ttm_dma_page_alloc_fini(void);
  */
 int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
 
-int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
+int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev,
+			struct ttm_operation_ctx *ctx);
 void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 
 #else
@@ -106,7 +108,8 @@ static inline int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data)
 	return 0;
 }
 static inline int ttm_dma_populate(struct ttm_dma_tt *ttm_dma,
-				   struct device *dev)
+				struct device *dev,
+				struct ttm_operation_ctx *ctx)
 {
 	return -ENOMEM;
 }

commit 1569d651f152870663fabd8f1c80af353f967ad5
Author: Christian König <christian.koenig@amd.com>
Date:   Mon Nov 27 13:12:35 2017 +0100

    drm/ttm: fix populate_and_map() functions once more
    
    This reverts "drm/ttm: Fix configuration error around populate_and_map()
    functions".
    
    This fix has gone into the wrong direction. Those helpers should be
    available even when neither CONFIG_INTEL_IOMMU nor CONFIG_SWIOTLB are
    set.
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Michel Dänzer <michel.daenzer@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 38a2b4770c35..593811362a91 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -58,12 +58,21 @@ int ttm_pool_populate(struct ttm_tt *ttm);
  */
 void ttm_pool_unpopulate(struct ttm_tt *ttm);
 
+/**
+ * Populates and DMA maps pages to fullfil a ttm_dma_populate() request
+ */
+int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt);
+
+/**
+ * Unpopulates and DMA unmaps pages as part of a
+ * ttm_dma_unpopulate() request */
+void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt);
+
 /**
  * Output the state of pools to debugfs file
  */
 int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
 
-
 #if defined(CONFIG_SWIOTLB) || defined(CONFIG_INTEL_IOMMU)
 /**
  * Initialize pool allocator.
@@ -83,17 +92,6 @@ int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
 int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 
-
-/**
- * Populates and DMA maps pages to fullfil a ttm_dma_populate() request
- */
-int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt);
-
-/**
- * Unpopulates and DMA unmaps pages as part of a
- * ttm_dma_unpopulate() request */
-void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt);
-
 #else
 static inline int ttm_dma_page_alloc_init(struct ttm_mem_global *glob,
 					  unsigned max_pages)
@@ -116,16 +114,6 @@ static inline void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma,
 				      struct device *dev)
 {
 }
-
-static inline int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt)
-{
-	return -ENOMEM;
-}
-
-static inline void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt)
-{
-}
-
 #endif
 
 #endif

commit 96bec198352799794b0f8937620e811ef8b9fa22
Author: Tom St Denis <tom.stdenis@amd.com>
Date:   Thu Aug 24 06:46:39 2017 -0400

    drm/ttm: Remove needless 'extern' on functions in header.
    
    Minor tidy up.
    
    Signed-off-by: Tom St Denis <tom.stdenis@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index bf21166f2b97..38a2b4770c35 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -47,7 +47,7 @@ void ttm_page_alloc_fini(void);
  *
  * Add backing pages to all of @ttm
  */
-extern int ttm_pool_populate(struct ttm_tt *ttm);
+int ttm_pool_populate(struct ttm_tt *ttm);
 
 /**
  * ttm_pool_unpopulate:
@@ -56,12 +56,12 @@ extern int ttm_pool_populate(struct ttm_tt *ttm);
  *
  * Free all pages of @ttm
  */
-extern void ttm_pool_unpopulate(struct ttm_tt *ttm);
+void ttm_pool_unpopulate(struct ttm_tt *ttm);
 
 /**
  * Output the state of pools to debugfs file
  */
-extern int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
+int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
 
 
 #if defined(CONFIG_SWIOTLB) || defined(CONFIG_INTEL_IOMMU)
@@ -78,10 +78,10 @@ void ttm_dma_page_alloc_fini(void);
 /**
  * Output the state of pools to debugfs file
  */
-extern int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
+int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
 
-extern int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
-extern void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
+int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
+void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 
 
 /**

commit a4dec819c8bba6365eb893a4ca88db4dd1210110
Author: Tom St Denis <tom.stdenis@amd.com>
Date:   Fri Aug 18 10:04:57 2017 -0400

    drm/ttm: Add helper functions to populate/map in one call (v2)
    
    These functions replace a section of common code found
    in radeon/amdgpu drivers (and possibly others) as part
    of the ttm_tt_*populate() callbacks.
    
    v2: squash in fix for sw iommu from Tom
    
    Signed-off-by: Tom St Denis <tom.stdenis@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 49a828425fa2..bf21166f2b97 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -83,6 +83,17 @@ extern int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
 extern int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 extern void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 
+
+/**
+ * Populates and DMA maps pages to fullfil a ttm_dma_populate() request
+ */
+int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt);
+
+/**
+ * Unpopulates and DMA unmaps pages as part of a
+ * ttm_dma_unpopulate() request */
+void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt);
+
 #else
 static inline int ttm_dma_page_alloc_init(struct ttm_mem_global *glob,
 					  unsigned max_pages)
@@ -105,6 +116,16 @@ static inline void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma,
 				      struct device *dev)
 {
 }
+
+static inline int ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt)
+{
+	return -ENOMEM;
+}
+
+static inline void ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt)
+{
+}
+
 #endif
 
 #endif

commit 728a0cdf065b9f42d1ff25d94858f824bb0989ba
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Sun Feb 9 18:43:18 2014 +0900

    drm/ttm: declare 'struct device' in ttm_page_alloc.h
    
    Declare 'struct device' explicitly in ttm_page_alloc.h as this file
    does not include any file declaring it. This removes the following
    warning:
    
            warning: 'struct device' declared inside parameter list
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Reviewed-by: Thierry Reding <treding@nvidia.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index d1f61bfe0ebe..49a828425fa2 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -29,6 +29,8 @@
 #include <drm/ttm/ttm_bo_driver.h>
 #include <drm/ttm/ttm_memory.h>
 
+struct device;
+
 /**
  * Initialize pool allocator.
  */

commit 7aeb7448d8d02868ef30a6d08e856b2220319273
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Thu Oct 24 01:24:54 2013 -0700

    drm/ttm: Enable the dma page pool also for intel IOMMUs
    
    Used by the vmwgfx driver
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Jakob Bornecrantz <jakob@vmware.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 706b962c6467..d1f61bfe0ebe 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -62,7 +62,7 @@ extern void ttm_pool_unpopulate(struct ttm_tt *ttm);
 extern int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
 
 
-#ifdef CONFIG_SWIOTLB
+#if defined(CONFIG_SWIOTLB) || defined(CONFIG_INTEL_IOMMU)
 /**
  * Initialize pool allocator.
  */
@@ -94,6 +94,15 @@ static inline int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data)
 {
 	return 0;
 }
+static inline int ttm_dma_populate(struct ttm_dma_tt *ttm_dma,
+				   struct device *dev)
+{
+	return -ENOMEM;
+}
+static inline void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma,
+				      struct device *dev)
+{
+}
 #endif
 
 #endif

commit a1ce39288e6fbefdd8d607021d02384eb4a20b99
Author: David Howells <dhowells@redhat.com>
Date:   Tue Oct 2 18:01:25 2012 +0100

    UAPI: (Scripted) Convert #include "..." to #include <path/...> in kernel system headers
    
    Convert #include "..." to #include <path/...> in kernel system headers.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 5fe27400d176..706b962c6467 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -26,8 +26,8 @@
 #ifndef TTM_PAGE_ALLOC
 #define TTM_PAGE_ALLOC
 
-#include "ttm_bo_driver.h"
-#include "ttm_memory.h"
+#include <drm/ttm/ttm_bo_driver.h>
+#include <drm/ttm/ttm_memory.h>
 
 /**
  * Initialize pool allocator.

commit 8e7e70522d760c4ccd4cd370ebfa0ba69e006c6e
Author: Jerome Glisse <jglisse@redhat.com>
Date:   Wed Nov 9 17:15:26 2011 -0500

    drm/ttm: isolate dma data from ttm_tt V4
    
    Move dma data to a superset ttm_dma_tt structure which herit
    from ttm_tt. This allow driver that don't use dma functionalities
    to not have to waste memory for it.
    
    V2 Rebase on top of no memory account changes (where/when is my
       delorean when i need it ?)
    V3 Make sure page list is initialized empty
    V4 typo/syntax fixes
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 1e1337e81f31..5fe27400d176 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -29,35 +29,6 @@
 #include "ttm_bo_driver.h"
 #include "ttm_memory.h"
 
-/**
- * Get count number of pages from pool to pages list.
- *
- * @pages: head of empty linked list where pages are filled.
- * @flags: ttm flags for page allocation.
- * @cstate: ttm caching state for the page.
- * @count: number of pages to allocate.
- * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
- */
-int ttm_get_pages(struct page **pages,
-		  int flags,
-		  enum ttm_caching_state cstate,
-		  unsigned npages,
-		  dma_addr_t *dma_address);
-/**
- * Put linked list of pages to pool.
- *
- * @pages: list of pages to free.
- * @page_count: number of pages in the list. Zero can be passed for unknown
- * count.
- * @flags: ttm flags for page allocation.
- * @cstate: ttm caching state.
- * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
- */
-void ttm_put_pages(struct page **pages,
-		   unsigned npages,
-		   int flags,
-		   enum ttm_caching_state cstate,
-		   dma_addr_t *dma_address);
 /**
  * Initialize pool allocator.
  */
@@ -107,8 +78,8 @@ void ttm_dma_page_alloc_fini(void);
  */
 extern int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
 
-int ttm_dma_populate(struct ttm_tt *ttm, struct device *dev);
-extern void ttm_dma_unpopulate(struct ttm_tt *ttm, struct device *dev);
+extern int ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev);
+extern void ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev);
 
 #else
 static inline int ttm_dma_page_alloc_init(struct ttm_mem_global *glob,

commit 2334b75ffbef6b8932f09ec4418b65ddb764ae99
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu Nov 3 16:46:34 2011 -0400

    drm/ttm: provide dma aware ttm page pool code V9
    
    In TTM world the pages for the graphic drivers are kept in three different
    pools: write combined, uncached, and cached (write-back). When the pages
    are used by the graphic driver the graphic adapter via its built in MMU
    (or AGP) programs these pages in. The programming requires the virtual address
    (from the graphic adapter perspective) and the physical address (either System RAM
    or the memory on the card) which is obtained using the pci_map_* calls (which does the
    virtual to physical - or bus address translation). During the graphic application's
    "life" those pages can be shuffled around, swapped out to disk, moved from the
    VRAM to System RAM or vice-versa. This all works with the existing TTM pool code
    - except when we want to use the software IOTLB (SWIOTLB) code to "map" the physical
    addresses to the graphic adapter MMU. We end up programming the bounce buffer's
    physical address instead of the TTM pool memory's and get a non-worky driver.
    There are two solutions:
    1) using the DMA API to allocate pages that are screened by the DMA API, or
    2) using the pci_sync_* calls to copy the pages from the bounce-buffer and back.
    
    This patch fixes the issue by allocating pages using the DMA API. The second
    is a viable option - but it has performance drawbacks and potential correctness
    issues - think of the write cache page being bounced (SWIOTLB->TTM), the
    WC is set on the TTM page and the copy from SWIOTLB not making it to the TTM
    page until the page has been recycled in the pool (and used by another application).
    
    The bounce buffer does not get activated often - only in cases where we have
    a 32-bit capable card and we want to use a page that is allocated above the
    4GB limit. The bounce buffer offers the solution of copying the contents
    of that 4GB page to an location below 4GB and then back when the operation has been
    completed (or vice-versa). This is done by using the 'pci_sync_*' calls.
    Note: If you look carefully enough in the existing TTM page pool code you will
    notice the GFP_DMA32 flag is used  - which should guarantee that the provided page
    is under 4GB. It certainly is the case, except this gets ignored in two cases:
     - If user specifies 'swiotlb=force' which bounces _every_ page.
     - If user is using a Xen's PV Linux guest (which uses the SWIOTLB and the
       underlaying PFN's aren't necessarily under 4GB).
    
    To not have this extra copying done the other option is to allocate the pages
    using the DMA API so that there is not need to map the page and perform the
    expensive 'pci_sync_*' calls.
    
    This DMA API capable TTM pool requires for this the 'struct device' to
    properly call the DMA API. It also has to track the virtual and bus address of
    the page being handed out in case it ends up being swapped out or de-allocated -
    to make sure it is de-allocated using the proper's 'struct device'.
    
    Implementation wise the code keeps two lists: one that is attached to the
    'struct device' (via the dev->dma_pools list) and a global one to be used when
    the 'struct device' is unavailable (think shrinker code). The global list can
    iterate over all of the 'struct device' and its associated dma_pool. The list
    in dev->dma_pools can only iterate the device's dma_pool.
                                                                /[struct device_pool]\
            /---------------------------------------------------| dev                |
           /                                            +-------| dma_pool           |
     /-----+------\                                    /        \--------------------/
     |struct device|     /-->[struct dma_pool for WC]</         /[struct device_pool]\
     | dma_pools   +----+                                     /-| dev                |
     |  ...        |    \--->[struct dma_pool for uncached]<-/--| dma_pool           |
     \-----+------/                                         /   \--------------------/
            \----------------------------------------------/
    [Two pools associated with the device (WC and UC), and the parallel list
    containing the 'struct dev' and 'struct dma_pool' entries]
    
    The maximum amount of dma pools a device can have is six: write-combined,
    uncached, and cached; then there are the DMA32 variants which are:
    write-combined dma32, uncached dma32, and cached dma32.
    
    Currently this code only gets activated when any variant of the SWIOTLB IOMMU
    code is running (Intel without VT-d, AMD without GART, IBM Calgary and Xen PV
    with PCI devices).
    
    Tested-by: Michel Dänzer <michel@daenzer.net>
    [v1: Using swiotlb_nr_tbl instead of swiotlb_enabled]
    [v2: Major overhaul - added 'inuse_list' to seperate used from inuse and reorder
    the order of lists to get better performance.]
    [v3: Added comments/and some logic based on review, Added Jerome tag]
    [v4: rebase on top of ttm_tt & ttm_backend merge]
    [v5: rebase on top of ttm memory accounting overhaul]
    [v6: New rebase on top of more memory accouting changes]
    [v7: well rebase on top of no memory accounting changes]
    [v8: make sure pages list is initialized empty]
    [v9: calll ttm_mem_global_free_page in unpopulate for accurate accountg]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Jerome Glisse <jglisse@redhat.com>
    Acked-by: Thomas Hellstrom <thellstrom@vmware.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 18deeee23494..1e1337e81f31 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -89,4 +89,40 @@ extern void ttm_pool_unpopulate(struct ttm_tt *ttm);
  * Output the state of pools to debugfs file
  */
 extern int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
+
+
+#ifdef CONFIG_SWIOTLB
+/**
+ * Initialize pool allocator.
+ */
+int ttm_dma_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages);
+
+/**
+ * Free pool allocator.
+ */
+void ttm_dma_page_alloc_fini(void);
+
+/**
+ * Output the state of pools to debugfs file
+ */
+extern int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data);
+
+int ttm_dma_populate(struct ttm_tt *ttm, struct device *dev);
+extern void ttm_dma_unpopulate(struct ttm_tt *ttm, struct device *dev);
+
+#else
+static inline int ttm_dma_page_alloc_init(struct ttm_mem_global *glob,
+					  unsigned max_pages)
+{
+	return -ENODEV;
+}
+
+static inline void ttm_dma_page_alloc_fini(void) { return; }
+
+static inline int ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data)
+{
+	return 0;
+}
+#endif
+
 #endif

commit b1e5f172325547270f35e7d1e42416a606e1dbd2
Author: Jerome Glisse <jglisse@redhat.com>
Date:   Wed Nov 2 23:59:28 2011 -0400

    drm/ttm: introduce callback for ttm_tt populate & unpopulate V4
    
    Move the page allocation and freeing to driver callback and
    provide ttm code helper function for those.
    
    Most intrusive change, is the fact that we now only fully
    populate an object this simplify some of code designed around
    the page fault design.
    
    V2 Rebase on top of memory accounting overhaul
    V3 New rebase on top of more memory accouting changes
    V4 Rebase on top of no memory account changes (where/when is my
       delorean when i need it ?)
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index fe61c8d85684..18deeee23494 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -67,6 +67,24 @@ int ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages);
  */
 void ttm_page_alloc_fini(void);
 
+/**
+ * ttm_pool_populate:
+ *
+ * @ttm: The struct ttm_tt to contain the backing pages.
+ *
+ * Add backing pages to all of @ttm
+ */
+extern int ttm_pool_populate(struct ttm_tt *ttm);
+
+/**
+ * ttm_pool_unpopulate:
+ *
+ * @ttm: The struct ttm_tt which to free backing pages.
+ *
+ * Free all pages of @ttm
+ */
+extern void ttm_pool_unpopulate(struct ttm_tt *ttm);
+
 /**
  * Output the state of pools to debugfs file
  */

commit 822c4d9ae0d55a4fcea9f0a462bc6406a06692e2
Author: Jerome Glisse <jglisse@redhat.com>
Date:   Thu Nov 10 18:24:09 2011 -0500

    drm/ttm: page allocation use page array instead of list
    
    Use the ttm_tt pages array for pages allocations, move the list
    unwinding into the page allocation functions.
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 129de12353f1..fe61c8d85684 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -38,10 +38,10 @@
  * @count: number of pages to allocate.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
  */
-int ttm_get_pages(struct list_head *pages,
+int ttm_get_pages(struct page **pages,
 		  int flags,
 		  enum ttm_caching_state cstate,
-		  unsigned count,
+		  unsigned npages,
 		  dma_addr_t *dma_address);
 /**
  * Put linked list of pages to pool.
@@ -53,8 +53,8 @@ int ttm_get_pages(struct list_head *pages,
  * @cstate: ttm caching state.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
  */
-void ttm_put_pages(struct list_head *pages,
-		   unsigned page_count,
+void ttm_put_pages(struct page **pages,
+		   unsigned npages,
 		   int flags,
 		   enum ttm_caching_state cstate,
 		   dma_addr_t *dma_address);

commit 0d74f86f37306da8619eb049d88ab7ee523eec9c
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Jun 8 17:06:15 2011 +0000

    ttm: Fix spelling mistakes and remove unused #ifdef
    
    . and some comments to make it easier to understand.
    
    Ackedby: Randy Dunlap <randy.dunlap@oracle.com>
    [v2: Added some more updates from Randy Dunlap]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 8062890f725e..129de12353f1 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -32,7 +32,7 @@
 /**
  * Get count number of pages from pool to pages list.
  *
- * @pages: heado of empty linked list where pages are filled.
+ * @pages: head of empty linked list where pages are filled.
  * @flags: ttm flags for page allocation.
  * @cstate: ttm caching state for the page.
  * @count: number of pages to allocate.

commit a2c06ee2fe5b48a71e697bae00c6e7195fc016b6
Author: Dave Airlie <airlied@redhat.com>
Date:   Wed Feb 23 14:24:01 2011 +1000

    Revert "ttm: Include the 'struct dev' when using the DMA API."
    
    This reverts commit 5a893fc28f0393adb7c885a871b8c59e623fd528.
    
    This causes a use after free in the ttm free alloc pages path,
    when it tries to get the be after the be has been destroyed.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index ccb6b7a240e2..8062890f725e 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -37,14 +37,12 @@
  * @cstate: ttm caching state for the page.
  * @count: number of pages to allocate.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
- * @dev: struct device for appropiate DMA accounting.
  */
 int ttm_get_pages(struct list_head *pages,
 		  int flags,
 		  enum ttm_caching_state cstate,
 		  unsigned count,
-		  dma_addr_t *dma_address,
-		  struct device *dev);
+		  dma_addr_t *dma_address);
 /**
  * Put linked list of pages to pool.
  *
@@ -54,14 +52,12 @@ int ttm_get_pages(struct list_head *pages,
  * @flags: ttm flags for page allocation.
  * @cstate: ttm caching state.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
- * @dev: struct device for appropiate DMA accounting.
  */
 void ttm_put_pages(struct list_head *pages,
 		   unsigned page_count,
 		   int flags,
 		   enum ttm_caching_state cstate,
-		   dma_addr_t *dma_address,
-		   struct device *dev);
+		   dma_addr_t *dma_address);
 /**
  * Initialize pool allocator.
  */

commit 5a893fc28f0393adb7c885a871b8c59e623fd528
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Feb 22 13:24:32 2011 -0500

    ttm: Include the 'struct dev' when using the DMA API.
    
    This makes the accounting when using 'debug_dma_dump_mappings()'
    and CONFIG_DMA_API_DEBUG=y be assigned to the correct device
    instead of 'fallback'.
    
    No functional change - just cosmetic.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 8062890f725e..ccb6b7a240e2 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -37,12 +37,14 @@
  * @cstate: ttm caching state for the page.
  * @count: number of pages to allocate.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
+ * @dev: struct device for appropiate DMA accounting.
  */
 int ttm_get_pages(struct list_head *pages,
 		  int flags,
 		  enum ttm_caching_state cstate,
 		  unsigned count,
-		  dma_addr_t *dma_address);
+		  dma_addr_t *dma_address,
+		  struct device *dev);
 /**
  * Put linked list of pages to pool.
  *
@@ -52,12 +54,14 @@ int ttm_get_pages(struct list_head *pages,
  * @flags: ttm flags for page allocation.
  * @cstate: ttm caching state.
  * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
+ * @dev: struct device for appropiate DMA accounting.
  */
 void ttm_put_pages(struct list_head *pages,
 		   unsigned page_count,
 		   int flags,
 		   enum ttm_caching_state cstate,
-		   dma_addr_t *dma_address);
+		   dma_addr_t *dma_address,
+		   struct device *dev);
 /**
  * Initialize pool allocator.
  */

commit f9820a46dd7888b05a36e81166fb1abcc47dcc3f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Nov 29 13:52:18 2010 -0500

    ttm: Introduce a placeholder for DMA (bus) addresses.
    
    This is right now limited to only non-pool constructs.
    
    [v2: Fixed indentation issues, add review-by tag]
    
    Reviewed-by: Thomas Hellstrom <thomas@shipmail.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Ian Campbell <ian.campbell@citrix.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 116821448c38..8062890f725e 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -36,11 +36,13 @@
  * @flags: ttm flags for page allocation.
  * @cstate: ttm caching state for the page.
  * @count: number of pages to allocate.
+ * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
  */
 int ttm_get_pages(struct list_head *pages,
 		  int flags,
 		  enum ttm_caching_state cstate,
-		  unsigned count);
+		  unsigned count,
+		  dma_addr_t *dma_address);
 /**
  * Put linked list of pages to pool.
  *
@@ -49,11 +51,13 @@ int ttm_get_pages(struct list_head *pages,
  * count.
  * @flags: ttm flags for page allocation.
  * @cstate: ttm caching state.
+ * @dma_address: The DMA (bus) address of pages (if TTM_PAGE_FLAG_DMA32 set).
  */
 void ttm_put_pages(struct list_head *pages,
 		   unsigned page_count,
 		   int flags,
-		   enum ttm_caching_state cstate);
+		   enum ttm_caching_state cstate,
+		   dma_addr_t *dma_address);
 /**
  * Initialize pool allocator.
  */

commit 5870a4d97da136908ca477e3a21bc9f4c2705161
Author: Francisco Jerez <currojerez@riseup.net>
Date:   Sun Jul 4 04:03:07 2010 +0200

    drm/ttm: Allocate the page pool manager in the heap.
    
    Repeated ttm_page_alloc_init/fini fails noisily because the pool
    manager kobj isn't zeroed out between uses (we could do just that but
    statically allocated kobjects are generally considered a bad thing).
    Move it to kzalloc'ed memory.
    
    Note that this patch drops the refcounting behavior of the pool
    allocator init/fini functions: it would have led to a race condition
    in its current form, and anyway it was never exploited.
    
    This fixes a regression with reloading kms modules at runtime, since
    page allocator was introduced.
    
    Signed-off-by: Francisco Jerez <currojerez@riseup.net>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 8bb4de567b2c..116821448c38 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -56,10 +56,6 @@ void ttm_put_pages(struct list_head *pages,
 		   enum ttm_caching_state cstate);
 /**
  * Initialize pool allocator.
- *
- * Pool allocator is internaly reference counted so it can be initialized
- * multiple times but ttm_page_alloc_fini has to be called same number of
- * times.
  */
 int ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages);
 /**

commit c96af79e3463d5d3f865625baa8bb8aa4c0944a0
Author: Pauli Nieminen <suokkos@gmail.com>
Date:   Thu Apr 1 12:45:03 2010 +0000

    drm/ttm: Add sysfs interface to control pool allocator.
    
    Sysfs interface allows user to configure pool allocator functionality and
    change limits for the size of pool.
    
    Signed-off-by: Pauli Nieminen <suokkos@gmail.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 8b091c309df4..8bb4de567b2c 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -61,7 +61,7 @@ void ttm_put_pages(struct list_head *pages,
  * multiple times but ttm_page_alloc_fini has to be called same number of
  * times.
  */
-int ttm_page_alloc_init(unsigned max_pages);
+int ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages);
 /**
  * Free pool allocator.
  */

commit 0745866165598b067442c472911280527b08be3e
Author: Pauli Nieminen <suokkos@gmail.com>
Date:   Thu Apr 1 12:44:58 2010 +0000

    drm/ttm: Add debugfs output entry to pool allocator.
    
    ttm_page_alloc_debugfs can be registered to output the state
    of pools.
    
    Debugfs file will output number of pages freed from the pool,
    number of pages in pool now and the lowes number of pages in
    pool since previous shrink.
    
    Signed-off-by: Pauli Nieminen <suokkos@gmail.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
index 043d817b8164..8b091c309df4 100644
--- a/include/drm/ttm/ttm_page_alloc.h
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -67,4 +67,8 @@ int ttm_page_alloc_init(unsigned max_pages);
  */
 void ttm_page_alloc_fini(void);
 
+/**
+ * Output the state of pools to debugfs file
+ */
+extern int ttm_page_alloc_debugfs(struct seq_file *m, void *data);
 #endif

commit 1403b1a38e8b19a4cc17e2c158e278628943a436
Author: Pauli Nieminen <suokkos@gmail.com>
Date:   Thu Apr 1 12:44:57 2010 +0000

    drm/ttm: add pool wc/uc page allocator V3
    
    On AGP system we might allocate/free routinely uncached or wc memory,
    changing page from cached (wb) to uc or wc is very expensive and involves
    a lot of flushing. To improve performance this allocator use a pool
    of uc,wc pages.
    
    Pools are protected with spinlocks to allow multiple threads to allocate pages
    simultanously. Expensive operations are done outside of spinlock to maximize
    concurrency.
    
    Pools are linked lists of pages that were recently freed. mm shrink callback
    allows kernel to claim back pages when they are required for something else.
    
    Fixes:
    * set_pages_array_wb handles highmem pages so we don't have to remove them
      from pool.
    * Add count parameter to ttm_put_pages to avoid looping in free code.
    * Change looping from _safe to normal in pool fill error path.
    * Initialize sum variable and make the loop prettier in get_num_unused_pages.
    
    * Moved pages_freed reseting inside the loop in ttm_page_pool_free.
    * Add warning comment about spinlock context in ttm_page_pool_free.
    
    Based on Jerome Glisse's and Dave Airlie's pool allocator.
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Signed-off-by: Pauli Nieminen <suokkos@gmail.com>
    Reviewed-by: Jerome Glisse <jglisse@redhat.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/ttm/ttm_page_alloc.h b/include/drm/ttm/ttm_page_alloc.h
new file mode 100644
index 000000000000..043d817b8164
--- /dev/null
+++ b/include/drm/ttm/ttm_page_alloc.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright (c) Red Hat Inc.
+
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sub license,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Dave Airlie <airlied@redhat.com>
+ *          Jerome Glisse <jglisse@redhat.com>
+ */
+#ifndef TTM_PAGE_ALLOC
+#define TTM_PAGE_ALLOC
+
+#include "ttm_bo_driver.h"
+#include "ttm_memory.h"
+
+/**
+ * Get count number of pages from pool to pages list.
+ *
+ * @pages: heado of empty linked list where pages are filled.
+ * @flags: ttm flags for page allocation.
+ * @cstate: ttm caching state for the page.
+ * @count: number of pages to allocate.
+ */
+int ttm_get_pages(struct list_head *pages,
+		  int flags,
+		  enum ttm_caching_state cstate,
+		  unsigned count);
+/**
+ * Put linked list of pages to pool.
+ *
+ * @pages: list of pages to free.
+ * @page_count: number of pages in the list. Zero can be passed for unknown
+ * count.
+ * @flags: ttm flags for page allocation.
+ * @cstate: ttm caching state.
+ */
+void ttm_put_pages(struct list_head *pages,
+		   unsigned page_count,
+		   int flags,
+		   enum ttm_caching_state cstate);
+/**
+ * Initialize pool allocator.
+ *
+ * Pool allocator is internaly reference counted so it can be initialized
+ * multiple times but ttm_page_alloc_fini has to be called same number of
+ * times.
+ */
+int ttm_page_alloc_init(unsigned max_pages);
+/**
+ * Free pool allocator.
+ */
+void ttm_page_alloc_fini(void);
+
+#endif
