commit fa3d55a14a7ccd8e908d0cce6b13d466803a32a9
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sat Mar 28 14:20:22 2020 +0100

    drm/sched: fix kernel-doc in gpu_scheduler.h
    
    Fix following warning:
    gpu_scheduler.h:103: warning: Function parameter or member 'priority' not described in 'drm_sched_entity'
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Nirmoy Das <nirmoy.das@amd.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200328132025.19910-4-sam@ravnborg.org

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 26b04ff62676..a21b3b92135a 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -56,6 +56,7 @@ enum drm_sched_priority {
  *              Jobs from this entity can be scheduled on any scheduler
  *              on this list.
  * @num_sched_list: number of drm_gpu_schedulers in the sched_list.
+ * @priority: priority of the entity
  * @rq_lock: lock to modify the runqueue to which this entity belongs.
  * @job_queue: the list of jobs of this entity.
  * @fence_seq: a linearly increasing seqno incremented with each

commit ec2edcc2796c892aa0cc4740ce00a22fe57d2a1c
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Fri Mar 13 11:39:27 2020 +0100

    drm/sched: implement and export drm_sched_pick_best
    
    Remove drm_sched_entity_get_free_sched() and use the logic of picking
    the least loaded drm scheduler from a drm scheduler list to implement
    drm_sched_pick_best(). This patch also exports drm_sched_pick_best() so
    that it can be utilized by other drm drivers.
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index ae39eacee250..26b04ff62676 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -341,5 +341,8 @@ void drm_sched_fence_finished(struct drm_sched_fence *fence);
 unsigned long drm_sched_suspend_timeout(struct drm_gpu_scheduler *sched);
 void drm_sched_resume_timeout(struct drm_gpu_scheduler *sched,
 		                unsigned long remaining);
+struct drm_gpu_scheduler *
+drm_sched_pick_best(struct drm_gpu_scheduler **sched_list,
+		     unsigned int num_sched_list);
 
 #endif

commit d164bebb95516c9dd2a63cf8c8e9fe0b13d7474e
Author: changzhu <Changfeng.Zhu@amd.com>
Date:   Wed Mar 11 19:12:52 2020 +0800

    Revert "drm/scheduler: improve job distribution with multiple queues"
    
    It needs to revert this patch to avoid amdgpu_test compute hang problem
    on picasso.
    
    This reverts commit 56822db194232c089601728d68ed078dccb97f8b.
    
    Signed-off-by: changzhu <Changfeng.Zhu@amd.com>
    Reviewed-by: Feifei Xu <Feifei.Xu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index d8972836d248..ae39eacee250 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -262,7 +262,7 @@ struct drm_sched_backend_ops {
  * @job_list_lock: lock to protect the ring_mirror_list.
  * @hang_limit: once the hangs by a job crosses this limit then it is marked
  *              guilty and it will be considered for scheduling further.
- * @score: score to help loadbalancer pick a idle sched
+ * @num_jobs: the number of jobs in queue in the scheduler
  * @ready: marks if the underlying HW is ready to work
  * @free_guilty: A hit to time out handler to free the guilty job.
  *
@@ -283,8 +283,8 @@ struct drm_gpu_scheduler {
 	struct list_head		ring_mirror_list;
 	spinlock_t			job_list_lock;
 	int				hang_limit;
-	atomic_t                        score;
-	bool				ready;
+	atomic_t                        num_jobs;
+	bool			ready;
 	bool				free_guilty;
 };
 

commit b37aced31eb08757875306137a0ac00004f2f783
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Thu Feb 27 15:34:15 2020 +0100

    drm/scheduler: implement a function to modify sched list
    
    Implement drm_sched_entity_modify_sched() which modifies existing
    sched_list with a different one. This is going to be helpful when
    userspace changes priority of a ctx/entity then the driver can switch
    to the corresponding HW scheduler list for that priority.
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 589be851f8a1..d8972836d248 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -297,6 +297,10 @@ void drm_sched_fini(struct drm_gpu_scheduler *sched);
 int drm_sched_job_init(struct drm_sched_job *job,
 		       struct drm_sched_entity *entity,
 		       void *owner);
+void drm_sched_entity_modify_sched(struct drm_sched_entity *entity,
+				    struct drm_gpu_scheduler **sched_list,
+                                   unsigned int num_sched_list);
+
 void drm_sched_job_cleanup(struct drm_sched_job *job);
 void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
 void drm_sched_stop(struct drm_gpu_scheduler *sched, struct drm_sched_job *bad);

commit 2639f453f28e71dc4149fb06c71bcf6f93eb468f
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Wed Jan 22 10:37:56 2020 +0100

    drm/amdgpu: fix doc by clarifying sched_list definition
    
    expand sched_list definition for better understanding.
    Also fix a typo atleast -> at least
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 9e71be129c30..589be851f8a1 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -52,8 +52,9 @@ enum drm_sched_priority {
  * @list: used to append this struct to the list of entities in the
  *        runqueue.
  * @rq: runqueue on which this entity is currently scheduled.
- * @sched_list: a list of drm_gpu_schedulers on which jobs from this entity can
- *              be scheduled
+ * @sched_list: A list of schedulers (drm_gpu_schedulers).
+ *              Jobs from this entity can be scheduled on any scheduler
+ *              on this list.
  * @num_sched_list: number of drm_gpu_schedulers in the sched_list.
  * @rq_lock: lock to modify the runqueue to which this entity belongs.
  * @job_queue: the list of jobs of this entity.

commit 9e3e90c50dd34fe961dc662f37ee9640e04cba97
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Tue Jan 14 10:38:42 2020 +0100

    drm/scheduler: fix documentation by replacing rq_list with sched_list
    
    This also replaces old artifacts with a correct one in drm_sched_entity_init()
    declaration
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 537f7a4655a5..9e71be129c30 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -52,9 +52,9 @@ enum drm_sched_priority {
  * @list: used to append this struct to the list of entities in the
  *        runqueue.
  * @rq: runqueue on which this entity is currently scheduled.
- * @rq_list: a list of run queues on which jobs from this entity can
- *           be scheduled
- * @num_rq_list: number of run queues in the rq_list
+ * @sched_list: a list of drm_gpu_schedulers on which jobs from this entity can
+ *              be scheduled
+ * @num_sched_list: number of drm_gpu_schedulers in the sched_list.
  * @rq_lock: lock to modify the runqueue to which this entity belongs.
  * @job_queue: the list of jobs of this entity.
  * @fence_seq: a linearly increasing seqno incremented with each
@@ -81,8 +81,8 @@ enum drm_sched_priority {
 struct drm_sched_entity {
 	struct list_head		list;
 	struct drm_sched_rq		*rq;
-	unsigned int                    num_sched_list;
 	struct drm_gpu_scheduler        **sched_list;
+	unsigned int                    num_sched_list;
 	enum drm_sched_priority         priority;
 	spinlock_t			rq_lock;
 
@@ -315,7 +315,7 @@ void drm_sched_rq_remove_entity(struct drm_sched_rq *rq,
 int drm_sched_entity_init(struct drm_sched_entity *entity,
 			  enum drm_sched_priority priority,
 			  struct drm_gpu_scheduler **sched_list,
-			  unsigned int num_rq_list,
+			  unsigned int num_sched_list,
 			  atomic_t *guilty);
 long drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout);
 void drm_sched_entity_fini(struct drm_sched_entity *entity);

commit 56822db194232c089601728d68ed078dccb97f8b
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Wed Jan 15 15:06:04 2020 +0100

    drm/scheduler: improve job distribution with multiple queues
    
    This patch uses score based logic to select a new rq for better
    loadbalance between multiple rq/scheds instead of num_jobs.
    
    Below are test results after running amdgpu_test from mesa drm
    
    Before this patch:
    
    sched_name     num of many times it got scheduled
    =========      ==================================
    sdma0          314
    sdma1          32
    comp_1.0.0     56
    comp_1.0.1     0
    comp_1.1.0     0
    comp_1.1.1     0
    comp_1.2.0     0
    comp_1.2.1     0
    comp_1.3.0     0
    comp_1.3.1     0
    After this patch:
    
    sched_name     num of many times it got scheduled
    =========      ==================================
    sdma0          216
    sdma1          185
    comp_1.0.0     39
    comp_1.0.1     9
    comp_1.1.0     12
    comp_1.1.1     0
    comp_1.2.0     12
    comp_1.2.1     0
    comp_1.3.0     12
    comp_1.3.1     0
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 96a1a1b7526e..537f7a4655a5 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -261,7 +261,7 @@ struct drm_sched_backend_ops {
  * @job_list_lock: lock to protect the ring_mirror_list.
  * @hang_limit: once the hangs by a job crosses this limit then it is marked
  *              guilty and it will be considered for scheduling further.
- * @num_jobs: the number of jobs in queue in the scheduler
+ * @score: score to help loadbalancer pick a idle sched
  * @ready: marks if the underlying HW is ready to work
  * @free_guilty: A hit to time out handler to free the guilty job.
  *
@@ -282,8 +282,8 @@ struct drm_gpu_scheduler {
 	struct list_head		ring_mirror_list;
 	spinlock_t			job_list_lock;
 	int				hang_limit;
-	atomic_t                        num_jobs;
-	bool			ready;
+	atomic_t                        score;
+	bool				ready;
 	bool				free_guilty;
 };
 

commit b3ac17667f115e64c67ea6101fc814f47134b530
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Thu Dec 5 11:38:00 2019 +0100

    drm/scheduler: rework entity creation
    
    Entity currently keeps a copy of run_queue list and modify it in
    drm_sched_entity_set_priority(). Entities shouldn't modify run_queue
    list. Use drm_gpu_scheduler list instead of drm_sched_rq list
    in drm_sched_entity struct. In this way we can select a runqueue based
    on entity/ctx's priority for a  drm scheduler.
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 684692a8ed76..96a1a1b7526e 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -81,8 +81,9 @@ enum drm_sched_priority {
 struct drm_sched_entity {
 	struct list_head		list;
 	struct drm_sched_rq		*rq;
-	struct drm_sched_rq		**rq_list;
-	unsigned int                    num_rq_list;
+	unsigned int                    num_sched_list;
+	struct drm_gpu_scheduler        **sched_list;
+	enum drm_sched_priority         priority;
 	spinlock_t			rq_lock;
 
 	struct spsc_queue		job_queue;
@@ -312,7 +313,8 @@ void drm_sched_rq_remove_entity(struct drm_sched_rq *rq,
 				struct drm_sched_entity *entity);
 
 int drm_sched_entity_init(struct drm_sched_entity *entity,
-			  struct drm_sched_rq **rq_list,
+			  enum drm_sched_priority priority,
+			  struct drm_gpu_scheduler **sched_list,
 			  unsigned int num_rq_list,
 			  atomic_t *guilty);
 long drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout);

commit dc10218da86b2ab40b9471c05edd53a9097c29c3
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Fri Nov 8 16:31:10 2019 +1100

    drm/sched: struct completion requires linux/completion.h inclusion
    
    Fixes: 83a7772ba223 ("drm/sched: Use completion to wait for sched->thread idle v2.")
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 6619d2ac6fa3..684692a8ed76 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -26,6 +26,7 @@
 
 #include <drm/spsc_queue.h>
 #include <linux/dma-fence.h>
+#include <linux/completion.h>
 
 #define MAX_WAIT_SCHED_ENTITY_Q_EMPTY msecs_to_jiffies(1000)
 

commit 83a7772ba223333755d8afd90ab8b2ea3f57d4e6
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Mon Nov 4 16:30:05 2019 -0500

    drm/sched: Use completion to wait for sched->thread idle v2.
    
    Removes thread park/unpark hack from drm_sched_entity_fini and
    by this fixes reactivation of scheduler thread while the thread
    is supposed to be stopped.
    
    v2: Per sched entity completion.
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Suggested-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 57b4121c750a..6619d2ac6fa3 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -71,6 +71,7 @@ enum drm_sched_priority {
  * @last_scheduled: points to the finished fence of the last scheduled job.
  * @last_user: last group leader pushing a job into the entity.
  * @stopped: Marks the enity as removed from rq and destined for termination.
+ * @entity_idle: Signals when enityt is not in use
  *
  * Entities will emit jobs in order to their corresponding hardware
  * ring, and the scheduler will alternate between entities based on
@@ -94,6 +95,7 @@ struct drm_sched_entity {
 	struct dma_fence                *last_scheduled;
 	struct task_struct		*last_user;
 	bool 				stopped;
+	struct completion		entity_idle;
 };
 
 /**

commit a5343b8a2ca5799ee6370e3cca77369a4c598221
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Thu Apr 18 11:00:23 2019 -0400

    drm/scheduler: Add flag to hint the release of guilty job.
    
    Problem:
    Sched thread's cleanup function races against TO handler
    and removes the guilty job from mirror list and we
    have no way of differentiating if the job was removed from within the
    TO handler or from the sched thread's clean-up function.
    
    Fix:
    Add a flag to scheduler to hint the TO handler that the guilty job needs
    to be explicitly released.
    
    v2: whitespace fix
    
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/1555599624-12285-5-git-send-email-andrey.grodzovsky@amd.com

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 9ee0f2735d71..57b4121c750a 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -259,6 +259,7 @@ struct drm_sched_backend_ops {
  *              guilty and it will be considered for scheduling further.
  * @num_jobs: the number of jobs in queue in the scheduler
  * @ready: marks if the underlying HW is ready to work
+ * @free_guilty: A hit to time out handler to free the guilty job.
  *
  * One scheduler is implemented for each hardware ring.
  */
@@ -279,6 +280,7 @@ struct drm_gpu_scheduler {
 	int				hang_limit;
 	atomic_t                        num_jobs;
 	bool			ready;
+	bool				free_guilty;
 };
 
 int drm_sched_init(struct drm_gpu_scheduler *sched,

commit 5918045c4ed492fb5813f980dcf89a90fefd0a4e
Author: Christian König <christian.koenig@amd.com>
Date:   Thu Apr 18 11:00:21 2019 -0400

    drm/scheduler: rework job destruction
    
    We now destroy finished jobs from the worker thread to make sure that
    we never destroy a job currently in timeout processing.
    By this we avoid holding lock around ring mirror list in drm_sched_stop
    which should solve a deadlock reported by a user.
    
    v2: Remove unused variable.
    v4: Move guilty job free into sched code.
    v5:
    Move sched->hw_rq_count to drm_sched_start to account for counter
    decrement in drm_sched_stop even when we don't call resubmit jobs
    if guily job did signal.
    v6: remove unused variable
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=109692
    
    Acked-by: Chunming Zhou <david1.zhou@amd.com>
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/1555599624-12285-3-git-send-email-andrey.grodzovsky@amd.com

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 0daca4d8dad9..9ee0f2735d71 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -167,9 +167,6 @@ struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
  * @sched: the scheduler instance on which this job is scheduled.
  * @s_fence: contains the fences for the scheduling of job.
  * @finish_cb: the callback for the finished fence.
- * @finish_work: schedules the function @drm_sched_job_finish once the job has
- *               finished to remove the job from the
- *               @drm_gpu_scheduler.ring_mirror_list.
  * @node: used to append this struct to the @drm_gpu_scheduler.ring_mirror_list.
  * @id: a unique id assigned to each job scheduled on the scheduler.
  * @karma: increment on every hang caused by this job. If this exceeds the hang
@@ -188,7 +185,6 @@ struct drm_sched_job {
 	struct drm_gpu_scheduler	*sched;
 	struct drm_sched_fence		*s_fence;
 	struct dma_fence_cb		finish_cb;
-	struct work_struct		finish_work;
 	struct list_head		node;
 	uint64_t			id;
 	atomic_t			karma;
@@ -296,7 +292,7 @@ int drm_sched_job_init(struct drm_sched_job *job,
 		       void *owner);
 void drm_sched_job_cleanup(struct drm_sched_job *job);
 void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
-void drm_sched_stop(struct drm_gpu_scheduler *sched);
+void drm_sched_stop(struct drm_gpu_scheduler *sched, struct drm_sched_job *bad);
 void drm_sched_start(struct drm_gpu_scheduler *sched, bool full_recovery);
 void drm_sched_resubmit_jobs(struct drm_gpu_scheduler *sched);
 void drm_sched_increase_karma(struct drm_sched_job *bad);

commit 3741540e04137256df82105bcd720a5e27423c34
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Wed Dec 5 14:21:28 2018 -0500

    drm/sched: Rework HW fence processing.
    
    Expedite job deletion from ring mirror list to the HW fence signal
    callback instead from finish_work, together with waiting for all
    such fences to signal in drm_sched_stop we garantee that
    already signaled job will not be processed twice.
    Remove the sched finish fence callback and just submit finish_work
    directly from the HW fence callback.
    
    v2: Fix comments.
    v3: Attach  hw fence cb to sched_job
    v5: Rebase
    
    Suggested-by: Christian Koenig <Christian.Koenig@amd.com>
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index c567bba91ba0..0daca4d8dad9 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -137,10 +137,6 @@ struct drm_sched_fence {
          */
 	struct dma_fence		finished;
 
-        /**
-         * @cb: the callback for the parent fence below.
-         */
-	struct dma_fence_cb		cb;
         /**
          * @parent: the fence returned by &drm_sched_backend_ops.run_job
          * when scheduling the job on hardware. We signal the
@@ -181,6 +177,7 @@ struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
  *         be scheduled further.
  * @s_priority: the priority of the job.
  * @entity: the entity to which this job belongs.
+ * @cb: the callback for the parent fence in s_fence.
  *
  * A job is created by the driver using drm_sched_job_init(), and
  * should call drm_sched_entity_push_job() once it wants the scheduler
@@ -197,6 +194,7 @@ struct drm_sched_job {
 	atomic_t			karma;
 	enum drm_sched_priority		s_priority;
 	struct drm_sched_entity  *entity;
+	struct dma_fence_cb		cb;
 };
 
 static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,

commit 222b5f044159877504dbac9bc1910f89a74136e2
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Tue Dec 4 16:56:14 2018 -0500

    drm/sched: Refactor ring mirror list handling.
    
    Decauple sched threads stop and start and ring mirror
    list handling from the policy of what to do about the
    guilty jobs.
    When stoppping the sched thread and detaching sched fences
    from non signaled HW fenes wait for all signaled HW fences
    to complete before rerunning the jobs.
    
    v2: Fix resubmission of guilty job into HW after refactoring.
    
    v4:
    Full restart for all the jobs, not only from guilty ring.
    Extract karma increase into standalone function.
    
    v5:
    Rework waiting for signaled jobs without relying on the job
    struct itself as those might already be freed for non 'guilty'
    job's schedulers.
    Expose karma increase to drivers.
    
    v6:
    Use list_for_each_entry_safe_continue and drm_sched_process_job
    in case fence already signaled.
    Call drm_sched_increase_karma only once for amdgpu and add documentation.
    
    v7:
    Wait only for the latest job's fence.
    
    Suggested-by: Christian Koenig <Christian.Koenig@amd.com>
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 47e19796c450..c567bba91ba0 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -298,9 +298,10 @@ int drm_sched_job_init(struct drm_sched_job *job,
 		       void *owner);
 void drm_sched_job_cleanup(struct drm_sched_job *job);
 void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
-void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
-			    struct drm_sched_job *job);
-void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
+void drm_sched_stop(struct drm_gpu_scheduler *sched);
+void drm_sched_start(struct drm_gpu_scheduler *sched, bool full_recovery);
+void drm_sched_resubmit_jobs(struct drm_gpu_scheduler *sched);
+void drm_sched_increase_karma(struct drm_sched_job *bad);
 bool drm_sched_dependency_optimized(struct dma_fence* fence,
 				    struct drm_sched_entity *entity);
 void drm_sched_fault(struct drm_gpu_scheduler *sched);

commit 1db8c142b6c557a951e8f9866b98953fe91cbdd6
Author: Sharat Masetty <smasetty@codeaurora.org>
Date:   Thu Nov 29 15:35:20 2018 +0530

    drm/scheduler: Add drm_sched_suspend/resume_timeout()
    
    This patch adds two new functions to help client drivers suspend and
    resume the scheduler job timeout. This can be useful in cases where the
    hardware has preemption support enabled. Using this, it is possible to have
    the timeout active only for the ring which is active on the ringbuffer.
    This patch also makes the job_list_lock IRQ safe.
    
    Suggested-by: Christian Koenig <Christian.Koenig@amd.com>
    Signed-off-by: Sharat Masetty <smasetty@codeaurora.org>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 926379d53484..47e19796c450 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -331,4 +331,8 @@ struct drm_sched_fence *drm_sched_fence_create(
 void drm_sched_fence_scheduled(struct drm_sched_fence *fence);
 void drm_sched_fence_finished(struct drm_sched_fence *fence);
 
+unsigned long drm_sched_suspend_timeout(struct drm_gpu_scheduler *sched);
+void drm_sched_resume_timeout(struct drm_gpu_scheduler *sched,
+		                unsigned long remaining);
+
 #endif

commit 26efecf9558895a89c2920d258601b4afba10fd0
Author: Sharat Masetty <smasetty@codeaurora.org>
Date:   Mon Oct 29 15:02:28 2018 +0530

    drm/scheduler: Add drm_sched_job_cleanup
    
    This patch adds a new API to clean up the scheduler job resources. This
    is primarliy needed in cases the job was created but was not queued to
    the scheduler queue. Additionally with this change, the layer which
    creates the scheduler job also gets to free up the job's resources and
    this entails moving the dma_fence_put(finished_fence) to the drivers
    ops free handler routines.
    
    Signed-off-by: Sharat Masetty <smasetty@codeaurora.org>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Acked-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 4ae192a21c3f..926379d53484 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -296,6 +296,7 @@ void drm_sched_fini(struct drm_gpu_scheduler *sched);
 int drm_sched_job_init(struct drm_sched_job *job,
 		       struct drm_sched_entity *entity,
 		       void *owner);
+void drm_sched_job_cleanup(struct drm_sched_job *job);
 void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
 void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
 			    struct drm_sched_job *job);

commit faf6e1a87e07423a729e04fb2e8188742e89ea4c
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Thu Oct 18 12:32:46 2018 -0400

    drm/sched: Add boolean to mark if sched is ready to work v5
    
    Problem:
    A particular scheduler may become unsuable (underlying HW) after
    some event (e.g. GPU reset). If it's later chosen by
    the get free sched. policy a command will fail to be
    submitted.
    
    Fix:
    Add a driver specific callback to report the sched status so
    rq with bad sched can be avoided in favor of working one or
    none in which case job init will fail.
    
    v2: Switch from driver callback to flag in scheduler.
    
    v3: rebase
    
    v4: Remove ready paramter from drm_sched_init, set
    uncoditionally to true once init done.
    
    v5: fix missed change in v3d in v4 (Alex)
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 0684dcd99c0f..4ae192a21c3f 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -264,6 +264,7 @@ struct drm_sched_backend_ops {
  * @hang_limit: once the hangs by a job crosses this limit then it is marked
  *              guilty and it will be considered for scheduling further.
  * @num_jobs: the number of jobs in queue in the scheduler
+ * @ready: marks if the underlying HW is ready to work
  *
  * One scheduler is implemented for each hardware ring.
  */
@@ -283,12 +284,14 @@ struct drm_gpu_scheduler {
 	spinlock_t			job_list_lock;
 	int				hang_limit;
 	atomic_t                        num_jobs;
+	bool			ready;
 };
 
 int drm_sched_init(struct drm_gpu_scheduler *sched,
 		   const struct drm_sched_backend_ops *ops,
 		   uint32_t hw_submission, unsigned hang_limit, long timeout,
 		   const char *name);
+
 void drm_sched_fini(struct drm_gpu_scheduler *sched);
 int drm_sched_job_init(struct drm_sched_job *job,
 		       struct drm_sched_entity *entity,

commit 8fe159b0143d817222c8799181deb799472b9339
Author: Christian König <christian.koenig@amd.com>
Date:   Fri Oct 12 16:47:13 2018 +0200

    drm/sched: add drm_sched_fault
    
    Add a helper to immediately start timeout handling in case of a hardware
    fault.
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index d87b268f1781..0684dcd99c0f 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -299,6 +299,7 @@ void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
 void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
 bool drm_sched_dependency_optimized(struct dma_fence* fence,
 				    struct drm_sched_entity *entity);
+void drm_sched_fault(struct drm_gpu_scheduler *sched);
 void drm_sched_job_kickout(struct drm_sched_job *s_job);
 
 void drm_sched_rq_add_entity(struct drm_sched_rq *rq,

commit 6a96243056217662843694a4cbc83158d0e84403
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Wed Sep 26 02:09:02 2018 +0900

    drm/scheduler: remove timeout work_struct from drm_sched_job (v3)
    
    having a delayed work item per job is redundant as we only need one
    per scheduler to track the time out the currently executing job.
    
    v2: the first element of the ring mirror list is the currently
    executing job so we don't need a additional variable for it
    
    v3: squash in fixes for v3d and etnaviv
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Suggested-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index daec50f887b3..d87b268f1781 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -175,8 +175,6 @@ struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
  *               finished to remove the job from the
  *               @drm_gpu_scheduler.ring_mirror_list.
  * @node: used to append this struct to the @drm_gpu_scheduler.ring_mirror_list.
- * @work_tdr: schedules a delayed call to @drm_sched_job_timedout after the timeout
- *            interval is over.
  * @id: a unique id assigned to each job scheduled on the scheduler.
  * @karma: increment on every hang caused by this job. If this exceeds the hang
  *         limit of the scheduler then the job is marked guilty and will not
@@ -195,7 +193,6 @@ struct drm_sched_job {
 	struct dma_fence_cb		finish_cb;
 	struct work_struct		finish_work;
 	struct list_head		node;
-	struct delayed_work		work_tdr;
 	uint64_t			id;
 	atomic_t			karma;
 	enum drm_sched_priority		s_priority;
@@ -259,6 +256,8 @@ struct drm_sched_backend_ops {
  *                 finished.
  * @hw_rq_count: the number of jobs currently in the hardware queue.
  * @job_id_count: used to assign unique id to the each job.
+ * @work_tdr: schedules a delayed call to @drm_sched_job_timedout after the
+ *            timeout interval is over.
  * @thread: the kthread on which the scheduler which run.
  * @ring_mirror_list: the list of jobs which are currently in the job queue.
  * @job_list_lock: lock to protect the ring_mirror_list.
@@ -278,6 +277,7 @@ struct drm_gpu_scheduler {
 	wait_queue_head_t		job_scheduled;
 	atomic_t			hw_rq_count;
 	atomic64_t			job_id_count;
+	struct delayed_work		work_tdr;
 	struct task_struct		*thread;
 	struct list_head		ring_mirror_list;
 	spinlock_t			job_list_lock;

commit 62347a33001c27b22465361aa4adcaa432497bdf
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Fri Aug 17 10:32:50 2018 -0400

    drm/scheduler: Add stopped flag to drm_sched_entity
    
    The flag will prevent another thread from same process to
    reinsert the entity queue into scheduler's rq after it was already
    removewd from there by another thread during drm_sched_entity_flush.
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Chunming Zhou <david1.zhou@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 919ae572f775..daec50f887b3 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -70,6 +70,7 @@ enum drm_sched_priority {
  * @fini_status: contains the exit status in case the process was signalled.
  * @last_scheduled: points to the finished fence of the last scheduled job.
  * @last_user: last group leader pushing a job into the entity.
+ * @stopped: Marks the enity as removed from rq and destined for termination.
  *
  * Entities will emit jobs in order to their corresponding hardware
  * ring, and the scheduler will alternate between entities based on
@@ -92,6 +93,7 @@ struct drm_sched_entity {
 	atomic_t			*guilty;
 	struct dma_fence                *last_scheduled;
 	struct task_struct		*last_user;
+	bool 				stopped;
 };
 
 /**

commit 620e762f9a984fc3f77cd6f757581a21605ce125
Author: Christian König <christian.koenig@amd.com>
Date:   Mon Aug 6 14:25:32 2018 +0200

    drm/scheduler: move entity handling into separate file
    
    This is complex enough on it's own. Move it into a separate C file.
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 22c0f88f7d8f..919ae572f775 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -288,6 +288,21 @@ int drm_sched_init(struct drm_gpu_scheduler *sched,
 		   uint32_t hw_submission, unsigned hang_limit, long timeout,
 		   const char *name);
 void drm_sched_fini(struct drm_gpu_scheduler *sched);
+int drm_sched_job_init(struct drm_sched_job *job,
+		       struct drm_sched_entity *entity,
+		       void *owner);
+void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
+void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
+			    struct drm_sched_job *job);
+void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
+bool drm_sched_dependency_optimized(struct dma_fence* fence,
+				    struct drm_sched_entity *entity);
+void drm_sched_job_kickout(struct drm_sched_job *s_job);
+
+void drm_sched_rq_add_entity(struct drm_sched_rq *rq,
+			     struct drm_sched_entity *entity);
+void drm_sched_rq_remove_entity(struct drm_sched_rq *rq,
+				struct drm_sched_entity *entity);
 
 int drm_sched_entity_init(struct drm_sched_entity *entity,
 			  struct drm_sched_rq **rq_list,
@@ -296,22 +311,17 @@ int drm_sched_entity_init(struct drm_sched_entity *entity,
 long drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout);
 void drm_sched_entity_fini(struct drm_sched_entity *entity);
 void drm_sched_entity_destroy(struct drm_sched_entity *entity);
+void drm_sched_entity_select_rq(struct drm_sched_entity *entity);
+struct drm_sched_job *drm_sched_entity_pop_job(struct drm_sched_entity *entity);
 void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
 			       struct drm_sched_entity *entity);
 void drm_sched_entity_set_priority(struct drm_sched_entity *entity,
 				   enum drm_sched_priority priority);
+bool drm_sched_entity_is_ready(struct drm_sched_entity *entity);
+
 struct drm_sched_fence *drm_sched_fence_create(
 	struct drm_sched_entity *s_entity, void *owner);
 void drm_sched_fence_scheduled(struct drm_sched_fence *fence);
 void drm_sched_fence_finished(struct drm_sched_fence *fence);
-int drm_sched_job_init(struct drm_sched_job *job,
-		       struct drm_sched_entity *entity,
-		       void *owner);
-void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
-			    struct drm_sched_job *job);
-void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
-bool drm_sched_dependency_optimized(struct dma_fence* fence,
-				    struct drm_sched_entity *entity);
-void drm_sched_job_kickout(struct drm_sched_job *s_job);
 
 #endif

commit 7febe4bfd5d477eba17f70d4879cb81e9787118e
Author: Christian König <christian.koenig@amd.com>
Date:   Wed Aug 1 16:22:39 2018 +0200

    drm/scheduler: fix setting the priorty for entities (v2)
    
    Since we now deal with multiple rq we need to update all of them, not
    just the current one.
    
    v2: Trivial: Removed unused variable (Alex)
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Acked-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 0c4cfe689d4c..22c0f88f7d8f 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -298,9 +298,8 @@ void drm_sched_entity_fini(struct drm_sched_entity *entity);
 void drm_sched_entity_destroy(struct drm_sched_entity *entity);
 void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
 			       struct drm_sched_entity *entity);
-void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
-			     struct drm_sched_rq *rq);
-
+void drm_sched_entity_set_priority(struct drm_sched_entity *entity,
+				   enum drm_sched_priority priority);
 struct drm_sched_fence *drm_sched_fence_create(
 	struct drm_sched_entity *s_entity, void *owner);
 void drm_sched_fence_scheduled(struct drm_sched_fence *fence);

commit 249a07c05a8da9637c2eb3205f1fc739c216f707
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Wed Aug 1 13:50:00 2018 +0530

    drm/scheduler: add counter for total jobs in scheduler
    
    To keep track of the scheduler load.
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 2419887e25eb..0c4cfe689d4c 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -262,6 +262,7 @@ struct drm_sched_backend_ops {
  * @job_list_lock: lock to protect the ring_mirror_list.
  * @hang_limit: once the hangs by a job crosses this limit then it is marked
  *              guilty and it will be considered for scheduling further.
+ * @num_jobs: the number of jobs in queue in the scheduler
  *
  * One scheduler is implemented for each hardware ring.
  */
@@ -279,6 +280,7 @@ struct drm_gpu_scheduler {
 	struct list_head		ring_mirror_list;
 	spinlock_t			job_list_lock;
 	int				hang_limit;
+	atomic_t                        num_jobs;
 };
 
 int drm_sched_init(struct drm_gpu_scheduler *sched,

commit ac0a6cf1c6ef91e4af2a9d56eeaee8fca61d6ad7
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Wed Aug 1 13:49:59 2018 +0530

    drm/scheduler: add a list of run queues to the entity
    
    These are the potential run queues on which the jobs from this
    entity can be scheduled. We will use this to do load balancing.
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 21c648b0b2a1..2419887e25eb 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -50,7 +50,10 @@ enum drm_sched_priority {
  *
  * @list: used to append this struct to the list of entities in the
  *        runqueue.
- * @rq: runqueue to which this entity belongs.
+ * @rq: runqueue on which this entity is currently scheduled.
+ * @rq_list: a list of run queues on which jobs from this entity can
+ *           be scheduled
+ * @num_rq_list: number of run queues in the rq_list
  * @rq_lock: lock to modify the runqueue to which this entity belongs.
  * @job_queue: the list of jobs of this entity.
  * @fence_seq: a linearly increasing seqno incremented with each
@@ -75,6 +78,8 @@ enum drm_sched_priority {
 struct drm_sched_entity {
 	struct list_head		list;
 	struct drm_sched_rq		*rq;
+	struct drm_sched_rq		**rq_list;
+	unsigned int                    num_rq_list;
 	spinlock_t			rq_lock;
 
 	struct spsc_queue		job_queue;

commit 43bce41cf48eb51eab5ad9e0d40ed382a7bb61d7
Author: Christian König <christian.koenig@amd.com>
Date:   Thu Jul 26 13:43:49 2018 +0200

    drm/scheduler: only kill entity if last user is killed v2
    
    Note which task is using the entity and only kill it if the last user of
    the entity is killed. This should prevent problems when entities are leaked to
    child processes.
    
    v2: add missing kernel doc
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Acked-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 091b9afcd184..21c648b0b2a1 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -66,6 +66,7 @@ enum drm_sched_priority {
  * @guilty: points to ctx's guilty.
  * @fini_status: contains the exit status in case the process was signalled.
  * @last_scheduled: points to the finished fence of the last scheduled job.
+ * @last_user: last group leader pushing a job into the entity.
  *
  * Entities will emit jobs in order to their corresponding hardware
  * ring, and the scheduler will alternate between entities based on
@@ -85,6 +86,7 @@ struct drm_sched_entity {
 	struct dma_fence_cb		cb;
 	atomic_t			*guilty;
 	struct dma_fence                *last_scheduled;
+	struct task_struct		*last_user;
 };
 
 /**

commit 068c330419ffb3422a43cb7d34351f1ef033950f
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Fri Jul 20 17:51:06 2018 +0530

    drm/scheduler: remove sched field from the entity
    
    The scheduler of the entity is decided by the run queue on which
    it is queued. This patch avoids us the effort required to maintain
    a sync between rq and sched field when we start shifting entites
    among different rqs.
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Eric Anholt <eric@anholt.net>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 728346abcc81..091b9afcd184 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -52,7 +52,6 @@ enum drm_sched_priority {
  *        runqueue.
  * @rq: runqueue to which this entity belongs.
  * @rq_lock: lock to modify the runqueue to which this entity belongs.
- * @sched: the scheduler instance to which this entity is enqueued.
  * @job_queue: the list of jobs of this entity.
  * @fence_seq: a linearly increasing seqno incremented with each
  *             new &drm_sched_fence which is part of the entity.
@@ -76,7 +75,6 @@ struct drm_sched_entity {
 	struct list_head		list;
 	struct drm_sched_rq		*rq;
 	spinlock_t			rq_lock;
-	struct drm_gpu_scheduler	*sched;
 
 	struct spsc_queue		job_queue;
 

commit cdc50176597cb44ce25eb7331c450058775b8d2a
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Fri Jul 20 17:51:05 2018 +0530

    drm/scheduler: modify API to avoid redundancy
    
    entity has a scheduler field and we don't need the sched argument
    in any of the functions where entity is provided.
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Eric Anholt <eric@anholt.net>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 2205e89722f6..728346abcc81 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -286,12 +286,9 @@ int drm_sched_entity_init(struct drm_sched_entity *entity,
 			  struct drm_sched_rq **rq_list,
 			  unsigned int num_rq_list,
 			  atomic_t *guilty);
-long drm_sched_entity_flush(struct drm_gpu_scheduler *sched,
-			   struct drm_sched_entity *entity, long timeout);
-void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
-			   struct drm_sched_entity *entity);
-void drm_sched_entity_destroy(struct drm_gpu_scheduler *sched,
-			   struct drm_sched_entity *entity);
+long drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout);
+void drm_sched_entity_fini(struct drm_sched_entity *entity);
+void drm_sched_entity_destroy(struct drm_sched_entity *entity);
 void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
 			       struct drm_sched_entity *entity);
 void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
@@ -302,7 +299,6 @@ struct drm_sched_fence *drm_sched_fence_create(
 void drm_sched_fence_scheduled(struct drm_sched_fence *fence);
 void drm_sched_fence_finished(struct drm_sched_fence *fence);
 int drm_sched_job_init(struct drm_sched_job *job,
-		       struct drm_gpu_scheduler *sched,
 		       struct drm_sched_entity *entity,
 		       void *owner);
 void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,

commit aa16b6c6b4d979234f830a48add47d02c12bb569
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Fri Jul 13 15:21:14 2018 +0530

    drm/scheduler: modify args of drm_sched_entity_init
    
    replace run queue by a list of run queues and remove the
    sched arg as that is part of run queue itself
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Acked-by: Eric Anholt <eric@anholt.net>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 43e93d6077cf..2205e89722f6 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -282,9 +282,9 @@ int drm_sched_init(struct drm_gpu_scheduler *sched,
 		   const char *name);
 void drm_sched_fini(struct drm_gpu_scheduler *sched);
 
-int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
-			  struct drm_sched_entity *entity,
-			  struct drm_sched_rq *rq,
+int drm_sched_entity_init(struct drm_sched_entity *entity,
+			  struct drm_sched_rq **rq_list,
+			  unsigned int num_rq_list,
 			  atomic_t *guilty);
 long drm_sched_entity_flush(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity, long timeout);

commit 8dc9fbbf274b7b2a647e06141aee70ffabf6dbc0
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Fri Jul 13 15:21:13 2018 +0530

    drm/scheduler: add a pointer to scheduler in the rq
    
    This patch is in preparation for a better load balancing in
    scheduler. It allows us to associate entities with the
    run queues instead of binding them to a scheduler.
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Acked-by: Eric Anholt <eric@anholt.net>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 4214ceb71c05..43e93d6077cf 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -93,6 +93,7 @@ struct drm_sched_entity {
  * struct drm_sched_rq - queue of entities to be scheduled.
  *
  * @lock: to modify the entities list.
+ * @sched: the scheduler to which this rq belongs to.
  * @entities: list of the entities to be scheduled.
  * @current_entity: the entity which is to be scheduled.
  *
@@ -102,6 +103,7 @@ struct drm_sched_entity {
  */
 struct drm_sched_rq {
 	spinlock_t			lock;
+	struct drm_gpu_scheduler	*sched;
 	struct list_head		entities;
 	struct drm_sched_entity		*current_entity;
 };

commit 180fc134d712a93a2bbc3d11ed657b5208e6f90f
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Tue Jun 5 12:43:23 2018 -0400

    drm/scheduler: Rename cleanup functions v2.
    
    Everything in the flush code path (i.e. waiting for SW queue
    to become empty) names with *_flush()
    and everything in the release code path names *_fini()
    
    This patch also effect the amdgpu and etnaviv drivers which
    use those functions.
    
    v2:
    Also pplay the change to vd3.
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Suggested-by: Christian König <christian.koenig@amd.com>
    Acked-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 7c2dfd6cc1af..4214ceb71c05 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -284,12 +284,12 @@ int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
 			  struct drm_sched_entity *entity,
 			  struct drm_sched_rq *rq,
 			  atomic_t *guilty);
-long drm_sched_entity_do_release(struct drm_gpu_scheduler *sched,
+long drm_sched_entity_flush(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity, long timeout);
-void drm_sched_entity_cleanup(struct drm_gpu_scheduler *sched,
-			   struct drm_sched_entity *entity);
 void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity);
+void drm_sched_entity_destroy(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity);
 void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
 			       struct drm_sched_entity *entity);
 void drm_sched_entity_set_rq(struct drm_sched_entity *entity,

commit 741f01e636b72ff3f81204fd595ac1078907671b
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Wed May 30 15:11:01 2018 -0400

    drm/scheduler: Avoid using wait_event_killable for dying process (V4)
    
    Dying process might be blocked from receiving any more signals
    so avoid using it.
    
    Also retire enity->fini_status and just check the SW queue,
    if it's not empty do the fallback cleanup.
    
    Also handle entity->last_scheduled == NULL use case which
    happens when HW ring is already hangged whem a  new entity
    tried to enqeue jobs.
    
    v2:
    Return the remaining timeout and use that as parameter for the next call.
    This way when we need to cleanup multiple queues we don't wait for the
    entire TO period for each queue but rather in total.
    Styling comments.
    Rebase.
    
    v3:
    Update types from unsigned to long.
    Work with jiffies instead of ms.
    Return 0 when TO expires.
    Rebase.
    
    v4:
    Remove unnecessary timeout calculation.
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 496442f12bff..7c2dfd6cc1af 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -27,6 +27,8 @@
 #include <drm/spsc_queue.h>
 #include <linux/dma-fence.h>
 
+#define MAX_WAIT_SCHED_ENTITY_Q_EMPTY msecs_to_jiffies(1000)
+
 struct drm_gpu_scheduler;
 struct drm_sched_rq;
 
@@ -84,7 +86,6 @@ struct drm_sched_entity {
 	struct dma_fence		*dependency;
 	struct dma_fence_cb		cb;
 	atomic_t			*guilty;
-	int                             fini_status;
 	struct dma_fence                *last_scheduled;
 };
 
@@ -283,8 +284,8 @@ int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
 			  struct drm_sched_entity *entity,
 			  struct drm_sched_rq *rq,
 			  atomic_t *guilty);
-void drm_sched_entity_do_release(struct drm_gpu_scheduler *sched,
-			   struct drm_sched_entity *entity);
+long drm_sched_entity_do_release(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity, long timeout);
 void drm_sched_entity_cleanup(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity);
 void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,

commit 2d33948e4e00b501b91367fed21243a948426591
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Tue May 29 11:23:07 2018 +0530

    drm/scheduler: add documentation
    
    convert existing raw comments into kernel-doc format as well
    as add new documentation
    
    v2: reword the overview
    
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Acked-by: Daniel Vetter <daniel@ffwll.ch>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index dec655894d08..496442f12bff 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -43,13 +43,33 @@ enum drm_sched_priority {
 };
 
 /**
- * drm_sched_entity - A wrapper around a job queue (typically attached
- * to the DRM file_priv).
+ * struct drm_sched_entity - A wrapper around a job queue (typically
+ * attached to the DRM file_priv).
+ *
+ * @list: used to append this struct to the list of entities in the
+ *        runqueue.
+ * @rq: runqueue to which this entity belongs.
+ * @rq_lock: lock to modify the runqueue to which this entity belongs.
+ * @sched: the scheduler instance to which this entity is enqueued.
+ * @job_queue: the list of jobs of this entity.
+ * @fence_seq: a linearly increasing seqno incremented with each
+ *             new &drm_sched_fence which is part of the entity.
+ * @fence_context: a unique context for all the fences which belong
+ *                 to this entity.
+ *                 The &drm_sched_fence.scheduled uses the
+ *                 fence_context but &drm_sched_fence.finished uses
+ *                 fence_context + 1.
+ * @dependency: the dependency fence of the job which is on the top
+ *              of the job queue.
+ * @cb: callback for the dependency fence above.
+ * @guilty: points to ctx's guilty.
+ * @fini_status: contains the exit status in case the process was signalled.
+ * @last_scheduled: points to the finished fence of the last scheduled job.
  *
  * Entities will emit jobs in order to their corresponding hardware
  * ring, and the scheduler will alternate between entities based on
  * scheduling policy.
-*/
+ */
 struct drm_sched_entity {
 	struct list_head		list;
 	struct drm_sched_rq		*rq;
@@ -63,47 +83,96 @@ struct drm_sched_entity {
 
 	struct dma_fence		*dependency;
 	struct dma_fence_cb		cb;
-	atomic_t			*guilty; /* points to ctx's guilty */
-	int            fini_status;
-	struct dma_fence    *last_scheduled;
+	atomic_t			*guilty;
+	int                             fini_status;
+	struct dma_fence                *last_scheduled;
 };
 
 /**
+ * struct drm_sched_rq - queue of entities to be scheduled.
+ *
+ * @lock: to modify the entities list.
+ * @entities: list of the entities to be scheduled.
+ * @current_entity: the entity which is to be scheduled.
+ *
  * Run queue is a set of entities scheduling command submissions for
  * one specific ring. It implements the scheduling policy that selects
  * the next entity to emit commands from.
-*/
+ */
 struct drm_sched_rq {
 	spinlock_t			lock;
 	struct list_head		entities;
 	struct drm_sched_entity		*current_entity;
 };
 
+/**
+ * struct drm_sched_fence - fences corresponding to the scheduling of a job.
+ */
 struct drm_sched_fence {
+        /**
+         * @scheduled: this fence is what will be signaled by the scheduler
+         * when the job is scheduled.
+         */
 	struct dma_fence		scheduled;
 
-	/* This fence is what will be signaled by the scheduler when
-	 * the job is completed.
-	 *
-	 * When setting up an out fence for the job, you should use
-	 * this, since it's available immediately upon
-	 * drm_sched_job_init(), and the fence returned by the driver
-	 * from run_job() won't be created until the dependencies have
-	 * resolved.
-	 */
+        /**
+         * @finished: this fence is what will be signaled by the scheduler
+         * when the job is completed.
+         *
+         * When setting up an out fence for the job, you should use
+         * this, since it's available immediately upon
+         * drm_sched_job_init(), and the fence returned by the driver
+         * from run_job() won't be created until the dependencies have
+         * resolved.
+         */
 	struct dma_fence		finished;
 
+        /**
+         * @cb: the callback for the parent fence below.
+         */
 	struct dma_fence_cb		cb;
+        /**
+         * @parent: the fence returned by &drm_sched_backend_ops.run_job
+         * when scheduling the job on hardware. We signal the
+         * &drm_sched_fence.finished fence once parent is signalled.
+         */
 	struct dma_fence		*parent;
+        /**
+         * @sched: the scheduler instance to which the job having this struct
+         * belongs to.
+         */
 	struct drm_gpu_scheduler	*sched;
+        /**
+         * @lock: the lock used by the scheduled and the finished fences.
+         */
 	spinlock_t			lock;
+        /**
+         * @owner: job owner for debugging
+         */
 	void				*owner;
 };
 
 struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
 
 /**
- * drm_sched_job - A job to be run by an entity.
+ * struct drm_sched_job - A job to be run by an entity.
+ *
+ * @queue_node: used to append this struct to the queue of jobs in an entity.
+ * @sched: the scheduler instance on which this job is scheduled.
+ * @s_fence: contains the fences for the scheduling of job.
+ * @finish_cb: the callback for the finished fence.
+ * @finish_work: schedules the function @drm_sched_job_finish once the job has
+ *               finished to remove the job from the
+ *               @drm_gpu_scheduler.ring_mirror_list.
+ * @node: used to append this struct to the @drm_gpu_scheduler.ring_mirror_list.
+ * @work_tdr: schedules a delayed call to @drm_sched_job_timedout after the timeout
+ *            interval is over.
+ * @id: a unique id assigned to each job scheduled on the scheduler.
+ * @karma: increment on every hang caused by this job. If this exceeds the hang
+ *         limit of the scheduler then the job is marked guilty and will not
+ *         be scheduled further.
+ * @s_priority: the priority of the job.
+ * @entity: the entity to which this job belongs.
  *
  * A job is created by the driver using drm_sched_job_init(), and
  * should call drm_sched_entity_push_job() once it wants the scheduler
@@ -130,38 +199,64 @@ static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,
 }
 
 /**
+ * struct drm_sched_backend_ops
+ *
  * Define the backend operations called by the scheduler,
- * these functions should be implemented in driver side
-*/
+ * these functions should be implemented in driver side.
+ */
 struct drm_sched_backend_ops {
-	/* Called when the scheduler is considering scheduling this
-	 * job next, to get another struct dma_fence for this job to
+	/**
+         * @dependency: Called when the scheduler is considering scheduling
+         * this job next, to get another struct dma_fence for this job to
 	 * block on.  Once it returns NULL, run_job() may be called.
 	 */
 	struct dma_fence *(*dependency)(struct drm_sched_job *sched_job,
 					struct drm_sched_entity *s_entity);
 
-	/* Called to execute the job once all of the dependencies have
-	 * been resolved.  This may be called multiple times, if
+	/**
+         * @run_job: Called to execute the job once all of the dependencies
+         * have been resolved.  This may be called multiple times, if
 	 * timedout_job() has happened and drm_sched_job_recovery()
 	 * decides to try it again.
 	 */
 	struct dma_fence *(*run_job)(struct drm_sched_job *sched_job);
 
-	/* Called when a job has taken too long to execute, to trigger
-	 * GPU recovery.
+	/**
+         * @timedout_job: Called when a job has taken too long to execute,
+         * to trigger GPU recovery.
 	 */
 	void (*timedout_job)(struct drm_sched_job *sched_job);
 
-	/* Called once the job's finished fence has been signaled and
-	 * it's time to clean it up.
+	/**
+         * @free_job: Called once the job's finished fence has been signaled
+         * and it's time to clean it up.
 	 */
 	void (*free_job)(struct drm_sched_job *sched_job);
 };
 
 /**
- * One scheduler is implemented for each hardware ring
-*/
+ * struct drm_gpu_scheduler
+ *
+ * @ops: backend operations provided by the driver.
+ * @hw_submission_limit: the max size of the hardware queue.
+ * @timeout: the time after which a job is removed from the scheduler.
+ * @name: name of the ring for which this scheduler is being used.
+ * @sched_rq: priority wise array of run queues.
+ * @wake_up_worker: the wait queue on which the scheduler sleeps until a job
+ *                  is ready to be scheduled.
+ * @job_scheduled: once @drm_sched_entity_do_release is called the scheduler
+ *                 waits on this wait queue until all the scheduled jobs are
+ *                 finished.
+ * @hw_rq_count: the number of jobs currently in the hardware queue.
+ * @job_id_count: used to assign unique id to the each job.
+ * @thread: the kthread on which the scheduler which run.
+ * @ring_mirror_list: the list of jobs which are currently in the job queue.
+ * @job_list_lock: lock to protect the ring_mirror_list.
+ * @hang_limit: once the hangs by a job crosses this limit then it is marked
+ *              guilty and it will be considered for scheduling further.
+ *
+ * One scheduler is implemented for each hardware ring.
+ */
 struct drm_gpu_scheduler {
 	const struct drm_sched_backend_ops	*ops;
 	uint32_t			hw_submission_limit;

commit 563e1e664d27292a3b55ca08366dc8c32db52450
Author: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
Date:   Tue May 15 14:42:20 2018 -0400

    drm/scheduler: Remove obsolete spinlock.
    
    This spinlock is superfluous, any call to drm_sched_entity_push_job
    should already be under a lock together with matching drm_sched_job_init
    to match the order of insertion into queue with job's fence seqence
    number.
    
    v2:
    Improve patch description.
    Add functions documentation describing the locking considerations
    
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Acked-by: Chunming Zhou <david1.zhou@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 52380067a43f..dec655894d08 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -56,7 +56,6 @@ struct drm_sched_entity {
 	spinlock_t			rq_lock;
 	struct drm_gpu_scheduler	*sched;
 
-	spinlock_t			queue_lock;
 	struct spsc_queue		job_queue;
 
 	atomic_t			fence_seq;

commit 8344c53f57057b42a5da87e9557c40fcda18fb7a
Author: Nayan Deshmukh <nayan26deshmukh@gmail.com>
Date:   Thu Mar 29 22:36:32 2018 +0530

    drm/scheduler: remove unused parameter
    
    this patch also effect the amdgpu and etnaviv drivers which
    use the function drm_sched_entity_init
    
    Signed-off-by: Nayan Deshmukh <nayan26deshmukh@gmail.com>
    Suggested-by: Christian König <christian.koenig@amd.com>
    Acked-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index 350a62c26b29..52380067a43f 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -188,7 +188,7 @@ void drm_sched_fini(struct drm_gpu_scheduler *sched);
 int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
 			  struct drm_sched_entity *entity,
 			  struct drm_sched_rq *rq,
-			  uint32_t jobs, atomic_t *guilty);
+			  atomic_t *guilty);
 void drm_sched_entity_do_release(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity);
 void drm_sched_entity_cleanup(struct drm_gpu_scheduler *sched,

commit 8ee3a52e3f35e064a3bf82f21dc74ddaf9843648
Author: Emily Deng <Emily.Deng@amd.com>
Date:   Mon Apr 16 10:07:02 2018 +0800

    drm/gpu-sched: fix force APP kill hang(v4)
    
    issue:
    there are VMC page fault occurred if force APP kill during
    3dmark test, the cause is in entity_fini we manually signal
    all those jobs in entity's queue which confuse the sync/dep
    mechanism:
    
    1)page fault occurred in sdma's clear job which operate on
    shadow buffer, and shadow buffer's Gart table is cleaned by
    ttm_bo_release since the fence in its reservation was fake signaled
    by entity_fini() under the case of SIGKILL received.
    
    2)page fault occurred in gfx' job because during the lifetime
    of gfx job we manually fake signal all jobs from its entity
    in entity_fini(), thus the unmapping/clear PTE job depend on those
    result fence is satisfied and sdma start clearing the PTE and lead
    to GFX page fault.
    
    fix:
    1)should at least wait all jobs already scheduled complete in entity_fini()
    if SIGKILL is the case.
    
    2)if a fence signaled and try to clear some entity's dependency, should
    set this entity guilty to prevent its job really run since the dependency
    is fake signaled.
    
    v2:
    splitting drm_sched_entity_fini() into two functions:
    1)The first one is does the waiting, removes the entity from the
    runqueue and returns an error when the process was killed.
    2)The second one then goes over the entity, install it as
    completion signal for the remaining jobs and signals all jobs
    with an error code.
    
    v3:
    1)Replace the fini1 and fini2 with better name
    2)Call the first part before the VM teardown in
    amdgpu_driver_postclose_kms() and the second part
    after the VM teardown
    3)Keep the original function drm_sched_entity_fini to
    refine the code.
    
    v4:
    1)Rename entity->finished to entity->last_scheduled;
    2)Rename drm_sched_entity_fini_job_cb() to
    drm_sched_entity_kill_jobs_cb();
    3)Pass NULL to drm_sched_entity_fini_job_cb() if -ENOENT;
    4)Replace the type of entity->fini_status with "int";
    5)Remove the check about entity->finished.
    
    Signed-off-by: Monk Liu <Monk.Liu@amd.com>
    Signed-off-by: Emily Deng <Emily.Deng@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index c053a32341bf..350a62c26b29 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -65,6 +65,8 @@ struct drm_sched_entity {
 	struct dma_fence		*dependency;
 	struct dma_fence_cb		cb;
 	atomic_t			*guilty; /* points to ctx's guilty */
+	int            fini_status;
+	struct dma_fence    *last_scheduled;
 };
 
 /**
@@ -119,6 +121,7 @@ struct drm_sched_job {
 	uint64_t			id;
 	atomic_t			karma;
 	enum drm_sched_priority		s_priority;
+	struct drm_sched_entity  *entity;
 };
 
 static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,
@@ -186,6 +189,10 @@ int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
 			  struct drm_sched_entity *entity,
 			  struct drm_sched_rq *rq,
 			  uint32_t jobs, atomic_t *guilty);
+void drm_sched_entity_do_release(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity);
+void drm_sched_entity_cleanup(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity);
 void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
 			   struct drm_sched_entity *entity);
 void drm_sched_entity_push_job(struct drm_sched_job *sched_job,

commit 1a61ee07211c543bf43e635fa703c162a78af0e1
Author: Eric Anholt <eric@anholt.net>
Date:   Wed Apr 4 15:32:51 2018 -0700

    drm/sched: Extend the documentation.
    
    These comments answer all the questions I had for myself when
    implementing a driver using the GPU scheduler.
    
    Signed-off-by: Eric Anholt <eric@anholt.net>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index dfd54fb94e10..c053a32341bf 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -43,10 +43,12 @@ enum drm_sched_priority {
 };
 
 /**
- * A scheduler entity is a wrapper around a job queue or a group
- * of other entities. Entities take turns emitting jobs from their
- * job queues to corresponding hardware ring based on scheduling
- * policy.
+ * drm_sched_entity - A wrapper around a job queue (typically attached
+ * to the DRM file_priv).
+ *
+ * Entities will emit jobs in order to their corresponding hardware
+ * ring, and the scheduler will alternate between entities based on
+ * scheduling policy.
 */
 struct drm_sched_entity {
 	struct list_head		list;
@@ -78,7 +80,18 @@ struct drm_sched_rq {
 
 struct drm_sched_fence {
 	struct dma_fence		scheduled;
+
+	/* This fence is what will be signaled by the scheduler when
+	 * the job is completed.
+	 *
+	 * When setting up an out fence for the job, you should use
+	 * this, since it's available immediately upon
+	 * drm_sched_job_init(), and the fence returned by the driver
+	 * from run_job() won't be created until the dependencies have
+	 * resolved.
+	 */
 	struct dma_fence		finished;
+
 	struct dma_fence_cb		cb;
 	struct dma_fence		*parent;
 	struct drm_gpu_scheduler	*sched;
@@ -88,6 +101,13 @@ struct drm_sched_fence {
 
 struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
 
+/**
+ * drm_sched_job - A job to be run by an entity.
+ *
+ * A job is created by the driver using drm_sched_job_init(), and
+ * should call drm_sched_entity_push_job() once it wants the scheduler
+ * to schedule the job.
+ */
 struct drm_sched_job {
 	struct spsc_node		queue_node;
 	struct drm_gpu_scheduler	*sched;
@@ -112,10 +132,28 @@ static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,
  * these functions should be implemented in driver side
 */
 struct drm_sched_backend_ops {
+	/* Called when the scheduler is considering scheduling this
+	 * job next, to get another struct dma_fence for this job to
+	 * block on.  Once it returns NULL, run_job() may be called.
+	 */
 	struct dma_fence *(*dependency)(struct drm_sched_job *sched_job,
 					struct drm_sched_entity *s_entity);
+
+	/* Called to execute the job once all of the dependencies have
+	 * been resolved.  This may be called multiple times, if
+	 * timedout_job() has happened and drm_sched_job_recovery()
+	 * decides to try it again.
+	 */
 	struct dma_fence *(*run_job)(struct drm_sched_job *sched_job);
+
+	/* Called when a job has taken too long to execute, to trigger
+	 * GPU recovery.
+	 */
 	void (*timedout_job)(struct drm_sched_job *sched_job);
+
+	/* Called once the job's finished fence has been signaled and
+	 * it's time to clean it up.
+	 */
 	void (*free_job)(struct drm_sched_job *sched_job);
 };
 

commit 4983e48c8539282be15f660bdd2c4260467b1190
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Wed Dec 6 17:49:40 2017 +0100

    drm/sched: move fence slab handling to module init/exit
    
    This is the only part of the scheduler which must not be called from
    different drivers. Move it to module init/exit so it is done a single
    time when loading the scheduler.
    
    Reviewed-by: Chunming Zhou <david1.zhou@amd.com>
    Tested-by: Dieter Nützel <Dieter@nuetzel-hh.de>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
index d29da4cbb042..dfd54fb94e10 100644
--- a/include/drm/gpu_scheduler.h
+++ b/include/drm/gpu_scheduler.h
@@ -155,9 +155,6 @@ void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
 void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
 			     struct drm_sched_rq *rq);
 
-int drm_sched_fence_slab_init(void);
-void drm_sched_fence_slab_fini(void);
-
 struct drm_sched_fence *drm_sched_fence_create(
 	struct drm_sched_entity *s_entity, void *owner);
 void drm_sched_fence_scheduled(struct drm_sched_fence *fence);

commit 1b1f42d8fde4fef1ed7873bf5aa91755f8c3de35
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Wed Dec 6 17:49:39 2017 +0100

    drm: move amd_gpu_scheduler into common location
    
    This moves and renames the AMDGPU scheduler to a common location in DRM
    in order to facilitate re-use by other drivers. This is mostly a straight
    forward rename with no code changes.
    
    One notable exception is the function to_drm_sched_fence(), which is no
    longer a inline header function to avoid the need to export the
    drm_sched_fence_ops_scheduled and drm_sched_fence_ops_finished structures.
    
    Reviewed-by: Chunming Zhou <david1.zhou@amd.com>
    Tested-by: Dieter Nützel <Dieter@nuetzel-hh.de>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
new file mode 100644
index 000000000000..d29da4cbb042
--- /dev/null
+++ b/include/drm/gpu_scheduler.h
@@ -0,0 +1,176 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef _DRM_GPU_SCHEDULER_H_
+#define _DRM_GPU_SCHEDULER_H_
+
+#include <drm/spsc_queue.h>
+#include <linux/dma-fence.h>
+
+struct drm_gpu_scheduler;
+struct drm_sched_rq;
+
+enum drm_sched_priority {
+	DRM_SCHED_PRIORITY_MIN,
+	DRM_SCHED_PRIORITY_LOW = DRM_SCHED_PRIORITY_MIN,
+	DRM_SCHED_PRIORITY_NORMAL,
+	DRM_SCHED_PRIORITY_HIGH_SW,
+	DRM_SCHED_PRIORITY_HIGH_HW,
+	DRM_SCHED_PRIORITY_KERNEL,
+	DRM_SCHED_PRIORITY_MAX,
+	DRM_SCHED_PRIORITY_INVALID = -1,
+	DRM_SCHED_PRIORITY_UNSET = -2
+};
+
+/**
+ * A scheduler entity is a wrapper around a job queue or a group
+ * of other entities. Entities take turns emitting jobs from their
+ * job queues to corresponding hardware ring based on scheduling
+ * policy.
+*/
+struct drm_sched_entity {
+	struct list_head		list;
+	struct drm_sched_rq		*rq;
+	spinlock_t			rq_lock;
+	struct drm_gpu_scheduler	*sched;
+
+	spinlock_t			queue_lock;
+	struct spsc_queue		job_queue;
+
+	atomic_t			fence_seq;
+	uint64_t			fence_context;
+
+	struct dma_fence		*dependency;
+	struct dma_fence_cb		cb;
+	atomic_t			*guilty; /* points to ctx's guilty */
+};
+
+/**
+ * Run queue is a set of entities scheduling command submissions for
+ * one specific ring. It implements the scheduling policy that selects
+ * the next entity to emit commands from.
+*/
+struct drm_sched_rq {
+	spinlock_t			lock;
+	struct list_head		entities;
+	struct drm_sched_entity		*current_entity;
+};
+
+struct drm_sched_fence {
+	struct dma_fence		scheduled;
+	struct dma_fence		finished;
+	struct dma_fence_cb		cb;
+	struct dma_fence		*parent;
+	struct drm_gpu_scheduler	*sched;
+	spinlock_t			lock;
+	void				*owner;
+};
+
+struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
+
+struct drm_sched_job {
+	struct spsc_node		queue_node;
+	struct drm_gpu_scheduler	*sched;
+	struct drm_sched_fence		*s_fence;
+	struct dma_fence_cb		finish_cb;
+	struct work_struct		finish_work;
+	struct list_head		node;
+	struct delayed_work		work_tdr;
+	uint64_t			id;
+	atomic_t			karma;
+	enum drm_sched_priority		s_priority;
+};
+
+static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,
+					    int threshold)
+{
+	return (s_job && atomic_inc_return(&s_job->karma) > threshold);
+}
+
+/**
+ * Define the backend operations called by the scheduler,
+ * these functions should be implemented in driver side
+*/
+struct drm_sched_backend_ops {
+	struct dma_fence *(*dependency)(struct drm_sched_job *sched_job,
+					struct drm_sched_entity *s_entity);
+	struct dma_fence *(*run_job)(struct drm_sched_job *sched_job);
+	void (*timedout_job)(struct drm_sched_job *sched_job);
+	void (*free_job)(struct drm_sched_job *sched_job);
+};
+
+/**
+ * One scheduler is implemented for each hardware ring
+*/
+struct drm_gpu_scheduler {
+	const struct drm_sched_backend_ops	*ops;
+	uint32_t			hw_submission_limit;
+	long				timeout;
+	const char			*name;
+	struct drm_sched_rq		sched_rq[DRM_SCHED_PRIORITY_MAX];
+	wait_queue_head_t		wake_up_worker;
+	wait_queue_head_t		job_scheduled;
+	atomic_t			hw_rq_count;
+	atomic64_t			job_id_count;
+	struct task_struct		*thread;
+	struct list_head		ring_mirror_list;
+	spinlock_t			job_list_lock;
+	int				hang_limit;
+};
+
+int drm_sched_init(struct drm_gpu_scheduler *sched,
+		   const struct drm_sched_backend_ops *ops,
+		   uint32_t hw_submission, unsigned hang_limit, long timeout,
+		   const char *name);
+void drm_sched_fini(struct drm_gpu_scheduler *sched);
+
+int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
+			  struct drm_sched_entity *entity,
+			  struct drm_sched_rq *rq,
+			  uint32_t jobs, atomic_t *guilty);
+void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity);
+void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
+			       struct drm_sched_entity *entity);
+void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
+			     struct drm_sched_rq *rq);
+
+int drm_sched_fence_slab_init(void);
+void drm_sched_fence_slab_fini(void);
+
+struct drm_sched_fence *drm_sched_fence_create(
+	struct drm_sched_entity *s_entity, void *owner);
+void drm_sched_fence_scheduled(struct drm_sched_fence *fence);
+void drm_sched_fence_finished(struct drm_sched_fence *fence);
+int drm_sched_job_init(struct drm_sched_job *job,
+		       struct drm_gpu_scheduler *sched,
+		       struct drm_sched_entity *entity,
+		       void *owner);
+void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
+			    struct drm_sched_job *job);
+void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
+bool drm_sched_dependency_optimized(struct dma_fence* fence,
+				    struct drm_sched_entity *entity);
+void drm_sched_job_kickout(struct drm_sched_job *s_job);
+
+#endif
