commit 0cdea4455acd350a7f62406478e3d6d1f764cef9
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Mon May 4 17:40:35 2020 +0200

    drm/mm: optimize rb_hole_addr rbtree search
    
    Userspace can severely fragment rb_hole_addr rbtree by manipulating
    alignment while allocating buffers. Fragmented rb_hole_addr rbtree
    would result in large delays while allocating buffer object for a
    userspace application. It takes long time to find suitable hole
    because if we fail to find a suitable hole in the first attempt
    then we look for neighbouring nodes using rb_prev()/rb_next().
    Traversing rbtree using rb_prev()/rb_next() can take really long
    time if the tree is fragmented.
    
    This patch improves searches in fragmented rb_hole_addr rbtree by
    modifying it to an augmented rbtree which will store an extra field
    in drm_mm_node, subtree_max_hole. Each drm_mm_node now stores maximum
    hole size for its subtree in drm_mm_node->subtree_max_hole. Using
    drm_mm_node->subtree_max_hole, it is possible to eliminate a complete
    subtree if that subtree is unable to serve a request hence reducing
    number of rb_prev()/rb_next() used.
    
    With this patch applied, 1 million bo allocs on amdgpu took ~8 sec,
    compared to 50k bo allocs which took 28 sec without it.
    
    partial test code:
    int test_fragmentation(void)
    {
    
            int i = 0;
            uint32_t  minor_version;
            uint32_t  major_version;
    
            struct amdgpu_bo_alloc_request request = {};
            amdgpu_bo_handle vram_handle[MAX_ALLOC] = {};
            amdgpu_device_handle device_handle;
    
            request.alloc_size = 4096;
            request.phys_alignment = 8192;
            request.preferred_heap = AMDGPU_GEM_DOMAIN_VRAM;
    
            int fd = open("/dev/dri/card0", O_RDWR | O_CLOEXEC);
            amdgpu_device_initialize(fd, &major_version,  &minor_version,
                                     &device_handle);
    
            for (i = 0; i < MAX_ALLOC; i++) {
                    amdgpu_bo_alloc(device_handle, &request, &vram_handle[i]);
            }
    
            for (i = 0; i < MAX_ALLOC; i++)
                    amdgpu_bo_free(vram_handle[i]);
    
            return 0;
    }
    
    v2:
    Use RB_DECLARE_CALLBACKS_MAX to maintain subtree_max_hole
    v3:
    insert_hole_addr() should be static a function
    fix return value of next_hole_high_addr()/next_hole_low_addr()
    Reported-by: kbuild test robot <lkp@intel.com>
    v4:
    Fix commit message.
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Christian König <christian.koenig@amd.com>
    Link: https://patchwork.freedesktop.org/patch/364341/
    Signed-off-by: Christian König <christian.koenig@amd.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index ee8b0e80ca90..a01bc6fac83c 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -168,6 +168,7 @@ struct drm_mm_node {
 	struct rb_node rb_hole_addr;
 	u64 __subtree_last;
 	u64 hole_size;
+	u64 subtree_max_hole;
 	unsigned long flags;
 #define DRM_MM_NODE_ALLOCATED_BIT	0
 #define DRM_MM_NODE_SCANNED_BIT		1

commit 2214ddc2fda78bf48a3c36bcb4c8b9a95203049c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Mar 9 12:15:29 2020 +0000

    drm/mm: Allow drm_mm_initialized() to be used outside of the locks
    
    Mark up the potential racy read in drm_mm_initialized(), as we want a
    cheap and cheerful check:
    
    [  121.098731] BUG: KCSAN: data-race in _i915_gem_object_create_stolen [i915] / rm_hole
    [  121.098766]
    [  121.098789] write (marked) to 0xffff8881f01ed330 of 8 bytes by task 3568 on cpu 3:
    [  121.098831]  rm_hole+0x64/0x140
    [  121.098860]  drm_mm_insert_node_in_range+0x3d3/0x6c0
    [  121.099254]  i915_gem_stolen_insert_node_in_range+0x91/0xe0 [i915]
    [  121.099646]  _i915_gem_object_create_stolen+0x9d/0x100 [i915]
    [  121.100047]  i915_gem_object_create_region+0x7a/0xa0 [i915]
    [  121.100451]  i915_gem_object_create_stolen+0x33/0x50 [i915]
    [  121.100849]  intel_engine_create_ring+0x1af/0x280 [i915]
    [  121.101242]  __execlists_context_alloc+0xce/0x3d0 [i915]
    [  121.101635]  execlists_context_alloc+0x25/0x40 [i915]
    [  121.102030]  intel_context_alloc_state+0xb6/0xf0 [i915]
    [  121.102420]  __intel_context_do_pin+0x1ff/0x220 [i915]
    [  121.102815]  i915_gem_do_execbuffer+0x46b4/0x4c20 [i915]
    [  121.103211]  i915_gem_execbuffer2_ioctl+0x2c3/0x580 [i915]
    [  121.103244]  drm_ioctl_kernel+0xe4/0x120
    [  121.103269]  drm_ioctl+0x297/0x4c7
    [  121.103296]  ksys_ioctl+0x89/0xb0
    [  121.103321]  __x64_sys_ioctl+0x42/0x60
    [  121.103349]  do_syscall_64+0x6e/0x2c0
    [  121.103377]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  121.103403]
    [  121.103426] read to 0xffff8881f01ed330 of 8 bytes by task 3109 on cpu 1:
    [  121.103819]  _i915_gem_object_create_stolen+0x30/0x100 [i915]
    [  121.104228]  i915_gem_object_create_region+0x7a/0xa0 [i915]
    [  121.104631]  i915_gem_object_create_stolen+0x33/0x50 [i915]
    [  121.105025]  intel_engine_create_ring+0x1af/0x280 [i915]
    [  121.105420]  __execlists_context_alloc+0xce/0x3d0 [i915]
    [  121.105818]  execlists_context_alloc+0x25/0x40 [i915]
    [  121.106202]  intel_context_alloc_state+0xb6/0xf0 [i915]
    [  121.106595]  __intel_context_do_pin+0x1ff/0x220 [i915]
    [  121.106985]  i915_gem_do_execbuffer+0x46b4/0x4c20 [i915]
    [  121.107375]  i915_gem_execbuffer2_ioctl+0x2c3/0x580 [i915]
    [  121.107409]  drm_ioctl_kernel+0xe4/0x120
    [  121.107437]  drm_ioctl+0x297/0x4c7
    [  121.107464]  ksys_ioctl+0x89/0xb0
    [  121.107489]  __x64_sys_ioctl+0x42/0x60
    [  121.107511]  do_syscall_64+0x6e/0x2c0
    [  121.107535]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200309121529.16497-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index d7939c054259..ee8b0e80ca90 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -272,7 +272,7 @@ static inline bool drm_mm_node_allocated(const struct drm_mm_node *node)
  */
 static inline bool drm_mm_initialized(const struct drm_mm *mm)
 {
-	return mm->hole_stack.next;
+	return READ_ONCE(mm->hole_stack.next);
 }
 
 /**

commit 4ee92c7149da9cb1991684628a9e47166a5e26f6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 3 22:00:59 2019 +0100

    drm/mm: Convert drm_mm_node booleans to bitops
    
    A straightforward conversion of assignment and checking of the boolean
    state flags (allocated, scanned) into non-atomic bitops. The caller
    remains responsible for all locking around the drm_mm and its nodes.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191003210100.22250-4-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 2c3bbb43c7d1..d7939c054259 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -168,8 +168,9 @@ struct drm_mm_node {
 	struct rb_node rb_hole_addr;
 	u64 __subtree_last;
 	u64 hole_size;
-	bool allocated : 1;
-	bool scanned_block : 1;
+	unsigned long flags;
+#define DRM_MM_NODE_ALLOCATED_BIT	0
+#define DRM_MM_NODE_SCANNED_BIT		1
 #ifdef CONFIG_DRM_DEBUG_MM
 	depot_stack_handle_t stack;
 #endif
@@ -253,7 +254,7 @@ struct drm_mm_scan {
  */
 static inline bool drm_mm_node_allocated(const struct drm_mm_node *node)
 {
-	return node->allocated;
+	return test_bit(DRM_MM_NODE_ALLOCATED_BIT, &node->flags);
 }
 
 /**

commit 83bc4ec37210b17bd611a58968b2ce0e9cc7f251
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon May 21 09:21:29 2018 +0100

    drm/mm: Add a search-by-address variant to only inspect a single hole
    
    Searching for an available hole by address is slow, as there no
    guarantee that a hole will be available and so we must walk over all
    nodes in the rbtree before we determine the search was futile. In many
    cases, the caller doesn't strictly care for the highest available hole
    and was just opportunistically laying out the address space in a
    preferred order. In such cases, the caller can accept any address and
    would rather do so then do a slow walk.
    
    To be able to mix search strategies, the caller wants to tell the drm_mm
    how long to spend on the search. Without a good guide for what should be
    the best split, start with a request to try once at most. That is return
    the top-most (or lowest) hole if it fulfils the alignment and size
    requirements.
    
    v2: Documentation, by why of example (selftests) and kerneldoc.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180521082131.13744-2-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index e3aa3bfd4860..2c3bbb43c7d1 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -109,6 +109,38 @@ enum drm_mm_insert_mode {
 	 * Allocates the node from the bottom of the found hole.
 	 */
 	DRM_MM_INSERT_EVICT,
+
+	/**
+	 * @DRM_MM_INSERT_ONCE:
+	 *
+	 * Only check the first hole for suitablity and report -ENOSPC
+	 * immediately otherwise, rather than check every hole until a
+	 * suitable one is found. Can only be used in conjunction with another
+	 * search method such as DRM_MM_INSERT_HIGH or DRM_MM_INSERT_LOW.
+	 */
+	DRM_MM_INSERT_ONCE = BIT(31),
+
+	/**
+	 * @DRM_MM_INSERT_HIGHEST:
+	 *
+	 * Only check the highest hole (the hole with the largest address) and
+	 * insert the node at the top of the hole or report -ENOSPC if
+	 * unsuitable.
+	 *
+	 * Does not search all holes.
+	 */
+	DRM_MM_INSERT_HIGHEST = DRM_MM_INSERT_HIGH | DRM_MM_INSERT_ONCE,
+
+	/**
+	 * @DRM_MM_INSERT_LOWEST:
+	 *
+	 * Only check the lowest hole (the hole with the smallest address) and
+	 * insert the node at the bottom of the hole or report -ENOSPC if
+	 * unsuitable.
+	 *
+	 * Does not search all holes.
+	 */
+	DRM_MM_INSERT_LOWEST  = DRM_MM_INSERT_LOW | DRM_MM_INSERT_ONCE,
 };
 
 /**

commit 2f7e87692e0441abf27a9714991edd136e87363a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon May 21 09:21:28 2018 +0100

    drm/mm: Reject over-sized allocation requests early
    
    As we keep an rbtree of available holes sorted by their size, we can
    very easily determine if there is any hole large enough that might
    satisfy the allocation request. This helps when dealing with a highly
    fragmented address space and a request for a search by address.
    
    To cache the largest size, we convert into the cached rbtree variant
    which tracks the leftmost node for us. However, currently we sorted into
    ascending size order so the leftmost node is the smallest, and so to
    make it the largest hole we need to invert our sorting.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180521082131.13744-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 101f566ae43d..e3aa3bfd4860 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -173,7 +173,7 @@ struct drm_mm {
 	struct drm_mm_node head_node;
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
 	struct rb_root_cached interval_tree;
-	struct rb_root holes_size;
+	struct rb_root_cached holes_size;
 	struct rb_root holes_addr;
 
 	unsigned long scan_active;

commit 0a2adb02d71e68e7e00913394ca2d9c5d9fe5eb7
Author: Liviu Dudau <Liviu.Dudau@arm.com>
Date:   Wed Nov 1 14:04:45 2017 +0000

    drm/drm_mm.h: Fix the name of the referenced function in comment
    
    drm_mm_insert_node_generic() is a simplified version of
    drm_mm_insert_node_in_range(), update comment to reflect correct
    function name.
    
    Signed-off-by: Liviu Dudau <liviu.dudau@arm.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Gustavo Padovan <gustavo.padovan@collabora.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171101140445.2798-1-Liviu.Dudau@arm.com

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 8d10fc97801c..101f566ae43d 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -386,7 +386,7 @@ int drm_mm_insert_node_in_range(struct drm_mm *mm,
  * @color: opaque tag value to use for this node
  * @mode: fine-tune the allocation search and placement
  *
- * This is a simplified version of drm_mm_insert_node_in_range_generic() with no
+ * This is a simplified version of drm_mm_insert_node_in_range() with no
  * range restrictions applied.
  *
  * The preallocated node must be cleared to 0.

commit f808c13fd3738948e10196496959871130612b61
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:15:08 2017 -0700

    lib/interval_tree: fast overlap detection
    
    Allow interval trees to quickly check for overlaps to avoid unnecesary
    tree lookups in interval_tree_iter_first().
    
    As of this patch, all interval tree flavors will require using a
    'rb_root_cached' such that we can have the leftmost node easily
    available.  While most users will make use of this feature, those with
    special functions (in addition to the generic insert, delete, search
    calls) will avoid using the cached option as they can do funky things
    with insertions -- for example, vma_interval_tree_insert_after().
    
    [jglisse@redhat.com: fix deadlock from typo vm_lock_anon_vma()]
      Link: http://lkml.kernel.org/r/20170808225719.20723-1-jglisse@redhat.com
    Link: http://lkml.kernel.org/r/20170719014603.19029-12-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Acked-by: Christian König <christian.koenig@amd.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Doug Ledford <dledford@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Christian Benvenuti <benve@cisco.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 49b292e98fec..8d10fc97801c 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -172,7 +172,7 @@ struct drm_mm {
 	 * according to the (increasing) start address of the memory node. */
 	struct drm_mm_node head_node;
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
-	struct rb_root interval_tree;
+	struct rb_root_cached interval_tree;
 	struct rb_root holes_size;
 	struct rb_root holes_addr;
 

commit b558dfd56a5c1f915327967ecfe1181cf2a7a494
Merge: c1ae3cfa0e89 ca39b449f6d0
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Mar 7 13:59:53 2017 +1000

    Merge tag 'drm-misc-next-2017-03-06' of git://anongit.freedesktop.org/git/drm-misc into drm-next
    
    First slice of drm-misc-next for 4.12:
    
    Core/subsystem-wide:
    - link status core patch from Manasi, for signalling link train fail
      to userspace. I also had the i915 patch in here, but that had a
      small buglet in our CI, so reverted.
    - more debugfs_remove removal from Noralf, almost there now (Noralf
      said he'll try to follow up with the stragglers).
    - drm todo moved into kerneldoc, for better visibility (see
      Documentation/gpu/todo.rst), lots of starter tasks in there.
    - devm_ of helpers + use it in sti (from Ben Gaignard, acked by Rob
      Herring)
    - extended framebuffer fbdev support (for fbdev flipping), and vblank
      wait ioctl fbdev support (Maxime Ripard)
    - misc small things all over, as usual
    - add vblank callbacks to drm_crtc_funcs, plus make lots of good use
      of this to simplify drivers (Shawn Guo)
    - new atomic iterator macros to unconfuse old vs. new state
    
    Small drivers:
    - vc4 improvements from Eric
    - vc4 kerneldocs (Eric)!
    - tons of improvements for dw-mipi-dsi in rockchip from John Keeping
      and Chris Zhong.
    - MAINTAINERS entries for drivers managed in drm-misc. It's not yet
      official, still an experiment, but definitely not complete fail and
      better to avoid confusion. We kinda screwed that up with drm-misc a
      bit when we started committers last year.
    - qxl atomic conversion (Gabriel Krisman)
    - bunch of virtual driver polish (qxl, virgl, ...)
    - misc tiny patches all over
    
    This is the first time we've done the same merge-window blackout for
    drm-misc as we've done for drm-intel for ages, hence why we have a
    _lot_ of stuff queued already. But it's still only half of drm-intel
    (room to grow!), and the drivers in drm-misc experiment seems to work
    at least insofar as that you also get lots of driver updates here
    alredy.
    
    * tag 'drm-misc-next-2017-03-06' of git://anongit.freedesktop.org/git/drm-misc: (141 commits)
      drm/vc4: Fix OOPSes from trying to cache a partially constructed BO.
      drm/vc4: Fulfill user BO creation requests from the kernel BO cache.
      Revert "drm/i915: Implement Link Rate fallback on Link training failure"
      drm/fb-helper: implement ioctl FBIO_WAITFORVSYNC
      drm: Update drm_fbdev_cma_init documentation
      drm/rockchip/dsi: add dw-mipi power domain support
      drm/rockchip/dsi: fix insufficient bandwidth of some panel
      dt-bindings: add power domain node for dw-mipi-rockchip
      drm/rockchip/dsi: remove mode_valid function
      drm/rockchip/dsi: dw-mipi: correct the coding style
      drm/rockchip/dsi: dw-mipi: support RK3399 mipi dsi
      dt-bindings: add rk3399 support for dw-mipi-rockchip
      drm/rockchip: dw-mipi-dsi: add reset control
      drm/rockchip: dw-mipi-dsi: support non-burst modes
      drm/rockchip: dw-mipi-dsi: defer probe if panel is not loaded
      drm/rockchip: vop: test for P{H,V}SYNC
      drm/rockchip: dw-mipi-dsi: use positive check for N{H, V}SYNC
      drm/rockchip: dw-mipi-dsi: use specific poll helper
      drm/rockchip: dw-mipi-dsi: improve PLL configuration
      drm/rockchip: dw-mipi-dsi: properly configure PHY timing
      ...

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index d81b0ba9921f..2ef16bf25826 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -40,6 +40,7 @@
 #include <linux/bug.h>
 #include <linux/rbtree.h>
 #include <linux/kernel.h>
+#include <linux/mm_types.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
 #ifdef CONFIG_DRM_DEBUG_MM

commit bbba96931762bcad8a691dfbf8d1520b71831c3a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Feb 4 11:19:13 2017 +0000

    drm: Micro-optimise drm_mm_for_each_node_in_range()
    
    As we require valid start/end parameters, we can replace the initial
    potential NULL with a pointer to the drm_mm.head_node and so reduce the
    test on every iteration from a NULL + address comparison to just an
    address comparison.
    
    add/remove: 0/0 grow/shrink: 0/1 up/down: 0/-26 (-26)
    function                                     old     new   delta
    i915_gem_evict_for_node                      719     693     -26
    
    (No other users outside of the test harness.)
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170204111913.12416-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index d81b0ba9921f..f262da180117 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -459,10 +459,13 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
  * but using the internal interval tree to accelerate the search for the
  * starting node, and so not safe against removal of elements. It assumes
  * that @end is within (or is the upper limit of) the drm_mm allocator.
+ * If [@start, @end] are beyond the range of the drm_mm, the iterator may walk
+ * over the special _unallocated_ &drm_mm.head_node, and may even continue
+ * indefinitely.
  */
 #define drm_mm_for_each_node_in_range(node__, mm__, start__, end__)	\
 	for (node__ = __drm_mm_interval_first((mm__), (start__), (end__)-1); \
-	     node__ && node__->start < (end__);				\
+	     node__->start < (end__);					\
 	     node__ = list_next_entry(node__, node_list))
 
 void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,

commit 4e64e5539d152e202ad6eea2b6f65f3ab58d9428
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Feb 2 21:04:38 2017 +0000

    drm: Improve drm_mm search (and fix topdown allocation) with rbtrees
    
    The drm_mm range manager claimed to support top-down insertion, but it
    was neither searching for the top-most hole that could fit the
    allocation request nor fitting the request to the hole correctly.
    
    In order to search the range efficiently, we create a secondary index
    for the holes using either their size or their address. This index
    allows us to find the smallest hole or the hole at the bottom or top of
    the range efficiently, whilst keeping the hole stack to rapidly service
    evictions.
    
    v2: Search for holes both high and low. Rename flags to mode.
    v3: Discover rb_entry_safe() and use it!
    v4: Kerneldoc for enum drm_mm_insert_mode.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: "Christian König" <christian.koenig@amd.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Daniel Vetter <daniel.vetter@intel.com>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Sean Paul <seanpaul@chromium.org>
    Cc: Lucas Stach <l.stach@pengutronix.de>
    Cc: Christian Gmeiner <christian.gmeiner@gmail.com>
    Cc: Rob Clark <robdclark@gmail.com>
    Cc: Thierry Reding <thierry.reding@gmail.com>
    Cc: Stephen Warren <swarren@wwwdotorg.org>
    Cc: Alexandre Courbot <gnurou@gmail.com>
    Cc: Eric Anholt <eric@anholt.net>
    Cc: Sinclair Yeh <syeh@vmware.com>
    Cc: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Sinclair Yeh <syeh@vmware.com> # vmwgfx
    Reviewed-by: Lucas Stach <l.stach@pengutronix.de> #etnaviv
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170202210438.28702-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 3bddca8fd2b5..d81b0ba9921f 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -53,19 +53,62 @@
 #define DRM_MM_BUG_ON(expr) BUILD_BUG_ON_INVALID(expr)
 #endif
 
-enum drm_mm_search_flags {
-	DRM_MM_SEARCH_DEFAULT =		0,
-	DRM_MM_SEARCH_BEST =		1 << 0,
-	DRM_MM_SEARCH_BELOW =		1 << 1,
-};
+/**
+ * enum drm_mm_insert_mode - control search and allocation behaviour
+ *
+ * The &struct drm_mm range manager supports finding a suitable modes using
+ * a number of search trees. These trees are oranised by size, by address and
+ * in most recent eviction order. This allows the user to find either the
+ * smallest hole to reuse, the lowest or highest address to reuse, or simply
+ * reuse the most recent eviction that fits. When allocating the &drm_mm_node
+ * from within the hole, the &drm_mm_insert_mode also dictate whether to
+ * allocate the lowest matching address or the highest.
+ */
+enum drm_mm_insert_mode {
+	/**
+	 * @DRM_MM_INSERT_BEST:
+	 *
+	 * Search for the smallest hole (within the search range) that fits
+	 * the desired node.
+	 *
+	 * Allocates the node from the bottom of the found hole.
+	 */
+	DRM_MM_INSERT_BEST = 0,
 
-enum drm_mm_allocator_flags {
-	DRM_MM_CREATE_DEFAULT =		0,
-	DRM_MM_CREATE_TOP =		1 << 0,
-};
+	/**
+	 * @DRM_MM_INSERT_LOW:
+	 *
+	 * Search for the lowest hole (address closest to 0, within the search
+	 * range) that fits the desired node.
+	 *
+	 * Allocates the node from the bottom of the found hole.
+	 */
+	DRM_MM_INSERT_LOW,
 
-#define DRM_MM_BOTTOMUP DRM_MM_SEARCH_DEFAULT, DRM_MM_CREATE_DEFAULT
-#define DRM_MM_TOPDOWN DRM_MM_SEARCH_BELOW, DRM_MM_CREATE_TOP
+	/**
+	 * @DRM_MM_INSERT_HIGH:
+	 *
+	 * Search for the highest hole (address closest to U64_MAX, within the
+	 * search range) that fits the desired node.
+	 *
+	 * Allocates the node from the *top* of the found hole. The specified
+	 * alignment for the node is applied to the base of the node
+	 * (&drm_mm_node.start).
+	 */
+	DRM_MM_INSERT_HIGH,
+
+	/**
+	 * @DRM_MM_INSERT_EVICT:
+	 *
+	 * Search for the most recently evicted hole (within the search range)
+	 * that fits the desired node. This is appropriate for use immediately
+	 * after performing an eviction scan (see drm_mm_scan_init()) and
+	 * removing the selected nodes to form a hole.
+	 *
+	 * Allocates the node from the bottom of the found hole.
+	 */
+	DRM_MM_INSERT_EVICT,
+};
 
 /**
  * struct drm_mm_node - allocated block in the DRM allocator
@@ -84,14 +127,16 @@ struct drm_mm_node {
 	/** @size: Size of the allocated block. */
 	u64 size;
 	/* private: */
+	struct drm_mm *mm;
 	struct list_head node_list;
 	struct list_head hole_stack;
 	struct rb_node rb;
-	unsigned hole_follows : 1;
-	unsigned allocated : 1;
-	bool scanned_block : 1;
+	struct rb_node rb_hole_size;
+	struct rb_node rb_hole_addr;
 	u64 __subtree_last;
-	struct drm_mm *mm;
+	u64 hole_size;
+	bool allocated : 1;
+	bool scanned_block : 1;
 #ifdef CONFIG_DRM_DEBUG_MM
 	depot_stack_handle_t stack;
 #endif
@@ -127,6 +172,8 @@ struct drm_mm {
 	struct drm_mm_node head_node;
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
 	struct rb_root interval_tree;
+	struct rb_root holes_size;
+	struct rb_root holes_addr;
 
 	unsigned long scan_active;
 };
@@ -155,7 +202,7 @@ struct drm_mm_scan {
 	u64 hit_end;
 
 	unsigned long color;
-	unsigned int flags;
+	enum drm_mm_insert_mode mode;
 };
 
 /**
@@ -208,7 +255,7 @@ static inline bool drm_mm_initialized(const struct drm_mm *mm)
  */
 static inline bool drm_mm_hole_follows(const struct drm_mm_node *node)
 {
-	return node->hole_follows;
+	return node->hole_size;
 }
 
 static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
@@ -291,17 +338,9 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 #define drm_mm_for_each_node_safe(entry, next, mm) \
 	list_for_each_entry_safe(entry, next, drm_mm_nodes(mm), node_list)
 
-#define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
-	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
-	     &entry->hole_stack != &(mm)->hole_stack ? \
-	     hole_start = drm_mm_hole_node_start(entry), \
-	     hole_end = drm_mm_hole_node_end(entry), \
-	     1 : 0; \
-	     entry = list_entry((backwards) ? entry->hole_stack.prev : entry->hole_stack.next, struct drm_mm_node, hole_stack))
-
 /**
  * drm_mm_for_each_hole - iterator to walk over all holes
- * @entry: &drm_mm_node used internally to track progress
+ * @pos: &drm_mm_node used internally to track progress
  * @mm: &drm_mm allocator to walk
  * @hole_start: ulong variable to assign the hole start to on each iteration
  * @hole_end: ulong variable to assign the hole end to on each iteration
@@ -314,57 +353,28 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  * Implementation Note:
  * We need to inline list_for_each_entry in order to be able to set hole_start
  * and hole_end on each iteration while keeping the macro sane.
- *
- * The __drm_mm_for_each_hole version is similar, but with added support for
- * going backwards.
  */
-#define drm_mm_for_each_hole(entry, mm, hole_start, hole_end) \
-	__drm_mm_for_each_hole(entry, mm, hole_start, hole_end, 0)
+#define drm_mm_for_each_hole(pos, mm, hole_start, hole_end) \
+	for (pos = list_first_entry(&(mm)->hole_stack, \
+				    typeof(*pos), hole_stack); \
+	     &pos->hole_stack != &(mm)->hole_stack ? \
+	     hole_start = drm_mm_hole_node_start(pos), \
+	     hole_end = hole_start + pos->hole_size, \
+	     1 : 0; \
+	     pos = list_next_entry(pos, hole_stack))
 
 /*
  * Basic range manager support (drm_mm.c)
  */
 int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
-int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
-					struct drm_mm_node *node,
-					u64 size,
-					u64 alignment,
-					unsigned long color,
-					u64 start,
-					u64 end,
-					enum drm_mm_search_flags sflags,
-					enum drm_mm_allocator_flags aflags);
-
-/**
- * drm_mm_insert_node_in_range - ranged search for space and insert @node
- * @mm: drm_mm to allocate from
- * @node: preallocate node to insert
- * @size: size of the allocation
- * @alignment: alignment of the allocation
- * @start: start of the allowed range for this node
- * @end: end of the allowed range for this node
- * @flags: flags to fine-tune the allocation
- *
- * This is a simplified version of drm_mm_insert_node_in_range_generic() with
- * @color set to 0.
- *
- * The preallocated node must be cleared to 0.
- *
- * Returns:
- * 0 on success, -ENOSPC if there's no suitable hole.
- */
-static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
-					      struct drm_mm_node *node,
-					      u64 size,
-					      u64 alignment,
-					      u64 start,
-					      u64 end,
-					      enum drm_mm_search_flags flags)
-{
-	return drm_mm_insert_node_in_range_generic(mm, node, size, alignment,
-						   0, start, end, flags,
-						   DRM_MM_CREATE_DEFAULT);
-}
+int drm_mm_insert_node_in_range(struct drm_mm *mm,
+				struct drm_mm_node *node,
+				u64 size,
+				u64 alignment,
+				unsigned long color,
+				u64 start,
+				u64 end,
+				enum drm_mm_insert_mode mode);
 
 /**
  * drm_mm_insert_node_generic - search for space and insert @node
@@ -373,8 +383,7 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
  * @size: size of the allocation
  * @alignment: alignment of the allocation
  * @color: opaque tag value to use for this node
- * @sflags: flags to fine-tune the allocation search
- * @aflags: flags to fine-tune the allocation behavior
+ * @mode: fine-tune the allocation search and placement
  *
  * This is a simplified version of drm_mm_insert_node_in_range_generic() with no
  * range restrictions applied.
@@ -388,13 +397,11 @@ static inline int
 drm_mm_insert_node_generic(struct drm_mm *mm, struct drm_mm_node *node,
 			   u64 size, u64 alignment,
 			   unsigned long color,
-			   enum drm_mm_search_flags sflags,
-			   enum drm_mm_allocator_flags aflags)
+			   enum drm_mm_insert_mode mode)
 {
-	return drm_mm_insert_node_in_range_generic(mm, node,
-						   size, alignment, 0,
-						   0, U64_MAX,
-						   sflags, aflags);
+	return drm_mm_insert_node_in_range(mm, node,
+					   size, alignment, color,
+					   0, U64_MAX, mode);
 }
 
 /**
@@ -402,8 +409,6 @@ drm_mm_insert_node_generic(struct drm_mm *mm, struct drm_mm_node *node,
  * @mm: drm_mm to allocate from
  * @node: preallocate node to insert
  * @size: size of the allocation
- * @alignment: alignment of the allocation
- * @flags: flags to fine-tune the allocation
  *
  * This is a simplified version of drm_mm_insert_node_generic() with @color set
  * to 0.
@@ -415,13 +420,9 @@ drm_mm_insert_node_generic(struct drm_mm *mm, struct drm_mm_node *node,
  */
 static inline int drm_mm_insert_node(struct drm_mm *mm,
 				     struct drm_mm_node *node,
-				     u64 size,
-				     u64 alignment,
-				     enum drm_mm_search_flags flags)
+				     u64 size)
 {
-	return drm_mm_insert_node_generic(mm, node,
-					  size, alignment, 0,
-					  flags, DRM_MM_CREATE_DEFAULT);
+	return drm_mm_insert_node_generic(mm, node, size, 0, 0, 0);
 }
 
 void drm_mm_remove_node(struct drm_mm_node *node);
@@ -468,7 +469,7 @@ void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
 				 struct drm_mm *mm,
 				 u64 size, u64 alignment, unsigned long color,
 				 u64 start, u64 end,
-				 unsigned int flags);
+				 enum drm_mm_insert_mode mode);
 
 /**
  * drm_mm_scan_init - initialize lru scanning
@@ -477,7 +478,7 @@ void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
  * @size: size of the allocation
  * @alignment: alignment of the allocation
  * @color: opaque tag value to use for the allocation
- * @flags: flags to specify how the allocation will be performed afterwards
+ * @mode: fine-tune the allocation search and placement
  *
  * This is a simplified version of drm_mm_scan_init_with_range() with no range
  * restrictions applied.
@@ -494,12 +495,11 @@ static inline void drm_mm_scan_init(struct drm_mm_scan *scan,
 				    u64 size,
 				    u64 alignment,
 				    unsigned long color,
-				    unsigned int flags)
+				    enum drm_mm_insert_mode mode)
 {
 	drm_mm_scan_init_with_range(scan, mm,
 				    size, alignment, color,
-				    0, U64_MAX,
-				    flags);
+				    0, U64_MAX, mode);
 }
 
 bool drm_mm_scan_add_block(struct drm_mm_scan *scan,

commit 05fc03217e08b90bff1ff22792d5f86dd32f15a6
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Thu Dec 29 21:48:23 2016 +0100

    drm/mm: Some doc polish
    
    Added some boilerplate for the structs, documented members where they
    are relevant and plenty of markup for hyperlinks all over. And a few
    small wording polish.
    
    Note that the intro needs some more love after the DRM_MM_INSERT_*
    patch from Chris has landed.
    
    v2: Spelling fixes (Chris).
    
    v3: Use &struct foo instead of &foo structure (Chris).
    
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/1483044517-5770-3-git-send-email-daniel.vetter@ffwll.ch

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 1383ac2328b8..3bddca8fd2b5 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -67,16 +67,29 @@ enum drm_mm_allocator_flags {
 #define DRM_MM_BOTTOMUP DRM_MM_SEARCH_DEFAULT, DRM_MM_CREATE_DEFAULT
 #define DRM_MM_TOPDOWN DRM_MM_SEARCH_BELOW, DRM_MM_CREATE_TOP
 
+/**
+ * struct drm_mm_node - allocated block in the DRM allocator
+ *
+ * This represents an allocated block in a &drm_mm allocator. Except for
+ * pre-reserved nodes inserted using drm_mm_reserve_node() the structure is
+ * entirely opaque and should only be accessed through the provided funcions.
+ * Since allocation of these nodes is entirely handled by the driver they can be
+ * embedded.
+ */
 struct drm_mm_node {
+	/** @color: Opaque driver-private tag. */
+	unsigned long color;
+	/** @start: Start address of the allocated block. */
+	u64 start;
+	/** @size: Size of the allocated block. */
+	u64 size;
+	/* private: */
 	struct list_head node_list;
 	struct list_head hole_stack;
 	struct rb_node rb;
 	unsigned hole_follows : 1;
 	unsigned allocated : 1;
 	bool scanned_block : 1;
-	unsigned long color;
-	u64 start;
-	u64 size;
 	u64 __subtree_last;
 	struct drm_mm *mm;
 #ifdef CONFIG_DRM_DEBUG_MM
@@ -84,7 +97,29 @@ struct drm_mm_node {
 #endif
 };
 
+/**
+ * struct drm_mm - DRM allocator
+ *
+ * DRM range allocator with a few special functions and features geared towards
+ * managing GPU memory. Except for the @color_adjust callback the structure is
+ * entirely opaque and should only be accessed through the provided functions
+ * and macros. This structure can be embedded into larger driver structures.
+ */
 struct drm_mm {
+	/**
+	 * @color_adjust:
+	 *
+	 * Optional driver callback to further apply restrictions on a hole. The
+	 * node argument points at the node containing the hole from which the
+	 * block would be allocated (see drm_mm_hole_follows() and friends). The
+	 * other arguments are the size of the block to be allocated. The driver
+	 * can adjust the start and end as needed to e.g. insert guard pages.
+	 */
+	void (*color_adjust)(const struct drm_mm_node *node,
+			     unsigned long color,
+			     u64 *start, u64 *end);
+
+	/* private: */
 	/* List of all memory nodes that immediately precede a free hole. */
 	struct list_head hole_stack;
 	/* head_node.node_list is the list of all memory nodes, ordered
@@ -93,14 +128,20 @@ struct drm_mm {
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
 	struct rb_root interval_tree;
 
-	void (*color_adjust)(const struct drm_mm_node *node,
-			     unsigned long color,
-			     u64 *start, u64 *end);
-
 	unsigned long scan_active;
 };
 
+/**
+ * struct drm_mm_scan - DRM allocator eviction roaster data
+ *
+ * This structure tracks data needed for the eviction roaster set up using
+ * drm_mm_scan_init(), and used with drm_mm_scan_add_block() and
+ * drm_mm_scan_remove_block(). The structure is entirely opaque and should only
+ * be accessed through the provided functions and macros. It is meant to be
+ * allocated temporarily by the driver on the stack.
+ */
 struct drm_mm_scan {
+	/* private: */
 	struct drm_mm *mm;
 
 	u64 size;
@@ -159,7 +200,8 @@ static inline bool drm_mm_initialized(const struct drm_mm *mm)
  *
  * Holes are embedded into the drm_mm using the tail of a drm_mm_node.
  * If you wish to know whether a hole follows this particular node,
- * query this function.
+ * query this function. See also drm_mm_hole_node_start() and
+ * drm_mm_hole_node_end().
  *
  * Returns:
  * True if a hole follows the @node.
@@ -228,23 +270,23 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 
 /**
  * drm_mm_for_each_node - iterator to walk over all allocated nodes
- * @entry: drm_mm_node structure to assign to in each iteration step
- * @mm: drm_mm allocator to walk
+ * @entry: &struct drm_mm_node to assign to in each iteration step
+ * @mm: &drm_mm allocator to walk
  *
  * This iterator walks over all nodes in the range allocator. It is implemented
- * with list_for_each, so not save against removal of elements.
+ * with list_for_each(), so not save against removal of elements.
  */
 #define drm_mm_for_each_node(entry, mm) \
 	list_for_each_entry(entry, drm_mm_nodes(mm), node_list)
 
 /**
  * drm_mm_for_each_node_safe - iterator to walk over all allocated nodes
- * @entry: drm_mm_node structure to assign to in each iteration step
- * @next: drm_mm_node structure to store the next step
- * @mm: drm_mm allocator to walk
+ * @entry: &struct drm_mm_node to assign to in each iteration step
+ * @next: &struct drm_mm_node to store the next step
+ * @mm: &drm_mm allocator to walk
  *
  * This iterator walks over all nodes in the range allocator. It is implemented
- * with list_for_each_safe, so save against removal of elements.
+ * with list_for_each_safe(), so save against removal of elements.
  */
 #define drm_mm_for_each_node_safe(entry, next, mm) \
 	list_for_each_entry_safe(entry, next, drm_mm_nodes(mm), node_list)
@@ -259,13 +301,13 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 
 /**
  * drm_mm_for_each_hole - iterator to walk over all holes
- * @entry: drm_mm_node used internally to track progress
- * @mm: drm_mm allocator to walk
+ * @entry: &drm_mm_node used internally to track progress
+ * @mm: &drm_mm allocator to walk
  * @hole_start: ulong variable to assign the hole start to on each iteration
  * @hole_end: ulong variable to assign the hole end to on each iteration
  *
  * This iterator walks over all holes in the range allocator. It is implemented
- * with list_for_each, so not save against removal of elements. @entry is used
+ * with list_for_each(), so not save against removal of elements. @entry is used
  * internally and will not reflect a real drm_mm_node for the very first hole.
  * Hence users of this iterator may not access it.
  *
@@ -334,6 +376,9 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
  * @sflags: flags to fine-tune the allocation search
  * @aflags: flags to fine-tune the allocation behavior
  *
+ * This is a simplified version of drm_mm_insert_node_in_range_generic() with no
+ * range restrictions applied.
+ *
  * The preallocated node must be cleared to 0.
  *
  * Returns:
@@ -434,6 +479,9 @@ void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
  * @color: opaque tag value to use for the allocation
  * @flags: flags to specify how the allocation will be performed afterwards
  *
+ * This is a simplified version of drm_mm_scan_init_with_range() with no range
+ * restrictions applied.
+ *
  * This simply sets up the scanning routines with the parameters for the desired
  * hole.
  *

commit b5c3714fe8789745521d8351d75049b9c6a0d26b
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Thu Dec 29 12:09:24 2016 +0100

    drm/mm: Convert to drm_printer
    
    Including all drivers. I thought about keeping small compat functions
    to avoid having to change all drivers. But I really like the
    drm_printer idea, so figured spreading it more widely is a good thing.
    
    v2: Review from Chris:
    - Natural argument order and better name for drm_mm_print.
    - show_mm() macro in the selftest.
    
    Cc: Rob Clark <robdclark@gmail.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Christian König <christian.koenig@amd.com>
    Cc: Lucas Stach <l.stach@pengutronix.de>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>
    Cc: Thierry Reding <thierry.reding@gmail.com>
    Cc: Jyri Sarha <jsarha@ti.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/1483009764-8281-1-git-send-email-daniel.vetter@ffwll.ch

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 92ec5759caae..1383ac2328b8 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -42,12 +42,10 @@
 #include <linux/kernel.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
-#ifdef CONFIG_DEBUG_FS
-#include <linux/seq_file.h>
-#endif
 #ifdef CONFIG_DRM_DEBUG_MM
 #include <linux/stackdepot.h>
 #endif
+#include <drm/drm_print.h>
 
 #ifdef CONFIG_DRM_DEBUG_MM
 #define DRM_MM_BUG_ON(expr) BUG_ON(expr)
@@ -462,9 +460,6 @@ bool drm_mm_scan_remove_block(struct drm_mm_scan *scan,
 			      struct drm_mm_node *node);
 struct drm_mm_node *drm_mm_scan_color_evict(struct drm_mm_scan *scan);
 
-void drm_mm_debug_table(const struct drm_mm *mm, const char *prefix);
-#ifdef CONFIG_DEBUG_FS
-int drm_mm_dump_table(struct seq_file *m, const struct drm_mm *mm);
-#endif
+void drm_mm_print(const struct drm_mm *mm, struct drm_printer *p);
 
 #endif

commit adb040b86bc290d3d8a339ad8e91c96a9f506095
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:39 2016 +0000

    drm: Use drm_mm_insert_node_in_range_generic() for everyone
    
    Remove a superfluous helper as drm_mm_insert_node is equivalent to
    insert_node_in_range with a range of [0, U64_MAX].
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-37-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 7da7a171d6d5..92ec5759caae 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -285,40 +285,6 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  * Basic range manager support (drm_mm.c)
  */
 int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
-
-int drm_mm_insert_node_generic(struct drm_mm *mm,
-			       struct drm_mm_node *node,
-			       u64 size,
-			       u64 alignment,
-			       unsigned long color,
-			       enum drm_mm_search_flags sflags,
-			       enum drm_mm_allocator_flags aflags);
-/**
- * drm_mm_insert_node - search for space and insert @node
- * @mm: drm_mm to allocate from
- * @node: preallocate node to insert
- * @size: size of the allocation
- * @alignment: alignment of the allocation
- * @flags: flags to fine-tune the allocation
- *
- * This is a simplified version of drm_mm_insert_node_generic() with @color set
- * to 0.
- *
- * The preallocated node must be cleared to 0.
- *
- * Returns:
- * 0 on success, -ENOSPC if there's no suitable hole.
- */
-static inline int drm_mm_insert_node(struct drm_mm *mm,
-				     struct drm_mm_node *node,
-				     u64 size,
-				     u64 alignment,
-				     enum drm_mm_search_flags flags)
-{
-	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags,
-					  DRM_MM_CREATE_DEFAULT);
-}
-
 int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 					struct drm_mm_node *node,
 					u64 size,
@@ -328,6 +294,7 @@ int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 					u64 end,
 					enum drm_mm_search_flags sflags,
 					enum drm_mm_allocator_flags aflags);
+
 /**
  * drm_mm_insert_node_in_range - ranged search for space and insert @node
  * @mm: drm_mm to allocate from
@@ -359,6 +326,61 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 						   DRM_MM_CREATE_DEFAULT);
 }
 
+/**
+ * drm_mm_insert_node_generic - search for space and insert @node
+ * @mm: drm_mm to allocate from
+ * @node: preallocate node to insert
+ * @size: size of the allocation
+ * @alignment: alignment of the allocation
+ * @color: opaque tag value to use for this node
+ * @sflags: flags to fine-tune the allocation search
+ * @aflags: flags to fine-tune the allocation behavior
+ *
+ * The preallocated node must be cleared to 0.
+ *
+ * Returns:
+ * 0 on success, -ENOSPC if there's no suitable hole.
+ */
+static inline int
+drm_mm_insert_node_generic(struct drm_mm *mm, struct drm_mm_node *node,
+			   u64 size, u64 alignment,
+			   unsigned long color,
+			   enum drm_mm_search_flags sflags,
+			   enum drm_mm_allocator_flags aflags)
+{
+	return drm_mm_insert_node_in_range_generic(mm, node,
+						   size, alignment, 0,
+						   0, U64_MAX,
+						   sflags, aflags);
+}
+
+/**
+ * drm_mm_insert_node - search for space and insert @node
+ * @mm: drm_mm to allocate from
+ * @node: preallocate node to insert
+ * @size: size of the allocation
+ * @alignment: alignment of the allocation
+ * @flags: flags to fine-tune the allocation
+ *
+ * This is a simplified version of drm_mm_insert_node_generic() with @color set
+ * to 0.
+ *
+ * The preallocated node must be cleared to 0.
+ *
+ * Returns:
+ * 0 on success, -ENOSPC if there's no suitable hole.
+ */
+static inline int drm_mm_insert_node(struct drm_mm *mm,
+				     struct drm_mm_node *node,
+				     u64 size,
+				     u64 alignment,
+				     enum drm_mm_search_flags flags)
+{
+	return drm_mm_insert_node_generic(mm, node,
+					  size, alignment, 0,
+					  flags, DRM_MM_CREATE_DEFAULT);
+}
+
 void drm_mm_remove_node(struct drm_mm_node *node);
 void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
 void drm_mm_init(struct drm_mm *mm, u64 start, u64 size);

commit 3f85fb3462dc1c87a9353eb38714468d46248b2e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:37 2016 +0000

    drm: Wrap drm_mm_node.hole_follows
    
    Insulate users from changes to the internal hole tracking within
    struct drm_mm_node by using an accessor for hole_follows.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    [danvet: resolve conflicts in i915_vma.c]
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index aed93cbc4bde..7da7a171d6d5 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -155,6 +155,22 @@ static inline bool drm_mm_initialized(const struct drm_mm *mm)
 	return mm->hole_stack.next;
 }
 
+/**
+ * drm_mm_hole_follows - checks whether a hole follows this node
+ * @node: drm_mm_node to check
+ *
+ * Holes are embedded into the drm_mm using the tail of a drm_mm_node.
+ * If you wish to know whether a hole follows this particular node,
+ * query this function.
+ *
+ * Returns:
+ * True if a hole follows the @node.
+ */
+static inline bool drm_mm_hole_follows(const struct drm_mm_node *node)
+{
+	return node->hole_follows;
+}
+
 static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
 {
 	return hole_node->start + hole_node->size;
@@ -166,14 +182,14 @@ static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
  *
  * This is useful for driver-specific debug dumpers. Otherwise drivers should
  * not inspect holes themselves. Drivers must check first whether a hole indeed
- * follows by looking at node->hole_follows.
+ * follows by looking at drm_mm_hole_follows()
  *
  * Returns:
  * Start of the subsequent hole.
  */
 static inline u64 drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
 {
-	DRM_MM_BUG_ON(!hole_node->hole_follows);
+	DRM_MM_BUG_ON(!drm_mm_hole_follows(hole_node));
 	return __drm_mm_hole_node_start(hole_node);
 }
 
@@ -188,7 +204,7 @@ static inline u64 __drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  *
  * This is useful for driver-specific debug dumpers. Otherwise drivers should
  * not inspect holes themselves. Drivers must check first whether a hole indeed
- * follows by looking at node->hole_follows.
+ * follows by looking at drm_mm_hole_follows().
  *
  * Returns:
  * End of the subsequent hole.

commit 3fa489dabea9a1cb0656e2da90354f7c4e53f890
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:36 2016 +0000

    drm: Apply tight eviction scanning to color_adjust
    
    Using mm->color_adjust makes the eviction scanner much tricker since we
    don't know the actual neighbours of the target hole until after it is
    created (after scanning is complete). To work out whether we need to
    evict the neighbours because they impact upon the hole, we have to then
    check the hole afterwards - requiring an extra step in the user of the
    eviction scanner when they apply color_adjust.
    
    v2: Massage kerneldoc.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-34-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index ff120b7d0f85..aed93cbc4bde 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -422,6 +422,7 @@ bool drm_mm_scan_add_block(struct drm_mm_scan *scan,
 			   struct drm_mm_node *node);
 bool drm_mm_scan_remove_block(struct drm_mm_scan *scan,
 			      struct drm_mm_node *node);
+struct drm_mm_node *drm_mm_scan_color_evict(struct drm_mm_scan *scan);
 
 void drm_mm_debug_table(const struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS

commit f29051f12f33078b81ac710bee9ebd7a85ee1849
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:35 2016 +0000

    drm: Simplify drm_mm scan-list manipulation
    
    Since we mandate a strict reverse-order of drm_mm_scan_remove_block()
    after drm_mm_scan_add_block() we can further simplify the list
    manipulations when generating the temporary scan-hole.
    
    v2: Highlight the games being played with the lists to track the scan
    holes without allocation.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-33-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index d6701d56ea74..ff120b7d0f85 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -74,11 +74,8 @@ struct drm_mm_node {
 	struct list_head hole_stack;
 	struct rb_node rb;
 	unsigned hole_follows : 1;
-	unsigned scanned_block : 1;
-	unsigned scanned_prev_free : 1;
-	unsigned scanned_next_free : 1;
-	unsigned scanned_preceeds_hole : 1;
 	unsigned allocated : 1;
+	bool scanned_block : 1;
 	unsigned long color;
 	u64 start;
 	u64 size;
@@ -118,8 +115,6 @@ struct drm_mm_scan {
 	u64 hit_start;
 	u64 hit_end;
 
-	struct drm_mm_node *prev_scanned_node;
-
 	unsigned long color;
 	unsigned int flags;
 };

commit 9a956b1548794033d5b893d6d6fcc00b197cfc9b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:34 2016 +0000

    drm: Optimise power-of-two alignments in drm_mm_scan_add_block()
    
    For power-of-two alignments, we can avoid the 64bit divide and do a
    simple bitwise add instead.
    
    v2: s/alignment_mask/remainder_mask/
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-32-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 606336fc229a..d6701d56ea74 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -110,6 +110,7 @@ struct drm_mm_scan {
 
 	u64 size;
 	u64 alignment;
+	u64 remainder_mask;
 
 	u64 range_start;
 	u64 range_end;

commit 0b04d474a611e2831d142e246422a03a10998ae1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:33 2016 +0000

    drm: Compute tight evictions for drm_mm_scan
    
    Compute the minimal required hole during scan and only evict those nodes
    that overlap. This enables us to reduce the number of nodes we need to
    evict to the bare minimum.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-31-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index bae0f10da8e3..606336fc229a 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -120,6 +120,7 @@ struct drm_mm_scan {
 	struct drm_mm_node *prev_scanned_node;
 
 	unsigned long color;
+	unsigned int flags;
 };
 
 /**
@@ -388,11 +389,9 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 
 void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
 				 struct drm_mm *mm,
-				 u64 size,
-				 u64 alignment,
-				 unsigned long color,
-				 u64 start,
-				 u64 end);
+				 u64 size, u64 alignment, unsigned long color,
+				 u64 start, u64 end,
+				 unsigned int flags);
 
 /**
  * drm_mm_scan_init - initialize lru scanning
@@ -401,10 +400,10 @@ void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
  * @size: size of the allocation
  * @alignment: alignment of the allocation
  * @color: opaque tag value to use for the allocation
+ * @flags: flags to specify how the allocation will be performed afterwards
  *
  * This simply sets up the scanning routines with the parameters for the desired
- * hole. Note that there's no need to specify allocation flags, since they only
- * change the place a node is allocated from within a suitable hole.
+ * hole.
  *
  * Warning:
  * As long as the scan list is non-empty, no other operations than
@@ -414,10 +413,13 @@ static inline void drm_mm_scan_init(struct drm_mm_scan *scan,
 				    struct drm_mm *mm,
 				    u64 size,
 				    u64 alignment,
-				    unsigned long color)
+				    unsigned long color,
+				    unsigned int flags)
 {
-	drm_mm_scan_init_with_range(scan, mm, size, alignment, color,
-				    0, U64_MAX);
+	drm_mm_scan_init_with_range(scan, mm,
+				    size, alignment, color,
+				    0, U64_MAX,
+				    flags);
 }
 
 bool drm_mm_scan_add_block(struct drm_mm_scan *scan,

commit 2c4b389518fbe552188928aadcd3815d5116a05c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:31 2016 +0000

    drm: Unconditionally do the range check in drm_mm_scan_add_block()
    
    Doing the check is trivial (low cost in comparison to overall eviction)
    and helps simplify the code.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-29-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index fcad718c5fb4..bae0f10da8e3 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -120,7 +120,6 @@ struct drm_mm_scan {
 	struct drm_mm_node *prev_scanned_node;
 
 	unsigned long color;
-	bool check_range : 1;
 };
 
 /**
@@ -387,11 +386,6 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 	     node__ && node__->start < (end__);				\
 	     node__ = list_next_entry(node__, node_list))
 
-void drm_mm_scan_init(struct drm_mm_scan *scan,
-		      struct drm_mm *mm,
-		      u64 size,
-		      u64 alignment,
-		      unsigned long color);
 void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
 				 struct drm_mm *mm,
 				 u64 size,
@@ -399,6 +393,33 @@ void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
 				 unsigned long color,
 				 u64 start,
 				 u64 end);
+
+/**
+ * drm_mm_scan_init - initialize lru scanning
+ * @scan: scan state
+ * @mm: drm_mm to scan
+ * @size: size of the allocation
+ * @alignment: alignment of the allocation
+ * @color: opaque tag value to use for the allocation
+ *
+ * This simply sets up the scanning routines with the parameters for the desired
+ * hole. Note that there's no need to specify allocation flags, since they only
+ * change the place a node is allocated from within a suitable hole.
+ *
+ * Warning:
+ * As long as the scan list is non-empty, no other operations than
+ * adding/removing nodes to/from the scan list are allowed.
+ */
+static inline void drm_mm_scan_init(struct drm_mm_scan *scan,
+				    struct drm_mm *mm,
+				    u64 size,
+				    u64 alignment,
+				    unsigned long color)
+{
+	drm_mm_scan_init_with_range(scan, mm, size, alignment, color,
+				    0, U64_MAX);
+}
+
 bool drm_mm_scan_add_block(struct drm_mm_scan *scan,
 			   struct drm_mm_node *node);
 bool drm_mm_scan_remove_block(struct drm_mm_scan *scan,

commit 9a71e277888b39b8f0e8364813ec1ba58a5a4371
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:29 2016 +0000

    drm: Extract struct drm_mm_scan from struct drm_mm
    
    The scan state occupies a large proportion of the struct drm_mm and is
    rarely used and only contains temporary state. That makes it suitable to
    moving to its struct and onto the stack of the callers.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    [danvet: Fix up etnaviv to compile, was missing a BUG_ON.]
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 72e0c0ddf8d0..fcad718c5fb4 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -98,20 +98,29 @@ struct drm_mm {
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
 	struct rb_root interval_tree;
 
-	unsigned int scan_check_range : 1;
-	unsigned int scanned_blocks;
-	unsigned long scan_color;
-	u64 scan_alignment;
-	u64 scan_size;
-	u64 scan_hit_start;
-	u64 scan_hit_end;
-	u64 scan_start;
-	u64 scan_end;
-	struct drm_mm_node *prev_scanned_node;
-
 	void (*color_adjust)(const struct drm_mm_node *node,
 			     unsigned long color,
 			     u64 *start, u64 *end);
+
+	unsigned long scan_active;
+};
+
+struct drm_mm_scan {
+	struct drm_mm *mm;
+
+	u64 size;
+	u64 alignment;
+
+	u64 range_start;
+	u64 range_end;
+
+	u64 hit_start;
+	u64 hit_end;
+
+	struct drm_mm_node *prev_scanned_node;
+
+	unsigned long color;
+	bool check_range : 1;
 };
 
 /**
@@ -378,18 +387,22 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 	     node__ && node__->start < (end__);				\
 	     node__ = list_next_entry(node__, node_list))
 
-void drm_mm_init_scan(struct drm_mm *mm,
+void drm_mm_scan_init(struct drm_mm_scan *scan,
+		      struct drm_mm *mm,
 		      u64 size,
 		      u64 alignment,
 		      unsigned long color);
-void drm_mm_init_scan_with_range(struct drm_mm *mm,
+void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
+				 struct drm_mm *mm,
 				 u64 size,
 				 u64 alignment,
 				 unsigned long color,
 				 u64 start,
 				 u64 end);
-bool drm_mm_scan_add_block(struct drm_mm_node *node);
-bool drm_mm_scan_remove_block(struct drm_mm_node *node);
+bool drm_mm_scan_add_block(struct drm_mm_scan *scan,
+			   struct drm_mm_node *node);
+bool drm_mm_scan_remove_block(struct drm_mm_scan *scan,
+			      struct drm_mm_node *node);
 
 void drm_mm_debug_table(const struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS

commit ac9bb7b7d33f6be9e333b24786a774145cc8c59e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:27 2016 +0000

    drm: Simplify drm_mm_clean()
    
    Since commit ea7b1dd44867 ("drm: mm: track free areas implicitly"),
    to test whether there are any nodes allocated within the range manager,
    we merely have to ask whether the node_list is empty.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-25-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 7eeb98b5bf70..72e0c0ddf8d0 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -342,7 +342,19 @@ void drm_mm_remove_node(struct drm_mm_node *node);
 void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
 void drm_mm_init(struct drm_mm *mm, u64 start, u64 size);
 void drm_mm_takedown(struct drm_mm *mm);
-bool drm_mm_clean(const struct drm_mm *mm);
+
+/**
+ * drm_mm_clean - checks whether an allocator is clean
+ * @mm: drm_mm allocator to check
+ *
+ * Returns:
+ * True if the allocator is completely free, false if there's still a node
+ * allocated in it.
+ */
+static inline bool drm_mm_clean(const struct drm_mm *mm)
+{
+	return list_empty(drm_mm_nodes(mm));
+}
 
 struct drm_mm_node *
 __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);

commit ba004e39b1997bb32150961eef4a5cd02ba3edec
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:25 2016 +0000

    drm: Fix kerneldoc for drm_mm_scan_remove_block()
    
    The nodes must be removed in the *reverse* order. This is correct in the
    overview, but backwards in the function description. Whilst here add
    Intel's copyright statement and tweak some formatting.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-23-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0ce8c3678c11..7eeb98b5bf70 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -1,6 +1,7 @@
 /**************************************************************************
  *
  * Copyright 2006-2008 Tungsten Graphics, Inc., Cedar Park, TX. USA.
+ * Copyright 2016 Intel Corporation
  * All Rights Reserved.
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
@@ -117,7 +118,10 @@ struct drm_mm {
  * drm_mm_node_allocated - checks whether a node is allocated
  * @node: drm_mm_node to check
  *
- * Drivers should use this helpers for proper encapusulation of drm_mm
+ * Drivers are required to clear a node prior to using it with the
+ * drm_mm range manager.
+ *
+ * Drivers should use this helper for proper encapsulation of drm_mm
  * internals.
  *
  * Returns:
@@ -132,7 +136,10 @@ static inline bool drm_mm_node_allocated(const struct drm_mm_node *node)
  * drm_mm_initialized - checks whether an allocator is initialized
  * @mm: drm_mm to check
  *
- * Drivers should use this helpers for proper encapusulation of drm_mm
+ * Drivers should clear the struct drm_mm prior to initialisation if they
+ * want to use this function.
+ *
+ * Drivers should use this helper for proper encapsulation of drm_mm
  * internals.
  *
  * Returns:
@@ -152,8 +159,8 @@ static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
  * drm_mm_hole_node_start - computes the start of the hole following @node
  * @hole_node: drm_mm_node which implicitly tracks the following hole
  *
- * This is useful for driver-sepific debug dumpers. Otherwise drivers should not
- * inspect holes themselves. Drivers must check first whether a hole indeed
+ * This is useful for driver-specific debug dumpers. Otherwise drivers should
+ * not inspect holes themselves. Drivers must check first whether a hole indeed
  * follows by looking at node->hole_follows.
  *
  * Returns:
@@ -174,8 +181,8 @@ static inline u64 __drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  * drm_mm_hole_node_end - computes the end of the hole following @node
  * @hole_node: drm_mm_node which implicitly tracks the following hole
  *
- * This is useful for driver-sepific debug dumpers. Otherwise drivers should not
- * inspect holes themselves. Drivers must check first whether a hole indeed
+ * This is useful for driver-specific debug dumpers. Otherwise drivers should
+ * not inspect holes themselves. Drivers must check first whether a hole indeed
  * follows by looking at node->hole_follows.
  *
  * Returns:

commit 7173320758e5dfcd7a47a51d3fe2b21c43d9633c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:24 2016 +0000

    drm: Promote drm_mm alignment to u64
    
    In places (e.g. i915.ko), the alignment is exported to userspace as u64
    and there now exists hardware for which we can indeed utilize a u64
    alignment. As such, we need to keep 64bit integers throughout when
    handling alignment.
    
    Testcase: igt/drm_mm/align64
    Testcase: igt/gem_exec_alignment
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Christian König <christian.koenig@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-22-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 525543019896..0ce8c3678c11 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -98,12 +98,12 @@ struct drm_mm {
 	struct rb_root interval_tree;
 
 	unsigned int scan_check_range : 1;
-	unsigned scan_alignment;
+	unsigned int scanned_blocks;
 	unsigned long scan_color;
+	u64 scan_alignment;
 	u64 scan_size;
 	u64 scan_hit_start;
 	u64 scan_hit_end;
-	unsigned scanned_blocks;
 	u64 scan_start;
 	u64 scan_end;
 	struct drm_mm_node *prev_scanned_node;
@@ -261,7 +261,7 @@ int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
 int drm_mm_insert_node_generic(struct drm_mm *mm,
 			       struct drm_mm_node *node,
 			       u64 size,
-			       unsigned alignment,
+			       u64 alignment,
 			       unsigned long color,
 			       enum drm_mm_search_flags sflags,
 			       enum drm_mm_allocator_flags aflags);
@@ -284,7 +284,7 @@ int drm_mm_insert_node_generic(struct drm_mm *mm,
 static inline int drm_mm_insert_node(struct drm_mm *mm,
 				     struct drm_mm_node *node,
 				     u64 size,
-				     unsigned alignment,
+				     u64 alignment,
 				     enum drm_mm_search_flags flags)
 {
 	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags,
@@ -294,7 +294,7 @@ static inline int drm_mm_insert_node(struct drm_mm *mm,
 int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 					struct drm_mm_node *node,
 					u64 size,
-					unsigned alignment,
+					u64 alignment,
 					unsigned long color,
 					u64 start,
 					u64 end,
@@ -321,7 +321,7 @@ int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 					      struct drm_mm_node *node,
 					      u64 size,
-					      unsigned alignment,
+					      u64 alignment,
 					      u64 start,
 					      u64 end,
 					      enum drm_mm_search_flags flags)
@@ -361,11 +361,11 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 
 void drm_mm_init_scan(struct drm_mm *mm,
 		      u64 size,
-		      unsigned alignment,
+		      u64 alignment,
 		      unsigned long color);
 void drm_mm_init_scan_with_range(struct drm_mm *mm,
 				 u64 size,
-				 unsigned alignment,
+				 u64 alignment,
 				 unsigned long color,
 				 u64 start,
 				 u64 end);

commit b3ee963fe41d0034cf8b6aff1f0cc9c91bf8d478
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:06 2016 +0000

    drm: Compile time enabling for asserts in drm_mm
    
    Use CONFIG_DRM_DEBUG_MM to conditionally enable the internal and
    validation checking using BUG_ON. Ideally these paths should all be
    exercised by CI selftests (with the asserts enabled).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-4-chris@chris-wilson.co.uk
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-4-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index f6a68ed5ecaf..525543019896 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -48,6 +48,12 @@
 #include <linux/stackdepot.h>
 #endif
 
+#ifdef CONFIG_DRM_DEBUG_MM
+#define DRM_MM_BUG_ON(expr) BUG_ON(expr)
+#else
+#define DRM_MM_BUG_ON(expr) BUILD_BUG_ON_INVALID(expr)
+#endif
+
 enum drm_mm_search_flags {
 	DRM_MM_SEARCH_DEFAULT =		0,
 	DRM_MM_SEARCH_BEST =		1 << 0,
@@ -155,7 +161,7 @@ static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
  */
 static inline u64 drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
 {
-	BUG_ON(!hole_node->hole_follows);
+	DRM_MM_BUG_ON(!hole_node->hole_follows);
 	return __drm_mm_hole_node_start(hole_node);
 }
 

commit 2bc98c86517b08304b5008f427b751c08659b100
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:05 2016 +0000

    drm: Use drm_mm_nodes() as shorthand for the list of nodes under struct drm_mm
    
    Fairly commonly we want to inspect the node list on the struct drm_mm,
    which is buried within an embedded node. Bring it to the surface with a
    bit of syntatic sugar.
    
    Note this was intended to be split from commit ad579002c8ec ("drm: Add
    drm_mm_for_each_node_safe()") before being applied, but my timing sucks.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-3-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 5c7f15875b6a..f6a68ed5ecaf 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -180,7 +180,19 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 	return __drm_mm_hole_node_end(hole_node);
 }
 
-#define __drm_mm_nodes(mm) (&(mm)->head_node.node_list)
+/**
+ * drm_mm_nodes - list of nodes under the drm_mm range manager
+ * @mm: the struct drm_mm range manger
+ *
+ * As the drm_mm range manager hides its node_list deep with its
+ * structure, extracting it looks painful and repetitive. This is
+ * not expected to be used outside of the drm_mm_for_each_node()
+ * macros and similar internal functions.
+ *
+ * Returns:
+ * The node list, may be empty.
+ */
+#define drm_mm_nodes(mm) (&(mm)->head_node.node_list)
 
 /**
  * drm_mm_for_each_node - iterator to walk over all allocated nodes
@@ -191,7 +203,7 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  * with list_for_each, so not save against removal of elements.
  */
 #define drm_mm_for_each_node(entry, mm) \
-	list_for_each_entry(entry, __drm_mm_nodes(mm), node_list)
+	list_for_each_entry(entry, drm_mm_nodes(mm), node_list)
 
 /**
  * drm_mm_for_each_node_safe - iterator to walk over all allocated nodes
@@ -203,7 +215,7 @@ static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
  * with list_for_each_safe, so save against removal of elements.
  */
 #define drm_mm_for_each_node_safe(entry, next, mm) \
-	list_for_each_entry_safe(entry, next, __drm_mm_nodes(mm), node_list)
+	list_for_each_entry_safe(entry, next, drm_mm_nodes(mm), node_list)
 
 #define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
 	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \

commit 45b186f111f1623b257d183920cd4aab16a1acd5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Dec 16 07:46:42 2016 +0000

    drm: Constify the drm_mm API
    
    Mark up the pointers as constant through the API where appropriate.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161216074718.32500-5-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0cc1b78c9ec2..5c7f15875b6a 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -102,7 +102,8 @@ struct drm_mm {
 	u64 scan_end;
 	struct drm_mm_node *prev_scanned_node;
 
-	void (*color_adjust)(struct drm_mm_node *node, unsigned long color,
+	void (*color_adjust)(const struct drm_mm_node *node,
+			     unsigned long color,
 			     u64 *start, u64 *end);
 };
 
@@ -116,7 +117,7 @@ struct drm_mm {
  * Returns:
  * True if the @node is allocated.
  */
-static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
+static inline bool drm_mm_node_allocated(const struct drm_mm_node *node)
 {
 	return node->allocated;
 }
@@ -131,12 +132,12 @@ static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
  * Returns:
  * True if the @mm is initialized.
  */
-static inline bool drm_mm_initialized(struct drm_mm *mm)
+static inline bool drm_mm_initialized(const struct drm_mm *mm)
 {
 	return mm->hole_stack.next;
 }
 
-static inline u64 __drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+static inline u64 __drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
 {
 	return hole_node->start + hole_node->size;
 }
@@ -152,13 +153,13 @@ static inline u64 __drm_mm_hole_node_start(struct drm_mm_node *hole_node)
  * Returns:
  * Start of the subsequent hole.
  */
-static inline u64 drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+static inline u64 drm_mm_hole_node_start(const struct drm_mm_node *hole_node)
 {
 	BUG_ON(!hole_node->hole_follows);
 	return __drm_mm_hole_node_start(hole_node);
 }
 
-static inline u64 __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+static inline u64 __drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 {
 	return list_next_entry(hole_node, node_list)->start;
 }
@@ -174,7 +175,7 @@ static inline u64 __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
  * Returns:
  * End of the subsequent hole.
  */
-static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+static inline u64 drm_mm_hole_node_end(const struct drm_mm_node *hole_node)
 {
 	return __drm_mm_hole_node_end(hole_node);
 }
@@ -314,14 +315,12 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 
 void drm_mm_remove_node(struct drm_mm_node *node);
 void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
-void drm_mm_init(struct drm_mm *mm,
-		 u64 start,
-		 u64 size);
+void drm_mm_init(struct drm_mm *mm, u64 start, u64 size);
 void drm_mm_takedown(struct drm_mm *mm);
-bool drm_mm_clean(struct drm_mm *mm);
+bool drm_mm_clean(const struct drm_mm *mm);
 
 struct drm_mm_node *
-__drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
+__drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 
 /**
  * drm_mm_for_each_node_in_range - iterator to walk over a range of
@@ -355,9 +354,9 @@ void drm_mm_init_scan_with_range(struct drm_mm *mm,
 bool drm_mm_scan_add_block(struct drm_mm_node *node);
 bool drm_mm_scan_remove_block(struct drm_mm_node *node);
 
-void drm_mm_debug_table(struct drm_mm *mm, const char *prefix);
+void drm_mm_debug_table(const struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS
-int drm_mm_dump_table(struct seq_file *m, struct drm_mm *mm);
+int drm_mm_dump_table(struct seq_file *m, const struct drm_mm *mm);
 #endif
 
 #endif

commit ad579002c8ec429930721c5bb8bd763e6c0c6286
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Dec 16 07:46:41 2016 +0000

    drm: Add drm_mm_for_each_node_safe()
    
    A complement to drm_mm_for_each_node(), wraps list_for_each_entry_safe()
    for walking the list of nodes safe against removal.
    
    Note from Joonas:
    
    "Most of the diff is about __drm_mm_nodes(mm), which could be split into
    own patch and keep the R-b's."
    
    But I don't feel like insisting on the resend.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    [danvet: Add note.]
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161216074718.32500-4-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0b8371795aeb..0cc1b78c9ec2 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -179,6 +179,8 @@ static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 	return __drm_mm_hole_node_end(hole_node);
 }
 
+#define __drm_mm_nodes(mm) (&(mm)->head_node.node_list)
+
 /**
  * drm_mm_for_each_node - iterator to walk over all allocated nodes
  * @entry: drm_mm_node structure to assign to in each iteration step
@@ -187,9 +189,20 @@ static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
  * This iterator walks over all nodes in the range allocator. It is implemented
  * with list_for_each, so not save against removal of elements.
  */
-#define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
-						&(mm)->head_node.node_list, \
-						node_list)
+#define drm_mm_for_each_node(entry, mm) \
+	list_for_each_entry(entry, __drm_mm_nodes(mm), node_list)
+
+/**
+ * drm_mm_for_each_node_safe - iterator to walk over all allocated nodes
+ * @entry: drm_mm_node structure to assign to in each iteration step
+ * @next: drm_mm_node structure to store the next step
+ * @mm: drm_mm allocator to walk
+ *
+ * This iterator walks over all nodes in the range allocator. It is implemented
+ * with list_for_each_safe, so save against removal of elements.
+ */
+#define drm_mm_for_each_node_safe(entry, next, mm) \
+	list_for_each_entry_safe(entry, next, __drm_mm_nodes(mm), node_list)
 
 #define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
 	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \

commit 8b2fb7b6518d143b382c3490d4a90f8676259ef9
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Nov 27 11:16:23 2016 +0000

    drm: Fix conflicting macro parameter in drm_mm_for_each_node_in_range()
    
    start is being used as both a macro parameter and as a member of struct
    drm_mm_node (node->start). This causes a conflict as cpp then tries to
    replace node->start with the passed in string for "start". Work just
    fine so long as you also happened to using local variables called start!
    
    Fixes: 522e85dd8677 ("drm: Define drm_mm_for_each_node_in_range()")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>.
    [danvet: Fixup kerneldoc.]
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161127111623.11124-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 6add455c651b..0b8371795aeb 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -313,10 +313,10 @@ __drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
 /**
  * drm_mm_for_each_node_in_range - iterator to walk over a range of
  * allocated nodes
- * @node: drm_mm_node structure to assign to in each iteration step
- * @mm: drm_mm allocator to walk
- * @start: starting offset, the first node will overlap this
- * @end: ending offset, the last node will start before this (but may overlap)
+ * @node__: drm_mm_node structure to assign to in each iteration step
+ * @mm__: drm_mm allocator to walk
+ * @start__: starting offset, the first node will overlap this
+ * @end__: ending offset, the last node will start before this (but may overlap)
  *
  * This iterator walks over all nodes in the range allocator that lie
  * between @start and @end. It is implemented similarly to list_for_each(),
@@ -324,10 +324,10 @@ __drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
  * starting node, and so not safe against removal of elements. It assumes
  * that @end is within (or is the upper limit of) the drm_mm allocator.
  */
-#define drm_mm_for_each_node_in_range(node, mm, start, end)		\
-	for (node = __drm_mm_interval_first((mm), (start), (end)-1);	\
-	     node && node->start < (end);				\
-	     node = list_next_entry(node, node_list))			\
+#define drm_mm_for_each_node_in_range(node__, mm__, start__, end__)	\
+	for (node__ = __drm_mm_interval_first((mm__), (start__), (end__)-1); \
+	     node__ && node__->start < (end__);				\
+	     node__ = list_next_entry(node__, node_list))
 
 void drm_mm_init_scan(struct drm_mm *mm,
 		      u64 size,

commit 522e85dd8677e9cca40c3ae773f171e6a9eece31
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 23 14:11:14 2016 +0000

    drm: Define drm_mm_for_each_node_in_range()
    
    Some clients would like to iterate over every node within a certain
    range. Make a nice little macro for them to hide the mixing of the
    rbtree search and linear walk.
    
    v2: Blurb
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: dri-devel@lists.freedesktop.org
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161123141118.23876-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 41ddafe92b2f..6add455c651b 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -308,10 +308,26 @@ void drm_mm_takedown(struct drm_mm *mm);
 bool drm_mm_clean(struct drm_mm *mm);
 
 struct drm_mm_node *
-drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
+__drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
 
-struct drm_mm_node *
-drm_mm_interval_next(struct drm_mm_node *node, u64 start, u64 last);
+/**
+ * drm_mm_for_each_node_in_range - iterator to walk over a range of
+ * allocated nodes
+ * @node: drm_mm_node structure to assign to in each iteration step
+ * @mm: drm_mm allocator to walk
+ * @start: starting offset, the first node will overlap this
+ * @end: ending offset, the last node will start before this (but may overlap)
+ *
+ * This iterator walks over all nodes in the range allocator that lie
+ * between @start and @end. It is implemented similarly to list_for_each(),
+ * but using the internal interval tree to accelerate the search for the
+ * starting node, and so not safe against removal of elements. It assumes
+ * that @end is within (or is the upper limit of) the drm_mm allocator.
+ */
+#define drm_mm_for_each_node_in_range(node, mm, start, end)		\
+	for (node = __drm_mm_interval_first((mm), (start), (end)-1);	\
+	     node && node->start < (end);				\
+	     node = list_next_entry(node, node_list))			\
 
 void drm_mm_init_scan(struct drm_mm *mm,
 		      u64 size,

commit 5705670d0463423337c82d00877989e7df8b169d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Oct 31 09:08:06 2016 +0000

    drm: Track drm_mm allocators and show leaks on shutdown
    
    We can use the kernel's stack tracer and depot to record the allocation
    site of every drm_mm user. Then on shutdown, as well as warning that
    allocated nodes still reside with the drm_mm range manager, we can
    display who allocated them to aide tracking down the leak.
    
    v2: Move Kconfig around so it lies underneath the DRM options submenu.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161031090806.20073-1-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 205ddcf6d55d..41ddafe92b2f 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -44,6 +44,9 @@
 #ifdef CONFIG_DEBUG_FS
 #include <linux/seq_file.h>
 #endif
+#ifdef CONFIG_DRM_DEBUG_MM
+#include <linux/stackdepot.h>
+#endif
 
 enum drm_mm_search_flags {
 	DRM_MM_SEARCH_DEFAULT =		0,
@@ -74,6 +77,9 @@ struct drm_mm_node {
 	u64 size;
 	u64 __subtree_last;
 	struct drm_mm *mm;
+#ifdef CONFIG_DRM_DEBUG_MM
+	depot_stack_handle_t stack;
+#endif
 };
 
 struct drm_mm {

commit 202b52b7fbf70858609ec20829c7d69a13ffa351
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Aug 3 16:04:09 2016 +0100

    drm: Track drm_mm nodes with an interval tree
    
    In addition to the last-in/first-out stack for accessing drm_mm nodes,
    we occasionally and in the future often want to find a drm_mm_node by an
    address. To do so efficiently we need to track the nodes in an interval
    tree - lookups for a particular address will then be O(lg(N)), where N
    is the number of nodes in the range manager as opposed to O(N).
    Insertion however gains an extra O(lg(N)) step for all nodes
    irrespective of whether the interval tree is in use. For future i915
    patches, eliminating the linear walk is a significant improvement.
    
    v2: Use generic interval-tree template for u64 and faster insertion.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: David Herrmann <dh.herrmann@gmail.com>
    Cc: dri-devel@lists.freedesktop.org
    Reviewed-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/1470236651-678-1-git-send-email-chris@chris-wilson.co.uk

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index fc65118e5077..205ddcf6d55d 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -37,6 +37,7 @@
  * Generic range manager structs
  */
 #include <linux/bug.h>
+#include <linux/rbtree.h>
 #include <linux/kernel.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
@@ -61,6 +62,7 @@ enum drm_mm_allocator_flags {
 struct drm_mm_node {
 	struct list_head node_list;
 	struct list_head hole_stack;
+	struct rb_node rb;
 	unsigned hole_follows : 1;
 	unsigned scanned_block : 1;
 	unsigned scanned_prev_free : 1;
@@ -70,6 +72,7 @@ struct drm_mm_node {
 	unsigned long color;
 	u64 start;
 	u64 size;
+	u64 __subtree_last;
 	struct drm_mm *mm;
 };
 
@@ -79,6 +82,9 @@ struct drm_mm {
 	/* head_node.node_list is the list of all memory nodes, ordered
 	 * according to the (increasing) start address of the memory node. */
 	struct drm_mm_node head_node;
+	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
+	struct rb_root interval_tree;
+
 	unsigned int scan_check_range : 1;
 	unsigned scan_alignment;
 	unsigned long scan_color;
@@ -295,6 +301,12 @@ void drm_mm_init(struct drm_mm *mm,
 void drm_mm_takedown(struct drm_mm *mm);
 bool drm_mm_clean(struct drm_mm *mm);
 
+struct drm_mm_node *
+drm_mm_interval_first(struct drm_mm *mm, u64 start, u64 last);
+
+struct drm_mm_node *
+drm_mm_interval_next(struct drm_mm_node *node, u64 start, u64 last);
+
 void drm_mm_init_scan(struct drm_mm *mm,
 		      u64 size,
 		      unsigned alignment,

commit 87069f4493b2101a71a92b7b9565f488a605a88f
Author: Geliang Tang <geliangtang@163.com>
Date:   Wed Nov 25 21:23:07 2015 +0800

    drm/mm: use list_next_entry
    
    To make the intention clearer, use list_next_entry instead of list_entry.
    
    Signed-off-by: Geliang Tang <geliangtang@163.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index a58cc6c05cd6..fc65118e5077 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -148,8 +148,7 @@ static inline u64 drm_mm_hole_node_start(struct drm_mm_node *hole_node)
 
 static inline u64 __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 {
-	return list_entry(hole_node->node_list.next,
-			  struct drm_mm_node, node_list)->start;
+	return list_next_entry(hole_node, node_list)->start;
 }
 
 /**

commit 18b40c58a184e99f01f3efa9c86d89e1a537e42e
Author: Geliang Tang <geliangtang@163.com>
Date:   Sat Nov 21 22:04:04 2015 +0800

    drm/mm: rewrite drm_mm_for_each_hole
    
    When backwards is 0, __drm_mm_for_each_hole is same as
    drm_mm_for_each_hole. So I rewrite drm_mm_for_each_hole
    by using __drm_mm_for_each_hole.
    
    Signed-off-by: Geliang Tang <geliangtang@163.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0de6290df4da..a58cc6c05cd6 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -180,6 +180,14 @@ static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 						&(mm)->head_node.node_list, \
 						node_list)
 
+#define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
+	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
+	     &entry->hole_stack != &(mm)->hole_stack ? \
+	     hole_start = drm_mm_hole_node_start(entry), \
+	     hole_end = drm_mm_hole_node_end(entry), \
+	     1 : 0; \
+	     entry = list_entry((backwards) ? entry->hole_stack.prev : entry->hole_stack.next, struct drm_mm_node, hole_stack))
+
 /**
  * drm_mm_for_each_hole - iterator to walk over all holes
  * @entry: drm_mm_node used internally to track progress
@@ -200,20 +208,7 @@ static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
  * going backwards.
  */
 #define drm_mm_for_each_hole(entry, mm, hole_start, hole_end) \
-	for (entry = list_entry((mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
-	     &entry->hole_stack != &(mm)->hole_stack ? \
-	     hole_start = drm_mm_hole_node_start(entry), \
-	     hole_end = drm_mm_hole_node_end(entry), \
-	     1 : 0; \
-	     entry = list_entry(entry->hole_stack.next, struct drm_mm_node, hole_stack))
-
-#define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
-	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
-	     &entry->hole_stack != &(mm)->hole_stack ? \
-	     hole_start = drm_mm_hole_node_start(entry), \
-	     hole_end = drm_mm_hole_node_end(entry), \
-	     1 : 0; \
-	     entry = list_entry((backwards) ? entry->hole_stack.prev : entry->hole_stack.next, struct drm_mm_node, hole_stack))
+	__drm_mm_for_each_hole(entry, mm, hole_start, hole_end, 0)
 
 /*
  * Basic range manager support (drm_mm.c)

commit 440fd5283a87345cdd4237bdf45fb01130ea0056
Author: Thierry Reding <treding@nvidia.com>
Date:   Fri Jan 23 09:05:06 2015 +0100

    drm/mm: Support 4 GiB and larger ranges
    
    The current implementation is limited by the number of addresses that
    fit into an unsigned long. This causes problems on 32-bit Tegra where
    unsigned long is 32-bit but drm_mm is used to manage an IOVA space of
    4 GiB. Given the 32-bit limitation, the range is limited to 4 GiB - 1
    (or 4 GiB - 4 KiB for page granularity).
    
    This commit changes the start and size of the range to be an unsigned
    64-bit integer, thus allowing much larger ranges to be supported.
    
    [airlied: fix i915 warnings and coloring callback]
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    fixupo

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index a24addfdfcec..0de6290df4da 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -68,8 +68,8 @@ struct drm_mm_node {
 	unsigned scanned_preceeds_hole : 1;
 	unsigned allocated : 1;
 	unsigned long color;
-	unsigned long start;
-	unsigned long size;
+	u64 start;
+	u64 size;
 	struct drm_mm *mm;
 };
 
@@ -82,16 +82,16 @@ struct drm_mm {
 	unsigned int scan_check_range : 1;
 	unsigned scan_alignment;
 	unsigned long scan_color;
-	unsigned long scan_size;
-	unsigned long scan_hit_start;
-	unsigned long scan_hit_end;
+	u64 scan_size;
+	u64 scan_hit_start;
+	u64 scan_hit_end;
 	unsigned scanned_blocks;
-	unsigned long scan_start;
-	unsigned long scan_end;
+	u64 scan_start;
+	u64 scan_end;
 	struct drm_mm_node *prev_scanned_node;
 
 	void (*color_adjust)(struct drm_mm_node *node, unsigned long color,
-			     unsigned long *start, unsigned long *end);
+			     u64 *start, u64 *end);
 };
 
 /**
@@ -124,7 +124,7 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 	return mm->hole_stack.next;
 }
 
-static inline unsigned long __drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+static inline u64 __drm_mm_hole_node_start(struct drm_mm_node *hole_node)
 {
 	return hole_node->start + hole_node->size;
 }
@@ -140,13 +140,13 @@ static inline unsigned long __drm_mm_hole_node_start(struct drm_mm_node *hole_no
  * Returns:
  * Start of the subsequent hole.
  */
-static inline unsigned long drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+static inline u64 drm_mm_hole_node_start(struct drm_mm_node *hole_node)
 {
 	BUG_ON(!hole_node->hole_follows);
 	return __drm_mm_hole_node_start(hole_node);
 }
 
-static inline unsigned long __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+static inline u64 __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 {
 	return list_entry(hole_node->node_list.next,
 			  struct drm_mm_node, node_list)->start;
@@ -163,7 +163,7 @@ static inline unsigned long __drm_mm_hole_node_end(struct drm_mm_node *hole_node
  * Returns:
  * End of the subsequent hole.
  */
-static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+static inline u64 drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 {
 	return __drm_mm_hole_node_end(hole_node);
 }
@@ -222,7 +222,7 @@ int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
 
 int drm_mm_insert_node_generic(struct drm_mm *mm,
 			       struct drm_mm_node *node,
-			       unsigned long size,
+			       u64 size,
 			       unsigned alignment,
 			       unsigned long color,
 			       enum drm_mm_search_flags sflags,
@@ -245,7 +245,7 @@ int drm_mm_insert_node_generic(struct drm_mm *mm,
  */
 static inline int drm_mm_insert_node(struct drm_mm *mm,
 				     struct drm_mm_node *node,
-				     unsigned long size,
+				     u64 size,
 				     unsigned alignment,
 				     enum drm_mm_search_flags flags)
 {
@@ -255,11 +255,11 @@ static inline int drm_mm_insert_node(struct drm_mm *mm,
 
 int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 					struct drm_mm_node *node,
-					unsigned long size,
+					u64 size,
 					unsigned alignment,
 					unsigned long color,
-					unsigned long start,
-					unsigned long end,
+					u64 start,
+					u64 end,
 					enum drm_mm_search_flags sflags,
 					enum drm_mm_allocator_flags aflags);
 /**
@@ -282,10 +282,10 @@ int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
  */
 static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 					      struct drm_mm_node *node,
-					      unsigned long size,
+					      u64 size,
 					      unsigned alignment,
-					      unsigned long start,
-					      unsigned long end,
+					      u64 start,
+					      u64 end,
 					      enum drm_mm_search_flags flags)
 {
 	return drm_mm_insert_node_in_range_generic(mm, node, size, alignment,
@@ -296,21 +296,21 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 void drm_mm_remove_node(struct drm_mm_node *node);
 void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
 void drm_mm_init(struct drm_mm *mm,
-		 unsigned long start,
-		 unsigned long size);
+		 u64 start,
+		 u64 size);
 void drm_mm_takedown(struct drm_mm *mm);
 bool drm_mm_clean(struct drm_mm *mm);
 
 void drm_mm_init_scan(struct drm_mm *mm,
-		      unsigned long size,
+		      u64 size,
 		      unsigned alignment,
 		      unsigned long color);
 void drm_mm_init_scan_with_range(struct drm_mm *mm,
-				 unsigned long size,
+				 u64 size,
 				 unsigned alignment,
 				 unsigned long color,
-				 unsigned long start,
-				 unsigned long end);
+				 u64 start,
+				 u64 end);
 bool drm_mm_scan_add_block(struct drm_mm_node *node);
 bool drm_mm_scan_remove_block(struct drm_mm_node *node);
 

commit 62347f9e0f81d50e9b0923ec8a192f60ab7a1801
Author: Lauri Kasanen <cand@gmx.com>
Date:   Wed Apr 2 20:03:57 2014 +0300

    drm: Add support for two-ended allocation, v3
    
    Clients like i915 need to segregate cache domains within the GTT which
    can lead to small amounts of fragmentation. By allocating the uncached
    buffers from the bottom and the cacheable buffers from the top, we can
    reduce the amount of wasted space and also optimize allocation of the
    mappable portion of the GTT to only those buffers that require CPU
    access through the GTT.
    
    For other drivers, allocating small bos from one end and large ones
    from the other helps improve the quality of fragmentation.
    
    Based on drm_mm work by Chris Wilson.
    
    v3: Changed to use a TTM placement flag
    v2: Updated kerneldoc
    
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Ben Widawsky <ben@bwidawsk.net>
    Cc: Christian König <deathsimple@vodafone.de>
    Signed-off-by: Lauri Kasanen <cand@gmx.com>
    Signed-off-by: David Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 8b6981ab3fcf..a24addfdfcec 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -47,8 +47,17 @@
 enum drm_mm_search_flags {
 	DRM_MM_SEARCH_DEFAULT =		0,
 	DRM_MM_SEARCH_BEST =		1 << 0,
+	DRM_MM_SEARCH_BELOW =		1 << 1,
 };
 
+enum drm_mm_allocator_flags {
+	DRM_MM_CREATE_DEFAULT =		0,
+	DRM_MM_CREATE_TOP =		1 << 0,
+};
+
+#define DRM_MM_BOTTOMUP DRM_MM_SEARCH_DEFAULT, DRM_MM_CREATE_DEFAULT
+#define DRM_MM_TOPDOWN DRM_MM_SEARCH_BELOW, DRM_MM_CREATE_TOP
+
 struct drm_mm_node {
 	struct list_head node_list;
 	struct list_head hole_stack;
@@ -186,6 +195,9 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
  * Implementation Note:
  * We need to inline list_for_each_entry in order to be able to set hole_start
  * and hole_end on each iteration while keeping the macro sane.
+ *
+ * The __drm_mm_for_each_hole version is similar, but with added support for
+ * going backwards.
  */
 #define drm_mm_for_each_hole(entry, mm, hole_start, hole_end) \
 	for (entry = list_entry((mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
@@ -195,6 +207,14 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 	     1 : 0; \
 	     entry = list_entry(entry->hole_stack.next, struct drm_mm_node, hole_stack))
 
+#define __drm_mm_for_each_hole(entry, mm, hole_start, hole_end, backwards) \
+	for (entry = list_entry((backwards) ? (mm)->hole_stack.prev : (mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
+	     &entry->hole_stack != &(mm)->hole_stack ? \
+	     hole_start = drm_mm_hole_node_start(entry), \
+	     hole_end = drm_mm_hole_node_end(entry), \
+	     1 : 0; \
+	     entry = list_entry((backwards) ? entry->hole_stack.prev : entry->hole_stack.next, struct drm_mm_node, hole_stack))
+
 /*
  * Basic range manager support (drm_mm.c)
  */
@@ -205,7 +225,8 @@ int drm_mm_insert_node_generic(struct drm_mm *mm,
 			       unsigned long size,
 			       unsigned alignment,
 			       unsigned long color,
-			       enum drm_mm_search_flags flags);
+			       enum drm_mm_search_flags sflags,
+			       enum drm_mm_allocator_flags aflags);
 /**
  * drm_mm_insert_node - search for space and insert @node
  * @mm: drm_mm to allocate from
@@ -228,7 +249,8 @@ static inline int drm_mm_insert_node(struct drm_mm *mm,
 				     unsigned alignment,
 				     enum drm_mm_search_flags flags)
 {
-	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags);
+	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags,
+					  DRM_MM_CREATE_DEFAULT);
 }
 
 int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
@@ -238,7 +260,8 @@ int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 					unsigned long color,
 					unsigned long start,
 					unsigned long end,
-					enum drm_mm_search_flags flags);
+					enum drm_mm_search_flags sflags,
+					enum drm_mm_allocator_flags aflags);
 /**
  * drm_mm_insert_node_in_range - ranged search for space and insert @node
  * @mm: drm_mm to allocate from
@@ -266,7 +289,8 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 					      enum drm_mm_search_flags flags)
 {
 	return drm_mm_insert_node_in_range_generic(mm, node, size, alignment,
-						   0, start, end, flags);
+						   0, start, end, flags,
+						   DRM_MM_CREATE_DEFAULT);
 }
 
 void drm_mm_remove_node(struct drm_mm_node *node);

commit e18c04128faa2aa08547f8b73b9ecbf8fd6936af
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Thu Jan 23 00:39:13 2014 +0100

    drm/doc: Add function reference documentation for drm_mm.c
    
    While at it do a tiny bit of interface cleanup and convert boolean
    return values to bool. With this patch all exported functions and inline
    helpers which are part of the drm_mm public interface are documented.
    
    Also drop superflous extern function modifiers since most of drm_mm.h
    doesn't use them - more consistent that way.
    
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index cba67865d18f..8b6981ab3fcf 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -85,11 +85,31 @@ struct drm_mm {
 			     unsigned long *start, unsigned long *end);
 };
 
+/**
+ * drm_mm_node_allocated - checks whether a node is allocated
+ * @node: drm_mm_node to check
+ *
+ * Drivers should use this helpers for proper encapusulation of drm_mm
+ * internals.
+ *
+ * Returns:
+ * True if the @node is allocated.
+ */
 static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
 {
 	return node->allocated;
 }
 
+/**
+ * drm_mm_initialized - checks whether an allocator is initialized
+ * @mm: drm_mm to check
+ *
+ * Drivers should use this helpers for proper encapusulation of drm_mm
+ * internals.
+ *
+ * Returns:
+ * True if the @mm is initialized.
+ */
 static inline bool drm_mm_initialized(struct drm_mm *mm)
 {
 	return mm->hole_stack.next;
@@ -100,6 +120,17 @@ static inline unsigned long __drm_mm_hole_node_start(struct drm_mm_node *hole_no
 	return hole_node->start + hole_node->size;
 }
 
+/**
+ * drm_mm_hole_node_start - computes the start of the hole following @node
+ * @hole_node: drm_mm_node which implicitly tracks the following hole
+ *
+ * This is useful for driver-sepific debug dumpers. Otherwise drivers should not
+ * inspect holes themselves. Drivers must check first whether a hole indeed
+ * follows by looking at node->hole_follows.
+ *
+ * Returns:
+ * Start of the subsequent hole.
+ */
 static inline unsigned long drm_mm_hole_node_start(struct drm_mm_node *hole_node)
 {
 	BUG_ON(!hole_node->hole_follows);
@@ -112,18 +143,49 @@ static inline unsigned long __drm_mm_hole_node_end(struct drm_mm_node *hole_node
 			  struct drm_mm_node, node_list)->start;
 }
 
+/**
+ * drm_mm_hole_node_end - computes the end of the hole following @node
+ * @hole_node: drm_mm_node which implicitly tracks the following hole
+ *
+ * This is useful for driver-sepific debug dumpers. Otherwise drivers should not
+ * inspect holes themselves. Drivers must check first whether a hole indeed
+ * follows by looking at node->hole_follows.
+ *
+ * Returns:
+ * End of the subsequent hole.
+ */
 static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 {
 	return __drm_mm_hole_node_end(hole_node);
 }
 
+/**
+ * drm_mm_for_each_node - iterator to walk over all allocated nodes
+ * @entry: drm_mm_node structure to assign to in each iteration step
+ * @mm: drm_mm allocator to walk
+ *
+ * This iterator walks over all nodes in the range allocator. It is implemented
+ * with list_for_each, so not save against removal of elements.
+ */
 #define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
 						&(mm)->head_node.node_list, \
 						node_list)
 
-/* Note that we need to unroll list_for_each_entry in order to inline
- * setting hole_start and hole_end on each iteration and keep the
- * macro sane.
+/**
+ * drm_mm_for_each_hole - iterator to walk over all holes
+ * @entry: drm_mm_node used internally to track progress
+ * @mm: drm_mm allocator to walk
+ * @hole_start: ulong variable to assign the hole start to on each iteration
+ * @hole_end: ulong variable to assign the hole end to on each iteration
+ *
+ * This iterator walks over all holes in the range allocator. It is implemented
+ * with list_for_each, so not save against removal of elements. @entry is used
+ * internally and will not reflect a real drm_mm_node for the very first hole.
+ * Hence users of this iterator may not access it.
+ *
+ * Implementation Note:
+ * We need to inline list_for_each_entry in order to be able to set hole_start
+ * and hole_end on each iteration while keeping the macro sane.
  */
 #define drm_mm_for_each_hole(entry, mm, hole_start, hole_end) \
 	for (entry = list_entry((mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
@@ -136,14 +198,30 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 /*
  * Basic range manager support (drm_mm.c)
  */
-extern int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
-
-extern int drm_mm_insert_node_generic(struct drm_mm *mm,
-				      struct drm_mm_node *node,
-				      unsigned long size,
-				      unsigned alignment,
-				      unsigned long color,
-				      enum drm_mm_search_flags flags);
+int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
+
+int drm_mm_insert_node_generic(struct drm_mm *mm,
+			       struct drm_mm_node *node,
+			       unsigned long size,
+			       unsigned alignment,
+			       unsigned long color,
+			       enum drm_mm_search_flags flags);
+/**
+ * drm_mm_insert_node - search for space and insert @node
+ * @mm: drm_mm to allocate from
+ * @node: preallocate node to insert
+ * @size: size of the allocation
+ * @alignment: alignment of the allocation
+ * @flags: flags to fine-tune the allocation
+ *
+ * This is a simplified version of drm_mm_insert_node_generic() with @color set
+ * to 0.
+ *
+ * The preallocated node must be cleared to 0.
+ *
+ * Returns:
+ * 0 on success, -ENOSPC if there's no suitable hole.
+ */
 static inline int drm_mm_insert_node(struct drm_mm *mm,
 				     struct drm_mm_node *node,
 				     unsigned long size,
@@ -153,14 +231,32 @@ static inline int drm_mm_insert_node(struct drm_mm *mm,
 	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags);
 }
 
-extern int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
-				       struct drm_mm_node *node,
-				       unsigned long size,
-				       unsigned alignment,
-				       unsigned long color,
-				       unsigned long start,
-				       unsigned long end,
-				       enum drm_mm_search_flags flags);
+int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
+					struct drm_mm_node *node,
+					unsigned long size,
+					unsigned alignment,
+					unsigned long color,
+					unsigned long start,
+					unsigned long end,
+					enum drm_mm_search_flags flags);
+/**
+ * drm_mm_insert_node_in_range - ranged search for space and insert @node
+ * @mm: drm_mm to allocate from
+ * @node: preallocate node to insert
+ * @size: size of the allocation
+ * @alignment: alignment of the allocation
+ * @start: start of the allowed range for this node
+ * @end: end of the allowed range for this node
+ * @flags: flags to fine-tune the allocation
+ *
+ * This is a simplified version of drm_mm_insert_node_in_range_generic() with
+ * @color set to 0.
+ *
+ * The preallocated node must be cleared to 0.
+ *
+ * Returns:
+ * 0 on success, -ENOSPC if there's no suitable hole.
+ */
 static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 					      struct drm_mm_node *node,
 					      unsigned long size,
@@ -173,13 +269,13 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 						   0, start, end, flags);
 }
 
-extern void drm_mm_remove_node(struct drm_mm_node *node);
-extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
-extern void drm_mm_init(struct drm_mm *mm,
-			unsigned long start,
-			unsigned long size);
-extern void drm_mm_takedown(struct drm_mm *mm);
-extern int drm_mm_clean(struct drm_mm *mm);
+void drm_mm_remove_node(struct drm_mm_node *node);
+void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
+void drm_mm_init(struct drm_mm *mm,
+		 unsigned long start,
+		 unsigned long size);
+void drm_mm_takedown(struct drm_mm *mm);
+bool drm_mm_clean(struct drm_mm *mm);
 
 void drm_mm_init_scan(struct drm_mm *mm,
 		      unsigned long size,
@@ -191,10 +287,10 @@ void drm_mm_init_scan_with_range(struct drm_mm *mm,
 				 unsigned long color,
 				 unsigned long start,
 				 unsigned long end);
-int drm_mm_scan_add_block(struct drm_mm_node *node);
-int drm_mm_scan_remove_block(struct drm_mm_node *node);
+bool drm_mm_scan_add_block(struct drm_mm_node *node);
+bool drm_mm_scan_remove_block(struct drm_mm_node *node);
 
-extern void drm_mm_debug_table(struct drm_mm *mm, const char *prefix);
+void drm_mm_debug_table(struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS
 int drm_mm_dump_table(struct seq_file *m, struct drm_mm *mm);
 #endif

commit c700c67bae6698fbc6bd20e2ae5dc62ddd367b3b
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Sat Jul 27 13:39:28 2013 +0200

    drm/mm: remove unused API
    
    We used to pre-allocate drm_mm nodes and save them in a linked list for
    later usage so we always have spare ones in atomic contexts. However, this
    is really racy if multiple threads are in an atomic context at the same
    time and we don't have enough spare nodes. Moreover, all remaining users
    run in user-context and just lock drm_mm with a spinlock. So we can easily
    preallocate the node, take the spinlock and insert the node.
    
    This may have worked well with BKL in place, however, with today's
    infrastructure it really doesn't make any sense. Besides, most users can
    easily embed drm_mm_node into their objects so no allocation is needed at
    all.
    
    Thus, remove the old pre-alloc API and all the helpers that it provides.
    Drivers have already been converted and we should not use the old API for
    new code, anymore.
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 439d1a17d3b1..cba67865d18f 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -70,9 +70,6 @@ struct drm_mm {
 	/* head_node.node_list is the list of all memory nodes, ordered
 	 * according to the (increasing) start address of the memory node. */
 	struct drm_mm_node head_node;
-	struct list_head unused_nodes;
-	int num_unused;
-	spinlock_t unused_lock;
 	unsigned int scan_check_range : 1;
 	unsigned scan_alignment;
 	unsigned long scan_color;
@@ -123,13 +120,6 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 #define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
 						&(mm)->head_node.node_list, \
 						node_list)
-#define drm_mm_for_each_scanned_node_reverse(entry, n, mm) \
-	for (entry = (mm)->prev_scanned_node, \
-		next = entry ? list_entry(entry->node_list.next, \
-			struct drm_mm_node, node_list) : NULL; \
-	     entry != NULL; entry = next, \
-		next = entry ? list_entry(entry->node_list.next, \
-			struct drm_mm_node, node_list) : NULL) \
 
 /* Note that we need to unroll list_for_each_entry in order to inline
  * setting hole_start and hole_end on each iteration and keep the
@@ -147,52 +137,6 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
  * Basic range manager support (drm_mm.c)
  */
 extern int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
-extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
-						    unsigned long size,
-						    unsigned alignment,
-						    unsigned long color,
-						    int atomic);
-extern struct drm_mm_node *drm_mm_get_block_range_generic(
-						struct drm_mm_node *node,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long color,
-						unsigned long start,
-						unsigned long end,
-						int atomic);
-
-static inline struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
-						   unsigned long size,
-						   unsigned alignment)
-{
-	return drm_mm_get_block_generic(parent, size, alignment, 0, 0);
-}
-static inline struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *parent,
-							  unsigned long size,
-							  unsigned alignment)
-{
-	return drm_mm_get_block_generic(parent, size, alignment, 0, 1);
-}
-static inline struct drm_mm_node *drm_mm_get_block_range(
-						struct drm_mm_node *parent,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long start,
-						unsigned long end)
-{
-	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
-					      start, end, 0);
-}
-static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
-						struct drm_mm_node *parent,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long start,
-						unsigned long end)
-{
-	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
-						start, end, 1);
-}
 
 extern int drm_mm_insert_node_generic(struct drm_mm *mm,
 				      struct drm_mm_node *node,
@@ -229,52 +173,13 @@ static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
 						   0, start, end, flags);
 }
 
-extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern void drm_mm_remove_node(struct drm_mm_node *node);
 extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
-extern struct drm_mm_node *drm_mm_search_free_generic(const struct drm_mm *mm,
-						      unsigned long size,
-						      unsigned alignment,
-						      unsigned long color,
-						      enum drm_mm_search_flags flags);
-extern struct drm_mm_node *drm_mm_search_free_in_range_generic(
-						const struct drm_mm *mm,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long color,
-						unsigned long start,
-						unsigned long end,
-						enum drm_mm_search_flags flags);
-static inline struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
-						     unsigned long size,
-						     unsigned alignment,
-						     enum drm_mm_search_flags flags)
-{
-	return drm_mm_search_free_generic(mm,size, alignment, 0, flags);
-}
-static inline  struct drm_mm_node *drm_mm_search_free_in_range(
-						const struct drm_mm *mm,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long start,
-						unsigned long end,
-						enum drm_mm_search_flags flags)
-{
-	return drm_mm_search_free_in_range_generic(mm, size, alignment, 0,
-						   start, end, flags);
-}
-
 extern void drm_mm_init(struct drm_mm *mm,
 			unsigned long start,
 			unsigned long size);
 extern void drm_mm_takedown(struct drm_mm *mm);
 extern int drm_mm_clean(struct drm_mm *mm);
-extern int drm_mm_pre_get(struct drm_mm *mm);
-
-static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
-{
-	return block->mm;
-}
 
 void drm_mm_init_scan(struct drm_mm *mm,
 		      unsigned long size,

commit 31e5d7c67bd492fd0b2988440e21e31809c7c9af
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Sat Jul 27 13:36:27 2013 +0200

    drm/mm: add "best_match" flag to drm_mm_insert_node()
    
    Add a "best_match" flag similar to the drm_mm_search_*() helpers so we
    can convert TTM to use them in follow up patches. We can also inline the
    non-generic helpers and move them into the header to allow compile-time
    optimizations.
    
    To make calls to drm_mm_{search,insert}_node() more readable, this
    converts the boolean argument to a flagset. There are pending patches that
    add additional flags for top-down allocators and more.
    
    v2:
     - use flag parameter instead of boolean "best_match"
     - convert *_search_free() helpers to also use flags argument
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 98cb50ea6acb..439d1a17d3b1 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -44,6 +44,11 @@
 #include <linux/seq_file.h>
 #endif
 
+enum drm_mm_search_flags {
+	DRM_MM_SEARCH_DEFAULT =		0,
+	DRM_MM_SEARCH_BEST =		1 << 0,
+};
+
 struct drm_mm_node {
 	struct list_head node_list;
 	struct list_head hole_stack;
@@ -189,28 +194,41 @@ static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 						start, end, 1);
 }
 
-extern int drm_mm_insert_node(struct drm_mm *mm,
-			      struct drm_mm_node *node,
-			      unsigned long size,
-			      unsigned alignment);
-extern int drm_mm_insert_node_in_range(struct drm_mm *mm,
-				       struct drm_mm_node *node,
-				       unsigned long size,
-				       unsigned alignment,
-				       unsigned long start,
-				       unsigned long end);
 extern int drm_mm_insert_node_generic(struct drm_mm *mm,
 				      struct drm_mm_node *node,
 				      unsigned long size,
 				      unsigned alignment,
-				      unsigned long color);
+				      unsigned long color,
+				      enum drm_mm_search_flags flags);
+static inline int drm_mm_insert_node(struct drm_mm *mm,
+				     struct drm_mm_node *node,
+				     unsigned long size,
+				     unsigned alignment,
+				     enum drm_mm_search_flags flags)
+{
+	return drm_mm_insert_node_generic(mm, node, size, alignment, 0, flags);
+}
+
 extern int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
 				       struct drm_mm_node *node,
 				       unsigned long size,
 				       unsigned alignment,
 				       unsigned long color,
 				       unsigned long start,
-				       unsigned long end);
+				       unsigned long end,
+				       enum drm_mm_search_flags flags);
+static inline int drm_mm_insert_node_in_range(struct drm_mm *mm,
+					      struct drm_mm_node *node,
+					      unsigned long size,
+					      unsigned alignment,
+					      unsigned long start,
+					      unsigned long end,
+					      enum drm_mm_search_flags flags)
+{
+	return drm_mm_insert_node_in_range_generic(mm, node, size, alignment,
+						   0, start, end, flags);
+}
+
 extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern void drm_mm_remove_node(struct drm_mm_node *node);
 extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
@@ -218,7 +236,7 @@ extern struct drm_mm_node *drm_mm_search_free_generic(const struct drm_mm *mm,
 						      unsigned long size,
 						      unsigned alignment,
 						      unsigned long color,
-						      bool best_match);
+						      enum drm_mm_search_flags flags);
 extern struct drm_mm_node *drm_mm_search_free_in_range_generic(
 						const struct drm_mm *mm,
 						unsigned long size,
@@ -226,13 +244,13 @@ extern struct drm_mm_node *drm_mm_search_free_in_range_generic(
 						unsigned long color,
 						unsigned long start,
 						unsigned long end,
-						bool best_match);
+						enum drm_mm_search_flags flags);
 static inline struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
 						     unsigned long size,
 						     unsigned alignment,
-						     bool best_match)
+						     enum drm_mm_search_flags flags)
 {
-	return drm_mm_search_free_generic(mm,size, alignment, 0, best_match);
+	return drm_mm_search_free_generic(mm,size, alignment, 0, flags);
 }
 static inline  struct drm_mm_node *drm_mm_search_free_in_range(
 						const struct drm_mm *mm,
@@ -240,10 +258,10 @@ static inline  struct drm_mm_node *drm_mm_search_free_in_range(
 						unsigned alignment,
 						unsigned long start,
 						unsigned long end,
-						bool best_match)
+						enum drm_mm_search_flags flags)
 {
 	return drm_mm_search_free_in_range_generic(mm, size, alignment, 0,
-						   start, end, best_match);
+						   start, end, flags);
 }
 
 extern void drm_mm_init(struct drm_mm *mm,

commit 86e81f0e624b55fa9f1560c3b64bc80e458c5168
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Thu Jul 25 18:02:31 2013 +0200

    drm/mm: include required headers in drm_mm.h
    
    We need BUG_ON(), spinlock_t and standard kernel data-types so include the
    right headers.
    
    Subject: [drm-intel:drm-intel-nightly 154/166] include/drm/drm_mm.h:67:2:
     error: unknown type name 'spinlock_t'
    Message-ID: <51f14693.g5HGdcuw2v3m8FOd%fengguang.wu@intel.com>
    
    In case it didn't link to it correctly. Somehow this bug doesn't occur
    here on my machine, hmm. But I think fixing drm_mm.h is better than
    changing the include-order in drm_vma_manager.h, so this is what I
    did.
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index b87d05e17d46..98cb50ea6acb 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -36,7 +36,10 @@
 /*
  * Generic range manager structs
  */
+#include <linux/bug.h>
+#include <linux/kernel.h>
 #include <linux/list.h>
+#include <linux/spinlock.h>
 #ifdef CONFIG_DEBUG_FS
 #include <linux/seq_file.h>
 #endif

commit e13af9a8340685cfe25d0c9f708da7121e0f51dd
Merge: ee114b97e67b 50b44a449ff1
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Jul 19 11:56:14 2013 +1000

    Merge tag 'drm-intel-next-2013-07-12' of git://people.freedesktop.org/~danvet/drm-intel into drm-next
    
     Highlights:
    - follow-up refactoring after the shared dpll rework that landed in 3.11
    - oddball prep cleanups from Ben for ppgtt
    - encoder->get_config state tracking infrastructure from Jesse
    - used by the experimental fastboot support from Jesse (disabled by
      default)
    - make the error state file official and add it to our sysfs interface
      (Mika)
    - drm_mm prep changes from Ben, prepares to embedd the drm_mm_node (which
      will be used by the vma rework later on)
    - interrupt handling rework, follow up cleanups to the VECS enabling, hpd
      storm handling and fifo underrun reporting.
    - Big pile of smaller cleanups, code improvements and related stuff.
    
    * tag 'drm-intel-next-2013-07-12' of git://people.freedesktop.org/~danvet/drm-intel: (72 commits)
      drm/i915: clear DPLL reg when disabling i9xx dplls
      drm/i915: Fix up cpt pixel multiplier enable sequence
      drm/i915: clean up vlv ->pre_pll_enable and pll enable sequence
      drm/i915: move error state to own compilation unit
      drm/i915: Don't attempt to read an unitialized stack value
      drm/i915: Use for_each_pipe() when possible
      drm/i915: don't enable PM_VEBOX_CS_ERROR_INTERRUPT
      drm/i915: unify ring irq refcounts (again)
      drm/i915: kill dev_priv->rps.lock
      drm/i915: queue work outside spinlock in hsw_pm_irq_handler
      drm/i915: streamline hsw_pm_irq_handler
      drm/i915: irq handlers don't need interrupt-safe spinlocks
      drm/i915: kill lpt pch transcoder->crtc mapping code for fifo underruns
      drm/i915: improve GEN7_ERR_INT clearing for fifo underrun reporting
      drm/i915: improve SERR_INT clearing for fifo underrun reporting
      drm/i915: extract ibx_display_interrupt_update
      drm/i915: remove unused members from drm_i915_private
      drm/i915: don't frob mm.suspended when not using ums
      drm/i915: Fix VLV DP RBR/HDMI/DAC PLL LPF coefficients
      drm/i915: WARN if the bios reserved range is bigger than stolen size
      ...
    
    Conflicts:
            drivers/gpu/drm/i915/i915_gem.c

commit 338710e7aff3428dc8170a03704a8ae981b58dcd
Author: Ben Widawsky <ben@bwidawsk.net>
Date:   Fri Jul 5 14:41:03 2013 -0700

    drm: Change create block to reserve node
    
    With the previous patch we no longer actually create a node, we simply
    find the correct hole and occupy it. This very well could have been
    squashed with the last patch, but since I already had David's review, I
    figured it's easiest to keep it distinct.
    
    Also update the users in i915. Conveniently this is the only user of the
    interface.
    
    CC: David Airlie <airlied@linux.ie>
    CC: <dri-devel@lists.freedesktop.org>
    Signed-off-by: Ben Widawsky <ben@bwidawsk.net>
    Acked-by: David Airlie <airlied@linux.ie>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index d8b56b7d1839..2de91e3da5cc 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -138,10 +138,7 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 /*
  * Basic range manager support (drm_mm.c)
  */
-extern int drm_mm_create_block(struct drm_mm *mm,
-			       struct drm_mm_node *node,
-			       unsigned long start,
-			       unsigned long size);
+extern int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node);
 extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
 						    unsigned long size,
 						    unsigned alignment,

commit b3a070cccb9135f8bec63d9f194ddaa422136fb0
Author: Ben Widawsky <ben@bwidawsk.net>
Date:   Fri Jul 5 14:41:02 2013 -0700

    drm: pre allocate node for create_block
    
    For an upcoming patch where we introduce the i915 VMA, it's ideal to
    have the drm_mm_node as part of the VMA struct (ie. it's pre-allocated).
    Part of the conversion to VMAs is to kill off obj->gtt_space. Doing this
    will break a bunch of code, but amongst them are 2 callers of
    drm_mm_create_block(), both related to stolen memory.
    
    It also allows us to embed the drm_mm_node into the object currently
    which provides a nice transition over to the new code.
    
    v2: Reordered to do before ripping out obj->gtt_offset.
    Some minor cleanups made available because of reordering.
    
    v3: s/continue/break on failed stolen node allocation (David)
    Set obj->gtt_space on failed node allocation (David)
    Only unref stolen (fix double free) on failed create_stolen (David)
    Free node, and NULL it in failed create_stolen (David)
    Add back accidentally removed newline (David)
    
    CC: <dri-devel@lists.freedesktop.org>
    Reviewed-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Ben Widawsky <ben@bwidawsk.net>
    Acked-by: David Airlie <airlied@linux.ie>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 88591ef8fa24..d8b56b7d1839 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -138,10 +138,10 @@ static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
 /*
  * Basic range manager support (drm_mm.c)
  */
-extern struct drm_mm_node *drm_mm_create_block(struct drm_mm *mm,
-					       unsigned long start,
-					       unsigned long size,
-					       bool atomic);
+extern int drm_mm_create_block(struct drm_mm *mm,
+			       struct drm_mm_node *node,
+			       unsigned long start,
+			       unsigned long size);
 extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
 						    unsigned long size,
 						    unsigned alignment,
@@ -155,6 +155,7 @@ extern struct drm_mm_node *drm_mm_get_block_range_generic(
 						unsigned long start,
 						unsigned long end,
 						int atomic);
+
 static inline struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
 						   unsigned long size,
 						   unsigned alignment)

commit 69163ea82732894e8c1e17df4010372ed078efdd
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Mon Jul 1 22:05:53 2013 +0200

    drm/mm: kill color_search_free/get_block
    
    drm/i915 is the only user of the color allocation handling and
    switched to insert_node a while ago. So we can ditch this.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index de9242542f05..4d06edb56d5f 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -177,17 +177,6 @@ static inline struct drm_mm_node *drm_mm_get_block_range(
 	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
 					      start, end, 0);
 }
-static inline struct drm_mm_node *drm_mm_get_color_block_range(
-						struct drm_mm_node *parent,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long color,
-						unsigned long start,
-						unsigned long end)
-{
-	return drm_mm_get_block_range_generic(parent, size, alignment, color,
-					      start, end, 0);
-}
 static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 						struct drm_mm_node *parent,
 						unsigned long size,
@@ -255,26 +244,7 @@ static inline  struct drm_mm_node *drm_mm_search_free_in_range(
 	return drm_mm_search_free_in_range_generic(mm, size, alignment, 0,
 						   start, end, best_match);
 }
-static inline struct drm_mm_node *drm_mm_search_free_color(const struct drm_mm *mm,
-							   unsigned long size,
-							   unsigned alignment,
-							   unsigned long color,
-							   bool best_match)
-{
-	return drm_mm_search_free_generic(mm,size, alignment, color, best_match);
-}
-static inline  struct drm_mm_node *drm_mm_search_free_in_range_color(
-						const struct drm_mm *mm,
-						unsigned long size,
-						unsigned alignment,
-						unsigned long color,
-						unsigned long start,
-						unsigned long end,
-						bool best_match)
-{
-	return drm_mm_search_free_in_range_generic(mm, size, alignment, color,
-						   start, end, best_match);
-}
+
 extern void drm_mm_init(struct drm_mm *mm,
 			unsigned long start,
 			unsigned long size);

commit 77ef8bbc87be7ad10b410247efc6d0f10676b401
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Mon Jul 1 20:32:58 2013 +0200

    drm: make drm_mm_init() return void
    
    There is no reason to return "int" as this function never fails.
    Furthermore, several drivers (ast, sis) already depend on this.
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 88591ef8fa24..de9242542f05 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -275,9 +275,9 @@ static inline  struct drm_mm_node *drm_mm_search_free_in_range_color(
 	return drm_mm_search_free_in_range_generic(mm, size, alignment, color,
 						   start, end, best_match);
 }
-extern int drm_mm_init(struct drm_mm *mm,
-		       unsigned long start,
-		       unsigned long size);
+extern void drm_mm_init(struct drm_mm *mm,
+			unsigned long start,
+			unsigned long size);
 extern void drm_mm_takedown(struct drm_mm *mm);
 extern int drm_mm_clean(struct drm_mm *mm);
 extern int drm_mm_pre_get(struct drm_mm *mm);

commit 735dc0d1e29329ff34ec97f66e130cce481c9607
Merge: bac4b7c3b5c0 20c60c35de32
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Jan 21 07:44:58 2013 +1000

    Merge branch 'drm-kms-locking' of git://people.freedesktop.org/~danvet/drm-intel into drm-next
    
    The aim of this locking rework is that ioctls which a compositor should be
    might call for every frame (set_cursor, page_flip, addfb, rmfb and
    getfb/create_handle) should not be able to block on kms background
    activities like output detection. And since each EDID read takes about
    25ms (in the best case), that always means we'll drop at least one frame.
    
    The solution is to add per-crtc locking for these ioctls, and restrict
    background activities to only use the global lock. Change-the-world type
    of events (modeset, dpms, ...) need to grab all locks.
    
    Two tricky parts arose in the conversion:
    - A lot of current code assumes that a kms fb object can't disappear while
      holding the global lock, since the current code serializes fb
      destruction with it. Hence proper lifetime management using the already
      created refcounting for fbs need to be instantiated for all ioctls and
      interfaces/users.
    
    - The rmfb ioctl removes the to-be-deleted fb from all active users. But
      unconditionally taking the global kms lock to do so introduces an
      unacceptable potential stall point. And obviously changing the userspace
      abi isn't on the table, either. Hence this conversion opportunistically
      checks whether the rmfb ioctl holds the very last reference, which
      guarantees that the fb isn't in active use on any crtc or plane (thanks
      to the conversion to the new lifetime rules using proper refcounting).
      Only if this is not the case will the code go through the slowpath and
      grab all modeset locks. Sane compositors will never hit this path and so
      avoid the stall, but userspace relying on these semantics will also not
      break.
    
    All these cases are exercised by the newly added subtests for the i-g-t
    kms_flip, tested on a machine where a full detect cycle takes around 100
    ms.  It works, and no frames are dropped any more with these patches
    applied.  kms_flip also contains a special case to exercise the
    above-describe rmfb slowpath.
    
    * 'drm-kms-locking' of git://people.freedesktop.org/~danvet/drm-intel: (335 commits)
      drm/fb_helper: check whether fbcon is bound
      drm/doc: updates for new framebuffer lifetime rules
      drm: don't hold crtc mutexes for connector ->detect callbacks
      drm: only grab the crtc lock for pageflips
      drm: optimize drm_framebuffer_remove
      drm/vmwgfx: add proper framebuffer refcounting
      drm/i915: dump refcount into framebuffer debugfs file
      drm: refcounting for crtc framebuffers
      drm: refcounting for sprite framebuffers
      drm: fb refcounting for dirtyfb_ioctl
      drm: don't take modeset locks in getfb ioctl
      drm: push modeset_lock_all into ->fb_create driver callbacks
      drm: nest modeset locks within fpriv->fbs_lock
      drm: reference framebuffers which are on the idr
      drm: revamp framebuffer cleanup interfaces
      drm: create drm_framebuffer_lookup
      drm: revamp locking around fb creation/destruction
      drm: only take the crtc lock for ->cursor_move
      drm: only take the crtc lock for ->cursor_set
      drm: add per-crtc locks
      ...

commit b5cc6c0387b2f8d269c1df1e68c97c958dd22fed
Merge: 9931faca02c6 c0c36b941b6f
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Jan 17 20:34:08 2013 +1000

    Merge tag 'drm-intel-next-2012-12-21' of git://people.freedesktop.org/~danvet/drm-intel into drm-next
    
    Daniel writes:
    - seqno wrap fixes and debug infrastructure from Mika Kuoppala and Chris
      Wilson
    - some leftover kill-agp on gen6+ patches from Ben
    - hotplug improvements from Damien
    - clear fb when allocated from stolen, avoids dirt on the fbcon (Chris)
    - Stolen mem support from Chris Wilson, one of the many steps to get to
      real fastboot support.
    - Some DDI code cleanups from Paulo.
    - Some refactorings around lvds and dp code.
    - some random little bits&pieces
    
    * tag 'drm-intel-next-2012-12-21' of git://people.freedesktop.org/~danvet/drm-intel: (93 commits)
      drm/i915: Return the real error code from intel_set_mode()
      drm/i915: Make GSM void
      drm/i915: Move GSM mapping into dev_priv
      drm/i915: Move even more gtt code to i915_gem_gtt
      drm/i915: Make next_seqno debugs entry to use i915_gem_set_seqno
      drm/i915: Introduce i915_gem_set_seqno()
      drm/i915: Always clear semaphore mboxes on seqno wrap
      drm/i915: Initialize hardware semaphore state on ring init
      drm/i915: Introduce ring set_seqno
      drm/i915: Missed conversion to gtt_pte_t
      drm/i915: Bug on unsupported swizzled platforms
      drm/i915: BUG() if fences are used on unsupported platform
      drm/i915: fixup overlay stolen memory leak
      drm/i915: clean up PIPECONF bpc #defines
      drm/i915: add intel_dp_set_signal_levels
      drm/i915: remove leftover display.update_wm assignment
      drm/i915: check for the PCH when setting pch_transcoder
      drm/i915: Clear the stolen fb before enabling
      drm/i915: Access to snooped system memory through the GTT is incoherent
      drm/i915: Remove stale comment about intel_dp_detect()
      ...
    
    Conflicts:
            drivers/gpu/drm/i915/intel_display.c

commit 901593f2bf221659a605bdc1dcb11376ea934163
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Dec 19 16:51:06 2012 +0000

    drm: Only evict the blocks required to create the requested hole
    
    Avoid clobbering adjacent blocks if they happen to expire earlier and
    amalgamate together to form the requested hole.
    
    In passing this fixes a regression from
    commit ea7b1dd44867e9cd6bac67e7c9fc3f128b5b255c
    Author: Daniel Vetter <daniel.vetter@ffwll.ch>
    Date:   Fri Feb 18 17:59:12 2011 +0100
    
        drm: mm: track free areas implicitly
    
    which swaps the end address for size (with a potential overflow) and
    effectively causes the eviction code to clobber almost all earlier
    buffers above the evictee.
    
    v2: Check the original hole not the adjusted as the coloring may confuse
    us when later searching for the overlapping nodes. Also make sure that
    we do apply the range restriction and color adjustment in the same
    order for both scanning, searching and insertion.
    
    v3: Send the version that was actually tested.
    
    Note that this seems to be ducttape of decent quality ot paper over
    some of our unbind related gpu hangs reported since 3.7. It is not
    fully effective though, and certainly doesn't fix the underlying bug.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    [danvet: Added note plus bugzilla link and tested-by.]
    Cc: stable@vger.kernel.org
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=55984
    Tested-by:  Norbert Preining <preining@logic.at>
    Acked-by: Dave Airlie <airlied@gmail.com
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0f4a366f6fa6..3527fb3f75bb 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -70,7 +70,7 @@ struct drm_mm {
 	unsigned long scan_color;
 	unsigned long scan_size;
 	unsigned long scan_hit_start;
-	unsigned scan_hit_size;
+	unsigned long scan_hit_end;
 	unsigned scanned_blocks;
 	unsigned long scan_start;
 	unsigned long scan_end;

commit b81034506fc9b879cb726feb01342be0cdbe6e25
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Dec 7 20:37:06 2012 +0000

    drm: Export routines for inserting preallocated nodes into the mm manager
    
    Required by i915 in order to avoid the allocation in the middle of
    manipulating the drm_mm lists.
    
    Use a pair of stubs to preserve the existing EXPORT_SYMBOLs for
    backporting; to be removed later.
    
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: dri-devel@lists.freedesktop.org
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Jani Nikula <jani.nikula@intel.com>
    [danvet: bikeshedded-away the atomic parameter, it's not yet used
    anywhere.]
    Acked-by: Dave Airlie <airlied@gmail.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 06d7f798a08c..0f4a366f6fa6 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -158,12 +158,29 @@ static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
 						start, end, 1);
 }
-extern int drm_mm_insert_node(struct drm_mm *mm, struct drm_mm_node *node,
-			      unsigned long size, unsigned alignment);
+
+extern int drm_mm_insert_node(struct drm_mm *mm,
+			      struct drm_mm_node *node,
+			      unsigned long size,
+			      unsigned alignment);
 extern int drm_mm_insert_node_in_range(struct drm_mm *mm,
 				       struct drm_mm_node *node,
-				       unsigned long size, unsigned alignment,
-				       unsigned long start, unsigned long end);
+				       unsigned long size,
+				       unsigned alignment,
+				       unsigned long start,
+				       unsigned long end);
+extern int drm_mm_insert_node_generic(struct drm_mm *mm,
+				      struct drm_mm_node *node,
+				      unsigned long size,
+				      unsigned alignment,
+				      unsigned long color);
+extern int drm_mm_insert_node_in_range_generic(struct drm_mm *mm,
+				       struct drm_mm_node *node,
+				       unsigned long size,
+				       unsigned alignment,
+				       unsigned long color,
+				       unsigned long start,
+				       unsigned long end);
 extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern void drm_mm_remove_node(struct drm_mm_node *node);
 extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);

commit 9e8944ab564f2e3dde90a518cd32048c58918608
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Nov 15 11:32:17 2012 +0000

    drm: Introduce an iterator over holes in the drm_mm range manager
    
    This will be used i915 in forthcoming patches in order to measure the
    largest contiguous chunk of memory available for enabling chipset
    features.
    
    v2: Try to make the macro marginally safer and more readable by not
    depending upon the drm_mm_hole_node_end() being non-zero. Note that we
    need to open code list_for_each() in order to update the hole_start,
    hole_end variable on each iteration and keep the macro sane.
    
    v3: Tidy up few BUG_ONs that fell foul of adding additional tests to
    drm_mm_hole_node_start().
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dave Airlie <airlied@redhat.com>
    Acked-by: Dave Airlie <airlied@redhat.com>
    Cc: dri-devel@lists.freedesktop.org
    Reviewed-by: Ben Widawsky <ben@bwidawsk.net>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 4020f9661c3c..cd453653f634 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -89,6 +89,29 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 {
 	return mm->hole_stack.next;
 }
+
+static inline unsigned long __drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+{
+	return hole_node->start + hole_node->size;
+}
+
+static inline unsigned long drm_mm_hole_node_start(struct drm_mm_node *hole_node)
+{
+	BUG_ON(!hole_node->hole_follows);
+	return __drm_mm_hole_node_start(hole_node);
+}
+
+static inline unsigned long __drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+{
+	return list_entry(hole_node->node_list.next,
+			  struct drm_mm_node, node_list)->start;
+}
+
+static inline unsigned long drm_mm_hole_node_end(struct drm_mm_node *hole_node)
+{
+	return __drm_mm_hole_node_end(hole_node);
+}
+
 #define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
 						&(mm)->head_node.node_list, \
 						node_list)
@@ -99,6 +122,19 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 	     entry != NULL; entry = next, \
 		next = entry ? list_entry(entry->node_list.next, \
 			struct drm_mm_node, node_list) : NULL) \
+
+/* Note that we need to unroll list_for_each_entry in order to inline
+ * setting hole_start and hole_end on each iteration and keep the
+ * macro sane.
+ */
+#define drm_mm_for_each_hole(entry, mm, hole_start, hole_end) \
+	for (entry = list_entry((mm)->hole_stack.next, struct drm_mm_node, hole_stack); \
+	     &entry->hole_stack != &(mm)->hole_stack ? \
+	     hole_start = drm_mm_hole_node_start(entry), \
+	     hole_end = drm_mm_hole_node_end(entry), \
+	     1 : 0; \
+	     entry = list_entry(entry->hole_stack.next, struct drm_mm_node, hole_stack))
+
 /*
  * Basic range manager support (drm_mm.c)
  */

commit 5973c7ee519e2a240c68b290a1836bdb25ed3701
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Nov 15 11:32:16 2012 +0000

    drm: Introduce drm_mm_create_block()
    
    To be used later by i915 to preallocate exact blocks of space from the
    range manager.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dave Airlie <airlied@redhat.com>
    Acked-by: Dave Airlie <airlied@redhat.com>
    Cc: dri-devel@lists.freedesktop.org
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 06d7f798a08c..4020f9661c3c 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -102,6 +102,10 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 /*
  * Basic range manager support (drm_mm.c)
  */
+extern struct drm_mm_node *drm_mm_create_block(struct drm_mm *mm,
+					       unsigned long start,
+					       unsigned long size,
+					       bool atomic);
 extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
 						    unsigned long size,
 						    unsigned alignment,

commit 6b9d89b4365ab52bc26f8259122f422e93d87821
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jul 10 11:15:23 2012 +0100

    drm: Add colouring to the range allocator
    
    In order to support snoopable memory on non-LLC architectures (so that
    we can bind vgem objects into the i915 GATT for example), we have to
    avoid the prefetcher on the GPU from crossing memory domains and so
    prevent allocation of a snoopable PTE immediately following an uncached
    PTE. To do that, we need to extend the range allocator with support for
    tracking and segregating different node colours.
    
    This will be used by i915 to segregate memory domains within the GTT.
    
    v2: Now with more drm_mm helpers and less driver interference.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dave Airlie <airlied@redhat.com
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ben Skeggs <bskeggs@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@gmail.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 564b14aa7e16..06d7f798a08c 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -50,6 +50,7 @@ struct drm_mm_node {
 	unsigned scanned_next_free : 1;
 	unsigned scanned_preceeds_hole : 1;
 	unsigned allocated : 1;
+	unsigned long color;
 	unsigned long start;
 	unsigned long size;
 	struct drm_mm *mm;
@@ -66,6 +67,7 @@ struct drm_mm {
 	spinlock_t unused_lock;
 	unsigned int scan_check_range : 1;
 	unsigned scan_alignment;
+	unsigned long scan_color;
 	unsigned long scan_size;
 	unsigned long scan_hit_start;
 	unsigned scan_hit_size;
@@ -73,6 +75,9 @@ struct drm_mm {
 	unsigned long scan_start;
 	unsigned long scan_end;
 	struct drm_mm_node *prev_scanned_node;
+
+	void (*color_adjust)(struct drm_mm_node *node, unsigned long color,
+			     unsigned long *start, unsigned long *end);
 };
 
 static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
@@ -100,11 +105,13 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
 						    unsigned long size,
 						    unsigned alignment,
+						    unsigned long color,
 						    int atomic);
 extern struct drm_mm_node *drm_mm_get_block_range_generic(
 						struct drm_mm_node *node,
 						unsigned long size,
 						unsigned alignment,
+						unsigned long color,
 						unsigned long start,
 						unsigned long end,
 						int atomic);
@@ -112,13 +119,13 @@ static inline struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
 						   unsigned long size,
 						   unsigned alignment)
 {
-	return drm_mm_get_block_generic(parent, size, alignment, 0);
+	return drm_mm_get_block_generic(parent, size, alignment, 0, 0);
 }
 static inline struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *parent,
 							  unsigned long size,
 							  unsigned alignment)
 {
-	return drm_mm_get_block_generic(parent, size, alignment, 1);
+	return drm_mm_get_block_generic(parent, size, alignment, 0, 1);
 }
 static inline struct drm_mm_node *drm_mm_get_block_range(
 						struct drm_mm_node *parent,
@@ -127,8 +134,19 @@ static inline struct drm_mm_node *drm_mm_get_block_range(
 						unsigned long start,
 						unsigned long end)
 {
-	return drm_mm_get_block_range_generic(parent, size, alignment,
-						start, end, 0);
+	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
+					      start, end, 0);
+}
+static inline struct drm_mm_node *drm_mm_get_color_block_range(
+						struct drm_mm_node *parent,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long color,
+						unsigned long start,
+						unsigned long end)
+{
+	return drm_mm_get_block_range_generic(parent, size, alignment, color,
+					      start, end, 0);
 }
 static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 						struct drm_mm_node *parent,
@@ -137,7 +155,7 @@ static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 						unsigned long start,
 						unsigned long end)
 {
-	return drm_mm_get_block_range_generic(parent, size, alignment,
+	return drm_mm_get_block_range_generic(parent, size, alignment, 0,
 						start, end, 1);
 }
 extern int drm_mm_insert_node(struct drm_mm *mm, struct drm_mm_node *node,
@@ -149,18 +167,59 @@ extern int drm_mm_insert_node_in_range(struct drm_mm *mm,
 extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern void drm_mm_remove_node(struct drm_mm_node *node);
 extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
-extern struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
-					      unsigned long size,
-					      unsigned alignment,
-					      int best_match);
-extern struct drm_mm_node *drm_mm_search_free_in_range(
+extern struct drm_mm_node *drm_mm_search_free_generic(const struct drm_mm *mm,
+						      unsigned long size,
+						      unsigned alignment,
+						      unsigned long color,
+						      bool best_match);
+extern struct drm_mm_node *drm_mm_search_free_in_range_generic(
+						const struct drm_mm *mm,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long color,
+						unsigned long start,
+						unsigned long end,
+						bool best_match);
+static inline struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
+						     unsigned long size,
+						     unsigned alignment,
+						     bool best_match)
+{
+	return drm_mm_search_free_generic(mm,size, alignment, 0, best_match);
+}
+static inline  struct drm_mm_node *drm_mm_search_free_in_range(
 						const struct drm_mm *mm,
 						unsigned long size,
 						unsigned alignment,
 						unsigned long start,
 						unsigned long end,
-						int best_match);
-extern int drm_mm_init(struct drm_mm *mm, unsigned long start,
+						bool best_match)
+{
+	return drm_mm_search_free_in_range_generic(mm, size, alignment, 0,
+						   start, end, best_match);
+}
+static inline struct drm_mm_node *drm_mm_search_free_color(const struct drm_mm *mm,
+							   unsigned long size,
+							   unsigned alignment,
+							   unsigned long color,
+							   bool best_match)
+{
+	return drm_mm_search_free_generic(mm,size, alignment, color, best_match);
+}
+static inline  struct drm_mm_node *drm_mm_search_free_in_range_color(
+						const struct drm_mm *mm,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long color,
+						unsigned long start,
+						unsigned long end,
+						bool best_match)
+{
+	return drm_mm_search_free_in_range_generic(mm, size, alignment, color,
+						   start, end, best_match);
+}
+extern int drm_mm_init(struct drm_mm *mm,
+		       unsigned long start,
 		       unsigned long size);
 extern void drm_mm_takedown(struct drm_mm *mm);
 extern int drm_mm_clean(struct drm_mm *mm);
@@ -171,10 +230,14 @@ static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
 	return block->mm;
 }
 
-void drm_mm_init_scan(struct drm_mm *mm, unsigned long size,
-		      unsigned alignment);
-void drm_mm_init_scan_with_range(struct drm_mm *mm, unsigned long size,
+void drm_mm_init_scan(struct drm_mm *mm,
+		      unsigned long size,
+		      unsigned alignment,
+		      unsigned long color);
+void drm_mm_init_scan_with_range(struct drm_mm *mm,
+				 unsigned long size,
 				 unsigned alignment,
+				 unsigned long color,
 				 unsigned long start,
 				 unsigned long end);
 int drm_mm_scan_add_block(struct drm_mm_node *node);

commit 2bbd4492552867053b5a618a2474297e2b1c355d
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri May 6 23:47:53 2011 +0200

    drm: mm: fix debug output
    
    The looping helper didn't do anything due to a superficial
    semicolon. Furthermore one of the two dump functions suffered
    from copy&paste fail.
    
    While staring at the code I've also noticed that the replace
    helper (currently unused) is a bit broken.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index c2f93a8ae2e1..564b14aa7e16 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -86,7 +86,7 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 }
 #define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
 						&(mm)->head_node.node_list, \
-						node_list);
+						node_list)
 #define drm_mm_for_each_scanned_node_reverse(entry, n, mm) \
 	for (entry = (mm)->prev_scanned_node, \
 		next = entry ? list_entry(entry->node_list.next, \

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index b1e7809e5e15..c2f93a8ae2e1 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -56,7 +56,7 @@ struct drm_mm_node {
 };
 
 struct drm_mm {
-	/* List of all memory nodes that immediatly preceed a free hole. */
+	/* List of all memory nodes that immediately precede a free hole. */
 	struct list_head hole_stack;
 	/* head_node.node_list is the list of all memory nodes, ordered
 	 * according to the (increasing) start address of the memory node. */

commit ae0cec2880a4dc6d90c7f8392bdc6705988389ca
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Feb 18 17:59:15 2011 +0100

    drm: mm: add helper to unwind scan state
    
    With the switch to implicit free space accounting one pointer
    got unused when scanning. Use it to create a single-linked list
    to ensure correct unwinding of the scan state.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 17a070e11d3c..b1e7809e5e15 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -72,6 +72,7 @@ struct drm_mm {
 	unsigned scanned_blocks;
 	unsigned long scan_start;
 	unsigned long scan_end;
+	struct drm_mm_node *prev_scanned_node;
 };
 
 static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
@@ -86,6 +87,13 @@ static inline bool drm_mm_initialized(struct drm_mm *mm)
 #define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
 						&(mm)->head_node.node_list, \
 						node_list);
+#define drm_mm_for_each_scanned_node_reverse(entry, n, mm) \
+	for (entry = (mm)->prev_scanned_node, \
+		next = entry ? list_entry(entry->node_list.next, \
+			struct drm_mm_node, node_list) : NULL; \
+	     entry != NULL; entry = next, \
+		next = entry ? list_entry(entry->node_list.next, \
+			struct drm_mm_node, node_list) : NULL) \
 /*
  * Basic range manager support (drm_mm.c)
  */

commit b0b7af1884b7d807a3504804f9825d472de78708
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Feb 18 17:59:14 2011 +0100

    drm: mm: add api for embedding struct drm_mm_node
    
    The old api has a two-step process: First search for a suitable
    free hole, then allocate from that specific hole. No user used
    this to do anything clever. So drop it for the embeddable variant
    of the drm_mm api (the old one retains this ability, for the time
    being).
    
    With struct drm_mm_node embedded, we cannot track allocations
    anymore by checking for a NULL pointer. So keep track of this
    and add a small helper drm_mm_node_allocated.
    
    Also add a function to move allocations between different struct
    drm_mm_node.
    
    v2: Implement suggestions by Chris Wilson.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 34fa36f2de70..17a070e11d3c 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -49,6 +49,7 @@ struct drm_mm_node {
 	unsigned scanned_prev_free : 1;
 	unsigned scanned_next_free : 1;
 	unsigned scanned_preceeds_hole : 1;
+	unsigned allocated : 1;
 	unsigned long start;
 	unsigned long size;
 	struct drm_mm *mm;
@@ -73,6 +74,11 @@ struct drm_mm {
 	unsigned long scan_end;
 };
 
+static inline bool drm_mm_node_allocated(struct drm_mm_node *node)
+{
+	return node->allocated;
+}
+
 static inline bool drm_mm_initialized(struct drm_mm *mm)
 {
 	return mm->hole_stack.next;
@@ -126,7 +132,15 @@ static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
 	return drm_mm_get_block_range_generic(parent, size, alignment,
 						start, end, 1);
 }
+extern int drm_mm_insert_node(struct drm_mm *mm, struct drm_mm_node *node,
+			      unsigned long size, unsigned alignment);
+extern int drm_mm_insert_node_in_range(struct drm_mm *mm,
+				       struct drm_mm_node *node,
+				       unsigned long size, unsigned alignment,
+				       unsigned long start, unsigned long end);
 extern void drm_mm_put_block(struct drm_mm_node *cur);
+extern void drm_mm_remove_node(struct drm_mm_node *node);
+extern void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new);
 extern struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
 					      unsigned long size,
 					      unsigned alignment,
@@ -142,11 +156,6 @@ extern int drm_mm_init(struct drm_mm *mm, unsigned long start,
 		       unsigned long size);
 extern void drm_mm_takedown(struct drm_mm *mm);
 extern int drm_mm_clean(struct drm_mm *mm);
-extern unsigned long drm_mm_tail_space(struct drm_mm *mm);
-extern int drm_mm_remove_space_from_tail(struct drm_mm *mm,
-					 unsigned long size);
-extern int drm_mm_add_space_to_tail(struct drm_mm *mm,
-				    unsigned long size, int atomic);
 extern int drm_mm_pre_get(struct drm_mm *mm);
 
 static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)

commit ea7b1dd44867e9cd6bac67e7c9fc3f128b5b255c
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Feb 18 17:59:12 2011 +0100

    drm: mm: track free areas implicitly
    
    The idea is to track free holes implicitly by marking the allocation
    immediatly preceeding a hole.
    
    To avoid an ugly corner case add a dummy head_node to struct drm_mm
    to track the hole that spans to complete allocation area when the
    memory manager is empty.
    
    To guarantee that there's always a preceeding/following node (that might
    be marked as hole_follows == 1), move the mm->node_list list_head to the
    head_node.
    
    The main allocator and fair-lru scan code actually becomes simpler.
    Only the debug code slightly suffers because free areas are no longer
    explicit.
    
    Also add drm_mm_for_each_node (which will be much more useful when
    struct drm_mm_node is embeddable).
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 0d791462f7b2..34fa36f2de70 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -42,23 +42,24 @@
 #endif
 
 struct drm_mm_node {
-	struct list_head free_stack;
 	struct list_head node_list;
-	unsigned free : 1;
+	struct list_head hole_stack;
+	unsigned hole_follows : 1;
 	unsigned scanned_block : 1;
 	unsigned scanned_prev_free : 1;
 	unsigned scanned_next_free : 1;
+	unsigned scanned_preceeds_hole : 1;
 	unsigned long start;
 	unsigned long size;
 	struct drm_mm *mm;
 };
 
 struct drm_mm {
-	/* List of free memory blocks, most recently freed ordered. */
-	struct list_head free_stack;
-	/* List of all memory nodes, ordered according to the (increasing) start
-	 * address of the memory node. */
-	struct list_head node_list;
+	/* List of all memory nodes that immediatly preceed a free hole. */
+	struct list_head hole_stack;
+	/* head_node.node_list is the list of all memory nodes, ordered
+	 * according to the (increasing) start address of the memory node. */
+	struct drm_mm_node head_node;
 	struct list_head unused_nodes;
 	int num_unused;
 	spinlock_t unused_lock;
@@ -74,9 +75,11 @@ struct drm_mm {
 
 static inline bool drm_mm_initialized(struct drm_mm *mm)
 {
-	return mm->free_stack.next;
+	return mm->hole_stack.next;
 }
-
+#define drm_mm_for_each_node(entry, mm) list_for_each_entry(entry, \
+						&(mm)->head_node.node_list, \
+						node_list);
 /*
  * Basic range manager support (drm_mm.c)
  */

commit 31a5b8ce8f3bf20799eb68da9602de2bee58fdd3
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Feb 18 17:59:11 2011 +0100

    drm/nouveau: don't munge in drm_mm internals
    
    Nouveau was checking drm_mm internals on teardown to see whether the
    memory manager was initialized. Hide these internals in a small
    inline helper function.
    
    Acked-by: Ben Skeggs <bskeggs@redhat.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index e39177778601..0d791462f7b2 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -72,6 +72,11 @@ struct drm_mm {
 	unsigned long scan_end;
 };
 
+static inline bool drm_mm_initialized(struct drm_mm *mm)
+{
+	return mm->free_stack.next;
+}
+
 /*
  * Basic range manager support (drm_mm.c)
  */

commit d935cc61d466f6cc7514032835f4fc379cb7e2ca
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Thu Sep 16 15:13:11 2010 +0200

    drm_mm: add support for range-restricted fair-lru scans
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index bf01531193d5..e39177778601 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -62,11 +62,14 @@ struct drm_mm {
 	struct list_head unused_nodes;
 	int num_unused;
 	spinlock_t unused_lock;
+	unsigned int scan_check_range : 1;
 	unsigned scan_alignment;
 	unsigned long scan_size;
 	unsigned long scan_hit_start;
 	unsigned scan_hit_size;
 	unsigned scanned_blocks;
+	unsigned long scan_start;
+	unsigned long scan_end;
 };
 
 /*
@@ -145,6 +148,10 @@ static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
 
 void drm_mm_init_scan(struct drm_mm *mm, unsigned long size,
 		      unsigned alignment);
+void drm_mm_init_scan_with_range(struct drm_mm *mm, unsigned long size,
+				 unsigned alignment,
+				 unsigned long start,
+				 unsigned long end);
 int drm_mm_scan_add_block(struct drm_mm_node *node);
 int drm_mm_scan_remove_block(struct drm_mm_node *node);
 

commit 709ea97145c125b3811ff70429e90ebdb0e832e5
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Jul 2 15:02:16 2010 +0100

    drm: implement helper functions for scanning lru list
    
    These helper functions can be used to efficiently scan lru list
    for eviction. Eviction becomes a three stage process:
    1. Scanning through the lru list until a suitable hole has been found.
    2. Scan backwards to restore drm_mm consistency and find out which
       objects fall into the hole.
    3. Evict the objects that fall into the hole.
    
    These helper functions don't allocate any memory (at the price of
    not allowing any other concurrent operations). Hence this can also be
    used for ttm (which does lru scanning under a spinlock).
    
    Evicting objects in this fashion should be more fair than the current
    approach by i915 (scan the lru for a object large enough to contain
    the new object). It's also more efficient than the current approach used
    by ttm (uncoditionally evict objects from the lru until there's enough
    free space).
    
    Signed-Off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Acked-by: Thomas Hellstrom <thellstrom@vmwgfx.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index e8740cc185cf..bf01531193d5 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -44,7 +44,10 @@
 struct drm_mm_node {
 	struct list_head free_stack;
 	struct list_head node_list;
-	int free;
+	unsigned free : 1;
+	unsigned scanned_block : 1;
+	unsigned scanned_prev_free : 1;
+	unsigned scanned_next_free : 1;
 	unsigned long start;
 	unsigned long size;
 	struct drm_mm *mm;
@@ -59,6 +62,11 @@ struct drm_mm {
 	struct list_head unused_nodes;
 	int num_unused;
 	spinlock_t unused_lock;
+	unsigned scan_alignment;
+	unsigned long scan_size;
+	unsigned long scan_hit_start;
+	unsigned scan_hit_size;
+	unsigned scanned_blocks;
 };
 
 /*
@@ -135,6 +143,11 @@ static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
 	return block->mm;
 }
 
+void drm_mm_init_scan(struct drm_mm *mm, unsigned long size,
+		      unsigned alignment);
+int drm_mm_scan_add_block(struct drm_mm_node *node);
+int drm_mm_scan_remove_block(struct drm_mm_node *node);
+
 extern void drm_mm_debug_table(struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS
 int drm_mm_dump_table(struct seq_file *m, struct drm_mm *mm);

commit d1024ce91ff4c2c4ccbf692d204c71cbf215157a
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Jul 2 15:02:14 2010 +0100

    drm: sane naming for drm_mm.c
    
    Yeah, I've kinda noticed that fl_entry is the free stack. Still
    give it (and the memory node list ml_entry) decent names.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Acked-by: Thomas Hellstrom <thellstrom@vmwgfx.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index da94071b1703..e8740cc185cf 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -42,8 +42,8 @@
 #endif
 
 struct drm_mm_node {
-	struct list_head fl_entry;
-	struct list_head ml_entry;
+	struct list_head free_stack;
+	struct list_head node_list;
 	int free;
 	unsigned long start;
 	unsigned long size;
@@ -51,8 +51,11 @@ struct drm_mm_node {
 };
 
 struct drm_mm {
-	struct list_head fl_entry;
-	struct list_head ml_entry;
+	/* List of free memory blocks, most recently freed ordered. */
+	struct list_head free_stack;
+	/* List of all memory nodes, ordered according to the (increasing) start
+	 * address of the memory node. */
+	struct list_head node_list;
 	struct list_head unused_nodes;
 	int num_unused;
 	spinlock_t unused_lock;

commit db3307a9f7b8078c654021e3b35354a2b09a8e67
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Fri Jul 2 15:02:12 2010 +0100

    drm: kill drm_mm_node->private
    
    Only ever assigned, never used.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    [glisse: I will re-add if needed for range-restricted allocations]
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 4c10be39a43b..da94071b1703 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -48,7 +48,6 @@ struct drm_mm_node {
 	unsigned long start;
 	unsigned long size;
 	struct drm_mm *mm;
-	void *private;
 };
 
 struct drm_mm {

commit 99d7e48e8cb867f303439ad40e995e203841bd94
Author: Jerome Glisse <jglisse@redhat.com>
Date:   Wed Dec 9 21:55:09 2009 +0100

    drm: Add memory manager debug function
    
    drm_mm_debug_table will print the memory manager state
    in table allowing to give a snapshot of the manager at
    given point in time. Usefull for debugging.
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index b40b2f062039..4c10be39a43b 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -133,6 +133,7 @@ static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
 	return block->mm;
 }
 
+extern void drm_mm_debug_table(struct drm_mm *mm, const char *prefix);
 #ifdef CONFIG_DEBUG_FS
 int drm_mm_dump_table(struct seq_file *m, struct drm_mm *mm);
 #endif

commit a2e68e92d384d37c8cc6bb7206d43b1eb9bc3f08
Author: Jerome Glisse <jglisse@redhat.com>
Date:   Mon Dec 7 15:52:56 2009 +0100

    drm: Add search/get functions to get a block in a specific range
    
    These are required for changes to TTM.
    
    Signed-off-by: Jerome Glisse <jglisse@redhat.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 62329f9a42cb..b40b2f062039 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -66,6 +66,13 @@ extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
 						    unsigned long size,
 						    unsigned alignment,
 						    int atomic);
+extern struct drm_mm_node *drm_mm_get_block_range_generic(
+						struct drm_mm_node *node,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long start,
+						unsigned long end,
+						int atomic);
 static inline struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
 						   unsigned long size,
 						   unsigned alignment)
@@ -78,11 +85,38 @@ static inline struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *pa
 {
 	return drm_mm_get_block_generic(parent, size, alignment, 1);
 }
+static inline struct drm_mm_node *drm_mm_get_block_range(
+						struct drm_mm_node *parent,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long start,
+						unsigned long end)
+{
+	return drm_mm_get_block_range_generic(parent, size, alignment,
+						start, end, 0);
+}
+static inline struct drm_mm_node *drm_mm_get_block_atomic_range(
+						struct drm_mm_node *parent,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long start,
+						unsigned long end)
+{
+	return drm_mm_get_block_range_generic(parent, size, alignment,
+						start, end, 1);
+}
 extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
 					      unsigned long size,
 					      unsigned alignment,
 					      int best_match);
+extern struct drm_mm_node *drm_mm_search_free_in_range(
+						const struct drm_mm *mm,
+						unsigned long size,
+						unsigned alignment,
+						unsigned long start,
+						unsigned long end,
+						int best_match);
 extern int drm_mm_init(struct drm_mm *mm, unsigned long start,
 		       unsigned long size);
 extern void drm_mm_takedown(struct drm_mm *mm);

commit f1938cd6e900a85de64184e46d841efc9efd3484
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Sep 8 11:32:08 2009 +1000

    drm: include seq_file.h for debugfs builds.
    
    Fixes a warning seen on powerpc.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index bc5a87e8aeea..62329f9a42cb 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -37,6 +37,9 @@
  * Generic range manager structs
  */
 #include <linux/list.h>
+#ifdef CONFIG_DEBUG_FS
+#include <linux/seq_file.h>
+#endif
 
 struct drm_mm_node {
 	struct list_head fl_entry;

commit fa8a123855e20068204982596b8fafceb1a67f0b
Author: Dave Airlie <airlied@linux.ie>
Date:   Wed Aug 26 13:13:37 2009 +1000

    drm/mm: add ability to dump mm lists via debugfs
    
    This adds code to the drm_mm to talk to debugfs, and adds
    support to radeon to add the VRAM and GTT mm lists to debugfs.
    
    I tested with spinlock debugging and it doesn't give out.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index f8332073d277..bc5a87e8aeea 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -96,4 +96,8 @@ static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
 	return block->mm;
 }
 
+#ifdef CONFIG_DEBUG_FS
+int drm_mm_dump_table(struct seq_file *m, struct drm_mm *mm);
+#endif
+
 #endif

commit 89579f778266d5a4d08d0c64c46b1565218de9f9
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Wed Jun 17 12:29:56 2009 +0200

    drm: Apply "Memory fragmentation from lost alignment blocks"
    
    also for the atomic path by using a common code-path.
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index 5662f4278ef3..f8332073d277 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -59,13 +59,22 @@ struct drm_mm {
 /*
  * Basic range manager support (drm_mm.c)
  */
-
-extern struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
-					    unsigned long size,
-					    unsigned alignment);
-extern struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *parent,
+extern struct drm_mm_node *drm_mm_get_block_generic(struct drm_mm_node *node,
+						    unsigned long size,
+						    unsigned alignment,
+						    int atomic);
+static inline struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
 						   unsigned long size,
-						   unsigned alignment);
+						   unsigned alignment)
+{
+	return drm_mm_get_block_generic(parent, size, alignment, 0);
+}
+static inline struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *parent,
+							  unsigned long size,
+							  unsigned alignment)
+{
+	return drm_mm_get_block_generic(parent, size, alignment, 1);
+}
 extern void drm_mm_put_block(struct drm_mm_node *cur);
 extern struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
 					      unsigned long size,

commit 249d6048ca98b5452105b0824abac1275661b8e3
Author: Jerome Glisse <glisse@freedesktop.org>
Date:   Wed Apr 8 17:11:16 2009 +0200

    drm: Split out the mm declarations in a separate header. Add atomic operations.
    
    this is a TTM preparation patch, it rearranges the mm and
    add operations needed to do mm operations in atomic context.
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
new file mode 100644
index 000000000000..5662f4278ef3
--- /dev/null
+++ b/include/drm/drm_mm.h
@@ -0,0 +1,90 @@
+/**************************************************************************
+ *
+ * Copyright 2006-2008 Tungsten Graphics, Inc., Cedar Park, TX. USA.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ *
+ **************************************************************************/
+/*
+ * Authors:
+ * Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#ifndef _DRM_MM_H_
+#define _DRM_MM_H_
+
+/*
+ * Generic range manager structs
+ */
+#include <linux/list.h>
+
+struct drm_mm_node {
+	struct list_head fl_entry;
+	struct list_head ml_entry;
+	int free;
+	unsigned long start;
+	unsigned long size;
+	struct drm_mm *mm;
+	void *private;
+};
+
+struct drm_mm {
+	struct list_head fl_entry;
+	struct list_head ml_entry;
+	struct list_head unused_nodes;
+	int num_unused;
+	spinlock_t unused_lock;
+};
+
+/*
+ * Basic range manager support (drm_mm.c)
+ */
+
+extern struct drm_mm_node *drm_mm_get_block(struct drm_mm_node *parent,
+					    unsigned long size,
+					    unsigned alignment);
+extern struct drm_mm_node *drm_mm_get_block_atomic(struct drm_mm_node *parent,
+						   unsigned long size,
+						   unsigned alignment);
+extern void drm_mm_put_block(struct drm_mm_node *cur);
+extern struct drm_mm_node *drm_mm_search_free(const struct drm_mm *mm,
+					      unsigned long size,
+					      unsigned alignment,
+					      int best_match);
+extern int drm_mm_init(struct drm_mm *mm, unsigned long start,
+		       unsigned long size);
+extern void drm_mm_takedown(struct drm_mm *mm);
+extern int drm_mm_clean(struct drm_mm *mm);
+extern unsigned long drm_mm_tail_space(struct drm_mm *mm);
+extern int drm_mm_remove_space_from_tail(struct drm_mm *mm,
+					 unsigned long size);
+extern int drm_mm_add_space_to_tail(struct drm_mm *mm,
+				    unsigned long size, int atomic);
+extern int drm_mm_pre_get(struct drm_mm *mm);
+
+static inline struct drm_mm *drm_get_mm(struct drm_mm_node *block)
+{
+	return block->mm;
+}
+
+#endif
